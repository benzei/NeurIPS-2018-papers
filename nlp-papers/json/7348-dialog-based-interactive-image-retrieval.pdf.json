{
    "filename": "7348-dialog-based-interactive-image-retrieval.pdf",
    "metadata": {
        "title": "Dialog-based Interactive Image Retrieval",
        "author": "Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio Feris",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7348-dialog-based-interactive-image-retrieval.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface."
    },
    "keywords": [
        {
            "term": "gated recurrent unit",
            "url": "https://en.wikipedia.org/wiki/gated_recurrent_unit"
        },
        {
            "term": "image retrieval",
            "url": "https://en.wikipedia.org/wiki/image_retrieval"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "dialog system",
            "url": "https://en.wikipedia.org/wiki/dialog_system"
        },
        {
            "term": "image search",
            "url": "https://en.wikipedia.org/wiki/image_search"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "The volume of searchable visual media has grown tremendously in recent years, and has intensified the need for retrieval systems that can more effectively identify relevant information, with applications in domains such as e-commerce [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], surveillance [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], and Internet search [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "All experiments were performed on the Shoes dataset [<a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>], with the same training and testing data split for all retrieval methods and for training the user simulator. 10, 000 database images were used during training, and 4, 658 images for testing",
        "To investigate how retrieval performance is affected by each component of the dialog manager, we compare our approach, denoted as Ours, against two variants: (1) SL: supervised learning where the agent is trained only with triplet loss; (2) reinforcement learning-Self-Critical Sequence Training: policy learning using Self-Critical Sequence Training (SCST) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] after pre-training the network using the triplet loss objective",
        "This paper introduced a novel and practical task residing at the intersection of computer vision and language understanding: dialog-based interactive image retrieval",
        "Enabling users to provide natural language feedback, significantly outperforms traditional methods relying on a pre-defined vocabulary of relative attributes, while offering more natural communication",
        "We are optimistic that our approach for image retrieval can be extended to other media types such as audio, video, and e-books, given the performance of deep learning on tasks such as speech recognition, machine translation, and activity recognition"
    ],
    "key_statements": [
        "The volume of searchable visual media has grown tremendously in recent years, and has intensified the need for retrieval systems that can more effectively identify relevant information, with applications in domains such as e-commerce [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], surveillance [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], and Internet search [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "We propose a new approach to interactive visual content retrieval by introducing a novel form of user feedback based on natural language",
        "We model the ranking percentile as the environment reward received by the agent and frame the learning process in a reinforcement learning setting with the goal of maximizing the expected sum of discounted rewards: max\u03c0 U \u03c0 = E",
        "To evaluate the value of using free-form dialog feedback, we show experiments considering both simulated user feedback (Section 5.2) and real-world user feedback (Section 5.3)",
        "All experiments were performed on the Shoes dataset [<a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>], with the same training and testing data split for all retrieval methods and for training the user simulator. 10, 000 database images were used during training, and 4, 658 images for testing",
        "The retrieval models are tested by retrieving images on the testing set, starting from a randomly selected candidate image for the first dialog turn",
        "Image retrieval performance is quantified by the average rank percentile of the image returned by the dialog manager on the test set",
        "Qualitative examination of the generated relative expressions showed that the user simulator can approximate feedback of real users at a very low annotation cost",
        "To investigate how retrieval performance is affected by each component of the dialog manager, we compare our approach, denoted as Ours, against two variants: (1) SL: supervised learning where the agent is trained only with triplet loss; (2) reinforcement learning-Self-Critical Sequence Training: policy learning using Self-Critical Sequence Training (SCST) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] after pre-training the network using the triplet loss objective",
        "As shown in Figure 4, the average ranking percentile of the target image in all methods increases monotonically as the number of dialog turns increases. Both reinforcement learning-based retrieval algorithms outperform the supervised pre-training, SL, which is expected since the triplet loss function does not directly optimize the retrieval ranking objective",
        "Ours achieves 98% average ranking percentile with only two dialog turns and consistently outperforms reinforcement learning-Self-Critical Sequence Training across different dialog turns, which demonstrates the benefit of the model-based policy improvement component",
        "We trained the dialog manager using both dialog-based feedback and attribute-based feedback (Attrn and Attrn), where the subscript number denotes the number of attributes used in the rule-based feedback generator and denote baselines using deep learning based attribute estimates as in [<a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>]",
        "The empirical result is summarized in Figure 4, including relative attribute feedback using one, three and ten attribute phrases",
        "The three attribute case matches the average length of user feedback in free-form texts and the ten case uses all possible pre-defined attributes to provide feedback",
        "The results suggest that feedback based on unrestricted natural language is more effective for retrieval than the predefined set of relative attributes used in [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "We conjecture that the main reason underlying the performance gap between attribute and free-form text based models is the effectively open domain for attribute use, which is difficult to realize in a practical user interface without natural language",
        "This paper introduced a novel and practical task residing at the intersection of computer vision and language understanding: dialog-based interactive image retrieval",
        "We demonstrated the value of the proposed learning architecture on the application of interactive fashion footwear retrieval",
        "Enabling users to provide natural language feedback, significantly outperforms traditional methods relying on a pre-defined vocabulary of relative attributes, while offering more natural communication",
        "We are optimistic that our approach for image retrieval can be extended to other media types such as audio, video, and e-books, given the performance of deep learning on tasks such as speech recognition, machine translation, and activity recognition"
    ],
    "summary": [
        "The volume of searchable visual media has grown tremendously in recent years, and has intensified the need for retrieval systems that can more effectively identify relevant information, with applications in domains such as e-commerce [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], surveillance [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], and Internet search [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].",
        "We propose a new approach to interactive visual content retrieval by introducing a novel form of user feedback based on natural language.",
        "These methods, retrieve images and videos in a single turn, whereas our proposed approach aggregates history information from dialog-based feedback and iteratively provides more refined results.",
        "These dialogs are purely text-based for both the questioner and answerer agents, whereas we address the interactive image retrieval problem, with an agent presenting images to the user to seek feedback in natural language.",
        "At the t-th dialog turn, the Response Encoder embeds a candidate image and the corresponding user feedback {at, ot} into a joint visual-semantic representation xt \u2208 RD.",
        "This memory-based design of the State Tracker allows our model to sequentially aggregate the information from user feedback to localize the candidate image to be retrieved.",
        "Specific, and relative user feedback, we provided a sentence prefix for the annotator to complete when composing their response to a retrieved image.",
        "Our captioner and dialog-based interactive retriever are trained on both discriminative and relative captions, so as to be respectively more representative of and responsive to real users.",
        "To investigate how retrieval performance is affected by each component of the dialog manager, we compare our approach, denoted as Ours, against two variants: (1) SL: supervised learning where the agent is trained only with triplet loss; (2) RL-SCST: policy learning using Self-Critical Sequence Training (SCST) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] after pre-training the network using the triplet loss objective.",
        "Across different numbers of dialog turns, the natural language based agent produced significantly higher target image average ranking percentile than the attribute based methods.",
        "The results suggest that feedback based on unrestricted natural language is more effective for retrieval than the predefined set of relative attributes used in [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "We conjecture that the main reason underlying the performance gap between attribute and free-form text based models is the effectively open domain for attribute use, which is difficult to realize in a practical user interface without natural language.",
        "In addition to improved retrieval accuracy, users reported that providing dialog-based feedback is more natural compared to selecting the most relevant attributes from a pre-defined list.",
        "Enabling users to provide natural language feedback, significantly outperforms traditional methods relying on a pre-defined vocabulary of relative attributes, while offering more natural communication.",
        "We are optimistic that our approach for image retrieval can be extended to other media types such as audio, video, and e-books, given the performance of deep learning on tasks such as speech recognition, machine translation, and activity recognition"
    ],
    "headline": "We introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Junshi Huang, Rogerio S Feris, Qiang Chen, and Shuicheng Yan. Cross-domain image retrieval with a dual attribute-aware ranking network. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Junshi%20Feris%2C%20Rogerio%20S.%20Chen%2C%20Qiang%20Yan%2C%20Shuicheng%20Cross-domain%20image%20retrieval%20with%20a%20dual%20attribute-aware%20ranking%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Junshi%20Feris%2C%20Rogerio%20S.%20Chen%2C%20Qiang%20Yan%2C%20Shuicheng%20Cross-domain%20image%20retrieval%20with%20a%20dual%20attribute-aware%20ranking%20network%202015"
        },
        {
            "id": "2",
            "entry": "[2] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Qiu%2C%20Shi%20Wang%2C%20Xiaogang%20Deepfashion%3A%20Powering%20robust%20clothes%20recognition%20and%20retrieval%20with%20rich%20annotations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Qiu%2C%20Shi%20Wang%2C%20Xiaogang%20Deepfashion%3A%20Powering%20robust%20clothes%20recognition%20and%20retrieval%20with%20rich%20annotations%202016"
        },
        {
            "id": "3",
            "entry": "[3] Daniel A Vaquero, Rogerio S Feris, Duan Tran, Lisa Brown, Arun Hampapur, and Matthew Turk. Attributebased people search in surveillance environments. In WACV, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaquero%2C%20Daniel%20A.%20Feris%2C%20Rogerio%20S.%20Tran%2C%20Duan%20Brown%2C%20Lisa%20Attributebased%20people%20search%20in%20surveillance%20environments%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaquero%2C%20Daniel%20A.%20Feris%2C%20Rogerio%20S.%20Tran%2C%20Duan%20Brown%2C%20Lisa%20Attributebased%20people%20search%20in%20surveillance%20environments%202009"
        },
        {
            "id": "4",
            "entry": "[4] Zhiyuan Shi, Timothy M Hospedales, and Tao Xiang. Transferring a semantic representation for person re-identification and search. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Zhiyuan%20Hospedales%2C%20Timothy%20M.%20Xiang%2C%20Tao%20Transferring%20a%20semantic%20representation%20for%20person%20re-identification%20and%20search%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Zhiyuan%20Hospedales%2C%20Timothy%20M.%20Xiang%2C%20Tao%20Transferring%20a%20semantic%20representation%20for%20person%20re-identification%20and%20search%202015"
        },
        {
            "id": "5",
            "entry": "[5] Albert Gordo, Jon Almaz\u00e1n, Jerome Revaud, and Diane Larlus. Deep image retrieval: Learning global representations for image search. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordo%2C%20Albert%20Almaz%C3%A1n%2C%20Jon%20Revaud%2C%20Jerome%20Larlus%2C%20Diane%20Deep%20image%20retrieval%3A%20Learning%20global%20representations%20for%20image%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordo%2C%20Albert%20Almaz%C3%A1n%2C%20Jon%20Revaud%2C%20Jerome%20Larlus%2C%20Diane%20Deep%20image%20retrieval%3A%20Learning%20global%20representations%20for%20image%20search%202016"
        },
        {
            "id": "6",
            "entry": "[6] Herve Jegou, Florent Perronnin, Matthijs Douze, Jorge S\u00e1nchez, Patrick Perez, and Cordelia Schmid. Aggregating local image descriptors into compact codes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(9):1704\u20131716, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aggregating%20local%20image%20descriptors%20into%20compact%20codes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aggregating%20local%20image%20descriptors%20into%20compact%20codes%202012"
        },
        {
            "id": "7",
            "entry": "[7] Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learning fine-grained image similarity with deep ranking. In CVPR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jiang%20Song%2C%20Yang%20Leung%2C%20Thomas%20Rosenberg%2C%20Chuck%20Learning%20fine-grained%20image%20similarity%20with%20deep%20ranking%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jiang%20Song%2C%20Yang%20Leung%2C%20Thomas%20Rosenberg%2C%20Chuck%20Learning%20fine-grained%20image%20similarity%20with%20deep%20ranking%202014"
        },
        {
            "id": "8",
            "entry": "[8] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Larlus. End-to-end learning of deep visual representations for image retrieval. International Journal of Computer Vision, 124(2):237\u2013254, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordo%2C%20Albert%20Almazan%2C%20Jon%20Revaud%2C%20Jerome%20Larlus%2C%20Diane%20End-to-end%20learning%20of%20deep%20visual%20representations%20for%20image%20retrieval%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordo%2C%20Albert%20Almazan%2C%20Jon%20Revaud%2C%20Jerome%20Larlus%2C%20Diane%20End-to-end%20learning%20of%20deep%20visual%20representations%20for%20image%20retrieval%202017"
        },
        {
            "id": "9",
            "entry": "[9] Xiang Sean Zhou and Thomas S Huang. Relevance feedback in image retrieval: A comprehensive review. Multimedia systems, 8(6):536\u2013544, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Xiang%20Sean%20Huang%2C%20Thomas%20S.%20Relevance%20feedback%20in%20image%20retrieval%3A%20A%20comprehensive%20review%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Xiang%20Sean%20Huang%2C%20Thomas%20S.%20Relevance%20feedback%20in%20image%20retrieval%3A%20A%20comprehensive%20review%202003"
        },
        {
            "id": "10",
            "entry": "[10] Bart Thomee and Michael S Lew. Interactive search in image retrieval: a survey. International Journal of Multimedia Information Retrieval, 1(2):71\u201386, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomee%2C%20Bart%20Lew%2C%20Michael%20S.%20Interactive%20search%20in%20image%20retrieval%3A%20a%20survey%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomee%2C%20Bart%20Lew%2C%20Michael%20S.%20Interactive%20search%20in%20image%20retrieval%3A%20a%20survey%202012"
        },
        {
            "id": "11",
            "entry": "[11] Yong Rui, Thomas S Huang, Michael Ortega, and Sharad Mehrotra. Relevance feedback: a power tool for interactive content-based image retrieval. IEEE Transactions on circuits and systems for video technology, 8(5):644\u2013655, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rui%2C%20Yong%20Huang%2C%20Thomas%20S.%20Ortega%2C%20Michael%20Mehrotra%2C%20Sharad%20Relevance%20feedback%3A%20a%20power%20tool%20for%20interactive%20content-based%20image%20retrieval%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rui%2C%20Yong%20Huang%2C%20Thomas%20S.%20Ortega%2C%20Michael%20Mehrotra%2C%20Sharad%20Relevance%20feedback%3A%20a%20power%20tool%20for%20interactive%20content-based%20image%20retrieval%201998"
        },
        {
            "id": "12",
            "entry": "[12] Adriana Kovashka, Devi Parikh, and Kristen Grauman. Whittlesearch: Image search with relative attribute feedback. In CVPR, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kovashka%2C%20Adriana%20Parikh%2C%20Devi%20Grauman%2C%20Kristen%20Whittlesearch%3A%20Image%20search%20with%20relative%20attribute%20feedback%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kovashka%2C%20Adriana%20Parikh%2C%20Devi%20Grauman%2C%20Kristen%20Whittlesearch%3A%20Image%20search%20with%20relative%20attribute%20feedback%202012"
        },
        {
            "id": "13",
            "entry": "[13] Stefanie Tellex and Deb Roy. Towards surveillance video search by natural language query. In ACM International Conference on Image and Video Retrieval, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tellex%2C%20Stefanie%20Roy%2C%20Deb%20Towards%20surveillance%20video%20search%20by%20natural%20language%20query%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tellex%2C%20Stefanie%20Roy%2C%20Deb%20Towards%20surveillance%20video%20search%20by%20natural%20language%20query%202009"
        },
        {
            "id": "14",
            "entry": "[14] Andrei Barbu, N Siddharth, and Jeffrey Mark Siskind. Saying what you\u2019re looking for: Linguistics meets video search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrei%20Barbu%2C%20N.Siddharth%20Siskind%2C%20Jeffrey%20Mark%20Saying%20what%20you%E2%80%99re%20looking%20for%3A%20Linguistics%20meets%20video%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrei%20Barbu%2C%20N.Siddharth%20Siskind%2C%20Jeffrey%20Mark%20Saying%20what%20you%E2%80%99re%20looking%20for%3A%20Linguistics%20meets%20video%20search%202016"
        },
        {
            "id": "15",
            "entry": "[15] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, and Xiaogang Wang. Person search with natural language description. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Shuang%20Xiao%2C%20Tong%20Li%2C%20Hongsheng%20Zhou%2C%20Bolei%20Person%20search%20with%20natural%20language%20description%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Shuang%20Xiao%2C%20Tong%20Li%2C%20Hongsheng%20Zhou%2C%20Bolei%20Person%20search%20with%20natural%20language%20description%202017"
        },
        {
            "id": "16",
            "entry": "[16] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. Natural language object retrieval. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ronghang%20Hu%20Huazhe%20Xu%20Marcus%20Rohrbach%20Jiashi%20Feng%20Kate%20Saenko%20and%20Trevor%20Darrell%20Natural%20language%20object%20retrieval%20In%20CVPR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ronghang%20Hu%20Huazhe%20Xu%20Marcus%20Rohrbach%20Jiashi%20Feng%20Kate%20Saenko%20and%20Trevor%20Darrell%20Natural%20language%20object%20retrieval%20In%20CVPR%202016"
        },
        {
            "id": "17",
            "entry": "[17] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. Baby talk: Understanding and generating image descriptions. In CVPR, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Girish%20Premraj%2C%20Visruth%20Dhar%2C%20Sagnik%20Li%2C%20Siming%20Baby%20talk%3A%20Understanding%20and%20generating%20image%20descriptions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Girish%20Premraj%2C%20Visruth%20Dhar%2C%20Sagnik%20Li%2C%20Siming%20Baby%20talk%3A%20Understanding%20and%20generating%20image%20descriptions%202011"
        },
        {
            "id": "18",
            "entry": "[18] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Toshev%2C%20Alexander%20Bengio%2C%20Samy%20Erhan%2C%20Dumitru%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Toshev%2C%20Alexander%20Bengio%2C%20Samy%20Erhan%2C%20Dumitru%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015"
        },
        {
            "id": "19",
            "entry": "[19] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rennie%2C%20Steven%20J.%20Marcheret%2C%20Etienne%20Mroueh%2C%20Youssef%20Ross%2C%20Jarret%20Self-critical%20sequence%20training%20for%20image%20captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rennie%2C%20Steven%20J.%20Marcheret%2C%20Etienne%20Mroueh%2C%20Youssef%20Ross%2C%20Jarret%20Self-critical%20sequence%20training%20for%20image%20captioning%202017"
        },
        {
            "id": "20",
            "entry": "[20] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "21",
            "entry": "[21] Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. Context-aware captions from context-agnostic supervision. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vedantam%2C%20Ramakrishna%20Bengio%2C%20Samy%20Murphy%2C%20Kevin%20Parikh%2C%20Devi%20Context-aware%20captions%20from%20context-agnostic%20supervision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vedantam%2C%20Ramakrishna%20Bengio%2C%20Samy%20Murphy%2C%20Kevin%20Parikh%2C%20Devi%20Context-aware%20captions%20from%20context-agnostic%20supervision%202017"
        },
        {
            "id": "22",
            "entry": "[22] Myron Flickner, Harpreet Sawhney, et al. Query by image and video content: The qbic system. IEEE Computer, 28(9):23\u201332, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flickner%2C%20Myron%20Sawhney%2C%20Harpreet%20Query%20by%20image%20and%20video%20content%3A%20The%20qbic%20system%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flickner%2C%20Myron%20Sawhney%2C%20Harpreet%20Query%20by%20image%20and%20video%20content%3A%20The%20qbic%20system%201995"
        },
        {
            "id": "23",
            "entry": "[23] Yong Rui, Thomas S Huang, and Shih-Fu Chang. Image retrieval: Current techniques, promising directions, and open issues. Journal of visual communication and image representation, 10(1):39\u201362, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rui%2C%20Yong%20Huang%2C%20Thomas%20S.%20Chang%2C%20Shih-Fu%20Image%20retrieval%3A%20Current%20techniques%2C%20promising%20directions%2C%20and%20open%20issues%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rui%2C%20Yong%20Huang%2C%20Thomas%20S.%20Chang%2C%20Shih-Fu%20Image%20retrieval%3A%20Current%20techniques%2C%20promising%20directions%2C%20and%20open%20issues%201999"
        },
        {
            "id": "24",
            "entry": "[24] Hong Wu, Hanqing Lu, and Songde Ma. Willhunter: interactive image retrieval with multilevel relevance. In ICPR, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Hong%20Lu%2C%20Hanqing%20Ma%2C%20Songde%20Willhunter%3A%20interactive%20image%20retrieval%20with%20multilevel%20relevance%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Hong%20Lu%2C%20Hanqing%20Ma%2C%20Songde%20Willhunter%3A%20interactive%20image%20retrieval%20with%20multilevel%20relevance%202004"
        },
        {
            "id": "25",
            "entry": "[25] Adriana Kovashka and Kristen Grauman. Attributes for image retrieval. In Visual Attributes. Springer, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kovashka%2C%20Adriana%20Grauman%2C%20Kristen%20Attributes%20for%20image%20retrieval.%20In%20Visual%20Attributes%202017"
        },
        {
            "id": "26",
            "entry": "[26] Adriana Kovashka and Kristen Grauman. Attribute pivots for guiding relevance feedback in image search. In ICCV, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kovashka%2C%20Adriana%20Grauman%2C%20Kristen%20Attribute%20pivots%20for%20guiding%20relevance%20feedback%20in%20image%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kovashka%2C%20Adriana%20Grauman%2C%20Kristen%20Attribute%20pivots%20for%20guiding%20relevance%20feedback%20in%20image%20search%202013"
        },
        {
            "id": "27",
            "entry": "[27] Devi Parikh and Kristen Grauman. Relative attributes. In ICCV, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parikh%2C%20Devi%20Grauman%2C%20Kristen%20Relative%20attributes%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parikh%2C%20Devi%20Grauman%2C%20Kristen%20Relative%20attributes%202011"
        },
        {
            "id": "28",
            "entry": "[28] Aron Yu and Kristen Grauman. Fine-grained comparisons with attributes. In Visual Attributes. Springer, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Aron%20Grauman%2C%20Kristen%20Fine-grained%20comparisons%20with%20attributes.%20In%20Visual%20Attributes%202017"
        },
        {
            "id": "29",
            "entry": "[29] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanislaw%20Antol%20Aishwarya%20Agrawal%20Jiasen%20Lu%20Margaret%20Mitchell%20Dhruv%20Batra%20C%20Lawrence%20Zitnick%20and%20Devi%20Parikh%20Vqa%20Visual%20question%20answering%20In%20ICCV%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanislaw%20Antol%20Aishwarya%20Agrawal%20Jiasen%20Lu%20Margaret%20Mitchell%20Dhruv%20Batra%20C%20Lawrence%20Zitnick%20and%20Devi%20Parikh%20Vqa%20Visual%20question%20answering%20In%20ICCV%202015"
        },
        {
            "id": "30",
            "entry": "[30] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tapaswi%2C%20Makarand%20Zhu%2C%20Yukun%20Stiefelhagen%2C%20Rainer%20Torralba%2C%20Antonio%20Movieqa%3A%20Understanding%20stories%20in%20movies%20through%20question-answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tapaswi%2C%20Makarand%20Zhu%2C%20Yukun%20Stiefelhagen%2C%20Rainer%20Torralba%2C%20Antonio%20Movieqa%3A%20Understanding%20stories%20in%20movies%20through%20question-answering%202016"
        },
        {
            "id": "31",
            "entry": "[31] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013"
        },
        {
            "id": "32",
            "entry": "[32] Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Liwei%20Li%2C%20Yin%20Lazebnik%2C%20Svetlana%20Learning%20deep%20structure-preserving%20image-text%20embeddings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Liwei%20Li%2C%20Yin%20Lazebnik%2C%20Svetlana%20Learning%20deep%20structure-preserving%20image-text%20embeddings%202016"
        },
        {
            "id": "33",
            "entry": "[33] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases in images by reconstruction. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rohrbach%2C%20Anna%20Rohrbach%2C%20Marcus%20Hu%2C%20Ronghang%20Darrell%2C%20Trevor%20Grounding%20of%20textual%20phrases%20in%20images%20by%20reconstruction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rohrbach%2C%20Anna%20Rohrbach%2C%20Marcus%20Hu%2C%20Ronghang%20Darrell%2C%20Trevor%20Grounding%20of%20textual%20phrases%20in%20images%20by%20reconstruction%202016"
        },
        {
            "id": "34",
            "entry": "[34] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Plummer%2C%20Bryan%20A.%20Wang%2C%20Liwei%20Cervantes%2C%20Chris%20M.%20Caicedo%2C%20Juan%20C.%20Flickr30k%20entities%3A%20Collecting%20region-to-phrase%20correspondences%20for%20richer%20image-to-sentence%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Plummer%2C%20Bryan%20A.%20Wang%2C%20Liwei%20Cervantes%2C%20Chris%20M.%20Caicedo%2C%20Juan%20C.%20Flickr30k%20entities%3A%20Collecting%20region-to-phrase%20correspondences%20for%20richer%20image-to-sentence%20models%202015"
        },
        {
            "id": "35",
            "entry": "[35] Jason D Williams and Steve Young. Partially observable markov decision processes for spoken dialog systems. Computer Speech & Language, 21(2):393\u2013422, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Jason%20D.%20Young%2C%20Steve%20Partially%20observable%20markov%20decision%20processes%20for%20spoken%20dialog%20systems%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Jason%20D.%20Young%2C%20Steve%20Partially%20observable%20markov%20decision%20processes%20for%20spoken%20dialog%20systems%202007"
        },
        {
            "id": "36",
            "entry": "[36] Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. In AAAI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20C.%20Building%20end-to-end%20dialogue%20systems%20using%20generative%20hierarchical%20neural%20network%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20C.%20Building%20end-to-end%20dialogue%20systems%20using%20generative%20hierarchical%20neural%20network%20models%202016"
        },
        {
            "id": "37",
            "entry": "[37] Antoine Bordes and Jason Weston. Learning end-to-end goal-oriented dialog. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bordes%2C%20Antoine%20Weston%2C%20Jason%20Learning%20end-to-end%20goal-oriented%20dialog%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bordes%2C%20Antoine%20Weston%2C%20Jason%20Learning%20end-to-end%20goal-oriented%20dialog%202017"
        },
        {
            "id": "38",
            "entry": "[38] Xiaoxiao Guo, Tim Klinger, Clemens Rosenbaum, Joseph P Bigus, Murray Campbell, Ban Kawas, Kartik Talamadupula, Gerry Tesauro, and Satinder Singh. Learning to query, reason, and answer questions on ambiguous texts. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Xiaoxiao%20Klinger%2C%20Tim%20Rosenbaum%2C%20Clemens%20Bigus%2C%20Joseph%20P.%20Learning%20to%20query%2C%20reason%2C%20and%20answer%20questions%20on%20ambiguous%20texts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Xiaoxiao%20Klinger%2C%20Tim%20Rosenbaum%2C%20Clemens%20Bigus%2C%20Joseph%20P.%20Learning%20to%20query%2C%20reason%2C%20and%20answer%20questions%20on%20ambiguous%20texts%202017"
        },
        {
            "id": "39",
            "entry": "[39] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abhishek%20Das%20Satwik%20Kottur%20Khushi%20Gupta%20Avi%20Singh%20Deshraj%20Yadav%20Jos%C3%A9%20MF%20Moura%20Devi%20Parikh%20and%20Dhruv%20Batra%20Visual%20dialog%20In%20CVPR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abhishek%20Das%20Satwik%20Kottur%20Khushi%20Gupta%20Avi%20Singh%20Deshraj%20Yadav%20Jos%C3%A9%20MF%20Moura%20Devi%20Parikh%20and%20Dhruv%20Batra%20Visual%20dialog%20In%20CVPR%202017"
        },
        {
            "id": "40",
            "entry": "[40] Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. Guesswhat?! visual object discovery through multi-modal dialogue. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Vries%2C%20Harm%20Strub%2C%20Florian%20Chandar%2C%20Sarath%20Pietquin%2C%20Olivier%20Guesswhat%3F%21%20visual%20object%20discovery%20through%20multi-modal%20dialogue%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Vries%2C%20Harm%20Strub%2C%20Florian%20Chandar%2C%20Sarath%20Pietquin%2C%20Olivier%20Guesswhat%3F%21%20visual%20object%20discovery%20through%20multi-modal%20dialogue%202017"
        },
        {
            "id": "41",
            "entry": "[41] Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, and Leonid Sigal. Visual reference resolution using attention memory for visual dialog. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20Paul%20Hongsuck%20Lehrmann%2C%20Andreas%20Han%2C%20Bohyung%20Sigal%2C%20Leonid%20Visual%20reference%20resolution%20using%20attention%20memory%20for%20visual%20dialog%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seo%2C%20Paul%20Hongsuck%20Lehrmann%2C%20Andreas%20Han%2C%20Bohyung%20Sigal%2C%20Leonid%20Visual%20reference%20resolution%20using%20attention%20memory%20for%20visual%20dialog%202017"
        },
        {
            "id": "42",
            "entry": "[42] Abhishek Das, Satwik Kottur, Jos\u00e9 MF Moura, Stefan Lee, and Dhruv Batra. Learning cooperative visual dialog agents with deep reinforcement learning. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20Abhishek%20Kottur%2C%20Satwik%20Moura%2C%20Jos%C3%A9%20M.F.%20Lee%2C%20Stefan%20Learning%20cooperative%20visual%20dialog%20agents%20with%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20Abhishek%20Kottur%2C%20Satwik%20Moura%2C%20Jos%C3%A9%20M.F.%20Lee%2C%20Stefan%20Learning%20cooperative%20visual%20dialog%20agents%20with%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "43",
            "entry": "[43] Florian Strub, Harm de Vries, Jeremie Mary, Bilal Piot, Aaron Courville, and Olivier Pietquin. End-to-end optimization of goal-driven and visually grounded dialogue systems. In IJCAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strub%2C%20Florian%20de%20Vries%2C%20Harm%20Mary%2C%20Jeremie%20Piot%2C%20Bilal%20End-to-end%20optimization%20of%20goal-driven%20and%20visually%20grounded%20dialogue%20systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strub%2C%20Florian%20de%20Vries%2C%20Harm%20Mary%2C%20Jeremie%20Piot%2C%20Bilal%20End-to-end%20optimization%20of%20goal-driven%20and%20visually%20grounded%20dialogue%20systems%202017"
        },
        {
            "id": "44",
            "entry": "[44] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "45",
            "entry": "[45] Yoon Kim. Convolutional neural networks for sentence classification. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014"
        },
        {
            "id": "46",
            "entry": "[46] Cheng Li, Paul Resnick, and Qiaozhu Mei. Multiple queries as bandit arms. In CIKM. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Cheng%20Resnick%2C%20Paul%20Mei%2C%20Qiaozhu%20Multiple%20queries%20as%20bandit%20arms.%20In%20CIKM%202016"
        },
        {
            "id": "47",
            "entry": "[47] Xuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, and Marc Najork. The lambdaloss framework for ranking metric optimization. In CIKM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xuanhui%20Li%2C%20Cheng%20Golbandi%2C%20Nadav%20Bendersky%2C%20Michael%20The%20lambdaloss%20framework%20for%20ranking%20metric%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xuanhui%20Li%2C%20Cheng%20Golbandi%2C%20Nadav%20Bendersky%2C%20Michael%20The%20lambdaloss%20framework%20for%20ranking%20metric%20optimization%202018"
        },
        {
            "id": "48",
            "entry": "[48] Xiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong Li, Jianfeng Gao, and Yun-Nung Chen. A user simulator for task-completion dialogues. arXiv preprint arXiv:1612.05688, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.05688"
        },
        {
            "id": "49",
            "entry": "[49] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Licheng%20Yu%20Patrick%20Poirson%20Shan%20Yang%20Alexander%20C%20Berg%20and%20Tamara%20L%20Berg%20Modeling%20context%20in%20referring%20expressions%20In%20ECCV%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Licheng%20Yu%20Patrick%20Poirson%20Shan%20Yang%20Alexander%20C%20Berg%20and%20Tamara%20L%20Berg%20Modeling%20context%20in%20referring%20expressions%20In%20ECCV%202016"
        },
        {
            "id": "50",
            "entry": "[50] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015"
        },
        {
            "id": "51",
            "entry": "[51] Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, and Xiaoshi Wang. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Xiaoxiao%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Deep%20learning%20for%20real-time%20atari%20game%20play%20using%20offline%20monte-carlo%20tree%20search%20planning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Xiaoxiao%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Deep%20learning%20for%20real-time%20atari%20game%20play%20using%20offline%20monte-carlo%20tree%20search%20planning%202014"
        },
        {
            "id": "52",
            "entry": "[52] Cheng Li, Yue Wang, Paul Resnick, and Qiaozhu Mei. Req-rec: High recall retrieval with query pooling and interactive classification. In SIGIR. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Cheng%20Wang%2C%20Yue%20Resnick%2C%20Paul%20Mei%2C%20Qiaozhu%20Req-rec%3A%20High%20recall%20retrieval%20with%20query%20pooling%20and%20interactive%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Cheng%20Wang%2C%20Yue%20Resnick%2C%20Paul%20Mei%2C%20Qiaozhu%20Req-rec%3A%20High%20recall%20retrieval%20with%20query%20pooling%20and%20interactive%20classification%202014"
        },
        {
            "id": "53",
            "entry": "[53] Tamara L Berg, Alexander C Berg, and Jonathan Shih. Automatic attribute discovery and characterization from noisy web data. In ECCV, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berg%2C%20Tamara%20L.%20Berg%2C%20Alexander%20C.%20Shih%2C%20Jonathan%20Automatic%20attribute%20discovery%20and%20characterization%20from%20noisy%20web%20data%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berg%2C%20Tamara%20L.%20Berg%2C%20Alexander%20C.%20Shih%2C%20Jonathan%20Automatic%20attribute%20discovery%20and%20characterization%20from%20noisy%20web%20data%202010"
        },
        {
            "id": "54",
            "entry": "[54] Yaser Souri, Erfan Noury, and Ehsan Adeli. Deep relative attributes. In Asian Conference on Computer Vision, pages 118\u2013133.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Souri%2C%20Yaser%20Noury%2C%20Erfan%20Adeli%2C%20Ehsan%20Deep%20relative%20attributes",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Souri%2C%20Yaser%20Noury%2C%20Erfan%20Adeli%2C%20Ehsan%20Deep%20relative%20attributes"
        },
        {
            "id": "55",
            "entry": "[55] Subhransu Maji. Discovering a lexicon of parts and attributes. In ECCV, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maji%2C%20Subhransu%20Discovering%20a%20lexicon%20of%20parts%20and%20attributes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maji%2C%20Subhransu%20Discovering%20a%20lexicon%20of%20parts%20and%20attributes%202012"
        }
    ]
}
