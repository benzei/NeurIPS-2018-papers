{
    "filename": "7959-unsupervised-text-style-transfer-using-language-models-as-discriminators.pdf",
    "metadata": {
        "title": "Unsupervised Text Style Transfer using Language Models as Discriminators",
        "author": "Zichao Yang, Zhiting Hu, Chris Dyer, Eric P. Xing, Taylor Berg-Kirkpatrick",
        "date": 2016,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7959-unsupervised-text-style-transfer-using-language-models-as-discriminators.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Binary classifiers are often employed as discriminators in GAN-based unsupervised style transfer systems to ensure that transferred sentences are similar to sentences in the target domain. One difficulty with this approach is that the error signal provided by the discriminator can be unstable and is sometimes insufficient to train the generator to produce fluent language. In this paper, we propose a new technique that uses a target domain language model as the discriminator, providing richer and more stable token-level feedback during the learning process. We train the generator to minimize the negative log likelihood (NLL) of generated sentences, evaluated by the language model. By using a continuous approximation of discrete sampling under the generator, our model can be trained using back-propagation in an end-to-end fashion. Moreover, our empirical results show that when using a language model as a structured discriminator, it is possible to forgo adversarial steps during training, making the process more stable. We compare our model with previous work that uses convolutional networks (CNNs) as discriminators, as well as a broad set of other approaches. Results show that the proposed method achieves improved performance on three tasks: word substitution decipherment, sentiment modification, and related language translation."
    },
    "keywords": [
        {
            "term": "language model",
            "url": "https://en.wikipedia.org/wiki/language_model"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "natural language generation",
            "url": "https://en.wikipedia.org/wiki/natural_language_generation"
        },
        {
            "term": "non parallel",
            "url": "https://en.wikipedia.org/wiki/non_parallel"
        },
        {
            "term": "binary classifier",
            "url": "https://en.wikipedia.org/wiki/binary_classifier"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        }
    ],
    "highlights": [
        "There has been growing interest in designing natural language generation (NLG) systems that allow for control over various attributes of generated text \u2013 for example, sentiment and other stylistic properties",
        "We propose to use an implicitly trained language model as a new type of discriminator, replacing the more conventional binary classifier",
        "We show that our approach, which uses only a language model as the discriminator, outperforms a broad set of state-of-the-art approaches on the three tasks.\n2 Unsupervised Text Style Transfer",
        "If a transfered sentence does not match the target style, it will have high perplexity when evaluated by a language model that was trained on target domain data",
        "We showed that by using language models as discriminators and we could outperform traditional binary classifier discriminators in three unsupervised text style transfer tasks including word substitution decipherment, sentiment modification and related language translation",
        "We empirically found that it is possible to eliminate adversarial training with negative samples if a structured model is used as the discriminator, pointing one possible direction to solve the training difficulty of generative adversarial networks"
    ],
    "key_statements": [
        "There has been growing interest in designing natural language generation (NLG) systems that allow for control over various attributes of generated text \u2013 for example, sentiment and other stylistic properties",
        "We propose to use an implicitly trained language model as a new type of discriminator, replacing the more conventional binary classifier",
        "Because the language model scores sentences directly using a product of locally normalized probabilities, it may offer more stable and more useful training signal to the generator",
        "We find empirically that when using the language model as a structured discriminator, it is possible to eliminate adversarial training steps that use negative samples\u2014a critical part of traditional adversarial training",
        "We show that our approach, which uses only a language model as the discriminator, outperforms a broad set of state-of-the-art approaches on the three tasks.\n2 Unsupervised Text Style Transfer",
        "Previous works (<a class=\"ref-link\" id=\"cBowman_et+al_2015_a\" href=\"#rBowman_et+al_2015_a\">Bowman et al, 2015</a>; Yang et al, 2017b) found that there is a training collapse problem with the variational auto-encoders for text modeling and the posterior distribution of z fails to capture the content of a sentence",
        "We propose instead to use locally-normalized language models as discriminators",
        "We argue that using an explicit language model with token-level locally normalized probabilities offers a more direct training signal to the generator",
        "If a transfered sentence does not match the target style, it will have high perplexity when evaluated by a language model that was trained on target domain data",
        "Since the LM is a structured discriminator, we would hope that a language model trained on the real sentences will automatically assign high perplexity to sentences not in the target domain, negative samples from the generator may not be necessary",
        "We use discrete samples from the generators as negative samples in training the language model discriminator step, while we use a continuous approximation in updating the generator step according to Equation 5",
        "We found that the language model prefers short sentences",
        "In order to verify the effectiveness of our model, we experiment on three tasks: word substitution decipherment, sentiment modification, and related language translation",
        "Comparing with (<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a>), we can see that the language model without adversarial training is already very effective and performs much better when the amount of change is less than 100%",
        "We have demonstrated that the language model can successfully crack word substitution cipher",
        "We showed that by using language models as discriminators and we could outperform traditional binary classifier discriminators in three unsupervised text style transfer tasks including word substitution decipherment, sentiment modification and related language translation",
        "We empirically found that it is possible to eliminate adversarial training with negative samples if a structured model is used as the discriminator, pointing one possible direction to solve the training difficulty of generative adversarial networks"
    ],
    "summary": [
        "There has been growing interest in designing natural language generation (NLG) systems that allow for control over various attributes of generated text \u2013 for example, sentiment and other stylistic properties.",
        "We propose to use an implicitly trained language model as a new type of discriminator, replacing the more conventional binary classifier.",
        "Because the language model scores sentences directly using a product of locally normalized probabilities, it may offer more stable and more useful training signal to the generator.",
        "Previous works (<a class=\"ref-link\" id=\"cBowman_et+al_2015_a\" href=\"#rBowman_et+al_2015_a\">Bowman et al, 2015</a>; Yang et al, 2017b) found that there is a training collapse problem with the VAE for text modeling and the posterior distribution of z fails to capture the content of a sentence.",
        "If a transfered sentence does not match the target style, it will have high perplexity when evaluated by a language model that was trained on target domain data.",
        "Since the LM is a structured discriminator, we would hope that a language model trained on the real sentences will automatically assign high perplexity to sentences not in the target domain, negative samples from the generator may not be necessary.",
        "Note that the use of the language model discriminator is a somewhat different in each of the two types of training update steps because of the continuous approximation.",
        "We use discrete samples from the generators as negative samples in training the language model discriminator step, while we use a continuous approximation in updating the generator step according to Equation 5.",
        "In order to verify the effectiveness of our model, we experiment on three tasks: word substitution decipherment, sentiment modification, and related language translation.",
        "Comparing with (<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a>), we can see that the language model without adversarial training is already very effective and performs much better when the amount of change is less than 100%.",
        "We would like to investigate whether a language model can distinguish sentences with positive and negative sentiments, help to transfer the sentiments of sentences while preserving the content.",
        "We use the news data and sample about 200k sentences of length less than 20 for each language, of which 80% are used for training, 10% are used for validation and remaining 10% are used for test.",
        "(<a class=\"ref-link\" id=\"cLample_et+al_2017_a\" href=\"#rLample_et+al_2017_a\">Lample et al, 2017</a>; <a class=\"ref-link\" id=\"cArtetxe_et+al_2017_a\" href=\"#rArtetxe_et+al_2017_a\">Artetxe et al, 2017</a>) propose unsupervised machine translation and use adversarial training to match the encoder representation of the sentences from different languages.",
        "We showed that by using language models as discriminators and we could outperform traditional binary classifier discriminators in three unsupervised text style transfer tasks including word substitution decipherment, sentiment modification and related language translation.",
        "We plan to explore and extend our model to semi-supervised learning"
    ],
    "headline": "We propose a new technique that uses a target domain language model as the discriminator, providing richer and more stable token-level feedback during the learning process",
    "reference_links": [
        {
            "id": "Arjovsky_2017_a",
            "entry": "M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "Artetxe_et+al_2017_a",
            "entry": "M. Artetxe, G. Labaka, E. Agirre, and K. Cho. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11041"
        },
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Bowman_et+al_2015_a",
            "entry": "S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06349"
        },
        {
            "id": "Brants_et+al_2007_a",
            "entry": "T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brants%2C%20T.%20Popat%2C%20A.C.%20Xu%2C%20P.%20Och%2C%20F.J.%20Large%20language%20models%20in%20machine%20translation%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brants%2C%20T.%20Popat%2C%20A.C.%20Xu%2C%20P.%20Och%2C%20F.J.%20Large%20language%20models%20in%20machine%20translation%202007"
        },
        {
            "id": "Che_et+al_2017_a",
            "entry": "T. Che, Y. Li, R. Zhang, R. D. Hjelm, W. Li, Y. Song, and Y. Bengio. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07983"
        },
        {
            "id": "Chen_et+al_0000_a",
            "entry": "J. Chen, P.-S. Huang, X. He, J. Gao, and L. Deng. Unsupervised learning of predictors from unpaired input-output samples. arXiv preprint arXiv:1606.04646, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04646"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172\u20132180, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "Denton_et+al_2015_a",
            "entry": "E. L. Denton, S. Chintala, R. Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pages 1486\u20131494, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.L.%20Chintala%2C%20S.%20Fergus%2C%20R.%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.L.%20Chintala%2C%20S.%20Fergus%2C%20R.%20Deep%20generative%20image%20models%20using%20a%20laplacian%20pyramid%20of%20adversarial%20networks%202015"
        },
        {
            "id": "Dou_2012_a",
            "entry": "Q. Dou and K. Knight. Large scale decipherment for out-of-domain machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 266\u2013275. Association for Computational Linguistics, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dou%2C%20Q.%20Knight%2C%20K.%20Large%20scale%20decipherment%20for%20out-of-domain%20machine%20translation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dou%2C%20Q.%20Knight%2C%20K.%20Large%20scale%20decipherment%20for%20out-of-domain%20machine%20translation%202012"
        },
        {
            "id": "Fu_et+al_2017_a",
            "entry": "Z. Fu, X. Tan, N. Peng, D. Zhao, and R. Yan. Style transfer in text: Exploration and evaluation. arXiv preprint arXiv:1711.06861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06861"
        },
        {
            "id": "Gatys_et+al_2016_a",
            "entry": "L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pages 2414\u20132423. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Gomez_et+al_2018_a",
            "entry": "A. N. Gomez, S. Huang, I. Zhang, B. M. Li, M. Osama, and L. Kaiser. Unsupervised cipher cracking using discrete gans. arXiv preprint arXiv:1801.04883, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04883"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulcehre_et+al_2015_a",
            "entry": "C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin, F. Bougares, H. Schwenk, and Y. Bengio. On using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.03535"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pages 820\u2013828, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20D.%20Xia%2C%20Y.%20Qin%2C%20T.%20Wang%2C%20L.%20Dual%20learning%20for%20machine%20translation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20D.%20Xia%2C%20Y.%20Qin%2C%20T.%20Wang%2C%20L.%20Dual%20learning%20for%20machine%20translation%202016"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In International Conference on Machine Learning, pages 1587\u20131596, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017"
        },
        {
            "id": "Hu_et+al_0000_a",
            "entry": "Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing. On unifying deep generative models. arXiv preprint arXiv:1706.00550, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00550"
        },
        {
            "id": "Hu_et+al_0000_b",
            "entry": "Z. Hu, Z. Yang, R. Salakhutdinov, X. Liang, L. Qin, H. Dong, and E. Xing. Deep generative models with learnable knowledge constraints. arXiv preprint arXiv:1806.09764, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09764"
        },
        {
            "id": "Hu_et+al_0000_c",
            "entry": "Z. Hu, Z. Yang, T. Zhao, H. Shi, J. He, D. Wang, X. Ma, Z. Liu, X. Liang, L. Qin, et al. Texar: A modularized, versatile, and extensible toolbox for text generation. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 13\u201322, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Yang%2C%20Z.%20Zhao%2C%20T.%20Shi%2C%20H.%20Texar%3A%20A%20modularized%2C%20versatile%2C%20and%20extensible%20toolbox%20for%20text%20generation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Yang%2C%20Z.%20Zhao%2C%20T.%20Shi%2C%20H.%20Texar%3A%20A%20modularized%2C%20versatile%2C%20and%20extensible%20toolbox%20for%20text%20generation"
        },
        {
            "id": "Huang_2017_a",
            "entry": "X. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. CoRR, abs/1703.06868, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06868"
        },
        {
            "id": "Isola_et+al_2017_a",
            "entry": "P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks.%20arXiv%20p%202017"
        },
        {
            "id": "Jang_et+al_2016_a",
            "entry": "E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Lamb_et+al_2016_a",
            "entry": "A. M. Lamb, A. G. A. P. GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances In Neural Information Processing Systems, pages 4601\u20134609, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lamb%2C%20A.M.%20GOYAL%2C%20A.G.A.P.%20Zhang%2C%20Y.%20Zhang%2C%20S.%20Professor%20forcing%3A%20A%20new%20algorithm%20for%20training%20recurrent%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lamb%2C%20A.M.%20GOYAL%2C%20A.G.A.P.%20Zhang%2C%20Y.%20Zhang%2C%20S.%20Professor%20forcing%3A%20A%20new%20algorithm%20for%20training%20recurrent%20networks%202016"
        },
        {
            "id": "Lample_et+al_2017_a",
            "entry": "G. Lample, L. Denoyer, and M. Ranzato. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00043"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "J. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.06547"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "J. Li, R. Jia, H. He, and P. Liang. Delete, retrieve, generate: A simple approach to sentiment and style transfer. arXiv preprint arXiv:1804.06437, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06437"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "K. Lin, D. Li, X. He, Z. Zhang, and M.-T. Sun. Adversarial ranking for language generation. In Advances in Neural Information Processing Systems, pages 3155\u20133165, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20K.%20Li%2C%20D.%20He%2C%20X.%20Zhang%2C%20Z.%20Adversarial%20ranking%20for%20language%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20K.%20Li%2C%20D.%20He%2C%20X.%20Zhang%2C%20Z.%20Adversarial%20ranking%20for%20language%20generation%202017"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Y. Liu, J. Chen, and L. Deng. Unsupervised sequence classification using sequential output statistics. In Advances in Neural Information Processing Systems, pages 3550\u20133559, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Y.%20Chen%2C%20J.%20Deng%2C%20L.%20Unsupervised%20sequence%20classification%20using%20sequential%20output%20statistics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Y.%20Chen%2C%20J.%20Deng%2C%20L.%20Unsupervised%20sequence%20classification%20using%20sequential%20output%20statistics%202017"
        },
        {
            "id": "Pourdamghani_2017_a",
            "entry": "N. Pourdamghani and K. Knight. Deciphering related languages. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2513\u20132518, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pourdamghani%2C%20N.%20Knight%2C%20K.%20Deciphering%20related%20languages%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pourdamghani%2C%20N.%20Knight%2C%20K.%20Deciphering%20related%20languages%202017"
        },
        {
            "id": "Prabhumoye_et+al_2018_a",
            "entry": "S. Prabhumoye, Y. Tsvetkov, R. Salakhutdinov, and A. W. Black. Style transfer through backtranslation. arXiv preprint arXiv:1804.09000, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09000"
        },
        {
            "id": "Radford_et+al_2015_a",
            "entry": "A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "T. Shen, T. Lei, R. Barzilay, and T. Jaakkola. Style transfer from non-parallel text by cross-alignment. In Advances in Neural Information Processing Systems, pages 6833\u20136844, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20T.%20Lei%2C%20T.%20Barzilay%2C%20R.%20Jaakkola%2C%20T.%20Style%20transfer%20from%20non-parallel%20text%20by%20cross-alignment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20T.%20Lei%2C%20T.%20Barzilay%2C%20R.%20Jaakkola%2C%20T.%20Style%20transfer%20from%20non-parallel%20text%20by%20cross-alignment%202017"
        },
        {
            "id": "Sutton_et+al_2000_a",
            "entry": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20McAllester%2C%20D.A.%20Singh%2C%20S.P.%20Mansour%2C%20Y.%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20McAllester%2C%20D.A.%20Singh%2C%20S.P.%20Mansour%2C%20Y.%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "Vinyals_2015_a",
            "entry": "O. Vinyals and Q. Le. A neural conversational model. arXiv preprint arXiv:1506.05869, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.05869"
        },
        {
            "id": "Vinyals_et+al_2015_b",
            "entry": "O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3156\u20133164. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Toshev%2C%20A.%20Bengio%2C%20S.%20Erhan%2C%20D.%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Toshev%2C%20A.%20Bengio%2C%20S.%20Erhan%2C%20D.%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015"
        },
        {
            "id": "Wen_et+al_2016_a",
            "entry": "T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona, P.-H. Su, S. Ultes, and S. Young. A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.04562"
        },
        {
            "id": "Yang_et+al_0000_a",
            "entry": "Z. Yang, W. Chen, F. Wang, and B. Xu. Improving neural machine translation with conditional sequence generative adversarial nets. arXiv preprint arXiv:1703.04887, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04887"
        },
        {
            "id": "Yang_et+al_0000_b",
            "entry": "Z. Yang, Z. Hu, R. Salakhutdinov, and T. Berg-Kirkpatrick. Improved variational autoencoders for text modeling using dilated convolutions. arXiv preprint arXiv:1702.08139, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08139"
        },
        {
            "id": "Yu_et+al_2017_a",
            "entry": "L. Yu, W. Zhang, J. Wang, and Y. Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852\u20132858, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20L.%20Zhang%2C%20W.%20Wang%2C%20J.%20Yu%2C%20Y.%20Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20L.%20Zhang%2C%20W.%20Wang%2C%20J.%20Yu%2C%20Y.%20Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient%202017"
        },
        {
            "id": "Zhao_et+al_2016_a",
            "entry": "J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03126"
        },
        {
            "id": "Zhu_et+al_2017_a",
            "entry": "J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        }
    ]
}
