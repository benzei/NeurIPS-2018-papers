{
    "filename": "7368-on-the-dimensionality-of-word-embedding.pdf",
    "metadata": {
        "title": "On the Dimensionality of Word Embedding",
        "date": 2018,
        "author": "Zi Yin Stanford University s0960974@gmail.com",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we provide a theoretical understanding of word embedding and its dimensionality. Motivated by the unitary-invariance of word embedding, we propose the Pairwise Inner Product (PIP) loss, a novel metric on the dissimilarity between word embeddings. Using techniques from matrix perturbation theory, we reveal a fundamental bias-variance trade-off in dimensionality selection for word embeddings. This bias-variance trade-off sheds light on many empirical observations which were previously unexplained, for example the existence of an optimal dimensionality. Moreover, new insights and discoveries, like when and how word embeddings are robust to over-fitting, are revealed. By optimizing over the biasvariance trade-off of the PIP loss, we can explicitly answer the open question of dimensionality selection for word embedding."
    },
    "keywords": [
        {
            "term": "perturbation theory",
            "url": "https://en.wikipedia.org/wiki/perturbation_theory"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "long short-term memory",
            "url": "https://en.wikipedia.org/wiki/long_short-term_memory"
        },
        {
            "term": "matrix factorization",
            "url": "https://en.wikipedia.org/wiki/matrix_factorization"
        },
        {
            "term": "short term",
            "url": "https://en.wikipedia.org/wiki/short_term"
        },
        {
            "term": "Pointwise Mutual Information",
            "url": "https://en.wikipedia.org/wiki/Pointwise_Mutual_Information"
        },
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "text summarization",
            "url": "https://en.wikipedia.org/wiki/text_summarization"
        }
    ],
    "highlights": [
        "Word embeddings are very useful and versatile tools, serving as keys to many fundamental problems in numerous NLP research [Turney and Pantel, 2010]",
        "Some prominent examples are long short-term memory (LSTM) networks [<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a>] that are used for language modeling [<a class=\"ref-link\" id=\"cBengio_et+al_2003_a\" href=\"#rBengio_et+al_2003_a\">Bengio et al, 2003</a>], machine translation [Sutskever et al, 2014, <a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\">Bahdanau et al, 2014</a>], text summarization [<a class=\"ref-link\" id=\"cNallapati_et+al_2016_a\" href=\"#rNallapati_et+al_2016_a\">Nallapati et al, 2016</a>] and image caption generation [Xu et al, 2015, Vinyals et al, 2015]",
        "We propose the Pairwise Inner Product (PIP) loss, which naturally arises from the unitary-invariance, as the dissimilarity metric between two word embeddings: Definition 1 (PIP matrix)",
        "We propose the Pairwise Inner Product loss, a metric of dissimilarity between word embeddings",
        "We focus on embedding algorithms that can be formulated as explicit or implicit matrix factorizations including the widely-used Latent Semantics Analysis, skip-gram and GloVe, and reveal a bias-variance trade-off in dimensionality selection using matrix perturbation theory",
        "We discover the robustness of word embeddings trained from these algorithms and its relationship to the exponent parameter \u03b1"
    ],
    "key_statements": [
        "Word embeddings are very useful and versatile tools, serving as keys to many fundamental problems in numerous NLP research [Turney and Pantel, 2010]",
        "Word embeddings are widely applied in information retrieval [<a class=\"ref-link\" id=\"cSalton_1971_a\" href=\"#rSalton_1971_a\">Salton, 1971</a>, <a class=\"ref-link\" id=\"cSalton_1988_a\" href=\"#rSalton_1988_a\">Salton and Buckley, 1988</a>, Sparck Jones, 1972], recommendation systems [<a class=\"ref-link\" id=\"cBreese_et+al_1998_a\" href=\"#rBreese_et+al_1998_a\">Breese et al, 1998</a>, Yin et al, 2017], image description [<a class=\"ref-link\" id=\"cFrome_et+al_2013_a\" href=\"#rFrome_et+al_2013_a\">Frome et al, 2013</a>], relation discovery [Mikolov et al, 2013c] and word level translation [Mikolov et al, 2013b]",
        "Numerous important applications are built on top of word embeddings",
        "Some prominent examples are long short-term memory (LSTM) networks [<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a>] that are used for language modeling [<a class=\"ref-link\" id=\"cBengio_et+al_2003_a\" href=\"#rBengio_et+al_2003_a\">Bengio et al, 2003</a>], machine translation [Sutskever et al, 2014, <a class=\"ref-link\" id=\"cBahdanau_et+al_2014_a\" href=\"#rBahdanau_et+al_2014_a\">Bahdanau et al, 2014</a>], text summarization [<a class=\"ref-link\" id=\"cNallapati_et+al_2016_a\" href=\"#rNallapati_et+al_2016_a\">Nallapati et al, 2016</a>] and image caption generation [Xu et al, 2015, Vinyals et al, 2015]",
        "We introduce the Pairwise Inner Product loss, a novel metric on the dissimilarity between word embeddings; 2",
        "We propose a mathematically rigorous answer to the open problem of dimensionality selection by minimizing the Pairwise Inner Product loss",
        "We propose the Pairwise Inner Product (PIP) loss, which naturally arises from the unitary-invariance, as the dissimilarity metric between two word embeddings: Definition 1 (PIP matrix)",
        "To compare E1 and E2, two embedding matrices on a common vocabulary, we propose the Pairwise Inner Product loss: Definition 2 (PIP loss)",
        "We present the main theorem, which shows that the bias-variance trade-off reflects the \u201csignalto-noise ratio\u201d in dimensionality selection",
        "Noise Estimation We note that for most NLP tasks, the signal matrices are estimated by counting or transformations of counting, including taking log or normalization. This holds for word embeddings that are based on co-occurrence statistics, e.g., Latent Semantics Analysis, skip-gram and GloVe",
        "For three popular embedding algorithms: Latent Semantics Analysis, skip-gram Word2Vec and GloVe, we find their optimal dimensionalities k\u2217 that minimize their respective Pairwise Inner Product loss",
        "We define the sub-optimality of a particular dimensionality k as the additional Pairwise Inner Product loss compared with k\u2217: EkEkT \u2212 EET \u2212 Ek\u2217 Ek\u2217 T \u2212 EET",
        "The above three experiments show that our method is a powerful tool in practice: the dimensionalities selected according to empirical grid search agree with the Pairwise Inner Product-loss minimizing criterion, which can be done by knowing the spectrum and noise standard deviation",
        "We propose the Pairwise Inner Product loss, a metric of dissimilarity between word embeddings",
        "We focus on embedding algorithms that can be formulated as explicit or implicit matrix factorizations including the widely-used Latent Semantics Analysis, skip-gram and GloVe, and reveal a bias-variance trade-off in dimensionality selection using matrix perturbation theory",
        "We discover the robustness of word embeddings trained from these algorithms and its relationship to the exponent parameter \u03b1"
    ],
    "summary": [
        "Word embeddings are very useful and versatile tools, serving as keys to many fundamental problems in numerous NLP research [Turney and Pantel, 2010].",
        "We propose the Pairwise Inner Product (PIP) loss, which naturally arises from the unitary-invariance, as the dissimilarity metric between two word embeddings: Definition 1 (PIP matrix).",
        "With the PIP loss, we can study the quality of trained word embeddings for any algorithm that uses matrix factorization.",
        "This PIP loss is affected by k, the dimensionality we select for the trained embedding.",
        "Equipped with the PIP loss, we give a mathematical presentation of the bias-variance trade-off using matrix perturbation theory.",
        "The second is a dimensionality selection method by explicitly minimizing the PIP loss between the oracle and trained embeddings3.",
        "We compute the PIP loss of word embeddings for the PPMI LSA algorithm, and plot them for different \u03b1\u2019s in Figure 1a.",
        "Figure 1b and 1c display the performances of word embeddings of various dimensionalities from the PPMI LSA algorithm, evaluated on two word correlation tests: WordSim353 [<a class=\"ref-link\" id=\"cFinkelstein_et+al_2001_a\" href=\"#rFinkelstein_et+al_2001_a\">Finkelstein et al, 2001</a>] and MTurk771 [<a class=\"ref-link\" id=\"cHalawi_et+al_2012_a\" href=\"#rHalawi_et+al_2012_a\">Halawi et al, 2012</a>].",
        "This holds for word embeddings that are based on co-occurrence statistics, e.g., LSA, skip-gram and GloVe. We use a count-twice trick to estimate the noise: we randomly split the data into two large subsets, and get matrices M 1 = M + Z1, M 2 = M + Z2 in Rm\u00d7n, where Z1, Z2 are two independent copies of noise with variance 2\u03c32.",
        "For three popular embedding algorithms: LSA, skip-gram Word2Vec and GloVe, we find their optimal dimensionalities k\u2217 that minimize their respective PIP loss.",
        "We define the sub-optimality of a particular dimensionality k as the additional PIP loss compared with k\u2217: EkEkT \u2212 EET \u2212 Ek\u2217 Ek\u2217 T \u2212 EET .",
        "The empirically optimal dimensionalities that achieve highest correlations with human labels for the two word relatedness tests (WordSim353 and MTurk777) lie close to the theoretically selected k\u2217\u2019s.",
        "The optimal dimensionalities for WordSim353, MTurk771 and Google analogy tests are 56, 102 and 220 respectively for skip-gram.",
        "The above three experiments show that our method is a powerful tool in practice: the dimensionalities selected according to empirical grid search agree with the PIP-loss minimizing criterion, which can be done by knowing the spectrum and noise standard deviation.",
        "We focus on embedding algorithms that can be formulated as explicit or implicit matrix factorizations including the widely-used LSA, skip-gram and GloVe, and reveal a bias-variance trade-off in dimensionality selection using matrix perturbation theory.",
        "All of our discoveries are concretely validated on real datasets"
    ],
    "headline": "We provide a theoretical understanding of word embedding and its dimensionality",
    "reference_links": [
        {
            "id": "Arora_2016_a",
            "entry": "Sanjeev Arora. Word embeddings: Explaining their properties, 2016. URL http://www.offconvex.org/2016/02/14/word-embeddings-2/.[Online; accessed 16-May-2018].",
            "url": "http://www.offconvex.org/2016/02/14/word-embeddings-2/.[Online"
        },
        {
            "id": "Artetxe_et+al_2016_a",
            "entry": "Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2289\u20132294, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Artetxe%2C%20Mikel%20Labaka%2C%20Gorka%20Agirre%2C%20Eneko%20Learning%20principled%20bilingual%20mappings%20of%20word%20embeddings%20while%20preserving%20monolingual%20invariance%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artetxe%2C%20Mikel%20Labaka%2C%20Gorka%20Agirre%2C%20Eneko%20Learning%20principled%20bilingual%20mappings%20of%20word%20embeddings%20while%20preserving%20monolingual%20invariance%202016"
        },
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Baroni_et+al_2014_a",
            "entry": "Marco Baroni, Georgiana Dinu, and German Kruszewski. Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 238\u2013247, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baroni%2C%20Marco%20Dinu%2C%20Georgiana%20Kruszewski%2C%20German%20Don%E2%80%99t%20count%2C%20predict%21%20a%20systematic%20comparison%20of%20context-counting%20vs.%20context-predicting%20semantic%20vectors%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baroni%2C%20Marco%20Dinu%2C%20Georgiana%20Kruszewski%2C%20German%20Don%E2%80%99t%20count%2C%20predict%21%20a%20systematic%20comparison%20of%20context-counting%20vs.%20context-predicting%20semantic%20vectors%202014"
        },
        {
            "id": "Bengio_et+al_2003_a",
            "entry": "Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137\u20131155, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Ducharme%2C%20Rejean%20Vincent%2C%20Pascal%20Jauvin%2C%20Christian%20A%20neural%20probabilistic%20language%20model%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Ducharme%2C%20Rejean%20Vincent%2C%20Pascal%20Jauvin%2C%20Christian%20A%20neural%20probabilistic%20language%20model%202003"
        },
        {
            "id": "Bojanowski_et+al_2017_a",
            "entry": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146, 2017. ISSN 2307-387X.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Enriching%20word%20vectors%20with%20subword%20information%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Enriching%20word%20vectors%20with%20subword%20information%202017"
        },
        {
            "id": "Breese_et+al_1998_a",
            "entry": "John S Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for collaborative filtering. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence, pages 43\u201352. Morgan Kaufmann Publishers Inc., 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breese%2C%20John%20S.%20Heckerman%2C%20David%20Kadie%2C%20Carl%20Empirical%20analysis%20of%20predictive%20algorithms%20for%20collaborative%20filtering%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breese%2C%20John%20S.%20Heckerman%2C%20David%20Kadie%2C%20Carl%20Empirical%20analysis%20of%20predictive%20algorithms%20for%20collaborative%20filtering%201998"
        },
        {
            "id": "Bullinaria_2012_a",
            "entry": "John A Bullinaria and Joseph P Levy. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. Behavior research methods, 44(3):890\u2013907, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bullinaria%2C%20John%20A.%20Levy%2C%20Joseph%20P.%20Extracting%20semantic%20representations%20from%20word%20co-occurrence%20statistics%3A%20stop-lists%2C%20stemming%2C%20and%20svd%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bullinaria%2C%20John%20A.%20Levy%2C%20Joseph%20P.%20Extracting%20semantic%20representations%20from%20word%20co-occurrence%20statistics%3A%20stop-lists%2C%20stemming%2C%20and%20svd%202012"
        },
        {
            "id": "Cai_et+al_2010_a",
            "entry": "Jian-Feng Cai, Emmanuel J Candes, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Jian-Feng%20Candes%2C%20Emmanuel%20J.%20Shen%2C%20Zuowei%20A%20singular%20value%20thresholding%20algorithm%20for%20matrix%20completion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Jian-Feng%20Candes%2C%20Emmanuel%20J.%20Shen%2C%20Zuowei%20A%20singular%20value%20thresholding%20algorithm%20for%20matrix%20completion%202010"
        },
        {
            "id": "Candes_2009_a",
            "entry": "Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009"
        },
        {
            "id": "Caron_2001_a",
            "entry": "John Caron. Experiments with LSA scoring: Optimal rank and basis. Computational information retrieval, pages 157\u2013169, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caron%2C%20John%20Experiments%20with%20LSA%20scoring%3A%20Optimal%20rank%20and%20basis%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caron%2C%20John%20Experiments%20with%20LSA%20scoring%3A%20Optimal%20rank%20and%20basis%202001"
        },
        {
            "id": "Chatterjee_2015_a",
            "entry": "Sourav Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43(1):177\u2013214, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chatterjee%2C%20Sourav%20Matrix%20estimation%20by%20universal%20singular%20value%20thresholding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chatterjee%2C%20Sourav%20Matrix%20estimation%20by%20universal%20singular%20value%20thresholding%202015"
        },
        {
            "id": "Davis_1970_a",
            "entry": "Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1\u201346, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davis%2C%20Chandler%20Kahan%2C%20William%20Morton%20The%20rotation%20of%20eigenvectors%20by%20a%20perturbation.%20iii%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davis%2C%20Chandler%20Kahan%2C%20William%20Morton%20The%20rotation%20of%20eigenvectors%20by%20a%20perturbation.%20iii%201970"
        },
        {
            "id": "Dhillon_et+al_2015_a",
            "entry": "Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar. Eigenwords: spectral word embeddings. Journal of Machine Learning Research, 16:3035\u20133078, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhillon%2C%20Paramveer%20S.%20Foster%2C%20Dean%20P.%20Ungar%2C%20Lyle%20H.%20Eigenwords%3A%20spectral%20word%20embeddings%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhillon%2C%20Paramveer%20S.%20Foster%2C%20Dean%20P.%20Ungar%2C%20Lyle%20H.%20Eigenwords%3A%20spectral%20word%20embeddings%202015"
        },
        {
            "id": "Finkelstein_et+al_2001_a",
            "entry": "Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406\u2013414. ACM, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finkelstein%2C%20Lev%20Gabrilovich%2C%20Evgeniy%20Matias%2C%20Yossi%20Rivlin%2C%20Ehud%20Placing%20search%20in%20context%3A%20The%20concept%20revisited%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finkelstein%2C%20Lev%20Gabrilovich%2C%20Evgeniy%20Matias%2C%20Yossi%20Rivlin%2C%20Ehud%20Placing%20search%20in%20context%3A%20The%20concept%20revisited%202001"
        },
        {
            "id": "Frome_et+al_2013_a",
            "entry": "Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121\u20132129, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013"
        },
        {
            "id": "Halawi_et+al_2012_a",
            "entry": "Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. Large-scale learning of word relatedness with constraints. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1406\u20131414. ACM, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Halawi%2C%20Guy%20Dror%2C%20Gideon%20Gabrilovich%2C%20Evgeniy%20Koren%2C%20Yehuda%20Large-scale%20learning%20of%20word%20relatedness%20with%20constraints%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Halawi%2C%20Guy%20Dror%2C%20Gideon%20Gabrilovich%2C%20Evgeniy%20Koren%2C%20Yehuda%20Large-scale%20learning%20of%20word%20relatedness%20with%20constraints%202012"
        },
        {
            "id": "Hamilton_et+al_2016_a",
            "entry": "William L Hamilton, Jure Leskovec, and Dan Jurafsky. Diachronic word embeddings reveal statistical laws of semantic change. arXiv preprint arXiv:1605.09096, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09096"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20Jurgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Kato_2013_a",
            "entry": "Tosio Kato. Perturbation theory for linear operators, volume 132. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kato%2C%20Tosio%20Perturbation%20theory%20for%20linear%20operators%2C%20volume%20132%202013"
        },
        {
            "id": "Kong_2017_a",
            "entry": "Weihao Kong and Gregory Valiant. Spectrum estimation from samples. The Annals of Statistics, 45 (5):2218\u20132247, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kong%2C%20Weihao%20Valiant%2C%20Gregory%20Spectrum%20estimation%20from%20samples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kong%2C%20Weihao%20Valiant%2C%20Gregory%20Spectrum%20estimation%20from%20samples%202017"
        },
        {
            "id": "Lample_et+al_2016_a",
            "entry": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. Neural architectures for named entity recognition. In Proceedings of NAACL-HLT, pages 260\u2013 270, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20Guillaume%20Ballesteros%2C%20Miguel%20Subramanian%2C%20Sandeep%20Kawakami%2C%20Kazuya%20Neural%20architectures%20for%20named%20entity%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lample%2C%20Guillaume%20Ballesteros%2C%20Miguel%20Subramanian%2C%20Sandeep%20Kawakami%2C%20Kazuya%20Neural%20architectures%20for%20named%20entity%20recognition%202016"
        },
        {
            "id": "Levy_2014_a",
            "entry": "Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pages 2177\u20132185, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Neural%20word%20embedding%20as%20implicit%20matrix%20factorization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Neural%20word%20embedding%20as%20implicit%20matrix%20factorization%202014"
        },
        {
            "id": "Levy_et+al_0000_a",
            "entry": "Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013 225, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Dagan%2C%20Ido%20Improving%20distributional%20similarity%20with%20lessons%20learned%20from%20word%20embeddings",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Dagan%2C%20Ido%20Improving%20distributional%20similarity%20with%20lessons%20learned%20from%20word%20embeddings"
        },
        {
            "id": "Mahoney_2011_a",
            "entry": "Matt Mahoney. Large text compression benchmark, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahoney%2C%20Matt%20Large%20text%20compression%20benchmark%202011"
        },
        {
            "id": "Mikolov_et+al_0000_a",
            "entry": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013a.",
            "arxiv_url": "https://arxiv.org/pdf/1301.3781"
        },
        {
            "id": "Mikolov_et+al_0000_b",
            "entry": "Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168, 2013b.",
            "arxiv_url": "https://arxiv.org/pdf/1309.4168"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc., 2013c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "Mirsky_1960_a",
            "entry": "Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The quarterly journal of mathematics, 11(1):50\u201359, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirsky%2C%20Leon%20Symmetric%20gauge%20functions%20and%20unitarily%20invariant%20norms%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirsky%2C%20Leon%20Symmetric%20gauge%20functions%20and%20unitarily%20invariant%20norms%201960"
        },
        {
            "id": "Nallapati_et+al_2016_a",
            "entry": "Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Ca glar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond. CoNLL 2016, page 280, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nallapati%2C%20Ramesh%20Zhou%2C%20Bowen%20dos%20Santos%2C%20Cicero%20glar%20Gulcehre%2C%20Ca%20Abstractive%20text%20summarization%20using%20sequence-to-sequence%20rnns%20and%20beyond%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nallapati%2C%20Ramesh%20Zhou%2C%20Bowen%20dos%20Santos%2C%20Cicero%20glar%20Gulcehre%2C%20Ca%20Abstractive%20text%20summarization%20using%20sequence-to-sequence%20rnns%20and%20beyond%202016"
        },
        {
            "id": "Paige_1994_a",
            "entry": "Christopher C Paige and Musheng Wei. History and generality of the CS decomposition. Linear Algebra and its Applications, 208:303\u2013326, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paige%2C%20Christopher%20C.%20Wei%2C%20Musheng%20History%20and%20generality%20of%20the%20CS%20decomposition%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paige%2C%20Christopher%20C.%20Wei%2C%20Musheng%20History%20and%20generality%20of%20the%20CS%20decomposition%201994"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20GloVe%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20GloVe%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Salton_1971_a",
            "entry": "Gerard Salton. The SMART retrieval system\u2014experiments in automatic document processing. Prentice-Hall, Inc., 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salton%2C%20Gerard%20The%20SMART%20retrieval%20system%E2%80%94experiments%20in%20automatic%20document%20processing%201971"
        },
        {
            "id": "Salton_1988_a",
            "entry": "Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information processing & management, 24(5):513\u2013523, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salton%2C%20Gerard%20Buckley%2C%20Christopher%20Term-weighting%20approaches%20in%20automatic%20text%20retrieval%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salton%2C%20Gerard%20Buckley%2C%20Christopher%20Term-weighting%20approaches%20in%20automatic%20text%20retrieval%201988"
        },
        {
            "id": "Schnabel_et+al_2015_a",
            "entry": "Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. Evaluation methods for unsupervised word embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 298\u2013307, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schnabel%2C%20Tobias%20Labutov%2C%20Igor%20Mimno%2C%20David%20Joachims%2C%20Thorsten%20Evaluation%20methods%20for%20unsupervised%20word%20embeddings%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schnabel%2C%20Tobias%20Labutov%2C%20Igor%20Mimno%2C%20David%20Joachims%2C%20Thorsten%20Evaluation%20methods%20for%20unsupervised%20word%20embeddings%202015"
        },
        {
            "id": "Smith_et+al_2017_a",
            "entry": "Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. arXiv preprint arXiv:1702.03859, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03859"
        },
        {
            "id": "Socher_et+al_2013_a",
            "entry": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013"
        },
        {
            "id": "Jones_1972_a",
            "entry": "Karen Sparck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):11\u201321, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jones%2C%20Karen%20Sparck%20A%20statistical%20interpretation%20of%20term%20specificity%20and%20its%20application%20in%20retrieval%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jones%2C%20Karen%20Sparck%20A%20statistical%20interpretation%20of%20term%20specificity%20and%20its%20application%20in%20retrieval%201972"
        },
        {
            "id": "Stewart_1990_a",
            "entry": "Gilbert W Stewart. Stochastic perturbation theory. SIAM review, 32(4):579\u2013610, 1990. Gilbert W Stewart and Ji-guang Sun. Matrix perturbation theory. Academic press, 1990. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stewart%2C%20Gilbert%20W.%20Stochastic%20perturbation%20theory%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stewart%2C%20Gilbert%20W.%20Stochastic%20perturbation%20theory%201990"
        },
        {
            "id": "In_2014_a",
            "entry": "In Advances in neural information processing systems, pages 3104\u20133112, 2014. Peter D Turney. Domain and function: A dual-space model of semantic relations and compositions.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Peter%20D%20Turney.%20Domain%20and%20function%3A%20A%20dual-space%20model%20of%20semantic%20relations%20and%20compositions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Peter%20D%20Turney.%20Domain%20and%20function%3A%20A%20dual-space%20model%20of%20semantic%20relations%20and%20compositions%202014"
        },
        {
            "id": "Journal_2012_a",
            "entry": "Journal of Artificial Intelligence Research, 44:533\u2013585, 2012. Peter D Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Peter%20D%20Turney%20and%20Patrick%20Pantel.%20From%20frequency%20to%20meaning%3A%20Vector%20space%20models%20of%20semantics%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Peter%20D%20Turney%20and%20Patrick%20Pantel.%20From%20frequency%20to%20meaning%3A%20Vector%20space%20models%20of%20semantics%202012"
        }
    ]
}
