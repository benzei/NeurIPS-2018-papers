{
    "filename": "7739-densely-connected-attention-propagation-for-reading-comprehension.pdf",
    "metadata": {
        "title": "Densely Connected Attention Propagation for Reading Comprehension",
        "author": "Yi Tay, Anh Tuan Luu, Siu Cheung Hui, Jian Su",
        "date": 2016,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7739-densely-connected-attention-propagation-for-reading-comprehension.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose DECAPROP (Densely Connected Attention Propagation), a new densely connected neural architecture for reading comprehension (RC). There are two distinct characteristics of our model. Firstly, our model densely connects all pairwise layers of the network, modeling relationships between passage and query across all hierarchical levels. Secondly, the dense connectors in our network are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging connections throughout the network. We conduct extensive experiments on four challenging RC benchmarks. Our proposed approach achieves state-of-the-art results on all four, outperforming existing baselines by up to 2.6% \u2212 14.2% in absolute F1 score."
    },
    "keywords": [
        {
            "term": "reading comprehension",
            "url": "https://en.wikipedia.org/wiki/reading_comprehension"
        },
        {
            "term": "open domain question answering",
            "url": "https://en.wikipedia.org/wiki/open_domain_question_answering"
        },
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "language understanding",
            "url": "https://en.wikipedia.org/wiki/language_understanding"
        }
    ],
    "highlights": [
        "The dominant neural architectures for reading comprehension (RC) typically follow a standard \u2018encode-interact-point\u2019 design [<a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\"><a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\"><a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\">Wang and Jiang, 2016</a></a></a>; <a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\">Seo et al, 2016</a></a></a>; <a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\">Xiong et al, 2016</a></a></a>; Wang et al, 2017c; Kundu and Ng, 2018]",
        "Based on the above mentioned intuitions, this paper proposes a new architecture with two distinct characteristics",
        "We propose efficient Bidirectional Attention Connectors (BAC) as a base building block to connect two sequences at arbitrary layers",
        "We propose DECAPROP (Densely Connected Attention Propagation), a novel architecture for reading comprehension",
        "We proposed a new Densely Connected Attention Propagation (DECAPROP) mechanism",
        "We proposed Bidirectional Attention Connectors (BAC) for efficient connection of any two arbitary layers, producing connectors that can be propagated to deeper layers"
    ],
    "key_statements": [
        "The dominant neural architectures for reading comprehension (RC) typically follow a standard \u2018encode-interact-point\u2019 design [<a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\"><a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\"><a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\">Wang and Jiang, 2016</a></a></a>; <a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\">Seo et al, 2016</a></a></a>; <a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\">Xiong et al, 2016</a></a></a>; Wang et al, 2017c; Kundu and Ng, 2018]",
        "Based on the above mentioned intuitions, this paper proposes a new architecture with two distinct characteristics",
        "We propose efficient Bidirectional Attention Connectors (BAC) as a base building block to connect two sequences at arbitrary layers",
        "We propose DECAPROP (Densely Connected Attention Propagation), a novel architecture for reading comprehension",
        "The key goals of this module are to (1) connect any two layers of P/Q in the network, returning a residual feature that can be propagated1 to deeper layers, (2) model cross-hierarchical interactions between P/Q and (3) minimize any costs incurred to other network components such that this component may be executed multiple times across all layers",
        "We proposed a new Densely Connected Attention Propagation (DECAPROP) mechanism",
        "We proposed Bidirectional Attention Connectors (BAC) for efficient connection of any two arbitary layers, producing connectors that can be propagated to deeper layers"
    ],
    "summary": [
        "The dominant neural architectures for reading comprehension (RC) typically follow a standard \u2018encode-interact-point\u2019 design [<a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\"><a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\"><a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\">Wang and Jiang, 2016</a></a></a>; <a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\">Seo et al, 2016</a></a></a>; <a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\">Xiong et al, 2016</a></a></a>; Wang et al, 2017c; Kundu and Ng, 2018].",
        "An attention layer is used to model interactions between P/Q.",
        "We propose efficient Bidirectional Attention Connectors (BAC) as a base building block to connect two sequences at arbitrary layers.",
        "We propose DECAPROP (Densely Connected Attention Propagation), a novel architecture for reading comprehension.",
        "The key goals of this module are to (1) connect any two layers of P/Q in the network, returning a residual feature that can be propagated1 to deeper layers, (2) model cross-hierarchical interactions between P/Q and (3) minimize any costs incurred to other network components such that this component may be executed multiple times across all layers.",
        "In many standard neural QA models, it is common to pass an augmented2 matching vector of this attentional representation to subsequent layers.",
        "3 Densely Connected Attention Propagation (DECAPROP)",
        "3.2 Densely Connected Attention Encoder (DECAENC)",
        "We apply our attention connector (BAC) to HP /HQ \u2208 R where H represents the hidden state outputs from the BiRNN encoder where the RNN cell can either be a GRU or LSTM encoder.",
        "Gated Attention The outputs of the densely connected encoder are passed into a standard gated attention layer.",
        "Unlike the Densely Connected Attention Encoder, we no longer have two representations at each hierarchical level since they have already been \u2018fused\u2019.",
        "DECAPROP outperforms the existing state-of-the-art, i.e., the recent AMANDA model by (+4.7% EM / +2.6% F1).",
        "These models include BiDAF [<a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2016_a\" href=\"#rSeo_et+al_2016_a\">Seo et al, 2016</a></a>], Match-LSTM [<a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\"><a class=\"ref-link\" id=\"cWang_2016_a\" href=\"#rWang_2016_a\">Wang and Jiang, 2016</a></a>], DCN/DCN+ [<a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2016_a\" href=\"#rXiong_et+al_2016_a\">Xiong et al, 2016</a></a>, 2017], R-NET [Wang et al, 2017c], DrQA [<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>], AoA Reader [<a class=\"ref-link\" id=\"cCui_et+al_2016_a\" href=\"#rCui_et+al_2016_a\">Cui et al, 2016</a>], Reinforced Mnemonic Reader [<a class=\"ref-link\" id=\"cHu_et+al_2017_a\" href=\"#rHu_et+al_2017_a\">Hu et al, 2017</a>], ReasoNet [<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a>], AMANDA [Kundu and Ng, 2018], R3 Reinforced Reader Ranker [Wang et al, 2017a] and QANet [<a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\">Yu et al, 2018</a>].",
        "Many of these models innovate at either (1) the bidirectional attention layer (BiDAF, DCN), (2) invoking multi-hop reasoning (Mnemonic Reader, ReasoNet), (3) reinforcement learning (R3, DCN+), (4) self-attention (AMANDA, R-NET, QANet) and (5) improvements at the encoder level (QANet).",
        "We proposed a new Densely Connected Attention Propagation (DECAPROP) mechanism.",
        "We proposed Bidirectional Attention Connectors (BAC) for efficient connection of any two arbitary layers, producing connectors that can be propagated to deeper layers.",
        "Our proposed architecture achieves state-of-the-art performance on four challenging QA datasets, outperforming strong and competitive baselines such as Reinforced Reader Ranker (R3), AMANDA, BiDAF and R-NET."
    ],
    "headline": "We propose DECAPROP , a new densely connected neural architecture for reading comprehension ",
    "reference_links": [
        {
            "id": "Mart_2015_a",
            "entry": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org.",
            "url": "http://tensorflow.org/"
        },
        {
            "id": "Buck_et+al_2017_a",
            "entry": "Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, and Wei Wang. Ask the right questions: Active question reformulation with reinforcement learning. arXiv preprint arXiv:1705.07830, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07830"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv:1704.00051, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00051"
        },
        {
            "id": "Cui_et+al_2016_a",
            "entry": "Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. Attention-over-attention neural networks for reading comprehension. arXiv preprint arXiv:1607.04423, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04423"
        },
        {
            "id": "Devlin_et+al_2018_a",
            "entry": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devlin%2C%20Jacob%20Chang%2C%20Ming-Wei%20Lee%2C%20Kenton%20Toutanova%2C%20Kristina%20Bert%3A%20Pre-training%20of%20deep%20bidirectional%20transformers%20for%20language%20understanding%202018"
        },
        {
            "id": "Dhingra_et+al_2017_a",
            "entry": "Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03904"
        },
        {
            "id": "Ding_et+al_2018_a",
            "entry": "Zixiang Ding, Rui Xia, Jianfei Yu, Xiang Li, and Jian Yang. Densely connected bidirectional lstm with applications to sentence classification. arXiv preprint arXiv:1802.00889, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00889"
        },
        {
            "id": "Dunn_et+al_2017_a",
            "entry": "Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05179"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pages 1019\u20131027, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "Minghao Hu, Yuxing Peng, and Xipeng Qiu. Mnemonic reader for machine comprehension. arXiv preprint arXiv:1705.02798, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02798"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2261\u20132269, 2017. doi: 10.1109/CVPR. 2017.243. URL https://doi.org/10.1109/CVPR.2017.243.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2017.243",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/CVPR.2017.243"
        },
        {
            "id": "Joshi_et+al_2017_a",
            "entry": "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03551"
        },
        {
            "id": "Kadlec_et+al_2016_a",
            "entry": "Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. Text understanding with the attention sum reader network. arXiv preprint arXiv:1603.01547, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.01547"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kocisky_et+al_2017_a",
            "entry": "Tom\u00e1\u0161 Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. arXiv preprint arXiv:1712.07040, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07040"
        },
        {
            "id": "Souvik_2018_a",
            "entry": "Souvik Kundu and Hwee Tou Ng. A question-focused multi-factor attention network for question answering. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Souvik%20Kundu%20and%20Hwee%20Tou%20Ng.%20A%20question-focused%20multi-factor%20attention%20network%20for%20question%20answering%202018"
        },
        {
            "id": "Lai_et+al_2017_a",
            "entry": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04683"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6294\u20136305, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10-25",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10-25"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05365"
        },
        {
            "id": "Rajpurkar_et+al_2016_a",
            "entry": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.05250"
        },
        {
            "id": "Rendle_2010_a",
            "entry": "Steffen Rendle. Factorization machines. In Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 995\u20131000. IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rendle%2C%20Steffen%20Factorization%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rendle%2C%20Steffen%20Factorization%20machines%202010"
        },
        {
            "id": "Seo_et+al_2016_a",
            "entry": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01603"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1047\u20131055. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Yelong%20Huang%2C%20Po-Sen%20Gao%2C%20Jianfeng%20Chen%2C%20Weizhu%20Reasonet%3A%20Learning%20to%20stop%20reading%20in%20machine%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Yelong%20Huang%2C%20Po-Sen%20Gao%2C%20Jianfeng%20Chen%2C%20Weizhu%20Reasonet%3A%20Learning%20to%20stop%20reading%20in%20machine%20comprehension%202017"
        },
        {
            "id": "Rupesh_2015_a",
            "entry": "Rupesh Kumar Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Highway networks. CoRR, abs/1505.00387, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.00387"
        },
        {
            "id": "Szegedy_et+al_2015_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "Tay_et+al_2017_a",
            "entry": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. A compare-propagate architecture with alignment factorization for natural language inference. arXiv preprint arXiv:1801.00102, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00102"
        },
        {
            "id": "Tay_et+al_2018_a",
            "entry": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Multi-range reasoning for machine comprehension. arXiv preprint arXiv:1803.09074, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09074"
        },
        {
            "id": "Trischler_et+al_2016_a",
            "entry": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint arXiv:1611.09830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09830"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "Wang_2016_a",
            "entry": "Shuohang Wang and Jing Jiang. Machine comprehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.07905"
        },
        {
            "id": "Wang_et+al_0000_a",
            "entry": "Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced reader-ranker for open-domain question answering. arXiv preprint arXiv:1709.00023, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1709.00023"
        },
        {
            "id": "Wang_et+al_0000_b",
            "entry": "Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. arXiv preprint arXiv:1711.05116, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05116"
        },
        {
            "id": "Wang_et+al_0000_c",
            "entry": "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks for reading comprehension and question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 189\u2013198, 2017c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering"
        },
        {
            "id": "Weissenborn_2017_a",
            "entry": "Dirk Weissenborn. Reading twice for natural language understanding. arXiv preprint arXiv:1706.02596, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02596"
        },
        {
            "id": "Weissenborn_et+al_2017_b",
            "entry": "Dirk Weissenborn, Georg Wiese, and Laura Seiffe. Making neural qa as simple as possible but not simpler. arXiv preprint arXiv:1703.04816, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04816"
        },
        {
            "id": "Xiong_et+al_2016_a",
            "entry": "Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answering. CoRR, abs/1611.01604, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01604"
        },
        {
            "id": "Xiong_et+al_2017_a",
            "entry": "Caiming Xiong, Victor Zhong, and Richard Socher. Dcn+: Mixed objective and deep residual coattention for question answering. arXiv preprint arXiv:1711.00106, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00106"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09541"
        },
        {
            "id": "Zeiler_2012_a",
            "entry": "Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        }
    ]
}
