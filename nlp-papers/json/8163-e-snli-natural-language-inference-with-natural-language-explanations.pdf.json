{
    "filename": "8163-e-snli-natural-language-inference-with-natural-language-explanations.pdf",
    "metadata": {
        "title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
        "author": "Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, Phil Blunsom",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model\u2019s decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset1 thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust."
    },
    "keywords": [
        {
            "term": "hypothesis",
            "url": "https://en.wikipedia.org/wiki/hypothesis"
        },
        {
            "term": "textual entailment",
            "url": "https://en.wikipedia.org/wiki/textual_entailment"
        },
        {
            "term": "movie reviews",
            "url": "https://en.wikipedia.org/wiki/Movie_Reviews"
        }
    ],
    "highlights": [
        "The task of recognizing textual entailment is a critical natural language understanding task",
        "We present a series of experiments to elucidate whether models trained on e-Stanford Natural Language Inference are able to: (i) predict a label and generate an explanation for the predicted label, generate an explanation predict the label given only the generated explanation",
        "We investigate how the typical architecture employed on Stanford Natural Language Inference can be enhanced with a module that aims to justify the decisions of the entire network",
        "Results In Table 3, we present the performance of e-INFERSENT and our 2 baselines when evaluated without fine-tuning on SICK-E and MultiNLI",
        "We introduced e-Stanford Natural Language Inference, a large dataset of natural language explanations for an influential task of recognizing textual entailment",
        "We investigated the usefulness of these explanations as an additional training signal for learning better universal sentence representations and the transfer capabilities to out-of-domain NLI datasets"
    ],
    "key_statements": [
        "The task of recognizing textual entailment is a critical natural language understanding task",
        "We present our collection methodology for e-Stanford Natural Language Inference, for which we used Amazon Mechanical Turk",
        "We present a series of experiments to elucidate whether models trained on e-Stanford Natural Language Inference are able to: (i) predict a label and generate an explanation for the predicted label, generate an explanation predict the label given only the generated explanation",
        "We investigate how the typical architecture employed on Stanford Natural Language Inference can be enhanced with a module that aims to justify the decisions of the entire network",
        "We used the same two explanations as the only references when computing the BLEU-score for the predicted explanations",
        "Given the low inter-annotator score and the fact that generated explanations almost match the inter-annotator BLEU-score, we conclude that this measure is not reliable for our task, and we further rely on human evaluation",
        "Since the explanation is conditioned on the predicted label, for incorrect labels, the model would not produce a correct explanation",
        "While this percentage is low, we keep in mind that the model was selected on the accuracy of the label classifier",
        "In PREDICTANDEXPLAIN, we conditioned the explanation on the label predicted by the MLP, because we wanted to see how the typical architecture used on Stanford Natural Language Inference can be adapted to justify its decisions in natural language",
        "We mark with * the results that appeared significant under the significance level of 0.05",
        "We notice that INFERSENTAUTOENC is performing significantly worse than InferSent on 6 tasks and significantly outperforms this baseline on only 2 tasks",
        "Results In Table 3, we present the performance of e-INFERSENT and our 2 baselines when evaluated without fine-tuning on SICK-E and MultiNLI",
        "We notice that the accuracy improvements obtained with e-INFERSENT are very small",
        "We introduced e-Stanford Natural Language Inference, a large dataset of natural language explanations for an influential task of recognizing textual entailment",
        "We investigated the usefulness of these explanations as an additional training signal for learning better universal sentence representations and the transfer capabilities to out-of-domain NLI datasets",
        "Similar to the evaluation performed for visual question answering in Das et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], our highlighted words could provide a source of supervision and evaluation for attention models [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] or post-hoc explanation models where the explanation consists of a subset of the input"
    ],
    "summary": [
        "The task of recognizing textual entailment is a critical natural language understanding task.",
        "We call our explanation-augmented dataset e-SNLI, which we collected to enable research in the direction of training with and generation of free-form textual justifications.",
        "We implement models that, given a premise and a hypothesis, predict a label and an explanation.",
        "We separately train the same hypothesis-only encoder for label prediction alone and obtain 66 correct labels in the same first 100 test examples.",
        "In PREDICTANDEXPLAIN, we conditioned the explanation on the label predicted by the MLP, because we wanted to see how the typical architecture used on SNLI can be adapted to justify its decisions in natural language.",
        "When we again manually annotate the first 100 generated explanations in the test set, we obtain significantly higher percentages of correct explanations with 49.8% for EXPLAINTHENPREDICTSEQ2SEQ and 64.27% for EXPLAINTHENPREDICTATTENTION.",
        "We refer to their work for a more detailed description of each of these tasks and of SentEval, which we use for comparing the quality of the sentence embeddings obtained by providing our explanations on top of the label supervision.",
        "We conclude that training with explanations helps the model to learn better sentence representations that overall generalize better on downstream tasks.",
        "Interpretability One main direction in interpretability for neural networks is providing ex-Table 3: The average performance over 5 seeds tractive justifications, i.e., explanations consistof e-INFERSENT and the 2 baselines on SICKing of subsets of the raw input, such as words E and MultiNLI with no fine-tunning.",
        "Breaking natural language inference Recently, an increasing amount of analysis has been carried out on the SNLI dataset and on the inner workings of different models trained on it.",
        "High overlap in words between premise and hypothesis usually predicts entailment, while most contradictory sentence pairs have no or very little overlap of words.",
        "Glockner et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] introduce a toy dataset, BreakingNLI, to test whether natural language inference models capture world knowledge and generalize beyond statistical regularities.",
        "We introduced e-SNLI, a large dataset of natural language explanations for an influential task of recognizing textual entailment.",
        "We investigated the usefulness of these explanations as an additional training signal for learning better universal sentence representations and the transfer capabilities to out-of-domain NLI datasets.",
        "Similar to the evaluation performed for visual question answering in Das et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], our highlighted words could provide a source of supervision and evaluation for attention models [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] or post-hoc explanation models where the explanation consists of a subset of the input.",
        "We hope that e-SNLI will be valuable for future research on more advanced models that would outperform our baselines"
    ],
    "headline": "We extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alvarez-Melis, D. and Jaakkola, T. S. (2017). A causal framework for explaining the predictions of black-box sequence-to-sequence models. CoRR, abs/1707.01943.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01943"
        },
        {
            "id": "2",
            "entry": "[2] Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "3",
            "entry": "[3] Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A large annotated corpus for learning natural language inference. CoRR, abs/1508.05326.",
            "arxiv_url": "https://arxiv.org/pdf/1508.05326"
        },
        {
            "id": "4",
            "entry": "[4] Chan, W., Jaitly, N., Le, Q. V., and Vinyals, O. (2015). Listen, attend and spell. CoRR, abs/1508.01211.",
            "arxiv_url": "https://arxiv.org/pdf/1508.01211"
        },
        {
            "id": "5",
            "entry": "[5] Chen, Q., Zhu, X., Ling, Z., Inkpen, D., and Wei, S. (2017). Natural language inference with external knowledge. CoRR, abs/1711.04289.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04289"
        },
        {
            "id": "6",
            "entry": "[6] Chen, Q., Zhu, X., Ling, Z., Wei, S., and Jiang, H. (2016). Enhancing and combining sequential and tree LSTM for natural language inference. CoRR, abs/1609.06038.",
            "arxiv_url": "https://arxiv.org/pdf/1609.06038"
        },
        {
            "id": "7",
            "entry": "[7] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. CoRR, abs/1705.02364.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02364"
        },
        {
            "id": "8",
            "entry": "[8] Das, A., Agrawal, H., Zitnick, C. L., Parikh, D., and Batra, D. (2016). Human attention in visual question answering: Do humans and deep networks look at the same regions? CoRR, abs/1606.03556.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03556"
        },
        {
            "id": "9",
            "entry": "[9] Dasgupta, I., Guo, D., Stuhlm\u00fcller, A., Gershman, S. J., and Goodman, N. D. (2018). Evaluating compositionality in sentence embeddings.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20I.%20Guo%2C%20D.%20Stuhlm%C3%BCller%2C%20A.%20Gershman%2C%20S.J.%20Evaluating%20compositionality%20in%20sentence%20embeddings%202018"
        },
        {
            "id": "10",
            "entry": "[10] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. In Proc. of CVPR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "11",
            "entry": "[11] Glockner, M., Shwartz, V., and Goldberg, Y. (2018). Breaking NLI systems with sentences that require simple lexical inferences. In Proc. of ACL.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glockner%2C%20M.%20Shwartz%2C%20V.%20Goldberg%2C%20Y.%20Breaking%20NLI%20systems%20with%20sentences%20that%20require%20simple%20lexical%20inferences%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glockner%2C%20M.%20Shwartz%2C%20V.%20Goldberg%2C%20Y.%20Breaking%20NLI%20systems%20with%20sentences%20that%20require%20simple%20lexical%20inferences%202018"
        },
        {
            "id": "12",
            "entry": "[12] Gong, Y., Luo, H., and Zhang, J. (2017). Natural language inference over interaction space. CoRR, abs/1709.04348.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04348"
        },
        {
            "id": "13",
            "entry": "[13] Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., and Smith, N. A. (2018). Annotation artifacts in natural language inference data. In Proc. of NAACL.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gururangan%2C%20S.%20Swayamdipta%2C%20S.%20Levy%2C%20O.%20Schwartz%2C%20R.%20Annotation%20artifacts%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gururangan%2C%20S.%20Swayamdipta%2C%20S.%20Levy%2C%20O.%20Schwartz%2C%20R.%20Annotation%20artifacts%202018"
        },
        {
            "id": "14",
            "entry": "[14] Hill, F., Cho, K., and Korhonen, A. (2016). Learning distributed representations of sentences from unlabelled data. CoRR, abs/1602.03483.",
            "arxiv_url": "https://arxiv.org/pdf/1602.03483"
        },
        {
            "id": "15",
            "entry": "[15] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Comput., 9(8):1735\u20131780.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "16",
            "entry": "[16] Jansen, P. A., Wainwright, E., Marmorstein, S., and Morrison, C. T. (2018). Worldtree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. CoRR, abs/1802.03052.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03052"
        },
        {
            "id": "17",
            "entry": "[17] Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., and Fidler, S. (2015). Skip-thought vectors. CoRR, abs/1506.06726.",
            "arxiv_url": "https://arxiv.org/pdf/1506.06726"
        },
        {
            "id": "18",
            "entry": "[18] Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. (2017). Program induction by rationale generation: Learning to solve and explain algebraic word problems. CoRR, abs/1705.04146.",
            "arxiv_url": "https://arxiv.org/pdf/1705.04146"
        },
        {
            "id": "19",
            "entry": "[19] Liu, P., Qiu, X., and Huang, X. (2016). Modelling interaction of sentence pair with coupledlstms. CoRR, abs/1605.05573.",
            "arxiv_url": "https://arxiv.org/pdf/1605.05573"
        },
        {
            "id": "20",
            "entry": "[20] Marelli, M., Menini, S., Baroni, M., Bentivogli, L., bernardi, R., and Zamparelli, R. (2014). A sick cure for the evaluation of compositional distributional semantic models.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marelli%2C%20M.%20Menini%2C%20S.%20Baroni%2C%20M.%20Bentivogli%2C%20L.%20A%20sick%20cure%20for%20the%20evaluation%20of%20compositional%20distributional%20semantic%20models%202014"
        },
        {
            "id": "21",
            "entry": "[21] Nie, Y. and Bansal, M. (2017). Shortcut-stacked sentence encoders for multi-domain inference. In Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP, pages 41\u201345. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nie%2C%20Y.%20Bansal%2C%20M.%20Shortcut-stacked%20sentence%20encoders%20for%20multi-domain%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nie%2C%20Y.%20Bansal%2C%20M.%20Shortcut-stacked%20sentence%20encoders%20for%20multi-domain%20inference%202017"
        },
        {
            "id": "22",
            "entry": "[22] Parikh, A. P., T\u00e4ckstr\u00f6m, O., Das, D., and Uszkoreit, J. (2016). A decomposable attention model for natural language inference. CoRR, abs/1606.01933.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01933"
        },
        {
            "id": "23",
            "entry": "[23] Park, D. H., Hendricks, L. A., Akata, Z., Rohrbach, A., Schiele, B., Darrell, T., and Rohrbach, M. (2018). Multimodal explanations: Justifying decisions and pointing to the evidence. CoRR, abs/1802.08129.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08129"
        },
        {
            "id": "24",
            "entry": "[24] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). \"why should I trust you?\": Explaining the predictions of any classifier. CoRR, abs/1602.04938.",
            "arxiv_url": "https://arxiv.org/pdf/1602.04938"
        },
        {
            "id": "25",
            "entry": "[25] Rockt\u00e4schel, T., Grefenstette, E., Hermann, K. M., Kocisk\u00fd, T., and Blunsom, P. (2015). Reasoning about entailment with neural attention. CoRR, abs/1509.06664.",
            "arxiv_url": "https://arxiv.org/pdf/1509.06664"
        },
        {
            "id": "26",
            "entry": "[26] Williams, A., Nangia, N., and Bowman, S. R. (2017). A broad-coverage challenge corpus for sentence understanding through inference. CoRR, abs/1704.05426.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05426"
        },
        {
            "id": "27",
            "entry": "[27] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A. C., Salakhutdinov, R., Zemel, R. S., and Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention. CoRR, abs/1502.03044. ",
            "arxiv_url": "https://arxiv.org/pdf/1502.03044"
        }
    ]
}
