{
    "filename": "7723-recurrently-controlled-recurrent-networks.pdf",
    "metadata": {
        "title": "Recurrently Controlled Recurrent Networks",
        "author": "Yi Tay, Anh Tuan Luu, Siu Cheung Hui",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7723-recurrently-controlled-recurrent-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Recurrent neural networks (RNNs) such as long short-term memory and gated recurrent units are pivotal building blocks across a broad spectrum of sequence modeling problems. This paper proposes a recurrently controlled recurrent network (RCRN) for expressive and powerful sequence encoding. More concretely, the key idea behind our approach is to learn the recurrent gating functions using recurrent networks. Our architecture is split into two components - a controller cell and a listener cell whereby the recurrent controller actively influences the compositionality of the listener cell. We conduct extensive experiments on a myriad of tasks in the NLP domain such as sentiment analysis (SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment classification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading comprehension (NarrativeQA). Across all 26 datasets, our results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture."
    },
    "keywords": [
        {
            "term": "Mean Reciprocal Rank",
            "url": "https://en.wikipedia.org/wiki/Mean_Reciprocal_Rank"
        },
        {
            "term": "long short term memory",
            "url": "https://en.wikipedia.org/wiki/long_short_term_memory"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_neural_networks"
        }
    ],
    "highlights": [
        "Recurrent neural networks (RNNs) live at the heart of many sequence modeling problems",
        "We note that it is possible to have a simplified variant1 of Recurrent Networks that uses Gated Recurrent Units as the atomic block which we found to have performed slightly better on certain datasets",
        "Recurrent Networks outperforms a wide range of competitive baselines such as DiSAN, Bi-simple recurrent units, Bi-Attentive classification network and Long Short-Term Memory-CNN, etc",
        "We proposed Recurrently Controlled Recurrent Networks (RCRN), a new recurrent architecture and encoder for a myriad of NLP tasks",
        "Recurrent Networks operates in a novel controller-listener architecture which uses Recurrent neural networks to learn the gating functions of another Recurrent neural networks",
        "Recurrent Networks remains efficient compared to stacked Recurrent neural networks of approximately equal parameterization"
    ],
    "key_statements": [
        "Recurrent neural networks (RNNs) live at the heart of many sequence modeling problems",
        "This paper proposes Recurrently Controlled Recurrent Networks (RCRN), a new recurrent architecture and a general purpose neural building block for sequence modeling",
        "We note that it is possible to have a simplified variant1 of Recurrent Networks that uses Gated Recurrent Units as the atomic block which we found to have performed slightly better on certain datasets",
        "Sentiment Analysis On the 16 review datasets (Table 1) from [<a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>; <a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a></a>], our proposed Recurrent Networks architecture achieves the highest score on all 16 datasets, outperforming the existing state-of-the-art model - sentence state Long Short-Term Memory (SLSTM) [<a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a></a>]",
        "Recurrent Networks outperforms a wide range of competitive baselines such as DiSAN, Bi-simple recurrent units, Bi-Attentive classification network and Long Short-Term Memory-CNN, etc",
        "Recurrent Networks provides comparable efficiency to using stacked BiLSTM and empirically we show that there is nothing to lose in this aspect",
        "We proposed Recurrently Controlled Recurrent Networks (RCRN), a new recurrent architecture and encoder for a myriad of NLP tasks",
        "Recurrent Networks operates in a novel controller-listener architecture which uses Recurrent neural networks to learn the gating functions of another Recurrent neural networks",
        "Overall findings suggest that our controller-listener architecture is more effective than stacking Recurrent neural networks layers",
        "Recurrent Networks remains efficient compared to stacked Recurrent neural networks of approximately equal parameterization",
        "The source code of our model can be found at https://github.com/ vanzytay/NIPS2018_RCRN"
    ],
    "summary": [
        "Recurrent neural networks (RNNs) live at the heart of many sequence modeling problems.",
        "The BCN model [McCann et al, 2017] uses multiple BiLSTM layers within their architecture.",
        "Our proposed RCRN consistently outperforms stacked BiLSTMs and achieves state-of-the-art results on several datasets.",
        "We outperform above-mentioned competitors such as DiSAN, SRUs, stacked BiLSTMs and sentence-state LSTMs. 3 Recurrently Controlled Recurrent Networks (RCRN)",
        "The listener cell uses a base recurrent model to process the sequence input.",
        "The overall architecture of the RCRN model can be explained as follows: Firstly, the controller cell can be thought of as two BiRNN models which hidden states are used as the forget and output gates for another recurrent model, i.e., the listener.",
        "We use the model architecture from Attentive Pooling BiLSTMs (AP-BiLSTM) [dos Santos et al, 2016] as our base and swap the RNN encoder with our RCRN encoder.",
        "We include an additional ablative baselines, swapping the RCRN with (1) a standard BiLSTM model and (2) a stacked BiLSTM of 3 layers (3L-BiLSTM).",
        "Sentiment Analysis On the 16 review datasets (Table 1) from [<a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>; <a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a></a>], our proposed RCRN architecture achieves the highest score on all 16 datasets, outperforming the existing state-of-the-art model - sentence state LSTMs (SLSTM) [<a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a></a>].",
        "RCRN outperforms many strong baselines such as DiSAN [<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\"><a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a></a>], a self-attentive model and Bi-Attentive classification network (BCN) [McCann et al, 2017] that use CoVe vectors.",
        "Our proposed RCRN outperforms Residual BiLSTMs [<a class=\"ref-link\" id=\"cLongpre_et+al_2016_a\" href=\"#rLongpre_et+al_2016_a\">Longpre et al, 2016</a>], 4-layered Quasi Recurrent Neural Networks (QRNN) [<a class=\"ref-link\" id=\"cBradbury_et+al_2016_a\" href=\"#rBradbury_et+al_2016_a\">Bradbury et al, 2016</a>] and the BCN model which can be considered to be very competitive baselines.",
        "RCRN outperforms a wide range of baselines, including self-attention based models as multi-head [<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a>] and DiSAN [<a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\"><a class=\"ref-link\" id=\"cShen_et+al_2017_a\" href=\"#rShen_et+al_2017_a\">Shen et al, 2017</a></a>].",
        "RCRN outperforms shortcut stacked encoders, which use a series of BiLSTM connected by shortcut layers.",
        "RCRN outperforms several baselines in [<a class=\"ref-link\" id=\"cKhot_et+al_2018_a\" href=\"#rKhot_et+al_2018_a\">Khot et al, 2018</a>] including models that use cross sentence attention such as DecompAtt [Parikh et al, 2016] and ESIM [<a class=\"ref-link\" id=\"cChen_et+al_2017_a\" href=\"#rChen_et+al_2017_a\">Chen et al, 2017</a>].",
        "RCRN outperforms a wide range of competitive baselines such as DiSAN, Bi-SRUs, BCN and LSTM-CNN, etc.",
        "We proposed Recurrently Controlled Recurrent Networks (RCRN), a new recurrent architecture and encoder for a myriad of NLP tasks.",
        "RCRN operates in a novel controller-listener architecture which uses RNNs to learn the gating functions of another RNN.",
        "Overall findings suggest that our controller-listener architecture is more effective than stacking RNN layers.",
        "The source code of our model can be found at https://github.com/ vanzytay/NIPS2018_RCRN"
    ],
    "headline": "This paper proposes a recurrently controlled recurrent network  for expressive and powerful sequence encoding",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2014_a",
            "entry": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "Bowman_et+al_2015_a",
            "entry": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 632\u2013642, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowman%2C%20Samuel%20R.%20Angeli%2C%20Gabor%20Potts%2C%20Christopher%20Manning%2C%20Christopher%20D.%20A%20large%20annotated%20corpus%20for%20learning%20natural%20language%20inference%202015-09-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowman%2C%20Samuel%20R.%20Angeli%2C%20Gabor%20Potts%2C%20Christopher%20Manning%2C%20Christopher%20D.%20A%20large%20annotated%20corpus%20for%20learning%20natural%20language%20inference%202015-09-17"
        },
        {
            "id": "Bradbury_et+al_2016_a",
            "entry": "James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. CoRR, abs/1611.01576, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01576"
        },
        {
            "id": "Chang_et+al_2017_a",
            "entry": "Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. In Advances in Neural Information Processing Systems, pages 76\u201386, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Shiyu%20Zhang%2C%20Yang%20Han%2C%20Wei%20Yu%2C%20Mo%20Dilated%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Shiyu%20Zhang%2C%20Yang%20Han%2C%20Wei%20Yu%2C%20Mo%20Dilated%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1657\u20131668, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Qian%20Zhu%2C%20Xiaodan%20Ling%2C%20Zhen-Hua%20Wei%2C%20Si%20Enhanced%20LSTM%20for%20natural%20language%20inference%202017-07-30",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Qian%20Zhu%2C%20Xiaodan%20Ling%2C%20Zhen-Hua%20Wei%2C%20Si%20Enhanced%20LSTM%20for%20natural%20language%20inference%202017-07-30"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "Choi_et+al_2017_a",
            "entry": "Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Unsupervised learning of task-specific tree structures with tree-lstms. arXiv preprint arXiv:1707.02786, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02786"
        },
        {
            "id": "Chung_et+al_2016_a",
            "entry": "Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. arXiv preprint arXiv:1609.01704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.01704"
        },
        {
            "id": "Danihelka_et+al_2016_a",
            "entry": "Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves. Associative long short-term memory. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1986\u20131994, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Danihelka%2C%20Ivo%20Wayne%2C%20Greg%20Uria%2C%20Benigno%20Kalchbrenner%2C%20Nal%20Associative%20long%20short-term%20memory%202016-06-19",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Danihelka%2C%20Ivo%20Wayne%2C%20Greg%20Uria%2C%20Benigno%20Kalchbrenner%2C%20Nal%20Associative%20long%20short-term%20memory%202016-06-19"
        },
        {
            "id": "Dieng_et+al_2016_a",
            "entry": "Adji B Dieng, Chong Wang, Jianfeng Gao, and John Paisley. Topicrnn: A recurrent neural network with long-range semantic dependency. arXiv preprint arXiv:1611.01702, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01702"
        },
        {
            "id": "Ding_et+al_2018_a",
            "entry": "Zixiang Ding, Rui Xia, Jianfei Yu, Xiang Li, and Jian Yang. Densely connected bidirectional lstm with applications to sentence classification. arXiv preprint arXiv:1802.00889, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00889"
        },
        {
            "id": "Dos_et+al_2016_a",
            "entry": "C\u00edcero Nogueira dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. CoRR, abs/1602.03609, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.03609"
        },
        {
            "id": "Hihi_1996_a",
            "entry": "Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In Advances in neural information processing systems, pages 493\u2013499, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hihi%2C%20Salah%20El%20Bengio%2C%20Yoshua%20Hierarchical%20recurrent%20neural%20networks%20for%20long-term%20dependencies%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hihi%2C%20Salah%20El%20Bengio%2C%20Yoshua%20Hierarchical%20recurrent%20neural%20networks%20for%20long-term%20dependencies%201996"
        },
        {
            "id": "Graves_et+al_2013_a",
            "entry": "Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages 6645\u20136649. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "Hongyu Guo, Colin Cherry, and Jiang Su. End-to-end multi-view networks for text classification. arXiv preprint arXiv:1704.05907, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05907"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Huang_et+al_2017_a",
            "entry": "Minlie Huang, Qiao Qian, and Xiaoyan Zhu. Encoding syntactic knowledge in neural networks for sentiment classification. ACM Transactions on Information Systems (TOIS), 35(3):26, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Minlie%20Qian%2C%20Qiao%20Zhu%2C%20Xiaoyan%20Encoding%20syntactic%20knowledge%20in%20neural%20networks%20for%20sentiment%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Minlie%20Qian%2C%20Qiao%20Zhu%2C%20Xiaoyan%20Encoding%20syntactic%20knowledge%20in%20neural%20networks%20for%20sentiment%20classification%202017"
        },
        {
            "id": "Johnson_2016_a",
            "entry": "Rie Johnson and Tong Zhang. Supervised and semi-supervised text categorization using lstm for region embeddings. arXiv preprint arXiv:1602.02373, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02373"
        },
        {
            "id": "Khot_et+al_2018_a",
            "entry": "Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science question answering. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khot%2C%20Tushar%20Sabharwal%2C%20Ashish%20Clark%2C%20Peter%20Scitail%3A%20A%20textual%20entailment%20dataset%20from%20science%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khot%2C%20Tushar%20Sabharwal%2C%20Ashish%20Clark%2C%20Peter%20Scitail%3A%20A%20textual%20entailment%20dataset%20from%20science%20question%20answering%202018"
        },
        {
            "id": "Kiela_et+al_2018_a",
            "entry": "Douwe Kiela, Changhan Wang, and Kyunghyun Cho. Context-attentive embeddings for improved sentence representations. arXiv preprint arXiv:1804.07983, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07983"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, and Nojun Kwak. Semantic sentence matching with densely-connected recurrent and co-attentive information. arXiv preprint arXiv:1805.11360, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11360"
        },
        {
            "id": "Kim_2014_a",
            "entry": "Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1408.5882"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kocisky_et+al_2017_a",
            "entry": "Tom\u00e1\u0161 Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. arXiv preprint arXiv:1712.07040, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07040"
        },
        {
            "id": "Koutnik_et+al_2014_a",
            "entry": "Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. arXiv preprint arXiv:1402.3511, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.3511"
        },
        {
            "id": "Kumar_et+al_2016_a",
            "entry": "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pages 1378\u20131387, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20Ankit%20Irsoy%2C%20Ozan%20Ondruska%2C%20Peter%20Iyyer%2C%20Mohit%20Ask%20me%20anything%3A%20Dynamic%20memory%20networks%20for%20natural%20language%20processing%202016"
        },
        {
            "id": "Lei_2017_a",
            "entry": "Tao Lei and Yu Zhang. Training rnns as fast as cnns. arXiv preprint arXiv:1709.02755, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.02755"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classification. arXiv preprint arXiv:1704.05742, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05742"
        },
        {
            "id": "Longpre_et+al_2016_a",
            "entry": "Shayne Longpre, Sabeek Pradhan, Caiming Xiong, and Richard Socher. A way out of the odyssey: Analyzing and combining recent insights for lstms. arXiv preprint arXiv:1611.05104, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05104"
        },
        {
            "id": "Looks_et+al_2017_a",
            "entry": "Moshe Looks, Marcello Herreshoff, DeLesley Hutchins, and Peter Norvig. Deep learning with dynamic computation graphs. arXiv preprint arXiv:1702.02181, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02181"
        },
        {
            "id": "Maas_et+al_2011_a",
            "entry": "Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142\u2013150. Association for Computational Linguistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6297\u20136308, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCann%2C%20Bryan%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Learned%20in%20translation%3A%20Contextualized%20word%20vectors%202017"
        },
        {
            "id": "Miyato_et+al_2016_a",
            "entry": "Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. arXiv preprint arXiv:1605.07725, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07725"
        },
        {
            "id": "Munkhdalai_2016_a",
            "entry": "Tsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. corr abs/1607.04315, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04315"
        },
        {
            "id": "Nie_2017_a",
            "entry": "Yixin Nie and Mohit Bansal. Shortcut-stacked sentence encoders for multi-domain inference. In Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP, RepEval@EMNLP 2017, Copenhagen, Denmark, September 8, 2017, pages 41\u201345, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nie%2C%20Yixin%20Bansal%2C%20Mohit%20Shortcut-stacked%20sentence%20encoders%20for%20multi-domain%20inference%202017-09-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nie%2C%20Yixin%20Bansal%2C%20Mohit%20Shortcut-stacked%20sentence%20encoders%20for%20multi-domain%20inference%202017-09-08"
        },
        {
            "id": "Ankur_2016_a",
            "entry": "Ankur P. Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2249\u20132255, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ankur%20P.%20Parikh%2C%20Oscar%20T%C3%A4ckstr%C3%B6m%2C%20Dipanjan%20Das%20Uszkoreit%2C%20Jakob%20A%20decomposable%20attention%20model%20for%20natural%20language%20inference%202016-11-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ankur%20P.%20Parikh%2C%20Oscar%20T%C3%A4ckstr%C3%B6m%2C%20Dipanjan%20Das%20Uszkoreit%2C%20Jakob%20A%20decomposable%20attention%20model%20for%20natural%20language%20inference%202016-11-01"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10-25",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10-25"
        },
        {
            "id": "Peters_et+al_2018_a",
            "entry": "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05365"
        },
        {
            "id": "Radford_et+al_2017_a",
            "entry": "Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01444"
        },
        {
            "id": "Radford_et+al_2018_a",
            "entry": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Narasimhan%2C%20Karthik%20Salimans%2C%20Tim%20Sutskever%2C%20Ilya%20Improving%20language%20understanding%20by%20generative%20pre-training%202018"
        },
        {
            "id": "Rao_et+al_2016_a",
            "entry": "Jinfeng Rao, Hua He, and Jimmy J. Lin. Noise-contrastive estimation for answer selection with deep neural networks. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016, pages 1913\u20131916, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rao%2C%20Jinfeng%20He%2C%20Hua%20Lin%2C%20Jimmy%20J.%20Noise-contrastive%20estimation%20for%20answer%20selection%20with%20deep%20neural%20networks%202016-10-24",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rao%2C%20Jinfeng%20He%2C%20Hua%20Lin%2C%20Jimmy%20J.%20Noise-contrastive%20estimation%20for%20answer%20selection%20with%20deep%20neural%20networks%202016-10-24"
        },
        {
            "id": "Tim_2015_a",
            "entry": "Tim Rockt\u00e4schel, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Kocisky, and Phil Blunsom. Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.06664"
        },
        {
            "id": "Seo_et+al_2016_a",
            "entry": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01603"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. Disan: Directional self-attention network for rnn/cnn-free language understanding. arXiv preprint arXiv:1709.04696, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04696"
        },
        {
            "id": "Shen_et+al_2018_a",
            "entry": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Bi-directional block selfattention for fast and memory-efficient sequence modeling. arXiv preprint arXiv:1804.00857, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00857"
        },
        {
            "id": "Socher_et+al_2013_a",
            "entry": "Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. Citeseer, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Y.%20Chuang%2C%20Jason%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Y.%20Chuang%2C%20Jason%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013"
        },
        {
            "id": "Rupesh_2015_a",
            "entry": "Rupesh Kumar Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Highway networks. CoRR, abs/1505.00387, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.00387"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "Tai_et+al_2015_a",
            "entry": "Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.00075"
        },
        {
            "id": "Tay_et+al_2017_a",
            "entry": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. A compare-propagate architecture with alignment factorization for natural language inference. arXiv preprint arXiv:1801.00102, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00102"
        },
        {
            "id": "Tay_et+al_0000_a",
            "entry": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Co-stack residual affinity networks with multi-level attention refinement for matching text sequences. arXiv preprint arXiv:1810.02938, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1810.02938"
        },
        {
            "id": "Tay_et+al_0000_b",
            "entry": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Multi-range reasoning for machine comprehension. arXiv preprint arXiv:1803.09074, 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09074"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "Voorhees_1999_a",
            "entry": "Ellen M Voorhees et al. The trec-8 question answering track report. In Trec, volume 99, pages 77\u201382, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Voorhees%2C%20Ellen%20M.%20The%20trec-8%20question%20answering%20track%20report%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Voorhees%2C%20Ellen%20M.%20The%20trec-8%20question%20answering%20track%20report%201999"
        },
        {
            "id": "Wang_et+al_2007_a",
            "entry": "Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. What is the jeopardy model? A quasisynchronous grammar for QA. In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic, pages 22\u201332, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mengqiu%20Wang%20Noah%20A%20Smith%20and%20Teruko%20Mitamura%20What%20is%20the%20jeopardy%20model%20A%20quasisynchronous%20grammar%20for%20QA%20In%20EMNLPCoNLL%202007%20Proceedings%20of%20the%202007%20Joint%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20and%20Computational%20Natural%20Language%20Learning%20June%202830%202007%20Prague%20Czech%20Republic%20pages%202232%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mengqiu%20Wang%20Noah%20A%20Smith%20and%20Teruko%20Mitamura%20What%20is%20the%20jeopardy%20model%20A%20quasisynchronous%20grammar%20for%20QA%20In%20EMNLPCoNLL%202007%20Proceedings%20of%20the%202007%20Joint%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20and%20Computational%20Natural%20Language%20Learning%20June%202830%202007%20Prague%20Czech%20Republic%20pages%202232%202007"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks for reading comprehension and question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 189\u2013198, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Wenhui%20Yang%2C%20Nan%20Wei%2C%20Furu%20Chang%2C%20Baobao%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering%202017"
        },
        {
            "id": "Wieting_et+al_2015_a",
            "entry": "John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic sentence embeddings. arXiv preprint arXiv:1511.08198, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.08198"
        },
        {
            "id": "Xingjian_et+al_2015_a",
            "entry": "SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems, pages 802\u2013810, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xingjian%2C%20S.H.I.%20Chen%2C%20Zhourong%20Wang%2C%20Hao%20Yeung%2C%20Dit-Yan%20Convolutional%20lstm%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xingjian%2C%20S.H.I.%20Chen%2C%20Zhourong%20Wang%2C%20Hao%20Yeung%2C%20Dit-Yan%20Convolutional%20lstm%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015"
        },
        {
            "id": "Xiong_et+al_2016_a",
            "entry": "Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answering. CoRR, abs/1611.01604, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01604"
        },
        {
            "id": "Yang_et+al_2015_a",
            "entry": "Yi Yang, Wen-tau Yih, and Christopher Meek. Wikiqa: A challenge dataset for open-domain question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 2013\u20132018, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yi%20Yih%2C%20Wen-tau%20Meek%2C%20Christopher%20Wikiqa%3A%20A%20challenge%20dataset%20for%20open-domain%20question%20answering%202015-09-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yi%20Yih%2C%20Wen-tau%20Meek%2C%20Christopher%20Wikiqa%3A%20A%20challenge%20dataset%20for%20open-domain%20question%20answering%202015-09-17"
        },
        {
            "id": "Yu_2017_a",
            "entry": "Hong Yu and Tsendsuren Munkhdalai. Neural tree indexers for text understanding. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 1: Long Papers, pages 11\u201321, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Hong%20Munkhdalai%2C%20Tsendsuren%20Neural%20tree%20indexers%20for%20text%20understanding%202017-04-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Hong%20Munkhdalai%2C%20Tsendsuren%20Neural%20tree%20indexers%20for%20text%20understanding%202017-04-03"
        },
        {
            "id": "Zeiler_2012_a",
            "entry": "Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        },
        {
            "id": "Zhang_et+al_0000_a",
            "entry": "Rui Zhang, Honglak Lee, and Dragomir Radev. Dependency sensitive convolutional neural networks for modeling sentences and documents. arXiv preprint arXiv:1611.02361, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02361"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco, Sanjeev Khudanpur, and James Glass. Highway long short-term memory rnns for distant speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 5755\u20135759. IEEE, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yu%20Chen%2C%20Guoguo%20Yu%2C%20Dong%20Yaco%2C%20Kaisheng%20Highway%20long%20short-term%20memory%20rnns%20for%20distant%20speech%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yu%20Chen%2C%20Guoguo%20Yu%2C%20Dong%20Yaco%2C%20Kaisheng%20Highway%20long%20short-term%20memory%20rnns%20for%20distant%20speech%20recognition%202016"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Yue Zhang, Qi Liu, and Linfeng Song. Sentence-state lstm for text representation. arXiv preprint arXiv:1805.02474, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.02474"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. Text classification improved by integrating bidirectional lstm with two-dimensional max pooling. arXiv preprint arXiv:1611.06639, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06639"
        }
    ]
}
