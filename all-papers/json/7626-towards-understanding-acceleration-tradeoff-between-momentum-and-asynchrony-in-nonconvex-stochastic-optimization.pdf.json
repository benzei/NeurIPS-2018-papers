{
    "filename": "7626-towards-understanding-acceleration-tradeoff-between-momentum-and-asynchrony-in-nonconvex-stochastic-optimization.pdf",
    "metadata": {
        "title": "Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization",
        "author": "Tianyi Liu, Shiyang Li, Jianping Shi, Enlu Zhou, Tuo Zhao",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7626-towards-understanding-acceleration-tradeoff-between-momentum-and-asynchrony-in-nonconvex-stochastic-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) have been widely used in distributed machine learning, e.g., training large collaborative filtering systems and deep neural networks. Due to current technical limit, however, establishing convergence properties of Async-MSGD for these highly complicated nonoconvex problems is generally infeasible. Therefore, we propose to analyze the algorithm through a simpler but nontrivial nonconvex problems \u2014 streaming PCA. This allows us to make progress toward understanding Aync-MSGD and gaining new insights for more general problems. Specifically, by exploiting the diffusion approximation of stochastic optimization, we establish the asymptotic rate of convergence of Async-MSGD for streaming PCA. Our results indicate a fundamental tradeoff between asynchrony and momentum: To ensure convergence and acceleration through asynchrony, we have to reduce the momentum (compared with Sync-MSGD). To the best of our knowledge, this is the first theoretical attempt on understanding Async-MSGD for distributed nonconvex stochastic optimization. Numerical experiments on both streaming PCA and training deep neural networks are provided to support our findings for Async-MSGD."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "collaborative filtering",
            "url": "https://en.wikipedia.org/wiki/collaborative_filtering"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "image recognition",
            "url": "https://en.wikipedia.org/wiki/image_recognition"
        },
        {
            "term": "diffusion approximation",
            "url": "https://en.wikipedia.org/wiki/diffusion_approximation"
        }
    ],
    "highlights": [
        "Modern machine learning models trained on large data sets have revolutionized a wide variety of domains, from speech and image recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) to natural language processing (<a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\"><a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\">Rumelhart et al, 1986</a></a>) to industry-focused applications such as recommendation systems (<a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\"><a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\">Salakhutdinov et al, 2007</a></a>)",
        "Modern machine learning models trained on large data sets have revolutionized a wide variety of domains, from speech and image recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a>) to natural language processing (<a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\">Rumelhart et al, 1986</a>) to industry-focused applications such as recommendation systems (<a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\">Salakhutdinov et al, 2007</a>)",
        "I=1 whereis a loss function, and f is a neural network function/operator associated with parameter \u2713",
        "Thanks to significant advances made in GPU hardware and training algorithms, we can train machine learning models on a GPU-equipped machine",
        "For Phase II, we study the algorithmic behavior once Async-MSGD has escaped from saddle points",
        "We present numerical experiments for both streaming PCA and training deep neural networks to demonstrate the tradeoff between the momentum and asynchrony"
    ],
    "key_statements": [
        "Modern machine learning models trained on large data sets have revolutionized a wide variety of domains, from speech and image recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) to natural language processing (<a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\"><a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\">Rumelhart et al, 1986</a></a>) to industry-focused applications such as recommendation systems (<a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\"><a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\">Salakhutdinov et al, 2007</a></a>)",
        "Modern machine learning models trained on large data sets have revolutionized a wide variety of domains, from speech and image recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a>) to natural language processing (<a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\">Rumelhart et al, 1986</a>) to industry-focused applications such as recommendation systems (<a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\">Salakhutdinov et al, 2007</a>)",
        "I=1 whereis a loss function, and f is a neural network function/operator associated with parameter \u2713",
        "Thanks to significant advances made in GPU hardware and training algorithms, we can train machine learning models on a GPU-equipped machine",
        "Recall that we study Async-MSGD for the streaming PCA problem formulated as (4)",
        "At the k-th iteration, given Xk 2 Rd independently sampled from the underlying zero-mean distribution D, Async-MSGD takes vk+1 = vk + \u03bc + \u2318(I vk \u2327k vk> \u2327k )XkXk>vk \u2327k , (5)",
        "For Phase II, we study the algorithmic behavior once Async-MSGD has escaped from saddle points",
        "Proposition 8 implies that asymptotically, the effective iteration complexity of Async-MSGD enjoys a linear acceleration by a factor \u2327 , i.e., N2\n2(1 \u03bc)( 1",
        "We present numerical experiments for both streaming PCA and training deep neural networks to demonstrate the tradeoff between the momentum and asynchrony",
        "Figure 1 shows the average optimization error obtained by Async-MSGD with \u03bc = 0.7, 0.8, 0.85, 0.9, 0.95 and delays from 0 to 100",
        "We provide numerical experiments for comparing different number workers and choices of momentum in training a 32-layer hyperspherical residual neural network (SphereResNet34) using the CIFAR-100 dataset for a 100-class image classification task"
    ],
    "summary": [
        "Modern machine learning models trained on large data sets have revolutionized a wide variety of domains, from speech and image recognition (<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>; <a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>) to natural language processing (<a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\"><a class=\"ref-link\" id=\"cRumelhart_et+al_1986_a\" href=\"#rRumelhart_et+al_1986_a\">Rumelhart et al, 1986</a></a>) to industry-focused applications such as recommendation systems (<a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\"><a class=\"ref-link\" id=\"cSalakhutdinov_et+al_2007_a\" href=\"#rSalakhutdinov_et+al_2007_a\">Salakhutdinov et al, 2007</a></a>).",
        "Illustrating through the example of streaming PCA, we intend to answer the fundamental question, which arises in <a class=\"ref-link\" id=\"cMitliagkas_et+al_2016_a\" href=\"#rMitliagkas_et+al_2016_a\">Mitliagkas et al (2016</a>): Does there exist a tradeoff between asynchrony and momentum in distributed nonconvex stochastic optimization?",
        "As shown in <a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\">Liu et al (2018</a></a>), the momentum accelerates optimization, when escaping from saddle points, or in nonstationary regions, but cannot improve the convergence to optima.",
        "We prove that the solution trajectory of Async-MSGD for streaming PCA converges weakly to the solution of an appropriately constructed ODE/SDE.",
        "This solution can provide intuitive characterization of the algorithmic behavior, and establish the asymptotic rate of convergence of Async-MSGD.",
        "At the k-th iteration, given Xk 2 Rd independently sampled from the underlying zero-mean distribution D, Async-MSGD takes vk+1 = vk + \u03bc + \u2318(I vk \u2327k vk> \u2327k )XkXk>vk \u2327k , (5)",
        "We first prove the global convergence of Async-MSGD using an ODE approximation.",
        "1. This implies that the limiting solution trajectory of Async-MSGD converges to the global optima, given the delay \u2327k .",
        "For Phase II, we study the algorithmic behavior once Async-MSGD has escaped from saddle points.",
        "Proposition 8 implies that asymptotically, the effective iteration complexity of Async-MSGD enjoys a linear acceleration by a factor \u2327 , i.e., N2",
        "We present numerical experiments for both streaming PCA and training deep neural networks to demonstrate the tradeoff between the momentum and asynchrony.",
        "We compare the performance of Async-MSGD with different delays and momentum parameters.",
        "Figure 1 shows the average optimization error obtained by Async-MSGD with \u03bc = 0.7, 0.8, 0.85, 0.9, 0.95 and delays from 0 to 100.",
        "We remark that the difference among Async-MSGD with different \u03bc when \u2327 = 0 is due to the fact that the momentum hurts convergence, as shown in <a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\"><a class=\"ref-link\" id=\"cLiu_et+al_2018_a\" href=\"#rLiu_et+al_2018_a\">Liu et al (2018</a></a>).",
        "We provide numerical experiments for comparing different number workers and choices of momentum in training a 32-layer hyperspherical residual neural network (SphereResNet34) using the CIFAR-100 dataset for a 100-class image classification task.",
        "We remark that though our theory helps explain some phenomena in training DNNs, there still exist some gaps: (1) The optimization landscapes of DNNs are much more challenging than that of our studied streaming PCA problem.",
        "Though we can expect large and small step sizes share some similar behaviors, they may lead to very different results; (3) Our analysis only explains how Async-MSGD minimizes the population objective."
    ],
    "headline": "We propose to analyze the algorithm through a simpler but nontrivial nonconvex problems \u2014 streaming PCA",
    "reference_links": [
        {
            "id": "Chen_et+al_2016_a",
            "entry": "CHEN, J., PAN, X., MONGA, R., BENGIO, S. and JOZEFOWICZ, R. (2016). Revisiting distributed synchronous sgd. arXiv preprint arXiv:1604.00981 .",
            "arxiv_url": "https://arxiv.org/pdf/1604.00981"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "CHEN, Z., YANG, F. L., LI, C. J. and ZHAO, T. (2017). Online multiview representation learning: Dropping convexity for better efficiency. arXiv preprint arXiv:1702.08134 .",
            "arxiv_url": "https://arxiv.org/pdf/1702.08134"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "GE, R., HUANG, F., JIN, C. and YUAN, Y. (2015). Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Conference on Learning Theory.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GE%2C%20R.%20HUANG%2C%20F.%20JIN%2C%20C.%20YUAN%2C%20Y.%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition.%20In%20Conference%20on%20Learning%20Theory%202015"
        },
        {
            "id": "Ge_et+al_2016_a",
            "entry": "GE, R., LEE, J. D. and MA, T. (2016). Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GE%2C%20R.%20LEE%2C%20J.D.%20MA%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GE%2C%20R.%20LEE%2C%20J.D.%20MA%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "HE, K., ZHANG, X., REN, S. and SUN, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HE%2C%20K.%20ZHANG%2C%20X.%20REN%2C%20S.%20SUN%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=HE%2C%20K.%20ZHANG%2C%20X.%20REN%2C%20S.%20SUN%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hinton_et+al_2012_a",
            "entry": "HINTON, G., DENG, L., YU, D., DAHL, G. E., MOHAMED, A.-R., JAITLY, N., SENIOR, A., VANHOUCKE, V., NGUYEN, P., SAINATH, T. N. ET AL. (2012). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine 29 82\u201397.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HINTON%2C%20G.%20DENG%2C%20L.%20YU%2C%20D.%20DAHL%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=HINTON%2C%20G.%20DENG%2C%20L.%20YU%2C%20D.%20DAHL%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "KINGMA, D. P. and BA, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "KRIZHEVSKY, A., SUTSKEVER, I. and HINTON, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KRIZHEVSKY%2C%20A.%20SUTSKEVER%2C%20I.%20HINTON%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks.%20In%20Advances%20in%20neural%20information%20processing%20systems%202012"
        },
        {
            "id": "Kushner_2003_a",
            "entry": "KUSHNER, H. J. and YIN, G. G. (2003). Stochastic approximation and recursive algorithms and applications, stochastic modelling and applied probability, vol. 35.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KUSHNER%2C%20H.J.%20YIN%2C%20G.G.%20Stochastic%20approximation%20and%20recursive%20algorithms%20and%20applications%2C%20stochastic%20modelling%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=KUSHNER%2C%20H.J.%20YIN%2C%20G.G.%20Stochastic%20approximation%20and%20recursive%20algorithms%20and%20applications%2C%20stochastic%20modelling%202003"
        },
        {
            "id": "Li_et+al_2014_a",
            "entry": "LI, M., ANDERSEN, D. G., PARK, J. W., SMOLA, A. J., AHMED, A., JOSIFOVSKI, V., LONG, J., SHEKITA, E. J. and SU, B.-Y. (2014). Scaling distributed machine learning with the parameter server. In OSDI, vol. 14.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LI%2C%20M.%20ANDERSEN%2C%20D.G.%20PARK%2C%20J.W.%20SMOLA%2C%20A.J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LI%2C%20M.%20ANDERSEN%2C%20D.G.%20PARK%2C%20J.W.%20SMOLA%2C%20A.J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "LI, X., WANG, Z., LU, J., ARORA, R., HAUPT, J., LIU, H. and ZHAO, T. (2016). Symmetry, saddle points, and global geometry of nonconvex matrix factorization. arXiv preprint arXiv:1612.09296 .",
            "arxiv_url": "https://arxiv.org/pdf/1612.09296"
        },
        {
            "id": "Lian_et+al_2015_a",
            "entry": "LIAN, X., HUANG, Y., LI, Y. and LIU, J. (2015). Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LIAN%2C%20X.%20HUANG%2C%20Y.%20LI%2C%20Y.%20LIU%2C%20J.%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LIAN%2C%20X.%20HUANG%2C%20Y.%20LI%2C%20Y.%20LIU%2C%20J.%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015"
        },
        {
            "id": "Lian_et+al_2016_a",
            "entry": "LIAN, X., ZHANG, H., HSIEH, C.-J., HUANG, Y. and LIU, J. (2016). A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LIAN%2C%20X.%20ZHANG%2C%20H.%20HSIEH%2C%20C.-J.%20HUANG%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LIAN%2C%20X.%20ZHANG%2C%20H.%20HSIEH%2C%20C.-J.%20HUANG%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%202016"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "LIU, T., CHEN, Z., ZHOU, E. and ZHAO, T. (2018). Toward deeper understanding of nonconvex stochastic optimization with momentum using diffusion approximations. arXiv preprint arXiv:1802.05155 .",
            "arxiv_url": "https://arxiv.org/pdf/1802.05155"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "LIU, W., ZHANG, Y.-M., LI, X., YU, Z., DAI, B., ZHAO, T. and SONG, L. (2017). Deep hyperspherical learning. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LIU%2C%20W.%20ZHANG%2C%20Y.-M.%20LI%2C%20X.%20YU%2C%20Z.%20Deep%20hyperspherical%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LIU%2C%20W.%20ZHANG%2C%20Y.-M.%20LI%2C%20X.%20YU%2C%20Z.%20Deep%20hyperspherical%20learning%202017"
        },
        {
            "id": "Mitliagkas_et+al_2016_a",
            "entry": "MITLIAGKAS, I., ZHANG, C., HADJIS, S. and R\u00c9, C. (2016). Asynchrony begets momentum, with an application to deep learning. In Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MITLIAGKAS%20I%20ZHANG%20C%20HADJIS%20S%20and%20R%C3%89%20C%202016%20Asynchrony%20begets%20momentum%20with%20an%20application%20to%20deep%20learning%20In%20Communication%20Control%20and%20Computing%20Allerton%202016%2054th%20Annual%20Allerton%20Conference%20on%20IEEE",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MITLIAGKAS%20I%20ZHANG%20C%20HADJIS%20S%20and%20R%C3%89%20C%202016%20Asynchrony%20begets%20momentum%20with%20an%20application%20to%20deep%20learning%20In%20Communication%20Control%20and%20Computing%20Allerton%202016%2054th%20Annual%20Allerton%20Conference%20on%20IEEE"
        },
        {
            "id": "Polyak_1964_a",
            "entry": "POLYAK, B. T. (1964). Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics 4 1\u201317.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=POLYAK%2C%20B.T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=POLYAK%2C%20B.T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964"
        },
        {
            "id": "Robbins_1951_a",
            "entry": "ROBBINS, H. and MONRO, S. (1951). A stochastic approximation method. The annals of mathematical statistics 400\u2013407.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ROBBINS%2C%20H.%20MONRO%2C%20S.%20A%20stochastic%20approximation%20method.%20The%20annals%20of%20mathematical%20statistics%20400%E2%80%93407%201951"
        },
        {
            "id": "Rumelhart_et+al_1986_a",
            "entry": "RUMELHART, D. E., HINTON, G. E. and WILLIAMS, R. J. (1986). Learning representations by back-propagating errors. nature 323 533.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RUMELHART%2C%20D.E.%20HINTON%2C%20G.E.%20WILLIAMS%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RUMELHART%2C%20D.E.%20HINTON%2C%20G.E.%20WILLIAMS%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        },
        {
            "id": "Salakhutdinov_et+al_2007_a",
            "entry": "SALAKHUTDINOV, R., MNIH, A. and HINTON, G. (2007). Restricted boltzmann machines for collaborative filtering. In Proceedings of the 24th international conference on Machine learning. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SALAKHUTDINOV%2C%20R.%20MNIH%2C%20A.%20HINTON%2C%20G.%20Restricted%20boltzmann%20machines%20for%20collaborative%20filtering%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SALAKHUTDINOV%2C%20R.%20MNIH%2C%20A.%20HINTON%2C%20G.%20Restricted%20boltzmann%20machines%20for%20collaborative%20filtering%202007"
        },
        {
            "id": "Sanger_1989_a",
            "entry": "SANGER, T. D. (1989). Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural networks 2 459\u2013473.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SANGER%2C%20T.D.%20Optimal%20unsupervised%20learning%20in%20a%20single-layer%20linear%20feedforward%20neural%20network%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SANGER%2C%20T.D.%20Optimal%20unsupervised%20learning%20in%20a%20single-layer%20linear%20feedforward%20neural%20network%201989"
        },
        {
            "id": "Sun_et+al_2016_a",
            "entry": "SUN, J., QU, Q. and WRIGHT, J. (2016). A geometric analysis of phase retrieval. In Information Theory (ISIT), 2016 IEEE International Symposium on. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SUN%2C%20J.%20QU%2C%20Q.%20WRIGHT%2C%20J.%20A%20geometric%20analysis%20of%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SUN%2C%20J.%20QU%2C%20Q.%20WRIGHT%2C%20J.%20A%20geometric%20analysis%20of%20phase%20retrieval%202016"
        },
        {
            "id": "Zhang_2018_a",
            "entry": "ZHANG, J. and MITLIAGKAS, I. (2018). Yellowfin: Adaptive optimization for (a) synchronous systems. Training 1 2\u20130.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ZHANG%2C%20J.%20MITLIAGKAS%2C%20I.%20Yellowfin%3A%20Adaptive%20optimization%20for%20%28a%29%20synchronous%20systems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ZHANG%2C%20J.%20MITLIAGKAS%2C%20I.%20Yellowfin%3A%20Adaptive%20optimization%20for%20%28a%29%20synchronous%20systems%202018"
        },
        {
            "id": "Zhang_et+al_2015_a",
            "entry": "ZHANG, W., GUPTA, S., LIAN, X. and LIU, J. (2015). Staleness-aware async-sgd for distributed deep learning. arXiv preprint arXiv:1511.05950 .",
            "arxiv_url": "https://arxiv.org/pdf/1511.05950"
        }
    ]
}
