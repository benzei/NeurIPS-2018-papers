{
    "filename": "7630-zeroth-order-stochastic-variance-reduction-for-nonconvex-optimization.pdf",
    "metadata": {
        "title": "Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization",
        "author": "Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, Lisa Amini",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7630-zeroth-order-stochastic-variance-reduction-for-nonconvex-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator could suffer an additional error of order O(1/b), where b is the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing variance reduced gradient estimators, which achieve the best rate known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance between the convergence rate and the function query complexity."
    },
    "keywords": [
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Zeroth-order optimization is increasingly embraced for solving machine learning problems where explicit expressions of the gradients are difficult or infeasible to obtain",
        "Contributions We propose and evaluate a novel ZO algorithm for nonconvex stochastic optimization, ZO-stochastic variance reduced gradient, which integrates stochastic variance reduced gradient with ZO gradient estimators",
        "Since ZO-stochastic variance reduced gradient belongs to the class of ZO counterparts of first-order algorithms using random/deterministic gradient estimation, we compare it with ZO-stochastic gradient descent and ZO sto\u221achas\u221atic variance reduced coordinate, the most relevant methods to ZO-stochastic variance reduced gradient",
        "We studied ZO-stochastic variance reduced gradient, a new ZO nonconvex optimization method",
        "We show that ZO-stochastic variance reduced gradient improves the convergence rate of ZO-stochastic gradient descent from O(1/ T ) to O(1/T ) but suffers a new correction term of order O(1/b)",
        "We propose two accelerated variants of ZO-stochastic variance reduced gradient based on improved gradient estimators of reduced variances"
    ],
    "key_statements": [
        "Zeroth-order optimization is increasingly embraced for solving machine learning problems where explicit expressions of the gradients are difficult or infeasible to obtain",
        "Contributions We propose and evaluate a novel ZO algorithm for nonconvex stochastic optimization, ZO-stochastic variance reduced gradient, which integrates stochastic variance reduced gradient with ZO gradient estimators",
        "To mitigate this error term, we propose two accelerated ZO-stochastic variance reduced gradient variants, utilizing reduced variance gradient estimators",
        "We study the effect of the coordinate-wise gradient estimator (CoordGradEst) on the convergence rate of ZO-stochastic variance reduced gradient, as formalized in Theorem 3",
        "Since ZO-stochastic variance reduced gradient belongs to the class of ZO counterparts of first-order algorithms using random/deterministic gradient estimation, we compare it with ZO-stochastic gradient descent and ZO sto\u221achas\u221atic variance reduced coordinate, the most relevant methods to ZO-stochastic variance reduced gradient",
        "Fig. 2-(a) presents the convergence trajectories of ZO algorithms as functions of the number of epochs, where ZO-stochastic variance reduced gradient is evaluated under different mini-batch sizes b \u2208 {1, 10, 40}",
        "We studied ZO-stochastic variance reduced gradient, a new ZO nonconvex optimization method",
        "We show that ZO-stochastic variance reduced gradient improves the convergence rate of ZO-stochastic gradient descent from O(1/ T ) to O(1/T ) but suffers a new correction term of order O(1/b)",
        "We propose two accelerated variants of ZO-stochastic variance reduced gradient based on improved gradient estimators of reduced variances"
    ],
    "summary": [
        "Zeroth-order optimization is increasingly embraced for solving machine learning problems where explicit expressions of the gradients are difficult or infeasible to obtain.",
        "In [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], a variant of ZO-SCD, known as ZO sto\u221achas\u221atic variance reduced coordinate (ZO-SVRC) descent, improved the convergence rate from O( d/ T ) to O(d/T ) under the same parameter setting for the gradient estimation.",
        "ZO-SVRG with Avg-RandGradEst and CoordGradEst. We start by investigating the second-order moment of the blended ZO gradient estimate vks in the form of (3); see Proposition 1.",
        "Compared to the convergence rate of SVRG as given in [20, Theorem 2], Theorem 1 exhibits two additional errors (L\u03bc2/(T \u03b3)) and (S\u03c7m/(T \u03b3)) due to the use of ZO gradient estimates.",
        "Note that a large mini-batch reduces the variance of RandGradEst and improves the convergence of ZO optimization methods.",
        "We study the effect of the coordinate-wise gradient estimator (CoordGradEst) on the convergence rate of ZO-SVRG, as formalized in Theorem 3.",
        "By fixing the number of iterations T , the function query complexity of ZO-SVRG using the studied estimators is given by O, O(q) and O(d), respectively.",
        "Since ZO-SVRG belongs to the class of ZO counterparts of first-order algorithms using random/deterministic gradient estimation, we compare it with ZO-SGD and ZO-SVRC, the most relevant methods to ZO-SVRG.",
        "Fig. 2-(a) presents the convergence trajectories of ZO algorithms as functions of the number of epochs, where ZO-SVRG is evaluated under different mini-batch sizes b \u2208 {1, 10, 40}.",
        "The use of multiple random direction samples in Avg-RandGradEst significantly accelerates ZO-SVRG since the error of order O(1/b) is reduced to O(1/), leading to a non-dominant factor versus O(d/T ) in the convergence rate of ZO-SVRG-Ave. Fig.",
        "Table 2 shows the number of iterations and the testing error of algorithms studied in Fig. 2-(b) using 7.3 \u00d7 106 function queries.",
        "We observe that the performance of CoordGradEst based algorithms (i.e., ZO-SVRC and ZO-SVRG-Coord) degrade due to the need of large number of function queries to construct coordinate-wise gradient estimates.",
        "Algorithms based on random gradient estimators (i.e., ZO-SGD, ZO-SVRG and ZO-SVRG-Ave) yield better both training and testing results, while ZO-SGD consumes an extremely large number of iterations (14600 epochs).",
        "In contrast with the iteration complexity, ZO-SVRG-Ave requires roughly 30\u00d7 (q = 10), 77\u00d7 (q = 20) and 380\u00d7 (q = 30) more function evaluations than ZO-SGD to reach a neighborhood of the smallest attack loss (e.g., 7 in our example).",
        "We propose two accelerated variants of ZO-SVRG based on improved gradient estimators of reduced variances.",
        "It will be interesting to study the problem of ZO distributed optimization, e.g., using CoordGradEst under a block coordinate descent framework [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>]"
    ],
    "headline": "This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order  optimization, b) a novel variance reduced ZO algorithm, called ZO-stochastic variance reduced gradient, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, \u201cPractical black-box attacks against machine learning,\u201d in Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 2017, pp. 506\u2013519.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Goodfellow%2C%20I.%20Jha%2C%20S.%20Practical%20black-box%20attacks%20against%20machine%20learning%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Goodfellow%2C%20I.%20Jha%2C%20S.%20Practical%20black-box%20attacks%20against%20machine%20learning%2C%202017"
        },
        {
            "id": "2",
            "entry": "[2] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "3",
            "entry": "[3] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, \u201cZoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models,\u201d in Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017, pp. 15\u201326.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20P.-Y.%20Zhang%2C%20H.%20Sharma%2C%20Y.%20Yi%2C%20J.%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20P.-Y.%20Zhang%2C%20H.%20Sharma%2C%20Y.%20Yi%2C%20J.%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%2C%202017"
        },
        {
            "id": "4",
            "entry": "[4] T. Chen and G. B. Giannakis, \u201cBandit convex optimization for scalable and dynamic iot management,\u201d arXiv preprint arXiv:1707.09060, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09060"
        },
        {
            "id": "5",
            "entry": "[5] S. Liu, J. Chen, P.-Y. Chen, and A. O. Hero, \u201cZeroth-order online admm: Convergence analysis and applications,\u201d in Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, April 2018, vol. 84, pp. 288\u2013297.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20S.%20Chen%2C%20J.%20Chen%2C%20P.-Y.%20Hero%2C%20A.O.%20Zeroth-order%20online%20admm%3A%20Convergence%20analysis%20and%20applications%2C%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20S.%20Chen%2C%20J.%20Chen%2C%20P.-Y.%20Hero%2C%20A.O.%20Zeroth-order%20online%20admm%3A%20Convergence%20analysis%20and%20applications%2C%202018-04"
        },
        {
            "id": "6",
            "entry": "[6] M. C. Fu, \u201cOptimization for simulation: Theory vs. practice,\u201d INFORMS Journal on Computing, vol. 14, no. 3, pp. 192\u2013215, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20M.C.%20Optimization%20for%20simulation%3A%20Theory%20vs.%20practice%2C%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20M.C.%20Optimization%20for%20simulation%3A%20Theory%20vs.%20practice%2C%202002"
        },
        {
            "id": "7",
            "entry": "[7] X. Lian, H. Zhang, C.-J. Hsieh, Y. Huang, and J. Liu, \u201cA comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order,\u201d in Advances in Neural Information Processing Systems, 2016, pp. 3054\u20133062.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Huang%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20X.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Huang%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%2C%202016"
        },
        {
            "id": "8",
            "entry": "[8] R. P. Brent, Algorithms for minimization without derivatives, Courier Corporation, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brent%2C%20R.P.%20Algorithms%20for%20minimization%20without%20derivatives%202013"
        },
        {
            "id": "9",
            "entry": "[9] J. C. Spall, Introduction to stochastic search and optimization: estimation, simulation, and control, vol. 65, John Wiley & Sons, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spall%2C%20J.C.%20Introduction%20to%20stochastic%20search%20and%20optimization%3A%20estimation%2C%20simulation%2C%20and%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spall%2C%20J.C.%20Introduction%20to%20stochastic%20search%20and%20optimization%3A%20estimation%2C%20simulation%2C%20and%202005"
        },
        {
            "id": "10",
            "entry": "[10] A. D. Flaxman, A. T. Kalai, and H. B. McMahan, \u201cOnline convex optimization in the bandit setting: gradient descent without a gradient,\u201d in Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms, 2005, pp. 385\u2013394.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flaxman%2C%20A.D.%20Kalai%2C%20A.T.%20McMahan%2C%20H.B.%20Online%20convex%20optimization%20in%20the%20bandit%20setting%3A%20gradient%20descent%20without%20a%20gradient%2C%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flaxman%2C%20A.D.%20Kalai%2C%20A.T.%20McMahan%2C%20H.B.%20Online%20convex%20optimization%20in%20the%20bandit%20setting%3A%20gradient%20descent%20without%20a%20gradient%2C%202005"
        },
        {
            "id": "11",
            "entry": "[11] O. Shamir, \u201cOn the complexity of bandit and derivative-free stochastic convex optimization,\u201d in Conference on Learning Theory, 2013, pp. 3\u201324.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20O.%20%E2%80%9COn%20the%20complexity%20of%20bandit%20and%20derivative-free%20stochastic%20convex%20optimization%2C%E2%80%9D%20in%20Conference%20on%20Learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20O.%20%E2%80%9COn%20the%20complexity%20of%20bandit%20and%20derivative-free%20stochastic%20convex%20optimization%2C%E2%80%9D%20in%20Conference%20on%20Learning%202013"
        },
        {
            "id": "12",
            "entry": "[12] A. Agarwal, O. Dekel, and L. Xiao, \u201cOptimal algorithms for online convex optimization with multi-point bandit feedback,\u201d in COLT, 2010, pp. 28\u201340.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Dekel%2C%20O.%20Xiao%2C%20L.%20Optimal%20algorithms%20for%20online%20convex%20optimization%20with%20multi-point%20bandit%20feedback%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Dekel%2C%20O.%20Xiao%2C%20L.%20Optimal%20algorithms%20for%20online%20convex%20optimization%20with%20multi-point%20bandit%20feedback%2C%202010"
        },
        {
            "id": "13",
            "entry": "[13] Y. Nesterov and V. Spokoiny, \u201cRandom gradient-free minimization of convex functions,\u201d Foundations of Computational Mathematics, vol. 2, no. 17, pp. 527\u2013566, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Spokoiny%2C%20V.%20Random%20gradient-free%20minimization%20of%20convex%20functions%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Spokoiny%2C%20V.%20Random%20gradient-free%20minimization%20of%20convex%20functions%2C%202015"
        },
        {
            "id": "14",
            "entry": "[14] J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono, \u201cOptimal rates for zero-order convex optimization: The power of two function evaluations,\u201d IEEE Transactions on Information Theory, vol. 61, no. 5, pp. 2788\u20132806, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.C.%20Jordan%2C%20M.I.%20Wainwright%2C%20M.J.%20Wibisono%2C%20A.%20Optimal%20rates%20for%20zero-order%20convex%20optimization%3A%20The%20power%20of%20two%20function%20evaluations%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.C.%20Jordan%2C%20M.I.%20Wainwright%2C%20M.J.%20Wibisono%2C%20A.%20Optimal%20rates%20for%20zero-order%20convex%20optimization%3A%20The%20power%20of%20two%20function%20evaluations%2C%202015"
        },
        {
            "id": "15",
            "entry": "[15] O. Shamir, \u201cAn optimal algorithm for bandit and zero-order convex optimization with two-point feedback,\u201d Journal of Machine Learning Research, vol. 18, no. 52, pp. 1\u201311, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20O.%20An%20optimal%20algorithm%20for%20bandit%20and%20zero-order%20convex%20optimization%20with%20two-point%20feedback%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20O.%20An%20optimal%20algorithm%20for%20bandit%20and%20zero-order%20convex%20optimization%20with%20two-point%20feedback%2C%202017"
        },
        {
            "id": "16",
            "entry": "[16] X. Gao, B. Jiang, and S. Zhang, \u201cOn the information-adaptive variants of the admm: an iteration complexity perspective,\u201d Optimization Online, vol. 12, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20X.%20Jiang%2C%20B.%20Zhang%2C%20S.%20On%20the%20information-adaptive%20variants%20of%20the%20admm%3A%20an%20iteration%20complexity%20perspective%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20X.%20Jiang%2C%20B.%20Zhang%2C%20S.%20On%20the%20information-adaptive%20variants%20of%20the%20admm%3A%20an%20iteration%20complexity%20perspective%2C%202014"
        },
        {
            "id": "17",
            "entry": "[17] P. Dvurechensky, A. Gasnikov, and E. Gorbunov, \u201cAn accelerated method for derivative-free smooth stochastic convex optimization,\u201d arXiv preprint arXiv:1802.09022, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09022"
        },
        {
            "id": "18",
            "entry": "[18] Y. Wang, S. Du, S. Balakrishnan, and A. Singh, \u201cStochastic zeroth-order optimization in high dimensions,\u201d in Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics. April 2018, vol. 84, pp. 1356\u20131365, PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Du%2C%20S.%20Balakrishnan%2C%20S.%20Singh%2C%20A.%20Stochastic%20zeroth-order%20optimization%20in%20high%20dimensions%2C%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Du%2C%20S.%20Balakrishnan%2C%20S.%20Singh%2C%20A.%20Stochastic%20zeroth-order%20optimization%20in%20high%20dimensions%2C%202018-04"
        },
        {
            "id": "19",
            "entry": "[19] R. Johnson and T. Zhang, \u201cAccelerating stochastic gradient descent using predictive variance reduction,\u201d in Advances in neural information processing systems, 2013, pp. 315\u2013323.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20%E2%80%9CAccelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%2C%E2%80%9D%20in%20Advances%20in%20neural%20information%20processing%20systems%202013"
        },
        {
            "id": "20",
            "entry": "[20] S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola, \u201cStochastic variance reduction for nonconvex optimization,\u201d in International conference on machine learning, 2016, pp. 314\u2013323.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%2C%202016"
        },
        {
            "id": "21",
            "entry": "[21] A. Nitanda, \u201cAccelerated stochastic gradient descent for minimizing finite sums,\u201d in Artificial Intelligence and Statistics, 2016, pp. 195\u2013203.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nitanda%2C%20A.%20Accelerated%20stochastic%20gradient%20descent%20for%20minimizing%20finite%20sums%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nitanda%2C%20A.%20Accelerated%20stochastic%20gradient%20descent%20for%20minimizing%20finite%20sums%2C%202016"
        },
        {
            "id": "22",
            "entry": "[22] Z. Allen-Zhu and Y. Yuan, \u201cImproved svrg for non-strongly-convex or sum-of-non-convex objectives,\u201d in International conference on machine learning, 2016, pp. 1080\u20131089.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Yuan%2C%20Y.%20Improved%20svrg%20for%20non-strongly-convex%20or%20sum-of-non-convex%20objectives%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Yuan%2C%20Y.%20Improved%20svrg%20for%20non-strongly-convex%20or%20sum-of-non-convex%20objectives%2C%202016"
        },
        {
            "id": "23",
            "entry": "[23] L. Lei, C. Ju, J. Chen, and M. I. Jordan, \u201cNon-convex finite-sum optimization via scsg methods,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 2345\u20132355.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%2C%202017"
        },
        {
            "id": "24",
            "entry": "[24] S. Ghadimi and G. Lan, \u201cStochastic first-and zeroth-order methods for nonconvex stochastic programming,\u201d SIAM Journal on Optimization, vol. 23, no. 4, pp. 2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%2C%202013"
        },
        {
            "id": "25",
            "entry": "[25] D. Hajinezhad, M. Hong, and A. Garcia, \u201cZeroth order nonconvex multi-agent optimization over networks,\u201d arXiv preprint arXiv:1710.09997, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09997"
        },
        {
            "id": "26",
            "entry": "[26] B. Gu, Z. Huo, and H. Huang, \u201cZeroth-order asynchronous doubly stochastic algorithm with variance reduction,\u201d arXiv preprint arXiv:1612.01425, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01425"
        },
        {
            "id": "27",
            "entry": "[27] K. M. Choromanski and V. Sindhwani, \u201cOn blackbox backpropagation and jacobian sensing,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 6524\u20136532.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanski%2C%20K.M.%20Sindhwani%2C%20V.%20On%20blackbox%20backpropagation%20and%20jacobian%20sensing%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanski%2C%20K.M.%20Sindhwani%2C%20V.%20On%20blackbox%20backpropagation%20and%20jacobian%20sensing%2C%202017"
        },
        {
            "id": "28",
            "entry": "[28] G. Tucker, A. Mnih, C. J. Maddison, J. Lawson, and J. Sohl-Dickstein, \u201cRebar: Low-variance, unbiased gradient estimates for discrete latent variable models,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 2624\u20132633.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tucker%2C%20G.%20Mnih%2C%20A.%20Maddison%2C%20C.J.%20Lawson%2C%20J.%20Rebar%3A%20Low-variance%2C%20unbiased%20gradient%20estimates%20for%20discrete%20latent%20variable%20models%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tucker%2C%20G.%20Mnih%2C%20A.%20Maddison%2C%20C.J.%20Lawson%2C%20J.%20Rebar%3A%20Low-variance%2C%20unbiased%20gradient%20estimates%20for%20discrete%20latent%20variable%20models%2C%202017"
        },
        {
            "id": "29",
            "entry": "[29] W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud, \u201cBackpropagation through the void: Optimizing control variates for black-box gradient estimation,\u201d arXiv preprint arXiv:1711.00123, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00123"
        },
        {
            "id": "30",
            "entry": "[30] N. S. Chatterji, N. Flammarion, Y.-A. Ma, P. L. Bartlett, and M. I. Jordan, \u201cOn the theory of variance reduction for stochastic gradient monte carlo,\u201d arXiv preprint arXiv:1802.05431, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05431"
        },
        {
            "id": "31",
            "entry": "[31] W. Yang and P. W. Ayers, \u201cDensity-functional theory,\u201d in Computational Medicinal Chemistry for Drug Discovery, pp. 103\u2013132. CRC Press, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20W.%20Ayers%2C%20P.W.%20%E2%80%9CDensity-functional%20theory%2C%E2%80%9D%20in%20Computational%20Medicinal%20Chemistry%20for%20Drug%20Discovery%202003"
        },
        {
            "id": "32",
            "entry": "[32] P. Xu, F. Roosta-Khorasan, and M. W. Mahoney, \u201cSecond-order optimization for non-convex machine learning: An empirical study,\u201d arXiv preprint arXiv:1708.07827, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07827"
        },
        {
            "id": "33",
            "entry": "[33] S. Kirklin, J. E. Saal, B. Meredig, A. Thompson, J. W. Doak, M. Aykol, S. R\u00fchl, and C. Wolverton, \u201cThe open quantum materials database (oqmd): assessing the accuracy of dft formation energies,\u201d npj Computational Materials, vol. 1, pp. 15010, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirklin%2C%20S.%20Saal%2C%20J.E.%20Meredig%2C%20B.%20Thompson%2C%20A.%20The%20open%20quantum%20materials%20database%20%28oqmd%29%3A%20assessing%20the%20accuracy%20of%20dft%20formation%20energies%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirklin%2C%20S.%20Saal%2C%20J.E.%20Meredig%2C%20B.%20Thompson%2C%20A.%20The%20open%20quantum%20materials%20database%20%28oqmd%29%3A%20assessing%20the%20accuracy%20of%20dft%20formation%20energies%2C%202015"
        },
        {
            "id": "34",
            "entry": "[34] G. Kresse and J. Furthm\u00fcller, \u201cEfficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set,\u201d Computational materials science, vol. 6, no. 1, pp. 15\u201350, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kresse%2C%20G.%20Furthm%C3%BCller%2C%20J.%20Efficiency%20of%20ab-initio%20total%20energy%20calculations%20for%20metals%20and%20semiconductors%20using%20a%20plane-wave%20basis%20set%2C%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kresse%2C%20G.%20Furthm%C3%BCller%2C%20J.%20Efficiency%20of%20ab-initio%20total%20energy%20calculations%20for%20metals%20and%20semiconductors%20using%20a%20plane-wave%20basis%20set%2C%201996"
        },
        {
            "id": "35",
            "entry": "[35] N. Carlini and D. Wagner, \u201cTowards evaluating the robustness of neural networks,\u201d in IEEE Symposium on Security and Privacy, 2017, pp. 39\u201357.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%2C%202017"
        },
        {
            "id": "36",
            "entry": "[36] H. Wang and A. Banerjee, \u201cRandomized block coordinate descent for online and stochastic optimization,\u201d arXiv preprint arXiv:1407.0107, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.0107"
        },
        {
            "id": "37",
            "entry": "[37] S. Shalev-Shwartz, \u201cOnline learning and online convex optimization,\u201d Foundations and Trends R in Machine Learning, vol. 4, no. 2, pp. 107\u2013194, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Online%20learning%20and%20online%20convex%20optimization%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Online%20learning%20and%20online%20convex%20optimization%2C%202012"
        },
        {
            "id": "38",
            "entry": "[38] L. Ward, A. Agrawal, A. Choudhary, and C. Wolverton, \u201cA general-purpose machine learning framework for predicting properties of inorganic materials,\u201d npj Computational Materials, vol. 2, pp. 16028, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ward%2C%20L.%20Agrawal%2C%20A.%20Choudhary%2C%20A.%20Wolverton%2C%20C.%20A%20general-purpose%20machine%20learning%20framework%20for%20predicting%20properties%20of%20inorganic%20materials%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ward%2C%20L.%20Agrawal%2C%20A.%20Choudhary%2C%20A.%20Wolverton%2C%20C.%20A%20general-purpose%20machine%20learning%20framework%20for%20predicting%20properties%20of%20inorganic%20materials%2C%202016"
        }
    ]
}
