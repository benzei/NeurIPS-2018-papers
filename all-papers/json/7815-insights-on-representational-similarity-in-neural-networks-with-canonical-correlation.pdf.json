{
    "filename": "7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf",
    "metadata": {
        "title": "Insights on representational similarity in neural networks with canonical correlation",
        "author": "Ari Morcos, Maithra Raghu, Samy Bengio",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "Penn Treebank",
            "url": "https://en.wikipedia.org/wiki/Penn_Treebank"
        },
        {
            "term": "Gaussian processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_processes"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "Canonical Correlation Analysis",
            "url": "https://en.wikipedia.org/wiki/Canonical_Correlation_Analysis"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "As neural networks have become more powerful, an increasing number of studies have sought to decipher their internal representations [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "Building off of [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], we demonstrate that groups of networks which generalize converge to more similar solutions than those which memorize (Section 3.1), that wider networks converge to more similar solutions than narrower networks (Section 3.2), and that networks with identical topology but distinct learning rates converge to a small set of diverse solutions (Section 3.3).\n3",
        "We use this property of Canonical Correlation Analysis to evaluate whether groups of networks trained on CIFAR-10 with different random initializations converge to similar solutions under the following conditions:",
        "We developed Canonical Correlation Analysis as a tool to gain insights on many representational properties of deep neural networks",
        "We found that the representations in hidden layers of a neural network contain both \u201csignal\u201d components, which are stable over training and correspond to performance curves, and an unstable \u201cnoise\u201d component",
        "Leveraging the ability of Canonical Correlation Analysis to compare across different networks, we investigated the properties of converged solutions of convolutional neural networks (Section 3), finding that networks which generalize converge to more similar solutions than those which memorize (Section 3.1), that wider networks converge to more similar solutions than narrow networks (Section 3.2), and that across otherwise identical networks with different random initializations and learning rates, networks converge to diverse clusters of solutions (Section 3.3)"
    ],
    "key_statements": [
        "As neural networks have become more powerful, an increasing number of studies have sought to decipher their internal representations [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "Building off of [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], we demonstrate that groups of networks which generalize converge to more similar solutions than those which memorize (Section 3.1), that wider networks converge to more similar solutions than narrower networks (Section 3.2), and that networks with identical topology but distinct learning rates converge to a small set of diverse solutions (Section 3.3).\n3",
        "As expected we found that while the naive mean was extremely sensitive to this ratio, the projection weighted mean was largely robust (Figure 2).\n3 Using Canonical Correlation Analysis to measure the similarity of converged solutions",
        "We use this property of Canonical Correlation Analysis to evaluate whether groups of networks trained on CIFAR-10 with different random initializations converge to similar solutions under the following conditions:",
        "To evaluate the similarity of converged solutions, we measured the pairwise projection weighted Canonical Correlation Analysis distance for each layer among networks trained on unmodified CIFAR-10 (\u201cGeneralizing\u201d), among networks trained on randomized label CIFAR-10 (\u201cMemorizing\u201d) and between each pair of networks trained on unmodified and random\n7Details of the architectures and training procedures for this and following experiments can be found in Appendix A.4",
        "Sets of both generalizing and memorizing networks converged to highly similar solutions when Canonical Correlation Analysis distance was computed based on training data; when test data was used, only generalizing networks converged to similar softmax outputs (Figure A10), again reflecting that each memorizing network memorizes the training data using a different strategy",
        "This is consistent with the equivalence of deep networks to Gaussian processes (GPs) in the limit of infinite width\n8To control for variability in Canonical Correlation Analysis distance due to comparisons across representations of different sizes, a random subset of 128 filters from the final layer were used for all network comparisons",
        "One specific question we explore is whether the learning dynamics of RNNs mirror the \u201cbottom up\u201d convergence observed in the feedforward case in [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], as well as investigating whether Canonical Correlation Analysis produces qualitatively better outputs than other metrics",
        "We primarily focus on the training notion of time; we perform a preliminary investigation of the sequence notion of time as well, demonstrating that Canonical Correlation Analysis is capable of finding similarity across sequence timesteps which are missed by traditional metrics (Figures A2, A4), but that even Canonical Correlation Analysis often fails to find similarity in the hidden state across sequence timesteps, suggesting that representations over sequence timesteps are often not linearly similar (Figure A3).\n4.1",
        "We developed Canonical Correlation Analysis as a tool to gain insights on many representational properties of deep neural networks",
        "We found that the representations in hidden layers of a neural network contain both \u201csignal\u201d components, which are stable over training and correspond to performance curves, and an unstable \u201cnoise\u201d component",
        "Leveraging the ability of Canonical Correlation Analysis to compare across different networks, we investigated the properties of converged solutions of convolutional neural networks (Section 3), finding that networks which generalize converge to more similar solutions than those which memorize (Section 3.1), that wider networks converge to more similar solutions than narrow networks (Section 3.2), and that across otherwise identical networks with different random initializations and learning rates, networks converge to diverse clusters of solutions (Section 3.3)"
    ],
    "summary": [
        "As neural networks have become more powerful, an increasing number of studies have sought to decipher their internal representations [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "We use this property of CCA to evaluate whether groups of networks trained on CIFAR-10 with different random initializations converge to similar solutions under the following conditions:",
        "To evaluate the similarity of converged solutions, we measured the pairwise projection weighted CCA distance for each layer among networks trained on unmodified CIFAR-10 (\u201cGeneralizing\u201d), among networks trained on randomized label CIFAR-10 (\u201cMemorizing\u201d) and between each pair of networks trained on unmodified and random",
        "We found that at early layers, all networks converged to similar solutions, regardless of whether they generalize or memorize (Figure 3).",
        "At later layers, groups of generalizing networks converged to substantially more similar solutions than groups of memorizing networks (Figure 3).",
        "Sets of both generalizing and memorizing networks converged to highly similar solutions when CCA distance was computed based on training data; when test data was used, only generalizing networks converged to similar softmax outputs (Figure A10), again reflecting that each memorizing network memorizes the training data using a different strategy.",
        "8To control for variability in CCA distance due to comparisons across representations of different sizes, a random subset of 128 filters from the final layer were used for all network comparisons.",
        "To measure the convergence of representations through training time, we computed the projection weighted mean CCA value for each layer\u2019s representation throughout training to its final representation.",
        "We found that the representations in hidden layers of a neural network contain both \u201csignal\u201d components, which are stable over training and correspond to performance curves, and an unstable \u201cnoise\u201d component.",
        "Leveraging the ability of CCA to compare across different networks, we investigated the properties of converged solutions of convolutional neural networks (Section 3), finding that networks which generalize converge to more similar solutions than those which memorize (Section 3.1), that wider networks converge to more similar solutions than narrow networks (Section 3.2), and that across otherwise identical networks with different random initializations and learning rates, networks converge to diverse clusters of solutions (Section 3.3).",
        "We used projection weighted CCA to study the dynamics of RNNs, (Section 4), finding that RNNs exhibit bottom-up convergence over the course of training (Section 4.1), and that across sequence timesteps, RNN representations vary nonlinearly (Section A.3).",
        "We observed that networks which converge to similar solutions exhibit higher generalization performance (Figure 4b)."
    ],
    "headline": "We develop projection weighted Canonical Correlation Analysis  as a tool for understanding neural networks, building off of SVCCA, a recently proposed method ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural networks. J. Emerg. Technol. Comput. Syst., 13(3):32:1\u201332:18, February 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anwar%2C%20Sajid%20Hwang%2C%20Kyuyeon%20Sung%2C%20Wonyong%20Structured%20pruning%20of%20deep%20convolutional%20neural%20networks%202017-02-18",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anwar%2C%20Sajid%20Hwang%2C%20Kyuyeon%20Sung%2C%20Wonyong%20Structured%20pruning%20of%20deep%20convolutional%20neural%20networks%202017-02-18"
        },
        {
            "id": "2",
            "entry": "[2] Devansh Arpit, Stanis\u0142aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML\u201917), June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arpit%2C%20Devansh%20Jastrzebski%2C%20Stanis%C5%82aw%20Ballas%2C%20Nicolas%20Krueger%2C%20David%20A%20closer%20look%20at%20memorization%20in%20deep%20networks%202017-06-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arpit%2C%20Devansh%20Jastrzebski%2C%20Stanis%C5%82aw%20Ballas%2C%20Nicolas%20Krueger%2C%20David%20A%20closer%20look%20at%20memorization%20in%20deep%20networks%202017-06-17"
        },
        {
            "id": "3",
            "entry": "[3] Maurice S. Bartlett. The statistical significance of canonical correlations. In Biometrika, volume 32, pages 29 \u2013 37, 1941.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Maurice%20S.%20The%20statistical%20significance%20of%20canonical%20correlations%201941",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Maurice%20S.%20The%20statistical%20significance%20of%20canonical%20correlations%201941"
        },
        {
            "id": "4",
            "entry": "[4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20dissection%3A%20Quantifying%20interpretability%20of%20deep%20visual%20representations%202017"
        },
        {
            "id": "5",
            "entry": "[5] Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilingual correlation. In Association for Computational Linguistics, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faruqui%2C%20Manaal%20Dyer%2C%20Chris%20Improving%20vector%20space%20word%20representations%20using%20multilingual%20correlation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faruqui%2C%20Manaal%20Dyer%2C%20Chris%20Improving%20vector%20space%20word%20representations%20using%20multilingual%20correlation%202014"
        },
        {
            "id": "6",
            "entry": "[6] Mikhail Figurnov, Aizhan Ibraimova, Dmitry P Vetrov, and Pushmeet Kohli. PerforatedCNNs: Acceleration through elimination of redundant convolutions. In D D Lee, M Sugiyama, U V Luxburg, I Guyon, and R Garnett, editors, Advances in Neural Information Processing Systems 29, pages 947\u2013955. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Figurnov%2C%20Mikhail%20Ibraimova%2C%20Aizhan%20Vetrov%2C%20Dmitry%20P.%20Kohli%2C%20Pushmeet%20PerforatedCNNs%3A%20Acceleration%20through%20elimination%20of%20redundant%20convolutions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Figurnov%2C%20Mikhail%20Ibraimova%2C%20Aizhan%20Vetrov%2C%20Dmitry%20P.%20Kohli%2C%20Pushmeet%20PerforatedCNNs%3A%20Acceleration%20through%20elimination%20of%20redundant%20convolutions%202016"
        },
        {
            "id": "7",
            "entry": "[7] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural networks. CoRR, abs/1803.03635, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03635"
        },
        {
            "id": "8",
            "entry": "[8] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In Proceedings of the 4th International Conference on Learning Representations (ICLR\u201916), October 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202015-10-16",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202015-10-16"
        },
        {
            "id": "9",
            "entry": "[9] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks. CoRR, abs/1506.02626, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02626"
        },
        {
            "id": "10",
            "entry": "[10] Harold Hotelling. Relations between two sets of variates. In Biometrika, volume 28, pages 321\u2013337, 1936.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hotelling%2C%20Harold%20Relations%20between%20two%20sets%20of%20variates",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hotelling%2C%20Harold%20Relations%20between%20two%20sets%20of%20variates"
        },
        {
            "id": "11",
            "entry": "[11] Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. International Conference on Learning Representations Workshop, abs/1506.02078, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Johnson%2C%20Justin%20Fei-Fei%2C%20Li%20Visualizing%20and%20understanding%20recurrent%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Johnson%2C%20Justin%20Fei-Fei%2C%20Li%20Visualizing%20and%20understanding%20recurrent%20networks%202016"
        },
        {
            "id": "12",
            "entry": "[12] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In D S Touretzky, editor, Advances in Neural Information Processing Systems 2, pages 598\u2013605. Morgan-Kaufmann, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990"
        },
        {
            "id": "13",
            "entry": "[13] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. In International Conference on Learning Representations (ICLR\u201917), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jaehoon%20Bahri%2C%20Yasaman%20Novak%2C%20Roman%20Schoenholz%2C%20Samuel%20S.%20Deep%20neural%20networks%20as%20gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jaehoon%20Bahri%2C%20Yasaman%20Novak%2C%20Roman%20Schoenholz%2C%20Samuel%20S.%20Deep%20neural%20networks%20as%20gaussian%20processes%202018"
        },
        {
            "id": "14",
            "entry": "[14] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, April 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chunyuan%20Farkhoor%2C%20Heerad%20Liu%2C%20Rosanne%20Yosinski%2C%20Jason%20Measuring%20the%20intrinsic%20dimension%20of%20objective%20landscapes%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chunyuan%20Farkhoor%2C%20Heerad%20Liu%2C%20Rosanne%20Yosinski%2C%20Jason%20Measuring%20the%20intrinsic%20dimension%20of%20objective%20landscapes%202018-04"
        },
        {
            "id": "15",
            "entry": "[15] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient ConvNets. In International Conference on Learning Representations (ICLR\u201917), pages 1\u201310, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Hao%20Kadav%2C%20Asim%20Durdanovic%2C%20Igor%20Samet%2C%20Hanan%20Pruning%20filters%20for%20efficient%20ConvNets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Hao%20Kadav%2C%20Asim%20Durdanovic%2C%20Igor%20Samet%2C%20Hanan%20Pruning%20filters%20for%20efficient%20ConvNets%202017"
        },
        {
            "id": "16",
            "entry": "[16] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do different neural networks learn the same representations? In Feature Extraction: Modern Questions and Challenges, pages 196\u2013212, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202015"
        },
        {
            "id": "17",
            "entry": "[17] AGDG Matthews, J Hron, M Rowland, RE Turner, and Z Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations (ICLR\u201918), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthews%2C%20A.G.D.G.%20Hron%2C%20J.%20Rowland%2C%20M.%20Turner%2C%20R.E.%20Gaussian%20process%20behaviour%20in%20wide%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matthews%2C%20A.G.D.G.%20Hron%2C%20J.%20Rowland%2C%20M.%20Turner%2C%20R.E.%20Gaussian%20process%20behaviour%20in%20wide%20deep%20neural%20networks%202018"
        },
        {
            "id": "18",
            "entry": "[18] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM Language Models. arXiv preprint arXiv:1708.02182, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02182"
        },
        {
            "id": "19",
            "entry": "[19] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An Analysis of Neural Language Modeling at Multiple Scales. arXiv preprint arXiv:1803.08240, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08240"
        },
        {
            "id": "20",
            "entry": "[20] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. In International Conference on Learning Representations (ICLR\u201917), November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molchanov%2C%20Pavlo%20Tyree%2C%20Stephen%20Karras%2C%20Tero%20Aila%2C%20Timo%20Pruning%20convolutional%20neural%20networks%20for%20resource%20efficient%20inference%202016-11-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Molchanov%2C%20Pavlo%20Tyree%2C%20Stephen%20Karras%2C%20Tero%20Aila%2C%20Timo%20Pruning%20convolutional%20neural%20networks%20for%20resource%20efficient%20inference%202016-11-17"
        },
        {
            "id": "21",
            "entry": "[21] Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. In International Conference on Learning Representations (ICLR\u201918), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morcos%2C%20Ari%20S.%20Barrett%2C%20David%20G.T.%20Rabinowitz%2C%20Neil%20C.%20Botvinick%2C%20Matthew%20On%20the%20importance%20of%20single%20directions%20for%20generalization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morcos%2C%20Ari%20S.%20Barrett%2C%20David%20G.T.%20Rabinowitz%2C%20Neil%20C.%20Botvinick%2C%20Matthew%20On%20the%20importance%20of%20single%20directions%20for%20generalization%202018"
        },
        {
            "id": "22",
            "entry": "[22] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20Maithra%20Gilmer%2C%20Justin%20Yosinski%2C%20Jason%20Sohl-Dickstein%2C%20Jascha%20Svcca%3A%20Singular%20vector%20canonical%20correlation%20analysis%20for%20deep%20learning%20dynamics%20and%20interpretability%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Gilmer%2C%20Justin%20Yosinski%2C%20Jason%20Sohl-Dickstein%2C%20Jascha%20Svcca%3A%20Singular%20vector%20canonical%20correlation%20analysis%20for%20deep%20learning%20dynamics%20and%20interpretability%202017"
        },
        {
            "id": "23",
            "entry": "[23] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural network that finds a naturalistic solution for the production of muscle activity. Nature neuroscience, 18(7):1025\u20131033, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sussillo%2C%20David%20Churchland%2C%20Mark%20M.%20Kaufman%2C%20Matthew%20T.%20Shenoy%2C%20Krishna%20V.%20A%20neural%20network%20that%20finds%20a%20naturalistic%20solution%20for%20the%20production%20of%20muscle%20activity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sussillo%2C%20David%20Churchland%2C%20Mark%20M.%20Kaufman%2C%20Matthew%20T.%20Shenoy%2C%20Krishna%20V.%20A%20neural%20network%20that%20finds%20a%20naturalistic%20solution%20for%20the%20production%20of%20muscle%20activity%202015"
        },
        {
            "id": "24",
            "entry": "[24] Viivi Uurtio, Jo\u00e3o M. Monteiro, Jaz Kandola, John Shawe-Taylor, Delmiro Fernandez-Reyes, and Juho Rousu. A tutorial on canonical correlation methods. ACM Comput. Surv., 50(6):95:1\u2013 95:33, November 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uurtio%2C%20Viivi%20Monteiro%2C%20Jo%C3%A3o%20M.%20Kandola%2C%20Jaz%20Shawe-Taylor%2C%20John%20A%20tutorial%20on%20canonical%20correlation%20methods%202017-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uurtio%2C%20Viivi%20Monteiro%2C%20Jo%C3%A3o%20M.%20Kandola%2C%20Jaz%20Shawe-Taylor%2C%20John%20A%20tutorial%20on%20canonical%20correlation%20methods%202017-11"
        },
        {
            "id": "25",
            "entry": "[25] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. In Deep Learning Workshop, International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Nguyen%2C%20Anh%20Fuchs%2C%20Thomas%20Understanding%20neural%20networks%20through%20deep%20visualization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Nguyen%2C%20Anh%20Fuchs%2C%20Thomas%20Understanding%20neural%20networks%20through%20deep%20visualization%202015"
        },
        {
            "id": "26",
            "entry": "[26] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818\u2013833.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks"
        },
        {
            "id": "27",
            "entry": "[27] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. International Conference on Learning Representations (ICLR\u201916), abs/1611.03530, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202016"
        }
    ]
}
