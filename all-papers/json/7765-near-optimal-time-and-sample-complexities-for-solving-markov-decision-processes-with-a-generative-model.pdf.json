{
    "filename": "7765-near-optimal-time-and-sample-complexities-for-solving-markov-decision-processes-with-a-generative-model.pdf",
    "metadata": {
        "title": "Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model",
        "author": "Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, Yinyu Ye",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7765-near-optimal-time-and-sample-complexities-for-solving-markov-decision-processes-with-a-generative-model.pdf"
        },
        "abstract": "In this paper we consider the problem of computing an -optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in O(1) time. Given such a DMDP with states S, actions A, discount factor \u03b3 \u2208 (0, 1), and rewards in range [0, 1] we provide an algorithm which computes an -optimal policy with probability 1 \u2212 \u03b4 where both the time spent and number of sample taken are upper bounded by"
    },
    "keywords": [
        {
            "term": "markov decision process",
            "url": "https://en.wikipedia.org/wiki/markov_decision_process"
        },
        {
            "term": "discount factor",
            "url": "https://en.wikipedia.org/wiki/discount_factor"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision processes",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_processes"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "transition function",
            "url": "https://en.wikipedia.org/wiki/transition_function"
        }
    ],
    "highlights": [
        "Markov decision processes (MDPs) are a fundamental mathematical abstraction used to model sequential decision making under uncertainty and are a basic model of discrete-time stochastic control and reinforcement learning (RL)",
        "Central to reinforcement learning is the case of computing or learning an approximately optimal policy when the Markov decision processes itself is not fully known beforehand",
        "The main result of this paper is that we provide the first algorithm that is sample-optimal and runtime-optimal for computing an -optimal policy of a discounted Markov Decision Process with a generative model|S|)",
        "We show that the number of samples needed by this algorithm is O(H3|S||A| \u22122), in order to obtain an -optimal policy for H-horizon Markov decision processes",
        "We describe a discounted Markov Decision Process by the tuple (S, A, P , r, \u03b3), where S is a finite state space, A is a finite action space, P \u2208 RS\u00d7A\u00d7S is the state-action-state transition matrix, r \u2208 RS\u00d7A is the state-action reward vector, and \u03b3 \u2208 (0, 1) is a discount factor"
    ],
    "key_statements": [
        "Markov decision processes (MDPs) are a fundamental mathematical abstraction used to model sequential decision making under uncertainty and are a basic model of discrete-time stochastic control and reinforcement learning (RL)",
        "Central to reinforcement learning is the case of computing or learning an approximately optimal policy when the Markov decision processes itself is not fully known beforehand",
        "Computing an approximately optimal policy with high probability in this case is known as PAC reinforcement learning with a generative model",
        "The main result of this paper is that we provide the first algorithm that is sample-optimal and runtime-optimal for computing an -optimal policy of a discounted Markov Decision Process with a generative model|S|)",
        "We show that the number of samples needed by this algorithm is O(H3|S||A| \u22122), in order to obtain an -optimal policy for H-horizon Markov decision processes",
        "We describe a discounted Markov Decision Process by the tuple (S, A, P , r, \u03b3), where S is a finite state space, A is a finite action space, P \u2208 RS\u00d7A\u00d7S is the state-action-state transition matrix, r \u2208 RS\u00d7A is the state-action reward vector, and \u03b3 \u2208 (0, 1) is a discount factor",
        "Another work [Wan17] gave a randomized mirror-prox method that applies to a special Bellman saddle point formulation of the discounted Markov Decision Process",
        "A recent closely related work is [SWWY18] which gave a variance-reduced randomized value iteration that works with the generative model and finds an approximate policy in sample size/run time O(|S||A| \u22122(1\u2212\u03b3)\u22124), without requiring any ergodicity assumption.\n4Although not explicitly stated, an immediate derivation shows that obtaining an -optimal policy in [AMK13] requires O(|S||A|(1 \u2212 \u03b3)\u22125 \u22122) samples.\n5The dependence on (1 \u2212 \u03b3) in [KS99] is not stated explicitly but we believe basic calculations yield O(1/(1 \u2212 \u03b3)7)"
    ],
    "summary": [
        "Markov decision processes (MDPs) are a fundamental mathematical abstraction used to model sequential decision making under uncertainty and are a basic model of discrete-time stochastic control and reinforcement learning (RL).",
        "The main result of this paper is that we provide the first algorithm that is sample-optimal and runtime-optimal for computing an -optimal policy of a DMDP with a generative model|S|).",
        "We develop a randomized Variance-Reduced Q-Value Iteration based algorithm that computes an -optimal policy with probability 1 \u2212 \u03b4 with a number of samples, i.e. queries to the generative model, bound by",
        "(See Section 5 for an in-depth comparison.) The paper [AMK13] provided the first algorithm that achieves the optimal sample complexity for finding -optimal value functions, as well as the matching lower bound.",
        "The Total-Variance Technique By combining the monotonicity technique and variance reduction technique, one can obtain a O((1 \u2212 \u03b3)\u22124) sample/running time complexity on computing a policy; this was one of the results [SWWY18].",
        "The estimation error vector of the value function is upper bounded by O( \u03c3\u03c0/m), where \u03c3\u03c0(s) = Vars \u223cP s,\u03c0(s)) denotes the variance of the value of the state if starting from state s by playing policy \u03c0, and m is the number of samples collected per state-action pair.",
        "We provide and analyze our near sample/time optimal -policy computation algorithm.",
        "As discussed in Section 3 our algorithm combines three main ideas: variance reduction, the monotone value/policy iteration, and the reduction of accumulated error via Bernstein inequality.",
        "Despite the aforementioned results of [Kak03, AMK13, SWWY18], there exists only a handful of additional RL methods that achieve a small sample complexity and a small run-time complexity at the same time for computing an -optimal policy.",
        "The phased Q-learning method finds an -optimal policy using O(|S||A| \u22122/poly(1 \u2212 \u03b3)) samples/updates, where each update uses O(1) run time.5 Another work [Wan17] gave a randomized mirror-prox method that applies to a special Bellman saddle point formulation of the DMDP.",
        "A recent closely related work is [SWWY18] which gave a variance-reduced randomized value iteration that works with the generative model and finds an approximate policy in sample size/run time O(|S||A| \u22122(1\u2212\u03b3)\u22124), without requiring any ergodicity assumption.",
        "For a discounted Markov Decision Process (DMDP) M = (S, A, P , r, \u03b3) provided we can only access the transition function of the DMDP through a generative sampling model, we provide an algorithm which computes an -approximate policy with probability 1 \u2212 \u03b4 where both the time spent and number of sample taken is upper bounded by O((1\u2212\u03b3)\u22123 \u22122|S||A|).",
        "Section F extends our method and results to the finite-horizon MDP and provides a nearly matching sample complexity lower bound."
    ],
    "headline": "Given such a discounted Markov Decision Process with states S, actions A, discount factor \u03b3 \u2208 , and rewards in range  we provide an algorithm which computes an -optimal policy with probability 1 \u2212 \u03b4 where both the time spent and number of sample taken are upper bounded by",
    "reference_links": [
        {
            "id": "Azar_et+al_2013_a",
            "entry": "[AMK13] Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):325\u2013349, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20Mohammad%20Gheshlaghi%20Munos%2C%20Remi%20Kappen%2C%20Hilbert%20J.%20Minimax%20pac%20bounds%20on%20the%20sample%20complexity%20of%20reinforcement%20learning%20with%20a%20generative%20model%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azar%2C%20Mohammad%20Gheshlaghi%20Munos%2C%20Remi%20Kappen%2C%20Hilbert%20J.%20Minimax%20pac%20bounds%20on%20the%20sample%20complexity%20of%20reinforcement%20learning%20with%20a%20generative%20model%202013"
        },
        {
            "id": "Bellman_1957_a",
            "entry": "[Bel57] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellman%2C%20Richard%20Dynamic%20Programming%201957"
        },
        {
            "id": "Bertsekas_2013_a",
            "entry": "[Ber13] Dimitri P Bertsekas. Abstract dynamic programming. Athena Scientific, Belmont, MA, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Abstract%20dynamic%20programming.%20Athena%20Scientific%202013"
        },
        {
            "id": "Dantzig_2016_a",
            "entry": "[Dan16] George Dantzig. Linear Programming and Extensions. Princeton University Press, Princeton, NJ, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dantzig%2C%20George%20Linear%20Programming%20and%20Extensions%202016"
        },
        {
            "id": "Dann_2015_a",
            "entry": "[DB15] Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. In Advances in Neural Information Processing Systems, pages 2818\u20132826, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dann%2C%20Christoph%20Brunskill%2C%20Emma%20Sample%20complexity%20of%20episodic%20fixed-horizon%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dann%2C%20Christoph%20Brunskill%2C%20Emma%20Sample%20complexity%20of%20episodic%20fixed-horizon%20reinforcement%20learning%202015"
        },
        {
            "id": "D_1963_a",
            "entry": "[d\u2019E63] F d\u2019Epenoux. A probabilistic production and inventory problem. Management Science, 10(1):98\u2013108, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20probabilistic%20production%20and%20inventory%20problem%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20probabilistic%20production%20and%20inventory%20problem%201963"
        },
        {
            "id": "Ghellinck_1960_a",
            "entry": "[DG60] Guy De Ghellinck. Les problemes de decisions sequentielles. Cahiers du Centre dEtudes de Recherche Operationnelle, 2(2):161\u2013179, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghellinck%2C%20Guy%20De%20Les%20problemes%20de%20decisions%20sequentielles.%20Cahiers%20du%20Centre%20dEtudes%20de%20Recherche%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghellinck%2C%20Guy%20De%20Les%20problemes%20de%20decisions%20sequentielles.%20Cahiers%20du%20Centre%20dEtudes%20de%20Recherche%201960"
        },
        {
            "id": "Hansen_et+al_2013_a",
            "entry": "[HMZ13] Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor. J. ACM, 60(1):1:1\u20131:16, February 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20Thomas%20Dueholm%20Miltersen%2C%20Peter%20Bro%20Zwick%2C%20Uri%20Strategy%20iteration%20is%20strongly%20polynomial%20for%202-player%20turn-based%20stochastic%20games%20with%20a%20constant%20discount%20factor%202013-02-16",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20Thomas%20Dueholm%20Miltersen%2C%20Peter%20Bro%20Zwick%2C%20Uri%20Strategy%20iteration%20is%20strongly%20polynomial%20for%202-player%20turn-based%20stochastic%20games%20with%20a%20constant%20discount%20factor%202013-02-16"
        },
        {
            "id": "Howard_1960_a",
            "entry": "[How60] Ronald A. Howard. Dynamic programming and Markov processes. The MIT press, Cambridge, MA, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Howard%2C%20Ronald%20A.%20Dynamic%20programming%20and%20Markov%20processes%201960"
        },
        {
            "id": "Kakade_2003_a",
            "entry": "[Kak03] Sham M Kakade. On the sample complexity of reinforcement learning. PhD thesis, University of London London, England, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20On%20the%20sample%20complexity%20of%20reinforcement%20learning%202003"
        },
        {
            "id": "Kalathil_et+al_2014_a",
            "entry": "[KBJ14] Dileep Kalathil, Vivek S Borkar, and Rahul Jain. Empirical q-value iteration. arXiv preprint arXiv:1412.0180, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.0180"
        },
        {
            "id": "Kearns_1999_a",
            "entry": "[KS99] Michael J Kearns and Satinder P Singh. Finite-sample convergence rates for q-learning and indirect algorithms. In Advances in neural information processing systems, pages 996\u20131002, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20Michael%20J.%20Singh%2C%20Satinder%20P.%20Finite-sample%20convergence%20rates%20for%20q-learning%20and%20indirect%20algorithms%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20Michael%20J.%20Singh%2C%20Satinder%20P.%20Finite-sample%20convergence%20rates%20for%20q-learning%20and%20indirect%20algorithms%201999"
        },
        {
            "id": "Littman_et+al_1995_a",
            "entry": "[LDK95] Michael L Littman, Thomas L Dean, and Leslie Pack Kaelbling. On the complexity of solving Markov decision problems. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 394\u2013402. Morgan Kaufmann Publishers Inc., 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Dean%2C%20Thomas%20L.%20Kaelbling%2C%20Leslie%20Pack%20On%20the%20complexity%20of%20solving%20Markov%20decision%20problems%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Dean%2C%20Thomas%20L.%20Kaelbling%2C%20Leslie%20Pack%20On%20the%20complexity%20of%20solving%20Markov%20decision%20problems%201995"
        },
        {
            "id": "Lee_2014_a",
            "entry": "[LS14] Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in o (vrank) iterations and faster algorithms for maximum flow. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 424\u2013433. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Path%20finding%20methods%20for%20linear%20programming%3A%20Solving%20linear%20programs%20in%20o%20%28vrank%29%20iterations%20and%20faster%20algorithms%20for%20maximum%20flow%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Path%20finding%20methods%20for%20linear%20programming%3A%20Solving%20linear%20programs%20in%20o%20%28vrank%29%20iterations%20and%20faster%20algorithms%20for%20maximum%20flow%202014"
        },
        {
            "id": "Lee_2015_a",
            "entry": "[LS15] Yin Tat Lee and Aaron Sidford. Efficient inverse maintenance and faster algorithms for linear programming. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 230\u2013249. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Efficient%20inverse%20maintenance%20and%20faster%20algorithms%20for%20linear%20programming%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Efficient%20inverse%20maintenance%20and%20faster%20algorithms%20for%20linear%20programming%202015"
        },
        {
            "id": "Munos_1999_a",
            "entry": "[MM99] Remi Munos and Andrew W Moore. Variable resolution discretization for highaccuracy solutions of optimal control problems. Robotics Institute, page 256, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20Remi%20Moore%2C%20Andrew%20W.%20Variable%20resolution%20discretization%20for%20highaccuracy%20solutions%20of%20optimal%20control%20problems%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20Remi%20Moore%2C%20Andrew%20W.%20Variable%20resolution%20discretization%20for%20highaccuracy%20solutions%20of%20optimal%20control%20problems%201999"
        },
        {
            "id": "Mansour_1999_a",
            "entry": "[MS99] Yishay Mansour and Satinder Singh. On the complexity of policy iteration. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 401\u2013408. Morgan Kaufmann Publishers Inc., 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Singh%2C%20Satinder%20On%20the%20complexity%20of%20policy%20iteration%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Singh%2C%20Satinder%20On%20the%20complexity%20of%20policy%20iteration%201999"
        },
        {
            "id": "Scherrer_2013_a",
            "entry": "[Sch13] Bruno Scherrer. Improved and generalized upper bounds on the complexity of policy iteration. In Advances in Neural Information Processing Systems, pages 386\u2013394, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scherrer%2C%20Bruno%20Improved%20and%20generalized%20upper%20bounds%20on%20the%20complexity%20of%20policy%20iteration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scherrer%2C%20Bruno%20Improved%20and%20generalized%20upper%20bounds%20on%20the%20complexity%20of%20policy%20iteration%202013"
        },
        {
            "id": "Strehl_et+al_2009_a",
            "entry": "[SLL09] Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finite mdps: Pac analysis. Journal of Machine Learning Research, 10(Nov):2413\u2013 2444, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Littman%2C%20Michael%20L.%20Reinforcement%20learning%20in%20finite%20mdps%3A%20Pac%20analysis%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Littman%2C%20Michael%20L.%20Reinforcement%20learning%20in%20finite%20mdps%3A%20Pac%20analysis%202009"
        },
        {
            "id": "Sidford_et+al_2018_a",
            "entry": "[SWWY18] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster algorithms for solving markov decision processes. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 770\u2013787. SIAM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sidford%2C%20Aaron%20Wang%2C%20Mengdi%20Wu%2C%20Xian%20Ye%2C%20Yinyu%20Variance%20reduced%20value%20iteration%20and%20faster%20algorithms%20for%20solving%20markov%20decision%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sidford%2C%20Aaron%20Wang%2C%20Mengdi%20Wu%2C%20Xian%20Ye%2C%20Yinyu%20Variance%20reduced%20value%20iteration%20and%20faster%20algorithms%20for%20solving%20markov%20decision%20processes%202018"
        },
        {
            "id": "Tseng_1990_a",
            "entry": "[Tse90] Paul Tseng. Solving h-horizon, stationary markov decision problems in time proportional to log (h). Operations Research Letters, 9(5):287\u2013297, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tseng%2C%20Paul%20Solving%20h-horizon%2C%20stationary%20markov%20decision%20problems%20in%20time%20proportional%20to%20log%20%28h%29%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tseng%2C%20Paul%20Solving%20h-horizon%2C%20stationary%20markov%20decision%20problems%20in%20time%20proportional%20to%20log%20%28h%29%201990"
        },
        {
            "id": "Wang_2017_a",
            "entry": "[Wan17] Mengdi Wang. Randomized linear programming solves the discounted Markov decision problem in nearly-linear running time. arXiv preprint arXiv:1704.01869, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01869"
        },
        {
            "id": "Ye_2005_a",
            "entry": "[Ye05] Yinyu Ye. A new complexity result on solving the Markov decision problem. Mathematics of Operations Research, 30(3):733\u2013749, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20Yinyu%20A%20new%20complexity%20result%20on%20solving%20the%20Markov%20decision%20problem%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20Yinyu%20A%20new%20complexity%20result%20on%20solving%20the%20Markov%20decision%20problem%202005"
        },
        {
            "id": "Ye_2011_a",
            "entry": "[Ye11] Yinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate. Mathematics of Operations Research, 36(4):593\u2013603, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20Yinyu%20The%20simplex%20and%20policy-iteration%20methods%20are%20strongly%20polynomial%20for%20the%20Markov%20decision%20problem%20with%20a%20fixed%20discount%20rate%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20Yinyu%20The%20simplex%20and%20policy-iteration%20methods%20are%20strongly%20polynomial%20for%20the%20Markov%20decision%20problem%20with%20a%20fixed%20discount%20rate%202011"
        }
    ]
}
