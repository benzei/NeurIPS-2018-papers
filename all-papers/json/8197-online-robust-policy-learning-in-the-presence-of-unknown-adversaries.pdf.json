{
    "filename": "8197-online-robust-policy-learning-in-the-presence-of-unknown-adversaries.pdf",
    "metadata": {
        "title": "Online Robust Policy Learning in the Presence of Unknown Adversaries",
        "author": "Aaron Havens, Zhanhong Jiang, Soumik Sarkar",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8197-online-robust-policy-learning-in-the-presence-of-unknown-adversaries.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The growing prospect of deep reinforcement learning (DRL) being used in cyberphysical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments."
    },
    "keywords": [
        {
            "term": "policy learning",
            "url": "https://en.wikipedia.org/wiki/policy_learning"
        },
        {
            "term": "expectation maximization",
            "url": "https://en.wikipedia.org/wiki/expectation_maximization"
        },
        {
            "term": "Deep Learning",
            "url": "https://en.wikipedia.org/wiki/Deep_Learning"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "Markov decision processes",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_processes"
        },
        {
            "term": "attack model",
            "url": "https://en.wikipedia.org/wiki/attack_model"
        }
    ],
    "highlights": [
        "Real applications of cyber-physical systems that utilize learning techniques are already abundant such as smart buildings [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], intelligent transportation networks [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], and intelligent surveillance and reconnaissance [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Deep Learning is generally useful for these control problems, Deep Learning has inherent vulnerabilities in the way that even very small perturbations in state inputs can result in significant loss in policy learning performance",
        "We show how reducing bias is inherently beneficial for policy learning (improvement in expected reward lower bound compared to the state-of-the-art as",
        "In order to justify the theoretical implications of bias reduction using a conditioned policy optimization, we implemented the proposed framework introduced in Section 3 with a selection of simple adversary models",
        "PPO parameters used in experiments such as deep network size and actor-batches can be found in the supplementary material",
        "Analyzing the hierarchical policy Meta-Learned Advantage Hierarchy in this way, we can show that under certain conditions, the return lower-bound is improved when compared to a single policy agent"
    ],
    "key_statements": [
        "Real applications of cyber-physical systems that utilize learning techniques are already abundant such as smart buildings [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], intelligent transportation networks [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], and intelligent surveillance and reconnaissance [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Deep Learning is generally useful for these control problems, Deep Learning has inherent vulnerabilities in the way that even very small perturbations in state inputs can result in significant loss in policy learning performance",
        "Previous attempts in mitigating adversarial attacks have been successful against specific attack models, such robust training strategies are typically off-line and may fail to adapt to different attacker strategies in an online fashion",
        "Contributions: In this paper, we consider a policy learning problem where there are periods of adversarial attacks when the agent is continuously learning in its environment",
        "The Reinforcement Learning agent is not aware of the specific mi at time t. This could alternatively be thought of as a partially observable Markov decision processes (POMDP), Figure 1: A meta learning hierarchy simhowever in this work we introduce a hierarchal Reinforcement Learning archiilar to Meta-Learned Shared Hierarchies in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We present analysis to show that the proposed Meta-Learned Advantage Hierarchy framework reduces bias in the value function baseline under adversarial attacks",
        "We show how reducing bias is inherently beneficial for policy learning (improvement in expected reward lower bound compared to the state-of-the-art as",
        "Due to the limit of space, we present the proof sketch and the complete proof can be found in the supplementary material and [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "In order to justify the theoretical implications of bias reduction using a conditioned policy optimization, we implemented the proposed framework introduced in Section 3 with a selection of simple adversary models",
        "Training denotes the agent acting with an -greedy exploration policy with adversarial attacks",
        "We run an evaluation which executes a deterministic actions with the same policy, without adversarial attacks, obtain much higher return values",
        "PPO parameters used in experiments such as deep network size and actor-batches can be found in the supplementary material",
        "Adversarial Bias Reduction with Meta-Learned Advantage Hierarchy We begin by examining an Reinforcement Learning environment where the master agent is asked to select the policy that corresponds to the current condition, i.e., nominal or adversarial",
        "The returns shown in Table 2 and Figure 3 for long and intermittent bias attacks clearly demonstrate the benefit of using distinct policies for nominal and adversarial states respectively",
        "Analyzing the hierarchical policy Meta-Learned Advantage Hierarchy in this way, we can show that under certain conditions, the return lower-bound is improved when compared to a single policy agent"
    ],
    "summary": [
        "Real applications of cyber-physical systems that utilize learning techniques are already abundant such as smart buildings [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], intelligent transportation networks [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], and intelligent surveillance and reconnaissance [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "Algorithm We propose a new hierarchal meta-learning framework, MLAH that can effectively detect and mitigate the impacts of adversarial state information attacks in a attackmodel agnostic manner, using only the advantage observation.",
        "An adversarial attack is any possible state observation perturbation that leads the agent into incurring a sub-optimal return, which is less than the return of the learned optimal policy.",
        "We do acknowledge that this method is slightly delayed such that the agent has to experience an off-trajectory reward before it can detect the adversary presence and may have to observe long attack periods before learning the advantage mapping.",
        "The expected discounted reward conditioned on the nominal state under adversarial attacks can be expressed as: Econ,s\u223cS|0V (s) = V0p0|0 + V1p1|0 = V0m + V1(1 \u2212 m)",
        "We begin with defining the observed bias in the state value for both the conditioned and unconditioned policies by comparing the expected discounted reward to the original nominal value primitive V0.",
        "While Lemma 2 shows that \u03b4 is reduced due to conditioning in our proposed framework, we note that the observed bias in the expected discounted reward can be different from that in the state value due to the complex and uncertain environment.",
        "Proof sketch: Based on Theorem 1 [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and Proposition 1, we get the approximation of the actual expected discounted reward in both conditioned and unconditioned policies.",
        "In order to justify the theoretical implications of bias reduction using a conditioned policy optimization, we implemented the proposed framework introduced in Section 3 with a selection of simple adversary models.",
        "5.2 Adversarial Bias Reduction with MLAH We begin by examining an RL environment where the master agent is asked to select the policy that corresponds to the current condition, i.e., nominal or adversarial.",
        "The returns shown in Table 2 and Figure 3 for long and intermittent bias attacks clearly demonstrate the benefit of using distinct policies for nominal and adversarial states respectively.",
        "Enabling the use of multiple polices in this intermittent attack case allows the agent to optimize for both mappings, even learning to mitigate the reduced return during the adversarial attack.",
        "It can be seen in table 2 that as the switching expectations between nominal and adversarial states rise, the unconditioned (Vanilla) policy performs increasingly well, but still less than that of the conditioned (MLAH) policy.",
        "We will attempt to extend MLAH to a more general framework for decision problems with multiple time-varying objectives"
    ],
    "headline": "This paper introduces a Meta-Learned Advantage Hierarchy  framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space  and directly mitigating learned bias introduced by the adversary",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Chi-Sheng Shih, Jyun-Jhe Chou, Niels Reijers, and Tei-Wei Kuo. Designing cps/iot applications for smart buildings and cities. IET Cyber-Physical Systems: Theory & Applications, 1(1):3\u201312, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shih%2C%20Chi-Sheng%20Chou%2C%20Jyun-Jhe%20Reijers%2C%20Niels%20Kuo%2C%20Tei-Wei%20Designing%20cps/iot%20applications%20for%20smart%20buildings%20and%20cities%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shih%2C%20Chi-Sheng%20Chou%2C%20Jyun-Jhe%20Reijers%2C%20Niels%20Kuo%2C%20Tei-Wei%20Designing%20cps/iot%20applications%20for%20smart%20buildings%20and%20cities%202016"
        },
        {
            "id": "2",
            "entry": "[2] Danda B Rawat, Chandra Bajracharya, and Gongjun Yan. Towards intelligent transportation cyber-physical systems: Real-time computing and communications perspectives. In SoutheastCon 2015, pages 1\u20136. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rawat%2C%20Danda%20B.%20Bajracharya%2C%20Chandra%20Yan%2C%20Gongjun%20Towards%20intelligent%20transportation%20cyber-physical%20systems%3A%20Real-time%20computing%20and%20communications%20perspectives%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rawat%2C%20Danda%20B.%20Bajracharya%2C%20Chandra%20Yan%2C%20Gongjun%20Towards%20intelligent%20transportation%20cyber-physical%20systems%3A%20Real-time%20computing%20and%20communications%20perspectives%202015"
        },
        {
            "id": "3",
            "entry": "[3] Antreas Antoniou and Plamen Angelov. A general purpose intelligent surveillance system for mobile devices using deep learning. In Neural Networks (IJCNN), 2016 International Joint Conference on, pages 2879\u20132886. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Antreas%20Antoniou%20and%20Plamen%20Angelov.%20A%20general%20purpose%20intelligent%20surveillance%20system%20for%20mobile%20devices%20using%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Antreas%20Antoniou%20and%20Plamen%20Angelov.%20A%20general%20purpose%20intelligent%20surveillance%20system%20for%20mobile%20devices%20using%20deep%20learning%202016"
        },
        {
            "id": "4",
            "entry": "[4] Richard S Sutton, Andrew G Barto, and Ronald J Williams. Reinforcement learning is direct adaptive optimal control. IEEE Control Systems, 12(2):19\u201322, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Williams%2C%20Ronald%20J.%20Reinforcement%20learning%20is%20direct%20adaptive%20optimal%20control%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Williams%2C%20Ronald%20J.%20Reinforcement%20learning%20is%20direct%20adaptive%20optimal%20control%201992"
        },
        {
            "id": "5",
            "entry": "[5] RS Sutton and AG Barto. Reinforcement learning: An introduction (in preparation), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%20%28in%20preparation%29%202017"
        },
        {
            "id": "6",
            "entry": "[6] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "7",
            "entry": "[7] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "8",
            "entry": "[8] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In AAAI, volume 16, pages 2094\u20132100, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hasselt%2C%20Hado%20Van%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20q-learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hasselt%2C%20Hado%20Van%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20q-learning%202016"
        },
        {
            "id": "9",
            "entry": "[9] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. ArXiv e-prints, September 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning.%20ArXiv%20e-prints%202015-09"
        },
        {
            "id": "10",
            "entry": "[10] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1\u201340, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "11",
            "entry": "[11] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "12",
            "entry": "[12] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "13",
            "entry": "[13] Y.-C. Lin, M.-Y. Liu, M. Sun, and J.-B. Huang. Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight. ArXiv e-prints, October 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Y.-C.%20Liu%2C%20M.-Y.%20Sun%2C%20M.%20Huang%2C%20J.-B.%20Detecting%20Adversarial%20Attacks%20on%20Neural%20Network%20Policies%20with%20Visual%20Foresight.%20ArXiv%20e-prints%202017-10"
        },
        {
            "id": "14",
            "entry": "[14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016"
        },
        {
            "id": "15",
            "entry": "[15] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary. Robust Deep Reinforcement Learning with Adversarial Attacks. ArXiv e-prints, December 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pattanaik%2C%20A.%20Tang%2C%20Z.%20Liu%2C%20S.%20Bommannan%2C%20G.%20Robust%20Deep%20Reinforcement%20Learning%20with%20Adversarial%20Attacks.%20ArXiv%20e-prints%202017-12"
        },
        {
            "id": "16",
            "entry": "[16] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta. Robust Adversarial Reinforcement Learning. ArXiv e-prints, March 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinto%2C%20L.%20Davidson%2C%20J.%20Sukthankar%2C%20R.%20Gupta%2C%20A.%20Robust%20Adversarial%20Reinforcement%20Learning%202017-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinto%2C%20L.%20Davidson%2C%20J.%20Sukthankar%2C%20R.%20Gupta%2C%20A.%20Robust%20Adversarial%20Reinforcement%20Learning%202017-03"
        },
        {
            "id": "17",
            "entry": "[17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "18",
            "entry": "[18] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. ArXiv e-prints, December 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20Harnessing%20Adversarial%20Examples.%20ArXiv%20e-prints%202014-12"
        },
        {
            "id": "19",
            "entry": "[19] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02284"
        },
        {
            "id": "20",
            "entry": "[20] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint arXiv:1705.06452, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06452"
        },
        {
            "id": "22",
            "entry": "[22] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust policy learning: Active construction of physically-plausible perturbations. In IEEE International Conference on Intelligent Robots and Systems (to appear), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandlekar%2C%20Ajay%20Zhu%2C%20Yuke%20Garg%2C%20Animesh%20Fei-Fei%2C%20Li%20Adversarially%20robust%20policy%20learning%3A%20Active%20construction%20of%20physically-plausible%20perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mandlekar%2C%20Ajay%20Zhu%2C%20Yuke%20Garg%2C%20Animesh%20Fei-Fei%2C%20Li%20Adversarially%20robust%20policy%20learning%3A%20Active%20construction%20of%20physically-plausible%20perturbations%202017"
        },
        {
            "id": "23",
            "entry": "[23] Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.01283"
        },
        {
            "id": "24",
            "entry": "[24] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-Dimensional Continuous Control Using Generalized Advantage Estimation. ArXiv e-prints, June 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Moritz%2C%20P.%20Levine%2C%20S.%20Jordan%2C%20M.%20High-Dimensional%20Continuous%20Control%20Using%20Generalized%20Advantage%20Estimation.%20ArXiv%20e-prints%202015-06"
        },
        {
            "id": "25",
            "entry": "[25] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman. Meta Learning Shared Hierarchies. ArXiv e-prints, October 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frans%2C%20K.%20Ho%2C%20J.%20Chen%2C%20X.%20Abbeel%2C%20P.%20Meta%20Learning%20Shared%20Hierarchies.%20ArXiv%20e-prints%202017-10"
        },
        {
            "id": "26",
            "entry": "[26] Aaron J Havens, Zhanhong Jiang, and Soumik Sarkar. Online robust policy learning in the presence of unknown adversaries. arXiv preprint arXiv:1807.06064, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06064"
        },
        {
            "id": "27",
            "entry": "[27] Emily Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. Bayesian nonparametric inference of switching dynamic linear models. IEEE Transactions on Signal Processing, 59(4):1569\u20131585, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fox%2C%20Emily%20Sudderth%2C%20Erik%20B.%20Jordan%2C%20Michael%20I.%20Willsky%2C%20Alan%20S.%20Bayesian%20nonparametric%20inference%20of%20switching%20dynamic%20linear%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fox%2C%20Emily%20Sudderth%2C%20Erik%20B.%20Jordan%2C%20Michael%20I.%20Willsky%2C%20Alan%20S.%20Bayesian%20nonparametric%20inference%20of%20switching%20dynamic%20linear%20models%202011"
        }
    ]
}
