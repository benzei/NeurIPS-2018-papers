{
    "filename": "7437-neighbourhood-consensus-networks.pdf",
    "metadata": {
        "title": "Neighbourhood Consensus Networks",
        "author": "Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi\u0107, Akihiko Torii, Tomas Pajdla, Josef Sivic",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7437-neighbourhood-consensus-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints, we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model. Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences. Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both categoryand instance-level matching, obtaining the state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark."
    },
    "keywords": [
        {
            "term": "optical flow",
            "url": "https://en.wikipedia.org/wiki/optical_flow"
        },
        {
            "term": "geometric model",
            "url": "https://en.wikipedia.org/wiki/geometric_model"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        }
    ],
    "highlights": [
        "Finding visual correspondences is one of the fundamental image understanding problems with applications in 3D reconstruction [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], visual localization [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] or object recognition [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "We develop a neighbourhood consensus network \u2013 a convolutional neural network architecture for dense matching that learns local geometric constraints between neighbouring correspondences without the need for a global geometric model",
        "In order to further process and filter the matches, we propose to use 4-D convolutional neural network (CNN) for the neighbourhood consensus task), which is illustrated in Fig. 2",
        "Denoted DensePE+neighbourhood consensus network, the DensePE [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] method is used for generating candidate image pairs, and our network (NC-Net) is used to produce the correspondences that are employed for pose estimation",
        "Results are summarised in Table 2 and clearly demonstrate benefits of our approach (DensePE+neighbourhood consensus network) compared to both sparse keypoint (DoG+SIFT) matching (SparsePE) and the convolutional neural networks feature matching used in [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] (DensePE)",
        "We have developed a neighbourhood consensus network \u2014 a convolutional neural networks architecture that learns local patterns of correspondences for image matching without the need for a global geometric model"
    ],
    "key_statements": [
        "Finding visual correspondences is one of the fundamental image understanding problems with applications in 3D reconstruction [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], visual localization [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] or object recognition [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "We develop a neighbourhood consensus network \u2013 a convolutional neural network architecture for dense matching that learns local geometric constraints between neighbouring correspondences without the need for a global geometric model",
        "In order to further process and filter the matches, we propose to use 4-D convolutional neural network (CNN) for the neighbourhood consensus task), which is illustrated in Fig. 2",
        "Denoted DensePE+neighbourhood consensus network, the DensePE [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] method is used for generating candidate image pairs, and our network (NC-Net) is used to produce the correspondences that are employed for pose estimation",
        "Results are summarised in Table 2 and clearly demonstrate benefits of our approach (DensePE+neighbourhood consensus network) compared to both sparse keypoint (DoG+SIFT) matching (SparsePE) and the convolutional neural networks feature matching used in [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] (DensePE)",
        "We have developed a neighbourhood consensus network \u2014 a convolutional neural networks architecture that learns local patterns of correspondences for image matching without the need for a global geometric model"
    ],
    "summary": [
        "Finding visual correspondences is one of the fundamental image understanding problems with applications in 3D reconstruction [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], visual localization [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] or object recognition [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "In this work we take a different direction and develop a trainable neural network architecture that disambiguates such challenging situations by analyzing local neighbourhood patterns in a full set of dense correspondences.",
        "We perform neighbourhood consensus before hard assignment of feature correspondence; that is, on the complete set of dense pair-wise matches.",
        "We develop a neighbourhood consensus network \u2013 a convolutional neural network architecture for dense matching that learns local geometric constraints between neighbouring correspondences without the need for a global geometric model.",
        "Inspired by this simple yet powerful idea we develop a neighbourhood consensus network \u2013 a convolutional neural architecture that (i) analyzes the full set of dense matches between a pair of images and learns patterns of locally consistent correspondences directly from data.",
        "There are five main components: (i) dense feature extraction and matching, the neighbourhood consensus network, a soft mutual nearest neighbour filtering, extraction of correspondences from the output 4D filtered match tensor, and (v) weakly supervised training loss.",
        "Given two sets of dense feature descriptors f A = {fiAj } and f B = {fiBj } corresponding to the images to be matched, the exhaustive pairwise cosine similarities between them are computed and stored in a 4-D tensor c \u2208 Rh\u00d7w\u00d7h\u00d7w referred to as correlation map, where: cijkl =",
        "In order to further process and filter the matches, we propose to use 4-D convolutional neural network (CNN) for the neighbourhood consensus task), which is illustrated in Fig. 2.",
        "The convolutional filters of the first layer of the proposed CNN span a local 4-D region of the matches space, which corresponds to the Cartesian product of local neighbourhoods NA and NB in each image, respectively.",
        "The soft mutual nearest neighbour filtering is used to filter both the correlation map, as well as the output of the neighbourhood consensus CNN, as illustrated in Fig. 1.",
        "The model is subsequently finetuned for 5 more epochs, training both the feature extraction and the neighbourhood consensus network, with a learning rate of 1 \u00d7 10\u22125.",
        "We have developed a neighbourhood consensus network \u2014 a CNN architecture that learns local patterns of correspondences for image matching without the need for a global geometric model.",
        "These results open up the possibility for end-to-end learning of other challenging visual correspondence tasks, such as 3D category-level matching [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], or visual localization across day/night illumination [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]"
    ],
    "headline": "We address the problem of finding reliable dense correspondences between a pair of images",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Project webpage (code/networks). http://www.di.ens.fr/willow/research/ncnet/.",
            "url": "http://www.di.ens.fr/willow/research/ncnet/"
        },
        {
            "id": "2",
            "entry": "[2] S. Agarwal, Y. Furukawa, N. Snavely, I. Simon, B. Curless, S. M. Seitz, and R. Szeliski. Building rome in a day. Communications of the ACM, 54(10):105\u2013112, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20S.%20Furukawa%2C%20Y.%20Snavely%2C%20N.%20Simon%2C%20I.%20Building%20rome%20in%20a%20day%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20S.%20Furukawa%2C%20Y.%20Snavely%2C%20N.%20Simon%2C%20I.%20Building%20rome%20in%20a%20day%202011"
        },
        {
            "id": "3",
            "entry": "[3] V. Balntas, E. Johns, L. Tang, and K. Mikolajczyk. PN-Net: Conjoined triple deep network for learning local image descriptors. arXiv preprint arXiv:1601.05030, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.05030"
        },
        {
            "id": "4",
            "entry": "[4] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learning local feature descriptors with triplets and shallow convolutional neural networks. In Proc. BMVC., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balntas%2C%20V.%20Riba%2C%20E.%20Ponsa%2C%20D.%20Mikolajczyk%2C%20K.%20Learning%20local%20feature%20descriptors%20with%20triplets%20and%20shallow%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balntas%2C%20V.%20Riba%2C%20E.%20Ponsa%2C%20D.%20Mikolajczyk%2C%20K.%20Learning%20local%20feature%20descriptors%20with%20triplets%20and%20shallow%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "5",
            "entry": "[5] J. Bian, W.-Y. Lin, Y. Matsushita, S.-K. Yeung, T.-D. Nguyen, and M.-M. Cheng. GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence. In Proc. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bian%2C%20J.%20Lin%2C%20W.-Y.%20Matsushita%2C%20Y.%20Yeung%2C%20S.-K.%20GMS%3A%20Grid-based%20motion%20statistics%20for%20fast%2C%20ultra-robust%20feature%20correspondence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bian%2C%20J.%20Lin%2C%20W.-Y.%20Matsushita%2C%20Y.%20Yeung%2C%20S.-K.%20GMS%3A%20Grid-based%20motion%20statistics%20for%20fast%2C%20ultra-robust%20feature%20correspondence%202017"
        },
        {
            "id": "6",
            "entry": "[6] T. Brox and J. Malik. Large displacement optical flow: descriptor matching in variational motion estimation. IEEE PAMI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brox%2C%20T.%20Malik%2C%20J.%20Large%20displacement%20optical%20flow%3A%20descriptor%20matching%20in%20variational%20motion%20estimation%202011"
        },
        {
            "id": "7",
            "entry": "[7] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker. Universal correspondence network. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choy%2C%20C.B.%20Gwak%2C%20J.%20Savarese%2C%20S.%20Chandraker%2C%20M.%20Universal%20correspondence%20network%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choy%2C%20C.B.%20Gwak%2C%20J.%20Savarese%2C%20S.%20Chandraker%2C%20M.%20Universal%20correspondence%20network%202016"
        },
        {
            "id": "8",
            "entry": "[8] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox. FlowNet: Learning optical flow with convolutional networks. In Proc. ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20A.%20Fischer%2C%20P.%20Ilg%2C%20E.%20Hausser%2C%20P.%20FlowNet%3A%20Learning%20optical%20flow%20with%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dosovitskiy%2C%20A.%20Fischer%2C%20P.%20Ilg%2C%20E.%20Hausser%2C%20P.%20FlowNet%3A%20Learning%20optical%20flow%20with%20convolutional%20networks%202015"
        },
        {
            "id": "9",
            "entry": "[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results. http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html.",
            "url": "http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Everingham%2C%20M.%20Gool%2C%20L.Van%20Williams%2C%20C.K.I.%20Winn%2C%20J.%20The%20PASCAL%20Visual%20Object%20Classes%20Challenge%202011"
        },
        {
            "id": "10",
            "entry": "[10] P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor matching with convolutional neural networks: a comparison to SIFT. arXiv preprint arXiv:1405.5769, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1405.5769"
        },
        {
            "id": "11",
            "entry": "[11] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal flow: Semantic correspondences from object proposals. IEEE PAMI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ham%2C%20B.%20Cho%2C%20M.%20Schmid%2C%20C.%20Ponce%2C%20J.%20Proposal%20flow%3A%20Semantic%20correspondences%20from%20object%20proposals%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ham%2C%20B.%20Cho%2C%20M.%20Schmid%2C%20C.%20Ponce%2C%20J.%20Proposal%20flow%3A%20Semantic%20correspondences%20from%20object%20proposals%202017"
        },
        {
            "id": "12",
            "entry": "[12] K. Han, R. S. Rezende, B. Ham, K.-Y. K. Wong, M. Cho, C. Schmid, and J. Ponce. SCNet: Learning Semantic Correspondence. In Proc. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20K.%20Rezende%2C%20R.S.%20Ham%2C%20B.%20Wong%2C%20K.-Y.K.%20Ponce.%20SCNet%3A%20Learning%20Semantic%20Correspondence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20K.%20Rezende%2C%20R.S.%20Ham%2C%20B.%20Wong%2C%20K.-Y.K.%20Ponce.%20SCNet%3A%20Learning%20Semantic%20Correspondence%202017"
        },
        {
            "id": "13",
            "entry": "[13] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. MatchNet: Unifying feature and metric learning for patch-based matching. In Proc. CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20X.%20Leung%2C%20T.%20Jia%2C%20Y.%20Sukthankar%2C%20R.%20MatchNet%3A%20Unifying%20feature%20and%20metric%20learning%20for%20patch-based%20matching%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20X.%20Leung%2C%20T.%20Jia%2C%20Y.%20Sukthankar%2C%20R.%20MatchNet%3A%20Unifying%20feature%20and%20metric%20learning%20for%20patch-based%20matching%202015"
        },
        {
            "id": "14",
            "entry": "[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "15",
            "entry": "[15] H. Hirschm\u00fcller. Stereo processing by semiglobal matching and mutual information. IEEE PAMI, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hirschm%C3%BCller%2C%20H.%20Stereo%20processing%20by%20semiglobal%20matching%20and%20mutual%20information%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hirschm%C3%BCller%2C%20H.%20Stereo%20processing%20by%20semiglobal%20matching%20and%20mutual%20information%202008"
        },
        {
            "id": "16",
            "entry": "[16] B. K. Horn and B. G. Schunck. Determining optical flow. Artificial Intelligence, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horn%2C%20B.K.%20Schunck%2C%20B.G.%20Determining%20optical%20flow%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Horn%2C%20B.K.%20Schunck%2C%20B.G.%20Determining%20optical%20flow%201981"
        },
        {
            "id": "17",
            "entry": "[17] M. Jahrer, M. Grabner, and H. Bischof. Learned local descriptors for recognition and matching. In Computer Vision Winter Workshop, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jahrer%2C%20M.%20Grabner%2C%20M.%20Bischof%2C%20H.%20Learned%20local%20descriptors%20for%20recognition%20and%20matching%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jahrer%2C%20M.%20Grabner%2C%20M.%20Bischof%2C%20H.%20Learned%20local%20descriptors%20for%20recognition%20and%20matching%202008"
        },
        {
            "id": "18",
            "entry": "[18] A. Kanazawa, S. Tulsiani, A. A. Efros, and J. Malik. Learning category-specific mesh reconstruction from image collections. In Proc. ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kanazawa%2C%20A.%20Tulsiani%2C%20S.%20Efros%2C%20A.A.%20Malik%2C%20J.%20Learning%20category-specific%20mesh%20reconstruction%20from%20image%20collections%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kanazawa%2C%20A.%20Tulsiani%2C%20S.%20Efros%2C%20A.A.%20Malik%2C%20J.%20Learning%20category-specific%20mesh%20reconstruction%20from%20image%20collections%202018"
        },
        {
            "id": "19",
            "entry": "[19] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach, and A. Bry. End-to-end learning of geometry and context for deep stereo regression. In Proc. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20A.%20Martirosyan%2C%20H.%20Dasgupta%2C%20S.%20Henry%2C%20P.%20End-to-end%20learning%20of%20geometry%20and%20context%20for%20deep%20stereo%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20A.%20Martirosyan%2C%20H.%20Dasgupta%2C%20S.%20Henry%2C%20P.%20End-to-end%20learning%20of%20geometry%20and%20context%20for%20deep%20stereo%20regression%202017"
        },
        {
            "id": "20",
            "entry": "[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "21",
            "entry": "[21] C. Liu, J. Yuen, A. Torralba, J. Sivic, and W. T. Freeman. Sift-flow: Dense correspondence across different scenes. In Proc. ECCV, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20C.%20Yuen%2C%20J.%20Torralba%2C%20A.%20Sivic%2C%20J.%20Sift-flow%3A%20Dense%20correspondence%20across%20different%20scenes%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20C.%20Yuen%2C%20J.%20Torralba%2C%20A.%20Sivic%2C%20J.%20Sift-flow%3A%20Dense%20correspondence%20across%20different%20scenes%202008"
        },
        {
            "id": "22",
            "entry": "[22] J. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence? In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20J.%20Zhang%2C%20N.%20Darrell%2C%20T.%20Do%20convnets%20learn%20correspondence%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20J.%20Zhang%2C%20N.%20Darrell%2C%20T.%20Do%20convnets%20learn%20correspondence%3F%202014"
        },
        {
            "id": "23",
            "entry": "[23] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lowe%2C%20D.G.%20Distinctive%20image%20features%20from%20scale-invariant%20keypoints%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowe%2C%20D.G.%20Distinctive%20image%20features%20from%20scale-invariant%20keypoints%202004"
        },
        {
            "id": "24",
            "entry": "[24] B. D. Lucas, T. Kanade, et al. An iterative image registration technique with an application to stereo vision. IJCAI, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucas%2C%20B.D.%20Kanade%2C%20T.%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucas%2C%20B.D.%20Kanade%2C%20T.%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981"
        },
        {
            "id": "25",
            "entry": "[25] K. Mikolajczyk and C. Schmid. An affine invariant interest point detector. In Proc. ECCV, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolajczyk%2C%20K.%20Schmid%2C%20C.%20An%20affine%20invariant%20interest%20point%20detector%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolajczyk%2C%20K.%20Schmid%2C%20C.%20An%20affine%20invariant%20interest%20point%20detector%202002"
        },
        {
            "id": "26",
            "entry": "[26] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han. Large-scale image retrieval with attentive deep local features. In Proc. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noh%2C%20H.%20Araujo%2C%20A.%20Sim%2C%20J.%20Weyand%2C%20T.%20Large-scale%20image%20retrieval%20with%20attentive%20deep%20local%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noh%2C%20H.%20Araujo%2C%20A.%20Sim%2C%20J.%20Weyand%2C%20T.%20Large-scale%20image%20retrieval%20with%20attentive%20deep%20local%20features%202017"
        },
        {
            "id": "27",
            "entry": "[27] A. Paszke, S. Gross, S. Chintala, and G. Chanan. PyTorch. http://pytorch.org/.",
            "url": "http://pytorch.org/"
        },
        {
            "id": "28",
            "entry": "[28] I. Rocco, R. Arandjelovic, and J. Sivic. Convolutional neural network architecture for geometric matching. In Proc. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rocco%2C%20I.%20Arandjelovic%2C%20R.%20Sivic%2C%20J.%20Convolutional%20neural%20network%20architecture%20for%20geometric%20matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rocco%2C%20I.%20Arandjelovic%2C%20R.%20Sivic%2C%20J.%20Convolutional%20neural%20network%20architecture%20for%20geometric%20matching%202017"
        },
        {
            "id": "29",
            "entry": "[29] I. Rocco, R. Arandjelovic, and J. Sivic. End-to-end weakly-supervised semantic alignment. In Proc. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rocco%2C%20I.%20Arandjelovic%2C%20R.%20Sivic%2C%20J.%20End-to-end%20weakly-supervised%20semantic%20alignment%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rocco%2C%20I.%20Arandjelovic%2C%20R.%20Sivic%2C%20J.%20End-to-end%20weakly-supervised%20semantic%20alignment%202018"
        },
        {
            "id": "30",
            "entry": "[30] I. Rocco, M. Cimpoi, R. Arandjelovic, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus networks. arXiv preprint arXiv:1810.10510, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.10510"
        },
        {
            "id": "31",
            "entry": "[31] T. Sattler, B. Leibe, and L. Kobbelt. SCRAMSAC: Improving RANSAC\u2018s Efficiency with a Spatial Consistency Filter. In Proc. ICCV, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sattler%2C%20T.%20Leibe%2C%20B.%20Kobbelt%2C%20L.%20SCRAMSAC%3A%20Improving%20RANSAC%E2%80%98s%20Efficiency%20with%20a%20Spatial%20Consistency%20Filter%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sattler%2C%20T.%20Leibe%2C%20B.%20Kobbelt%2C%20L.%20SCRAMSAC%3A%20Improving%20RANSAC%E2%80%98s%20Efficiency%20with%20a%20Spatial%20Consistency%20Filter%202009"
        },
        {
            "id": "32",
            "entry": "[32] T. Sattler, W. Maddern, A. Torii, J. Sivic, T. Pajdla, M. Pollefeys, and M. Okutomi. Benchmarking 6dof urban visual localization in changing conditions. In Proc. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sattler%2C%20T.%20Maddern%2C%20W.%20Torii%2C%20A.%20Sivic%2C%20J.%20Benchmarking%206dof%20urban%20visual%20localization%20in%20changing%20conditions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sattler%2C%20T.%20Maddern%2C%20W.%20Torii%2C%20A.%20Sivic%2C%20J.%20Benchmarking%206dof%20urban%20visual%20localization%20in%20changing%20conditions%202018"
        },
        {
            "id": "33",
            "entry": "[33] N. Savinov, L. Ladicky, and M. Pollefeys. Matching neural paths: transfer from recognition to correspondence search. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Savinov%2C%20N.%20Ladicky%2C%20L.%20M.%20Pollefeys.%20Matching%20neural%20paths%3A%20transfer%20from%20recognition%20to%20correspondence%20search%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Savinov%2C%20N.%20Ladicky%2C%20L.%20M.%20Pollefeys.%20Matching%20neural%20paths%3A%20transfer%20from%20recognition%20to%20correspondence%20search%202017"
        },
        {
            "id": "34",
            "entry": "[34] F. Schaffalitzky and A. Zisserman. Automated scene matching in movies. In International Conference on Image and Video Retrieval, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaffalitzky%2C%20F.%20Zisserman%2C%20A.%20Automated%20scene%20matching%20in%20movies%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schaffalitzky%2C%20F.%20Zisserman%2C%20A.%20Automated%20scene%20matching%20in%20movies%202002"
        },
        {
            "id": "35",
            "entry": "[35] C. Schmid and R. Mohr. Local grayvalue invariants for image retrieval. IEEE PAMI, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmid%2C%20C.%20Mohr%2C%20R.%20Local%20grayvalue%20invariants%20for%20image%20retrieval%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmid%2C%20C.%20Mohr%2C%20R.%20Local%20grayvalue%20invariants%20for%20image%20retrieval%201997"
        },
        {
            "id": "36",
            "entry": "[36] J. L. Schonberger, H. Hardmeier, T. Sattler, and M. Pollefeys. Comparative evaluation of hand-crafted and learned local features. In Proc. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schonberger%2C%20J.L.%20Hardmeier%2C%20H.%20Sattler%2C%20T.%20M.%20Pollefeys.%20Comparative%20evaluation%20of%20hand-crafted%20and%20learned%20local%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schonberger%2C%20J.L.%20Hardmeier%2C%20H.%20Sattler%2C%20T.%20M.%20Pollefeys.%20Comparative%20evaluation%20of%20hand-crafted%20and%20learned%20local%20features%202017"
        },
        {
            "id": "37",
            "entry": "[37] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative learning of deep convolutional feature point descriptors. In Proc. ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simo-Serra%2C%20E.%20Trulls%2C%20E.%20Ferraz%2C%20L.%20Kokkinos%2C%20I.%20Discriminative%20learning%20of%20deep%20convolutional%20feature%20point%20descriptors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simo-Serra%2C%20E.%20Trulls%2C%20E.%20Ferraz%2C%20L.%20Kokkinos%2C%20I.%20Discriminative%20learning%20of%20deep%20convolutional%20feature%20point%20descriptors%202015"
        },
        {
            "id": "38",
            "entry": "[38] K. Simonyan, A. Vedaldi, and A. Zisserman. Learning local feature descriptors using convex optimisation. IEEE PAMI, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Vedaldi%2C%20A.%20Zisserman%2C%20A.%20Learning%20local%20feature%20descriptors%20using%20convex%20optimisation%202014"
        },
        {
            "id": "39",
            "entry": "[39] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In Proc. ICCV, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sivic%2C%20J.%20Zisserman%2C%20A.%20Video%20Google%3A%20A%20text%20retrieval%20approach%20to%20object%20matching%20in%20videos%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sivic%2C%20J.%20Zisserman%2C%20A.%20Video%20Google%3A%20A%20text%20retrieval%20approach%20to%20object%20matching%20in%20videos%202003"
        },
        {
            "id": "40",
            "entry": "[40] D. Sun, S. Roth, and M. J. Black. Secrets of optical flow estimation and their principles. In Proc. CVPR, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20D.%20Roth%2C%20S.%20Black%2C%20M.J.%20Secrets%20of%20optical%20flow%20estimation%20and%20their%20principles%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20D.%20Roth%2C%20S.%20Black%2C%20M.J.%20Secrets%20of%20optical%20flow%20estimation%20and%20their%20principles%202010"
        },
        {
            "id": "41",
            "entry": "[41] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In Proc. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20D.%20Yang%2C%20X.%20Liu%2C%20M.-Y.%20Kautz%2C%20J.%20PWC-Net%3A%20CNNs%20for%20optical%20flow%20using%20pyramid%2C%20warping%2C%20and%20cost%20volume%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20D.%20Yang%2C%20X.%20Liu%2C%20M.-Y.%20Kautz%2C%20J.%20PWC-Net%3A%20CNNs%20for%20optical%20flow%20using%20pyramid%2C%20warping%2C%20and%20cost%20volume%202018"
        },
        {
            "id": "42",
            "entry": "[42] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic, T. Pajdla, and A. Torii. InLoc: Indoor visual localization with dense matching and view synthesis. In Proc. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taira%2C%20H.%20Okutomi%2C%20M.%20Sattler%2C%20T.%20Cimpoi%2C%20M.%20InLoc%3A%20Indoor%20visual%20localization%20with%20dense%20matching%20and%20view%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taira%2C%20H.%20Okutomi%2C%20M.%20Sattler%2C%20T.%20Cimpoi%2C%20M.%20InLoc%3A%20Indoor%20visual%20localization%20with%20dense%20matching%20and%20view%20synthesis%202018"
        },
        {
            "id": "43",
            "entry": "[43] T. Tuytelaars and K. Mikolajczyk. Local invariant feature detectors: A survey. Foundations and Trends in Computer Graphics and Vision, 3(3):177\u2013280, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tuytelaars%2C%20T.%20Mikolajczyk%2C%20K.%20Local%20invariant%20feature%20detectors%3A%20A%20survey.%20Foundations%20and%20Trends%20in%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tuytelaars%2C%20T.%20Mikolajczyk%2C%20K.%20Local%20invariant%20feature%20detectors%3A%20A%20survey.%20Foundations%20and%20Trends%20in%202008"
        },
        {
            "id": "44",
            "entry": "[44] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. LIFT: Learned invariant feature transform. In Proc. ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20K.M.%20Trulls%2C%20E.%20Lepetit%2C%20V.%20Fua%2C%20P.%20LIFT%3A%20Learned%20invariant%20feature%20transform%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yi%2C%20K.M.%20Trulls%2C%20E.%20Lepetit%2C%20V.%20Fua%2C%20P.%20LIFT%3A%20Learned%20invariant%20feature%20transform%202016"
        },
        {
            "id": "45",
            "entry": "[45] S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. In Proc. CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Learning%20to%20compare%20image%20patches%20via%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Learning%20to%20compare%20image%20patches%20via%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "46",
            "entry": "[46] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Proc. ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20M.D.%20Fergus%2C%20R.%20Visualizing%20and%20understanding%20convolutional%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeiler%2C%20M.D.%20Fergus%2C%20R.%20Visualizing%20and%20understanding%20convolutional%20networks%202014"
        },
        {
            "id": "47",
            "entry": "[47] Z. Zhang, R. Deriche, O. Faugeras, and Q.-T. Luong. A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry. Artificial intelligence, 1995. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Z.%20Deriche%2C%20R.%20Faugeras%2C%20O.%20Luong%2C%20Q.-T.%20A%20robust%20technique%20for%20matching%20two%20uncalibrated%20images%20through%20the%20recovery%20of%20the%20unknown%20epipolar%20geometry%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Z.%20Deriche%2C%20R.%20Faugeras%2C%20O.%20Luong%2C%20Q.-T.%20A%20robust%20technique%20for%20matching%20two%20uncalibrated%20images%20through%20the%20recovery%20of%20the%20unknown%20epipolar%20geometry%201995"
        }
    ]
}
