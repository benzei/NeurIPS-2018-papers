{
    "filename": "7990-improved-algorithms-for-collaborative-pac-learning.pdf",
    "metadata": {
        "title": "Improved Algorithms for Collaborative PAC Learning",
        "author": "Huy Nguyen, Lydia Zakynthinou",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7990-improved-algorithms-for-collaborative-pac-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study a recent model of collaborative PAC learning where k players with k different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small error on all tasks, there is a collaborative algorithm that finds a single classifier for all tasks and has O((ln(k))2) times the worst-case sample complexity for learning a single task. In this work, we design new algorithms for both the realizable and the non-realizable setting, having sample complexity only O(ln(k)) times the worst-case sample complexity for learning a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters are even better than previous algorithms that are allowed to output different classifiers for different tasks."
    },
    "keywords": [
        "multiple task",
        "multi task learning",
        "collaborative pac learning",
        "bad case sample complexity",
        "single classifier",
        "sample complexity upper bound",
        "non realizable setting",
        "different task",
        "(r) \u2190 OF",
        "single task"
    ],
    "highlights": [
        "There has been a lot of work in machine learning concerning learning multiple tasks simultaneously, ranging from multi-task learning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], to domain adaptation [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], to distributed learning [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]",
        "Another area in similar spirit to this work is meta-learning, where one leverages samples from many different tasks to train a single algorithm that adapts well to all tasks",
        "We focus on a model of collaborative PAC learning, proposed by [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "Our sample complexity upper bounds are summarized in the following table"
    ],
    "key_statements": [
        "There has been a lot of work in machine learning concerning learning multiple tasks simultaneously, ranging from multi-task learning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], to domain adaptation [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], to distributed learning [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]",
        "Another area in similar spirit to this work is meta-learning, where one leverages samples from many different tasks to train a single algorithm that adapts well to all tasks",
        "We focus on a model of collaborative PAC learning, proposed by [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "Our sample complexity upper bounds are summarized in the following table"
    ],
    "summary": [
        "There has been a lot of work in machine learning concerning learning multiple tasks simultaneously, ranging from multi-task learning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], to domain adaptation [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], to distributed learning [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "These algorithms learn a classifier with error at most (2 + \u03b1)OPT + on all the tasks, where \u03b1 is set to a constant value, and have sample complexities O",
        "For any , \u03b4 \u2208 (0, 1), and hypothesis class F of VC dimension d, Algorithm R2 returns a classifier fR2 with errDi \u2264 \u2200i \u2208 [k] with probability at least 1 \u2212 \u03b4 using m samples, where",
        "We prove that Algorithm R2 learns a good classifier, meaning that, with probability at least 1 \u2212 \u03b4, for every player i \u2208 [k] the returned classifier fR2 has error errDi \u2264 .",
        "For any , \u03b4 \u2208 (0, 1), 5 /4 < \u03b1 < 1, and hypothesis class F of VC dimension d, Algorithm NR2 returns a classifier fNR2 such that errDi \u2264 (2 + \u03b1)OPT + holds for all i \u2208 [k] with probability 1 \u2212 \u03b4 using m samples, where",
        "NR1-AVG and NR2-AVG that return a classifier which satisfies this form of guarantee on the error without the 2-approximation factor but use roughly \u03b1 times more samples.",
        "For any , \u03b4 \u2208 (0, 1), 30 /29 < \u03b1 < 1, and hypothesis class F of VC dimension d, Algorithm NR2-AVG returns a classifier fNR2-AVG such that for the expected error errDi \u2264 (1 + \u03b1)OPT + holds for all i \u2208 [k] with probability 1 \u2212 \u03b4 using m samples, where k m = O \u03b13 2 ln \u03b4 (d + k) ln",
        "In the non-realizable setting, our generalization of algorithms R1 and R2, NR1 and NR2 respectively, have the same sample complexity as in the realizable setting and match the error guarantee for OPT = 0."
    ],
    "headline": "We study a recent model of collaborative PAC learning where k players with k different tasks collaborate to learn a single classifier that works for all tasks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, New York, NY, USA, 1st edition, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20Network%20Learning%3A%20Theoretical%20Foundations%202009"
        },
        {
            "id": "2",
            "entry": "[2] Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, communication complexity and privacy. In Proceedings of the 25th Conference on Computational Learning Theory (COLT), pages 26.1\u201326.22, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20Maria-Florina%20Blum%2C%20Avrim%20Fine%2C%20Shai%20Mansour%2C%20Yishay%20Distributed%20learning%2C%20communication%20complexity%20and%20privacy%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20Maria-Florina%20Blum%2C%20Avrim%20Fine%2C%20Shai%20Mansour%2C%20Yishay%20Distributed%20learning%2C%20communication%20complexity%20and%20privacy%202012"
        },
        {
            "id": "3",
            "entry": "[3] Jonathan Baxter. A Bayesian/information theoretic model of learning to learn via multiple task sampling. Machine Learning, 28(1):7\u201339, July 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baxter%2C%20Jonathan%20A%20Bayesian/information%20theoretic%20model%20of%20learning%20to%20learn%20via%20multiple%20task%20sampling%201997-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baxter%2C%20Jonathan%20A%20Bayesian/information%20theoretic%20model%20of%20learning%20to%20learn%20via%20multiple%20task%20sampling%201997-07"
        },
        {
            "id": "4",
            "entry": "[4] Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12(1):149\u2013198, March 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baxter%2C%20Jonathan%20A%20model%20of%20inductive%20bias%20learning%202000-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baxter%2C%20Jonathan%20A%20model%20of%20inductive%20bias%20learning%202000-03"
        },
        {
            "id": "5",
            "entry": "[5] Avrim Blum, Nika Haghtalab, Ariel D. Procaccia, and Mingda Qiao. Collaborative PAC learning. In Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NIPS), pages 2389\u20132398, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Haghtalab%2C%20Nika%20Procaccia%2C%20Ariel%20D.%20Qiao%2C%20Mingda%20Collaborative%20PAC%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Haghtalab%2C%20Nika%20Procaccia%2C%20Ariel%20D.%20Qiao%2C%20Mingda%20Collaborative%20PAC%20learning%202017"
        },
        {
            "id": "6",
            "entry": "[6] Jiecao Chen, Qin Zhang, and Yuan Zhou. Tight bounds for collaborative PAC learning via multiplicative weights, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jiecao%20Zhang%2C%20Qin%20Zhou%2C%20Yuan%20Tight%20bounds%20for%20collaborative%20PAC%20learning%20via%20multiplicative%20weights%202018"
        },
        {
            "id": "7",
            "entry": "[7] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 713\u2013720, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%202011"
        },
        {
            "id": "8",
            "entry": "[8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 1126\u20131135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "9",
            "entry": "[9] Christos Koufogiannakis and Neal E. Young. A nearly linear-time PTAS for explicit fractional packing and covering linear programs. Algorithmica, 70(4):648\u2013674, December 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koufogiannakis%2C%20Christos%20Young%2C%20Neal%20E.%20A%20nearly%20linear-time%20PTAS%20for%20explicit%20fractional%20packing%20and%20covering%20linear%20programs%202014-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koufogiannakis%2C%20Christos%20Young%2C%20Neal%20E.%20A%20nearly%20linear-time%20PTAS%20for%20explicit%20fractional%20packing%20and%20covering%20linear%20programs%202014-12"
        },
        {
            "id": "10",
            "entry": "[10] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In Proceedings of the 22nd Conference on Computational Learning Theory (COLT), pages 19\u201330, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%3A%20Learning%20bounds%20and%20algorithms%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%3A%20Learning%20bounds%20and%20algorithms%202009"
        },
        {
            "id": "11",
            "entry": "[11] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS), pages 1041\u20131048, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%20with%20multiple%20sources%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%20with%20multiple%20sources%202009"
        },
        {
            "id": "12",
            "entry": "[12] Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomization and Probabilistic Techniques in Algorithms and Data Analysis. Cambridge University Press, 2nd edition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitzenmacher%2C%20Michael%20Upfal%2C%20Eli%20Probability%20and%20Computing%3A%20Randomization%20and%20Probabilistic%20Techniques%20in%20Algorithms%20and%20Data%20Analysis%202017"
        },
        {
            "id": "13",
            "entry": "[13] L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134\u20131142, November 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valiant%2C%20L.G.%20A%20theory%20of%20the%20learnable%201984-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valiant%2C%20L.G.%20A%20theory%20of%20the%20learnable%201984-11"
        },
        {
            "id": "14",
            "entry": "[14] Jialei Wang, Mladen Kolar, and Nathan Srebro. Distributed multi-task learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 751\u2013760, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jialei%20Kolar%2C%20Mladen%20Srebro%2C%20Nathan%20Distributed%20multi-task%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jialei%20Kolar%2C%20Mladen%20Srebro%2C%20Nathan%20Distributed%20multi-task%20learning%202016"
        }
    ]
}
