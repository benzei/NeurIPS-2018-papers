{
    "filename": "8038-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-data.pdf",
    "metadata": {
        "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data",
        "author": "Yuanzhi Li, Yingyu Liang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8038-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-data.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset."
    },
    "keywords": [
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "structured data",
            "url": "https://en.wikipedia.org/wiki/structured_data"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Neural networks have achieved great success in many applications, but despite a recent increase of theoretical studies, much remains to be explained",
        "Some recent studies use the low complexity of the learned solution to explain the generalization, but usually do not explain how the stochastic gradient descent or its variants favors low complexity solutions [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We prove that when the network is sufficiently overparameterized, stochastic gradient descent provably learns a network close to the random initialization and with a small generalization error",
        "Our work makes a step towrads explaining how overparameterization and random initialization help optimization, and how the inductive bias and good generalization arise from the stochastic gradient descent dynamics on structured data",
        "This work studied the problem of learning a two-layer overparameterized ReLU neural network via stochastic gradient descent (SGD) from random initialization, on data with structure inspired by practical datasets",
        "While our work makes a step towards theoretical understanding of stochastic gradient descent for training neural networs, it is far from being conclusive"
    ],
    "key_statements": [
        "Neural networks have achieved great success in many applications, but despite a recent increase of theoretical studies, much remains to be explained",
        "Some recent studies use the low complexity of the learned solution to explain the generalization, but usually do not explain how the stochastic gradient descent or its variants favors low complexity solutions [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "On the dataset MNIST [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], each class corresponds to a digit and can have several components corresponding to different writing styles of the digit, and an image in it is a small perturbation of one of the components",
        "We prove that when the network is sufficiently overparameterized, stochastic gradient descent provably learns a network close to the random initialization and with a small generalization error",
        "Our work makes a step towrads explaining how overparameterization and random initialization help optimization, and how the inductive bias and good generalization arise from the stochastic gradient descent dynamics on structured data",
        "We have shown that in a neighborhood of the random initialization, w.h.p. the gradients are similar to those of another benign learning process, and stochastic gradient descent can reduce the error and reach a good solution while still in the neighborhood",
        "Existing work has analyzed the implicit regularization of stochastic gradient descent on logistic regression [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], matrix factorization [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], and learning two-layer networks on linearly separable data [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "One motivation to study on structured data is to understand the role of structured data play in the implicit regularization, i.e., the observation that the solution learned on less structured or even random data is further away from the initialization",
        "Figure 2 shows the results on MNIST",
        "This work studied the problem of learning a two-layer overparameterized ReLU neural network via stochastic gradient descent (SGD) from random initialization, on data with structure inspired by practical datasets",
        "While our work makes a step towards theoretical understanding of stochastic gradient descent for training neural networs, it is far from being conclusive"
    ],
    "summary": [
        "Neural networks have achieved great success in many applications, but despite a recent increase of theoretical studies, much remains to be explained.",
        "We prove that when the network is sufficiently overparameterized, SGD provably learns a network close to the random initialization and with a small generalization error.",
        "Our work makes a step towrads explaining how overparameterization and random initialization help optimization, and how the inductive bias and good generalization arise from the SGD dynamics on structured data.",
        "More related to our work is [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], which studies the problem of learning a two-layer overparameterized network on linearly separable data and shows that SGD converges to a global optimum with good generalization.",
        "Since the total iteration only depends on log m, when m = poly(N, 1/\u03b4 ) but the input data is structured, SGD can achieve a small generalization error, even when the network has enough capacity to fit arbitrary labels of the training examples.",
        "Our proof is based on the following important observation: The more we overparameterize the network, the less likely the activation pattern for one neuron and one data point will change in a fixed number of iterations.",
        "This observation allows us to couple the gradient of the true neural network with a \u201cpseudo gradient\u201d where the activation pattern for each data point and each neuron is fixed.",
        "Existing work has analyzed the implicit regularization of SGD on logistic regression [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], matrix factorization [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], and learning two-layer networks on linearly separable data [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "Our analysis shows that when the network size is fixed, learning over poorly structured data needs more iterations and the solution can deviate more from the initialization and has higher complexity.",
        "An extreme and especially interesting case is when the network is overparameterized so that in principle it can fit the training data by viewing each point as a component while they come from structured distributions with small number of components.",
        "This section aims at verifying some key implications: (1) the activation patterns of the hidden units couple with those at initialization; (2) The distance from the learned solution from the initialization is relatively small compared to the size of initialization; (3) The accumulated updates have approximately low rank.",
        "This work studied the problem of learning a two-layer overparameterized ReLU neural network via stochastic gradient descent (SGD) from random initialization, on data with structure inspired by practical datasets."
    ],
    "headline": "We study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent  from random initialization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05296"
        },
        {
            "id": "2",
            "entry": "[2] Devansh Arpit, Stanis\u0142aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05394"
        },
        {
            "id": "3",
            "entry": "[3] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela Rus. Datadependent coresets for compressing neural networks with applications to generalization bounds. arXiv preprint arXiv:1804.05345, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05345"
        },
        {
            "id": "5",
            "entry": "[5] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer neural network. arXiv preprint arXiv:1710.11241, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11241"
        },
        {
            "id": "6",
            "entry": "[6] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07966"
        },
        {
            "id": "7",
            "entry": "[7] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10174"
        },
        {
            "id": "8",
            "entry": "[8] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04933"
        },
        {
            "id": "9",
            "entry": "[9] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.11008"
        },
        {
            "id": "10",
            "entry": "[10] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "11",
            "entry": "[11] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pages 6152\u20136160, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017"
        },
        {
            "id": "12",
            "entry": "[12] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04231"
        },
        {
            "id": "13",
            "entry": "[13] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pages 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "14",
            "entry": "[14] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "15",
            "entry": "[15] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "16",
            "entry": "[16] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix recovery. arXiv preprint arXiv:1712.09203, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09203"
        },
        {
            "id": "17",
            "entry": "[17] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pages 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017"
        },
        {
            "id": "18",
            "entry": "[18] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855\u2013863, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014"
        },
        {
            "id": "19",
            "entry": "[19] Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10467"
        },
        {
            "id": "20",
            "entry": "[20] James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pages 735\u2013742, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20James%20Deep%20learning%20via%20hessian-free%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20James%20Deep%20learning%20via%20hessian-free%20optimization%202010"
        },
        {
            "id": "21",
            "entry": "[21] Cisse Moustapha, Bojanowski Piotr, Grave Edouard, Dauphin Yann, and Usunier Nicolas. Parseval networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08847"
        },
        {
            "id": "22",
            "entry": "[22] Vaishnavh Nagarajan and Zico Kolter. Generalization in deep networks: The role of distance from initialization. NIPS workshop on Deep Learning: Bridging Theory and Practice, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20Zico%20Generalization%20in%20deep%20networks%3A%20The%20role%20of%20distance%20from%20initialization.%20NIPS%20workshop%20on%20Deep%20Learning%3A%20Bridging%20Theory%20and%20Practice%202017"
        },
        {
            "id": "23",
            "entry": "[23] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "24",
            "entry": "[24] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6614"
        },
        {
            "id": "25",
            "entry": "[25] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04926"
        },
        {
            "id": "26",
            "entry": "[26] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08361"
        },
        {
            "id": "27",
            "entry": "[27] Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345"
        },
        {
            "id": "28",
            "entry": "[28] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u20131147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "29",
            "entry": "[29] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00560"
        },
        {
            "id": "30",
            "entry": "[30] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. arXiv preprint Arxiv:1611.03131, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03131"
        },
        {
            "id": "31",
            "entry": "[31] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "32",
            "entry": "[32] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        },
        {
            "id": "33",
            "entry": "[33] Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Compressibility and generalization in large-scale deep learning. arXiv preprint arXiv:1804.05862, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1804.05862"
        }
    ]
}
