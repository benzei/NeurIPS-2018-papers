{
    "filename": "7513-ridge-regression-and-provable-deterministic-ridge-leverage-score-sampling.pdf",
    "metadata": {
        "title": "Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling",
        "author": "Shannon McCurdy",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7513-ridge-regression-and-provable-deterministic-ridge-leverage-score-sampling.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Ridge leverage scores provide a balance between low-rank approximation and regularization, and are ubiquitous in randomized linear algebra and machine learning. Deterministic algorithms are also of interest in the moderately big data regime, because deterministic algorithms provide interpretability to the practitioner by having no failure probability and always returning the same results. We provide provable guarantees for deterministic column sampling using ridge leverage scores. The matrix sketch returned by our algorithm is a column subset of the original matrix, yielding additional interpretability. Like the randomized counterparts, the deterministic algorithm provides (1 + ) error column subset selection, (1 + ) error projection-cost preservation, and an additive-multiplicative spectral bound. We also show that under the assumption of power-law decay of ridge leverage scores, this deterministic algorithm is provably as accurate as randomized algorithms. Lastly, ridge regression is frequently used to regularize ill-posed linear least-squares problems. While ridge regression provides shrinkage for the regression coefficients, many of the coefficients remain small but non-zero. Performing ridge regression with the matrix sketch returned by our algorithm and a particular regularization parameter forces coefficients to zero and has a provable (1 + ) bound on the statistical risk. As such, it is an interesting alternative to elastic net regularization."
    },
    "keywords": [
        {
            "term": "messenger RNA",
            "url": "https://en.wikipedia.org/wiki/messenger_RNA"
        },
        {
            "term": "TCGA",
            "url": "https://en.wikipedia.org/wiki/TCGA"
        },
        {
            "term": "elastic net",
            "url": "https://en.wikipedia.org/wiki/elastic_net"
        },
        {
            "term": "ridge regression",
            "url": "https://en.wikipedia.org/wiki/ridge_regression"
        },
        {
            "term": "deterministic algorithm",
            "url": "https://en.wikipedia.org/wiki/deterministic_algorithm"
        },
        {
            "term": "low rank approximation",
            "url": "https://en.wikipedia.org/wiki/low_rank_approximation"
        },
        {
            "term": "singular value decomposition",
            "url": "https://en.wikipedia.org/wiki/singular_value_decomposition"
        }
    ],
    "highlights": [
        "Classical leverage scores quantify the importance of each column i for the range space of the sampleby-feature data matrix A \u2208 Rn\u00d7d",
        "We explore deterministic ridge leverage score (DRLS) sampling for matrix approximation and for feature selection in concert with ridge regression",
        "We introduce a deterministic algorithm (Algorithm 1) for ridge leverage score sampling inspired by the deterministic algorithm for rank-k subspace leverage score sampling (<a class=\"ref-link\" id=\"cPapailiopoulos_et+al_2014_a\" href=\"#rPapailiopoulos_et+al_2014_a\">Papailiopoulos et al, 2014</a>)",
        "We show that under the condition of power-law decay in the ridge leverage scores, the deterministic algorithm chooses fewer columns than random sampling with the same error when max((4k/",
        "Theorem 4 means that there are bounds on the statistical risk for substituting the Deterministic Ridge Leverage Score selected column subset matrix for the complete matrix when performing ridge regression with the appropriate regularization parameter",
        "We provide a biological data illustration of ridge leverage scores and ridge regression with multi-omic data from lower-grade glioma (LGG) tumor samples collected by the TCGA Research Network"
    ],
    "key_statements": [
        "Classical leverage scores quantify the importance of each column i for the range space of the sampleby-feature data matrix A \u2208 Rn\u00d7d",
        "We explore deterministic ridge leverage score (DRLS) sampling for matrix approximation and for feature selection in concert with ridge regression",
        "We introduce a deterministic algorithm (Algorithm 1) for ridge leverage score sampling inspired by the deterministic algorithm for rank-k subspace leverage score sampling (<a class=\"ref-link\" id=\"cPapailiopoulos_et+al_2014_a\" href=\"#rPapailiopoulos_et+al_2014_a\">Papailiopoulos et al, 2014</a>)",
        "We show that under the condition of power-law decay in the ridge leverage scores, the deterministic algorithm chooses fewer columns than random sampling with the same error when max((4k/",
        "Theorem 4 means that there are bounds on the statistical risk for substituting the Deterministic Ridge Leverage Score selected column subset matrix for the complete matrix when performing ridge regression with the appropriate regularization parameter",
        "We provide a biological data illustration of ridge leverage scores and ridge regression with multi-omic data from lower-grade glioma (LGG) tumor samples collected by the TCGA Research Network"
    ],
    "summary": [
        "Classical leverage scores quantify the importance of each column i for the range space of the sampleby-feature data matrix A \u2208 Rn\u00d7d.",
        "For randomized algorithms with ridge leverage score sampling, <a class=\"ref-link\" id=\"cCohen_et+al_2017_a\" href=\"#rCohen_et+al_2017_a\">Cohen et al (2017</a>) prove bounds for the spectrum, column subset selection, and projection-cost preservation.",
        "Introduce a deterministic algorithm for sampling columns from rank-k subspace leverage scores and provide a columns subset selection bound.",
        "McCurdy et al (2017) prove a (1 + ) spectral bound for <a class=\"ref-link\" id=\"cPapailiopoulos_et+al_2014_a\" href=\"#rPapailiopoulos_et+al_2014_a\">Papailiopoulos et al (2014</a>)\u2019s deterministic algorithm and for random sampling with rank-k subspace leverage scores.",
        "We explore deterministic ridge leverage score (DRLS) sampling for matrix approximation and for feature selection in concert with ridge regression.",
        "By using ridge leverage scores instead of rank-k subspace scores in the deterministic algorithm, we prove significantly better bounds for the column subset matrix C.",
        "We show that under the condition of power-law decay in the ridge leverage scores, the deterministic algorithm chooses fewer columns than random sampling with the same error when max((4k/",
        "We combine deterministic ridge leverage score column subset selection with ridge regression for a particular value of the regularization parameter, providing automatic feature selection and continuous shrinkage.",
        "The real data exhibits striking power law decay of the ridge leverage scores (Figure 7), justifying the assumptions underlying the use of DRLS sampling (Theorem 5).",
        "The DRLS algorithm selects for the submatrix C all columns i with ridge leverage score \u03c4i(A) above a threshold \u03b8, determined by the error tolerance .",
        "Approximate ridge leverage score kernel: Let A \u2208 Rn\u00d7d be a matrix of at least rank k and \u03c4i(A) be defined as in Eqn 1.",
        "Approximate Ridge Regression with DRLS: Let A \u2208 Rn\u00d7d be a matrix of at least rank k and \u03c4i(A) be defined as in Eqn 1.",
        "Theorem 4 means that there are bounds on the statistical risk for substituting the DRLS selected column subset matrix for the complete matrix when performing ridge regression with the appropriate regularization parameter.",
        "Ridge Leverage Power-law Decay: Let A \u2208 Rn\u00d7d be a matrix of at least rank k and \u03c4i(A) be defined as in Eqn 1.",
        "DRLS selects fewer columns with the same accuracy in Eqn 16 for power-law decay in the ridge leverage scores when, max",
        "Applying the DRLS algorithm with k = 3, = 0.1 leads to |\u0398| = 1512, selecting approximately 0.02% of the total multi-omic features for the column subset matrix C.",
        "Figure 7 shows the power-law decay of the LGG k = 3 ridge leverage scores with sorted column index."
    ],
    "headline": "We provide provable guarantees for deterministic column sampling using ridge leverage scores",
    "reference_links": [
        {
            "id": "Alaoui_2015_a",
            "entry": "Ahmed El Alaoui and Michael W. Mahoney. 2015. Fast Randomized Kernel Ridge Regression with Statistical Guarantees. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS\u201915). MIT Press, Cambridge, MA, USA, 775\u2013783. http://dl.acm.org/citation.cfm?id=2969239.2969326 http://arxiv.org/abs/1411.0306.",
            "url": "http://dl.acm.org/citation.cfm?id=2969239.2969326",
            "arxiv_url": "https://arxiv.org/pdf/1411.0306"
        },
        {
            "id": "Boutsidis_et+al_2009_a",
            "entry": "Christos Boutsidis, Petros Drineas, and Michael W Mahoney. 2009. Unsupervised Feature Selection for the k-means Clustering Problem. In Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta (Eds.). Curran Associates, Inc., 153\u2013161. http://papers.nips.cc/paper/3724-unsupervised-feature-selection-for-the-k-means-clustering-problem.pdf",
            "url": "http://papers.nips.cc/paper/3724-unsupervised-feature-selection-for-the-k-means-clustering-problem.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boutsidis%2C%20Christos%20Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20Unsupervised%20Feature%20Selection%20for%20the%20k-means%20Clustering%20Problem%202009"
        },
        {
            "id": "Breiman_1996_a",
            "entry": "Leo Breiman. 1996. Heuristics of instability and stabilization in model selection. The Annals of Statistics 24, 6 (Dec. 1996), 2350\u20132383. https://doi.org/10.1214/aos/1032181158",
            "crossref": "https://dx.doi.org/10.1214/aos/1032181158",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/aos/1032181158"
        },
        {
            "id": "Broad_2016_a",
            "entry": "Broad Institute of MIT and Harvard. 2016. Broad Institute TCGA Genome Data Analysis Center (2016): Analysis-ready standardized TCGA data from Broad GDAC Firehose 2016_01_28 run. (Jan. 2016). https://doi.org/10.7908/C11G0KM9 Dataset.",
            "crossref": "https://dx.doi.org/10.7908/C11G0KM9"
        },
        {
            "id": "Chatterjee_1986_a",
            "entry": "Samprit Chatterjee and Ali S. Hadi. 1986. Influential Observations, High Leverage Points, and Outliers in Linear Regression. Statist. Sci. 1, 3 (Aug. 1986), 379\u2013393. https://doi.org/10.1214/ss/1177013622",
            "crossref": "https://dx.doi.org/10.1214/ss/1177013622",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/ss/1177013622"
        },
        {
            "id": "Cohen_et+al_2015_a",
            "entry": "Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. 2015. Dimensionality Reduction for k-Means Clustering and Low Rank Approximation. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing (STOC \u201915). ACM, New York, NY, USA, 163\u2013172. https://doi.org/10.1145/2746539.2746569",
            "crossref": "https://dx.doi.org/10.1145/2746539.2746569",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2746539.2746569"
        },
        {
            "id": "Cohen_et+al_2017_a",
            "entry": "Michael B. Cohen, Cameron Musco, and Christopher Musco. 2017. Input Sparsity Time Low-rank Approximation via Ridge Leverage Score Sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA \u201917). Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 1758\u20131777. http://dl.acm.org/citation.cfm?id=3039686.3039801",
            "url": "http://dl.acm.org/citation.cfm?id=3039686.3039801",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Michael%20B.%20Musco%2C%20Cameron%20Musco%2C%20Christopher%20Input%20Sparsity%20Time%20Low-rank%20Approximation%20via%20Ridge%20Leverage%20Score%20Sampling%202017"
        },
        {
            "id": "Drineas_et+al_2008_a",
            "entry": "Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. 2008. Relative-Error $CUR$ Matrix Decompositions. SIAM J. Matrix Anal. Appl. 30, 2 (Sept. 2008), 844\u2013881. https://doi.org/10.1137/07070471X",
            "crossref": "https://dx.doi.org/10.1137/07070471X",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/07070471X"
        },
        {
            "id": "Feldman_et+al_2013_a",
            "entry": "D. Feldman, M. Schmidt, and C. Sohler. 2013. Turning Big data into tiny data: Constant-size coresets for k-means, PCA and projective clustering. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 1434\u20131453. https://doi.org/10.1137/1.9781611973105.103",
            "crossref": "https://dx.doi.org/10.1137/1.9781611973105.103",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/1.9781611973105.103"
        },
        {
            "id": "Hoerl_1970_a",
            "entry": "Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics 12, 1 (Feb. 1970), 55\u201367. https://doi.org/10.1080/00401706.1970.10488634",
            "crossref": "https://dx.doi.org/10.1080/00401706.1970.10488634",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1080/00401706.1970.10488634"
        },
        {
            "id": "Horn_2013_a",
            "entry": "Roger A. Horn and Charles R. Johnson. 2013. Matrix analysis (2nd ed ed.). Cambridge University Press, New York.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Matrix%20analysis%202013"
        },
        {
            "id": "Mccurdy_et+al_2017_a",
            "entry": "Shannon McCurdy, Vasilis Ntranos, and Lior Pachter. 2017. Column subset selection for single-cell RNA-Seq clustering. bioRxiv (July 2017), 159079. https://doi.org/10.1101/159079",
            "crossref": "https://dx.doi.org/10.1101/159079"
        },
        {
            "id": "Mezzadri_2006_a",
            "entry": "Francesco Mezzadri. 2006. How to generate random matrices from the classical compact groups. Notices of the American Mathematical Society 54 (Oct. 2006).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mezzadri%2C%20Francesco%20How%20to%20generate%20random%20matrices%20from%20the%20classical%20compact%20groups%202006-10-54",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mezzadri%2C%20Francesco%20How%20to%20generate%20random%20matrices%20from%20the%20classical%20compact%20groups%202006-10-54"
        },
        {
            "id": "Papailiopoulos_et+al_2014_a",
            "entry": "Dimitris Papailiopoulos, Anastasios Kyrillidis, and Christos Boutsidis. 2014. Provable Deterministic Leverage Score Sampling. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \u201914). ACM, New York, NY, USA, 997\u20131006. https://doi.org/10.1145/2623330.2623698",
            "crossref": "https://dx.doi.org/10.1145/2623330.2623698",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2623330.2623698"
        },
        {
            "id": "Rudi_et+al_2015_a",
            "entry": "Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. 2015. Less is More: Nystr\\\"om Computational Regularization. arXiv:1507.04717 [cs, stat] (July 2015). http://arxiv.org/abs/1507.04717 arXiv:1507.04717.",
            "url": "http://arxiv.org/abs/1507.04717",
            "arxiv_url": "https://arxiv.org/pdf/1507.04717"
        },
        {
            "id": "The_2015_a",
            "entry": "The Cancer Genome Atlas Research Network. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20The%20Cancer%20Genome%20Atlas%20Research%20Network%202015"
        },
        {
            "id": "Comprehensive,_2015_b",
            "entry": "Comprehensive, Integrative Genomic Analysis of Diffuse Lower-Grade Gliomas. The New England journal of medicine 372, 26 (June 2015), 2481\u20132498. https://doi.org/10.1056/NEJMoa1402121",
            "crossref": "https://dx.doi.org/10.1056/NEJMoa1402121",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1056/NEJMoa1402121"
        },
        {
            "id": "Tibshirani_1994_a",
            "entry": "Robert Tibshirani. 1994. Regression Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical Society, Series B 58 (1994), 267\u2013288.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20Shrinkage%20and%20Selection%20Via%20the%20Lasso%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20Shrinkage%20and%20Selection%20Via%20the%20Lasso%201994"
        },
        {
            "id": "Velleman_1981_a",
            "entry": "Paul F. Velleman and Roy E. Welsch. 1981. Efficient Computing of Regression Diagnostics. The American Statistician 35, 4 (1981), 234\u2013242. https://doi.org/10.2307/2683296",
            "crossref": "https://dx.doi.org/10.2307/2683296",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.2307/2683296"
        },
        {
            "id": "Wan_et+al_2016_a",
            "entry": "Ying-Wooi Wan, Genevera I. Allen, and Zhandong Liu. 2016. TCGA2STAT: simple TCGA data access for integrated statistical analysis in R. Bioinformatics 32, 6 (March 2016), 952\u2013954. https://doi.org/10.1093/bioinformatics/btv677",
            "crossref": "https://dx.doi.org/10.1093/bioinformatics/btv677",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1093/bioinformatics/btv677"
        },
        {
            "id": "Zhang_2015_a",
            "entry": "Jianhua Zhang. 2015. CNTools: Convert segment data into a region by sample matrix to allow for other high level computational analyses. (2015). http://bioconductor.org/packages/CNTools/ R package version 1.26.0.",
            "url": "http://bioconductor.org/packages/CNTools/R",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jianhua%20CNTools%3A%20Convert%20segment%20data%20into%20a%20region%20by%20sample%20matrix%20to%20allow%20for%20other%20high%20level%20computational%20analyses%202015"
        },
        {
            "id": "Zou_2005_a",
            "entry": "Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the Elastic Net. Journal of the Royal Statistical Society, Series B 67 (2005), 301\u2013320.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Regularization%20and%20variable%20selection%20via%20the%20Elastic%20Net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Regularization%20and%20variable%20selection%20via%20the%20Elastic%20Net%202005"
        },
        {
            "id": "Zou_2009_a",
            "entry": "Hui Zou and Hao Helen Zhang. 2009. On the adaptive elastic-net with a diverging number of parameters. The Annals of Statistics 37, 4 (Aug. 2009), 1733\u20131751. https://doi.org/10.1214/08-AOS625",
            "crossref": "https://dx.doi.org/10.1214/08-AOS625",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/08-AOS625"
        }
    ]
}
