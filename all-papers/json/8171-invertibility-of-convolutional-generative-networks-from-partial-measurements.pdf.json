{
    "filename": "8171-invertibility-of-convolutional-generative-networks-from-partial-measurements.pdf",
    "metadata": {
        "title": "Invertibility of Convolutional Generative Networks from Partial Measurements",
        "author": "Fangchang Ma, Ulas Ayaz, Sertac Karaman",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this work, we present new theoretical results on convolutional generative neural networks, in particular their invertibility (i.e., the recovery of input latent code given the network output). The study of network inversion problem is motivated by image inpainting and the mode collapse problem in training GAN. Network inversion is highly non-convex, and thus is typically computationally intractable and without optimality guarantees. However, we rigorously prove that, under some mild technical assumptions, the input of a two-layer convolutional generative network can be deduced from the network output efficiently using simple gradient descent. This new theoretical finding implies that the mapping from the lowdimensional latent space to the high-dimensional image space is bijective (i.e., one-to-one). In addition, the same conclusion holds even when the network output is only partially observed (i.e., with missing pixels). Our theorems hold for 2-layer convolutional generative network with ReLU as the activation function, but we demonstrate empirically that the same conclusion extends to multi-layer networks and networks with other activation functions, including the leaky ReLU, sigmoid and tanh."
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        },
        {
            "term": "image space",
            "url": "https://en.wikipedia.org/wiki/image_space"
        }
    ],
    "highlights": [
        "Generative models have made significant progress in learning representations for complex and multi-modal data distributions, such as those of natural images [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "We prove that a convolutional generative neural network is invertible, with high probability, under the following assumptions: (1) the network consists of two layers of transposed convolutions followed by rectified linear unit activation functions; (2) the network is expansive; (3) the filter weights follow a Gaussian distribution",
        "We present our main theoretical results regarding the invertibility of a 2-layer convolutional generative network with rectified linear unit",
        "In this work we prove rigorously that a 2-layer rectified linear unit convolutional generative neural network is invertible, even when only partial output is observed",
        "This result provides a sufficient condition for the generator network to be one-to-one, which avoids the mode collapse problem in training of generative adversarial network",
        "We empirically demonstrate that the same conclusion holds even if the generative models have other nonlinear activation functions (LeakyReLU, Sigmoid and Tanh) and multiple layers"
    ],
    "key_statements": [
        "Generative models have made significant progress in learning representations for complex and multi-modal data distributions, such as those of natural images [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "We prove that a convolutional generative neural network is invertible, with high probability, under the following assumptions: (1) the network consists of two layers of transposed convolutions followed by rectified linear unit activation functions; (2) the network is expansive; (3) the filter weights follow a Gaussian distribution",
        "We present our main theoretical results regarding the invertibility of a 2-layer convolutional generative network with rectified linear unit",
        "In this work we prove rigorously that a 2-layer rectified linear unit convolutional generative neural network is invertible, even when only partial output is observed",
        "This result provides a sufficient condition for the generator network to be one-to-one, which avoids the mode collapse problem in training of generative adversarial network",
        "We empirically demonstrate that the same conclusion holds even if the generative models have other nonlinear activation functions (LeakyReLU, Sigmoid and Tanh) and multiple layers"
    ],
    "summary": [
        "Generative models have made significant progress in learning representations for complex and multi-modal data distributions, such as those of natural images [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "We address the following question: given a convolutional generative network2, is it possible to \u201cdecode\u201d an output image and recover the corresponding input latent code?",
        "We note the work [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], which studies a 1-layer network with a special activation function (Concatenated ReLU, which is essentially linear) and a strong assumption on the latent code.",
        "We prove that a convolutional generative neural network is invertible, with high probability, under the following assumptions: (1) the network consists of two layers of transposed convolutions followed by ReLU activation functions; (2) the network is expansive; (3) the filter weights follow a Gaussian distribution.",
        "When these conditions are satisfied, the input latent code can be recovered from partial output of a generative neural network by minimizing a L2 empirical loss function using gradient descent.",
        "We further demonstrate empirically that that our theoretical results generalize to (1) multiple-layer networks; (2) networks with other nonlinear activation functions, including Leaky ReLU, Sigmoid and Tanh.",
        "We present our main theoretical results regarding the invertibility of a 2-layer convolutional generative network with ReLUs. Our first main theoretical contribution is as follows: the problem in (3) is non-convex, under appropriate conditions there is a strict descent direction everywhere, except in the neighborhood of zand that of a negative multiple of z.",
        "The second condition is on the approximate angle contraction property of an effective weight matrix Wi. Lemma Sup.4 [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] shows that the angle between two arbitrary input vectors x and y does not vanish under a transposed convolution layer and the ReLU.",
        "Under the assumptions of Theorem 1, let the network weights follow any zero-mean subgaussian distribution Pp|x| \u0105 tq \u010f ce\u03b3t2 , @t \u0105 0 instead of Gaussian.",
        "As a sanity check on Theorem 4, we construct a generative neural network with 2 transposed convolution layers, each followed by a ReLU.",
        "In this work we prove rigorously that a 2-layer ReLU convolutional generative neural network is invertible, even when only partial output is observed.",
        "This result provides a sufficient condition for the generator network to be one-to-one, which avoids the mode collapse problem in training of GAN.",
        "We empirically demonstrate that the same conclusion holds even if the generative models have other nonlinear activation functions (LeakyReLU, Sigmoid and Tanh) and multiple layers.",
        "Some interesting future research directions include rigorous proofs for leaky ReLUs and other activation functions, subgaussian network weights, as well as inversion under noisy measurements."
    ],
    "headline": "We present new theoretical results on convolutional generative neural networks, in particular their invertibility ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with implications for training. ICLR workshop.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Liang%2C%20Yingyu%20and%20Tengyu%20Ma.%20Why%20are%20deep%20nets%20reversible%3A%20A%20simple%20theory%2C%20with%20implications%20for%20training.%20ICLR%20workshop"
        },
        {
            "id": "2",
            "entry": "[2] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative models. arXiv preprint arXiv:1703.03208, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03208"
        },
        {
            "id": "3",
            "entry": "[3] E. Cand\u00e8s, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. Comm. Pure Appl. Math., 59(8):1207\u20131223, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20E.%20Romberg%2C%20J.%20Tao%2C%20T.%20Stable%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20measurements%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20E.%20Romberg%2C%20J.%20Tao%2C%20T.%20Stable%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20measurements%202006"
        },
        {
            "id": "4",
            "entry": "[4] David L. Donoho. For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution. Comm. Pure Appl. Math, 59:797\u2013829, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20For%20most%20large%20underdetermined%20systems%20of%20linear%20equations%20the%20minimal%20l1-norm%20solution%20is%20also%20the%20sparsest%20solution%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20For%20most%20large%20underdetermined%20systems%20of%20linear%20equations%20the%20minimal%20l1-norm%20solution%20is%20also%20the%20sparsest%20solution%202004"
        },
        {
            "id": "5",
            "entry": "[5] Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00779"
        },
        {
            "id": "6",
            "entry": "[6] Anna C Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, and Honglak Lee. Towards understanding the invertibility of convolutional neural networks. arXiv preprint arXiv:1705.08664, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08664"
        },
        {
            "id": "7",
            "entry": "[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "8",
            "entry": "[8] Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by empirical risk. arXiv preprint arXiv:1705.07576, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07576"
        },
        {
            "id": "9",
            "entry": "[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "10",
            "entry": "[10] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "11",
            "entry": "[11] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "12",
            "entry": "[12] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "13",
            "entry": "[13] Fangchang Ma and Sertac Karaman. Sparse-to-dense: Depth prediction from sparse depth samples and a single image. arXiv preprint arXiv:1709.07492, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.07492"
        },
        {
            "id": "14",
            "entry": "[14] Fangchang Ma, Luca Carlone, Ulas Ayaz, and Sertac Karaman. Sparse depth sensing for resource-constrained robots. arXiv preprint arXiv:1703.01398, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01398"
        },
        {
            "id": "15",
            "entry": "[15] Fangchang Ma, Ulas Ayaz, and Sertac Karaman. Supplementary materials - invertibility of convolutional generative networks from partial measurements. NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Fangchang%20Ayaz%2C%20Ulas%20Karaman%2C%20Sertac%20Supplementary%20materials%20-%20invertibility%20of%20convolutional%20generative%20networks%20from%20partial%20measurements%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Fangchang%20Ayaz%2C%20Ulas%20Karaman%2C%20Sertac%20Supplementary%20materials%20-%20invertibility%20of%20convolutional%20generative%20networks%20from%20partial%20measurements%202018"
        },
        {
            "id": "16",
            "entry": "[16] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "17",
            "entry": "[17] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2536\u20132544, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "18",
            "entry": "[18] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "19",
            "entry": "[19] Mahdi Soltanolkotabi. Learning relus via gradient descent. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2004\u20132014. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20Mahdi%20Learning%20relus%20via%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20Mahdi%20Learning%20relus%20via%20gradient%20descent%202017"
        },
        {
            "id": "20",
            "entry": "[20] Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing Systems, pages 3310\u20133320, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017"
        },
        {
            "id": "21",
            "entry": "[21] Raymond A Yeh, Chen Chen, Teck Yian Lim, Alexander G Schwing, Mark Hasegawa-Johnson, and Minh N Do. Semantic image inpainting with deep generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5485\u20135493, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20Raymond%20A.%20Chen%2C%20Chen%20Lim%2C%20Teck%20Yian%20Schwing%2C%20Alexander%20G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017"
        },
        {
            "id": "22",
            "entry": "[22] Jun-Yan Zhu, Philipp Kr\u00e4henb\u00fchl, Eli Shechtman, and Alexei A. Efros. Generative visual manipulation on the natural image manifold. In Proceedings of European Conference on Computer Vision (ECCV), 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Shechtman%2C%20Eli%20Efros%2C%20Alexei%20A.%20Generative%20visual%20manipulation%20on%20the%20natural%20image%20manifold%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Shechtman%2C%20Eli%20Efros%2C%20Alexei%20A.%20Generative%20visual%20manipulation%20on%20the%20natural%20image%20manifold%202016"
        }
    ]
}
