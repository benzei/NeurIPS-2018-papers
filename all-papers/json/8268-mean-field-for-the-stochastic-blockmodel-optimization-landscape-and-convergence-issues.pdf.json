{
    "filename": "8268-mean-field-for-the-stochastic-blockmodel-optimization-landscape-and-convergence-issues.pdf",
    "metadata": {
        "title": "Mean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues",
        "author": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, Y. X. Rachel Wang, Bowei Yan",
        "date": 2006,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8268-mean-field-for-the-stochastic-blockmodel-optimization-landscape-and-convergence-issues.pdf"
        },
        "journal": "University of Sydney NSW",
        "abstract": "Variational approximation has been widely used in large-scale Bayesian inference recently, the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field, theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper, we focus on the problem of community detection for a simple twoclass Stochastic Blockmodel (SBM). Using batch co-ordinate ascent (BCAVI) for updates, we show different convergence behavior with respect to different initializations. When the parameters are known, we show that a significant proportion of random initializations will converge to the ground truth. On the other hand, when the parameters themselves need to be estimated, a random initialization will converge to an uninformative local optimum."
    },
    "keywords": [
        {
            "term": "expectation maximization",
            "url": "https://en.wikipedia.org/wiki/expectation_maximization"
        },
        {
            "term": "topic model",
            "url": "https://en.wikipedia.org/wiki/topic_model"
        },
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "ground truth",
            "url": "https://en.wikipedia.org/wiki/ground_truth"
        }
    ],
    "highlights": [
        "Variational approximation has recently gained a huge momentum in contemporary Bayesian statistics [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "Mean field is the simplest type of variational approximation, and is a popular tool in large scale Bayesian inference",
        "When the model parameters are known, a considerable volume of the random initializations converge to the ground truth",
        "Existing theoretical works typically analyze the behavior of the global optima, or the local convergence behavior when initialized near the ground truth",
        "We show that when the model parameters are known, a substantial proportion of random initializations will lead to convergence to the ground truth",
        "When parameter values are not known, but estimated, we show that a random initialization with high probability converges to a meaningless local optima. This shows the futility of using many random initializations, which is typically done in practice when no prior knowledge is available"
    ],
    "key_statements": [
        "Variational approximation has recently gained a huge momentum in contemporary Bayesian statistics [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "Mean field is the simplest type of variational approximation, and is a popular tool in large scale Bayesian inference",
        "This is akin to the Expectation Maximization algorithm [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], where one obtains a lower bound of the marginal log likelihood function via the expectation with respect to the conditional distribution of the latent variables under the current estimates of the underlying parameters",
        "For mean-field variational approximation, the lower bound or ELBO is computed using the expectation with respect to a product distribution of the latent variables",
        "In [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], the authors fully characterize the landscape of the likelihood of the two equal proportion Gaussian Mixture Model, where the main message is that most random initializations should converge to the ground truth",
        "When the model parameters are known, a considerable volume of the random initializations converge to the ground truth",
        "Existing theoretical works typically analyze the behavior of the global optima, or the local convergence behavior when initialized near the ground truth",
        "We show that when the model parameters are known, a substantial proportion of random initializations will lead to convergence to the ground truth",
        "When parameter values are not known, but estimated, we show that a random initialization with high probability converges to a meaningless local optima. This shows the futility of using many random initializations, which is typically done in practice when no prior knowledge is available"
    ],
    "summary": [
        "Variational approximation has recently gained a huge momentum in contemporary Bayesian statistics [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "For mean-field variational approximation, the lower bound or ELBO is computed using the expectation with respect to a product distribution of the latent variables.",
        "In [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], the authors fully characterize the landscape of the likelihood of the two equal proportion Gaussian Mixture Model, where the main message is that most random initializations should converge to the ground truth.",
        "When the model parameters are known, a considerable volume of the random initializations converge to the ground truth.",
        "1 2 is a saddle point of the population mean field log-likelihood.",
        "1 2 is a saddle point of the population mean field log-likelihood when p and q are known, for all n large enough.",
        "\u221e, 1}, From Theorem 3.3, we can calculate lower bounds to the volumes of the basins of attractions of the limit points of the population BCAVI updates.",
        "Using arguments that goes into the proof of Theorem 3.3, we can show that in the large n limit, there are only five stationary points of the mean field log-likelihood, namely",
        "Using the update forms in Lemma 3.1, the following result shows that the stationary points of the population mean field log-likelihood lie in the principle eigenspace span{u1, u2} of P in a limiting sense.",
        "Let (\u03c8, p, q) be a stationary point of the population mean field log-likeliho\u221aod.",
        "We work with the mean field variational algorithm for a simple two class stochastic blockmodel with equal sized classes.",
        "We show that when the model parameters are known, a substantial proportion of random initializations will lead to convergence to the ground truth.",
        "When parameter values are not known, but estimated, we show that a random initialization with high probability converges to a meaningless local optima.",
        "The posterior probabilities of the latent labels in the latter model can be estimated when the parameters are known, whereas this is not the case for SBM since the posterior probability P(Zi|A) depends on the entire network.",
        "In Fig 2 all 1000 random initializations converge to stationary points such that each column of \u03c8 lies in the span of {1C1 , 1C2 , 1C3 }, which are membership vectors for each class.",
        "When p, q are unknown, random initializations always converge to uninformative stationary point (1/3, 1/3, 1/3), analogous to Lemma 3.2."
    ],
    "headline": "We focus on the problem of community detection for a simple twoclass Stochastic Blockmodel ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Appendix for \u201cMean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues\u201d. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Appendix%20for%20%E2%80%9CMean%20Field%20for%20the%20Stochastic%20Blockmodel%3A%20Optimization%20Landscape%20and%20Convergence%20Issues%E2%80%9D%202018"
        },
        {
            "id": "2",
            "entry": "[2] Pranjal Awasthi and Andrej Risteski. On some provably correct cases of variational inference for topic models. In Advances in Neural Information Processing Systems, pages 2098\u20132106, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pranjal%20Awasthi%20and%20Andrej%20Risteski.%20On%20some%20provably%20correct%20cases%20of%20variational%20inference%20for%20topic%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pranjal%20Awasthi%20and%20Andrej%20Risteski.%20On%20some%20provably%20correct%20cases%20of%20variational%20inference%20for%20topic%20models%202015"
        },
        {
            "id": "3",
            "entry": "[3] Peter Bickel, David Choi, Xiangyu Chang, and Hai Zhang. Asymptotic normality of maximum likelihood and its variational approximation for stochastic blockmodels. The Annals of Statistics, pages 1922\u20131943, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bickel%2C%20Peter%20Choi%2C%20David%20Chang%2C%20Xiangyu%20Zhang%2C%20Hai%20Asymptotic%20normality%20of%20maximum%20likelihood%20and%20its%20variational%20approximation%20for%20stochastic%20blockmodels%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bickel%2C%20Peter%20Choi%2C%20David%20Chang%2C%20Xiangyu%20Zhang%2C%20Hai%20Asymptotic%20normality%20of%20maximum%20likelihood%20and%20its%20variational%20approximation%20for%20stochastic%20blockmodels%202013"
        },
        {
            "id": "4",
            "entry": "[4] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "5",
            "entry": "[5] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993\u20131022, March 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20dirichlet%20allocation%202003-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20dirichlet%20allocation%202003-03"
        },
        {
            "id": "6",
            "entry": "[6] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993\u20131022, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Andrew%20Y%20Ng%2C%20and%20Michael%20I%20Jordan.%20Latent%20dirichlet%20allocation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Andrew%20Y%20Ng%2C%20and%20Michael%20I%20Jordan.%20Latent%20dirichlet%20allocation%202003"
        },
        {
            "id": "7",
            "entry": "[7] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977"
        },
        {
            "id": "8",
            "entry": "[8] Behrooz Ghorbani, Hamid Javadi, and Andrea Montanari. An instability in variational inference for topic models. arXiv preprint arXiv:1802.00568, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00568"
        },
        {
            "id": "9",
            "entry": "[9] Ryan Giordano, Tamara Broderick, and Michael I Jordan. Covariances, robustness, and variational bayes. arXiv preprint arXiv:1709.02536, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.02536"
        },
        {
            "id": "10",
            "entry": "[10] Jake M Hofman and Chris H Wiggins. Bayesian approach to network modularity. Physical review letters, 100(25):258701, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hofman%2C%20Jake%20M.%20Wiggins%2C%20Chris%20H.%20Bayesian%20approach%20to%20network%20modularity%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hofman%2C%20Jake%20M.%20Wiggins%2C%20Chris%20H.%20Bayesian%20approach%20to%20network%20modularity%202008"
        },
        {
            "id": "11",
            "entry": "[11] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. Social networks, 5(2):109\u2013137, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holland%2C%20Paul%20W.%20Laskey%2C%20Kathryn%20Blackmond%20Leinhardt%2C%20Samuel%20Stochastic%20blockmodels%3A%20First%20steps%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holland%2C%20Paul%20W.%20Laskey%2C%20Kathryn%20Blackmond%20Leinhardt%2C%20Samuel%20Stochastic%20blockmodels%3A%20First%20steps%201983"
        },
        {
            "id": "12",
            "entry": "[12] Tommi S. Jaakkola and Michael I. Jordon. Learning in graphical models. chapter Improving the Mean Field Approximation via the Use of Mixture Distributions, pages 163\u2013173. MIT Press, Cambridge, MA, USA, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaakkola%2C%20Tommi%20S.%20Jordon%2C%20Michael%20I.%20Learning%20in%20graphical%20models.%20chapter%20Improving%20the%20Mean%20Field%20Approximation%20via%20the%20Use%20of%20Mixture%20Distributions%201999"
        },
        {
            "id": "13",
            "entry": "[13] Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J Wainwright, and Michael I Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic conse quences. In Advances in Neural Information Processing Systems, pages 4116\u20134124, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Zhang%2C%20Yuchen%20Balakrishnan%2C%20Sivaraman%20Wainwright%2C%20Martin%20J.%20Local%20maxima%20in%20the%20likelihood%20of%20gaussian%20mixture%20models%3A%20Structural%20results%20and%20algorithmic%20conse%20quences%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Zhang%2C%20Yuchen%20Balakrishnan%2C%20Sivaraman%20Wainwright%2C%20Martin%20J.%20Local%20maxima%20in%20the%20likelihood%20of%20gaussian%20mixture%20models%3A%20Structural%20results%20and%20algorithmic%20conse%20quences%202016"
        },
        {
            "id": "14",
            "entry": "[14] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. Mach. Learn., 37(2):183\u2013233, November 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999-11"
        },
        {
            "id": "15",
            "entry": "[15] Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. 07 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20Song%20Bai%2C%20Yu%20Montanari%2C%20Andrea%20The%20landscape%20of%20empirical%20risk%20for%20non-convex%20losses%202016"
        },
        {
            "id": "16",
            "entry": "[16] Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In Advances in neural information processing systems, pages 849\u2013856, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Weiss%2C%20Yair%20On%20spectral%20clustering%3A%20Analysis%20and%20an%20algorithm%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Weiss%2C%20Yair%20On%20spectral%20clustering%3A%20Analysis%20and%20an%20algorithm%202002"
        },
        {
            "id": "17",
            "entry": "[17] Debdeep Pati, Anirban Bhattacharya, and Yun Yang. On statistical optimality of variational bayes. arXiv preprint arXiv:1712.08983, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08983"
        },
        {
            "id": "18",
            "entry": "[18] Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional stochastic blockmodel. The Annals of Statistics, pages 1878\u20131915, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rohe%2C%20Karl%20Chatterjee%2C%20Sourav%20Yu%2C%20Bin%20Spectral%20clustering%20and%20the%20high-dimensional%20stochastic%20blockmodel%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rohe%2C%20Karl%20Chatterjee%2C%20Sourav%20Yu%2C%20Bin%20Spectral%20clustering%20and%20the%20high-dimensional%20stochastic%20blockmodel%202011"
        },
        {
            "id": "19",
            "entry": "[19] Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1\u2013305, January 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference.%20Found%202008-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference.%20Found%202008-01"
        },
        {
            "id": "20",
            "entry": "[20] Bo Wang and D Michael Titterington. Convergence and asymptotic normality of variational bayesian approximations for exponential family models with missing values. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 577\u2013584. AUAI Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Bo%20Titterington%2C%20D.Michael%20Convergence%20and%20asymptotic%20normality%20of%20variational%20bayesian%20approximations%20for%20exponential%20family%20models%20with%20missing%20values%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Bo%20Titterington%2C%20D.Michael%20Convergence%20and%20asymptotic%20normality%20of%20variational%20bayesian%20approximations%20for%20exponential%20family%20models%20with%20missing%20values%202004"
        },
        {
            "id": "21",
            "entry": "[21] Bo Wang and DM Titterington. Inadequacy of interval estimates corresponding to variational bayesian approximations. In AISTATS, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Bo%20Titterington%2C%20D.M.%20Inadequacy%20of%20interval%20estimates%20corresponding%20to%20variational%20bayesian%20approximations%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Bo%20Titterington%2C%20D.M.%20Inadequacy%20of%20interval%20estimates%20corresponding%20to%20variational%20bayesian%20approximations%202005"
        },
        {
            "id": "22",
            "entry": "[22] Bo Wang, DM Titterington, et al. Convergence properties of a general algorithm for calculating variational bayesian estimates for a normal mixture model. Bayesian Analysis, 1(3):625\u2013650, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bo%20Wang%2C%20D.M.Titterington%20Convergence%20properties%20of%20a%20general%20algorithm%20for%20calculating%20variational%20bayesian%20estimates%20for%20a%20normal%20mixture%20model%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bo%20Wang%2C%20D.M.Titterington%20Convergence%20properties%20of%20a%20general%20algorithm%20for%20calculating%20variational%20bayesian%20estimates%20for%20a%20normal%20mixture%20model%202006"
        },
        {
            "id": "23",
            "entry": "[23] Yixin Wang and David M Blei. Frequentist consistency of variational bayes. arXiv preprint arXiv:1705.03439, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03439"
        },
        {
            "id": "24",
            "entry": "[24] Ted Westling and Tyler H McCormick. Beyond prediction: A framework for inference with variational approximations in mixture models. arXiv preprint arXiv:1510.08151, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.08151"
        },
        {
            "id": "25",
            "entry": "[25] Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2676\u20132684. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Ji%20Hsu%2C%20Daniel%20J.%20Maleki%2C%20Arian%20Global%20analysis%20of%20expectation%20maximization%20for%20mixtures%20of%20two%20gaussians%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Ji%20Hsu%2C%20Daniel%20J.%20Maleki%2C%20Arian%20Global%20analysis%20of%20expectation%20maximization%20for%20mixtures%20of%20two%20gaussians%202016"
        },
        {
            "id": "26",
            "entry": "[26] Anderson Y Zhang and Harrison H Zhou. Theoretical and computational guarantees of mean field variational inference for community detection. arXiv preprint arXiv:1710.11268, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11268"
        },
        {
            "id": "27",
            "entry": "[27] Fengshuo Zhang and Chao Gao. Convergence rates of variational posterior distributions. arXiv preprint arXiv:1712.02519, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1712.02519"
        }
    ]
}
