{
    "filename": "8021-stochastic-expectation-maximization-with-variance-reduction.pdf",
    "metadata": {
        "date": 2018,
        "title": "Stochastic Expectation Maximization with Variance Reduction",
        "author": "Jianfei Chen\u2020, Jun Zhu\u2020\u2217, Yee Whye Teh\u2021 and Tong Zhang\u00a7 \u2020 Dept. of Comp. Sci. & Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys.,",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8021-stochastic-expectation-maximization-with-variance-reduction.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Expectation-Maximization (EM) is a popular tool for learning latent variable models, but the vanilla batch EM does not scale to large data sets because the whole data set is needed at every E-step. Stochastic Expectation Maximization (sEM) reduces the cost of E-step by stochastic approximation. However, sEM has a slower asymptotic convergence rate than batch EM, and requires a decreasing sequence of step sizes, which is difficult to tune. In this paper, we propose a variance reduced stochastic EM (sEM-vr) algorithm inspired by variance reduced stochastic gradient descent algorithms. We show that sEM-vr has the same exponential asymptotic convergence rate as batch EM. Moreover, sEM-vr only requires a constant step size to achieve this rate, which alleviates the burden of parameter tuning. We compare sEM-vr with batch EM, sEM and other algorithms on Gaussian mixture models and probabilistic latent semantic analysis, and sEM-vr converges significantly faster than these baselines."
    },
    "keywords": [
        {
            "term": "semantic analysis",
            "url": "https://en.wikipedia.org/wiki/semantic_analysis"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "em algorithm",
            "url": "https://en.wikipedia.org/wiki/em_algorithm"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "exponential family",
            "url": "https://en.wikipedia.org/wiki/exponential_family"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "maximum a posteriori",
            "url": "https://en.wikipedia.org/wiki/maximum_a_posteriori"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "local convergence",
            "url": "https://en.wikipedia.org/wiki/local_convergence"
        },
        {
            "term": "maximum likelihood estimation",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimation"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "latent Dirichlet allocation",
            "url": "https://en.wikipedia.org/wiki/latent_Dirichlet_allocation"
        },
        {
            "term": "expectation maximization",
            "url": "https://en.wikipedia.org/wiki/expectation_maximization"
        },
        {
            "term": "stochastic approximation",
            "url": "https://en.wikipedia.org/wiki/stochastic_approximation"
        }
    ],
    "highlights": [
        "We review batch and stochastic Expectation Maximization algorithms",
        "We demonstrate our algorithm on Gaussian mixture models and probabilistic latent semantic analysis [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]. stochastic EM-vr achieves significantly faster convergence comparing with stochastic EM, Batch expectation-maximization, and other gradient-based and Bayesian algorithms",
        "We find that gradient-based algorithms and Batch expectation-maximization are not competitive with stochastic EM and stochastic EM-vr, so we only report their results on NIPS, to make the distinction stochastic EM and stochastic EM-vr more clear",
        "Due to the reduced variance, stochastic EM-vr consistently converges faster to better training objective than stochastic EM and Batch expectation-maximization on all the datasets, while the constant step size of stochastic EM-vr is easier to tune than the decreasing sequence of step sizes for stochastic EM.\n3We find that all the algorithms have the same best hyperparameter configuration. 4We have tried constant step sizes for stochastic mirror descent-vr and reparameterized stochastic gradient descent-vr but found it worse than decreasing step sizes.\n1.300 1e7 1.325 1.350 1.375 1.400 1.425 1.450 1.475 1.50010 1",
        "We compare with a state-of-the-art algorithm, Gibbs online expectation maximization (GOEM) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], which outperforms a wide range of algorithms including stochastic variational inference [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], hybrid variational-Gibbs [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], and SGRLD [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]",
        "We propose a variance reduced stochastic Expectation Maximization algorithm. stochastic EM-vr achieves a 1 + log(M/\u03ba2) \u2212E local convergence rate, which is faster than both the (1 \u2212 \u03bb)\u22122E rate of batch Expectation Maximization and O(T \u22121) rate of plain stochastic Expectation Maximization"
    ],
    "key_statements": [
        "We review batch and stochastic Expectation Maximization algorithms",
        "We demonstrate our algorithm on Gaussian mixture models and probabilistic latent semantic analysis [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]. stochastic EM-vr achieves significantly faster convergence comparing with stochastic EM, Batch expectation-maximization, and other gradient-based and Bayesian algorithms",
        "We show that under stronger assumptions, there exists a constant step size, such that stochastic EM-vr can globally converge to a stationary point s\u2217 = F (s\u2217), one with \u2207L(s\u2217) = 0 [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "We find that gradient-based algorithms and Batch expectation-maximization are not competitive with stochastic EM and stochastic EM-vr, so we only report their results on NIPS, to make the distinction stochastic EM and stochastic EM-vr more clear",
        "Due to the reduced variance, stochastic EM-vr consistently converges faster to better training objective than stochastic EM and Batch expectation-maximization on all the datasets, while the constant step size of stochastic EM-vr is easier to tune than the decreasing sequence of step sizes for stochastic EM.\n3We find that all the algorithms have the same best hyperparameter configuration. 4We have tried constant step sizes for stochastic mirror descent-vr and reparameterized stochastic gradient descent-vr but found it worse than decreasing step sizes.\n1.300 1e7 1.325 1.350 1.375 1.400 1.425 1.450 1.475 1.50010 1",
        "We compare with a state-of-the-art algorithm, Gibbs online expectation maximization (GOEM) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], which outperforms a wide range of algorithms including stochastic variational inference [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], hybrid variational-Gibbs [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], and SGRLD [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]",
        "For a larger number of topics, such as 100, we find that Gibbs online expectation maximization performs the best since it does not approximate latent Dirichlet allocation as Probabilistic Latent Semantic Analysis, and does not make mean field assumptions as stochastic variational inference and stochastic variational inference",
        "We propose a variance reduced stochastic Expectation Maximization algorithm. stochastic EM-vr achieves a 1 + log(M/\u03ba2) \u2212E local convergence rate, which is faster than both the (1 \u2212 \u03bb)\u22122E rate of batch Expectation Maximization and O(T \u22121) rate of plain stochastic Expectation Maximization"
    ],
    "summary": [
        "We review batch and stochastic EM algorithms .",
        "That is, a full pass through the data set, our algorithm computes the full batch expectation as a control variate, and uses this to reduce the variance of minibatch updates in that epoch.",
        "We show that near a local optimum, our algorithm, with a constant step size, enjoys a convergence rate of O((M \u22121 log M )E/2) to the optimum.",
        "Like bEM, our convergence rate is exponential with respect to the number of epochs, and is asymptotically faster than sEM.",
        "Because of the cheap updates, sEM can converge faster than bEM on large data sets in the beginning.",
        "Due to the variance of the estimator st, sEM has a slower asymptotic convergence rate than bEM for finite data sets.",
        "SEM-vr enjoys an exponential convergence rate with a constant step size.",
        "We analyze the local convergence rate of a sequence {se,t} of sEM-vr iterates to a stationary point s\u2217 with s\u2217 = F (s\u2217).",
        "If M is large, sEM-vr (Eq 7) converges much faster than bEM because 1 + log(M/\u03ba2) \u03ba2/M (1 \u2212 \u03bb)2, thanks to its cheap stochastic updates.",
        "We show that under stronger assumptions, there exists a constant step size, such that sEM-vr can globally converge to a stationary point s\u2217 = F (s\u2217), one with \u2207L(s\u2217) = 0 [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "For any constant step size \u03c1 < \u03b3/ (M (M \u2212 1)L\u03b7Lf ), sEM-vr converges to a stationary point, starting from any valid sufficient statistics vector s0,0.",
        "SEM approximates the full batch expected sufficient statistics \u03b3dk and \u03b3kv with exponential moving averages st,d,k and st,k,v at iteration t, and updates st+1,d,k",
        "Due to the reduced variance, sEM-vr consistently converges faster to better training objective than sEM and bEM on all the datasets, while the constant step size of sEM-vr is easier to tune than the decreasing sequence of step sizes for sEM.",
        "SVI and SSVI converge slower due to their inexact mean field assumption and expensive iterations, including an inner loop for inferring the local latent variables and frequent evaluation of the expensive digamma function.",
        "We compare sEM-vr against bEM, sEM and other gradient and Bayesian algorithms, on GMM and pLSA tasks, and find that sEM-vr converges significantly faster than these alternatives.",
        "An interesting future direction is leveraging recent progress on variance reduced stochastic gradient descent for non-convex optimization [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to relax our assumptions on strongly-log-concavity, and extend sEM-vr to stochastic control variates, which works better on very large data sets.",
        "Extending our work to variational EM and Monte-Carlo EM is interesting"
    ],
    "headline": "We propose a variance reduced stochastic Expectation Maximization  algorithm inspired by variance reduced stochastic gradient descent algorithms",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asuncion%2C%20Arthur%20Newman%2C%20David%20Uci%20machine%20learning%20repository%202007"
        },
        {
            "id": "2",
            "entry": "[2] Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and inference for topic models. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 27\u201334. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asuncion%2C%20Arthur%20Welling%2C%20Max%20Smyth%2C%20Padhraic%20Teh%2C%20Yee%20Whye%20On%20smoothing%20and%20inference%20for%20topic%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asuncion%2C%20Arthur%20Welling%2C%20Max%20Smyth%2C%20Padhraic%20Teh%2C%20Yee%20Whye%20On%20smoothing%20and%20inference%20for%20topic%20models%202009"
        },
        {
            "id": "3",
            "entry": "[3] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993\u20131022, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Andrew%20Y%20Ng%2C%20and%20Michael%20I%20Jordan.%20Latent%20dirichlet%20allocation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Andrew%20Y%20Ng%2C%20and%20Michael%20I%20Jordan.%20Latent%20dirichlet%20allocation%202003"
        },
        {
            "id": "4",
            "entry": "[4] Olivier Capp\u00e9. Online em algorithm for hidden markov models. Journal of Computational and Graphical Statistics, 20(3):728\u2013749, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Capp%C3%A9%2C%20Olivier%20Online%20em%20algorithm%20for%20hidden%20markov%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Capp%C3%A9%2C%20Olivier%20Online%20em%20algorithm%20for%20hidden%20markov%20models%202011"
        },
        {
            "id": "5",
            "entry": "[5] Olivier Capp\u00e9 and Eric Moulines. On-line expectation\u2013maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593\u2013613, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Olivier%20Capp%C3%A9%20and%20Eric%20Moulines.%20On-line%20expectation%E2%80%93maximization%20algorithm%20for%20latent%20data%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Olivier%20Capp%C3%A9%20and%20Eric%20Moulines.%20On-line%20expectation%E2%80%93maximization%20algorithm%20for%20latent%20data%20models%202009"
        },
        {
            "id": "6",
            "entry": "[6] Niladri S Chatterji, Nicolas Flammarion, Yi-An Ma, Peter L Bartlett, and Michael I Jordan. On the theory of variance reduction for stochastic gradient monte carlo. arXiv preprint arXiv:1802.05431, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05431"
        },
        {
            "id": "7",
            "entry": "[7] Changyou Chen, Wenlin Wang, Yizhe Zhang, Qinliang Su, and Lawrence Carin. A convergence analysis for a class of practical variance-reduction stochastic gradient mcmc. arXiv preprint arXiv:1709.01180, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01180"
        },
        {
            "id": "8",
            "entry": "[8] Jianfei Chen, Kaiwei Li, Jun Zhu, and Wenguang Chen. Warplda: a cache efficient o (1) algorithm for latent dirichlet allocation. Proceedings of the VLDB Endowment, 9(10):744\u2013755, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jianfei%20Li%2C%20Kaiwei%20Zhu%2C%20Jun%20Chen%2C%20Wenguang%20Warplda%3A%20a%20cache%20efficient%20o%20%281%29%20algorithm%20for%20latent%20dirichlet%20allocation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Jianfei%20Li%2C%20Kaiwei%20Zhu%2C%20Jun%20Chen%2C%20Wenguang%20Warplda%3A%20a%20cache%20efficient%20o%20%281%29%20algorithm%20for%20latent%20dirichlet%20allocation%202016"
        },
        {
            "id": "9",
            "entry": "[9] Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao, Xinying Song, and Li Deng. End-to-end learning of lda by mirror-descent back propagation over a deep architecture. In Advances in Neural Information Processing Systems, pages 1765\u20131773, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jianshu%20He%2C%20Ji%20Shen%2C%20Yelong%20Xiao%2C%20Lin%20End-to-end%20learning%20of%20lda%20by%20mirror-descent%20back%20propagation%20over%20a%20deep%20architecture%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Jianshu%20He%2C%20Ji%20Shen%2C%20Yelong%20Xiao%2C%20Lin%20End-to-end%20learning%20of%20lda%20by%20mirror-descent%20back%20propagation%20over%20a%20deep%20architecture%202015"
        },
        {
            "id": "10",
            "entry": "[10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "11",
            "entry": "[11] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977"
        },
        {
            "id": "12",
            "entry": "[12] Kumar Avinava Dubey, Sashank J Reddi, Sinead A Williamson, Barnabas Poczos, Alexander J Smola, and Eric P Xing. Variance reduction in stochastic gradient langevin dynamics. In Advances in neural information processing systems, pages 1154\u20131162, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dubey%2C%20Kumar%20Avinava%20Reddi%2C%20Sashank%20J.%20Williamson%2C%20Sinead%20A.%20Poczos%2C%20Barnabas%20and%20Eric%20P%20Xing.%20Variance%20reduction%20in%20stochastic%20gradient%20langevin%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dubey%2C%20Kumar%20Avinava%20Reddi%2C%20Sashank%20J.%20Williamson%2C%20Sinead%20A.%20Poczos%2C%20Barnabas%20and%20Eric%20P%20Xing.%20Variance%20reduction%20in%20stochastic%20gradient%20langevin%20dynamics%202016"
        },
        {
            "id": "13",
            "entry": "[13] Christophe Dupuy and Francis Bach. Online but accurate inference for latent variable models with local gibbs sampling. The Journal of Machine Learning Research, 18(1):4581\u20134625, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dupuy%2C%20Christophe%20Bach%2C%20Francis%20Online%20but%20accurate%20inference%20for%20latent%20variable%20models%20with%20local%20gibbs%20sampling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dupuy%2C%20Christophe%20Bach%2C%20Francis%20Online%20but%20accurate%20inference%20for%20latent%20variable%20models%20with%20local%20gibbs%20sampling%202017"
        },
        {
            "id": "14",
            "entry": "[14] Richard Durbin, Sean R Eddy, Anders Krogh, and Graeme Mitchison. Biological sequence analysis: probabilistic models of proteins and nucleic acids. Cambridge university press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durbin%2C%20Richard%20Eddy%2C%20Sean%20R.%20Krogh%2C%20Anders%20Mitchison%2C%20Graeme%20Biological%20sequence%20analysis%3A%20probabilistic%20models%20of%20proteins%20and%20nucleic%20acids%201998"
        },
        {
            "id": "15",
            "entry": "[15] James Foulds, Levi Boyles, Christopher DuBois, Padhraic Smyth, and Max Welling. Stochastic collapsed variational bayesian inference for latent dirichlet allocation. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 446\u2013454. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foulds%2C%20James%20Boyles%2C%20Levi%20DuBois%2C%20Christopher%20Smyth%2C%20Padhraic%20Stochastic%20collapsed%20variational%20bayesian%20inference%20for%20latent%20dirichlet%20allocation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foulds%2C%20James%20Boyles%2C%20Levi%20DuBois%2C%20Christopher%20Smyth%2C%20Padhraic%20Stochastic%20collapsed%20variational%20bayesian%20inference%20for%20latent%20dirichlet%20allocation%202013"
        },
        {
            "id": "16",
            "entry": "[16] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "17",
            "entry": "[17] Thomas Hofmann. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 289\u2013296. Morgan Kaufmann Publishers Inc., 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hofmann%2C%20Thomas%20Probabilistic%20latent%20semantic%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hofmann%2C%20Thomas%20Probabilistic%20latent%20semantic%20analysis%201999"
        },
        {
            "id": "18",
            "entry": "[18] Katsuhiko Ishiguro, Issei Sato, and Naonori Ueda. Averaged collapsed variational bayes inference. Journal of Machine Learning Research, 18(1):1\u201329, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ishiguro%2C%20Katsuhiko%20Sato%2C%20Issei%20Ueda%2C%20Naonori%20Averaged%20collapsed%20variational%20bayes%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ishiguro%2C%20Katsuhiko%20Sato%2C%20Issei%20Ueda%2C%20Naonori%20Averaged%20collapsed%20variational%20bayes%20inference%202017"
        },
        {
            "id": "19",
            "entry": "[19] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "20",
            "entry": "[20] Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kushner%2C%20Harold%20Yin%2C%20G.George%20Stochastic%20approximation%20and%20recursive%20algorithms%20and%20applications%2C%20volume%2035%202003"
        },
        {
            "id": "21",
            "entry": "[21] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In Advances in Neural Information Processing Systems, pages 2663\u20132671, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20Nicolas%20Le%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20Nicolas%20Le%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "22",
            "entry": "[22] Lihua Lei and Michael Jordan. Less than a single pass: Stochastically controlled stochastic gradient. In Artificial Intelligence and Statistics, pages 148\u2013156, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Lihua%20Jordan%2C%20Michael%20Less%20than%20a%20single%20pass%3A%20Stochastically%20controlled%20stochastic%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Lihua%20Jordan%2C%20Michael%20Less%20than%20a%20single%20pass%3A%20Stochastically%20controlled%20stochastic%20gradient%202017"
        },
        {
            "id": "23",
            "entry": "[23] Percy Liang and Dan Klein. Online em for unsupervised models. In Proceedings of human language technologies: The 2009 annual conference of the North American chapter of the association for computational linguistics, pages 611\u2013619. Association for Computational Linguistics, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Percy%20Klein%2C%20Dan%20Online%20em%20for%20unsupervised%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Percy%20Klein%2C%20Dan%20Online%20em%20for%20unsupervised%20models%202009"
        },
        {
            "id": "24",
            "entry": "[24] Stephan Mandt and David Blei. Smoothed gradients for stochastic variational inference. In Advances in Neural Information Processing Systems, pages 2438\u20132446, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandt%2C%20Stephan%20Blei%2C%20David%20Smoothed%20gradients%20for%20stochastic%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mandt%2C%20Stephan%20Blei%2C%20David%20Smoothed%20gradients%20for%20stochastic%20variational%20inference%202014"
        },
        {
            "id": "25",
            "entry": "[25] Geoffrey McLachlan and David Peel. Finite mixture models. John Wiley & Sons, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McLachlan%2C%20Geoffrey%20Peel%2C%20David%20Finite%20mixture%20models%202004"
        },
        {
            "id": "26",
            "entry": "[26] David Mimno, Matt Hoffman, and David Blei. Sparse stochastic inference for latent dirichlet allocation. arXiv preprint arXiv:1206.6425, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6425"
        },
        {
            "id": "27",
            "entry": "[27] Sam Patterson and Yee Whye Teh. Stochastic gradient riemannian langevin dynamics on the probability simplex. In Advances in Neural Information Processing Systems, pages 3102\u20133110, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patterson%2C%20Sam%20Teh%2C%20Yee%20Whye%20Stochastic%20gradient%20riemannian%20langevin%20dynamics%20on%20the%20probability%20simplex%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patterson%2C%20Sam%20Teh%2C%20Yee%20Whye%20Stochastic%20gradient%20riemannian%20langevin%20dynamics%20on%20the%20probability%20simplex%202013"
        },
        {
            "id": "28",
            "entry": "[28] Lawrence R Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257\u2013286, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rabiner%2C%20Lawrence%20R.%20A%20tutorial%20on%20hidden%20markov%20models%20and%20selected%20applications%20in%20speech%20recognition%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rabiner%2C%20Lawrence%20R.%20A%20tutorial%20on%20hidden%20markov%20models%20and%20selected%20applications%20in%20speech%20recognition%201989"
        },
        {
            "id": "29",
            "entry": "[29] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method.%20The%20annals%20of%20mathematical%20statistics%201951"
        },
        {
            "id": "30",
            "entry": "[30] Issei Sato and Hiroshi Nakagawa. Rethinking collapsed variational bayes inference for lda. In ICML, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sato%2C%20Issei%20Nakagawa%2C%20Hiroshi%20Rethinking%20collapsed%20variational%20bayes%20inference%20for%20lda%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sato%2C%20Issei%20Nakagawa%2C%20Hiroshi%20Rethinking%20collapsed%20variational%20bayes%20inference%20for%20lda%202012"
        },
        {
            "id": "31",
            "entry": "[31] Charles Spearman and L. W. Jones. Human Ability. Macmillan, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spearman%2C%20Charles%20Jones%2C%20L.W.%20Human%20Ability%201950"
        },
        {
            "id": "32",
            "entry": "[32] D Michael Titterington. Recursive parameter estimation using incomplete data. Journal of the Royal Statistical Society. Series B (Methodological), pages 257\u2013267, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titterington%2C%20D.Michael%20Recursive%20parameter%20estimation%20using%20incomplete%20data%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titterington%2C%20D.Michael%20Recursive%20parameter%20estimation%20using%20incomplete%20data%201984"
        },
        {
            "id": "33",
            "entry": "[33] Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. Evaluation methods for topic models. In Proceedings of the 26th annual international conference on machine learning, pages 1105\u20131112. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wallach%2C%20Hanna%20M.%20Murray%2C%20Iain%20Salakhutdinov%2C%20Ruslan%20Mimno%2C%20David%20Evaluation%20methods%20for%20topic%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wallach%2C%20Hanna%20M.%20Murray%2C%20Iain%20Salakhutdinov%2C%20Ruslan%20Mimno%2C%20David%20Evaluation%20methods%20for%20topic%20models%202009"
        },
        {
            "id": "34",
            "entry": "[34] CF Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics, pages 95\u2013103, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20C.F.Jeff%20On%20the%20convergence%20properties%20of%20the%20em%20algorithm%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20C.F.Jeff%20On%20the%20convergence%20properties%20of%20the%20em%20algorithm%201983"
        },
        {
            "id": "35",
            "entry": "[35] Manzil Zaheer, Michael Wick, Jean-Baptiste Tristan, Alex Smola, and Guy Steele. Exponential stochastic cellular automata for massively parallel inference. In Artificial Intelligence and Statistics, pages 966\u2013975, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaheer%2C%20Manzil%20Wick%2C%20Michael%20Tristan%2C%20Jean-Baptiste%20Smola%2C%20Alex%20Exponential%20stochastic%20cellular%20automata%20for%20massively%20parallel%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaheer%2C%20Manzil%20Wick%2C%20Michael%20Tristan%2C%20Jean-Baptiste%20Smola%2C%20Alex%20Exponential%20stochastic%20cellular%20automata%20for%20massively%20parallel%20inference%202016"
        },
        {
            "id": "36",
            "entry": "[36] Aonan Zhang, Jun Zhu, and Bo Zhang. Sparse online topic models. In Proceedings of the 22nd international conference on World Wide Web, pages 1489\u20131500. ACM, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Aonan%20Zhu%2C%20Jun%20Zhang%2C%20Bo%20Sparse%20online%20topic%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Aonan%20Zhu%2C%20Jun%20Zhang%2C%20Bo%20Sparse%20online%20topic%20models%202013"
        }
    ]
}
