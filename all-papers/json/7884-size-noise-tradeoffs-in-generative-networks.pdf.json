{
    "filename": "7884-size-noise-tradeoffs-in-generative-networks.pdf",
    "metadata": {
        "title": "Universal approximation bounds for superpositions of a sigmoidal function",
        "author": "A.R. Barron",
        "date": 2002,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7884-size-noise-tradeoffs-in-generative-networks.pdf",
            "doi": "10.1109/18.256500"
        },
        "journal": "IEEE Transactions on Information Theory",
        "volume": "39",
        "abstract": "This paper investigates the ability of generative networks to convert their input noise distributions into other distributions. Firstly, we demonstrate a construction that allows ReLU networks to increase the dimensionality of their noise distribution by implementing a \u201cspace-filling\u201d function based on iterated tent maps. We show this construction is optimal by analyzing the number of affine pieces in functions computed by multivariate ReLU networks. Secondly, we provide efficient ways (using polylog(1/ ) nodes) for networks to pass between univariate uniform and normal distributions, using a Taylor series approximation and a binary search gadget for computing function inverses. Lastly, we indicate how high dimensional distributions can be efficiently transformed into low dimensional distributions.",
        "pages": "930-945"
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "uniform distribution",
            "url": "https://en.wikipedia.org/wiki/uniform_distribution"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "parallel construction",
            "url": "https://en.wikipedia.org/wiki/parallel_construction"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "normal distribution",
            "url": "https://en.wikipedia.org/wiki/normal_distribution"
        }
    ],
    "highlights": [
        "This paper focuses on the representational capabilities of generative networks",
        "We focus on the two most standard choices for noise distributions: The normal distribution, and the uniform distribution on the unit hypercube [Arjovsky et al, 2017]",
        "We prove tight upper and lower bounds for the task of approximating higher dimensional uniform distributions with lower dimensional distributions in terms of the average width W and depth L of the network",
        "A generative network with polylog(1/ ) nodes and univariate uniform noise can output a distribution with Wasserstein distance from a normal distribution",
        "An interesting open question is whether the results of Section 3 can be applied more generally to multidimensional distributions",
        "Is a parallel construction of the form described here the most efficient way to create a network that pushes forward a d-dimensional uniform distribution to a d-dimensional normal? For that matter, if f : Rd \u2192 Rd is of the form of a univariate function evaluated componentwise on the input, is the best neural network approximation for f of a given size a parallel construction?"
    ],
    "key_statements": [
        "This paper focuses on the representational capabilities of generative networks",
        "We focus on the two most standard choices for noise distributions: The normal distribution, and the uniform distribution on the unit hypercube [Arjovsky et al, 2017]",
        "We prove tight upper and lower bounds for the task of approximating higher dimensional uniform distributions with lower dimensional distributions in terms of the average width W and depth L of the network",
        "Proof. This follows from applying Theorem 3 taking f as a neural network with the affine piece bound from Lemma 1, and P as \u03bc, the uniform distribution on [0, 1]n.\n3 Transporting between Univariate Distributions",
        "A generative network with polylog(1/ ) nodes and univariate uniform noise can output a distribution with Wasserstein distance from a normal distribution",
        "Considering the case of approximating a univariate normal distribution with a high dimensional distribution, we note that there is the simplistic approach which involves summing the inputs and reasoning that the output is close to a normal distribution by the Berry-Esseen theorem",
        "An interesting open question is whether the results of Section 3 can be applied more generally to multidimensional distributions",
        "Is a parallel construction of the form described here the most efficient way to create a network that pushes forward a d-dimensional uniform distribution to a d-dimensional normal? For that matter, if f : Rd \u2192 Rd is of the form of a univariate function evaluated componentwise on the input, is the best neural network approximation for f of a given size a parallel construction?",
        "We ran some simple initial experiments measuring how well Generative Adversarial Networks of different architectures and noise distributions learned MNIST generation, and we found them inconclusive; in particular, we could not be certain if our empirical observations were a consequence purely of representation, or some combination of representation and training"
    ],
    "summary": [
        "This paper focuses on the representational capabilities of generative networks.",
        "This work produced a result on the number of affine pieces of deep networks [<a class=\"ref-link\" id=\"cZhang_et+al_2018_a\" href=\"#rZhang_et+al_2018_a\">Zhang et al, 2018</a>, Theorem 6.3], which matches our bound in Lemma 1.",
        "The one-dimensional tent map construction tells us that for a given number of nodes and number of layers, we can construct a function with a number of affine pieces bounded by the size of the network.",
        "To show that our construction is optimal, we need to show that it approximates the high-dimensional uniform distribution about as accurately as any piecewise affine function with the same number of pieces NA.",
        "This follows from applying Theorem 3 taking f as a neural network with the affine piece bound from Lemma 1, and P as \u03bc, the uniform distribution on [0, 1]n.",
        "We can construct a generative network with polylog(1/ ) nodes and univariate normal noise that can output a distribution with Wasserstein distance from uniform.",
        "A generative network with polylog(1/ ) nodes and univariate uniform noise can output a distribution with Wasserstein distance from a normal distribution.",
        "If we can establish a bound on the accuracy with which a piecewise affine function can approximate the normal CDF, we can use the univariate affine pieces lemma above to lower bound the accuracy of any uniform univariate noise approximation of the normal.",
        "A generative network taking uniform noise can approximate a normal with Wasserstein accuracy exponential in the number of nodes.",
        "Considering the case of approximating a univariate normal distribution with a high dimensional distribution, we note that there is the simplistic approach which involves summing the inputs and reasoning that the output is close to a normal distribution by the Berry-Esseen theorem.",
        "This theorem suggests another way of approaching Theorem 9: Use the results of section 2 to increase a 1-dimensional uniform distribution to a d-dimensional uniform distribution, apply Theorem 12 as the final layer of that construction to get an approximately normal distribution.",
        "Is a parallel construction of the form described here the most efficient way to create a network that pushes forward a d-dimensional uniform distribution to a d-dimensional normal?",
        "If f : Rd \u2192 Rd is of the form of a univariate function evaluated componentwise on the input, is the best neural network approximation for f of a given size a parallel construction?",
        "Is a parallel construction of the form described here the most efficient way to create a network that pushes forward a d-dimensional uniform distribution to a d-dimensional normal? For that matter, if f : Rd \u2192 Rd is of the form of a univariate function evaluated componentwise on the input, is the best neural network approximation for f of a given size a parallel construction?"
    ],
    "headline": "This paper investigates the ability of generative networks to convert their input noise distributions into other distributions",
    "reference_links": [
        {
            "id": "Anthony_2009_a",
            "entry": "Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, New York, NY, USA, 1st edition, 2009. ISBN 052111862X, 9780521118620.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20Network%20Learning%3A%20Theoretical%20Foundations%202009"
        },
        {
            "id": "Martin_2017_a",
            "entry": "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 214\u2013 223, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/arjovsky17a.html.",
            "url": "http://proceedings.mlr.press/v70/arjovsky17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017-08"
        },
        {
            "id": "Barron_1993_a",
            "entry": "A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930\u2013945, May 1993. ISSN 0018-9448. doi: 10.1109/ 18.256500.",
            "crossref": "https://dx.doi.org/10.1109/18.256500",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/18.256500"
        },
        {
            "id": "Box_1958_a",
            "entry": "George EP Box, Mervin E Muller, et al. A note on the generation of random normal deviates. The annals of mathematical statistics, 29(2):610\u2013611, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Box%2C%20George%20E.P.%20Muller%2C%20Mervin%20E.%20A%20note%20on%20the%20generation%20of%20random%20normal%20deviates%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Box%2C%20George%20E.P.%20Muller%2C%20Mervin%20E.%20A%20note%20on%20the%20generation%20of%20random%20normal%20deviates%201958"
        },
        {
            "id": "Chen_2017_a",
            "entry": "Zhimin Chen and Yuguang Tong. Face super-resolution through wasserstein gans. arXiv preprint arXiv:1705.02438, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02438"
        },
        {
            "id": "Creswell_et+al_2018_a",
            "entry": "Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1): 53\u201365, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Creswell%2C%20Antonia%20White%2C%20Tom%20Dumoulin%2C%20Vincent%20Arulkumaran%2C%20Kai%20and%20Anil%20A%20Bharath.%20Generative%20adversarial%20networks%3A%20An%20overview%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Creswell%2C%20Antonia%20White%2C%20Tom%20Dumoulin%2C%20Vincent%20Arulkumaran%2C%20Kai%20and%20Anil%20A%20Bharath.%20Generative%20adversarial%20networks%3A%20An%20overview%202018"
        },
        {
            "id": "Cybenko_1989_a",
            "entry": "George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303\u2013314, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20George%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20George%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989"
        },
        {
            "id": "Donahue_et+al_2018_a",
            "entry": "Chris Donahue, Julian McAuley, and Miller Puckette. Synthesizing audio with generative adversarial networks. arXiv preprint arXiv:1802.04208, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04208"
        },
        {
            "id": "Eldan_2016_a",
            "entry": "Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In COLT, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Hornik_et+al_1989_a",
            "entry": "K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359\u2013366, july 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989-07"
        },
        {
            "id": "Kolmogorov_1975_a",
            "entry": "A.N. Kolmogorov and S.V. Fomin. Introductory Real Analysis. Dover Books on Mathematics. Dover Publications, 1975. ISBN 9780486612263. URL https://books.google.com/books?id=z8IaHgZ9PwQC.",
            "url": "https://books.google.com/books?id=z8IaHgZ9PwQC"
        },
        {
            "id": "Lee_et+al_2017_a",
            "entry": "Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets to express distributions. 65:1271\u20131296, 07\u201310 Jul 2017. URL http://proceedings.mlr.press/v65/lee17a.html.",
            "url": "http://proceedings.mlr.press/v65/lee17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Holden%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Risteski%2C%20Andrej%20On%20the%20ability%20of%20neural%20nets%20to%20express%20distributions.%2065%202017-07"
        },
        {
            "id": "Montufar_et+al_2014_a",
            "entry": "Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pages 2924\u20132932, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20Guido%20F.%20Pascanu%2C%20Razvan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20Guido%20F.%20Pascanu%2C%20Razvan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014"
        },
        {
            "id": "Osokin_et+al_2017_a",
            "entry": "Anton Osokin, Anatole Chessel, Rafael E Carazo Salas, and Federico Vaggi. Gans for biological image synthesis. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2252\u20132261. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20Anton%20Chessel%2C%20Anatole%20Salas%2C%20Rafael%20E.Carazo%20Vaggi%2C%20Federico%20Gans%20for%20biological%20image%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20Anton%20Chessel%2C%20Anatole%20Salas%2C%20Rafael%20E.Carazo%20Vaggi%2C%20Federico%20Gans%20for%20biological%20image%20synthesis%202017"
        },
        {
            "id": "Pinelis_2013_a",
            "entry": "Iosif Pinelis. On the nonuniform berry\u2013esseen bound. https://arxiv.org/pdf/1301.2828.pdf, May 2013. (Accessed on 05/15/2018).",
            "url": "https://arxiv.org/pdf/1301.2828.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1301.2828"
        },
        {
            "id": "Safran_2016_a",
            "entry": "Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non-linear functions. CoRR, abs/1610.09887, 2016. URL http://arxiv.org/abs/1610.09887.",
            "url": "http://arxiv.org/abs/1610.09887",
            "arxiv_url": "https://arxiv.org/pdf/1610.09887"
        },
        {
            "id": "Matus_2016_a",
            "entry": "Matus Telgarsky. benefits of depth in neural networks. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 1517\u20131539, Columbia University, New York, New York, USA, 23\u201326 Jun 2016. PMLR. URL http://proceedings.mlr.press/v49/telgarsky16.html.",
            "url": "http://proceedings.mlr.press/v49/telgarsky16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Matus%20Telgarsky.%20benefits%20of%20depth%20in%20neural%20networks%202016-06-23"
        },
        {
            "id": "Villani_2003_a",
            "entry": "C. Villani. Topics in Optimal Transportation. Graduate studies in mathematics. American Mathematical Society, 2003. ISBN 9780821833124. URL https://books.google.com/books?id= GqRXYFxe0l0C.",
            "url": "https://books.google.com/books?id="
        },
        {
            "id": "Yarotsky_2017_a",
            "entry": "Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94: 103\u2013114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. arXiv preprint arXiv:1805.07091, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07091"
        }
    ]
}
