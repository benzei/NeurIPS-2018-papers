{
    "filename": "7842-dimensionally-tight-bounds-for-second-order-hamiltonian-monte-carlo.pdf",
    "metadata": {
        "title": "Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo",
        "author": "Oren Mangoubi, Nisheeth Vishnoi",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7842-dimensionally-tight-bounds-for-second-order-hamiltonian-monte-carlo.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from highdimensional distributions in Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order \u201cleapfrog\" implementation has long been conjectured to run in d1 4 gradient evaluations. Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data. Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone. Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an \u201cincoherence\" property. Our result compares favorably with the best available bounds for the class of strongly log-concave distributions, which grow like d1 2 gradient evaluations with the dimension. Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, leapfrog HMC performs better than its competitors \u2013 both in terms of accuracy and in terms of the number of gradient evaluations it requires."
    },
    "keywords": [
        {
            "term": "logistic regression",
            "url": "https://en.wikipedia.org/wiki/logistic_regression"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "Metropolis-adjusted Langevin algorithm",
            "url": "https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm"
        },
        {
            "term": "second order",
            "url": "https://en.wikipedia.org/wiki/second_order"
        },
        {
            "term": "MCMC",
            "url": "https://en.wikipedia.org/wiki/MCMC"
        },
        {
            "term": "Hamiltonian Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo"
        },
        {
            "term": "hybrid monte carlo",
            "url": "https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo"
        },
        {
            "term": "Markov chain Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"
        },
        {
            "term": "markov chain",
            "url": "https://en.wikipedia.org/wiki/markov_chain"
        }
    ],
    "highlights": [
        "[<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] show that idealized Hamiltonian Monte Carlo can sample from general m-strongly logconcave target distributions with M -Lipschitz gradient in O(\u03ba2) steps, where \u03ba \u2236=",
        "Initialization: In this paper we prove gradient evaluation bounds for unadjusted HMC algorithm from both a warm start and cold start, which we define as follows: Definition 1. (Warm start) Let X0, X1, . . . be a Markov chain, and let \u03c0 be our target distribution",
        "We start by noting that if one attempts to bound the number of gradient evaluations required by Hamiltonian Monte Carlo using a conventional Lipschitz bound on the Hessian",
        "We show that the conjecture of [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], which says that Hamiltonian Monte Carlo requires O\u2217(d1 4) gradient evaluations, is true when sampling from strongly log-concave targets satisfying weak regularity properties associated with the input data",
        "We introduce a new regularity property for the Hessian (Assumption 1) that is much weaker than the Lipschitz Hessian property, and show that for a class of functions arising in statistics and machine learning this property holds for natural conditions on the data",
        "Our simulations show that unadjusted HMC algorithm is competitive with MHMC on synthetic data that satisfies our regularity assumption"
    ],
    "key_statements": [
        "[<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] show that idealized Hamiltonian Monte Carlo can sample from general m-strongly logconcave target distributions with M -Lipschitz gradient in O(\u03ba2) steps, where \u03ba \u2236=",
        "Initialization: In this paper we prove gradient evaluation bounds for unadjusted HMC algorithm from both a warm start and cold start, which we define as follows: Definition 1. (Warm start) Let X0, X1, . . . be a Markov chain, and let \u03c0 be our target distribution",
        "We start by noting that if one attempts to bound the number of gradient evaluations required by Hamiltonian Monte Carlo using a conventional Lipschitz bound on the Hessian",
        "To bound the error of a second-order method such as the leapfrog method used by Hamiltonian Monte Carlo, we must bound the change of the directional derivative of the gradient along the path taken by the trajectories of the Markov chain",
        "Among all four of the algorithms, we found that unadjusted HMC algorithm had the highest accuracy at the accuracy-optimizing step size",
        "We found that the autocorrelation time of unadjusted HMC algorithm was fastest at the autocorrelation time-optimizing step size (Figure 1, middle)",
        "We show that the conjecture of [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], which says that Hamiltonian Monte Carlo requires O\u2217(d1 4) gradient evaluations, is true when sampling from strongly log-concave targets satisfying weak regularity properties associated with the input data",
        "We introduce a new regularity property for the Hessian (Assumption 1) that is much weaker than the Lipschitz Hessian property, and show that for a class of functions arising in statistics and machine learning this property holds for natural conditions on the data",
        "Our simulations show that unadjusted HMC algorithm is competitive with MHMC on synthetic data that satisfies our regularity assumption",
        "We show that the constant in our regularity assumption grows much more slowly with the dimension than the Euclidean Lipschitz constant of the Hessian"
    ],
    "summary": [
        "[<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] show that idealized HMC can sample from general m-strongly logconcave target distributions with M -Lipschitz gradient in O(\u03ba2) steps, where \u03ba \u2236=.",
        "Initialization: In this paper we prove gradient evaluation bounds for UHMC from both a warm start and cold start, which we define as follows: Definition 1.",
        "Let Hx denote the Hessian of U at x \u2208 Rd. We start by noting that if one attempts to bound the number of gradient evaluations required by HMC using a conventional Lipschitz bound on the Hessian",
        "To bound the error of a second-order method such as the leapfrog method used by HMC, we must bound the change of the directional derivative of the gradient along the path taken by the trajectories of the Markov chain.",
        "Our main result is a bound on the number of gradient evaluations required by HMC with second-order leapfrog integrator under the infinity-norm Lipschitz condition (Assumption 1), when sampling from \u03c0(x) \u221d e\u2212U(x) if U is m-strongly convex with M -Lipschitz gradient.",
        "We bound the required number of gradient evaluations for both a warm and cold start.",
        "We use the fact that the momentum of the HMC trajectories is roughly N (0, Id) to show that the continuous HMC trajectories of the idealized chain X are unlikely to travel quickly in any of the \u201cbad\" directions ui specified in Assumption 1 (Step 2).",
        "To show that the trajectories of our warm-started chain approximately satisfy this Gibbs distribution property, we couple the two copies X and Yof the idealized HMC chain by defining the Ychain using the update rule Yi+1 = qT (Yi, pi) with the same sequence of initial momenta p1, p2, .",
        "Since the algorithm uses a total of numerical steps to compute Xi for i \u2264 I, for T = \u0398(1) the number of gradient evaluations is roughly",
        "The purpose of our first set of simulations is to show that in practical situations analyzed in this paper the unadjusted HMC algorithm (UHMC) is competitive with other popular sampling algorithms in terms of both accuracy and in terms of the number of gradient evaluations required.",
        "We show that the conjecture of [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], which says that HMC requires O\u2217(d1 4) gradient evaluations, is true when sampling from strongly log-concave targets satisfying weak regularity properties associated with the input data.",
        "It would be interesting to extend our results to non-logconcave targets, and to kth-order numerical implementations of generalizations of HMC, such as RHMC"
    ],
    "headline": "We show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna, and Andrew Stuart. Optimal tuning of the hybrid Monte Carlo algorithm. Bernoulli, 19(5A):1501\u20131534, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beskos%2C%20Alexandros%20Pillai%2C%20Natesh%20Roberts%2C%20Gareth%20Sanz-Serna%2C%20Jesus-Maria%20Optimal%20tuning%20of%20the%20hybrid%20Monte%20Carlo%20algorithm%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beskos%2C%20Alexandros%20Pillai%2C%20Natesh%20Roberts%2C%20Gareth%20Sanz-Serna%2C%20Jesus-Maria%20Optimal%20tuning%20of%20the%20hybrid%20Monte%20Carlo%20algorithm%202013"
        },
        {
            "id": "2",
            "entry": "[2] Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint arXiv:1701.02434, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.02434"
        },
        {
            "id": "3",
            "entry": "[3] MJ Betancourt, Simon Byrne, and Mark Girolami. Optimizing the integrator step size for Hamiltonian Monte Carlo. arXiv preprint arXiv:1411.6669, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.6669"
        },
        {
            "id": "4",
            "entry": "[4] Nawaf Bou-Rabee, Andreas Eberle, and Raphael Zimmer. Coupling and convergence for Hamiltonian Monte Carlo. arXiv preprint arXiv:1805.00452, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00452"
        },
        {
            "id": "5",
            "entry": "[5] Bob Carpenter, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Michael A Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal of Statistical Software, 20:1\u201337, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carpenter%2C%20Bob%20Gelman%2C%20Andrew%20Hoffman%2C%20Matt%20Lee%2C%20Daniel%20Stan%3A%20A%20probabilistic%20programming%20language%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carpenter%2C%20Bob%20Gelman%2C%20Andrew%20Hoffman%2C%20Matt%20Lee%2C%20Daniel%20Stan%3A%20A%20probabilistic%20programming%20language%202016"
        },
        {
            "id": "6",
            "entry": "[6] Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In International Conference on Machine Learning, pages 1683\u20131691, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Fox%2C%20Emily%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Tianqi%20Fox%2C%20Emily%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202014"
        },
        {
            "id": "7",
            "entry": "[7] Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. In Proceedings of Algorithmic Learning Theory, pages 186\u2013211, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Xiang%20Bartlett%2C%20Peter%20Convergence%20of%20Langevin%20MCMC%20in%20KL-divergence%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Xiang%20Bartlett%2C%20Peter%20Convergence%20of%20Langevin%20MCMC%20in%20KL-divergence%202018"
        },
        {
            "id": "8",
            "entry": "[8] Xiang Cheng, Niladri S Chatterji, Yasin Abbasi-Yadkori, Peter L Bartlett, and Michael I Jordan. Sharp convergence rates for Langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01648"
        },
        {
            "id": "9",
            "entry": "[9] Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, and Michael I. Jordan. Underdamped Langevin MCMC: A non-asymptotic analysis. In Conference on Learning Theory, pages 300\u2013323, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Xiang%20Chatterji%2C%20Niladri%20S.%20Bartlett%2C%20Peter%20L.%20Jordan%2C%20Michael%20I.%20Underdamped%20Langevin%20MCMC%3A%20A%20non-asymptotic%20analysis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Xiang%20Chatterji%2C%20Niladri%20S.%20Bartlett%2C%20Peter%20L.%20Jordan%2C%20Michael%20I.%20Underdamped%20Langevin%20MCMC%3A%20A%20non-asymptotic%20analysis%202018"
        },
        {
            "id": "10",
            "entry": "[10] Nicolas Chopin, James Ridgway, et al. Leave Pima indians alone: binary regression as a benchmark for Bayesian computation. Statistical Science, 32(1):64\u201387, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chopin%2C%20Nicolas%20Ridgway%2C%20James%20Leave%20Pima%20indians%20alone%3A%20binary%20regression%20as%20a%20benchmark%20for%20Bayesian%20computation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chopin%2C%20Nicolas%20Ridgway%2C%20James%20Leave%20Pima%20indians%20alone%3A%20binary%20regression%20as%20a%20benchmark%20for%20Bayesian%20computation%202017"
        },
        {
            "id": "11",
            "entry": "[11] Michael Creutz. Global Monte Carlo algorithms for many-Fermion systems. Physical Review D, 38(4):1228, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Creutz%2C%20Michael%20Global%20Monte%20Carlo%20algorithms%20for%20many-Fermion%20systems%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Creutz%2C%20Michael%20Global%20Monte%20Carlo%20algorithms%20for%20many-Fermion%20systems%201988"
        },
        {
            "id": "12",
            "entry": "[12] Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent. In Conference on Learning Theory, pages 678\u2013689, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalalyan%2C%20Arnak%20Further%20and%20stronger%20analogy%20between%20sampling%20and%20optimization%3A%20Langevin%20Monte%20Carlo%20and%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalalyan%2C%20Arnak%20Further%20and%20stronger%20analogy%20between%20sampling%20and%20optimization%3A%20Langevin%20Monte%20Carlo%20and%20gradient%20descent%202017"
        },
        {
            "id": "13",
            "entry": "[13] Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):651\u2013676, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalalyan%2C%20Arnak%20S.%20Theoretical%20guarantees%20for%20approximate%20sampling%20from%20smooth%20and%20log-concave%20densities%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalalyan%2C%20Arnak%20S.%20Theoretical%20guarantees%20for%20approximate%20sampling%20from%20smooth%20and%20log-concave%20densities%202017"
        },
        {
            "id": "14",
            "entry": "[14] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid Monte Carlo. Physics letters B, 195(2):216\u2013222, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%20Duane%20Anthony%20D%20Kennedy%20Brian%20J%20Pendleton%20and%20Duncan%20Roweth%20Hybrid%20Monte%20Carlo%20Physics%20letters%20B%201952216222%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simon%20Duane%20Anthony%20D%20Kennedy%20Brian%20J%20Pendleton%20and%20Duncan%20Roweth%20Hybrid%20Monte%20Carlo%20Physics%20letters%20B%201952216222%201987"
        },
        {
            "id": "15",
            "entry": "[15] Alain Durmus, Szymon Majewski, and B\u0142azej Miasojedow. Analysis of Langevin Monte Carlo via convex optimization. arXiv preprint arXiv:1802.09188, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09188"
        },
        {
            "id": "16",
            "entry": "[16] Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the unadjusted Langevin algorithm. arXiv preprint arXiv:1605.01559, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.01559"
        },
        {
            "id": "17",
            "entry": "[17] Alain Durmus and Eric Moulines. Non-asymptotic convergence analysis for the Unadjusted Langevin Algorithm. The Annals of Applied Probability, in press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Alain%20Durmus%20and%20Eric%20Moulines.%20Non-asymptotic%20convergence%20analysis%20for%20the%20Unadjusted%20Langevin%20Algorithm",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Alain%20Durmus%20and%20Eric%20Moulines.%20Non-asymptotic%20convergence%20analysis%20for%20the%20Unadjusted%20Langevin%20Algorithm"
        },
        {
            "id": "18",
            "entry": "[18] Alain Durmus, Eric Moulines, and Eero Saksman. On the convergence of Hamiltonian Monte Carlo. arXiv preprint arXiv:1705.00166, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.00166"
        },
        {
            "id": "19",
            "entry": "[19] Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu. Log-concave sampling: MetropolisHastings algorithms are fast! In Conference on Learning Theory, pages 793\u2013797, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwivedi%2C%20Raaz%20Chen%2C%20Yuansi%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Log-concave%20sampling%3A%20MetropolisHastings%20algorithms%20are%20fast%21%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwivedi%2C%20Raaz%20Chen%2C%20Yuansi%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Log-concave%20sampling%3A%20MetropolisHastings%20algorithms%20are%20fast%21%202018"
        },
        {
            "id": "20",
            "entry": "[20] Christel Faes, John T Ormerod, and Matt P Wand. Variational Bayesian inference for parametric and nonparametric regression with missing data. Journal of the American Statistical Association, 106(495):959\u2013 971, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faes%2C%20Christel%20Ormerod%2C%20John%20T.%20Wand%2C%20Matt%20P.%20Variational%20Bayesian%20inference%20for%20parametric%20and%20nonparametric%20regression%20with%20missing%20data%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faes%2C%20Christel%20Ormerod%2C%20John%20T.%20Wand%2C%20Matt%20P.%20Variational%20Bayesian%20inference%20for%20parametric%20and%20nonparametric%20regression%20with%20missing%20data%202011"
        },
        {
            "id": "21",
            "entry": "[21] Andrew Gelman, Walter R Gilks, and Gareth O Roberts. Weak convergence and optimal scaling of random walk Metropolis algorithms. The Annals of Applied Probability, 7(1):110\u2013120, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gelman%2C%20Andrew%20Gilks%2C%20Walter%20R.%20Roberts%2C%20Gareth%20O.%20Weak%20convergence%20and%20optimal%20scaling%20of%20random%20walk%20Metropolis%20algorithms%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gelman%2C%20Andrew%20Gilks%2C%20Walter%20R.%20Roberts%2C%20Gareth%20O.%20Weak%20convergence%20and%20optimal%20scaling%20of%20random%20walk%20Metropolis%20algorithms%201997"
        },
        {
            "id": "22",
            "entry": "[22] Alexander Genkin, David D Lewis, and David Madigan. Large-scale Bayesian logistic regression for text categorization. Technometrics, 49(3):291\u2013304, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genkin%2C%20Alexander%20Lewis%2C%20David%20D.%20Madigan%2C%20David%20Large-scale%20Bayesian%20logistic%20regression%20for%20text%20categorization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genkin%2C%20Alexander%20Lewis%2C%20David%20D.%20Madigan%2C%20David%20Large-scale%20Bayesian%20logistic%20regression%20for%20text%20categorization%202007"
        },
        {
            "id": "23",
            "entry": "[23] Jonathan Goodman and Jonathan Weare. Ensemble samplers with affine invariance. Communications in applied mathematics and computational science, 5(1):65\u201380, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodman%2C%20Jonathan%20Weare%2C%20Jonathan%20Ensemble%20samplers%20with%20affine%20invariance%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodman%2C%20Jonathan%20Weare%2C%20Jonathan%20Ensemble%20samplers%20with%20affine%20invariance%202010"
        },
        {
            "id": "24",
            "entry": "[24] Ernst Hairer, Christian Lubich, and Gerhard Wanner. Geometric numerical integration illustrated by the St\u00f6rmer\u2013Verlet method. Acta numerica, 12:399\u2013450, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hairer%2C%20Ernst%20Lubich%2C%20Christian%20Wanner%2C%20Gerhard%20Geometric%20numerical%20integration%20illustrated%20by%20the%20St%C3%B6rmer%E2%80%93Verlet%20method%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hairer%2C%20Ernst%20Lubich%2C%20Christian%20Wanner%2C%20Gerhard%20Geometric%20numerical%20integration%20illustrated%20by%20the%20St%C3%B6rmer%E2%80%93Verlet%20method%202003"
        },
        {
            "id": "25",
            "entry": "[25] Matthew D Hoffman and Andrew Gelman. The no-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1):1593\u20131623, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Gelman%2C%20Andrew%20The%20no-U-turn%20sampler%3A%20adaptively%20setting%20path%20lengths%20in%20Hamiltonian%20Monte%20Carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Gelman%2C%20Andrew%20The%20no-U-turn%20sampler%3A%20adaptively%20setting%20path%20lengths%20in%20Hamiltonian%20Monte%20Carlo%202014"
        },
        {
            "id": "26",
            "entry": "[26] Valen E Johnson. On numerical aspects of Bayesian model selection in high and ultrahigh-dimensional settings. Bayesian analysis (Online), 8(4):741, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Valen%20E.%20On%20numerical%20aspects%20of%20Bayesian%20model%20selection%20in%20high%20and%20ultrahigh-dimensional%20settings%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Valen%20E.%20On%20numerical%20aspects%20of%20Bayesian%20model%20selection%20in%20high%20and%20ultrahigh-dimensional%20settings%202013"
        },
        {
            "id": "27",
            "entry": "[27] AD Kennedy and Brian Pendleton. Acceptances and autocorrelations in hybrid Monte Carlo. Nuclear Physics B-Proceedings Supplements, 20:118\u2013121, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kennedy%2C%20A.D.%20Pendleton%2C%20Brian%20Acceptances%20and%20autocorrelations%20in%20hybrid%20Monte%20Carlo%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kennedy%2C%20A.D.%20Pendleton%2C%20Brian%20Acceptances%20and%20autocorrelations%20in%20hybrid%20Monte%20Carlo%201991"
        },
        {
            "id": "28",
            "entry": "[28] Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. To appear in Proceedings of STOC 2018, arXiv preprint arXiv:1710.06261, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06261"
        },
        {
            "id": "29",
            "entry": "[29] Benedict Leimkuhler and Sebastian Reich. Simulating Hamiltonian dynamics, volume 14. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benedict%20Leimkuhler%20and%20Sebastian%20Reich%20Simulating%20Hamiltonian%20dynamics%20volume%2014%20Cambridge%20university%20press%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benedict%20Leimkuhler%20and%20Sebastian%20Reich%20Simulating%20Hamiltonian%20dynamics%20volume%2014%20Cambridge%20university%20press%202004"
        },
        {
            "id": "30",
            "entry": "[30] Samuel Livingstone, Michael Betancourt, Simon Byrne, and Mark Girolami. On the geometric ergodicity of Hamiltonian Monte Carlo. arXiv preprint arXiv:1601.08057, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.08057"
        },
        {
            "id": "31",
            "entry": "[31] L\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. Hit-and-run is fast and fun. preprint, Microsoft Research, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lov%C3%A1sz%2C%20L%C3%A1szl%C3%B3%20Vempala%2C%20Santosh%20Hit-and-run%20is%20fast%20and%20fun%202003"
        },
        {
            "id": "32",
            "entry": "[32] David Madigan, Alexander Genkin, David D Lewis, and Dmitriy Fradkin. Bayesian multinomial logistic regression for author identification. In AIP Conference Proceedings, volume 803, pages 509\u2013516. AIP, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madigan%2C%20David%20Genkin%2C%20Alexander%20Lewis%2C%20David%20D.%20Fradkin%2C%20Dmitriy%20Bayesian%20multinomial%20logistic%20regression%20for%20author%20identification%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madigan%2C%20David%20Genkin%2C%20Alexander%20Lewis%2C%20David%20D.%20Fradkin%2C%20Dmitriy%20Bayesian%20multinomial%20logistic%20regression%20for%20author%20identification%202005"
        },
        {
            "id": "33",
            "entry": "[33] Oren Mangoubi and Aaron Smith. Rapid mixing of Hamiltonian Monte Carlo on strongly log-concave distributions. arXiv preprint arXiv:1708.07114, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07114"
        },
        {
            "id": "34",
            "entry": "[34] Oren Mangoubi and Nisheeth K Vishnoi. Dimensionally tight bounds for second-order Hamiltonian Monte Carlo. arXiv preprint arXiv:1802.08898, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08898"
        },
        {
            "id": "35",
            "entry": "[35] Jonathan C Mattingly, Natesh S Pillai, Andrew M Stuart, et al. Diffusion limits of the random walk Metropolis algorithm in high dimensions. The Annals of Applied Probability, 22(3):881\u2013930, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mattingly%2C%20Jonathan%20C.%20Pillai%2C%20Natesh%20S.%20Stuart%2C%20Andrew%20M.%20Diffusion%20limits%20of%20the%20random%20walk%20Metropolis%20algorithm%20in%20high%20dimensions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mattingly%2C%20Jonathan%20C.%20Pillai%2C%20Natesh%20S.%20Stuart%2C%20Andrew%20M.%20Diffusion%20limits%20of%20the%20random%20walk%20Metropolis%20algorithm%20in%20high%20dimensions%202012"
        },
        {
            "id": "36",
            "entry": "[36] Radford M Neal. Bayesian training of backpropagation networks by the hybrid Monte Carlo method. Technical report, Technical Report CRG-TR-92-1, Dept. of Computer Science, University of Toronto, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Bayesian%20training%20of%20backpropagation%20networks%20by%20the%20hybrid%20Monte%20Carlo%20method%201992"
        },
        {
            "id": "37",
            "entry": "[37] Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Bayesian%20Learning%20for%20Neural%20Networks%201996"
        },
        {
            "id": "38",
            "entry": "[38] Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2:113\u2013162, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20MCMC%20using%20Hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%20Monte%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20Radford%20M.%20MCMC%20using%20Hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%20Monte%202011"
        },
        {
            "id": "39",
            "entry": "[39] Natesh S Pillai, Andrew M Stuart, and Alexandre H Thi\u00e9ry. Optimal scaling and diffusion limits for the Langevin algorithm in high dimensions. The Annals of Applied Probability, 22(6):2320\u20132356, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pillai%2C%20Natesh%20S.%20Stuart%2C%20Andrew%20M.%20Thi%C3%A9ry%2C%20Alexandre%20H.%20Optimal%20scaling%20and%20diffusion%20limits%20for%20the%20Langevin%20algorithm%20in%20high%20dimensions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pillai%2C%20Natesh%20S.%20Stuart%2C%20Andrew%20M.%20Thi%C3%A9ry%2C%20Alexandre%20H.%20Optimal%20scaling%20and%20diffusion%20limits%20for%20the%20Langevin%20algorithm%20in%20high%20dimensions%202012"
        },
        {
            "id": "40",
            "entry": "[40] Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models using P\u00f3lya\u2013Gamma latent variables. Journal of the American statistical Association, 108(504):1339\u20131349, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polson%2C%20Nicholas%20G.%20Scott%2C%20James%20G.%20Windle%2C%20Jesse%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B3lya%E2%80%93Gamma%20latent%20variables%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polson%2C%20Nicholas%20G.%20Scott%2C%20James%20G.%20Windle%2C%20Jesse%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B3lya%E2%80%93Gamma%20latent%20variables%202013"
        },
        {
            "id": "41",
            "entry": "[41] Gareth O Roberts, Richard L Tweedie, et al. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 2(4):341\u2013363, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roberts%2C%20Gareth%20O.%20Tweedie%2C%20Richard%20L.%20Exponential%20convergence%20of%20Langevin%20distributions%20and%20their%20discrete%20approximations%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roberts%2C%20Gareth%20O.%20Tweedie%2C%20Richard%20L.%20Exponential%20convergence%20of%20Langevin%20distributions%20and%20their%20discrete%20approximations%201996"
        },
        {
            "id": "42",
            "entry": "[42] Christof Seiler, Simon Rubinstein-Salzedo, and Susan Holmes. Positive curvature and Hamiltonian Monte Carlo. In Advances in Neural Information Processing Systems, pages 586\u2013594, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seiler%2C%20Christof%20Rubinstein-Salzedo%2C%20Simon%20Holmes%2C%20Susan%20Positive%20curvature%20and%20Hamiltonian%20Monte%20Carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seiler%2C%20Christof%20Rubinstein-Salzedo%2C%20Simon%20Holmes%2C%20Susan%20Positive%20curvature%20and%20Hamiltonian%20Monte%20Carlo%202014"
        },
        {
            "id": "43",
            "entry": "[43] Santosh Vempala. Geometric random walks: a survey. Combinatorial and computational geometry, 52(573-612):2, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vempala%2C%20Santosh%20Geometric%20random%20walks%3A%20a%20survey%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vempala%2C%20Santosh%20Geometric%20random%20walks%3A%20a%20survey%202005"
        },
        {
            "id": "44",
            "entry": "[44] Tong Zhang and Frank J Oles. Text categorization based on regularized linear classification methods. Information retrieval, 4(1):5\u201331, 2001. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Oles%2C%20Frank%20J.%20Text%20categorization%20based%20on%20regularized%20linear%20classification%20methods%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Oles%2C%20Frank%20J.%20Text%20categorization%20based%20on%20regularized%20linear%20classification%20methods%202001"
        }
    ]
}
