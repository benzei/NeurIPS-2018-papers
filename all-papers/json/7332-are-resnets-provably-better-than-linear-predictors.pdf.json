{
    "filename": "7332-are-resnets-provably-better-than-linear-predictors.pdf",
    "metadata": {
        "date": 2018,
        "title": "Are ResNets Provably Better than Linear Predictors?",
        "author": "Ohad Shamir Department of Computer Science and Applied Mathematics",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7332-are-resnets-provably-better-than-linear-predictors.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A residual network (or ResNet) is a standard deep neural net architecture, with stateof-the-art performance across numerous applications. The main premise of ResNets is that they allow the training of each layer to focus on fitting just the residual of the previous layer\u2019s output and the target output. Thus, we should expect that the trained network is no worse than what we can obtain if we remove the residual layers and train a shallower network instead. However, due to the non-convexity of the optimization problem, it is not at all clear that ResNets indeed achieve this behavior, rather than getting stuck at some arbitrarily poor local minimum. In this paper, we rigorously prove that arbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor (namely a 1-layer network). Notably, we show this under minimal or no assumptions on the precise network architecture, data distribution, or loss function used. We also provide a quantitative analysis of approximate stationary points for this problem. Finally, we show that with a certain tweak to the architecture, training the network with standard stochastic gradient descent achieves an objective value close or better than any linear predictor."
    },
    "keywords": [
        {
            "term": "local minimum",
            "url": "https://en.wikipedia.org/wiki/local_minimum"
        },
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "residual network",
            "url": "https://en.wikipedia.org/wiki/residual_network"
        },
        {
            "term": "feedforward neural network",
            "url": "https://en.wikipedia.org/wiki/feedforward_neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Residual networks are a popular class of artificial neural networks, providing state-ofthe-art performance across numerous applications [He et al, 2016a,b, Kim et al, 2016, <a class=\"ref-link\" id=\"cXie_et+al_2017_a\" href=\"#rXie_et+al_2017_a\"><a class=\"ref-link\" id=\"cXie_et+al_2017_a\" href=\"#rXie_et+al_2017_a\">Xie et al., 2017</a></a>, <a class=\"ref-link\" id=\"cXiong_et+al_2017_a\" href=\"#rXiong_et+al_2017_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2017_a\" href=\"#rXiong_et+al_2017_a\">Xiong et al, 2017</a></a>]",
        "Unlike vanilla feedforward neural networks, ResNets are characterized by skip connections, in which the output of one layer is directly added to the output of some following layer",
        "Whereas feedforward neural networks can be expressed as stacking layers of the form y = g\u03a6(x) , is the input-output pair and \u03a6 are the tunable parameters of the function g\u03a6), ResNets are built from \u201cresidual units\u201d of the form y = f (h(x) + g\u03a6(x)), where f, h are fixed functions",
        "We study these questions by considering the competitiveness of a simple residual network with respect to linear predictors",
        "We prove that the optimization landscape has no local minima with a value higher than what can be achieved with a linear predictor on the same data",
        "We provide an algorithmic result, showing that if the residual architecture is changed a bit, a standard stochastic gradient descent (SGD) procedure will result in a predictor similar or better than the best linear predictor"
    ],
    "key_statements": [
        "Residual networks are a popular class of artificial neural networks, providing state-ofthe-art performance across numerous applications [He et al, 2016a,b, Kim et al, 2016, <a class=\"ref-link\" id=\"cXie_et+al_2017_a\" href=\"#rXie_et+al_2017_a\"><a class=\"ref-link\" id=\"cXie_et+al_2017_a\" href=\"#rXie_et+al_2017_a\">Xie et al., 2017</a></a>, <a class=\"ref-link\" id=\"cXiong_et+al_2017_a\" href=\"#rXiong_et+al_2017_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2017_a\" href=\"#rXiong_et+al_2017_a\">Xiong et al, 2017</a></a>]",
        "Unlike vanilla feedforward neural networks, ResNets are characterized by skip connections, in which the output of one layer is directly added to the output of some following layer",
        "Whereas feedforward neural networks can be expressed as stacking layers of the form y = g\u03a6(x) , is the input-output pair and \u03a6 are the tunable parameters of the function g\u03a6), ResNets are built from \u201cresidual units\u201d of the form y = f (h(x) + g\u03a6(x)), where f, h are fixed functions",
        "We study these questions by considering the competitiveness of a simple residual network with respect to linear predictors",
        "We prove that the optimization landscape has no local minima with a value higher than what can be achieved with a linear predictor on the same data",
        "We provide an algorithmic result, showing that if the residual architecture is changed a bit, a standard stochastic gradient descent (SGD) procedure will result in a predictor similar or better than the best linear predictor"
    ],
    "summary": [
        "Residual networks are a popular class of artificial neural networks, providing state-ofthe-art performance across numerous applications [He et al, 2016a,b, Kim et al, 2016, <a class=\"ref-link\" id=\"cXie_et+al_2017_a\" href=\"#rXie_et+al_2017_a\"><a class=\"ref-link\" id=\"cXie_et+al_2017_a\" href=\"#rXie_et+al_2017_a\">Xie et al., 2017</a></a>, <a class=\"ref-link\" id=\"cXiong_et+al_2017_a\" href=\"#rXiong_et+al_2017_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2017_a\" href=\"#rXiong_et+al_2017_a\">Xiong et al, 2017</a></a>].",
        "We study these questions by considering the competitiveness of a simple residual network with respect to linear predictors.",
        "We consider the optimization problem associated with training such a residual network, which is in general non-convex and can have a complicated structure.",
        "<a class=\"ref-link\" id=\"cHardt_2016_a\" href=\"#rHardt_2016_a\">Hardt and Ma [2016</a>] showed that linear residual networks with the squared loss have no spurious local minima.",
        "We focus on non-linear residual units, but consider only local minima above some level set.",
        "We assume that our network is trained with respect to some data distribution, using a loss function (p, y), where p is the network\u2019s prediction and y is the target value.",
        "-SOSP is one of the most general classes of points in non-convex optimization, to which gradient-based methods can be shown to converge in poly(1/ ) iterations.",
        "Our main results are Thm. 3 and Corollary 1 below, which are proven in two stages: First, we show that at any point such that w = 0, \u2207F\u03b8(w, V ) is lower bounded in terms of the suboptimality with respect to the best linear predictor (Thm. 1).",
        "We consider the case w = 0, and show that for such points, if they are suboptimal with respect to the best linear predictor, either \u2207F\u03b8(w, V ) is strictly positive, or \u03bbmin(\u22072F\u03b8(w, V )) is strictly negative (Thm. 2).",
        "The theorem implies that for any point (w, V, \u03b8) for which the objective value F (w, V, \u03b8) is larger than that of some linear predictor Flin(w\u2217), and unless w = 0, its partial derivative with respect to",
        "The objective F has no spurious local minima with value above the smallest attainable with a linear predictor.",
        "One can consider a generalization of our setting to networks with vector-valued outputs, namely x \u2192 W (x + V f\u03b8(x)), where W is a matrix, and with losses (p, y) taking vector-valued arguments and convex in p.",
        "We consider a standard stochastic gradient descent (SGD) algorithm to train our network: Fixing a step size \u03b7 and some convex parameter domain M, we",
        "The following theorem establishes that under mild conditions, running stochastic gradient descent with sufficiently many iterations results in a network competitive with any fixed linear predictor: Theorem 4."
    ],
    "headline": "We rigorously prove that arbitrarily deep, nonlinear residual units exhibit this behavior, in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor ",
    "reference_links": [
        {
            "id": "Bartlett_et+al_2018_a",
            "entry": "Peter L Bartlett, David P Helmbold, and Philip M Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. arXiv preprint arXiv:1802.06093, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06093"
        },
        {
            "id": "Brutzkus_et+al_2017_a",
            "entry": "Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10174"
        },
        {
            "id": "Du_2018_a",
            "entry": "Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01206"
        },
        {
            "id": "Du_et+al_2017_a",
            "entry": "Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00779"
        },
        {
            "id": "Ge_2017_a",
            "entry": "Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. In Advances in Neural Information Processing Systems, pages 3656\u20133666, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Ma%2C%20Tengyu%20On%20the%20optimization%20landscape%20of%20tensor%20decompositions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Ma%2C%20Tengyu%20On%20the%20optimization%20landscape%20of%20tensor%20decompositions%202017"
        },
        {
            "id": "Ge_et+al_2017_b",
            "entry": "Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "Hardt_2016_a",
            "entry": "Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04231"
        },
        {
            "id": "Hazan_2016_a",
            "entry": "Elad Hazan. Introduction to online convex optimization. Foundations and Trends R in Optimization, 2(3-4):157\u2013325, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016"
        },
        {
            "id": "He_et+al_0000_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition"
        },
        {
            "id": "Springer,_2016_a",
            "entry": "Springer, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016b"
        },
        {
            "id": "Jin_et+al_2017_a",
            "entry": "Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00887"
        },
        {
            "id": "Kim_2016_a",
            "entry": "Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1646\u20131654, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Jiwon%20Lee%2C%20Jung%20Kwon%20and%20Kyoung%20Mu%20Lee.%20Accurate%20image%20super-resolution%20using%20very%20deep%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Jiwon%20Lee%2C%20Jung%20Kwon%20and%20Kyoung%20Mu%20Lee.%20Accurate%20image%20super-resolution%20using%20very%20deep%20convolutional%20networks%202016"
        },
        {
            "id": "Liang_et+al_2018_a",
            "entry": "Shiyu Liang, Ruoyu Sun, Yixuan Li, and R Srikant. Understanding the loss surface of neural networks for binary classification. arXiv preprint arXiv:1803.00909, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.00909"
        },
        {
            "id": "Mccormick_1977_a",
            "entry": "Garth P McCormick. A modification of armijo\u2019s step-size rule for negative curvature. Mathematical Programming, 13(1):111\u2013115, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCormick%2C%20Garth%20P.%20A%20modification%20of%20armijo%E2%80%99s%20step-size%20rule%20for%20negative%20curvature%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCormick%2C%20Garth%20P.%20A%20modification%20of%20armijo%E2%80%99s%20step-size%20rule%20for%20negative%20curvature%201977"
        },
        {
            "id": "Nesterov_2006_a",
            "entry": "Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "Safran_2017_a",
            "entry": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08968"
        },
        {
            "id": "Shalev-Shwartz_2012_a",
            "entry": "Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R in Machine Learning, 4(2):107\u2013194, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Online%20learning%20and%20online%20convex%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Online%20learning%20and%20online%20convex%20optimization%202012"
        },
        {
            "id": "Soltanolkotabi_et+al_2017_a",
            "entry": "Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04926"
        },
        {
            "id": "Soudry_2017_a",
            "entry": "Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv preprint arXiv:1702.05777, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05777"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 5987\u20135995. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "Xiong_et+al_2017_a",
            "entry": "Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. The microsoft 2016 conversational speech recognition system. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pages 5255\u20135259. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wayne%20Xiong%20Jasha%20Droppo%20Xuedong%20Huang%20Frank%20Seide%20Mike%20Seltzer%20Andreas%20Stolcke%20Dong%20Yu%20and%20Geoffrey%20Zweig%20The%20microsoft%202016%20conversational%20speech%20recognition%20system%20In%20Acoustics%20Speech%20and%20Signal%20Processing%20ICASSP%202017%20IEEE%20International%20Conference%20on%20pages%2052555259%20IEEE%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wayne%20Xiong%20Jasha%20Droppo%20Xuedong%20Huang%20Frank%20Seide%20Mike%20Seltzer%20Andreas%20Stolcke%20Dong%20Yu%20and%20Geoffrey%20Zweig%20The%20microsoft%202016%20conversational%20speech%20recognition%20system%20In%20Acoustics%20Speech%20and%20Signal%20Processing%20ICASSP%202017%20IEEE%20International%20Conference%20on%20pages%2052555259%20IEEE%202017"
        },
        {
            "id": "Yun_et+al_2018_a",
            "entry": "Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. A critical view of global optimality in deep learning. arXiv preprint arXiv:1802.03487, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03487"
        },
        {
            "id": "Zinkevich_2003_a",
            "entry": "Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928\u2013936, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
