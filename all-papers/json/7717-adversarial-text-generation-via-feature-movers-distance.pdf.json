{
    "filename": "7717-adversarial-text-generation-via-feature-movers-distance.pdf",
    "metadata": {
        "title": "Adversarial Text Generation via Feature-Mover's Distance",
        "author": "Liqun Chen, Shuyang Dai, Chenyang Tao, Haichao Zhang, Zhe Gan, Dinghan Shen, Yizhe Zhang, Guoyin Wang, Ruiyi Zhang, Lawrence Carin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7717-adversarial-text-generation-via-feature-movers-distance.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generative adversarial networks (GANs) have achieved significant success in generating real-valued data. However, the discrete nature of text hinders the application of GAN to text-generation tasks. Instead of using the standard GAN objective, we propose to improve text-generation GAN via a novel approach inspired by optimal transport. Specifically, we consider matching the latent feature distributions of real and synthetic sentences using a novel metric, termed the featuremover\u2019s distance (FMD). This formulation leads to a highly discriminative critic and easy-to-optimize objective, overcoming the mode-collapsing and brittle-training problems in existing methods. Extensive experiments are conducted on a variety of tasks to evaluate the proposed model empirically, including unconditional text generation, style transfer from non-parallel text, and unsupervised cipher cracking. The proposed model yields superior performance, demonstrating wide applicability and effectiveness."
    },
    "keywords": [
        {
            "term": "optimal transport",
            "url": "https://en.wikipedia.org/wiki/optimal_transport"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "joint distribution",
            "url": "https://en.wikipedia.org/wiki/joint_distribution"
        },
        {
            "term": "parallel text",
            "url": "https://en.wikipedia.org/wiki/parallel_text"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Jensen-Shannon divergence",
            "url": "https://en.wikipedia.org/wiki/Jensen-Shannon_divergence"
        },
        {
            "term": "non parallel",
            "url": "https://en.wikipedia.org/wiki/non_parallel"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "text generation",
            "url": "https://en.wikipedia.org/wiki/text_generation"
        }
    ],
    "highlights": [
        "Adversarial training for distribution matching<br/><br/>We review the basic idea of adversarial distribution matching (ADM), which avoids the specification of a likelihood function",
        "We present feature mover Generative adversarial networks (FM-Generative adversarial networks), a novel adversarial approach that leverages optimal transport (OT) to construct a new model for text generation",
        "We review the basic idea of adversarial distribution matching (ADM), which avoids the specification of a likelihood function",
        "We introduce a novel approach for text generation using feature-mover\u2019s distance (FMD), called feature mover Generative adversarial networks (FM-Generative adversarial networks)",
        "By applying our model to several tasks, we demonstrate that it delivers good performance compared to existing text generation approaches",
        "FMGAN has the potential to be applied on other tasks such as image captioning [<a class=\"ref-link\" id=\"c56\" href=\"#r56\">56</a>], joint distribution matching [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>, <a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>], unsupervised sequence classification [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>], and unsupervised machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]"
    ],
    "key_statements": [
        "Adversarial training for distribution matching<br/><br/>We review the basic idea of adversarial distribution matching (ADM), which avoids the specification of a likelihood function",
        "We present feature mover Generative adversarial networks (FM-Generative adversarial networks), a novel adversarial approach that leverages optimal transport (OT) to construct a new model for text generation",
        "A variant of the Earth-Mover\u2019s Distance between the feature distributions of real and synthetic sentences is proposed as the new objective, denoted as the feature-mover\u2019s distance (FMD)",
        "The main contributions of this paper are as follows: (i) A new Generative adversarial networks model based on optimal transport is proposed for text generation",
        "We review the basic idea of adversarial distribution matching (ADM), which avoids the specification of a likelihood function",
        "We propose a new Generative adversarial networks framework for discrete text data, called feature mover Generative adversarial networks (FM-Generative adversarial networks)",
        "We propose to use the Inexact Proximal point method for Optimal Transport (IPOT) algorithm to compute the optimal transport matrix T\u2217, which provides a solution to the original optimal transport problem (3) [<a class=\"ref-link\" id=\"c59\" href=\"#r59\">59</a>]",
        "While the latter uses Maximum Mean Discrepancy to match the features of real and synthetic sentences, both models still keep the original Generative adversarial networks loss function, which may result in the gradient-vanishing issue of the discriminator",
        "CipherGAN [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] uses Generative adversarial networks to tackle the task of unsupervised cipher cracking, utilizing the framework of CycleGAN [<a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>] and adopting techniques such as Gumbel-softmax [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] that deal with discrete data",
        "We introduce a novel approach for text generation using feature-mover\u2019s distance (FMD), called feature mover Generative adversarial networks (FM-Generative adversarial networks)",
        "By applying our model to several tasks, we demonstrate that it delivers good performance compared to existing text generation approaches",
        "FMGAN has the potential to be applied on other tasks such as image captioning [<a class=\"ref-link\" id=\"c56\" href=\"#r56\">56</a>], joint distribution matching [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>, <a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>], unsupervised sequence classification [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>], and unsupervised machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]"
    ],
    "summary": [
        "Adversarial training for distribution matching<br/><br/>We review the basic idea of adversarial distribution matching (ADM), which avoids the specification of a likelihood function.",
        "We present feature mover GAN (FM-GAN), a novel adversarial approach that leverages optimal transport (OT) to construct a new model for text generation.",
        "A variant of the EMD between the feature distributions of real and synthetic sentences is proposed as the new objective, denoted as the feature-mover\u2019s distance (FMD).",
        "The discriminator aims to maximize the dissimilarity of the feature distributions based on the FMD, while the generator is trained to minimize the FMD by synthesizing more-realistic text.",
        "The main contributions of this paper are as follows: (i) A new GAN model based on optimal transport is proposed for text generation.",
        "In order to demonstrate the versatility of the proposed method, we generalize our model to conditional-generation tasks, including non-parallel text style transfer [<a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>], and unsupervised cipher cracking [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "The original critic function in GANs is replaced by the Earth-Mover\u2019s Distance (EMD) between the sentence features of real and synthetic data.",
        "While the latter uses MMD to match the features of real and synthetic sentences, both models still keep the original GAN loss function, which may result in the gradient-vanishing issue of the discriminator.",
        "We consider the task of transferring an original sentence to the opposite sentiment, in the case where parallel data are not available.",
        "For the unsupervised decipher task, we follow the experimental setup in CipherGAN [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and evaluate the model improvement after replacing the critic with the proposed FMD objective.",
        "For the non-parallel text style transfer experiment, following [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>], we use a pretrained classifier to calculate the sentiment accuracy of transferred sentences.",
        "The superior performance of the proposed method highlights the ability of FMD to mitigate the vanishing-gradient issue caused by the discrete nature of text samples, and give rises to better matching between the distributions of reviews belonging to two different sentiments.",
        "CipherGAN [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] uses GANs to tackle the task of unsupervised cipher cracking, utilizing the framework of CycleGAN [<a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>] and adopting techniques such as Gumbel-softmax [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] that deal with discrete data.",
        "We adapt the idea of feature mover\u2019s distance to the original framework of CipherGAN and test this modified model on the Brown English text dataset [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "By applying our model to several tasks, we demonstrate that it delivers good performance compared to existing text generation approaches.",
        "FMGAN has the potential to be applied on other tasks such as image captioning [<a class=\"ref-link\" id=\"c56\" href=\"#r56\">56</a>], joint distribution matching [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>, <a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>], unsupervised sequence classification [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>], and unsupervised machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]"
    ],
    "headline": "Instead of using the standard Generative adversarial networks objective, we propose to improve text-generation Generative adversarial networks via a novel approach inspired by optimal transport",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Afriat. Theory of maxima and the method of lagrange. SIAM Journal on Applied Mathematics, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Afriat%2C%20S.%20Theory%20of%20maxima%20and%20the%20method%20of%20lagrange%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Afriat%2C%20S.%20Theory%20of%20maxima%20and%20the%20method%20of%20lagrange%201971"
        },
        {
            "id": "2",
            "entry": "[2] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Bottou%2C%20L.%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Bottou%2C%20L.%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] M. Artetxe, G. Labaka, E. Agirre, and K. Cho. Unsupervised neural machine translation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Artetxe%20G%20Labaka%20E%20Agirre%20and%20K%20Cho%20Unsupervised%20neural%20machine%20translation%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Artetxe%20G%20Labaka%20E%20Agirre%20and%20K%20Cho%20Unsupervised%20neural%20machine%20translation%20In%20ICLR%202018"
        },
        {
            "id": "5",
            "entry": "[5] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "6",
            "entry": "[6] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20S.%20Vinyals%2C%20O.%20Jaitly%2C%20N.%20Shazeer%2C%20N.%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20S.%20Vinyals%2C%20O.%20Jaitly%2C%20N.%20Shazeer%2C%20N.%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015"
        },
        {
            "id": "7",
            "entry": "[7] A. A. Bruen and M. A. Forcinito. Cryptography, information theory, and error-correction: a handbook for the 21st century, volume 68. John Wiley & Sons, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruen%2C%20A.A.%20Forcinito%2C%20M.A.%20Cryptography%2C%20information%20theory%2C%20and%20error-correction%3A%20a%20handbook%20for%20the%2021st%20century%2C%20volume%2068%202011"
        },
        {
            "id": "8",
            "entry": "[8] T. Che, Y. Li, R. Zhang, R. D. Hjelm, W. Li, Y. Song, and Y. Bengio. Maximum-likelihood augmented discrete generative adversarial networks. In arXiv:1702.07983, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07983"
        },
        {
            "id": "9",
            "entry": "[9] L. Chen, S. Dai, Y. Pu, E. Zhou, C. Li, Q. Su, C. Chen, and L. Carin. Symmetric variational autoencoder and connections to adversarial learning. In AISTATS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20L.%20Dai%2C%20S.%20Pu%2C%20Y.%20Zhou%2C%20E.%20Symmetric%20variational%20autoencoder%20and%20connections%20to%20adversarial%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20L.%20Dai%2C%20S.%20Pu%2C%20Y.%20Zhou%2C%20E.%20Symmetric%20variational%20autoencoder%20and%20connections%20to%20adversarial%20learning%202018"
        },
        {
            "id": "10",
            "entry": "[10] K. Cho, B. Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20K.%20Merrienboer%2C%20B.%20Gulcehre%2C%20C.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20K.%20Merrienboer%2C%20B.%20Gulcehre%2C%20C.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20rnn%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "11",
            "entry": "[11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. JMLR, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collobert%2C%20R.%20Weston%2C%20J.%20Bottou%2C%20L.%20Karlen%2C%20M.%20Natural%20language%20processing%20%28almost%29%20from%20scratch%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collobert%2C%20R.%20Weston%2C%20J.%20Bottou%2C%20L.%20Karlen%2C%20M.%20Natural%20language%20processing%20%28almost%29%20from%20scratch%202011"
        },
        {
            "id": "12",
            "entry": "[12] A. Conneau, G. Lample, M. Ranzato, L. Denoyer, and H. J\u00e9gou. Word translation without parallel data. arXiv preprint arXiv:1710.04087, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.04087"
        },
        {
            "id": "13",
            "entry": "[13] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20M.%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20M.%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013"
        },
        {
            "id": "14",
            "entry": "[14] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll\u00e1r, and J. Gao. From captions to visual concepts and back. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=From%20captions%20to%20visual%20concepts%20and%20back%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=From%20captions%20to%20visual%20concepts%20and%20back%202015"
        },
        {
            "id": "15",
            "entry": "[15] W. Fedus, I. Goodfellow, and A. M. Dai. MaskGAN: Better text generation via filling in the _. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fedus%2C%20W.%20Goodfellow%2C%20I.%20Dai%2C%20A.M.%20MaskGAN%3A%20Better%20text%20generation%20via%20filling%20in%20the%20_%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fedus%2C%20W.%20Goodfellow%2C%20I.%20Dai%2C%20A.M.%20MaskGAN%3A%20Better%20text%20generation%20via%20filling%20in%20the%20_%202018"
        },
        {
            "id": "16",
            "entry": "[16] W. N. Francis. Brown corpus manual. http://icame.uib.no/brown/bcm.html, 1979.",
            "url": "http://icame.uib.no/brown/bcm.html"
        },
        {
            "id": "17",
            "entry": "[17] Z. Gan, L. Chen, W. Wang, Y. Pu, Y. Zhang, H. Liu, C. Li, and L. Carin. Triangle generative adversarial networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Z%20Gan%20L%20Chen%20W%20Wang%20Y%20Pu%20Y%20Zhang%20H%20Liu%20C%20Li%20and%20L%20Carin%20Triangle%20generative%20adversarial%20networks%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Z%20Gan%20L%20Chen%20W%20Wang%20Y%20Pu%20Y%20Zhang%20H%20Liu%20C%20Li%20and%20L%20Carin%20Triangle%20generative%20adversarial%20networks%20In%20NIPS%202017"
        },
        {
            "id": "18",
            "entry": "[18] Z. Gan, Y. Pu, R. Henao, C. Li, X. He, and L. Carin. Learning generic sentence representations using convolutional neural networks. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Z.%20Pu%2C%20Y.%20Henao%2C%20R.%20Li%2C%20C.%20Learning%20generic%20sentence%20representations%20using%20convolutional%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20Z.%20Pu%2C%20Y.%20Henao%2C%20R.%20Li%2C%20C.%20Learning%20generic%20sentence%20representations%20using%20convolutional%20neural%20networks%202017"
        },
        {
            "id": "19",
            "entry": "[19] A. Genevay, G. Peyr\u00e9, and M. Cuturi. Learning generative models with sinkhorn divergences. In AISTATS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20Genevay%2C%20G.%20Peyr%C3%A9%20Cuturi%2C%20M.%20Learning%20generative%20models%20with%20sinkhorn%20divergences%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20Genevay%2C%20G.%20Peyr%C3%A9%20Cuturi%2C%20M.%20Learning%20generative%20models%20with%20sinkhorn%20divergences%202018"
        },
        {
            "id": "20",
            "entry": "[20] A. N. Gomez, S. Huang, I. Zhang, B. M. Li, M. Osama, and L. Kaiser. Unsupervised cipher cracking using discrete gans. In arXiv:1801.04883, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04883"
        },
        {
            "id": "21",
            "entry": "[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "22",
            "entry": "[22] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel two-sample test. JMLR, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Sch%C3%B6lkopf%2C%20B.%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Sch%C3%B6lkopf%2C%20B.%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "23",
            "entry": "[23] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein GANs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "24",
            "entry": "[24] J. Guo, S. Lu, H. Cai, W. Zhang, Y. Yu, and J. Wang. Long text generation via adversarial training with leaked information. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20J.%20Lu%2C%20S.%20Cai%2C%20H.%20Zhang%2C%20W.%20Long%20text%20generation%20via%20adversarial%20training%20with%20leaked%20information%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20J.%20Lu%2C%20S.%20Cai%2C%20H.%20Zhang%2C%20W.%20Long%20text%20generation%20via%20adversarial%20training%20with%20leaked%20information%202018"
        },
        {
            "id": "25",
            "entry": "[25] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "26",
            "entry": "[26] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017"
        },
        {
            "id": "27",
            "entry": "[27] F. Husz\u00e1r. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? In arXiv:1511.05101, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05101"
        },
        {
            "id": "28",
            "entry": "[28] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with Gumbel-softmax. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jang%2C%20E.%20Gu%2C%20S.%20Poole%2C%20B.%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jang%2C%20E.%20Gu%2C%20S.%20Poole%2C%20B.%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017"
        },
        {
            "id": "29",
            "entry": "[29] Y. Kim. Convolutional neural networks for sentence classification. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014"
        },
        {
            "id": "30",
            "entry": "[30] H. Kucera and W. Francis. A standard corpus of present-day edited american english, for use with digital computers (revised and amplified from 1967 version), 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H%20Kucera%20and%20W%20Francis%20A%20standard%20corpus%20of%20presentday%20edited%20american%20english%20for%20use%20with%20digital%20computers%20revised%20and%20amplified%20from%201967%20version%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H%20Kucera%20and%20W%20Francis%20A%20standard%20corpus%20of%20presentday%20edited%20american%20english%20for%20use%20with%20digital%20computers%20revised%20and%20amplified%20from%201967%20version%201979"
        },
        {
            "id": "31",
            "entry": "[31] M. J. Kusner and J. M. Hern\u00e1ndez-Lobato. GANS for sequences of discrete elements with the Gumbel-softmax distribution. In arXiv:1611.04051, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04051"
        },
        {
            "id": "32",
            "entry": "[32] A. Lamb, V. Dumoulin, and A. Courville. Discriminative regularization for generative models. In arXiv:1602.03220, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.03220"
        },
        {
            "id": "33",
            "entry": "[33] G. Lample, M. Ott, A. Conneau, L. Denoyer, and M. Ranzato. Phrase-based & neural unsupervised machine translation. arXiv preprint arXiv:1804.07755, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07755"
        },
        {
            "id": "34",
            "entry": "[34] C. Li, H. Liu, C. Chen, Y. Pu, L. Chen, R. Henao, and L. Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Liu%2C%20H.%20Chen%2C%20C.%20Pu%2C%20Y.%20Alice%3A%20Towards%20understanding%20adversarial%20learning%20for%20joint%20distribution%20matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Liu%2C%20H.%20Chen%2C%20C.%20Pu%2C%20Y.%20Alice%3A%20Towards%20understanding%20adversarial%20learning%20for%20joint%20distribution%20matching%202017"
        },
        {
            "id": "35",
            "entry": "[35] J. Li, R. Jia, H. He, and P. Liang. Delete, retrieve, generate: A simple approach to sentiment and style transfer. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Jia%2C%20R.%20He%2C%20H.%20Liang%2C%20P.%20Delete%2C%20retrieve%2C%20generate%3A%20A%20simple%20approach%20to%20sentiment%20and%20style%20transfer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Jia%2C%20R.%20He%2C%20H.%20Liang%2C%20P.%20Delete%2C%20retrieve%2C%20generate%3A%20A%20simple%20approach%20to%20sentiment%20and%20style%20transfer%202018"
        },
        {
            "id": "36",
            "entry": "[36] J. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky. Adversarial learning for neural dialogue generation. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Monroe%2C%20W.%20Shi%2C%20T.%20Ritter%2C%20A.%20Adversarial%20learning%20for%20neural%20dialogue%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Monroe%2C%20W.%20Shi%2C%20T.%20Ritter%2C%20A.%20Adversarial%20learning%20for%20neural%20dialogue%20generation%202017"
        },
        {
            "id": "37",
            "entry": "[37] K. Lin, D. Li, X. He, Z. Zhang, and M.-T. Sun. Adversarial ranking for language generation. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20K.%20Li%2C%20D.%20He%2C%20X.%20Zhang%2C%20Z.%20Adversarial%20ranking%20for%20language%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20K.%20Li%2C%20D.%20He%2C%20X.%20Zhang%2C%20Z.%20Adversarial%20ranking%20for%20language%20generation%202017"
        },
        {
            "id": "38",
            "entry": "[38] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Doll%C3%A1r%20and%20C%20L%20Zitnick%20Microsoft%20COCO%20Common%20objects%20in%20context%20In%20ECCV%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Doll%C3%A1r%20and%20C%20L%20Zitnick%20Microsoft%20COCO%20Common%20objects%20in%20context%20In%20ECCV%202014"
        },
        {
            "id": "39",
            "entry": "[39] Y. Liu, J. Chen, and L. Deng. An unsupervised learning method exploiting sequential output statistics. In arXiv:1702.07817, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07817"
        },
        {
            "id": "40",
            "entry": "[40] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maddison%2C%20C.J.%20Mnih%2C%20A.%20W%2C%20Y.%20Teh.%20The%20concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maddison%2C%20C.J.%20Mnih%2C%20A.%20W%2C%20Y.%20Teh.%20The%20concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017"
        },
        {
            "id": "41",
            "entry": "[41] T. Mikolov, M. Karafi\u00e1t, L. Burget, J. Cernock\u00fd, and S. Khudanpur. Recurrent neural network based language model. In ISCA, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernock%C3%BD%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernock%C3%BD%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "42",
            "entry": "[42] S. Nowozin, B. Cseke, and R. Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20S.%20Cseke%2C%20B.%20Tomioka%2C%20R.%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20S.%20Cseke%2C%20B.%20Tomioka%2C%20R.%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "43",
            "entry": "[43] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a method for automatic evaluation of machine translation. In ACL, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.-J.%20BLEU%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.-J.%20BLEU%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "44",
            "entry": "[44] S. Prabhumoye, Y. Tsvetkov, R. Salakhutdinov, and A. W. Black. Style transfer through back-translation. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prabhumoye%2C%20S.%20Tsvetkov%2C%20Y.%20Salakhutdinov%2C%20R.%20Black%2C%20A.W.%20Style%20transfer%20through%20back-translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prabhumoye%2C%20S.%20Tsvetkov%2C%20Y.%20Salakhutdinov%2C%20R.%20Black%2C%20A.W.%20Style%20transfer%20through%20back-translation%202018"
        },
        {
            "id": "45",
            "entry": "[45] Y. Pu, S. Dai, Z. Gan, W. Wang, G. Wang, Y. Zhang, R. Henao, and L. Carin. Jointgan: Multi-domain joint distribution learning with generative adversarial nets. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pu%2C%20Y.%20Dai%2C%20S.%20Gan%2C%20Z.%20Wang%2C%20W.%20Jointgan%3A%20Multi-domain%20joint%20distribution%20learning%20with%20generative%20adversarial%20nets%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pu%2C%20Y.%20Dai%2C%20S.%20Gan%2C%20Z.%20Wang%2C%20W.%20Jointgan%3A%20Multi-domain%20joint%20distribution%20learning%20with%20generative%20adversarial%20nets%202018"
        },
        {
            "id": "46",
            "entry": "[46] Y. Pu, W. Wang, R. Henao, L. Chen, Z. Gan, C. Li, and L. Carin. Adversarial symmetric variational autoencoder. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pu%2C%20Y.%20Wang%2C%20W.%20Henao%2C%20R.%20Chen%2C%20L.%20Adversarial%20symmetric%20variational%20autoencoder%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pu%2C%20Y.%20Wang%2C%20W.%20Henao%2C%20R.%20Chen%2C%20L.%20Adversarial%20symmetric%20variational%20autoencoder%202017"
        },
        {
            "id": "47",
            "entry": "[47] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20M.%20Chopra%2C%20S.%20Auli%2C%20M.%20Zaremba%2C%20W.%20Sequence%20level%20training%20with%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20M.%20Chopra%2C%20S.%20Auli%2C%20M.%20Zaremba%2C%20W.%20Sequence%20level%20training%20with%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "48",
            "entry": "[48] Y. Rubner, C. Tomasi, and L. J. Guibas. A metric for distributions with applications to image databases. In ICCV, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubner%2C%20Y.%20Tomasi%2C%20C.%20Guibas%2C%20L.J.%20A%20metric%20for%20distributions%20with%20applications%20to%20image%20databases%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubner%2C%20Y.%20Tomasi%2C%20C.%20Guibas%2C%20L.J.%20A%20metric%20for%20distributions%20with%20applications%20to%20image%20databases%201998"
        },
        {
            "id": "49",
            "entry": "[49] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "50",
            "entry": "[50] T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs using optimal transport. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Zhang%2C%20H.%20Radford%2C%20A.%20Metaxas%2C%20D.%20Improving%20GANs%20using%20optimal%20transport%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Zhang%2C%20H.%20Radford%2C%20A.%20Metaxas%2C%20D.%20Improving%20GANs%20using%20optimal%20transport%202018"
        },
        {
            "id": "51",
            "entry": "[51] D. Shen, G. Wang, W. Wang, M. R. Min, Q. Su, Y. Zhang, C. Li, R. Henao, and L. Carin. Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20D.%20Wang%2C%20G.%20Wang%2C%20W.%20Min%2C%20M.R.%20Baseline%20needs%20more%20love%3A%20On%20simple%20word-embedding-based%20models%20and%20associated%20pooling%20mechanisms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20D.%20Wang%2C%20G.%20Wang%2C%20W.%20Min%2C%20M.R.%20Baseline%20needs%20more%20love%3A%20On%20simple%20word-embedding-based%20models%20and%20associated%20pooling%20mechanisms%202018"
        },
        {
            "id": "52",
            "entry": "[52] D. Shen, Y. Zhang, R. Henao, Q. Su, and L. Carin. Deconvolutional latent-variable model for text sequence matching. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20D.%20Zhang%2C%20Y.%20Henao%2C%20R.%20Su%2C%20Q.%20Deconvolutional%20latent-variable%20model%20for%20text%20sequence%20matching%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20D.%20Zhang%2C%20Y.%20Henao%2C%20R.%20Su%2C%20Q.%20Deconvolutional%20latent-variable%20model%20for%20text%20sequence%20matching%202018"
        },
        {
            "id": "53",
            "entry": "[53] S. Shen, Y. Cheng, Z. He, W. He, H. Wu, M. Sun, and Y. Liu. Minimum risk training for neural machine translation. In ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20S.%20Cheng%2C%20Y.%20He%2C%20Z.%20He%2C%20W.%20Minimum%20risk%20training%20for%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20S.%20Cheng%2C%20Y.%20He%2C%20Z.%20He%2C%20W.%20Minimum%20risk%20training%20for%20neural%20machine%20translation%202015"
        },
        {
            "id": "54",
            "entry": "[54] T. Shen, T. Lei, R. Barzilay, and T. Jaakkola. Style transfer from non-parallel text by crossalignment. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20T.%20Lei%2C%20T.%20Barzilay%2C%20R.%20Jaakkola%2C%20T.%20Style%20transfer%20from%20non-parallel%20text%20by%20crossalignment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20T.%20Lei%2C%20T.%20Barzilay%2C%20R.%20Jaakkola%2C%20T.%20Style%20transfer%20from%20non-parallel%20text%20by%20crossalignment%202017"
        },
        {
            "id": "55",
            "entry": "[55] C. Tao, L. Chen, R. Henao, J. Feng, and L. Carin. Chi-square generative adversarial network. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tao%2C%20C.%20Chen%2C%20L.%20Henao%2C%20R.%20Feng%2C%20J.%20Chi-square%20generative%20adversarial%20network%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tao%2C%20C.%20Chen%2C%20L.%20Henao%2C%20R.%20Feng%2C%20J.%20Chi-square%20generative%20adversarial%20network%202018"
        },
        {
            "id": "56",
            "entry": "[56] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Toshev%2C%20A.%20Bengio%2C%20S.%20Erhan%2C%20D.%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Toshev%2C%20A.%20Bengio%2C%20S.%20Erhan%2C%20D.%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015"
        },
        {
            "id": "57",
            "entry": "[57] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welinder%2C%20P.%20Branson%2C%20S.%20Mita%2C%20T.%20Wah%2C%20C.%20Caltech-UCSD%20Birds%20200%202010"
        },
        {
            "id": "58",
            "entry": "[58] S. Wiseman and A. M. Rush. Sequence-to-sequence learning as beam-search optimization. In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiseman%2C%20S.%20Rush%2C%20A.M.%20Sequence-to-sequence%20learning%20as%20beam-search%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiseman%2C%20S.%20Rush%2C%20A.M.%20Sequence-to-sequence%20learning%20as%20beam-search%20optimization%202016"
        },
        {
            "id": "59",
            "entry": "[59] Y. Xie, X. Wang, R. Wang, and H. Zha. A fast proximal point method for Wasserstein distance. In arXiv:1802.04307, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04307"
        },
        {
            "id": "60",
            "entry": "[60] L. Yu, W. Zhang, J. Wang, and Y. Yu. SeqGAN: Sequence generative adversarial nets with policy gradient. In AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20L.%20Zhang%2C%20W.%20Wang%2C%20J.%20Yu%2C%20Y.%20SeqGAN%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient.%20In%20AAAI%202017"
        },
        {
            "id": "61",
            "entry": "[61] Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin. Adversarial feature matching for text generation. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Gan%2C%20Z.%20Fan%2C%20K.%20Chen%2C%20Z.%20Adversarial%20feature%20matching%20for%20text%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Gan%2C%20Z.%20Fan%2C%20K.%20Chen%2C%20Z.%20Adversarial%20feature%20matching%20for%20text%20generation%202017"
        },
        {
            "id": "62",
            "entry": "[62] J. Zhu, T. Park, P. Isola, and A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.%20Unpaired%20image-to-image%20translation%20using%20cycleconsistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.%20Unpaired%20image-to-image%20translation%20using%20cycleconsistent%20adversarial%20networks%202017"
        },
        {
            "id": "63",
            "entry": "[63] Y. Zhu, S. Lu, L. Zheng, J. Guo, W. Zhang, J. Wang, and Y. Yu. Texygen: A benchmarking platform for text generation models. In SIGIR, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Y.%20Lu%2C%20S.%20Zheng%2C%20L.%20Guo%2C%20J.%20Texygen%3A%20A%20benchmarking%20platform%20for%20text%20generation%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Y.%20Lu%2C%20S.%20Zheng%2C%20L.%20Guo%2C%20J.%20Texygen%3A%20A%20benchmarking%20platform%20for%20text%20generation%20models%202018"
        }
    ]
}
