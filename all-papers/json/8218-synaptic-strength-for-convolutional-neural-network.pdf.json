{
    "filename": "8218-synaptic-strength-for-convolutional-neural-network.pdf",
    "metadata": {
        "title": "Synaptic Strength For Convolutional Neural Network",
        "author": "CHEN LIN, Zhao Zhong, Wu Wei, Junjie Yan",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8218-synaptic-strength-for-convolutional-neural-network.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Convolutional Neural Networks(CNNs) are both computation and memory intensive which hindered their deployment in mobile devices. Inspired by the relevant concept in neural science literature, we propose Synaptic Pruning: a data-driven method to prune connections between input and output feature maps with a newly proposed class of parameters called Synaptic Strength. Synaptic Strength is designed to capture the importance of a connection based on the amount of information it transports. Experiment results show the effectiveness of our approach. On CIFAR-10, we prune connections for various CNN models with up to 96% , which results in significant size reduction and computation saving. Further evaluation on ImageNet demonstrates that synaptic pruning is able to discover efficient models which is competitive to state-of-the-art compact CNNs such as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as following: (1) We introduce Synaptic Strength, a new class of parameters for CNNs to indicate the importance of each connections. (2) Our approach can prune various CNNs with high compression without compromising accuracy. (3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compared with the previous approach in both empirical result and theoretical analysis."
    },
    "keywords": [
        {
            "term": "mobile device",
            "url": "https://en.wikipedia.org/wiki/mobile_device"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "new class",
            "url": "https://en.wikipedia.org/wiki/new_class"
        },
        {
            "term": "synaptic strength",
            "url": "https://en.wikipedia.org/wiki/synaptic_strength"
        },
        {
            "term": "synaptic pruning",
            "url": "https://en.wikipedia.org/wiki/synaptic_pruning"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "Convolutional Neural Networks(CNNs) gradually become dominant in the computer vision community",
        "We introduce a new class of parameters to realize the function of synaptic strength.\n3.2",
        "Strength is explicitly designed to be a good indicator to the variance of the intermediate feature produced by a single kernel, which is immediately reduced by summation across C input channels",
        "Inspired by the synaptic pruning mechanism inside the human brain, we introduced a new class of parameters called Synaptic Strength",
        "We show that we can achieve high pruning with almost no cost at performance using Synaptic Strength",
        "Further analysis proves that Synaptic Pruning is a better indicator of importance compared with the existing method"
    ],
    "key_statements": [
        "Convolutional Neural Networks(CNNs) gradually become dominant in the computer vision community",
        "(3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compared with the previous approach in both empirical result and theoretical analysis",
        "Convolutional Neural Networks have a huge number of parameters, resulting in high resource demand for storage and computation",
        "Various methods have been proposed to increase the efficiency of Convolutional Neural Networks",
        "Recent work shows that there exists large amount of redundancy in Convolutional Neural Networks[<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "Synaptic strength is a measure of the connectivity between axon and dendrite",
        "Synaptic strength is a changing attribute which represents the activeness of the connection",
        "If a connection is hardly used by the overall processing procedure, synaptic strength is decreased which would cause disconnection",
        "In order to adopt the same mechanism for modern Convolutional Neural Networks, two question should be answered: (1) How to analog the same mechanism in Convolutional Neural Networks. (2) How to define the usefulness of certain connection",
        "We introduce a new class of parameters to realize the function of synaptic strength.\n3.2",
        "Strength is explicitly designed to be a good indicator to the variance of the intermediate feature produced by a single kernel, which is immediately reduced by summation across C input channels",
        "Synaptic Strength parameters are forced to represent the multiplication of data variance and kernel norm which makes it a good indicator of information.\n3.4",
        "Pruning and finetune Since Synaptic strength could represent the information extracted by its owner kernel from the corresponding input channel, we apply a simple pruning strategy which is to remove all the synapse connection under threshold t",
        "Inspired by the synaptic pruning mechanism inside the human brain, we introduced a new class of parameters called Synaptic Strength",
        "We show that we can achieve high pruning with almost no cost at performance using Synaptic Strength",
        "Further analysis proves that Synaptic Pruning is a better indicator of importance compared with the existing method"
    ],
    "summary": [
        "Convolutional Neural Networks(CNNs) gradually become dominant in the computer vision community.",
        "(3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compared with the previous approach in both empirical result and theoretical analysis.",
        "We evaluate the proposed method on CIFAR-10 and ImageNet. Experiment shows our approach can achieve much higher compression rates while brings less impact on performance compared with existing pruning methods.",
        "On ImageNet, our pruned ResNet-50 achieves competitive or even better efficiency compared with state-of-the-art compact models [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] in term of accuracy and parameters.",
        "Liu et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] utilize L1 regularization on scale factors in batch-normalization layer to learn the importance indicator for channel pruning.",
        "Strength is explicitly designed to be a good indicator to the variance of the intermediate feature produced by a single kernel, which is immediately reduced by summation across C input channels.",
        "Synaptic Strength parameters are forced to represent the multiplication of data variance and kernel norm which makes it a good indicator of information.",
        "Where xi, yi denote the i-th training data and label, W denotes all the trainable weights in the model, l is the classification loss, the second sum-term is the sparse-inducing regularization, and s the Synaptic Strength.",
        "Pruning and finetune Since Synaptic strength could represent the information extracted by its owner kernel from the corresponding input channel, we apply a simple pruning strategy which is to remove all the synapse connection under threshold t.",
        "Compared to baseline The motivation of our synapse pruning is to minimize the computation and storage cost for CNNs. Each of our pruned models could achieve up to 95% sparsity taking 2D-kernel as a unit, while still maintaining similar accuracy compared with baselines.",
        "Even for DenseNet-40, a relatively compact model which conduct feature reuse between layers intensively, synapse pruning could still remove a majority of synapses with 0.34% loss in accuracy.",
        "Compared with other methods As shown in Figure 3(Right), compared to state-of-the-art filterlevel weight pruning methods, synapse pruning achieves similar accuracy with roughly 1/3 parameters and at most 1/2 flops for all three models.",
        "Condensenet [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] take advantage of irregular connection patterns using a special indexing layer followed by group convolutions, which introduce extra computation burdens compared with filter-level pruning.",
        "Ablation There are two modifications to the original model in order to perform synaptic pruning (1) discard the scale factor from previous batch-norm layer \u03b3 and (2) apply normalization to the kernel and explicitly parameterize kernel\u2019s L2-norm."
    ],
    "headline": "Inspired by the relevant concept in neural science literature, we propose Synaptic Pruning: a data-driven method to prune connections between input and output feature maps with a newly proposed class of parameters called Synaptic Strength",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Sajid Anwar and Wonyong Sung. Compact deep convolutional neural networks with coarse pruning. arXiv preprint arXiv:1610.09639, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09639"
        },
        {
            "id": "2",
            "entry": "[2] Gal Chechik, Isaac Meilijson, and Eytan Ruppin. Synaptic pruning in development: A computational account. 10:1759\u201377, 11 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chechik%2C%20Gal%20Meilijson%2C%20Isaac%20Ruppin%2C%20Eytan%20Synaptic%20pruning%20in%20development%3A%20A%20computational%20account%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chechik%2C%20Gal%20Meilijson%2C%20Isaac%20Ruppin%2C%20Eytan%20Synaptic%20pruning%20in%20development%3A%20A%20computational%20account%201998"
        },
        {
            "id": "3",
            "entry": "[3] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.0759"
        },
        {
            "id": "4",
            "entry": "[4] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems, pages 3123\u20133131, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015"
        },
        {
            "id": "5",
            "entry": "[5] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "6",
            "entry": "[6] Fergus Craik and Ellen Bialystok. Cognition through the lifespan: Mechanisms of change. 10: 131\u20138, 04 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Craik%2C%20Fergus%20Bialystok%2C%20Ellen%20Cognition%20through%20the%20lifespan%3A%20Mechanisms%20of%20change%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Craik%2C%20Fergus%20Bialystok%2C%20Ellen%20Cognition%20through%20the%20lifespan%3A%20Mechanisms%20of%20change%202006"
        },
        {
            "id": "7",
            "entry": "[7] J. Bruna Y. LeCun E. L. Denton, W. Zaremba and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20J.Bruna%20Y.LeCun%20E.L.%20Zaremba%2C%20W.%20Fergus%2C%20R.%20Exploiting%20linear%20structure%20within%20convolutional%20networks%20for%20efficient%20evaluation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20J.Bruna%20Y.LeCun%20E.L.%20Zaremba%2C%20W.%20Fergus%2C%20R.%20Exploiting%20linear%20structure%20within%20convolutional%20networks%20for%20efficient%20evaluation%202014"
        },
        {
            "id": "8",
            "entry": "[8] Scott Gray, Alec Radford, and Diederik P. Kingma. Gpu kernels for block-sparse weights. 2017. URL https://s3-us-west-2.amazonaws.com/openai-assets/blocksparse/blocksparsepaper.pdf.",
            "url": "https://s3-us-west-2.amazonaws.com/openai-assets/blocksparse/blocksparsepaper.pdf"
        },
        {
            "id": "9",
            "entry": "[9] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pages 1135\u20131143, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "10",
            "entry": "[10] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. Eie: efficient inference engine on compressed deep neural network. In Proceedings of the 43rd International Symposium on Computer Architecture, pages 243\u2013254. IEEE Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Liu%2C%20Xingyu%20Mao%2C%20Huizi%20Pu%2C%20Jing%20Eie%3A%20efficient%20inference%20engine%20on%20compressed%20deep%20neural%20network%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Liu%2C%20Xingyu%20Mao%2C%20Huizi%20Pu%2C%20Jing%20Eie%3A%20efficient%20inference%20engine%20on%20compressed%20deep%20neural%20network%202016"
        },
        {
            "id": "11",
            "entry": "[11] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. arXiv preprint arXiv:1707.06168, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06168"
        },
        {
            "id": "12",
            "entry": "[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "13",
            "entry": "[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "14",
            "entry": "[14] Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient densenet using learned group convolutions. arXiv preprint arXiv:1711.09224, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09224"
        },
        {
            "id": "15",
            "entry": "[15] Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. arXiv preprint arXiv:1707.01213, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01213"
        },
        {
            "id": "16",
            "entry": "[16] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "17",
            "entry": "[17] Jianxin Wu Jian-Hao Luo and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. arXiv preprint arXiv:1707.06342, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06342"
        },
        {
            "id": "18",
            "entry": "[18] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lavin%2C%20Andrew%20Gray%2C%20Scott%20Fast%20algorithms%20for%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "19",
            "entry": "[19] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.08710"
        },
        {
            "id": "20",
            "entry": "[20] Sheng Li, Jongsoo Park, and Ping Tak Peter Tang. Enabling sparse winograd convolution by native pruning. arXiv preprint arXiv:1702.08597, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08597"
        },
        {
            "id": "21",
            "entry": "[21] Xingyu Liu, Jeff Pool, Song Han, and William J Dally. Efficient sparse-winograd convolutional neural networks. arXiv preprint arXiv:1802.06367, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06367"
        },
        {
            "id": "22",
            "entry": "[22] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In 2017 IEEE International Conference on Computer Vision, pages 2755\u20132763. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Zhuang%20Li%2C%20Jianguo%20Shen%2C%20Zhiqiang%20Huang%2C%20Gao%20Learning%20efficient%20convolutional%20networks%20through%20network%20slimming%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Zhuang%20Li%2C%20Jianguo%20Shen%2C%20Zhiqiang%20Huang%2C%20Gao%20Learning%20efficient%20convolutional%20networks%20through%20network%20slimming%202017"
        },
        {
            "id": "23",
            "entry": "[23] Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=H1Y8hhg0b.",
            "url": "https://openreview.net/forum?id=H1Y8hhg0b",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Kingma%2C%20Diederik%20P.%20Learning%20sparse%20neural%20networks%20through%20l0%20regularization%202018"
        },
        {
            "id": "24",
            "entry": "[24] A. Vedaldi M. Jaderberg and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. BMVC, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20A.Vedaldi%20M.%20Zisserman%2C%20A.%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014"
        },
        {
            "id": "25",
            "entry": "[25] Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, and William J Dally. Exploring the granularity of sparsity in convolutional neural networks, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Huizi%20Han%2C%20Song%20Pool%2C%20Jeff%20Li%2C%20Wenshuo%20Exploring%20the%20granularity%20of%20sparsity%20in%20convolutional%20neural%20networks%202017"
        },
        {
            "id": "26",
            "entry": "[26] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06440"
        },
        {
            "id": "27",
            "entry": "[27] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525\u2013542.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks"
        },
        {
            "id": "28",
            "entry": "[28] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6550"
        },
        {
            "id": "29",
            "entry": "[29] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. arXiv preprint arXiv:1801.04381, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04381"
        },
        {
            "id": "30",
            "entry": "[30] Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary computation, 10(2):99\u2013127, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002"
        },
        {
            "id": "31",
            "entry": "[31] Pierre Vanderhaeghen and Hwai-Jong Cheng. Guidance molecules in axon pruning and cell death. Cold Spring Harbor perspectives in biology, 2(6):a001859, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vanderhaeghen%2C%20Pierre%20Cheng%2C%20Hwai-Jong%20Guidance%20molecules%20in%20axon%20pruning%20and%20cell%20death%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vanderhaeghen%2C%20Pierre%20Cheng%2C%20Hwai-Jong%20Guidance%20molecules%20in%20axon%20pruning%20and%20cell%20death%202010"
        },
        {
            "id": "32",
            "entry": "[32] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2074\u20132082. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf.",
            "url": "http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "33",
            "entry": "[33] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 5987\u20135995. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "34",
            "entry": "[34] John S Denker Yann LeCun and Sara A Solla. Optimal brain damage. page 598\u2013605, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yann%2C%20John%20S.Denker%20LeCun%20and%20Sara%20A%20Solla.%20Optimal%20brain%20damage"
        },
        {
            "id": "35",
            "entry": "[35] Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. Interleaved group convolutions for deep neural networks. CoRR, abs/1707.02725, 2017. URL http://arxiv.org/abs/1707.02725.",
            "url": "http://arxiv.org/abs/1707.02725",
            "arxiv_url": "https://arxiv.org/pdf/1707.02725"
        },
        {
            "id": "36",
            "entry": "[36] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01083"
        },
        {
            "id": "37",
            "entry": "[37] Zhao Zhong, Junjie Yan, and Cheng-Lin Liu. Practical network blocks design with q-learning. CoRR, abs/1708.05552, 2017. URL http://arxiv.org/abs/1708.05552.",
            "url": "http://arxiv.org/abs/1708.05552",
            "arxiv_url": "https://arxiv.org/pdf/1708.05552"
        },
        {
            "id": "38",
            "entry": "[38] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03044"
        },
        {
            "id": "39",
            "entry": "[39] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        }
    ]
}
