{
    "filename": "7755-geometry-aware-recurrent-neural-networks-for-active-visual-recognition.pdf",
    "metadata": {
        "title": "Geometry-Aware Recurrent Neural Networks for Active Visual Recognition",
        "author": "Ricson Cheng, Ziyan Wang, Katerina Fragkiadaki",
        "date": 1980,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7755-geometry-aware-recurrent-neural-networks-for-active-visual-recognition.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present recurrent geometry-aware neural networks that integrate visual information across multiple views of a scene into 3D latent feature tensors, while maintaining an one-to-one mapping between 3D physical locations in the world scene and latent feature locations. Object detection, object segmentation, and 3D reconstruction is then carried out directly using the constructed 3D feature memory, as opposed to any of the input 2D images. The proposed models are equipped with differentiable egomotion-aware feature warping and (learned) depth-aware unprojection operations to achieve geometrically consistent mapping between the features in the input frame and the constructed latent model of the scene. We empirically show the proposed model generalizes much better than geometryunaware LSTM/GRU networks, especially under the presence of multiple objects and cross-object occlusions. Combined with active view selection policies, our model learns to select informative viewpoints to integrate information from by \u201cundoing\" cross-object occlusions, seamlessly combining geometry with learning from experience."
    },
    "keywords": [
        {
            "term": "active vision",
            "url": "https://en.wikipedia.org/wiki/active_vision"
        },
        {
            "term": "object detection",
            "url": "https://en.wikipedia.org/wiki/object_detection"
        }
    ],
    "highlights": [
        "Cross-object occlusions remain an important source of failures for current state-of-the-art object detectors [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], which, despite their formidable performance increase in recent years, still carry the biases and idiosyncrasies of the data they were trained on [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]: static images from Imagenet and COCO datasets",
        "We propose geometry-aware recurrent networks and loss functions for active object detection, segmentation and 3D reconstruction in cluttered scenes, following the old active vision premise",
        "Though our agent solves for object segmentation, classification and 3D reconstruction, we found it adequate to provide reconstruction-driven rewards Rt, as the increase in Intesection over Union (IoU) of the discretized voxel occupancy from each view to the",
        "We report Intersection over Union (IoU) between the predicted and groundtruth occupancy grids in each of four camera views for our model and baselines in the test set in Table 1",
        "Active view selection We evaluate the performance of our active view selection policy on 3D reconstruction of single and multiple objects"
    ],
    "key_statements": [
        "Cross-object occlusions remain an important source of failures for current state-of-the-art object detectors [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], which, despite their formidable performance increase in recent years, still carry the biases and idiosyncrasies of the data they were trained on [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]: static images from Imagenet and COCO datasets",
        "Detectors do well on viewpoints that human photographers tend to prefer, but may fail on images captured by robotic agents, that often feature severe cross-object occlusions and viewpoints which are unusual for humans",
        "Robotic agents on the other hand do not abide by the constraints of passive vision: they can actively choose where to look in order to recover from occlusions and difficult viewpoints",
        "It has been argued that without such active view selection, no formal performance guarantees can be provided for visual recognition algorithms [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "1980\u2019s active vision was not equipped with deep neural detectors, memory modules, or view selection policies, and often attempted tasks and imagery that would appear elementary with current detectors, even from a single camera view",
        "We propose geometry-aware recurrent networks and loss functions for active object detection, segmentation and 3D reconstruction in cluttered scenes, following the old active vision premise",
        "Though our agent solves for object segmentation, classification and 3D reconstruction, we found it adequate to provide reconstruction-driven rewards Rt, as the increase in Intesection over Union (IoU) of the discretized voxel occupancy from each view to the",
        "We report Intersection over Union (IoU) between the predicted and groundtruth occupancy grids in each of four camera views for our model and baselines in the test set in Table 1",
        "Intersection over Union (IoU) between the predicted and groundtruth voxel occupancy grids in each of four consecutive camera views for our model and LSTM baseline are shown in Table 2",
        "Active view selection We evaluate the performance of our active view selection policy on 3D reconstruction of single and multiple objects",
        "We show Intersection over Union (IoU) between the prediction and groundtruth 3D voxel grids",
        "We show Intersection over Union for the different test-time scenarios listed in the left column",
        "We presented a method for active object detection, segmentation and 3D reconstruction in cluttered static scenes, that selects camera viewpoints and integrates information into a 3D feature memory map, using geometry-aware recurrent neural updates",
        "Segmentation and 3D reconstruction are directly predicted from the constructed 3D feature map, as opposed to any single 2D view"
    ],
    "summary": [
        "Cross-object occlusions remain an important source of failures for current state-of-the-art object detectors [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], which, despite their formidable performance increase in recent years, still carry the biases and idiosyncrasies of the data they were trained on [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]: static images from Imagenet and COCO datasets.",
        "We propose geometry-aware recurrent networks and loss functions for active object detection, segmentation and 3D reconstruction in cluttered scenes, following the old active vision premise.",
        "Our model \u201cunprojects\" RGB, depth and object foreground mask into a 3D feature tensor, which is rotated to match the pose of the first camera view, using the relative egomotion, which is assumed known.",
        "Introducing the problem of active state estimation, namely, selecting camera views for jointly optimizing object instance segmentation, classification and 3D reconstruction, a problem very relevant for robotic computer vision.",
        "3D ShapeNets [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] uses a deep generative model that given a depth map as input predicts a 3D occupancy voxel grid, and has an unprojection operation similar to the one we propose.",
        "Our memory model is a recurrent neural network whose latent state corresponds to a 3D feature map of the visual scene.",
        "Multi-view reconstruction of single objects We compare the performance of the geometry-aware recurrent network against alternatives on the task of 3D reconstruction of single objects using randomly selected camera views.",
        "This model aggregates information using a 3D GRU layer, similar to ours, by unprojecting convolutional features extracted from RGB images using absolute groundtruth camera poses.",
        "We report Intersection over Union (IoU) between the predicted and groundtruth occupancy grids in each of four camera views for our model and baselines in the test set in Table 1.",
        "For SUNCG, we test our proposed model on 3D reconstruction of objects using randomly selected camera views as input.",
        "Intersection over Union (IoU) between the predicted and groundtruth voxel occupancy grids in each of four consecutive camera views for our model and LSTM baseline are shown in Table 2.",
        "An important contribution of this paper is to address joint object instance segmentation, object category prediction and 3D reconstruction while accumulating information across multiple views in order to reveal occluded objects.",
        "Sensitivity to egomotion estimation errors Our reconstruction method uses the relative camera pose between views in order to align the unprojected feature tensors, before feeding them to a 3D GRU layer.",
        "We presented a method for active object detection, segmentation and 3D reconstruction in cluttered static scenes, that selects camera viewpoints and integrates information into a 3D feature memory map, using geometry-aware recurrent neural updates.",
        "Segmentation and 3D reconstruction are directly predicted from the constructed 3D feature map, as opposed to any single 2D view."
    ],
    "headline": "We present recurrent geometry-aware neural networks that integrate visual information across multiple views of a scene into 3D latent feature tensors, while maintaining an one-to-one mapping between 3D physical locations in the world scene and latent feature locations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J. Aloimonos, I. Weiss, and A. Bandyopadhyay. Active vision. In IJCV, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aloimonos%2C%20J.%20Weiss%2C%20I.%20Bandyopadhyay%2C%20A.%20Active%20vision%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aloimonos%2C%20J.%20Weiss%2C%20I.%20Bandyopadhyay%2C%20A.%20Active%20vision%201988"
        },
        {
            "id": "2",
            "entry": "[2] P. Ammirato, P. Poirson, E. Park, J. Kosecka, and A. C. Berg. A dataset for developing and benchmarking active vision. In ICRA, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ammirato%2C%20P.%20Poirson%2C%20P.%20Park%2C%20E.%20Kosecka%2C%20J.%20A%20dataset%20for%20developing%20and%20benchmarking%20active%20vision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ammirato%2C%20P.%20Poirson%2C%20P.%20Park%2C%20E.%20Kosecka%2C%20J.%20A%20dataset%20for%20developing%20and%20benchmarking%20active%20vision%202017"
        },
        {
            "id": "3",
            "entry": "[3] R. Bajcsy. Active perception. In in Proceedings of the IEEE, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bajcsy%2C%20R.%20Active%20perception%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bajcsy%2C%20R.%20Active%20perception%201988"
        },
        {
            "id": "4",
            "entry": "[4] J. Caicedo and S. Lazebnik. Active object localization with deep reinforcement learning. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caicedo%2C%20J.%20Lazebnik%2C%20S.%20Active%20object%20localization%20with%20deep%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caicedo%2C%20J.%20Lazebnik%2C%20S.%20Active%20object%20localization%20with%20deep%20reinforcement%20learning"
        },
        {
            "id": "5",
            "entry": "[5] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03012"
        },
        {
            "id": "6",
            "entry": "[6] K. Cho, B. van Merrienboer, \u00c7. G\u00fcl\u00e7ehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "7",
            "entry": "[7] J. Denzler, M. Zobel, and H. Niemann. Information theoretic focal length selection for real-time active 3d object tracking. In Proceedings Ninth IEEE International Conference on Computer Vision, pages 400\u2013407 vol.1, Oct 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denzler%2C%20J.%20Zobel%2C%20M.%20Niemann%2C%20H.%20Information%20theoretic%20focal%20length%20selection%20for%20real-time%20active%203d%20object%20tracking%202003-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denzler%2C%20J.%20Zobel%2C%20M.%20Niemann%2C%20H.%20Information%20theoretic%20focal%20length%20selection%20for%20real-time%20active%203d%20object%20tracking%202003-10"
        },
        {
            "id": "8",
            "entry": "[8] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised monocular depth estimation with left-right consistency. CoRR, abs/1609.03677, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03677"
        },
        {
            "id": "9",
            "entry": "[9] A. Gonzalez-Garcia, A. Vezhnevets, and V. Ferrari. An active search strategy for efficient object class detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3022\u20133031, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gonzalez-Garcia%2C%20A.%20Vezhnevets%2C%20A.%20Ferrari%2C%20V.%20An%20active%20search%20strategy%20for%20efficient%20object%20class%20detection%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gonzalez-Garcia%2C%20A.%20Vezhnevets%2C%20A.%20Ferrari%2C%20V.%20An%20active%20search%20strategy%20for%20efficient%20object%20class%20detection%202015"
        },
        {
            "id": "10",
            "entry": "[10] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. IQA: visual question answering in interactive environments. CoRR, abs/1712.03316, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03316"
        },
        {
            "id": "11",
            "entry": "[11] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2616\u20132625, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20S.%20Davidson%2C%20J.%20Levine%2C%20S.%20Sukthankar%2C%20R.%20Cognitive%20mapping%20and%20planning%20for%20visual%20navigation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20S.%20Davidson%2C%20J.%20Levine%2C%20S.%20Sukthankar%2C%20R.%20Cognitive%20mapping%20and%20planning%20for%20visual%20navigation%202017"
        },
        {
            "id": "12",
            "entry": "[12] R. Hadsell, S. Chopra, and Y. Lecun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadsell%2C%20R.%20Chopra%2C%20S.%20Lecun%2C%20Y.%20Dimensionality%20reduction%20by%20learning%20an%20invariant%20mapping%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadsell%2C%20R.%20Chopra%2C%20S.%20Lecun%2C%20Y.%20Dimensionality%20reduction%20by%20learning%20an%20invariant%20mapping%202006"
        },
        {
            "id": "13",
            "entry": "[13] R. Held and A. Hein. Movement-produced stimulation in the development of visually guided behavior. In Journal of Comparative and Physiological Psychology, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Held%2C%20R.%20Hein%2C%20A.%20Movement-produced%20stimulation%20in%20the%20development%20of%20visually%20guided%20behavior%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Held%2C%20R.%20Hein%2C%20A.%20Movement-produced%20stimulation%20in%20the%20development%20of%20visually%20guided%20behavior%201963"
        },
        {
            "id": "14",
            "entry": "[14] J. F. Henriques and A. Vedaldi. Mapnet: An allocentric spatial memory for mapping environments. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henriques%2C%20J.F.%20Vedaldi%2C%20A.%20Mapnet%3A%20An%20allocentric%20spatial%20memory%20for%20mapping%20environments%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henriques%2C%20J.F.%20Vedaldi%2C%20A.%20Mapnet%3A%20An%20allocentric%20spatial%20memory%20for%20mapping%20environments%202018"
        },
        {
            "id": "15",
            "entry": "[15] J. Hoffman, E. Tzeng, T. Park, J. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation. CoRR, abs/1711.03213, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03213"
        },
        {
            "id": "16",
            "entry": "[16] D. Jayaraman and K. Grauman. Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion. In European Conference on Computer Vision, pages 489\u2013505.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jayaraman%2C%20D.%20Grauman%2C%20K.%20Look-ahead%20before%20you%20leap%3A%20end-to-end%20active%20recognition%20by%20forecasting%20the%20effect%20of%20motion",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jayaraman%2C%20D.%20Grauman%2C%20K.%20Look-ahead%20before%20you%20leap%3A%20end-to-end%20active%20recognition%20by%20forecasting%20the%20effect%20of%20motion"
        },
        {
            "id": "17",
            "entry": "[17] D. Jayaraman and K. Grauman. Learning to look around. CoRR, abs/1709.00507, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.00507"
        },
        {
            "id": "18",
            "entry": "[18] E. Johns, S. Leutenegger, and A. J. Davison. Pairwise decomposition of image sequences for active multi-view recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3813\u20133822, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johns%2C%20E.%20Leutenegger%2C%20S.%20Davison%2C%20A.J.%20Pairwise%20decomposition%20of%20image%20sequences%20for%20active%20multi-view%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johns%2C%20E.%20Leutenegger%2C%20S.%20Davison%2C%20A.J.%20Pairwise%20decomposition%20of%20image%20sequences%20for%20active%20multi-view%20recognition%202016"
        },
        {
            "id": "19",
            "entry": "[19] A. Kar, C. H\u00e4ne, and J. Malik. Learning a multi-view stereo machine. CoRR, abs/1708.05375, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05375"
        },
        {
            "id": "20",
            "entry": "[20] C. Kerl, J. Sturm, and D. Cremers. Dense visual SLAM for RGB-D cameras. In IROS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kerl%2C%20C.%20Sturm%2C%20J.%20Cremers%2C%20D.%20Dense%20visual%20SLAM%20for%20RGB-D%20cameras%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kerl%2C%20C.%20Sturm%2C%20J.%20Cremers%2C%20D.%20Dense%20visual%20SLAM%20for%20RGB-D%20cameras%202013"
        },
        {
            "id": "21",
            "entry": "[21] M. Malmir, K. Sikka, D. Forster, J. R. Movellan, and G. Cottrell. Deep q-learning for active recognition of germs. In BMVC, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malmir%2C%20M.%20Sikka%2C%20K.%20Forster%2C%20D.%20Movellan%2C%20J.R.%20Deep%20q-learning%20for%20active%20recognition%20of%20germs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malmir%2C%20M.%20Sikka%2C%20K.%20Forster%2C%20D.%20Movellan%2C%20J.R.%20Deep%20q-learning%20for%20active%20recognition%20of%20germs%202015"
        },
        {
            "id": "22",
            "entry": "[22] S. Mathe, A. Pirinen, and C. Sminchisescu. Reinforcement learning for visual object detection. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathe%2C%20S.%20Pirinen%2C%20A.%20Sminchisescu%2C%20C.%20Reinforcement%20learning%20for%20visual%20object%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathe%2C%20S.%20Pirinen%2C%20A.%20Sminchisescu%2C%20C.%20Reinforcement%20learning%20for%20visual%20object%20detection%202016"
        },
        {
            "id": "23",
            "entry": "[23] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03673"
        },
        {
            "id": "24",
            "entry": "[24] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu. Recurrent models of visual attention. CoRR, abs/1406.6247, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.6247"
        },
        {
            "id": "25",
            "entry": "[25] E. Parisotto and R. Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. CoRR, abs/1702.08360, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08360"
        },
        {
            "id": "26",
            "entry": "[26] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. CoRR, abs/1612.00593, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00593"
        },
        {
            "id": "27",
            "entry": "[27] M. Ranzato. On learning where to look. In arXiv preprint arXiv:1405.5488, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1405.5488"
        },
        {
            "id": "28",
            "entry": "[28] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN: towards real-time object detection with region proposal networks. CoRR, abs/1506.01497, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.01497"
        },
        {
            "id": "29",
            "entry": "[29] E. Rivlin and H. Rotstein. Control of a camera for active vision: Foveal vision, smooth tracking and saccade. In IJCV, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rivlin%2C%20E.%20Rotstein%2C%20H.%20Control%20of%20a%20camera%20for%20active%20vision%3A%20Foveal%20vision%2C%20smooth%20tracking%20and%20saccade%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rivlin%2C%20E.%20Rotstein%2C%20H.%20Control%20of%20a%20camera%20for%20active%20vision%3A%20Foveal%20vision%2C%20smooth%20tracking%20and%20saccade%202000"
        },
        {
            "id": "30",
            "entry": "[30] T. Sch\u00f6ps, J. Engel, and D. Cremers. Semi-dense visual odometry for AR on a smartphone. In ISMAR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6ps%2C%20T.%20Engel%2C%20J.%20Cremers%2C%20D.%20Semi-dense%20visual%20odometry%20for%20AR%20on%20a%20smartphone%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6ps%2C%20T.%20Engel%2C%20J.%20Cremers%2C%20D.%20Semi-dense%20visual%20odometry%20for%20AR%20on%20a%20smartphone%202014"
        },
        {
            "id": "31",
            "entry": "[31] S. Soatto. Actionable information in vision. In ICCV, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soatto%2C%20S.%20Actionable%20information%20in%20vision%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soatto%2C%20S.%20Actionable%20information%20in%20vision%202009"
        },
        {
            "id": "32",
            "entry": "[32] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene completion from a single depth image. IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20S.%20Yu%2C%20F.%20Zeng%2C%20A.%20Chang%2C%20A.X.%20Semantic%20scene%20completion%20from%20a%20single%20depth%20image%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20S.%20Yu%2C%20F.%20Zeng%2C%20A.%20Chang%2C%20A.X.%20Semantic%20scene%20completion%20from%20a%20single%20depth%20image%202017"
        },
        {
            "id": "33",
            "entry": "[33] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20McAllester%2C%20D.A.%20Singh%2C%20S.P.%20Mansour%2C%20Y.%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20McAllester%2C%20D.A.%20Singh%2C%20S.P.%20Mansour%2C%20Y.%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "34",
            "entry": "[34] B. W. Tatler, M. M. Hayhoe, M. F. Land, and D. H. Ballard. Eye guidance in natural vision: Reinterpreting salience. Journal of Vision, 11(5):5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tatler%2C%20B.W.%20Hayhoe%2C%20M.M.%20Land%2C%20M.F.%20Ballard%2C%20D.H.%20Eye%20guidance%20in%20natural%20vision%3A%20Reinterpreting%20salience%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tatler%2C%20B.W.%20Hayhoe%2C%20M.M.%20Land%2C%20M.F.%20Ballard%2C%20D.H.%20Eye%20guidance%20in%20natural%20vision%3A%20Reinterpreting%20salience%202011"
        },
        {
            "id": "35",
            "entry": "[35] S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. CoRR, abs/1704.06254, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06254"
        },
        {
            "id": "36",
            "entry": "[36] H. F. Tung, A. Harley, W. Seto, and K. Fragkiadaki. Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation with unpaired supervision. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tung%2C%20H.F.%20Harley%2C%20A.%20Seto%2C%20W.%20Fragkiadaki%2C%20K.%20Adversarial%20inverse%20graphics%20networks%3A%20Learning%202d-to-3d%20lifting%20and%20image-to-image%20translation%20with%20unpaired%20supervision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tung%2C%20H.F.%20Harley%2C%20A.%20Seto%2C%20W.%20Fragkiadaki%2C%20K.%20Adversarial%20inverse%20graphics%20networks%3A%20Learning%202d-to-3d%20lifting%20and%20image-to-image%20translation%20with%20unpaired%20supervision%202017"
        },
        {
            "id": "37",
            "entry": "[37] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki. Sfm-net: Learning of structure and motion from video. arXiv preprint arXiv:1704.07804, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.07804"
        },
        {
            "id": "38",
            "entry": "[38] D. Wilkes and J. Tsotsos. Active object recognition. In CVPR, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilkes%2C%20D.%20Tsotsos%2C%20J.%20Active%20object%20recognition%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilkes%2C%20D.%20Tsotsos%2C%20J.%20Active%20object%20recognition%201992"
        },
        {
            "id": "39",
            "entry": "[39] L. L. Wong, L. P. Kaelbling, and T. Lozano-Perez. Constructing semantic world models from partial views. In Robotics: Science and Systems (RSS) Workshop on Robots in Clutter, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wong%2C%20L.L.%20Kaelbling%2C%20L.P.%20Lozano-Perez%2C%20T.%20Constructing%20semantic%20world%20models%20from%20partial%20views%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wong%2C%20L.L.%20Kaelbling%2C%20L.P.%20Lozano-Perez%2C%20T.%20Constructing%20semantic%20world%20models%20from%20partial%20views%202013"
        },
        {
            "id": "40",
            "entry": "[40] J. Wu, Y. Wang, T. Xue, X. Sun, W. T. Freeman, and J. B. Tenenbaum. Marrnet: 3d shape reconstruction via 2.5d sketches. CoRR, abs/1711.03129, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03129"
        },
        {
            "id": "41",
            "entry": "[41] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Torralba, and W. T. Freeman. 3d interpreter networks for viewer-centered wireframe modeling. International Journal of Computer Vision (IJCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20J.%20Xue%2C%20T.%20Lim%2C%20J.J.%20Tian%2C%20Y.%203d%20interpreter%20networks%20for%20viewer-centered%20wireframe%20modeling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20J.%20Xue%2C%20T.%20Lim%2C%20J.J.%20Tian%2C%20Y.%203d%20interpreter%20networks%20for%20viewer-centered%20wireframe%20modeling%202018"
        },
        {
            "id": "42",
            "entry": "[42] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR, pages 1912\u20131920. IEEE Computer Society, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Z.%20Song%2C%20S.%20Khosla%2C%20A.%20Yu%2C%20F.%20Xiao.%203d%20shapenets%3A%20A%20deep%20representation%20for%20volumetric%20shapes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Z.%20Song%2C%20S.%20Khosla%2C%20A.%20Yu%2C%20F.%20Xiao.%203d%20shapenets%3A%20A%20deep%20representation%20for%20volumetric%20shapes%202015"
        },
        {
            "id": "43",
            "entry": "[43] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsupervised learning of depth and egomotion from video. In arxiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20T.%20Brown%2C%20M.%20Snavely%2C%20N.%20Lowe%2C%20D.G.%20Unsupervised%20learning%20of%20depth%20and%20egomotion%20from%20video%202017"
        },
        {
            "id": "44",
            "entry": "[44] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Y.%20Mottaghi%2C%20R.%20Kolve%2C%20E.%20Lim%2C%20J.J.%20Target-driven%20visual%20navigation%20in%20indoor%20scenes%20using%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Y.%20Mottaghi%2C%20R.%20Kolve%2C%20E.%20Lim%2C%20J.J.%20Target-driven%20visual%20navigation%20in%20indoor%20scenes%20using%20deep%20reinforcement%20learning%202017"
        }
    ]
}
