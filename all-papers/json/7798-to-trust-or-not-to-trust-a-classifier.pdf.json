{
    "filename": "7798-to-trust-or-not-to-trust-a-classifier.pdf",
    "metadata": {
        "title": "To Trust Or Not To Trust A Classifier",
        "author": "Heinrich Jiang, Been Kim, Melody Guan, Maya Gupta",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7798-to-trust-or-not-to-trust-a-classifier.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Knowing when a classifier\u2019s prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier\u2019s predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier\u2019s discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier\u2019s confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "Machine learning",
            "url": "https://en.wikipedia.org/wiki/Machine_learning"
        }
    ],
    "highlights": [
        "Machine learning (ML) is a powerful and widely-used tool for making potentially important decisions, from product recommendations to medical diagnosis",
        "We show that high/low trust scores correspond to high probability of agreement/disagreement with the Bayes-optimal classifier",
        "We find that the vector space used to compute the distances in the trust score matters, and that computing trust scores on more-processed layers of a deep model generally works better",
        "We provide a guarantee about the trust score, making the same assumptions as in Theorem 3 for each of the label distributions",
        "The trust score provides information about the relative positions of the datapoints, which may be lost in common approaches such as the model confidence when the model is trained using SGD",
        "We show high-probability non-asymptotic statistical guarantees that high trust scores correspond to agreement with the Bayes-optimal classifier under various nonparametric settings, which build on recent results in topological data analysis"
    ],
    "key_statements": [
        "Machine learning (ML) is a powerful and widely-used tool for making potentially important decisions, from product recommendations to medical diagnosis",
        "We show that high/low trust scores correspond to high probability of agreement/disagreement with the Bayes-optimal classifier",
        "We find that the vector space used to compute the distances in the trust score matters, and that computing trust scores on more-processed layers of a deep model generally works better",
        "We provide a guarantee about the trust score, making the same assumptions as in Theorem 3 for each of the label distributions",
        "Since we evaluate the trust score on its precision at a given recall percentile level, we are interested in the relative ranking of the scores rather than their absolute values",
        "The results show that our method consistently has a higher precision vs percentile curve than the rest of the methods across the datasets and models. This suggests the trust score considerably improves upon known methods as a signal for identifying trustworthy and suspicious testing examples for low-dimensional data",
        "In Figure 2, we show how the performance of trust score changes as the accuracy of the classifier changes",
        "As we show in Section 5.3, the trust score can still have added value even when the classifier is known to be of high performance on some benchmark larger-scale datasets.\n5.3",
        "Conclusion: In this paper, we provide the trust score: a new, simple, and effective way to judge if one should trust the prediction from a classifier",
        "The trust score provides information about the relative positions of the datapoints, which may be lost in common approaches such as the model confidence when the model is trained using SGD",
        "We show high-probability non-asymptotic statistical guarantees that high trust scores correspond to agreement with the Bayes-optimal classifier under various nonparametric settings, which build on recent results in topological data analysis"
    ],
    "summary": [
        "Machine learning (ML) is a powerful and widely-used tool for making potentially important decisions, from product recommendations to medical diagnosis.",
        "Our analysis is done under various settings including when the data lies near a lower dimensional manifold and we provide rates that depend only on the lower dimension.",
        "We show that Algorithm 1 is a statistically consistent estimator of the \u21b5-high-density-level set with finite-sample estimation rates.",
        "For setting (i), where the data lies in RD, the estimation rate has a dependence on the dimension D, which may be unattractive in high-dimensional situations: this is known as the curse of dimensionality, suffered by density-based procedures in general.",
        "It says that as long as our density function satisfies the regularity assumptions stated earlier, and the parameter k lies within a certain range, we can bound the Hausdorff distance between what Algorithm 1 recovers and H\u21b5(f ), the true \u21b5-high-density set, from an i.i.d. sample drawn from f of size n.",
        "One of the disadvantages of Theorem 1 is that the estimation errors have a dependence on D, the dimension of the data, which may be highly undesirable in high-dimensional settings.",
        "We provide a guarantee about the trust score, making the same assumptions as in Theorem 3 for each of the label distributions.",
        "The trust score \u21e0 of Algorithm 2 satisfies the following with high probability uniformly over all x 2 X and all classifiers h : X !",
        "To speed things up for the larger datasets MNIST, SVHN, CIFAR-10 and CIFAR-100, we skipped the initial filtering step of Algorithm 1 altogether and reduced the intermediate layers down to 20 dimensions using PCA before being processed by the trust score which showed similar performance.",
        "This suggests the trust score considerably improves upon known methods as a signal for identifying trustworthy and suspicious testing examples for low-dimensional data.",
        "As we show in Section 5.3, the trust score can still have added value even when the classifier is known to be of high performance on some benchmark larger-scale datasets.",
        "They suggest that for high dimensional datasets, the trust score may only provide little or no improvement over the model confidence at detecting trustworthy and suspicious examples.",
        "We show high-probability non-asymptotic statistical guarantees that high trust scores correspond to agreement with the Bayes-optimal classifier under various nonparametric settings, which build on recent results in topological data analysis.",
        "Our empirical results across many datasets, classifiers, and representations of the data show that our method consistently outperforms the classifier\u2019s own reported confidence in identifying trustworthy and suspicious examples in low to mid dimensional datasets.",
        "The theoretical and empirical results suggest that this approach may have important practical implications in low to mid dimension settings"
    ],
    "headline": "The standard approach is to use the classifier\u2019s discriminant or confidence score; we show there exists an alternative that is more effective in many situations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Kush R Varshney and Homa Alemzadeh. On the safety of machine learning: Cyber-physical systems, decision sciences, and data products. Big data, 5(3):246\u2013255, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Varshney%2C%20Kush%20R.%20Alemzadeh%2C%20Homa%20On%20the%20safety%20of%20machine%20learning%3A%20Cyber-physical%20systems%2C%20decision%20sciences%2C%20and%20data%20products%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Varshney%2C%20Kush%20R.%20Alemzadeh%2C%20Homa%20On%20the%20safety%20of%20machine%20learning%3A%20Cyber-physical%20systems%2C%20decision%20sciences%2C%20and%20data%20products%202017"
        },
        {
            "id": "2",
            "entry": "[2] John D Lee and Katrina A See. Trust in automation: Designing for appropriate reliance. Human factors, 46(1):50\u201380, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20John%20D.%20See%2C%20Katrina%20A.%20Trust%20in%20automation%3A%20Designing%20for%20appropriate%20reliance%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20John%20D.%20See%2C%20Katrina%20A.%20Trust%20in%20automation%3A%20Designing%20for%20appropriate%20reliance%202004"
        },
        {
            "id": "3",
            "entry": "[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "4",
            "entry": "[4] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. arXiv preprint arXiv:1706.04599, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04599"
        },
        {
            "id": "5",
            "entry": "[5] Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In Advances in Neural Information Processing Systems, pages 3474\u20133482, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuleshov%2C%20Volodymyr%20Liang%2C%20Percy%20S.%20Calibrated%20structured%20prediction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuleshov%2C%20Volodymyr%20Liang%2C%20Percy%20S.%20Calibrated%20structured%20prediction%202015"
        },
        {
            "id": "6",
            "entry": "[6] Foster J Provost, Tom Fawcett, and Ron Kohavi. The case against accuracy estimation for comparing induction algorithms. In ICML, volume 98, pages 445\u2013453, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Provost%2C%20Foster%20J.%20Fawcett%2C%20Tom%20Kohavi%2C%20Ron%20The%20case%20against%20accuracy%20estimation%20for%20comparing%20induction%20algorithms%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Provost%2C%20Foster%20J.%20Fawcett%2C%20Tom%20Kohavi%2C%20Ron%20The%20case%20against%20accuracy%20estimation%20for%20comparing%20induction%20algorithms%201998"
        },
        {
            "id": "7",
            "entry": "[7] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "8",
            "entry": "[8] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427\u2013436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015"
        },
        {
            "id": "9",
            "entry": "[9] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10(3):61\u201374, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Platt%2C%20John%20Probabilistic%20outputs%20for%20support%20vector%20machines%20and%20comparisons%20to%20regularized%20likelihood%20methods%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Platt%2C%20John%20Probabilistic%20outputs%20for%20support%20vector%20machines%20and%20comparisons%20to%20regularized%20likelihood%20methods%201999"
        },
        {
            "id": "10",
            "entry": "[10] Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 694\u2013699. ACM, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zadrozny%2C%20Bianca%20Elkan%2C%20Charles%20Transforming%20classifier%20scores%20into%20accurate%20multiclass%20probability%20estimates%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zadrozny%2C%20Bianca%20Elkan%2C%20Charles%20Transforming%20classifier%20scores%20into%20accurate%20multiclass%20probability%20estimates%202002"
        },
        {
            "id": "11",
            "entry": "[11] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd International Conference on Machine Learning, pages 625\u2013632. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niculescu-Mizil%2C%20Alexandru%20Caruana%2C%20Rich%20Predicting%20good%20probabilities%20with%20supervised%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niculescu-Mizil%2C%20Alexandru%20Caruana%2C%20Rich%20Predicting%20good%20probabilities%20with%20supervised%20learning%202005"
        },
        {
            "id": "12",
            "entry": "[12] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6405\u20136416, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20Balaji%20Pritzel%2C%20Alexander%20Blundell%2C%20Charles%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshminarayanan%2C%20Balaji%20Pritzel%2C%20Alexander%20Blundell%2C%20Charles%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017"
        },
        {
            "id": "13",
            "entry": "[13] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.02136"
        },
        {
            "id": "14",
            "entry": "[14] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, pages 1050\u20131059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "15",
            "entry": "[15] Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems, pages 5580\u20135590, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20Alex%20Gal%2C%20Yarin%20What%20uncertainties%20do%20we%20need%20in%20Bayesian%20deep%20learning%20for%20computer%20vision%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20Alex%20Gal%2C%20Yarin%20What%20uncertainties%20do%20we%20need%20in%20Bayesian%20deep%20learning%20for%20computer%20vision%3F%202017"
        },
        {
            "id": "16",
            "entry": "[16] Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 9(Aug):1823\u20131840, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20a%20reject%20option%20using%20a%20hinge%20loss%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20a%20reject%20option%20using%20a%20hinge%20loss%202008"
        },
        {
            "id": "17",
            "entry": "[17] Ming Yuan and Marten Wegkamp. Classification methods with reject option based on convex risk minimization. Journal of Machine Learning Research, 11(Jan):111\u2013130, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Ming%20Wegkamp%2C%20Marten%20Classification%20methods%20with%20reject%20option%20based%20on%20convex%20risk%20minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Ming%20Wegkamp%2C%20Marten%20Classification%20methods%20with%20reject%20option%20based%20on%20convex%20risk%20minimization%202010"
        },
        {
            "id": "18",
            "entry": "[18] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In International Conference on Algorithmic Learning Theory, pages 67\u201382.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20DeSalvo%2C%20Giulia%20Mohri%2C%20Mehryar%20Learning%20with%20rejection",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20DeSalvo%2C%20Giulia%20Mohri%2C%20Mehryar%20Learning%20with%20rejection"
        },
        {
            "id": "19",
            "entry": "[19] Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, and St\u00e9phane Canu. Support vector machines with a reject option. In Advances in Neural Information Processing Systems, pages 537\u2013544, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grandvalet%2C%20Yves%20Rakotomamonjy%2C%20Alain%20Keshet%2C%20Joseph%20Canu%2C%20St%C3%A9phane%20Support%20vector%20machines%20with%20a%20reject%20option%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grandvalet%2C%20Yves%20Rakotomamonjy%2C%20Alain%20Keshet%2C%20Joseph%20Canu%2C%20St%C3%A9phane%20Support%20vector%20machines%20with%20a%20reject%20option%202009"
        },
        {
            "id": "20",
            "entry": "[20] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. In Advances in Neural Information Processing Systems, pages 1660\u20131668, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20DeSalvo%2C%20Giulia%20Mohri%2C%20Mehryar%20Boosting%20with%20abstention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20DeSalvo%2C%20Giulia%20Mohri%2C%20Mehryar%20Boosting%20with%20abstention%202016"
        },
        {
            "id": "21",
            "entry": "[21] Radu Herbei and Marten H Wegkamp. Classification with reject option. Canadian Journal of Statistics, 34(4):709\u2013721, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herbei%2C%20Radu%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20reject%20option%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herbei%2C%20Radu%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20reject%20option%202006"
        },
        {
            "id": "22",
            "entry": "[22] Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, and Scott Yang. Online learning with abstention. arXiv preprint arXiv:1703.03478, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03478"
        },
        {
            "id": "23",
            "entry": "[23] C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory, 16(1):41\u201346, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20C.%20On%20optimum%20recognition%20error%20and%20reject%20tradeoff%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20C.%20On%20optimum%20recognition%20error%20and%20reject%20tradeoff%201970"
        },
        {
            "id": "24",
            "entry": "[24] Bernard Dubuisson and Mylene Masson. A statistical decision rule with incomplete knowledge about classes. Pattern Recognition, 26(1):155\u2013165, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dubuisson%2C%20Bernard%20Masson%2C%20Mylene%20A%20statistical%20decision%20rule%20with%20incomplete%20knowledge%20about%20classes%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dubuisson%2C%20Bernard%20Masson%2C%20Mylene%20A%20statistical%20decision%20rule%20with%20incomplete%20knowledge%20about%20classes%201993"
        },
        {
            "id": "25",
            "entry": "[25] Giorgio Fumera, Fabio Roli, and Giorgio Giacinto. Multiple reject thresholds for improving classification reliability. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages 863\u2013871.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fumera%2C%20Giorgio%20Roli%2C%20Fabio%20Giacinto%2C%20Giorgio%20Multiple%20reject%20thresholds%20for%20improving%20classification%20reliability",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fumera%2C%20Giorgio%20Roli%2C%20Fabio%20Giacinto%2C%20Giorgio%20Multiple%20reject%20thresholds%20for%20improving%20classification%20reliability"
        },
        {
            "id": "26",
            "entry": "[26] Carla M Santos-Pereira and Ana M Pires. On optimal reject rules and ROC curves. Pattern Recognition Letters, 26(7):943\u2013952, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santos-Pereira%2C%20Carla%20M.%20Pires%2C%20Ana%20M.%20On%20optimal%20reject%20rules%20and%20ROC%20curves%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santos-Pereira%2C%20Carla%20M.%20Pires%2C%20Ana%20M.%20On%20optimal%20reject%20rules%20and%20ROC%20curves%202005"
        },
        {
            "id": "27",
            "entry": "[27] Francesco Tortorella. An optimal reject rule for binary classifiers. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages 611\u2013620.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tortorella%2C%20Francesco%20An%20optimal%20reject%20rule%20for%20binary%20classifiers",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tortorella%2C%20Francesco%20An%20optimal%20reject%20rule%20for%20binary%20classifiers"
        },
        {
            "id": "28",
            "entry": "[28] Giorgio Fumera and Fabio Roli. Support vector machines with embedded reject option. In Pattern Recognition with Support Vector Machines, pages 68\u201382.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fumera%2C%20Giorgio%20Roli%2C%20Fabio%20Support%20vector%20machines%20with%20embedded%20reject%20option",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fumera%2C%20Giorgio%20Roli%2C%20Fabio%20Support%20vector%20machines%20with%20embedded%20reject%20option"
        },
        {
            "id": "29",
            "entry": "[29] Thomas CW Landgrebe, David MJ Tax, Pavel Pacl\u00edk, and Robert PW Duin. The interaction between classification and reject performance for distance-based reject-option classifiers. Pattern Recognition Letters, 27(8):908\u2013917, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Landgrebe%2C%20Thomas%20C.W.%20Tax%2C%20David%20M.J.%20Pacl%C3%ADk%2C%20Pavel%20Duin%2C%20Robert%20P.W.%20The%20interaction%20between%20classification%20and%20reject%20performance%20for%20distance-based%20reject-option%20classifiers%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Landgrebe%2C%20Thomas%20C.W.%20Tax%2C%20David%20M.J.%20Pacl%C3%ADk%2C%20Pavel%20Duin%2C%20Robert%20P.W.%20The%20interaction%20between%20classification%20and%20reject%20performance%20for%20distance-based%20reject-option%20classifiers%202006"
        },
        {
            "id": "30",
            "entry": "[30] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(May):1605\u20131641, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=El-Yaniv%2C%20Ran%20Wiener%2C%20Yair%20On%20the%20foundations%20of%20noise-free%20selective%20classification%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=El-Yaniv%2C%20Ran%20Wiener%2C%20Yair%20On%20the%20foundations%20of%20noise-free%20selective%20classification%202010"
        },
        {
            "id": "31",
            "entry": "[31] Yair Wiener and Ran El-Yaniv. Agnostic selective classification. In Advances in Neural Information Processing Systems, pages 1665\u20131673, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiener%2C%20Yair%20El-Yaniv%2C%20Ran%20Agnostic%20selective%20classification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiener%2C%20Yair%20El-Yaniv%2C%20Ran%20Agnostic%20selective%20classification%202011"
        },
        {
            "id": "32",
            "entry": "[32] David MJ Tax and Robert PW Duin. Growing a multi-class classifier with a reject option. Pattern Recognition Letters, 29(10):1565\u20131570, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tax%2C%20David%20M.J.%20Duin%2C%20Robert%20P.W.%20Growing%20a%20multi-class%20classifier%20with%20a%20reject%20option%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tax%2C%20David%20M.J.%20Duin%2C%20Robert%20P.W.%20Growing%20a%20multi-class%20classifier%20with%20a%20reject%20option%202008"
        },
        {
            "id": "33",
            "entry": "[33] Joseph Wang, Kirill Trapeznikov, and Venkatesh Saligrama. Efficient learning by directed acyclic graph for resource constrained prediction. Advances in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Joseph%20Kirill%20Trapeznikov%2C%20and%20Venkatesh%20Saligrama.%20Efficient%20learning%20by%20directed%20acyclic%20graph%20for%20resource%20constrained%20prediction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Joseph%20Kirill%20Trapeznikov%2C%20and%20Venkatesh%20Saligrama.%20Efficient%20learning%20by%20directed%20acyclic%20graph%20for%20resource%20constrained%20prediction%202015"
        },
        {
            "id": "34",
            "entry": "[34] Nathan Parrish, Hyrum S. Anderson, Maya R. Gupta, and Dun Yu Hsaio. Classifying with confidence from incomplete information. Journal of Machine Learning Research, 14(December):3561\u20133589, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parrish%2C%20Nathan%20Anderson%2C%20Hyrum%20S.%20Gupta%2C%20Maya%20R.%20Hsaio%2C%20Dun%20Yu%20Classifying%20with%20confidence%20from%20incomplete%20information%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parrish%2C%20Nathan%20Anderson%2C%20Hyrum%20S.%20Gupta%2C%20Maya%20R.%20Hsaio%2C%20Dun%20Yu%20Classifying%20with%20confidence%20from%20incomplete%20information%202013"
        },
        {
            "id": "35",
            "entry": "[35] Wei Fan, Fang Chu, Haixun Wang, and Philip S. Yu. Pruning and dynamic scheduling of cost-sensitive ensembles. AAAI, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Wei%20Chu%2C%20Fang%20Wang%2C%20Haixun%20Yu%2C%20Philip%20S.%20Pruning%20and%20dynamic%20scheduling%20of%20cost-sensitive%20ensembles%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Wei%20Chu%2C%20Fang%20Wang%2C%20Haixun%20Yu%2C%20Philip%20S.%20Pruning%20and%20dynamic%20scheduling%20of%20cost-sensitive%20ensembles%202002"
        },
        {
            "id": "36",
            "entry": "[36] Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.04765"
        },
        {
            "id": "37",
            "entry": "[37] John A Hartigan. Clustering algorithms. 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartigan%2C%20John%20A.%20Clustering%20algorithms%201975"
        },
        {
            "id": "38",
            "entry": "[38] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In Kdd, pages 226\u2013231, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ester%2C%20Martin%20Kriegel%2C%20Hans-Peter%20Sander%2C%20J%C3%B6rg%20Xu%2C%20Xiaowei%20A%20density-based%20algorithm%20for%20discovering%20clusters%20in%20large%20spatial%20databases%20with%20noise%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ester%2C%20Martin%20Kriegel%2C%20Hans-Peter%20Sander%2C%20J%C3%B6rg%20Xu%2C%20Xiaowei%20A%20density-based%20algorithm%20for%20discovering%20clusters%20in%20large%20spatial%20databases%20with%20noise%201996"
        },
        {
            "id": "39",
            "entry": "[39] Alexandre B Tsybakov et al. On nonparametric estimation of density level sets. The Annals of Statistics, 25(3):948\u2013969, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexandre%20B.%20On%20nonparametric%20estimation%20of%20density%20level%20sets%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsybakov%2C%20Alexandre%20B.%20On%20nonparametric%20estimation%20of%20density%20level%20sets%201997"
        },
        {
            "id": "40",
            "entry": "[40] Aarti Singh, Clayton Scott, Robert Nowak, et al. Adaptive Hausdorff estimation of density level sets. The Annals of Statistics, 37(5B):2760\u20132782, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Aarti%20Scott%2C%20Clayton%20Nowak%2C%20Robert%20Adaptive%20Hausdorff%20estimation%20of%20density%20level%20sets%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Aarti%20Scott%2C%20Clayton%20Nowak%2C%20Robert%20Adaptive%20Hausdorff%20estimation%20of%20density%20level%20sets%202009"
        },
        {
            "id": "41",
            "entry": "[41] Philippe Rigollet, R\u00e9gis Vert, et al. Optimal rates for plug-in estimators of density level sets. Bernoulli, 15(4):1154\u20131178, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20Philippe%20Vert%2C%20R%C3%A9gis%20Optimal%20rates%20for%20plug-in%20estimators%20of%20density%20level%20sets%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rigollet%2C%20Philippe%20Vert%2C%20R%C3%A9gis%20Optimal%20rates%20for%20plug-in%20estimators%20of%20density%20level%20sets%202009"
        },
        {
            "id": "42",
            "entry": "[42] Heinrich Jiang. Density level set estimation on manifolds with DBSCAN. In International Conference on Machine Learning, pages 1684\u20131693, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Heinrich%20Density%20level%20set%20estimation%20on%20manifolds%20with%20DBSCAN%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Heinrich%20Density%20level%20set%20estimation%20on%20manifolds%20with%20DBSCAN%202017"
        },
        {
            "id": "43",
            "entry": "[43] Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. The Annals of Statistics, 38(5):2678\u20132722, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rinaldo%2C%20Alessandro%20Wasserman%2C%20Larry%20Generalized%20density%20clustering%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rinaldo%2C%20Alessandro%20Wasserman%2C%20Larry%20Generalized%20density%20clustering%202010"
        },
        {
            "id": "44",
            "entry": "[44] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high confidence from random samples. Discrete & Computational Geometry, 39(1-3):419\u2013 441, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niyogi%2C%20Partha%20Smale%2C%20Stephen%20Weinberger%2C%20Shmuel%20Finding%20the%20homology%20of%20submanifolds%20with%20high%20confidence%20from%20random%20samples%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niyogi%2C%20Partha%20Smale%2C%20Stephen%20Weinberger%2C%20Shmuel%20Finding%20the%20homology%20of%20submanifolds%20with%20high%20confidence%20from%20random%20samples%202008"
        },
        {
            "id": "45",
            "entry": "[45] Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. Minimax manifold estimation. Journal of Machine Learning Research, 13(May):1263\u20131291, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genovese%2C%20Christopher%20Perone-Pacifico%2C%20Marco%20Verdinelli%2C%20Isabella%20Wasserman%2C%20Larry%20Minimax%20manifold%20estimation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genovese%2C%20Christopher%20Perone-Pacifico%2C%20Marco%20Verdinelli%2C%20Isabella%20Wasserman%2C%20Larry%20Minimax%20manifold%20estimation%202012"
        },
        {
            "id": "46",
            "entry": "[46] Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, and Larry Wasserman. Cluster trees on manifolds. In Advances in Neural Information Processing Systems, pages 2679\u20132687, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balakrishnan%2C%20Sivaraman%20Narayanan%2C%20Srivatsan%20Rinaldo%2C%20Alessandro%20Singh%2C%20Aarti%20Cluster%20trees%20on%20manifolds%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balakrishnan%2C%20Sivaraman%20Narayanan%2C%20Srivatsan%20Rinaldo%2C%20Alessandro%20Singh%2C%20Aarti%20Cluster%20trees%20on%20manifolds%202013"
        },
        {
            "id": "47",
            "entry": "[47] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning. Springer, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20Jerome%20Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20The%20Elements%20of%20Statistical%20Learning%202001"
        },
        {
            "id": "48",
            "entry": "[48] Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "49",
            "entry": "[49] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20and%20Andrew%20Y%20Ng.%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning"
        },
        {
            "id": "50",
            "entry": "[50] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "51",
            "entry": "[51] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "52",
            "entry": "[52] Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classification using small training sample size. 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), pages 730\u2013734, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shuying%20Liu%20and%20Weihong%20Deng%20Very%20deep%20convolutional%20neural%20network%20based%20image%20classification%20using%20small%20training%20sample%20size%202015%203rd%20IAPR%20Asian%20Conference%20on%20Pattern%20Recognition%20ACPR%20pages%20730734%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shuying%20Liu%20and%20Weihong%20Deng%20Very%20deep%20convolutional%20neural%20network%20based%20image%20classification%20using%20small%20training%20sample%20size%202015%203rd%20IAPR%20Asian%20Conference%20on%20Pattern%20Recognition%20ACPR%20pages%20730734%202015"
        },
        {
            "id": "53",
            "entry": "[53] Fran\u00e7ois Chollet et al. Keras. https://github.com/fchollet/keras, 2015.",
            "url": "https://github.com/fchollet/keras"
        },
        {
            "id": "54",
            "entry": "[54] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree. In Advances in Neural Information Processing Systems, pages 343\u2013351, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20Kamalika%20Dasgupta%2C%20Sanjoy%20Rates%20of%20convergence%20for%20the%20cluster%20tree%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20Kamalika%20Dasgupta%2C%20Sanjoy%20Rates%20of%20convergence%20for%20the%20cluster%20tree%202010"
        },
        {
            "id": "55",
            "entry": "[55] Luc Devroye, Laszlo Gyorfi, Adam Krzyzak, and G\u00e1bor Lugosi. On the strong universal consistency of nearest neighbor regression function estimates. The Annals of Statistics, pages 1371\u20131385, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luc%20Devroye%2C%20Laszlo%20Gyorfi%2C%20Adam%20Krzyzak%20Lugosi%2C%20G%C3%A1bor%20On%20the%20strong%20universal%20consistency%20of%20nearest%20neighbor%20regression%20function%20estimates%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luc%20Devroye%2C%20Laszlo%20Gyorfi%2C%20Adam%20Krzyzak%20Lugosi%2C%20G%C3%A1bor%20On%20the%20strong%20universal%20consistency%20of%20nearest%20neighbor%20regression%20function%20estimates%201994"
        },
        {
            "id": "56",
            "entry": "[56] Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-NN density and mode estimation. In Advances in Neural Information Processing Systems, pages 2555\u20132563, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20Sanjoy%20Kpotufe%2C%20Samory%20Optimal%20rates%20for%20k-NN%20density%20and%20mode%20estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20Sanjoy%20Kpotufe%2C%20Samory%20Optimal%20rates%20for%20k-NN%20density%20and%20mode%20estimation%202014"
        },
        {
            "id": "57",
            "entry": "[57] Fr\u00e9d\u00e9ric Chazal. An upper bound for the volume of geodesic balls in submanifolds of Euclidean spaces. https://geometrica.saclay.inria.fr/team/Fred.Chazal/BallVolumeJan2013.pdf, 2013.",
            "url": "https://geometrica.saclay.inria.fr/team/Fred.Chazal/BallVolumeJan2013.pdf"
        },
        {
            "id": "58",
            "entry": "[58] Heinrich Jiang. Uniform convergence rates for kernel density estimation. In International Conference on Machine Learning, pages 1694\u20131703, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Heinrich%20Uniform%20convergence%20rates%20for%20kernel%20density%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Heinrich%20Uniform%20convergence%20rates%20for%20kernel%20density%20estimation%202017"
        }
    ]
}
