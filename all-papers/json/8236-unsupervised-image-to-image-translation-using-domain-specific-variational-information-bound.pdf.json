{
    "filename": "8236-unsupervised-image-to-image-translation-using-domain-specific-variational-information-bound.pdf",
    "metadata": {
        "title": "Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound",
        "author": "Hadi Kazemi, Sobhan Soleymani, Fariborz Taherkhani, Seyed Iranmanesh, Nasser Nasrabadi",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8236-unsupervised-image-to-image-translation-using-domain-specific-variational-information-bound.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches usually fail to model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images."
    },
    "keywords": [
        {
            "term": "image translation",
            "url": "https://en.wikipedia.org/wiki/image_translation"
        },
        {
            "term": "UNIT",
            "url": "https://en.wikipedia.org/wiki/UNIT"
        },
        {
            "term": "super resolution",
            "url": "https://en.wikipedia.org/wiki/super_resolution"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        }
    ],
    "highlights": [
        "Image-to-image translation is the major goal for many computer vision problems, such as sketch to photo-realistic image translation [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], style transfer [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], inpainting missing image regions [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], colorization of grayscale images [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], and super-resolution [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "If corresponding image pairs are available in both source and target domains, these problems can be studied in a supervised setting",
        "In this paper, we focus on the design of a framework for unsupervised image-to-image translation",
        "The learned domain-specific code is supposed to represent the properties of the source image which have no representation in the target domain",
        "We introduced a framework for one-to-many cross-domain image-to-image translation in an unsupervised setting",
        "A unit normal distribution is imposed over the domain-specific latent distribution, which let us control the domainspecific properties of the generated image in the output domain"
    ],
    "key_statements": [
        "Image-to-image translation is the major goal for many computer vision problems, such as sketch to photo-realistic image translation [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], style transfer [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], inpainting missing image regions [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], colorization of grayscale images [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], and super-resolution [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "If corresponding image pairs are available in both source and target domains, these problems can be studied in a supervised setting",
        "In this paper, we focus on the design of a framework for unsupervised image-to-image translation",
        "The former is learned by maximizing its mutual information with the source domain while simultaneously we minimize the mutual information between this code and the translated image in the target domain",
        "The cycle-consistency constraint assumes that the source image can be reconstructed from the generated image, in the target domain, without any extra domain-specific information [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>]",
        "The learned domain-specific code is supposed to represent the properties of the source image which have no representation in the target domain",
        "We propose an unsupervised method to learn the domain-specific information of the source data distribution which has minimum information about the target domain",
        "To learn the source domain-specific code, vx, we propose to minimize the mutual information between vx and the target domain distribution, while simultaneously, we maximize the mutual information between vx and the source domain distribution",
        "Our framework learns a disentangled representation of content and style, which provides users more control on the image translation outputs",
        "We introduced a framework for one-to-many cross-domain image-to-image translation in an unsupervised setting",
        "A unit normal distribution is imposed over the domain-specific latent distribution, which let us control the domainspecific properties of the generated image in the output domain"
    ],
    "summary": [
        "Image-to-image translation is the major goal for many computer vision problems, such as sketch to photo-realistic image translation [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], style transfer [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], inpainting missing image regions [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], colorization of grayscale images [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], and super-resolution [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "Since the current unsupervised techniques are implemented mainly based on the \"cycle consistency\" [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], the translated image in the target domain may encode domain-specific information of the source domain (Figure 1).",
        "The domain-invariant code in combination with a target domain-specific code, sampled from a desired distribution, is fed to a generator which translates them into the corresponding target domain image.",
        "The former is learned by maximizing its mutual information with the source domain while simultaneously we minimize the mutual information between this code and the translated image in the target domain.",
        "These loss terms are crucial in the unsupervised framework, since domain-invariant information may go through the domain-specific path to satisfy the cycle consistency in the backward path.",
        "Based on the proposed domain-specific learning scheme, we introduce a framework for one-to-many cross-domain image-to-image translation in an unsupervised setting.",
        "The cycle-consistency constraint assumes that the source image can be reconstructed from the generated image, in the target domain, without any extra domain-specific information [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>].",
        "The learned domain-specific code is supposed to represent the properties of the source image which have no representation in the target domain.",
        "The encoder-generators, {Exd, Gx} and {Eyd, Gy}, constitute two VAEs. Inspired by CycleGAN model [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], we trained our network in two cycles; X \u2192 Y \u2192 X and Y \u2192 X \u2192 Y, where X and Y represent the source and target domains, respectively.1 Each cycle consists of forward and backward paths.",
        "Visual results on translation task show how domain-specific code can alter the style of generated images in a new domain.",
        "The reference image is fed to the output domainspecific encoder to extract its domain-specific code.",
        "Figures 6 show the results using domain-specific codes extracted from multiple reference images to translate edges into realistic photos.",
        "Removing the domain-specific code cycle-consistency criteria results in a partial mode collapse in the model, with many outputs being almost identical, which reduces the LPIPS.",
        "In contrast to the previous works, our approach learns a distinct domainspecific code for each of the two modalities, maximizing a domain-specific variational information bound.",
        "A unit normal distribution is imposed over the domain-specific latent distribution, which let us control the domainspecific properties of the generated image in the output domain.",
        "To generate diverse target domain images, we extract domain-specific codes from reference images, or sample them from a prior distribution.",
        "These domain-specific codes, combined with the learned domain-invariant code, result in target domain images with different target domain-specific properties"
    ],
    "headline": "We propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00410"
        },
        {
            "id": "2",
            "entry": "[2] A. Almahairi, S. Rajeswar, A. Sordoni, P. Bachman, and A. Courville. Augmented CycleGAN: Learning Many-to-Many mappings from unpaired data. arXiv preprint arXiv:1802.10151, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10151"
        },
        {
            "id": "3",
            "entry": "[3] A. Bansal, Y. Sheikh, and D. Ramanan. Pixelnn: Example-based image synthesis. arXiv preprint arXiv:1708.05349, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05349"
        },
        {
            "id": "4",
            "entry": "[4] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousmalis%2C%20K.%20Silberman%2C%20N.%20Dohan%2C%20D.%20Erhan%2C%20D.%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Q.%20Koltun%2C%20V.%20Photographic%20image%20synthesis%20with%20cascaded%20refinement%20networks%202017"
        },
        {
            "id": "6",
            "entry": "[6] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "7",
            "entry": "[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "8",
            "entry": "[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "9",
            "entry": "[9] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626\u20136637, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "10",
            "entry": "[10] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "11",
            "entry": "[11] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification. ACM Transactions on Graphics (TOG), 35(4):110, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Let%20there%20be%20color%21%3A%20joint%20end-to-end%20learning%20of%20global%20and%20local%20image%20priors%20for%20automatic%20image%20colorization%20with%20simultaneous%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Let%20there%20be%20color%21%3A%20joint%20end-to-end%20learning%20of%20global%20and%20local%20image%20priors%20for%20automatic%20image%20colorization%20with%20simultaneous%20classification%202016"
        },
        {
            "id": "12",
            "entry": "[12] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "13",
            "entry": "[13] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694\u2013711.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution"
        },
        {
            "id": "14",
            "entry": "[14] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "15",
            "entry": "[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "16",
            "entry": "[16] A. Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1404.5997"
        },
        {
            "id": "17",
            "entry": "[17] A. B. L. Larsen, S. K. S\u00f8nderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.09300"
        },
        {
            "id": "18",
            "entry": "[18] C. Ledig, L. Theis, F. Husz\u00e1r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network.%20arXiv%20p%202016"
        },
        {
            "id": "19",
            "entry": "[19] C. Li and M. Wand. Precomputed real-time texture synthesis with Markovian generative adversarial networks. In European Conference on Computer Vision, pages 702\u2013716.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Wand%2C%20M.%20Precomputed%20real-time%20texture%20synthesis%20with%20Markovian%20generative%20adversarial%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Wand%2C%20M.%20Precomputed%20real-time%20texture%20synthesis%20with%20Markovian%20generative%20adversarial%20networks"
        },
        {
            "id": "20",
            "entry": "[20] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, pages 700\u2013708, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "22",
            "entry": "[22] C. Peng, X. Gao, N. Wang, and J. Li. Superpixel-based face sketch\u2013photo synthesis. IEEE Transactions on Circuits and Systems for Video Technology, 27(2):288\u2013299, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20C.%20Gao%2C%20X.%20Wang%2C%20N.%20Li%2C%20J.%20Superpixel-based%20face%20sketch%E2%80%93photo%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20C.%20Gao%2C%20X.%20Wang%2C%20N.%20Li%2C%20J.%20Superpixel-based%20face%20sketch%E2%80%93photo%20synthesis%202017"
        },
        {
            "id": "23",
            "entry": "[23] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and variational inference in deep latent gaussian models. In International Conference on Machine Learning, volume 2, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20variational%20inference%20in%20deep%20latent%20gaussian%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20variational%20inference%20in%20deep%20latent%20gaussian%20models%202014"
        },
        {
            "id": "24",
            "entry": "[24] A. Royer, K. Bousmalis, S. Gouws, F. Bertsch, I. Moressi, F. Cole, and K. Murphy. Xgan: Unsupervised image-to-image translation for many-to-many mappings. arXiv preprint arXiv:1711.05139, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05139"
        },
        {
            "id": "25",
            "entry": "[25] P. Sangkloy, J. Lu, C. Fang, F. Yu, and J. Hays. Scribbler: Controlling deep image synthesis with sketch and color. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sangkloy%2C%20P.%20Lu%2C%20J.%20Fang%2C%20C.%20Yu%2C%20F.%20Scribbler%3A%20Controlling%20deep%20image%20synthesis%20with%20sketch%20and%20color%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sangkloy%2C%20P.%20Lu%2C%20J.%20Fang%2C%20C.%20Yu%2C%20F.%20Scribbler%3A%20Controlling%20deep%20image%20synthesis%20with%20sketch%20and%20color%202017"
        },
        {
            "id": "26",
            "entry": "[26] P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. Technical report, Colorado University at Boulder Department of Computer Science, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20P.%20Information%20processing%20in%20dynamical%20systems%3A%20Foundations%20of%20harmony%20theory%201986"
        },
        {
            "id": "27",
            "entry": "[27] X. Tang and X. Wang. Face sketch synthesis and recognition. In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, pages 687\u2013694, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20X.%20Wang%2C%20X.%20Face%20sketch%20synthesis%20and%20recognition%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20X.%20Wang%2C%20X.%20Face%20sketch%20synthesis%20and%20recognition%202003"
        },
        {
            "id": "28",
            "entry": "[28] R. Tylecek and R. \u0160\u00e1ra. Spatial pattern templates for recognition of objects with regular structure. In German Conference on Pattern Recognition, pages 364\u2013374.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tylecek%2C%20R.%20%C5%A0%C3%A1ra%2C%20R.%20Spatial%20pattern%20templates%20for%20recognition%20of%20objects%20with%20regular%20structure",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tylecek%2C%20R.%20%C5%A0%C3%A1ra%2C%20R.%20Spatial%20pattern%20templates%20for%20recognition%20of%20objects%20with%20regular%20structure"
        },
        {
            "id": "29",
            "entry": "[29] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, pages 4790\u20134798, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20A.%20Kalchbrenner%2C%20N.%20Espeholt%2C%20L.%20Vinyals%2C%20O.%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20A.%20Kalchbrenner%2C%20N.%20Espeholt%2C%20L.%20Vinyals%2C%20O.%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016"
        },
        {
            "id": "30",
            "entry": "[30] X. Wang and A. Gupta. Generative image modeling using style and structure adversarial networks. In European Conference on Computer Vision, pages 318\u2013335.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Gupta%2C%20A.%20Generative%20image%20modeling%20using%20style%20and%20structure%20adversarial%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Gupta%2C%20A.%20Generative%20image%20modeling%20using%20style%20and%20structure%20adversarial%20networks"
        },
        {
            "id": "31",
            "entry": "[31] A. Yu and K. Grauman. Fine-grained visual comparisons with local learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 192\u2013199, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20A.%20Grauman%2C%20K.%20Fine-grained%20visual%20comparisons%20with%20local%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20A.%20Grauman%2C%20K.%20Fine-grained%20visual%20comparisons%20with%20local%20learning%202014"
        },
        {
            "id": "32",
            "entry": "[32] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. pages 649\u2013666.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Colorful%20image%20colorization"
        },
        {
            "id": "33",
            "entry": "[33] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. arXiv preprint arXiv:1801.03924, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03924"
        },
        {
            "id": "34",
            "entry": "[34] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03126"
        },
        {
            "id": "35",
            "entry": "[35] S. Zhao, J. Song, and S. Ermon. Infovae: Information maximizing variational autoencoders. arXiv preprint arXiv:1706.02262, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02262"
        },
        {
            "id": "36",
            "entry": "[36] J.-Y. Zhu, P. Kr\u00e4henb\u00fchl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, pages 597\u2013613.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Kr%C3%A4henb%C3%BChl%2C%20P.%20Shechtman%2C%20E.%20Efros%2C%20A.A.%20Generative%20visual%20manipulation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Kr%C3%A4henb%C3%BChl%2C%20P.%20Shechtman%2C%20E.%20Efros%2C%20A.A.%20Generative%20visual%20manipulation"
        },
        {
            "id": "37",
            "entry": "[37] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        },
        {
            "id": "38",
            "entry": "[38] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, pages 465\u2013476, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017"
        },
        {
            "id": "39",
            "entry": "[39] F. Zohrizadeh, M. Kheirandishfard, and F. Kamangar. Image segmentation using sparse subset selection. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1470\u20131479, March 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zohrizadeh%2C%20F.%20Kheirandishfard%2C%20M.%20Kamangar%2C%20F.%20Image%20segmentation%20using%20sparse%20subset%20selection%202018-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zohrizadeh%2C%20F.%20Kheirandishfard%2C%20M.%20Kamangar%2C%20F.%20Image%20segmentation%20using%20sparse%20subset%20selection%202018-03"
        }
    ]
}
