{
    "filename": "7306-an-efficient-pruning-algorithm-for-robust-isotonic-regression.pdf",
    "metadata": {
        "title": "An Efficient Pruning Algorithm for Robust Isotonic Regression",
        "author": "Cong Han Lim",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7306-an-efficient-pruning-algorithm-for-robust-isotonic-regression.pdf"
        },
        "journal": "Problem",
        "volume": "1",
        "abstract": "We study a generalization of the classic isotonic regression problem where we allow separable nonconvex objective functions, focusing on the case where the functions are estimators used in robust regression. One can solve this problem to within -accuracy (of the global minimum) in O(n/ ) using a simple dynamic program, and the complexity of this approach is independent of the underlying functions. We introduce an algorithm that combines techniques from the convex case with branchand-bound ideas that naturally exploits the shape of the functions. Our algorithm achieves the best known bounds for both the convex case (O(n log(1/ ))) and the general nonconvex case. Our experiments show that this algorithm performs much faster than the dynamic programming approach on robust estimators, especially as the desired accuracy increases."
    },
    "keywords": [
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        }
    ],
    "highlights": [
        "The main approach for solving Problem (2) for general nonconvex functions is to use dynamic programming) that runs in O",
        "We propose a fast pruning algorithm that builds upon the standard DP algorithm for solving the separable nonconvex isotonic regression problem (1) to any arbitrary accuracy",
        "We demonstrate that the pruning rules we develop retain the correct points and intervals required to reach the global optimal value, and in the convex case our algorithm becomes a variant of the fast O(n log k) scaling algorithm",
        "Besides developing more pruning rules that can work on a larger range of nonconvex fi functions, there are two main directions for extensions to this work, mirroring the line of developments for the classic isotonic regression problem",
        "The first is go beyond monotonicity constraints and instead consider chain functions gi that link together adjacent indices"
    ],
    "key_statements": [
        "The main approach for solving Problem (2) for general nonconvex functions is to use dynamic programming) that runs in O",
        "We propose a fast pruning algorithm that builds upon the standard DP algorithm for solving the separable nonconvex isotonic regression problem (1) to any arbitrary accuracy",
        "We demonstrate that the pruning rules we develop retain the correct points and intervals required to reach the global optimal value, and in the convex case our algorithm becomes a variant of the fast O(n log k) scaling algorithm",
        "Besides developing more pruning rules that can work on a larger range of nonconvex fi functions, there are two main directions for extensions to this work, mirroring the line of developments for the classic isotonic regression problem",
        "The first is go beyond monotonicity constraints and instead consider chain functions gi that link together adjacent indices"
    ],
    "summary": [
        "The main approach for solving Problem (2) for general nonconvex functions is to use dynamic programming) that runs in O.",
        "Our main contribution is an algorithm that achieves O in the general case and O(n log k) in the convex case by exploiting the following key fact \u2014 the dynamic programming method runs in time linear in the sum of possible xi values over all xi.",
        "This leads to an algorithm that requires far less function evaluations to achieve the same accuracy as dynamic programming, which translates into a faster running time even considering the additional work needed in each iteration.",
        "The main drawback of the dynamic programming approach is that it requires us to pick the desired accuracy a priori via choosing an appropriate k value and the overall running time is O, no matter the properties of the fi functions.",
        "The values of the Cik, Aki functions are the same for both problem formulations on the points in Si. The modified operations can be performed efficiently by maintaining the appropriate minimum values, and this results in an algorithm with a complexity of just O(|S1| + .",
        "The previous pruning rule used information on function values on the points in Si. We describe a rule that uses linear underestimators on intervals in Ii. In the convex case, one can think of this as using subgradient information 3.",
        "Algorithm 6 solves Problem (2) in O time in general, and O(n log k) time for convex functions if we use subgradient information.",
        "We only require the linear underestimator rule for the O(n log k) convex bound, since that suffices to ensure that sets Si have at most a few points.",
        "We see that the number of points generated throughout the course of the pruning algorithm scales logarithmically with k, showing that even though we are using a nonconvex estimator, the computational results reflect what we expect to see in the convex case.",
        "We demonstrate that the pruning rules we develop retain the correct points and intervals required to reach the global optimal value, and in the convex case our algorithm becomes a variant of the fast O(n log k) scaling algorithm.",
        "Besides developing more pruning rules that can work on a larger range of nonconvex fi functions, there are two main directions for extensions to this work, mirroring the line of developments for the classic isotonic regression problem.",
        "It may be possible to adapt the general submodular-based approach developed by <a class=\"ref-link\" id=\"cBach_2017_a\" href=\"#rBach_2017_a\"><a class=\"ref-link\" id=\"cBach_2017_a\" href=\"#rBach_2017_a\">Bach (2017</a></a>), which works in both the above mentioned extensions"
    ],
    "headline": "We study a generalization of the classic isotonic regression problem where we allow separable nonconvex objective functions, focusing on the case where the functions are estimators used in robust regression",
    "reference_links": [
        {
            "id": "Ahuja_2001_a",
            "entry": "Ahuja, R. K. and Orlin, J. B. (2001). A Fast Scaling Algorithm for Minimizing Separable Convex Functions Subject to Chain Constraints. Operations Research, 49(5):784\u2013789.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahuja%2C%20R.K.%20Orlin%2C%20J.B.%20A%20Fast%20Scaling%20Algorithm%20for%20Minimizing%20Separable%20Convex%20Functions%20Subject%20to%20Chain%20Constraints%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahuja%2C%20R.K.%20Orlin%2C%20J.B.%20A%20Fast%20Scaling%20Algorithm%20for%20Minimizing%20Separable%20Convex%20Functions%20Subject%20to%20Chain%20Constraints%202001"
        },
        {
            "id": "Ayer_et+al_1955_a",
            "entry": "Ayer, M., Brunk, H. D., Ewing, G. M., Reid, W. T., and Silverman, E. (1955). An Empirical Distribution Function for Sampling with Incomplete Information. The Annals of Mathematical Statistics, 26(4):641\u2013647.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ayer%2C%20M.%20Brunk%2C%20H.D.%20Ewing%2C%20G.M.%20Reid%2C%20W.T.%20An%20Empirical%20Distribution%20Function%20for%20Sampling%20with%20Incomplete%20Information%201955",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ayer%2C%20M.%20Brunk%2C%20H.D.%20Ewing%2C%20G.M.%20Reid%2C%20W.T.%20An%20Empirical%20Distribution%20Function%20for%20Sampling%20with%20Incomplete%20Information%201955"
        },
        {
            "id": "Bach_2013_a",
            "entry": "Bach, F. (2013). Learning with submodular functions: A convex optimization perspective. Foundations and Trends R in Machine Learning, 6(2-3):145\u2013373.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013"
        },
        {
            "id": "Bach_2015_a",
            "entry": "Bach, F. (2015). Submodular Functions: from Discrete to Continous Domains. arXiv:1511.00394 [cs, math]. arXiv: 1511.00394.",
            "arxiv_url": "https://arxiv.org/pdf/1511.00394"
        },
        {
            "id": "Bach_2017_a",
            "entry": "Bach, F. (2017). Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization. arXiv:1707.09157 [cs, stat]. arXiv: 1707.09157.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09157"
        },
        {
            "id": "Best_et+al_2000_a",
            "entry": "Best, M. J., Chakravarti, N., and Ubhaya, V. A. (2000). Minimizing Separable Convex Functions Subject to Simple Chain Constraints. SIAM Journal on Optimization, 10(3):658\u2013672.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Best%2C%20M.J.%20Chakravarti%2C%20N.%20Ubhaya%2C%20V.A.%20Minimizing%20Separable%20Convex%20Functions%20Subject%20to%20Simple%20Chain%20Constraints%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Best%2C%20M.J.%20Chakravarti%2C%20N.%20Ubhaya%2C%20V.A.%20Minimizing%20Separable%20Convex%20Functions%20Subject%20to%20Simple%20Chain%20Constraints%202000"
        },
        {
            "id": "Bogdan_et+al_2013_a",
            "entry": "Bogdan, M., van den Berg, E., Su, W., and Candes, E. (2013). Statistical estimation and testing via the sorted L1 norm. arXiv:1310.1969.",
            "arxiv_url": "https://arxiv.org/pdf/1310.1969"
        },
        {
            "id": "Brunk_1955_a",
            "entry": "Brunk, H. D. (1955). Maximum Likelihood Estimates of Monotone Parameters. The Annals of Mathematical Statistics, 26(4):607\u2013616.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brunk%2C%20H.D.%20Maximum%20Likelihood%20Estimates%20of%20Monotone%20Parameters%201955",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brunk%2C%20H.D.%20Maximum%20Likelihood%20Estimates%20of%20Monotone%20Parameters%201955"
        },
        {
            "id": "Burdakov_2017_a",
            "entry": "Burdakov, O. and Sysoev, O. (2017). A Dual Active-Set Algorithm for Regularized Monotonic Regression. Journal of Optimization Theory and Applications, 172(3):929\u2013949.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burdakov%2C%20O.%20Sysoev%2C%20O.%20A%20Dual%20Active-Set%20Algorithm%20for%20Regularized%20Monotonic%20Regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burdakov%2C%20O.%20Sysoev%2C%20O.%20A%20Dual%20Active-Set%20Algorithm%20for%20Regularized%20Monotonic%20Regression%202017"
        },
        {
            "id": "Felzenszwalb_2012_a",
            "entry": "Felzenszwalb, P. F. and Huttenlocher, D. P. (2012). Distance Transforms of Sampled Functions. Theory of Computing, 8:415\u2013428.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Felzenszwalb%2C%20P.F.%20Huttenlocher%2C%20D.P.%20Distance%20Transforms%20of%20Sampled%20Functions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Felzenszwalb%2C%20P.F.%20Huttenlocher%2C%20D.P.%20Distance%20Transforms%20of%20Sampled%20Functions%202012"
        },
        {
            "id": "Gunasekar_et+al_2016_a",
            "entry": "Gunasekar, S., Koyejo, O. O., and Ghosh, J. (2016). Preference Completion from Partial Rankings. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R., editors, Advances in Neural Information Processing Systems 29, pages 1370\u20131378. Curran Associates, Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20S.%20Koyejo%2C%20O.O.%20Ghosh%2C%20J.%20Preference%20Completion%20from%20Partial%20Rankings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20S.%20Koyejo%2C%20O.O.%20Ghosh%2C%20J.%20Preference%20Completion%20from%20Partial%20Rankings%202016"
        },
        {
            "id": "Hampel_et+al_2011_a",
            "entry": "Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. (2011). Robust statistics: the approach based on influence functions, volume 196. John Wiley & Sons.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hampel%2C%20F.R.%20Ronchetti%2C%20E.M.%20Rousseeuw%2C%20P.J.%20Stahel%2C%20W.A.%20Robust%20statistics%3A%20the%20approach%20based%20on%20influence%20functions%2C%20volume%20196%202011"
        },
        {
            "id": "Hochbaum_2017_a",
            "entry": "Hochbaum, D. and Lu, C. (2017). A Faster Algorithm Solving a Generalization of Isotonic Median Regression and a Class of Fused Lasso Problems. SIAM Journal on Optimization, 27(4):2563\u2013 2596.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochbaum%2C%20D.%20Lu%2C%20C.%20A%20Faster%20Algorithm%20Solving%20a%20Generalization%20of%20Isotonic%20Median%20Regression%20and%20a%20Class%20of%20Fused%20Lasso%20Problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochbaum%2C%20D.%20Lu%2C%20C.%20A%20Faster%20Algorithm%20Solving%20a%20Generalization%20of%20Isotonic%20Median%20Regression%20and%20a%20Class%20of%20Fused%20Lasso%20Problems%202017"
        },
        {
            "id": "Hochbaum_2001_a",
            "entry": "Hochbaum, D. S. (2001). An Efficient Algorithm for Image Segmentation, Markov Random Fields and Related Problems. J. ACM, 48(4):686\u2013701.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochbaum%2C%20D.S.%20An%20Efficient%20Algorithm%20for%20Image%20Segmentation%2C%20Markov%20Random%20Fields%20and%20Related%20Problems%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochbaum%2C%20D.S.%20An%20Efficient%20Algorithm%20for%20Image%20Segmentation%2C%20Markov%20Random%20Fields%20and%20Related%20Problems%202001"
        },
        {
            "id": "Huber_2004_a",
            "entry": "Huber, P. (2004). Robust Statistics. Wiley Series in Probability and Statistics - Applied Probability and Statistics Section Series. Wiley.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huber%2C%20P.%20Robust%20Statistics.%20Wiley%20Series%20in%20Probability%20and%20Statistics%20-%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huber%2C%20P.%20Robust%20Statistics.%20Wiley%20Series%20in%20Probability%20and%20Statistics%20-%202004"
        },
        {
            "id": "Kalai_2009_a",
            "entry": "Kalai, A. and Sastry, R. (2009). The Isotron Algorithm: High-Dimensional Isotonic Regression. In Conference on Learning Theory.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalai%2C%20A.%20Sastry%2C%20R.%20The%20Isotron%20Algorithm%3A%20High-Dimensional%20Isotonic%20Regression.%20In%20Conference%20on%20Learning%20Theory%202009"
        },
        {
            "id": "Kolmogorov_et+al_2016_a",
            "entry": "Kolmogorov, V., Pock, T., and Rolinek, M. (2016). Total Variation on a Tree. SIAM Journal on Imaging Sciences, 9(2):605\u2013636.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolmogorov%2C%20V.%20Pock%2C%20T.%20Rolinek%2C%20M.%20Total%20Variation%20on%20a%20Tree%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolmogorov%2C%20V.%20Pock%2C%20T.%20Rolinek%2C%20M.%20Total%20Variation%20on%20a%20Tree%202016"
        },
        {
            "id": "Lim_2016_a",
            "entry": "Lim, C. H. and Wright, S. J. (2016). Efficient bregman projections onto the permutahedron and related polytopes. In Gretton, A. and Robert, C. C., editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016), page 1205\u20131213.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lim%2C%20C.H.%20Wright%2C%20S.J.%20Efficient%20bregman%20projections%20onto%20the%20permutahedron%20and%20related%20polytopes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lim%2C%20C.H.%20Wright%2C%20S.J.%20Efficient%20bregman%20projections%20onto%20the%20permutahedron%20and%20related%20polytopes%202016"
        },
        {
            "id": "Stout_2014_a",
            "entry": "Stout, Q. F. (2014). Fastest isotonic regression algorithms.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stout%2C%20Q.F.%20Fastest%20isotonic%20regression%20algorithms%202014"
        },
        {
            "id": "Suehiro_et+al_2012_a",
            "entry": "Suehiro, D., Hatano, K., Kijima, S., Takimoto, E., and Nagano, K. (2012). Online prediction under submodular constraints. In Bshouty, N., Stoltz, G., Vayatis, N., and Zeugmann, T., editors, Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pages 260\u2013274. Springer Berlin Heidelberg.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suehiro%2C%20D.%20Hatano%2C%20K.%20Kijima%2C%20S.%20Takimoto%2C%20E.%20Online%20prediction%20under%20submodular%20constraints%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suehiro%2C%20D.%20Hatano%2C%20K.%20Kijima%2C%20S.%20Takimoto%2C%20E.%20Online%20prediction%20under%20submodular%20constraints%202012"
        },
        {
            "id": "Sysoev_2016_a",
            "entry": "Sysoev, O. and Burdakov, O. (2016). A Smoothed Monotonic Regression via L2 Regularization. Link\u00f6ping University Electronic Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sysoev%2C%20O.%20Burdakov%2C%20O.%20A%20Smoothed%20Monotonic%20Regression%20via%20L2%20Regularization%202016"
        },
        {
            "id": "Tibshirani_et+al_2011_a",
            "entry": "Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-Isotonic Regression. Technometrics, 53(1):54\u201361.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20R.J.%20Hoefling%2C%20H.%20Tibshirani%2C%20R.%20Nearly-Isotonic%20Regression%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20R.J.%20Hoefling%2C%20H.%20Tibshirani%2C%20R.%20Nearly-Isotonic%20Regression%202011"
        },
        {
            "id": "Yasutake_et+al_2011_a",
            "entry": "Yasutake, S., Hatano, K., Kijima, S., Takimoto, E., and Takeda, M. (2011). Online linear optimization over permutations. In Asano, T., Nakano, S.-i., Okamoto, Y., and Watanabe, O., editors, Algorithms and Computation, volume 7074 of Lecture Notes in Computer Science, pages 534\u2013543. Springer Berlin Heidelberg.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yasutake%2C%20S.%20Hatano%2C%20K.%20Kijima%2C%20S.%20Takimoto%2C%20E.%20Online%20linear%20optimization%20over%20permutations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yasutake%2C%20S.%20Hatano%2C%20K.%20Kijima%2C%20S.%20Takimoto%2C%20E.%20Online%20linear%20optimization%20over%20permutations%202011"
        },
        {
            "id": "Zeng_2014_a",
            "entry": "Zeng, X. and Figueiredo, M. A. T. (2014). The Ordered Weighted 1 Norm: Atomic Formulation, Projections, and Algorithms. arXiv:1409.4271.",
            "arxiv_url": "https://arxiv.org/pdf/1409.4271"
        }
    ]
}
