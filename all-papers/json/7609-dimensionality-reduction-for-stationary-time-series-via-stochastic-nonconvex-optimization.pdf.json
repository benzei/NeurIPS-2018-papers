{
    "filename": "7609-dimensionality-reduction-for-stationary-time-series-via-stochastic-nonconvex-optimization.pdf",
    "metadata": {
        "title": "Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization",
        "author": "Minshuo Chen, Lin Yang, Mengdi Wang, Tuo Zhao",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7609-dimensionality-reduction-for-stationary-time-series-via-stochastic-nonconvex-optimization.pdf"
        },
        "abstract": "Stochastic optimization naturally arises in machine learning. Efficient algorithms with provable guarantees, however, are still largely missing, when the objective function is nonconvex and the data points are dependent. This paper studies this fundamental challenge through a streaming PCA problem for stationary time series data. Specifically, our goal is to estimate the principle component of time series data with respect to the covariance matrix of the stationary distribution. Computationally, we propose a variant of Oja\u2019s algorithm combined with downsampling to control the bias of the stochastic gradient caused by the data dependency. Theoretically, we quantify the uncertainty of our proposed stochastic algorithm based on diffusion approximations. This allows us to prove the asymptotic rate of convergence and further implies near optimal asymptotic sample complexity. Numerical experiments are provided to support our analysis."
    },
    "keywords": [
        {
            "term": "principal component analysis",
            "url": "https://en.wikipedia.org/wiki/principal_component_analysis"
        },
        {
            "term": "stochastic differential equation",
            "url": "https://en.wikipedia.org/wiki/stochastic_differential_equation"
        },
        {
            "term": "diffusion approximation",
            "url": "https://en.wikipedia.org/wiki/diffusion_approximation"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "dimensionality reduction",
            "url": "https://en.wikipedia.org/wiki/dimensionality_reduction"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "This paper proposes to attack stochastic nonconvex optimization problems for dependent data by investigating a simple but fundamental problem in machine learning \u2014 Streaming principal component analysis for stationary time series",
        "Many machine learning problems can be formulated as a stochastic optimization problem in the following form, min u",
        "Where f is a possibly nonconvex loss function, Z denotes the random sample generated from some underlying distribution D ( known as statistical model), u is the parameter of our interest, and",
        "This paper proposes to attack stochastic nonconvex optimization problems for dependent data by investigating a simple but fundamental problem in machine learning \u2014 Streaming principal component analysis for stationary time series",
        "To address the first challenge, we propose a variant of Oja\u2019s algorithm to handle data dependency",
        "We show that the solution trajectory weakly converges to the solution of a stochastic differential equation (SDE), which enables us to quantify the uncertainty of the proposed algorithm"
    ],
    "key_statements": [
        "This paper proposes to attack stochastic nonconvex optimization problems for dependent data by investigating a simple but fundamental problem in machine learning \u2014 Streaming principal component analysis for stationary time series",
        "Many machine learning problems can be formulated as a stochastic optimization problem in the following form, min u",
        "Where f is a possibly nonconvex loss function, Z denotes the random sample generated from some underlying distribution D ( known as statistical model), u is the parameter of our interest, and",
        "This paper proposes to attack stochastic nonconvex optimization problems for dependent data by investigating a simple but fundamental problem in machine learning \u2014 Streaming principal component analysis for stationary time series",
        "To address the first challenge, we propose a variant of Oja\u2019s algorithm to handle data dependency",
        "We show that the solution trajectory weakly converges to the solution of a stochastic differential equation (SDE), which enables us to quantify the uncertainty of the proposed algorithm",
        "We show that for Gaussian vector autoregressive with \u21e2 =\u21e3 kAk2 < \u23181 and = I, the bias of matrix the covariance estimator can of the stationary distribution",
        "By the weak convergence of the discrete algorithm trajectory to the ODE and stochastic differential equation, we show that downsampled stochastic gradient descent achieves an nearly optimal asymptotic sample complexity (Corollary 3.10)",
        "We present the result of projecting data points onto the eigenspace of sample covariance matrix indicated by Batch in Figure 2",
        "Our analysis is inspired by diffusion approximations in existing applied probability literature (<a class=\"ref-link\" id=\"cGlynn_1990_a\" href=\"#rGlynn_1990_a\">Glynn, 1990</a>; Freidlin and Wentzell, 1998; <a class=\"ref-link\" id=\"cKushner_2003_a\" href=\"#rKushner_2003_a\">Kushner and Yin, 2003</a>; <a class=\"ref-link\" id=\"cEthier_2009_a\" href=\"#rEthier_2009_a\">Ethier and Kurtz, 2009</a>), which target to capture the uncertainty of stochastic algorithms for general optimization problems"
    ],
    "summary": [
        "This paper proposes to attack stochastic nonconvex optimization problems for dependent data by investigating a simple but fundamental problem in machine learning \u2014 Streaming PCA for stationary time series.",
        "For time series, it is difficult to get unbiased estimators of the covariance matrix of the stationary distribution because of the data dependency.",
        "We show that the downsampled data point yields a sequence of stochastic approximations of the covariance matrix of the stationary distribution with controllable small bias.",
        "Investigating the analytical solution of the SDE allows us to characterize the algorithmic behavior of SGD in three different scenarios: escaping from saddle points, traversing between stationary points, and converging to global optima.",
        "One key challenge of solving the streaming PCA problem for time series is that it is difficult to get unbiased estimators of the covariance matrix \u2303 of the stationary distribution.",
        "By the weak convergence of the discrete algorithm trajectory to the ODE and SDE, we show that downsampled SGD achieves an nearly optimal asymptotic sample complexity (Corollary 3.10).",
        "Data points {zk}k 1 are generated from a geometrically ergodic time series with parameter \u21e2, and the stationary distribution has mean zero.",
        "We give the following proposition characterizing the time for the algorithm to escape from a saddle point.",
        "Stage 3: Converge to Global Optima Similar to stage 1, we focus on \u21e3ij,\u2318(t) to characterize the dynamics of the algorithm around the global optima using an SDE approximation.",
        "Combining all the results in three stages, we know that after T1 + T2 + T3 time, the algorithm converges to an \u270f-optimal solution asymptotically.",
        "We initialize the solution at the saddle point whose column span is the subspace spanned by the eigenvectors corresponding to 3.0175, 3.0170 and 1.0077.",
        "We present the result of projecting data points onto the eigenspace of sample covariance matrix indicated by Batch in Figure 2.",
        "We remark that our analysis characterizes how our proposed algorithm escapes from the saddle point.",
        "Our analysis is inspired by diffusion approximations in existing applied probability literature (<a class=\"ref-link\" id=\"cGlynn_1990_a\" href=\"#rGlynn_1990_a\">Glynn, 1990</a>; Freidlin and Wentzell, 1998; <a class=\"ref-link\" id=\"cKushner_2003_a\" href=\"#rKushner_2003_a\">Kushner and Yin, 2003</a>; <a class=\"ref-link\" id=\"cEthier_2009_a\" href=\"#rEthier_2009_a\">Ethier and Kurtz, 2009</a>), which target to capture the uncertainty of stochastic algorithms for general optimization problems.",
        "This eventually allows us to precisely characterize the algorithmic dynamics and provide concrete convergence guarantees, which further lead to a deeper understanding of the uncertainty in nonconvex stochastic optimization.",
        "The block size h of downsampled Oja\u2019s algorithm is based on the mixing property of the time series."
    ],
    "headline": "This paper studies this fundamental challenge through a streaming principal component analysis problem for stationary time series data",
    "reference_links": [
        {
            "id": "Allen-Zhu_2017_a",
            "entry": "ALLEN-ZHU, Z. (2017). Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arXiv:1708.08694 .",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "Allen-Zhu_2016_a",
            "entry": "ALLEN-ZHU, Z. and LI, Y. (2016). First efficient convergence for streaming k-pca: a global, gap-free, and near-optimal rate. arXiv preprint arXiv:1607.07837 .",
            "arxiv_url": "https://arxiv.org/pdf/1607.07837"
        },
        {
            "id": "Bottou_1998_a",
            "entry": "BOTTOU, L. (1998). Online learning and stochastic approximations. On-line learning in neural networks 17 142.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BOTTOU%2C%20L.%20Online%20learning%20and%20stochastic%20approximations.%20On-line%20learning%20in%20neural%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BOTTOU%2C%20L.%20Online%20learning%20and%20stochastic%20approximations.%20On-line%20learning%20in%20neural%201998"
        },
        {
            "id": "Bradley_2005_a",
            "entry": "BRADLEY, R. C. ET AL. (2005). Basic properties of strong mixing conditions. a survey and some open questions. Probability surveys 2 107\u2013144.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BRADLEY%2C%20R.C.E.T.A.L.%20Basic%20properties%20of%20strong%20mixing%20conditions.%20a%20survey%20and%20some%20open%20questions.%20Probability%20surveys%202%20107%E2%80%93144%202005"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "CHEN, Z., YANG, L. F., LI, C. J. and ZHAO, T. (2017). Online partial least square optimization: Dropping convexity for better efficiency and scalability. In International Conference on Machine Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CHEN%2C%20Z.%20YANG%2C%20L.F.%20LI%2C%20C.J.%20ZHAO%2C%20T.%20Online%20partial%20least%20square%20optimization%3A%20Dropping%20convexity%20for%20better%20efficiency%20and%20scalability%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CHEN%2C%20Z.%20YANG%2C%20L.F.%20LI%2C%20C.J.%20ZHAO%2C%20T.%20Online%20partial%20least%20square%20optimization%3A%20Dropping%20convexity%20for%20better%20efficiency%20and%20scalability%202017"
        },
        {
            "id": "Chung_2004_a",
            "entry": "CHUNG, K. L. (2004). On a stochastic approximation method. In Chance And Choice: Memorabilia. World Scientific, 79\u201399.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CHUNG%2C%20K.L.%20On%20a%20stochastic%20approximation%20method.%20In%20Chance%20And%20Choice%3A%20Memorabilia%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=CHUNG%2C%20K.L.%20On%20a%20stochastic%20approximation%20method.%20In%20Chance%20And%20Choice%3A%20Memorabilia%202004"
        },
        {
            "id": "Dang_2015_a",
            "entry": "DANG, C. D. and LAN, G. (2015). Stochastic block mirror descent methods for nonsmooth and stochastic optimization. SIAM Journal on Optimization 25 856\u2013881.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DANG%2C%20C.D.%20LAN%2C%20G.%20Stochastic%20block%20mirror%20descent%20methods%20for%20nonsmooth%20and%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DANG%2C%20C.D.%20LAN%2C%20G.%20Stochastic%20block%20mirror%20descent%20methods%20for%20nonsmooth%20and%20stochastic%20optimization%202015"
        },
        {
            "id": "De_et+al_2008_a",
            "entry": "DE VITO, S., MASSERA, E., PIGA, M., MARTINOTTO, L. and DI FRANCIA, G. (2008). On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario. Sensors and Actuators B: Chemical 129 750\u2013757.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DE%2C%20V.I.T.O.%20S.%2C%20M.A.S.S.E.R.A.%20E.%2C%20P.I.G.A.%20M.%2C%20M.A.R.T.I.N.O.T.T.O.%20On%20field%20calibration%20of%20an%20electronic%20nose%20for%20benzene%20estimation%20in%20an%20urban%20pollution%20monitoring%20scenario%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DE%2C%20V.I.T.O.%20S.%2C%20M.A.S.S.E.R.A.%20E.%2C%20P.I.G.A.%20M.%2C%20M.A.R.T.I.N.O.T.T.O.%20On%20field%20calibration%20of%20an%20electronic%20nose%20for%20benzene%20estimation%20in%20an%20urban%20pollution%20monitoring%20scenario%202008"
        },
        {
            "id": "Duchi_et+al_2012_a",
            "entry": "DUCHI, J. C., AGARWAL, A., JOHANSSON, M. and JORDAN, M. I. (2012a). Ergodic mirror descent. SIAM Journal on Optimization 22 1549\u20131578.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DUCHI%2C%20J.C.%20AGARWAL%2C%20A.%20JOHANSSON%2C%20M.%20JORDAN%2C%20M.I.%20Ergodic%20mirror%20descent%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DUCHI%2C%20J.C.%20AGARWAL%2C%20A.%20JOHANSSON%2C%20M.%20JORDAN%2C%20M.I.%20Ergodic%20mirror%20descent%202012"
        },
        {
            "id": "Duchi_et+al_2012_b",
            "entry": "DUCHI, J. C., BARTLETT, P. L. and WAINWRIGHT, M. J. (2012b). Randomized smoothing for stochastic optimization. SIAM Journal on Optimization 22 674\u2013701.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DUCHI%2C%20J.C.%20BARTLETT%2C%20P.L.%20WAINWRIGHT%2C%20M.J.%20Randomized%20smoothing%20for%20stochastic%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DUCHI%2C%20J.C.%20BARTLETT%2C%20P.L.%20WAINWRIGHT%2C%20M.J.%20Randomized%20smoothing%20for%20stochastic%20optimization%202012"
        },
        {
            "id": "Durrett_2010_a",
            "entry": "DURRETT, R. (2010). Probability: theory and examples. Cambridge university press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DURRETT%2C%20R.%20Probability%3A%20theory%20and%20examples%202010"
        },
        {
            "id": "Ethier_2009_a",
            "entry": "ETHIER, S. N. and KURTZ, T. G. (2009). Markov processes: characterization and convergence, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ETHIER%2C%20S.N.%20KURTZ%2C%20T.G.%20Markov%20processes%3A%20characterization%20and%20convergence%202009"
        },
        {
            "id": "282",
            "entry": "282. John Wiley &amp; Sons. FREIDLIN, M. I. and WENTZELL, A. D. (1998). Random perturbations. In Random Perturbations of Dynamical Systems. Springer, 15\u201343.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiley%2C%20John%20amp%2C%20Sons%20F.R.E.I.D.L.I.N.%20I.%2C%20M.%20WENTZELL%2C%20A.D.%20Random%20perturbations.%20In%20Random%20Perturbations%20of%20Dynamical%20Systems%201998"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "GE, R., HUANG, F., JIN, C. and YUAN, Y. (2015). Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Conference on Learning Theory.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GE%2C%20R.%20HUANG%2C%20F.%20JIN%2C%20C.%20YUAN%2C%20Y.%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition.%20In%20Conference%20on%20Learning%20Theory%202015"
        },
        {
            "id": "Ge_et+al_2016_a",
            "entry": "GE, R., LEE, J. D. and MA, T. (2016). Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GE%2C%20R.%20LEE%2C%20J.D.%20MA%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GE%2C%20R.%20LEE%2C%20J.D.%20MA%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016"
        },
        {
            "id": "Glynn_1990_a",
            "entry": "GLYNN, P. W. (1990). Diffusion approximations. Handbooks in Operations research and management Science 2 145\u2013198.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GLYNN%2C%20P.W.%20Diffusion%20approximations%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GLYNN%2C%20P.W.%20Diffusion%20approximations%201990"
        },
        {
            "id": "Griffiths_2010_a",
            "entry": "GRIFFITHS, D. F. and HIGHAM, D. J. (2010). Numerical methods for ordinary differential equations: initial value problems. Springer Science & Business Media.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GRIFFITHS%2C%20D.F.%20HIGHAM%2C%20D.J.%20Numerical%20methods%20for%20ordinary%20differential%20equations%3A%20initial%20value%20problems%202010"
        },
        {
            "id": "Hall_et+al_2016_a",
            "entry": "HALL, E. C., RASKUTTI, G. and WILLETT, R. (2016). Inference of high-dimensional autoregressive generalized linear models. arXiv preprint arXiv:1605.02693 .",
            "arxiv_url": "https://arxiv.org/pdf/1605.02693"
        },
        {
            "id": "Mello_2008_a",
            "entry": "HOMEM-DE MELLO, T. (2008). On rates of convergence for stochastic optimization problems under non\u2013independent and identically distributed sampling. SIAM Journal on Optimization 19 524\u2013551.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MELLO%2C%20H.O.M.E.M.-D.E.%20T.%20On%20rates%20of%20convergence%20for%20stochastic%20optimization%20problems%20under%20non%E2%80%93independent%20and%20identically%20distributed%20sampling%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MELLO%2C%20H.O.M.E.M.-D.E.%20T.%20On%20rates%20of%20convergence%20for%20stochastic%20optimization%20problems%20under%20non%E2%80%93independent%20and%20identically%20distributed%20sampling%202008"
        },
        {
            "id": "Hsu_et+al_2015_a",
            "entry": "HSU, D. J., KONTOROVICH, A. and SZEPESV\u00c1RI, C. (2015). Mixing time estimation in reversible markov chains from a single sample path. In Advances in neural information processing systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HSU%2C%20D.J.%20KONTOROVICH%2C%20A.%20SZEPESV%C3%81RI%2C%20C.%20Mixing%20time%20estimation%20in%20reversible%20markov%20chains%20from%20a%20single%20sample%20path.%20In%20Advances%20in%20neural%20information%20processing%20systems%202015"
        },
        {
            "id": "Jain_et+al_2016_a",
            "entry": "JAIN, P., JIN, C., KAKADE, S. M., NETRAPALLI, P. and SIDFORD, A. (2016). Streaming pca: Matching matrix bernstein and near-optimal finite sample guarantees for oja\u2019s algorithm. In Conference on Learning Theory.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=JAIN%2C%20P.%20JIN%2C%20C.%20KAKADE%2C%20S.M.%20NETRAPALLI%2C%20P.%20Streaming%20pca%3A%20Matching%20matrix%20bernstein%20and%20near-optimal%20finite%20sample%20guarantees%20for%20oja%E2%80%99s%20algorithm.%20In%20Conference%20on%20Learning%20Theory%202016"
        },
        {
            "id": "Kushner_2003_a",
            "entry": "KUSHNER, H. and YIN, G. G. (2003). Stochastic approximation and recursive algorithms and applications, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KUSHNER%2C%20H.%20YIN%2C%20G.G.%20Stochastic%20approximation%20and%20recursive%20algorithms%20and%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=KUSHNER%2C%20H.%20YIN%2C%20G.G.%20Stochastic%20approximation%20and%20recursive%20algorithms%20and%202003"
        },
        {
            "id": "35",
            "entry": "35. Springer Science &amp; Business Media. LEE, J. D., PANAGEAS, I., PILIOURAS, G., SIMCHOWITZ, M., JORDAN, M. I. and RECHT, B. (2017). First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406 .",
            "arxiv_url": "https://arxiv.org/pdf/1710.07406"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "LI, X., WANG, Z., LU, J., ARORA, R., HAUPT, J., LIU, H. and ZHAO, T. (2016). Symmetry, saddle points, and global geometry of nonconvex matrix factorization. arXiv preprint arXiv:1612.09296 .",
            "arxiv_url": "https://arxiv.org/pdf/1612.09296"
        },
        {
            "id": "Nemirovski_et+al_2009_a",
            "entry": "NEMIROVSKI, A., JUDITSKY, A., LAN, G. and SHAPIRO, A. (2009). Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization 19 1574\u20131609.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=NEMIROVSKI%2C%20A.%20JUDITSKY%2C%20A.%20LAN%2C%20G.%20SHAPIRO%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=NEMIROVSKI%2C%20A.%20JUDITSKY%2C%20A.%20LAN%2C%20G.%20SHAPIRO%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "Reddi_et+al_2016_a",
            "entry": "REDDI, S. J., SRA, S., POCZOS, B. and SMOLA, A. J. (2016). Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=REDDI%2C%20S.J.%20SRA%2C%20S.%20POCZOS%2C%20B.%20SMOLA%2C%20A.J.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=REDDI%2C%20S.J.%20SRA%2C%20S.%20POCZOS%2C%20B.%20SMOLA%2C%20A.J.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016"
        },
        {
            "id": "Robbins_1951_a",
            "entry": "ROBBINS, H. and MONRO, S. (1951). A stochastic approximation method. The annals of mathematical statistics 400\u2013407.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ROBBINS%2C%20H.%20MONRO%2C%20S.%20A%20stochastic%20approximation%20method.%20The%20annals%20of%20mathematical%20statistics%20400%E2%80%93407%201951"
        },
        {
            "id": "Sacks_1958_a",
            "entry": "SACKS, J. (1958). Asymptotic distribution of stochastic approximation procedures. The Annals of Mathematical Statistics 29 373\u2013405.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SACKS%2C%20J.%20Asymptotic%20distribution%20of%20stochastic%20approximation%20procedures%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SACKS%2C%20J.%20Asymptotic%20distribution%20of%20stochastic%20approximation%20procedures%201958"
        },
        {
            "id": "Shalev-Shwartz_et+al_2011_a",
            "entry": "SHALEV-SHWARTZ, S., SINGER, Y., SREBRO, N. and COTTER, A. (2011). Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming 127 3\u201330.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SHALEV-SHWARTZ%2C%20S.%20SINGER%2C%20Y.%20SREBRO%2C%20N.%20COTTER%2C%20A.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20svm%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SHALEV-SHWARTZ%2C%20S.%20SINGER%2C%20Y.%20SREBRO%2C%20N.%20COTTER%2C%20A.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20svm%202011"
        },
        {
            "id": "Shamir_2013_a",
            "entry": "SHAMIR, O. and ZHANG, T. (2013). Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International Conference on Machine Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SHAMIR%2C%20O.%20ZHANG%2C%20T.%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%3A%20Convergence%20results%20and%20optimal%20averaging%20schemes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SHAMIR%2C%20O.%20ZHANG%2C%20T.%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%3A%20Convergence%20results%20and%20optimal%20averaging%20schemes%202013"
        },
        {
            "id": "Srebro_2004_a",
            "entry": "SREBRO, N. and JAAKKOLA, T. S. (2004). Linear dependent dimensionality reduction. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SREBRO%2C%20N.%20JAAKKOLA%2C%20T.S.%20Linear%20dependent%20dimensionality%20reduction%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SREBRO%2C%20N.%20JAAKKOLA%2C%20T.S.%20Linear%20dependent%20dimensionality%20reduction%202004"
        },
        {
            "id": "Sun_et+al_2015_a",
            "entry": "SUN, J., QU, Q. and WRIGHT, J. (2015). Complete dictionary recovery over the sphere. In Sampling Theory and Applications (SampTA), 2015 International Conference on. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SUN%2C%20J.%20QU%2C%20Q.%20WRIGHT%2C%20J.%20Complete%20dictionary%20recovery%20over%20the%20sphere%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SUN%2C%20J.%20QU%2C%20Q.%20WRIGHT%2C%20J.%20Complete%20dictionary%20recovery%20over%20the%20sphere%202015"
        },
        {
            "id": "Sun_et+al_2016_a",
            "entry": "SUN, J., QU, Q. and WRIGHT, J. (2016). A geometric analysis of phase retrieval. In Information Theory (ISIT), 2016 IEEE International Symposium on. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SUN%2C%20J.%20QU%2C%20Q.%20WRIGHT%2C%20J.%20A%20geometric%20analysis%20of%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=SUN%2C%20J.%20QU%2C%20Q.%20WRIGHT%2C%20J.%20A%20geometric%20analysis%20of%20phase%20retrieval%202016"
        },
        {
            "id": "Tj_1990_a",
            "entry": "TJ\u00d8STHEIM, D. (1990). Non-linear time series and markov chains. Advances in Applied Probability 22 587\u2013611.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TJ%C3%98STHEIM%2C%20D.%20Non-linear%20time%20series%20and%20markov%20chains%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TJ%C3%98STHEIM%2C%20D.%20Non-linear%20time%20series%20and%20markov%20chains%201990"
        }
    ]
}
