{
    "filename": "7554-stochastic-cubic-regularization-for-fast-nonconvex-optimization.pdf",
    "metadata": {
        "title": "Stochastic Cubic Regularization for Fast Nonconvex Optimization",
        "date": 2018,
        "author": "Nilesh Tripuraneni Mitchell Stern Chi Jin Jeffrey Regier Michael I. Jordan",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7554-stochastic-cubic-regularization-for-fast-nonconvex-optimization.pdf"
        },
        "abstract": "This paper proposes a stochastic variant of a classic algorithm\u2014the cubicregularized Newton method [<a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak, 2006</a>]. The proposed algorithm efficiently escapes saddle points and finds approximate local minima for general smooth, nonconvex functions in only O(\u270f 3.5) stochastic gradient and stochastic Hessian-vector product evaluations. The latter can be computed as efficiently as stochastic gradients. This improves upon the O(\u270f 4) rate of stochastic gradient descent. Our rate matches the best-known result for finding local minima without requiring any delicate acceleration or variance-reduction techniques."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "stochastic approximation",
            "url": "https://en.wikipedia.org/wiki/stochastic_approximation"
        },
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "newton method",
            "url": "https://en.wikipedia.org/wiki/newton_method"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "The work of <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak [2006</a>] first proposed the cubic-regularized Newton method, which requires O(\u270f 1.5) gradient and Hessian oracle calls to the entire f , to find an \u270fsecond-order stationary point",
        "One of the most prominent applications of stochastic optimization has been in large-scale statistics and machine learning problems, such as the optimization of deep neural networks",
        "The work of <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak [2006</a>] first proposed the cubic-regularized Newton method, which requires O(\u270f 1.5) gradient and Hessian oracle calls to the entire f , to find an \u270fsecond-order stationary point",
        "Adapting their result for our setting, we obtain the following lemma, which states that the gradient descent subsolver satisfies our Condition 1",
        "We presented a stochastic algorithm based on the classic cubic-regularized Newton method for nonconvex optimization",
        "Our experiments show the favorable performance of our method relative to stochastic gradient descent on both a synthetic and a real-world problem"
    ],
    "key_statements": [
        "The work of <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak [2006</a>] first proposed the cubic-regularized Newton method, which requires O(\u270f 1.5) gradient and Hessian oracle calls to the entire f , to find an \u270fsecond-order stationary point",
        "One of the most prominent applications of stochastic optimization has been in large-scale statistics and machine learning problems, such as the optimization of deep neural networks",
        "The work of <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak [2006</a>] first proposed the cubic-regularized Newton method, which requires O(\u270f 1.5) gradient and Hessian oracle calls to the entire f , to find an \u270fsecond-order stationary point",
        "Adapting their result for our setting, we obtain the following lemma, which states that the gradient descent subsolver satisfies our Condition 1",
        "We presented a stochastic algorithm based on the classic cubic-regularized Newton method for nonconvex optimization",
        "Our experiments show the favorable performance of our method relative to stochastic gradient descent on both a synthetic and a real-world problem"
    ],
    "summary": [
        "The work of <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak [2006</a>] first proposed the cubic-regularized Newton method, which requires O(\u270f 1.5) gradient and Hessian oracle calls to the entire f , to find an \u270fsecond-order stationary point.",
        "Instead of using the full Hessian, Carmon and Duchi [2016] showed that using a gradient descent solver for the cubic regularization subproblem allows their algorithm to find \u270f-second-order stationary points in O(\u270f 2) Hessian-vector product evaluations.",
        "<a class=\"ref-link\" id=\"cGe_et+al_2015_a\" href=\"#rGe_et+al_2015_a\">Ge et al [2015</a>] showed that the total gradient iteration complexity for SGD to find an \u270f-second-order stationary point was O(\u270f 4poly(d)).",
        "<a class=\"ref-link\" id=\"cKohler_2017_a\" href=\"#rKohler_2017_a\">Kohler and Lucchi [2017</a>] consider a subsampled version of the cubic regularization algorithm, but do not provide a non-asymptotic analysis for their algorithm to find an approximate local minimum; they assume access to exact function values at each iteration which are not available in the fully stochastic setting.",
        "Allen-Zhu [2017] proposed an algorithm with a mechanism exploiting variance reduction that finds a second-order stationary point with O(\u270f 3.5)2 Hessian-vector product evaluations.",
        "Cubic regularization [<a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak, 2006</a>] is a classic algorithm for finding a second-order stationary point of a \u21e2-Hessian-Lipschitz function f (x).",
        "After sampling minibatches for the gradient and the Hessian, Algorithm 1 makes a call to a black-box cubic subsolver to optimize the stochastic submodel mt(x).",
        "Cubic-Subsolver may only produce an inexact solution satisfying Condition 1, which is not sufficient for xt + to be an \u270f-second-order stationary point.",
        "The two main differences relative to standard gradient descent are: (1) lines 1\u20133: when g is large, the submodel m ( ) may be ill-conditioned, so instead of doing gradient descent, the iterate only moves one step in the g direction, which already guarantees sufficient descent; (2) line 6: the algorithm adds a small perturbation to g to avoid a certain \u201chard\" case for the cubic submodel.",
        "Adapting their result for our setting, we obtain the following lemma, which states that the gradient descent subsolver satisfies our Condition 1.",
        "We describe our high-level approach, and provide a proof sketch in Appendix A in the stochastic setting, assuming oracle access to an exact subsolver.",
        "We presented a stochastic algorithm based on the classic cubic-regularized Newton method for nonconvex optimization.",
        "Our algorithm provably finds \u270f-approximate local minima in O(\u270f 3.5) total gradient and Hessian-vector product evaluations, improving upon the O(\u270f 4poly(d)) rate of SGD.",
        "Our experiments show the favorable performance of our method relative to SGD on both a synthetic and a real-world problem"
    ],
    "headline": "This paper proposes a stochastic variant of a classic algorithm\u2014the cubicregularized Newton method ",
    "reference_links": [
        {
            "id": "Abadi_2016_a",
            "entry": "Martin Abadi, Paul Barham, et al. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Martin%20Barham%2C%20Paul%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Martin%20Barham%2C%20Paul%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Agarwal_et+al_2017_a",
            "entry": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Naman%20Allen-Zhu%2C%20Zeyuan%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Naman%20Allen-Zhu%2C%20Zeyuan%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017"
        },
        {
            "id": "Allen-Zhu_2017_a",
            "entry": "Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD. arXiv preprint arXiv:1708.08694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "Allen-Zhu_2017_b",
            "entry": "Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv preprint arXiv:1711.06673, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06673"
        },
        {
            "id": "Carmon_2016_a",
            "entry": "Yair Carmon and John Duchi. Gradient descent efficiently finds the cubic-regularized non-convex Newton step. arXiv preprint arXiv:1612.00547, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00547"
        },
        {
            "id": "Carmon_et+al_2016_b",
            "entry": "Yair Carmon, John Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization. arXiv preprint arXiv:1611.00756, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "Cartis_et+al_2011_a",
            "entry": "Coralia Cartis, Nicholas Gould, and Philippe Toint. Adaptive cubic regularisation methods for unconstrained optimization. Part II: worst-case function-and derivative-evaluation complexity. Mathematical Programming, 130(2):295\u2013319, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20Toint%2C%20Philippe%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20Part%20II%3A%20worst-case%20function-and%20derivative-evaluation%20complexity%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20Toint%2C%20Philippe%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20Part%20II%3A%20worst-case%20function-and%20derivative-evaluation%20complexity%202011"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Curtis_et+al_2017_a",
            "entry": "Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a worst-case iteration complexity of O(\u270f 3/2) for nonconvex optimization. Mathematical Programming, 162(1-2):1\u201332, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Curtis%2C%20Frank%20E.%20Robinson%2C%20Daniel%20P.%20Samadi%2C%20Mohammadreza%20A%20trust%20region%20algorithm%20with%20a%20worst-case%20iteration%20complexity%20of%20O%28%E2%9C%8F%203/2%29%20for%20nonconvex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Curtis%2C%20Frank%20E.%20Robinson%2C%20Daniel%20P.%20Samadi%2C%20Mohammadreza%20A%20trust%20region%20algorithm%20with%20a%20worst-case%20iteration%20complexity%20of%20O%28%E2%9C%8F%203/2%29%20for%20nonconvex%20optimization%202017"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "Ge_et+al_2017_a",
            "entry": "Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Jin%2C%20Chi%20Zheng%2C%20Yi%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Jin%2C%20Chi%20Zheng%2C%20Yi%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017"
        },
        {
            "id": "Jain_et+al_2016_a",
            "entry": "Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming PCA: Matching matrix Bernstein and near-optimal finite sample guarantees for Oja\u2019s algorithm. In Conference on Learning Theory, pages 1147\u20131164, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Prateek%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Netrapalli%2C%20Praneeth%20Streaming%20PCA%3A%20Matching%20matrix%20Bernstein%20and%20near-optimal%20finite%20sample%20guarantees%20for%20Oja%E2%80%99s%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Prateek%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Netrapalli%2C%20Praneeth%20Streaming%20PCA%3A%20Matching%20matrix%20Bernstein%20and%20near-optimal%20finite%20sample%20guarantees%20for%20Oja%E2%80%99s%20algorithm%202016"
        },
        {
            "id": "Jin_et+al_2017_a",
            "entry": "Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017"
        },
        {
            "id": "Johnson_2013_a",
            "entry": "Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "Kohler_2017_a",
            "entry": "Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex optimization. In Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohler%2C%20Jonas%20Moritz%20Lucchi%2C%20Aurelien%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohler%2C%20Jonas%20Moritz%20Lucchi%2C%20Aurelien%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017"
        },
        {
            "id": "Lecun_2010_a",
            "entry": "Yann LeCun and Corinna Cortes. MNIST Handwritten Digit Database, 2010. URL http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yann%20LeCun%20and%20Corinna%20Cortes%20MNIST%20Handwritten%20Digit%20Database%202010%20URL%20httpyannlecuncomexdbmnist"
        },
        {
            "id": "Nesterov_2013_a",
            "entry": "Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20on%20Convex%20Optimization%3A%20A%20Basic%20Course%2C%20volume%2087%202013"
        },
        {
            "id": "Nesterov_2006_a",
            "entry": "Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20Newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20Newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "Pearlmutter_1994_a",
            "entry": "Barak A Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation, 6(1):147\u2013160, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearlmutter%2C%20Barak%20A.%20Fast%20exact%20multiplication%20by%20the%20Hessian%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pearlmutter%2C%20Barak%20A.%20Fast%20exact%20multiplication%20by%20the%20Hessian%201994"
        },
        {
            "id": "Reddi_2017_a",
            "entry": "Sashank J Reddi, Manzil Zaheer, et al. A generic approach for escaping saddle points. arXiv preprint arXiv:1709.01434, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01434"
        },
        {
            "id": "Robbins_1951_a",
            "entry": "Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "Royer_2017_a",
            "entry": "Cl\u00e9ment W Royer and Stephen J Wright. Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03131"
        },
        {
            "id": "Sun_et+al_2016_a",
            "entry": "Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the geometric picture. IEEE Transactions on Information Theory, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20Complete%20dictionary%20recovery%20over%20the%20sphere%20I%3A%20Overview%20and%20the%20geometric%20picture%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20Complete%20dictionary%20recovery%20over%20the%20sphere%20I%3A%20Overview%20and%20the%20geometric%20picture%202016"
        },
        {
            "id": "Sun_et+al_2016_b",
            "entry": "Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In IEEE International Symposium on Information Theory. IEEE, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20A%20geometric%20analysis%20of%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20A%20geometric%20analysis%20of%20phase%20retrieval%202016"
        },
        {
            "id": "Tropp_2015_a",
            "entry": "Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends R in Machine Learning, 8(1-2):1\u2013230, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20An%20introduction%20to%20matrix%20concentration%20inequalities%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20An%20introduction%20to%20matrix%20concentration%20inequalities%202015"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Newton-type methods for non-convex optimization under inexact Hessian information. arXiv preprint arXiv:1708.07164, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07164"
        },
        {
            "id": "Xu_2017_b",
            "entry": "Yi Xu and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. arXiv preprint arXiv:1711.01944, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01944"
        }
    ]
}
