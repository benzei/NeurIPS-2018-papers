{
    "filename": "7891-on-coresets-for-logistic-regression.pdf",
    "metadata": {
        "title": "On Coresets for Logistic Regression",
        "author": "Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, David Woodruff",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7891-on-coresets-for-logistic-regression.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Coresets are one of the central methods to facilitate the analysis of large data. We continue a recent line of research applying the theory of coresets to logistic regression. First, we show the negative result that no strongly sublinear sized coresets exist for logistic regression. To deal with intractable worst-case instances we introduce a complexity measure \u03bc(X), which quantifies the hardness of compressing a data set for logistic regression. \u03bc(X) has an intuitive statistical interpretation that may be of independent interest. For data sets with bounded \u03bc(X)-complexity, we show that a novel sensitivity sampling scheme produces the first provably sublinear (1 \u00b1 \u03b5)-coreset. We illustrate the performance of our method by comparing to uniform sampling as well as to state of the art methods in the area. The experiments are conducted on real world benchmark data for logistic regression."
    },
    "keywords": [
        {
            "term": "nystr\u00f6m method",
            "url": "https://en.wikipedia.org/wiki/nystr\u00f6m_method"
        },
        {
            "term": "logistic regression",
            "url": "https://en.wikipedia.org/wiki/logistic_regression"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "low rank approximation",
            "url": "https://en.wikipedia.org/wiki/low_rank_approximation"
        },
        {
            "term": "data set",
            "url": "https://en.wikipedia.org/wiki/data_set"
        }
    ],
    "highlights": [
        "Scalability is one of the central challenges of modern data analysis and machine learning",
        "A natural approach is to sub-sample the data according to a certain probability distribution. This approach has been successfully applied to a variety of problems including clustering [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], mixture models [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], low rank approximation [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], spectral approximation [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], and Nystr\u00f6m methods [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>]",
        "Our experiments show that this distribution outperforms uniform and k-means based sensitivity sampling by a wide margin on real data sets",
        "We show that the total sensitivity of logistic regression can be bounded in terms of \u03bc, and that our sampling scheme produces the first coreset of provably sublinear size, provided that \u03bc is small",
        "Given a data set X \u2208 Rn\u00d7d weighted by w \u2208 Rn>0 and a vector \u03b2 \u2208 Rd let (DwX\u03b2)\u2212 denote the vector comprising only the negative entries of DwX\u03b2. Similarly let",
        "Our lower bounds and theirs are incomparable; they show that if a coreset can only consist of input points it comprises the entire data set in the worst-case"
    ],
    "key_statements": [
        "Scalability is one of the central challenges of modern data analysis and machine learning",
        "A natural approach is to sub-sample the data according to a certain probability distribution. This approach has been successfully applied to a variety of problems including clustering [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], mixture models [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], low rank approximation [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], spectral approximation [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], and Nystr\u00f6m methods [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>]",
        "Our experiments show that this distribution outperforms uniform and k-means based sensitivity sampling by a wide margin on real data sets",
        "We show that the total sensitivity of logistic regression can be bounded in terms of \u03bc, and that our sampling scheme produces the first coreset of provably sublinear size, provided that \u03bc is small",
        "Given a data set X \u2208 Rn\u00d7d weighted by w \u2208 Rn>0 and a vector \u03b2 \u2208 Rd let (DwX\u03b2)\u2212 denote the vector comprising only the negative entries of DwX\u03b2. Similarly let",
        "Our lower bounds and theirs are incomparable; they show that if a coreset can only consist of input points it comprises the entire data set in the worst-case",
        "We show that the VC dimension of the range space induced by the set of functions studied in logistic regression can be related to the VC dimension of the set of linear classifiers",
        "Our algorithms are space efficient, and can be implemented in a variety of models, used to tackle the challenges of large data sets, such as 2-pass streaming, and massively parallel frameworks like Hadoop and MapReduce, and can be implemented to run in input sparsity time O), which is especially beneficial for sparsely encoded input data"
    ],
    "summary": [
        "Scalability is one of the central challenges of modern data analysis and machine learning.",
        "Due to a standard reduction between coresets and streaming algorithms, this implies that logistic regression admits no coresets or bounded sensitivity scores in general.",
        "Our experiments show that this distribution outperforms uniform and k-means based sensitivity sampling by a wide margin on real data sets.",
        "We show that the total sensitivity of logistic regression can be bounded in terms of \u03bc, and that our sampling scheme produces the first coreset of provably sublinear size, provided that \u03bc is small.",
        "The authors recovered the result that bounded sensitivity scores for logistic regression imply coresets.",
        "\u039c-Complex Data Sets We will see in Section 3 that in general, there is no sublinear one-pass streaming algorithm approximating the objective function up to any finite constant factor.",
        "For the sake of developing coreset constructions that work reasonably well, as well as conducting a formal analysis beyond worst-case instances, we introduce a measure \u03bc that quantifies the complexity of compressing a given data set.",
        "Given a data set X \u2208 Rn\u00d7d weighted by w \u2208 Rn>0 and a vector \u03b2 \u2208 Rd let (DwX\u03b2)\u2212 denote the vector comprising only the negative entries of DwX\u03b2.",
        "The size of our (1 \u00b1 \u03b5)-coreset constructions for logistic regression for a given \u03bc-complex data set X will have low polynomial dependency on \u03bc, d, 1/\u03b5 but only sublinear dependency on its original size parameter n.",
        "In the following we will show a much stronger result, namely that no efficient streaming algorithms or coresets for logistic regression can exist in general, even if we assume that the points lie in 2-dimensional Euclidean space.",
        "Our lower bounds and theirs are incomparable; they show that if a coreset can only consist of input points it comprises the entire data set in the worst-case.",
        "The sensitivity of a point measures its worst-case importance for approximating the objective function on the entire input data set.",
        "Our algorithms are space efficient, and can be implemented in a variety of models, used to tackle the challenges of large data sets, such as 2-pass streaming, and massively parallel frameworks like Hadoop and MapReduce, and can be implemented to run in input sparsity time O), which is especially beneficial for sparsely encoded input data.",
        "Our experimental evaluation shows that our implementation of the basic algorithm outperforms uniform sampling as well as state of the art methods in the area of coresets for logistic regression while being competitive to both regarding its running time.",
        "We developed the first rigorously sublinear (1 \u00b1 \u03b5)-coresets"
    ],
    "headline": "We show the negative result that no strongly sublinear sized coresets exist for logistic regression",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. K. Agarwal, S. Har-Peled, and K. R. Varadarajan. Approximating extent measures of points. Journal of the ACM, 51(4):606\u2013635, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20P.K.%20Har-Peled%2C%20S.%20Varadarajan%2C%20K.R.%20Approximating%20extent%20measures%20of%20points%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20P.K.%20Har-Peled%2C%20S.%20Varadarajan%2C%20K.R.%20Approximating%20extent%20measures%20of%20points%202004"
        },
        {
            "id": "2",
            "entry": "[2] A. E. Alaoui and M. W. Mahoney. Fast randomized kernel ridge regression with statistical guarantees. In Advances in Neural Information Processing Systems 28 (NIPS), pages 775\u2013783, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alaoui%2C%20A.E.%20Mahoney%2C%20M.W.%20Fast%20randomized%20kernel%20ridge%20regression%20with%20statistical%20guarantees%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alaoui%2C%20A.E.%20Mahoney%2C%20M.W.%20Fast%20randomized%20kernel%20ridge%20regression%20with%20statistical%20guarantees%202015"
        },
        {
            "id": "3",
            "entry": "[3] D. Arthur and S. Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1027\u20131035, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arthur%2C%20D.%20Vassilvitskii%2C%20S.%20k-means%2B%2B%3A%20the%20advantages%20of%20careful%20seeding%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arthur%2C%20D.%20Vassilvitskii%2C%20S.%20k-means%2B%2B%3A%20the%20advantages%20of%20careful%20seeding%202007"
        },
        {
            "id": "4",
            "entry": "[4] O. Bachem, M. Lucic, and A. Krause. Scalable k-means clustering via lightweight coresets. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), pages 1119\u20131127, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bachem%2C%20O.%20Lucic%2C%20M.%20Krause%2C%20A.%20Scalable%20k-means%20clustering%20via%20lightweight%20coresets%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bachem%2C%20O.%20Lucic%2C%20M.%20Krause%2C%20A.%20Scalable%20k-means%20clustering%20via%20lightweight%20coresets%202018"
        },
        {
            "id": "5",
            "entry": "[5] M.-F. Balcan, B. Manthey, H. R\u00f6glin, and T. Roughgarden. Analysis of algorithms beyond the worst case (Dagstuhl seminar 14372). Dagstuhl Reports, 4(9):30\u201349, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.-F.%20Manthey%2C%20B.%20R%C3%B6glin%2C%20H.%20Roughgarden%2C%20T.%20Analysis%20of%20algorithms%20beyond%20the%20worst%20case%20%28Dagstuhl%20seminar%2014372%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.-F.%20Manthey%2C%20B.%20R%C3%B6glin%2C%20H.%20Roughgarden%2C%20T.%20Analysis%20of%20algorithms%20beyond%20the%20worst%20case%20%28Dagstuhl%20seminar%2014372%29%202015"
        },
        {
            "id": "6",
            "entry": "[6] A. Barger and D. Feldman. k-means for streaming and distributed big sparse data. In Proceedings of the SIAM International Conference on Data Mining (SDM), pages 342\u2013350, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barger%2C%20A.%20Feldman%2C%20D.%20k-means%20for%20streaming%20and%20distributed%20big%20sparse%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barger%2C%20A.%20Feldman%2C%20D.%20k-means%20for%20streaming%20and%20distributed%20big%20sparse%20data%202016"
        },
        {
            "id": "7",
            "entry": "[7] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929\u2013965, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumer%2C%20A.%20Ehrenfeucht%2C%20A.%20Haussler%2C%20D.%20Warmuth%2C%20M.K.%20Learnability%20and%20the%20Vapnik-Chervonenkis%20dimension%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumer%2C%20A.%20Ehrenfeucht%2C%20A.%20Haussler%2C%20D.%20Warmuth%2C%20M.K.%20Learnability%20and%20the%20Vapnik-Chervonenkis%20dimension%201989"
        },
        {
            "id": "8",
            "entry": "[8] V. Braverman, D. Feldman, and H. Lang. New frameworks for offline and streaming coreset constructions. arXiv preprint CoRR, abs/1612.00889, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00889"
        },
        {
            "id": "9",
            "entry": "[9] M. T. Chao. A general purpose unequal probability sampling plan. Biometrika, 69(3):653\u2013656, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chao%2C%20M.T.%20A%20general%20purpose%20unequal%20probability%20sampling%20plan%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chao%2C%20M.T.%20A%20general%20purpose%20unequal%20probability%20sampling%20plan%201982"
        },
        {
            "id": "10",
            "entry": "[10] K. L. Clarkson. Subgradient and sampling algorithms for 1 regression. In Proceedings of the 16th annual ACM-SIAM symposium on Discrete algorithms (SODA), pages 257\u2013266, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.L.%20Subgradient%20and%20sampling%20algorithms%20for%201%20regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.L.%20Subgradient%20and%20sampling%20algorithms%20for%201%20regression%202005"
        },
        {
            "id": "11",
            "entry": "[11] K. L. Clarkson, P. Drineas, M. Magdon-Ismail, M. W. Mahoney, X. Meng, and D. P. Woodruff. The fast Cauchy transform and faster robust linear regression. SIAM J. Comput., 45(3):763\u2013810, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.L.%20Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.W.%20The%20fast%20Cauchy%20transform%20and%20faster%20robust%20linear%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.L.%20Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.W.%20The%20fast%20Cauchy%20transform%20and%20faster%20robust%20linear%20regression%202016"
        },
        {
            "id": "12",
            "entry": "[12] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In Symposium on Theory of Computing (STOC), pages 81\u201390, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Low%20rank%20approximation%20and%20regression%20in%20input%20sparsity%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Low%20rank%20approximation%20and%20regression%20in%20input%20sparsity%20time%202013"
        },
        {
            "id": "13",
            "entry": "[13] K. L. Clarkson and D. P. Woodruff. Input sparsity and hardness for robust subspace approximation. In IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), pages 310\u2013329, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Input%20sparsity%20and%20hardness%20for%20robust%20subspace%20approximation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Input%20sparsity%20and%20hardness%20for%20robust%20subspace%20approximation%202015"
        },
        {
            "id": "14",
            "entry": "[14] K. L. Clarkson and D. P. Woodruff. Sketching for M-estimators: A unified approach to robust regression. In Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 921\u2013939, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Sketching%20for%20M-estimators%3A%20A%20unified%20approach%20to%20robust%20regression%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Sketching%20for%20M-estimators%3A%20A%20unified%20approach%20to%20robust%20regression%202015"
        },
        {
            "id": "15",
            "entry": "[15] M. B. Cohen, Y. T. Lee, C. Musco, C. Musco, R. Peng, and A. Sidford. Uniform sampling for matrix approximation. In Proceedings of the Conference on Innovations in Theoretical Computer Science (ITCS), pages 181\u2013190, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20M.B.%20Lee%2C%20Y.T.%20Musco%2C%20C.%20Musco%2C%20C.%20Uniform%20sampling%20for%20matrix%20approximation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20M.B.%20Lee%2C%20Y.T.%20Musco%2C%20C.%20Musco%2C%20C.%20Uniform%20sampling%20for%20matrix%20approximation%202015"
        },
        {
            "id": "16",
            "entry": "[16] M. B. Cohen, C. Musco, and C. Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1758\u20131777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20M.B.%20Musco%2C%20C.%20Musco%2C%20C.%20Input%20sparsity%20time%20low-rank%20approximation%20via%20ridge%20leverage%20score%20sampling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20M.B.%20Musco%2C%20C.%20Musco%2C%20C.%20Input%20sparsity%20time%20low-rank%20approximation%20via%20ridge%20leverage%20score%20sampling%202017"
        },
        {
            "id": "17",
            "entry": "[17] A. Dasgupta, P. Drineas, B. Harb, R. Kumar, and M. W. Mahoney. Sampling algorithms and coresets for p regression. SIAM Journal on Computing, 38(5):2060\u20132078, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20A.%20Drineas%2C%20P.%20Harb%2C%20B.%20Kumar%2C%20R.%20Sampling%20algorithms%20and%20coresets%20for%20p%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20A.%20Drineas%2C%20P.%20Harb%2C%20B.%20Kumar%2C%20R.%20Sampling%20algorithms%20and%20coresets%20for%20p%20regression%202009"
        },
        {
            "id": "18",
            "entry": "[18] P. Drineas, M. Magdon-Ismail, M. W. Mahoney, and D. P. Woodruff. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13:3475\u20133506, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.W.%20Woodruff%2C%20D.P.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.W.%20Woodruff%2C%20D.P.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012"
        },
        {
            "id": "19",
            "entry": "[19] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Sampling algorithms for 2 regression and applications. In Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1127\u2013 1136, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20P.%20Mahoney%2C%20M.W.%20Muthukrishnan%2C%20S.%20Sampling%20algorithms%20for%202%20regression%20and%20applications%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20P.%20Mahoney%2C%20M.W.%20Muthukrishnan%2C%20S.%20Sampling%20algorithms%20for%202%20regression%20and%20applications%202006"
        },
        {
            "id": "20",
            "entry": "[20] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844\u2013881, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20P.%20Mahoney%2C%20M.W.%20Muthukrishnan%2C%20S.%20Relative-error%20CUR%20matrix%20decompositions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20P.%20Mahoney%2C%20M.W.%20Muthukrishnan%2C%20S.%20Relative-error%20CUR%20matrix%20decompositions%202008"
        },
        {
            "id": "21",
            "entry": "[21] D. Feldman, M. Faulkner, and A. Krause. Scalable training of mixture models via coresets. In Advances in Neural Information Processing Systems 24 (NIPS), pages 2142\u20132150, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20D.%20Faulkner%2C%20M.%20Krause%2C%20A.%20Scalable%20training%20of%20mixture%20models%20via%20coresets%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20D.%20Faulkner%2C%20M.%20Krause%2C%20A.%20Scalable%20training%20of%20mixture%20models%20via%20coresets%202011"
        },
        {
            "id": "22",
            "entry": "[22] D. Feldman and M. Langberg. A unified framework for approximating and clustering data. In Proceedings of the 43rd ACM Symposium on Theory of Computing (STOC), pages 569\u2013578, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20D.%20Langberg%2C%20M.%20A%20unified%20framework%20for%20approximating%20and%20clustering%20data%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20D.%20Langberg%2C%20M.%20A%20unified%20framework%20for%20approximating%20and%20clustering%20data%202011"
        },
        {
            "id": "23",
            "entry": "[23] D. Feldman, M. Schmidt, and C. Sohler. Turning big data into tiny data: Constant-size coresets for k-means, PCA and projective clustering. In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1434\u20131453, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20D.%20Schmidt%2C%20M.%20Sohler%2C%20C.%20Turning%20big%20data%20into%20tiny%20data%3A%20Constant-size%20coresets%20for%20k-means%2C%20PCA%20and%20projective%20clustering%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20D.%20Schmidt%2C%20M.%20Sohler%2C%20C.%20Turning%20big%20data%20into%20tiny%20data%3A%20Constant-size%20coresets%20for%20k-means%2C%20PCA%20and%20projective%20clustering%202013"
        },
        {
            "id": "24",
            "entry": "[24] L. N. Geppert, K. Ickstadt, A. Munteanu, J. Quedenfeld, and C. Sohler. Random projections for Bayesian regression. Statistics and Computing, 27(1):79\u2013101, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geppert%2C%20L.N.%20Ickstadt%2C%20K.%20Munteanu%2C%20A.%20Quedenfeld%2C%20J.%20Random%20projections%20for%20Bayesian%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geppert%2C%20L.N.%20Ickstadt%2C%20K.%20Munteanu%2C%20A.%20Quedenfeld%2C%20J.%20Random%20projections%20for%20Bayesian%20regression%202017"
        },
        {
            "id": "25",
            "entry": "[25] G. H. Golub and C. F. van Loan. Matrix computations (4. ed.). J. Hopkins Univ. Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20G.H.%20van%20Loan%2C%20C.F.%20Matrix%20computations%20%284%202013"
        },
        {
            "id": "26",
            "entry": "[26] G. Heinze and M. Schemper. A solution to the problem of separation in logistic regression. Statistics in Medicine, 21(16):2409\u20132419, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heinze%2C%20G.%20Schemper%2C%20M.%20A%20solution%20to%20the%20problem%20of%20separation%20in%20logistic%20regression%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heinze%2C%20G.%20Schemper%2C%20M.%20A%20solution%20to%20the%20problem%20of%20separation%20in%20logistic%20regression%202002"
        },
        {
            "id": "27",
            "entry": "[27] J. H. Huggins, T. Campbell, and T. Broderick. Coresets for scalable Bayesian logistic regression. In Advances in Neural Information Processing Systems 29 (NIPS), pages 4080\u20134088, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huggins%2C%20J.H.%20Campbell%2C%20T.%20Broderick%2C%20T.%20Coresets%20for%20scalable%20Bayesian%20logistic%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huggins%2C%20J.H.%20Campbell%2C%20T.%20Broderick%2C%20T.%20Coresets%20for%20scalable%20Bayesian%20logistic%20regression%202016"
        },
        {
            "id": "28",
            "entry": "[28] W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary Mathematics, 26(1):189\u2013206, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20W.B.%20Lindenstrauss%2C%20J.%20Extensions%20of%20Lipschitz%20mappings%20into%20a%20Hilbert%20space%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20W.B.%20Lindenstrauss%2C%20J.%20Extensions%20of%20Lipschitz%20mappings%20into%20a%20Hilbert%20space%201984"
        },
        {
            "id": "29",
            "entry": "[29] M. J. Kearns and U. V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20M.J.%20Vazirani%2C%20U.V.%20An%20Introduction%20to%20Computational%20Learning%20Theory%201994"
        },
        {
            "id": "30",
            "entry": "[30] I. Kremer, N. Nisan, and D. Ron. On randomized one-round communication complexity. Computational Complexity, 8(1):21\u201349, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kremer%2C%20I.%20Nisan%2C%20N.%20Ron%2C%20D.%20On%20randomized%20one-round%20communication%20complexity%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kremer%2C%20I.%20Nisan%2C%20N.%20Ron%2C%20D.%20On%20randomized%20one-round%20communication%20complexity%201999"
        },
        {
            "id": "31",
            "entry": "[31] M. Langberg and L. J. Schulman. Universal \u03b5-approximators for integrals. In Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 598\u2013607, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langberg%2C%20M.%20Schulman%2C%20L.J.%20Universal%20%CE%B5-approximators%20for%20integrals%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langberg%2C%20M.%20Schulman%2C%20L.J.%20Universal%20%CE%B5-approximators%20for%20integrals%202010"
        },
        {
            "id": "32",
            "entry": "[32] M. Li, G. L. Miller, and R. Peng. Iterative row sampling. In 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 127\u2013136, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20M.%20Miller%2C%20G.L.%20Peng%2C%20R.%20Iterative%20row%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20M.%20Miller%2C%20G.L.%20Peng%2C%20R.%20Iterative%20row%20sampling%202013"
        },
        {
            "id": "33",
            "entry": "[33] M. Lucic, O. Bachem, and A. Krause. Strong coresets for hard and soft Bregman clustering with applications to exponential family mixtures. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1\u20139, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucic%2C%20M.%20Bachem%2C%20O.%20Krause%2C%20A.%20Strong%20coresets%20for%20hard%20and%20soft%20Bregman%20clustering%20with%20applications%20to%20exponential%20family%20mixtures%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucic%2C%20M.%20Bachem%2C%20O.%20Krause%2C%20A.%20Strong%20coresets%20for%20hard%20and%20soft%20Bregman%20clustering%20with%20applications%20to%20exponential%20family%20mixtures%202016"
        },
        {
            "id": "34",
            "entry": "[34] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, London, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCullagh%2C%20P.%20Nelder%2C%20J.A.%20Generalized%20Linear%20Models%201989"
        },
        {
            "id": "35",
            "entry": "[35] C. R. Mehta and N. R. Patel. Exact logistic regression: Theory and examples. Statistics in Medicine, 14(19):2143\u20132160, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mehta%2C%20C.R.%20Patel%2C%20N.R.%20Exact%20logistic%20regression%3A%20Theory%20and%20examples%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mehta%2C%20C.R.%20Patel%2C%20N.R.%20Exact%20logistic%20regression%3A%20Theory%20and%20examples%201995"
        },
        {
            "id": "36",
            "entry": "[36] A. Molina, A. Munteanu, and K. Kersting. Core dependency networks. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molina%2C%20A.%20Munteanu%2C%20A.%20Kersting%2C%20K.%20Core%20dependency%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Molina%2C%20A.%20Munteanu%2C%20A.%20Kersting%2C%20K.%20Core%20dependency%20networks%202018"
        },
        {
            "id": "37",
            "entry": "[37] C. Musco and C. Musco. Recursive sampling for the Nystr\u00f6m method. In Advances in Neural Information Processing Systems 30 (NIPS), pages 3836\u20133848, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Musco%2C%20C.%20Musco%2C%20C.%20Recursive%20sampling%20for%20the%20Nystr%C3%B6m%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Musco%2C%20C.%20Musco%2C%20C.%20Recursive%20sampling%20for%20the%20Nystr%C3%B6m%20method%202017"
        },
        {
            "id": "38",
            "entry": "[38] S. J. Reddi, B. P\u00f3czos, and A. J. Smola. Communication efficient coresets for empirical loss minimization. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence (UAI), pages 752\u2013761, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20P%C3%B3czos%2C%20B.%20Smola%2C%20A.J.%20Communication%20efficient%20coresets%20for%20empirical%20loss%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20P%C3%B3czos%2C%20B.%20Smola%2C%20A.J.%20Communication%20efficient%20coresets%20for%20empirical%20loss%20minimization%202015"
        },
        {
            "id": "39",
            "entry": "[39] T. Roughgarden. Beyond worst-case analysis, 2017. Invited talk held at the Highlights of Algorithms conference (HALG), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roughgarden%2C%20T.%20Beyond%20worst-case%20analysis%202017"
        },
        {
            "id": "40",
            "entry": "[40] C. Sohler and D. P. Woodruff. Subspace embeddings for the L1-norm with applications. In Proceedings of the 43rd ACM Symposium on Theory of Computing (STOC), pages 755\u2013764, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sohler%2C%20C.%20Woodruff%2C%20D.P.%20Subspace%20embeddings%20for%20the%20L1-norm%20with%20applications%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sohler%2C%20C.%20Woodruff%2C%20D.P.%20Subspace%20embeddings%20for%20the%20L1-norm%20with%20applications%202011"
        },
        {
            "id": "41",
            "entry": "[41] E. Tolochinsky and D. Feldman. Coresets for monotonic functions with applications to deep learning. CoRR, abs/1802.07382, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07382"
        },
        {
            "id": "42",
            "entry": "[42] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, USA, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20V.N.%20The%20Nature%20of%20Statistical%20Learning%20Theory%201995"
        },
        {
            "id": "43",
            "entry": "[43] D. P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in Theoretical Computer Science, 10(1-2):1\u2013157, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodruff%2C%20D.P.%20Sketching%20as%20a%20tool%20for%20numerical%20linear%20algebra%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodruff%2C%20D.P.%20Sketching%20as%20a%20tool%20for%20numerical%20linear%20algebra%202014"
        },
        {
            "id": "44",
            "entry": "[44] D. P. Woodruff and Q. Zhang. Subspace embeddings and p-regression using exponential random variables. In The 26th Conference on Learning Theory (COLT), pages 546\u2013567, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodruff%2C%20D.P.%20Zhang%2C%20Q.%20Subspace%20embeddings%20and%20p-regression%20using%20exponential%20random%20variables%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodruff%2C%20D.P.%20Zhang%2C%20Q.%20Subspace%20embeddings%20and%20p-regression%20using%20exponential%20random%20variables%202013"
        }
    ]
}
