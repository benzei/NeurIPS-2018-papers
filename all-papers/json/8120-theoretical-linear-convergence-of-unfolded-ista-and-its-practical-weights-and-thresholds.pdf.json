{
    "filename": "8120-theoretical-linear-convergence-of-unfolded-ista-and-its-practical-weights-and-thresholds.pdf",
    "metadata": {
        "title": "Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds",
        "author": "Xiaohan Chen, Jialin Liu, Zhangyang Wang, Wotao Yin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8120-theoretical-linear-convergence-of-unfolded-ista-and-its-practical-weights-and-thresholds.pdf"
        },
        "abstract": "In recent years, unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available.2."
    },
    "keywords": [
        {
            "term": "ISTA",
            "url": "https://en.wikipedia.org/wiki/ISTA"
        },
        {
            "term": "linear convergence",
            "url": "https://en.wikipedia.org/wiki/linear_convergence"
        },
        {
            "term": "iterative algorithm",
            "url": "https://en.wikipedia.org/wiki/iterative_algorithm"
        },
        {
            "term": "restricted isometry property",
            "url": "https://en.wikipedia.org/wiki/restricted_isometry_property"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "compressive sensing",
            "url": "https://en.wikipedia.org/wiki/compressive_sensing"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "sparse vector",
            "url": "https://en.wikipedia.org/wiki/sparse_vector"
        }
    ],
    "highlights": [
        "E.g., [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], show that a trained K-layer Learned ISTA or its variants can generalize more than well to unseen samples (x , b ) from the same P(x, b) and recover x from b to the same accuracy within one or two order-of-magnitude fewer iterations than the original iterative shrinkage thresholding algorithm",
        "We evaluated Learned ISTA equipped with those proposed techniques in an image compressive sensing task, obtaining superior performance over several of the state-of-the-arts.\n2 Algorithm Description",
        "We introduce a special thresholding scheme to Learned ISTA, called support selection, which is inspired by \u201ckicking\u201d [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] in linearized Bregman iteration",
        "We select support w.r.t. a percentage of the largest magnitudes within the whole batch rather than within a single sample as we do in theorems and simulated experiments, which we emprically find is beneficial to the recovery performance",
        "We have introduced a partial weight coupling structure to Learned ISTA, which reduces the number of trainable parameters but does not hurt the performance",
        "We have further proposed support selection, which improves the convergence rate both theoretically and empirically"
    ],
    "key_statements": [
        "E.g., [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], show that a trained K-layer Learned ISTA or its variants can generalize more than well to unseen samples (x , b ) from the same P(x, b) and recover x from b to the same accuracy within one or two order-of-magnitude fewer iterations than the original iterative shrinkage thresholding algorithm",
        "There exists a sequence of parameters that makes Learned ISTA converge linearly since its first iteration",
        "We evaluated Learned ISTA equipped with those proposed techniques in an image compressive sensing task, obtaining superior performance over several of the state-of-the-arts.\n2 Algorithm Description",
        "We introduce a special thresholding scheme to Learned ISTA, called support selection, which is inspired by \u201ckicking\u201d [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] in linearized Bregman iteration",
        "We report the test-set NMSE of Learned ISTA-CP (10) in Fig. 3",
        "On the training set, we are inspired by [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]: first using layer-wise pre-training with a reconstruction loss w.r.t. dictionary D plus an l1 loss, shown in (19), where k is the layer index and \u0398k denotes all parameters in the k-th and previous layers; appending another learnable fully-connected layer to Learned ISTA-CPSS and perform an end-to-end training with the cost function (20)",
        "We compare our results with three baselines: the classical iterative compressive sensing solver, TVAL3 [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]; the \u201cblack-box\u201d deep learning compressive sensing solver, Recon-Net [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>];a l0-based network unfolded from iterative hard thresholding algorithm [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], noted as LIHT; and the baseline Learned ISTA network, in terms of PSNR8",
        "We select support w.r.t. a percentage of the largest magnitudes within the whole batch rather than within a single sample as we do in theorems and simulated experiments, which we emprically find is beneficial to the recovery performance",
        "We have introduced a partial weight coupling structure to Learned ISTA, which reduces the number of trainable parameters but does not hurt the performance",
        "We have further proposed support selection, which improves the convergence rate both theoretically and empirically",
        "Our theories are endorsed by extensive simulations and a real-data experiment"
    ],
    "summary": [
        "E.g., [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], show that a trained K-layer LISTA or its variants can generalize more than well to unseen samples (x , b ) from the same P(x, b) and recover x from b to the same accuracy within one or two order-of-magnitude fewer iterations than the original ISTA.",
        "[<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] investigated the convergence property of a sibling architecture to LISTA, proposed in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], which was obtained by instead unfolding/truncating the iterative hard thresholding (IHT) algorithm rather than ISTA.",
        "Empirical results in Fig. 3 illustrate that the structure (9), though having fewer parameters, improves the performance of LISTA.",
        "Combined with Theorem 1, we see that the partial weight coupling structure (10) is both necessary and sufficient to guarantee convergence in the noiseless case.",
        "In Fig 2, we report two values, W2k \u2212 (I \u2212 W1kA) 2 and \u03b8k, obtained by the baseline LISTA model (4) trained under the noiseless setting.",
        "This supports Theorem 2: there exist a sequence of parameters {(W k, \u03b8k)}Kk=\u221201 leading to linear convergence for LISTA-CP, and they can be obtained by data-driven learning.",
        "The difference is significant with the number of layers k \u2265 10, which supports our theoretical result: \u201cckss > c as k large enough\u201d in Theorem 3.",
        "The result of the noisy case (Fig. 5(b)) shows that LISTA-CPSS has better recovery error than LISTA-CP.",
        "On the training set, we are inspired by [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]: first using layer-wise pre-training with a reconstruction loss w.r.t. dictionary D plus an l1 loss, shown in (19), where k is the layer index and \u0398k denotes all parameters in the k-th and previous layers; appending another learnable fully-connected layer to LISTA-CPSS and perform an end-to-end training with the cost function (20).",
        "We compare our results with three baselines: the classical iterative CS solver, TVAL3 [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]; the \u201cblack-box\u201d deep learning CS solver, Recon-Net [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>];a l0-based network unfolded from IHT algorithm [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], noted as LIHT; and the baseline LISTA network, in terms of PSNR8.",
        "We select support w.r.t. a percentage of the largest magnitudes within the whole batch rather than within a single sample as we do in theorems and simulated experiments, which we emprically find is beneficial to the recovery performance.",
        "We have introduced a partial weight coupling structure to LISTA, which reduces the number of trainable parameters but does not hurt the performance.",
        "We believe that the methodology in this paper can be extended to analyzing and enhancing other unfolded iterative algorithms"
    ],
    "headline": "We study unfolded iterative shrinkage thresholding algorithm  for sparse signal recovery",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Thomas Blumensath and Mike E Davies. Iterative thresholding for sparse approximations. Journal of Fourier analysis and Applications, 14(5-6):629\u2013654, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20thresholding%20for%20sparse%20approximations%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20thresholding%20for%20sparse%20approximations%202008"
        },
        {
            "id": "2",
            "entry": "[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "3",
            "entry": "[3] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 399\u2013406. Omnipress, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20LeCun%2C%20Yann%20Learning%20fast%20approximations%20of%20sparse%20coding%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20Karol%20LeCun%2C%20Yann%20Learning%20fast%20approximations%20of%20sparse%20coding%202010"
        },
        {
            "id": "4",
            "entry": "[4] Zhangyang Wang, Qing Ling, and Thomas Huang. Learning deep l0 encoders. In AAAI Conference on Artificial Intelligence, pages 2194\u20132200, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Ling%2C%20Qing%20Huang%2C%20Thomas%20Learning%20deep%20l0%20encoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhangyang%20Ling%2C%20Qing%20Huang%2C%20Thomas%20Learning%20deep%20l0%20encoders%202016"
        },
        {
            "id": "5",
            "entry": "[5] Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and Thomas S Huang. D3: Deep dual-domain based fast restoration of jpeg-compressed images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2764\u20132772, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Liu%2C%20Ding%20Chang%2C%20Shiyu%20Ling%2C%20Qing%20D3%3A%20Deep%20dual-domain%20based%20fast%20restoration%20of%20jpeg-compressed%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhangyang%20Liu%2C%20Ding%20Chang%2C%20Shiyu%20Ling%2C%20Qing%20D3%3A%20Deep%20dual-domain%20based%20fast%20restoration%20of%20jpeg-compressed%20images%202016"
        },
        {
            "id": "6",
            "entry": "[6] Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, and Thomas S Huang. Learning a task-specific deep architecture for clustering. In Proceedings of the 2016 SIAM International Conference on Data Mining, pages 369\u2013377. SIAM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Chang%2C%20Shiyu%20Zhou%2C%20Jiayu%20Wang%2C%20Meng%20Learning%20a%20task-specific%20deep%20architecture%20for%20clustering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhangyang%20Chang%2C%20Shiyu%20Zhou%2C%20Jiayu%20Wang%2C%20Meng%20Learning%20a%20task-specific%20deep%20architecture%20for%20clustering%202016"
        },
        {
            "id": "7",
            "entry": "[7] Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, and Thomas S Huang. Learning a deep \u221e encoder for hashing. pages 2174\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhangyang%20Yang%2C%20Yingzhen%20Chang%2C%20Shiyu%20Ling%2C%20Qing%20Learning%20a%20deep%20%E2%88%9E%20encoder%20for%20hashing%202016"
        },
        {
            "id": "8",
            "entry": "[8] Pablo Sprechmann, Alexander M Bronstein, and Guillermo Sapiro. Learning efficient sparse and low rank models. IEEE transactions on pattern analysis and machine intelligence, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sprechmann%2C%20Pablo%20Bronstein%2C%20Alexander%20M.%20Sapiro%2C%20Guillermo%20Learning%20efficient%20sparse%20and%20low%20rank%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sprechmann%2C%20Pablo%20Bronstein%2C%20Alexander%20M.%20Sapiro%2C%20Guillermo%20Learning%20efficient%20sparse%20and%20low%20rank%20models%202015"
        },
        {
            "id": "9",
            "entry": "[9] Zhaowen Wang, Jianchao Yang, Haichao Zhang, Zhangyang Wang, Yingzhen Yang, Ding Liu, and Thomas S Huang. Sparse Coding and its Applications in Computer Vision. World Scientific.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhaowen%20Wang%20Jianchao%20Yang%20Haichao%20Zhang%20Zhangyang%20Wang%20Yingzhen%20Yang%20Ding%20Liu%20and%20Thomas%20S%20Huang%20Sparse%20Coding%20and%20its%20Applications%20in%20Computer%20Vision%20World%20Scientific",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhaowen%20Wang%20Jianchao%20Yang%20Haichao%20Zhang%20Zhangyang%20Wang%20Yingzhen%20Yang%20Ding%20Liu%20and%20Thomas%20S%20Huang%20Sparse%20Coding%20and%20its%20Applications%20in%20Computer%20Vision%20World%20Scientific"
        },
        {
            "id": "10",
            "entry": "[10] Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing. In IEEE CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jian%20Ghanem%2C%20Bernard%20ISTA-Net%3A%20Interpretable%20optimization-inspired%20deep%20network%20for%20image%20compressive%20sensing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jian%20Ghanem%2C%20Bernard%20ISTA-Net%3A%20Interpretable%20optimization-inspired%20deep%20network%20for%20image%20compressive%20sensing%202018"
        },
        {
            "id": "11",
            "entry": "[11] Joey Tianyi Zhou, Kai Di, Jiawei Du, Xi Peng, Hao Yang, Sinno Jialin Pan, Ivor W Tsang, Yong Liu, Zheng Qin, and Rick Siow Mong Goh. SC2Net: Sparse LSTMs for sparse coding. In AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Joey%20Tianyi%20Di%2C%20Kai%20Du%2C%20Jiawei%20Peng%2C%20Xi%20and%20Rick%20Siow%20Mong%20Goh.%20SC2Net%3A%20Sparse%20LSTMs%20for%20sparse%20coding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Joey%20Tianyi%20Di%2C%20Kai%20Du%2C%20Jiawei%20Peng%2C%20Xi%20and%20Rick%20Siow%20Mong%20Goh.%20SC2Net%3A%20Sparse%20LSTMs%20for%20sparse%20coding%202018"
        },
        {
            "id": "12",
            "entry": "[12] Thomas Moreau and Joan Bruna. Understanding trainable sparse coding with matrix factorization. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moreau%2C%20Thomas%20Bruna%2C%20Joan%20Understanding%20trainable%20sparse%20coding%20with%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moreau%2C%20Thomas%20Bruna%2C%20Joan%20Understanding%20trainable%20sparse%20coding%20with%20matrix%20factorization%202017"
        },
        {
            "id": "13",
            "entry": "[13] Raja Giryes, Yonina C Eldar, Alex Bronstein, and Guillermo Sapiro. Tradeoffs between convergence speed and reconstruction accuracy in inverse problems. IEEE Transactions on Signal Processing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giryes%2C%20Raja%20Eldar%2C%20Yonina%20C.%20Bronstein%2C%20Alex%20Sapiro%2C%20Guillermo%20Tradeoffs%20between%20convergence%20speed%20and%20reconstruction%20accuracy%20in%20inverse%20problems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giryes%2C%20Raja%20Eldar%2C%20Yonina%20C.%20Bronstein%2C%20Alex%20Sapiro%2C%20Guillermo%20Tradeoffs%20between%20convergence%20speed%20and%20reconstruction%20accuracy%20in%20inverse%20problems%202018"
        },
        {
            "id": "14",
            "entry": "[14] Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal sparsity with deep networks? In Advances in Neural Information Processing Systems, pages 4340\u20134348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xin%2C%20Bo%20Wang%2C%20Yizhou%20Gao%2C%20Wen%20Wipf%2C%20David%20Maximal%20sparsity%20with%20deep%20networks%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xin%2C%20Bo%20Wang%2C%20Yizhou%20Gao%2C%20Wen%20Wipf%2C%20David%20Maximal%20sparsity%20with%20deep%20networks%3F%202016"
        },
        {
            "id": "15",
            "entry": "[15] Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Applied and computational harmonic analysis, 27(3):265\u2013274, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009"
        },
        {
            "id": "16",
            "entry": "[16] Mark Borgerding, Philip Schniter, and Sundeep Rangan. AMP-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borgerding%2C%20Mark%20Schniter%2C%20Philip%20Rangan%2C%20Sundeep%20AMP-inspired%20deep%20networks%20for%20sparse%20linear%20inverse%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borgerding%2C%20Mark%20Schniter%2C%20Philip%20Rangan%2C%20Sundeep%20AMP-inspired%20deep%20networks%20for%20sparse%20linear%20inverse%20problems%202017"
        },
        {
            "id": "17",
            "entry": "[17] Christopher A Metzler, Ali Mousavi, and Richard G Baraniuk. Learned D-AMP: Principled neural network based compressive image recovery. In Advances in Neural Information Processing Systems, pages 1770\u20131781, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzler%2C%20Christopher%20A.%20Mousavi%2C%20Ali%20Baraniuk%2C%20Richard%20G.%20Learned%20D-AMP%3A%20Principled%20neural%20network%20based%20compressive%20image%20recovery%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzler%2C%20Christopher%20A.%20Mousavi%2C%20Ali%20Baraniuk%2C%20Richard%20G.%20Learned%20D-AMP%3A%20Principled%20neural%20network%20based%20compressive%20image%20recovery%202017"
        },
        {
            "id": "18",
            "entry": "[18] Mark Borgerding and Philip Schniter. Onsager-corrected deep learning for sparse linear inverse problems. In 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borgerding%2C%20Mark%20Schniter%2C%20Philip%20Onsager-corrected%20deep%20learning%20for%20sparse%20linear%20inverse%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borgerding%2C%20Mark%20Schniter%2C%20Philip%20Onsager-corrected%20deep%20learning%20for%20sparse%20linear%20inverse%20problems%202016"
        },
        {
            "id": "19",
            "entry": "[19] Stanley Osher, Yu Mao, Bin Dong, and Wotao Yin. Fast linearized bregman iteration for compressive sensing and sparse denoising. Communications in Mathematical Sciences, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osher%2C%20Stanley%20Mao%2C%20Yu%20Dong%2C%20Bin%20Yin%2C%20Wotao%20Fast%20linearized%20bregman%20iteration%20for%20compressive%20sensing%20and%20sparse%20denoising%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osher%2C%20Stanley%20Mao%2C%20Yu%20Dong%2C%20Bin%20Yin%2C%20Wotao%20Fast%20linearized%20bregman%20iteration%20for%20compressive%20sensing%20and%20sparse%20denoising%202010"
        },
        {
            "id": "20",
            "entry": "[20] Kristian Bredies and Dirk A Lorenz. Linear convergence of iterative soft-thresholding. Journal of Fourier Analysis and Applications, 14(5-6):813\u2013837, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bredies%2C%20Kristian%20Lorenz%2C%20Dirk%20A.%20Linear%20convergence%20of%20iterative%20soft-thresholding%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bredies%2C%20Kristian%20Lorenz%2C%20Dirk%20A.%20Linear%20convergence%20of%20iterative%20soft-thresholding%202008"
        },
        {
            "id": "21",
            "entry": "[21] Lufang Zhang, Yaohua Hu, Chong Li, and Jen-Chih Yao. A new linear convergence result for the iterative soft thresholding algorithm. Optimization, 66(7):1177\u20131189, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Lufang%20Hu%2C%20Yaohua%20Li%2C%20Chong%20Yao%2C%20Jen-Chih%20A%20new%20linear%20convergence%20result%20for%20the%20iterative%20soft%20thresholding%20algorithm%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Lufang%20Hu%2C%20Yaohua%20Li%2C%20Chong%20Yao%2C%20Jen-Chih%20A%20new%20linear%20convergence%20result%20for%20the%20iterative%20soft%20thresholding%20algorithm%202017"
        },
        {
            "id": "22",
            "entry": "[22] Shaozhe Tao, Daniel Boley, and Shuzhong Zhang. Local linear convergence of ista and fista on the lasso problem. SIAM Journal on Optimization, 26(1):313\u2013336, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tao%2C%20Shaozhe%20Boley%2C%20Daniel%20Zhang%2C%20Shuzhong%20Local%20linear%20convergence%20of%20ista%20and%20fista%20on%20the%20lasso%20problem%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tao%2C%20Shaozhe%20Boley%2C%20Daniel%20Zhang%2C%20Shuzhong%20Local%20linear%20convergence%20of%20ista%20and%20fista%20on%20the%20lasso%20problem%202016"
        },
        {
            "id": "23",
            "entry": "[23] Elaine T. Hale, Wotao Yin, and Yin Zhang. Fixed-point continuation for 1-minimization: methodology and convergence. SIAM Journal on Optimization, 19(3):1107\u20131130, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hale%2C%20Elaine%20T.%20Yin%2C%20Wotao%20Zhang%2C%20Yin%20Fixed-point%20continuation%20for%201-minimization%3A%20methodology%20and%20convergence%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hale%2C%20Elaine%20T.%20Yin%2C%20Wotao%20Zhang%2C%20Yin%20Fixed-point%20continuation%20for%201-minimization%3A%20methodology%20and%20convergence%202008"
        },
        {
            "id": "24",
            "entry": "[24] Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares problem. SIAM Journal on Optimization, 23(2):1062\u20131091, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal-gradient%20homotopy%20method%20for%20the%20sparse%20least-squares%20problem%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal-gradient%20homotopy%20method%20for%20the%20sparse%20least-squares%20problem%202013"
        },
        {
            "id": "25",
            "entry": "[25] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the International Conference on Computer Vision, volume 2, pages 416\u2013423, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%2C%20David%20Fowlkes%2C%20Charless%20Tal%2C%20Doron%20Malik%2C%20Jitendra%20A%20database%20of%20human%20segmented%20natural%20images%20and%20its%20application%20to%20evaluating%20segmentation%20algorithms%20and%20measuring%20ecological%20statistics%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%2C%20David%20Fowlkes%2C%20Charless%20Tal%2C%20Doron%20Malik%2C%20Jitendra%20A%20database%20of%20human%20segmented%20natural%20images%20and%20its%20application%20to%20evaluating%20segmentation%20algorithms%20and%20measuring%20ecological%20statistics%202001"
        },
        {
            "id": "26",
            "entry": "[26] Yangyang Xu and Wotao Yin. A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion. SIAM Journal on imaging sciences, 6(3):1758\u20131789, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Yangyang%20Yin%2C%20Wotao%20A%20block%20coordinate%20descent%20method%20for%20regularized%20multiconvex%20optimization%20with%20applications%20to%20nonnegative%20tensor%20factorization%20and%20completion%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Yangyang%20Yin%2C%20Wotao%20A%20block%20coordinate%20descent%20method%20for%20regularized%20multiconvex%20optimization%20with%20applications%20to%20nonnegative%20tensor%20factorization%20and%20completion%202013"
        },
        {
            "id": "27",
            "entry": "[27] Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, and Amit Ashok. ReconNet: Non-iterative reconstruction of images from compressively sensed measurements. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Kuldeep%20Lohit%2C%20Suhas%20Turaga%2C%20Pavan%20Kerviche%2C%20Ronan%20ReconNet%3A%20Non-iterative%20reconstruction%20of%20images%20from%20compressively%20sensed%20measurements%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Kuldeep%20Lohit%2C%20Suhas%20Turaga%2C%20Pavan%20Kerviche%2C%20Ronan%20ReconNet%3A%20Non-iterative%20reconstruction%20of%20images%20from%20compressively%20sensed%20measurements%202016"
        },
        {
            "id": "28",
            "entry": "[28] Chengbo Li, Wotao Yin, Hong Jiang, and Yin Zhang. An efficient augmented lagrangian method with applications to total variation minimization. Computational Optimization and Applications, 56(3):507\u2013530, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chengbo%20Yin%2C%20Wotao%20Jiang%2C%20Hong%20Zhang%2C%20Yin%20An%20efficient%20augmented%20lagrangian%20method%20with%20applications%20to%20total%20variation%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chengbo%20Yin%2C%20Wotao%20Jiang%2C%20Hong%20Zhang%2C%20Yin%20An%20efficient%20augmented%20lagrangian%20method%20with%20applications%20to%20total%20variation%20minimization%202013"
        },
        {
            "id": "29",
            "entry": "[29] Dimitris Bertsimas and John N Tsitsiklis. Introduction to linear optimization. Athena Scientific Belmont, MA, 1997. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Tsitsiklis%2C%20John%20N.%20Introduction%20to%20linear%20optimization.%20Athena%20Scientific%201997"
        }
    ]
}
