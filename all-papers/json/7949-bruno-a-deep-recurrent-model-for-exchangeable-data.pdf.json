{
    "filename": "7949-bruno-a-deep-recurrent-model-for-exchangeable-data.pdf",
    "metadata": {
        "title": "BRUNO: A Deep Recurrent Model for Exchangeable Data",
        "author": "Iryna Korshunova, Jonas Degrave, Ferenc Huszar, Yarin Gal, Arthur Gretton, Joni Dambre",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7949-bruno-a-deep-recurrent-model-for-exchangeable-data.pdf"
        },
        "abstract": "We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection."
    },
    "keywords": [
        {
            "term": "Gaussian processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_processes"
        },
        {
            "term": "BRUNO",
            "url": "https://en.wikipedia.org/wiki/BRUNO"
        },
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "k-nearest neighbours",
            "url": "https://en.wikipedia.org/wiki/K-Nearest_Neighbours"
        },
        {
            "term": "exchangeable sequence",
            "url": "https://en.wikipedia.org/wiki/exchangeable_sequence"
        },
        {
            "term": "student t",
            "url": "https://en.wikipedia.org/wiki/student_t"
        },
        {
            "term": "anomaly detection",
            "url": "https://en.wikipedia.org/wiki/anomaly_detection"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        }
    ],
    "highlights": [
        "We construct a bijective mapping between random variables xi \u2208 X in the observation space and features zi \u2208 Z, and explicitly define an exchangeable model for the sequences z1, z2, z3,",
        "We propose BRUNO, wherein we combine an exchangeable Student-t process with the Real NVP, and derive recurrent equations for the predictive distribution such that our model can be trained as an recurrent neural network",
        "We introduced BRUNO, a new technique combining deep learning and Student-t or Gaussian processes for modelling exchangeable data",
        "We may carry out implicit Bayesian inference, avoiding the need to compute posteriors and eliminating the high computational cost or approximation errors often associated with explicit Bayesian inference",
        "BRUNO shows promise for applications such as conditional image generation, few-shot concept learning, few-shot classification and online anomaly detection",
        "We showed that BRUNO trained in a generative way achieves good performance in a downstream few-shot classification task without any task-specific retraining"
    ],
    "key_statements": [
        "We construct a bijective mapping between random variables xi \u2208 X in the observation space and features zi \u2208 Z, and explicitly define an exchangeable model for the sequences z1, z2, z3,",
        "Set modelling has been a recent focus in machine learning, both due to relevant application domains and to efficiency gains when dealing with groups of objects [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We perform an implicit inference over \u03b8 and can exactly compute predictive distributions and the marginal likelihood. Both neural statistician and BRUNO can be applied in similar settings, namely few-shot learning and conditional image generation, albeit with some restrictions, as we will see in Section 4",
        "We propose BRUNO, wherein we combine an exchangeable Student-t process with the Real NVP, and derive recurrent equations for the predictive distribution such that our model can be trained as an recurrent neural network",
        "We combine Bayesian and deep learning tools from the previous sections and present our model for exchangeable sequences whose schematic is given in Figure 1",
        "We considered three models from Vinyals et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]: (a) k-nearest neighbours (k-NN), where matching is done on raw pixels (Pixels), (b) k-nearest neighbours with matching on discriminative features from a state-of-the-art classifier (Baseline Classifier), and (c) Matching networks",
        "As an extension to the few-shot learning task, we showed that BRUNO could be used for online set anomaly detection",
        "We introduced BRUNO, a new technique combining deep learning and Student-t or Gaussian processes for modelling exchangeable data",
        "We may carry out implicit Bayesian inference, avoiding the need to compute posteriors and eliminating the high computational cost or approximation errors often associated with explicit Bayesian inference",
        "BRUNO shows promise for applications such as conditional image generation, few-shot concept learning, few-shot classification and online anomaly detection",
        "We showed that BRUNO trained in a generative way achieves good performance in a downstream few-shot classification task without any task-specific retraining"
    ],
    "summary": [
        "We construct a bijective mapping between random variables xi \u2208 X in the observation space and features zi \u2208 Z, and explicitly define an exchangeable model for the sequences z1, z2, z3, .",
        "In respect of model training, evaluating the predictive distribution requires a single pass through the neural network that implements X \u2192 Z mapping.",
        "Bayesian sets [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] aim to model exchangeable sequences of binary random variables by analytically computing the integrals in Eq 2, 3.",
        "Both neural statistician and BRUNO can be applied in similar settings, namely few-shot learning and conditional image generation, albeit with some restrictions, as we will see in Section 4.",
        "We propose BRUNO, wherein we combine an exchangeable Student-t process with the Real NVP, and derive recurrent equations for the predictive distribution such that our model can be trained as an RNN.",
        "Real NVP [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] is a member of the normalising flows family of models, where some density in the input space X is transformed into a desired probability distribution in space Z through a sequence of invertible mappings [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "We combine Bayesian and deep learning tools from the previous sections and present our model for exchangeable sequences whose schematic is given in Figure 1.",
        "We see that computational complexity of making predictions in exchangeable GP s or T P s scales linearly with the number of observations, i.e. O(n) instead of a general O(n3) case where one needs to compute an inverse covariance matrix.",
        "Having an easy-to-evaluate autoregressive distribution p allows us to use a training scheme that is common for RNNs, i.e. maximise the likelihood of the element in the sequence at every step.",
        "We update the weights of the Real NVP model and learn the parameters of the prior Student-t distribution.",
        "We observe that BRUNO model from Section 4.1 outperforms the baseline classifier, despite having been trained on relatively long sequences with a generative objective, i.e. maximising the likelihood of the input images.",
        "We were not able to obtain convergent training with GP-based models which was not the case when using T Ps; an example is given in Appendix G.",
        "We introduced BRUNO, a new technique combining deep learning and Student-t or Gaussian processes for modelling exchangeable data.",
        "BRUNO shows promise for applications such as conditional image generation, few-shot concept learning, few-shot classification and online anomaly detection."
    ],
    "headline": "We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Aldous, D., Hennequin, P., Ibragimov, I., and Jacod, J. (1985). Ecole d\u2019Ete de Probabilites de Saint-Flour XIII, 1983. Lecture Notes in Mathematics. Springer Berlin Heidelberg.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aldous%2C%20D.%20Hennequin%2C%20P.%20Ibragimov%2C%20I.%20Jacod%2C%20J.%20Ecole%20d%E2%80%99Ete%20de%20Probabilites%20de%20Saint-Flour%20XIII%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aldous%2C%20D.%20Hennequin%2C%20P.%20Ibragimov%2C%20I.%20Jacod%2C%20J.%20Ecole%20d%E2%80%99Ete%20de%20Probabilites%20de%20Saint-Flour%20XIII%201985"
        },
        {
            "id": "2",
            "entry": "[2] Bailey, R. W. (1994). Polar generation of random variates with the t-distribution. Math. Comp., 62(206):779\u2013781.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bailey%2C%20R.W.%20Polar%20generation%20of%20random%20variates%20with%20the%20t-distribution%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bailey%2C%20R.W.%20Polar%20generation%20of%20random%20variates%20with%20the%20t-distribution%201994"
        },
        {
            "id": "3",
            "entry": "[3] Chen, F., Dong, Z., Li, Z., and He, X. (2018). Federated meta-learning for recommendation. arXiv preprint arXiv:1802.07876.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07876"
        },
        {
            "id": "4",
            "entry": "[4] Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). Density estimation using Real NVP. In Proceedings of the 5th International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20L.%20Sohl-Dickstein%2C%20J.%20Bengio%2C%20S.%20Density%20estimation%20using%20Real%20NVP%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20L.%20Sohl-Dickstein%2C%20J.%20Bengio%2C%20S.%20Density%20estimation%20using%20Real%20NVP%202017"
        },
        {
            "id": "5",
            "entry": "[5] Edwards, H. and Storkey, A. (2017). Towards a neural statistician. In Proceedings of the 5th International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edwards%2C%20H.%20Storkey%2C%20A.%20Towards%20a%20neural%20statistician%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edwards%2C%20H.%20Storkey%2C%20A.%20Towards%20a%20neural%20statistician%202017"
        },
        {
            "id": "6",
            "entry": "[6] Ghahramani, Z. and Heller, K. A. (2006). Bayesian sets. In Weiss, Y., Sch\u00f6lkopf, B., and Platt, J. C., editors, Advances in Neural Information Processing Systems 18, pages 435\u2013442. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Z.%20Heller%2C%20K.A.%20Bayesian%20sets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghahramani%2C%20Z.%20Heller%2C%20K.A.%20Bayesian%20sets%202006"
        },
        {
            "id": "7",
            "entry": "[7] Heller, K. A. and Ghahramani, Z. (2006). A simple bayesian framework for content-based image retrieval. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 2110\u20132117.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heller%2C%20K.A.%20Ghahramani%2C%20Z.%20A%20simple%20bayesian%20framework%20for%20content-based%20image%20retrieval%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heller%2C%20K.A.%20Ghahramani%2C%20Z.%20A%20simple%20bayesian%20framework%20for%20content-based%20image%20retrieval%202006"
        },
        {
            "id": "8",
            "entry": "[8] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the 2nd International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "9",
            "entry": "[9] Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images.%20Technical%20report%202009"
        },
        {
            "id": "10",
            "entry": "[10] Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "11",
            "entry": "[11] LeCun, Y., Cortes, C., and Burges, C. J. (1998). The MNIST database of handwritten digits.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Cortes%2C%20C.%20Burges%2C%20C.J.%20The%20MNIST%20database%20of%20handwritten%20digits%201998"
        },
        {
            "id": "12",
            "entry": "[12] Papamakarios, G., Murray, I., and Pavlakou, T. (2017). Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems 30, pages 2335\u20132344.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papamakarios%2C%20G.%20Murray%2C%20I.%20Pavlakou%2C%20T.%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papamakarios%2C%20G.%20Murray%2C%20I.%20Pavlakou%2C%20T.%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017"
        },
        {
            "id": "13",
            "entry": "[13] Rasmussen, C. E. and Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%20%28Adaptive%20Computation%20and%20Machine%20Learning%29%202005"
        },
        {
            "id": "14",
            "entry": "[14] Rezende, D. and Mohamed, S. (2015). Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1530\u20131538.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "15",
            "entry": "[15] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pages 1278\u20131286.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "16",
            "entry": "[16] Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Kingma%2C%20D.P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Kingma%2C%20D.P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] Shah, A., Wilson, A. G., and Ghahramani, Z. (2014). Student-t processes as alternatives to gaussian processes. In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, pages 877\u2013885.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shah%2C%20A.%20Wilson%2C%20A.G.%20Ghahramani%2C%20Z.%20Student-t%20processes%20as%20alternatives%20to%20gaussian%20processes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shah%2C%20A.%20Wilson%2C%20A.G.%20Ghahramani%2C%20Z.%20Student-t%20processes%20as%20alternatives%20to%20gaussian%20processes%202014"
        },
        {
            "id": "18",
            "entry": "[18] Szabo, Z., Sriperumbudur, B., Poczos, B., and Gretton, A. (2016). Learning theory for distribution regression. Journal of Machine Learning Research, 17(152).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szabo%2C%20Z.%20Sriperumbudur%2C%20B.%20Poczos%2C%20B.%20Gretton%2C%20A.%20Learning%20theory%20for%20distribution%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szabo%2C%20Z.%20Sriperumbudur%2C%20B.%20Poczos%2C%20B.%20Gretton%2C%20A.%20Learning%20theory%20for%20distribution%20regression%202016"
        },
        {
            "id": "19",
            "entry": "[19] Theis, L., van den Oord, A., and Bethge, M. (2016). A note on the evaluation of generative models. In Proceedings of the 4th International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20L.%20van%20den%20Oord%2C%20A.%20Bethge%2C%20M.%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20L.%20van%20den%20Oord%2C%20A.%20Bethge%2C%20M.%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016"
        },
        {
            "id": "20",
            "entry": "[20] Vinyals, O., Bengio, S., and Kudlur, M. (2016a). Order matters: Sequence to sequence for sets. In Proceedings of the 4th International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Bengio%2C%20S.%20Kudlur%2C%20M.%20Order%20matters%3A%20Sequence%20to%20sequence%20for%20sets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Bengio%2C%20S.%20Kudlur%2C%20M.%20Order%20matters%3A%20Sequence%20to%20sequence%20for%20sets%202016"
        },
        {
            "id": "21",
            "entry": "[21] Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016b). Matching networks for one shot learning. In Advances in Neural Information Processing Systems 29, pages 3630\u20133638.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Blundell%2C%20C.%20Lillicrap%2C%20T.%20Kavukcuoglu%2C%20K.%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Blundell%2C%20C.%20Lillicrap%2C%20T.%20Kavukcuoglu%2C%20K.%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "22",
            "entry": "[22] Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint, abs/1708.07747.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "23",
            "entry": "[23] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017). Deep sets. In Advances in Neural Information Processing Systems 30, pages 3394\u20133404. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20Poczos%2C%20B.%20Deep%20sets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20Poczos%2C%20B.%20Deep%20sets%202017"
        }
    ]
}
