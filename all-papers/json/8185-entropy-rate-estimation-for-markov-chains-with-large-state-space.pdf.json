{
    "filename": "8185-entropy-rate-estimation-for-markov-chains-with-large-state-space.pdf",
    "metadata": {
        "title": "Entropy Rate Estimation for Markov Chains with Large State Space",
        "author": "Yanjun Han, Jiantao Jiao, Chuan-Zheng Lee, Tsachy Weissman, Yihong Wu, Tiancheng Yu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8185-entropy-rate-estimation-for-markov-chains-with-large-state-space.pdf"
        },
        "abstract": "Entropy estimation is one of the prototypical problems in distribution property testing. To consistently estimate the Shannon entropy of a distribution on S elements with independent samples, the optimal sample complexity scales sublinearly with"
    },
    "keywords": [
        {
            "term": "discrete distribution",
            "url": "https://en.wikipedia.org/wiki/discrete_distribution"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "markov chain",
            "url": "https://en.wikipedia.org/wiki/markov_chain"
        },
        {
            "term": "entropy estimation",
            "url": "https://en.wikipedia.org/wiki/entropy_estimation"
        },
        {
            "term": "shannon entropy",
            "url": "https://en.wikipedia.org/wiki/shannon_entropy"
        },
        {
            "term": "Penn Treebank",
            "url": "https://en.wikipedia.org/wiki/Penn_Treebank"
        },
        {
            "term": "entropy rate",
            "url": "https://en.wikipedia.org/wiki/entropy_rate"
        },
        {
            "term": "reversible markov chain",
            "url": "https://en.wikipedia.org/wiki/reversible_markov_chain"
        },
        {
            "term": "random variable",
            "url": "https://en.wikipedia.org/wiki/random_variable"
        }
    ],
    "highlights": [
        "Consider a stationary stochastic process {Xt}\u221e t=1, where each Xt takes values in a finite alphabet X of size S",
        "Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with",
        "We provide a tight analysis of the sample complexity of the empirical entropy rate for Markov chains when the mixing time is not too large",
        "We focus on first-order Markov chains, since any finite-order Markov chain can be converted to a first-order one by extending the state space [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "The following result shows that mixing assumptions are only needed to control the variance of the empirical entropy rate: the bias of the empirical entropy rate vanishes uniformly over all Markov chains regardless of reversibility and mixing time assumptions as long as n S2",
        "To conclude this section we summarize our result in terms of the sample complexity for estimating the entropy rate within a few bits ( = \u0398(1)), classified according to the relaxation time:"
    ],
    "key_statements": [
        "Consider a stationary stochastic process {Xt}\u221e t=1, where each Xt takes values in a finite alphabet X of size S",
        "Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with",
        "We provide a tight analysis of the sample complexity of the empirical entropy rate for Markov chains when the mixing time is not too large",
        "We focus on first-order Markov chains, since any finite-order Markov chain can be converted to a first-order one by extending the state space [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "The following result shows that mixing assumptions are only needed to control the variance of the empirical entropy rate: the bias of the empirical entropy rate vanishes uniformly over all Markov chains regardless of reversibility and mixing time assumptions as long as n S2",
        "To conclude this section we summarize our result in terms of the sample complexity for estimating the entropy rate within a few bits ( = \u0398(1)), classified according to the relaxation time:",
        "To prove the lower bound for the independent Poisson model, the goal is to construct two symmetric random matrices, such that (a) they are sufficiently concentrated near the desired parameter space R(S, \u03b3, \u03c4, q) for properly chosen parameters \u03b3, \u03c4, q; (b) their entropy rates are separated; (c) the induced marginal laws of the sufficient statistic C = X0 \u222a{Cij +Cji : i = j, 1 \u2264 i \u2264 j \u2264 S}\u222a{Cii : 1 \u2264 i \u2264 S} are statistically indistinguishable"
    ],
    "summary": [
        "Consider a stationary stochastic process {Xt}\u221e t=1, where each Xt takes values in a finite alphabet X of size S.",
        "The empirical entropy rate requires at least \u03a9(S2) samples to be consistent, even when the Markov chain is memoryless.",
        "We provide a tight analysis of the sample complexity of the empirical entropy rate for Markov chains when the mixing time is not too large.",
        "The bias of the empirical entropy rate vanishes uniformly over all Markov chains regardless of mixing time and reversibility as long as the number of samples grows faster than the number of parameters.",
        "We obtain a characterization of the optimal sample complexity for estimating the entropy rate of a stationary reversible Markov chain in terms of the sample size, state space size, and mixing time, and partially resolve one of the open questions raised in [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "Note that we have made mixing time assumptions in the upper bound analysis of the empirical entropy rate in Theorem 1, which is natural since [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] showed that it is necessary to impose mixing time assumptions to provide meaningful statistical guarantees for entropy rate estimation in Markov chains.",
        "The following result shows that mixing assumptions are only needed to control the variance of the empirical entropy rate: the bias of the empirical entropy rate vanishes uniformly over all Markov chains regardless of reversibility and mixing time assumptions as long as n S2.",
        "Theorem 2 implies that if n S2, the bias of the empirical entropy rate estimator universally vanishes for any stationary Markov chains.",
        "Hwhich estimates the entropy rate with a uniformly vanishing error over Markov chains M2,rev(S, \u03b3\u2217) if and only if n",
        "To conclude this section we summarize our result in terms of the sample complexity for estimating the entropy rate within a few bits ( = \u0398(1)), classified according to the relaxation time:",
        "A key step in the analysis of Hemp and Hopt is the idea of simulating a finite-state Markov chain from independent samples [3, p.",
        "To prove the lower bound for the independent Poisson model, the goal is to construct two symmetric random matrices, such that (a) they are sufficiently concentrated near the desired parameter space R(S, \u03b3, \u03c4, q) for properly chosen parameters \u03b3, \u03c4, q; (b) their entropy rates are separated; (c) the induced marginal laws of the sufficient statistic C = X0 \u222a{Cij +Cji : i = j, 1 \u2264 i \u2264 j \u2264 S}\u222a{Cii : 1 \u2264 i \u2264 S} are statistically indistinguishable."
    ],
    "headline": "Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A unified maximum likelihood approach for estimating symmetric properties of discrete distributions. In International Conference on Machine Learning, pages 11\u201321, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Acharya%2C%20Jayadev%20Das%2C%20Hirakendu%20Orlitsky%2C%20Alon%20and%20Ananda%20Theertha%20Suresh.%20A%20unified%20maximum%20likelihood%20approach%20for%20estimating%20symmetric%20properties%20of%20discrete%20distributions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Acharya%2C%20Jayadev%20Das%2C%20Hirakendu%20Orlitsky%2C%20Alon%20and%20Ananda%20Theertha%20Suresh.%20A%20unified%20maximum%20likelihood%20approach%20for%20estimating%20symmetric%20properties%20of%20discrete%20distributions%202017"
        },
        {
            "id": "2",
            "entry": "[2] Andr\u00e1s Antos and Ioannis Kontoyiannis. Convergence properties of functional estimates for discrete distributions. Random Structures & Algorithms, 19(3-4):163\u2013193, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antos%2C%20Andr%C3%A1s%20Kontoyiannis%2C%20Ioannis%20Convergence%20properties%20of%20functional%20estimates%20for%20discrete%20distributions%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antos%2C%20Andr%C3%A1s%20Kontoyiannis%2C%20Ioannis%20Convergence%20properties%20of%20functional%20estimates%20for%20discrete%20distributions%202001"
        },
        {
            "id": "3",
            "entry": "[3] Patrick Billingsley. Statistical methods in Markov chains. The Annals of Mathematical Statistics, pages 12\u201340, 1961.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Billingsley%2C%20Patrick%20Statistical%20methods%20in%20Markov%20chains%201961",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Billingsley%2C%20Patrick%20Statistical%20methods%20in%20Markov%20chains%201961"
        },
        {
            "id": "4",
            "entry": "[4] Charles Bordenave, Pietro Caputo, and Djalil Chafai. Spectrum of large random reversible Markov chains: two examples. ALEA: Latin American Journal of Probability and Mathematical Statistics, 7:41\u201364, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bordenave%2C%20Charles%20Caputo%2C%20Pietro%20Chafai%2C%20Djalil%20Spectrum%20of%20large%20random%20reversible%20Markov%20chains%3A%20two%20examples%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bordenave%2C%20Charles%20Caputo%2C%20Pietro%20Chafai%2C%20Djalil%20Spectrum%20of%20large%20random%20reversible%20Markov%20chains%3A%20two%20examples%202010"
        },
        {
            "id": "5",
            "entry": "[5] Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer, Stephen A. Della Pietra, and Jennifer C. Lai. An estimate of an upper bound for the entropy of english. Comput. Linguist., 18(1):31\u201340, March 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20Peter%20F.%20Pietra%2C%20Vincent%20J.Della%20Mercer%2C%20Robert%20L.%20Pietra%2C%20Stephen%20A.Della%20An%20estimate%20of%20an%20upper%20bound%20for%20the%20entropy%20of%20english%201992-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brown%2C%20Peter%20F.%20Pietra%2C%20Vincent%20J.Della%20Mercer%2C%20Robert%20L.%20Pietra%2C%20Stephen%20A.Della%20An%20estimate%20of%20an%20upper%20bound%20for%20the%20entropy%20of%20english%201992-03"
        },
        {
            "id": "6",
            "entry": "[6] Haixiao Cai, Sanjeev R. Kulkarni, and Sergio Verd\u00fa. Universal entropy estimation via block sorting. IEEE Trans. Inf. Theory, 50(7):1551\u20131561, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Haixiao%20Kulkarni%2C%20Sanjeev%20R.%20Verd%C3%BA%2C%20Sergio%20Universal%20entropy%20estimation%20via%20block%20sorting%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Haixiao%20Kulkarni%2C%20Sanjeev%20R.%20Verd%C3%BA%2C%20Sergio%20Universal%20entropy%20estimation%20via%20block%20sorting%202004"
        },
        {
            "id": "7",
            "entry": "[7] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicolo%20Lugosi%2C%20Gabor%20Prediction%2C%20learning%2C%20and%20games%202006"
        },
        {
            "id": "8",
            "entry": "[8] Gabriela Ciuperca and Valerie Girardin. On the estimation of the entropy rate of finite Markov chains. In Proceedings of the International Symposium on Applied Stochastic Models and Data Analysis, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciuperca%2C%20Gabriela%20Girardin%2C%20Valerie%20On%20the%20estimation%20of%20the%20entropy%20rate%20of%20finite%20Markov%20chains%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciuperca%2C%20Gabriela%20Girardin%2C%20Valerie%20On%20the%20estimation%20of%20the%20entropy%20rate%20of%20finite%20Markov%20chains%202005"
        },
        {
            "id": "9",
            "entry": "[9] Thomas Cover and Roger King. A convergent gambling estimate of the entropy of English. IEEE Transactions on Information Theory, 24(4):413\u2013421, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20King%2C%20Roger%20A%20convergent%20gambling%20estimate%20of%20the%20entropy%20of%20English%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cover%2C%20Thomas%20King%2C%20Roger%20A%20convergent%20gambling%20estimate%20of%20the%20entropy%20of%20English%201978"
        },
        {
            "id": "10",
            "entry": "[10] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley, New York, second edition, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20M.%20Thomas%2C%20Joy%20A.%20Elements%20of%20Information%20Theory%202006"
        },
        {
            "id": "11",
            "entry": "[11] Constantinos Daskalakis, Nishanth Dikkala, and Nick Gravin. Testing symmetric markov chains from a single trajectory. arXiv preprint arXiv:1704.06850, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06850"
        },
        {
            "id": "12",
            "entry": "[12] Michelle Effros, Karthik Visweswariah, Sanjeev R Kulkarni, and Sergio Verd\u00fa. Universal lossless source coding with the Burrows Wheeler transform. IEEE Transactions on Information Theory, 48(5):1061\u20131081, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Effros%2C%20Michelle%20Visweswariah%2C%20Karthik%20Kulkarni%2C%20Sanjeev%20R.%20Verd%C3%BA%2C%20Sergio%20Universal%20lossless%20source%20coding%20with%20the%20Burrows%20Wheeler%20transform%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Effros%2C%20Michelle%20Visweswariah%2C%20Karthik%20Kulkarni%2C%20Sanjeev%20R.%20Verd%C3%BA%2C%20Sergio%20Universal%20lossless%20source%20coding%20with%20the%20Burrows%20Wheeler%20transform%202002"
        },
        {
            "id": "13",
            "entry": "[13] Moein Falahatgar, Alon Orlitsky, Venkatadheeraj Pichapati, and Ananda Theertha Suresh. Learning markov distributions: Does estimation trump compression? In Information Theory (ISIT), 2016 IEEE International Symposium on, pages 2689\u20132693. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Falahatgar%2C%20Moein%20Orlitsky%2C%20Alon%20Pichapati%2C%20Venkatadheeraj%20and%20Ananda%20Theertha%20Suresh.%20Learning%20markov%20distributions%3A%20Does%20estimation%20trump%20compression%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Falahatgar%2C%20Moein%20Orlitsky%2C%20Alon%20Pichapati%2C%20Venkatadheeraj%20and%20Ananda%20Theertha%20Suresh.%20Learning%20markov%20distributions%3A%20Does%20estimation%20trump%20compression%3F%202016"
        },
        {
            "id": "14",
            "entry": "[14] Yanjun Han, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. Optimal rates of entropy estimation over lipschitz balls. arxiv preprint arxiv:1711.02141, Nov 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02141"
        },
        {
            "id": "15",
            "entry": "[15] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 58(301):13\u201330, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoeffding%2C%20Wassily%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoeffding%2C%20Wassily%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963"
        },
        {
            "id": "16",
            "entry": "[16] Daniel J Hsu, Aryeh Kontorovich, and Csaba Szepesv\u00e1ri. Mixing time estimation in reversible Markov chains from a single sample path. In Advances in neural information processing systems, pages 1459\u20131467, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Daniel%20J.%20Kontorovich%2C%20Aryeh%20Szepesv%C3%A1ri%2C%20Csaba%20Mixing%20time%20estimation%20in%20reversible%20Markov%20chains%20from%20a%20single%20sample%20path%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20Daniel%20J.%20Kontorovich%2C%20Aryeh%20Szepesv%C3%A1ri%2C%20Csaba%20Mixing%20time%20estimation%20in%20reversible%20Markov%20chains%20from%20a%20single%20sample%20path%202015"
        },
        {
            "id": "17",
            "entry": "[17] Qian Jiang. Construction of transition matrices of reversible Markov chains. M. Sc. Major Paper. Department of Mathematics and Statistics. University of Windsor, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Qian%20Construction%20of%20transition%20matrices%20of%20reversible%20Markov%20chains.%20M%202009"
        },
        {
            "id": "18",
            "entry": "[18] Jiantao Jiao, H.H. Permuter, Lei Zhao, Young-Han Kim, and T. Weissman. Universal estimation of directed information. Information Theory, IEEE Transactions on, 59(10):6220\u20136242, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiantao%20Jiao%2C%20H.H.Permuter%20Zhao%2C%20Lei%20Kim%2C%20Young-Han%20Weissman%2C%20T.%20Universal%20estimation%20of%20directed%20information%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiantao%20Jiao%2C%20H.H.Permuter%20Zhao%2C%20Lei%20Kim%2C%20Young-Han%20Weissman%2C%20T.%20Universal%20estimation%20of%20directed%20information%202013"
        },
        {
            "id": "19",
            "entry": "[19] Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Minimax estimation of functionals of discrete distributions. Information Theory, IEEE Transactions on, 61(5):2835\u2013 2885, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiao%2C%20Jiantao%20Venkat%2C%20Kartik%20Han%2C%20Yanjun%20Weissman%2C%20Tsachy%20Minimax%20estimation%20of%20functionals%20of%20discrete%20distributions",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiao%2C%20Jiantao%20Venkat%2C%20Kartik%20Han%2C%20Yanjun%20Weissman%2C%20Tsachy%20Minimax%20estimation%20of%20functionals%20of%20discrete%20distributions"
        },
        {
            "id": "20",
            "entry": "[20] Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Maximum likelihood estimation of functionals of discrete distributions. IEEE Transactions on Information Theory, 63(10):6774\u2013 6798, Oct 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiao%2C%20Jiantao%20Venkat%2C%20Kartik%20Han%2C%20Yanjun%20Weissman%2C%20Tsachy%20Maximum%20likelihood%20estimation%20of%20functionals%20of%20discrete%20distributions%206798-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiao%2C%20Jiantao%20Venkat%2C%20Kartik%20Han%2C%20Yanjun%20Weissman%2C%20Tsachy%20Maximum%20likelihood%20estimation%20of%20functionals%20of%20discrete%20distributions%206798-10"
        },
        {
            "id": "21",
            "entry": "[21] Daniel Jurafsky and James H. Martin. Speech and Language Processing (2Nd Edition). PrenticeHall, Inc., Upper Saddle River, NJ, USA, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jurafsky%2C%20Daniel%20Martin%2C%20James%20H.%20Speech%20and%20Language%20Processing%202009"
        },
        {
            "id": "22",
            "entry": "[22] Sudeep Kamath and Sergio Verd\u00fa. Estimation of entropy rate and R\u00e9nyi entropy rate for Markov chains. In Information Theory (ISIT), 2016 IEEE International Symposium on, pages 685\u2013689. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamath%2C%20Sudeep%20Verd%C3%BA%2C%20Sergio%20Estimation%20of%20entropy%20rate%20and%20R%C3%A9nyi%20entropy%20rate%20for%20Markov%20chains%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamath%2C%20Sudeep%20Verd%C3%BA%2C%20Sergio%20Estimation%20of%20entropy%20rate%20and%20R%C3%A9nyi%20entropy%20rate%20for%20Markov%20chains%202016"
        },
        {
            "id": "23",
            "entry": "[23] John C Kieffer. Sample converses in source coding theory. IEEE Transactions on Information Theory, 37(2):263\u2013268, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kieffer%2C%20John%20C.%20Sample%20converses%20in%20source%20coding%20theory%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kieffer%2C%20John%20C.%20Sample%20converses%20in%20source%20coding%20theory%201991"
        },
        {
            "id": "24",
            "entry": "[24] Ioannis Kontoyiannis, Paul H Algoet, Yu M Suhov, and AJ Wyner. Nonparametric entropy estimation for stationary processes and random fields, with applications to English text. Information Theory, IEEE Transactions on, 44(3):1319\u20131327, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kontoyiannis%2C%20Ioannis%20Algoet%2C%20Paul%20H.%20Suhov%2C%20Yu%20M.%20Wyner%2C%20A.J.%20Nonparametric%20entropy%20estimation%20for%20stationary%20processes%20and%20random%20fields%2C%20with%20applications%20to%20English%20text%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kontoyiannis%2C%20Ioannis%20Algoet%2C%20Paul%20H.%20Suhov%2C%20Yu%20M.%20Wyner%2C%20A.J.%20Nonparametric%20entropy%20estimation%20for%20stationary%20processes%20and%20random%20fields%2C%20with%20applications%20to%20English%20text%201998"
        },
        {
            "id": "25",
            "entry": "[25] Coco Krumme, Alejandro Llorente, Alex Manuel Cebrian, and Esteban Moro Pentland. The predictability of consumer visitation patterns. Scientific reports, 3, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krumme%2C%20Coco%20Llorente%2C%20Alejandro%20Cebrian%2C%20Alex%20Manuel%20Pentland%2C%20Esteban%20Moro%20The%20predictability%20of%20consumer%20visitation%20patterns%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krumme%2C%20Coco%20Llorente%2C%20Alejandro%20Cebrian%2C%20Alex%20Manuel%20Pentland%2C%20Esteban%20Moro%20The%20predictability%20of%20consumer%20visitation%20patterns%202013"
        },
        {
            "id": "26",
            "entry": "[26] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. CoRR, abs/1703.10722, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10722"
        },
        {
            "id": "27",
            "entry": "[27] J Kevin Lanctot, Ming Li, and En-hui Yang. Estimating DNA sequence entropy. In Symposium on discrete algorithms: proceedings of the eleventh annual ACM-SIAM symposium on discrete algorithms, volume 9, pages 409\u2013418, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20J.Kevin%20Li%2C%20Ming%20Yang%2C%20En-hui%20Estimating%20DNA%20sequence%20entropy%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lanctot%2C%20J.Kevin%20Li%2C%20Ming%20Yang%2C%20En-hui%20Estimating%20DNA%20sequence%20entropy%202000"
        },
        {
            "id": "28",
            "entry": "[28] David A Levin and Yuval Peres. Estimating the spectral gap of a reversible markov chain from a short trajectory. arXiv preprint arXiv:1612.05330, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.05330"
        },
        {
            "id": "29",
            "entry": "[29] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms and probabilistic analysis. Cambridge University Press, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitzenmacher%2C%20Michael%20Upfal%2C%20Eli%20Probability%20and%20computing%3A%20Randomized%20algorithms%20and%20probabilistic%20analysis%202005"
        },
        {
            "id": "30",
            "entry": "[30] Ravi Montenegro and Prasad Tetali. Mathematical aspects of mixing times in Markov chains. Foundations and Trends R in Theoretical Computer Science, 1(3):237\u2013354, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montenegro%2C%20Ravi%20Tetali%2C%20Prasad%20Mathematical%20aspects%20of%20mixing%20times%20in%20Markov%20chains%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montenegro%2C%20Ravi%20Tetali%2C%20Prasad%20Mathematical%20aspects%20of%20mixing%20times%20in%20Markov%20chains%202006"
        },
        {
            "id": "31",
            "entry": "[31] Liam Paninski. Estimation of entropy and mutual information. Neural Computation, 15(6):1191\u2013 1253, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paninski%2C%20Liam%20Estimation%20of%20entropy%20and%20mutual%20information%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paninski%2C%20Liam%20Estimation%20of%20entropy%20and%20mutual%20information%202003"
        },
        {
            "id": "32",
            "entry": "[32] Liam Paninski. Estimating entropy on m bins given fewer than m samples. Information Theory, IEEE Transactions on, 50(9):2200\u20132203, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paninski%2C%20Liam%20Estimating%20entropy%20on%20m%20bins%20given%20fewer%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paninski%2C%20Liam%20Estimating%20entropy%20on%20m%20bins%20given%20fewer%202004"
        },
        {
            "id": "33",
            "entry": "[33] Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral methods. Electronic Journal of Probability, 20, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paulin%2C%20Daniel%20Concentration%20inequalities%20for%20markov%20chains%20by%20marton%20couplings%20and%20spectral%20methods%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paulin%2C%20Daniel%20Concentration%20inequalities%20for%20markov%20chains%20by%20marton%20couplings%20and%20spectral%20methods%202015"
        },
        {
            "id": "34",
            "entry": "[34] Claude E. Shannon. Prediction and entropy of printed English. The Bell System Technical Journal, 30(1):50\u201364, Jan 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shannon%2C%20Claude%20E.%20Prediction%20and%20entropy%20of%20printed%20English%201951-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shannon%2C%20Claude%20E.%20Prediction%20and%20entropy%20of%20printed%20English%201951-01"
        },
        {
            "id": "35",
            "entry": "[35] Paul C Shields. The ergodic theory of discrete sample paths. Graduate Studies in Mathematics, American Mathematics Society, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shields%2C%20Paul%20C.%20The%20ergodic%20theory%20of%20discrete%20sample%20paths%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shields%2C%20Paul%20C.%20The%20ergodic%20theory%20of%20discrete%20sample%20paths%201996"
        },
        {
            "id": "36",
            "entry": "[36] Chaoming Song, Zehui Qu, Nicholas Blumm, and Albert-L\u00e1szl\u00f3 Barab\u00e1si. Limits of predictability in human mobility. Science, 327(5968):1018\u20131021, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Chaoming%20Qu%2C%20Zehui%20Blumm%2C%20Nicholas%20Barab%C3%A1si%2C%20Albert-L%C3%A1szl%C3%B3%20Limits%20of%20predictability%20in%20human%20mobility%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Chaoming%20Qu%2C%20Zehui%20Blumm%2C%20Nicholas%20Barab%C3%A1si%2C%20Albert-L%C3%A1szl%C3%B3%20Limits%20of%20predictability%20in%20human%20mobility%202010"
        },
        {
            "id": "37",
            "entry": "[37] Taro Takaguchi, Mitsuhiro Nakamura, Nobuo Sato, Kazuo Yano, and Naoki Masuda. Predictability of conversation partners. Physical Review X, 1(1):011008, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takaguchi%2C%20Taro%20Nakamura%2C%20Mitsuhiro%20Sato%2C%20Nobuo%20Yano%2C%20Kazuo%20Predictability%20of%20conversation%20partners%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takaguchi%2C%20Taro%20Nakamura%2C%20Mitsuhiro%20Sato%2C%20Nobuo%20Yano%2C%20Kazuo%20Predictability%20of%20conversation%20partners%202011"
        },
        {
            "id": "38",
            "entry": "[38] Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Society Providence, RI, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tao%2C%20Terence%20Topics%20in%20random%20matrix%20theory%2C%20volume%20132%202012"
        },
        {
            "id": "39",
            "entry": "[39] Kedar Tatwawadi, Jiantao Jiao, and Tsachy Weissman. Minimax redundancy for markov chains with large state space. arXiv preprint arXiv:1805.01355, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01355"
        },
        {
            "id": "40",
            "entry": "[40] A. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20A.%20Introduction%20to%20Nonparametric%20Estimation%202008"
        },
        {
            "id": "41",
            "entry": "[41] Gregory Valiant and Paul Valiant. Estimating the unseen: an n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs. In Proceedings of the 43rd annual ACM symposium on Theory of computing, pages 685\u2013694. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valiant%2C%20Gregory%20Valiant%2C%20Paul%20Estimating%20the%20unseen%3A%20an%20n/%20log%20n-sample%20estimator%20for%20entropy%20and%20support%20size%2C%20shown%20optimal%20via%20new%20CLTs%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valiant%2C%20Gregory%20Valiant%2C%20Paul%20Estimating%20the%20unseen%3A%20an%20n/%20log%20n-sample%20estimator%20for%20entropy%20and%20support%20size%2C%20shown%20optimal%20via%20new%20CLTs%202011"
        },
        {
            "id": "42",
            "entry": "[42] Gregory Valiant and Paul Valiant. The power of linear estimators. In Foundations of Computer Science (FOCS), 2011 IEEE 52nd Annual Symposium on, pages 403\u2013412. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valiant%2C%20Gregory%20Valiant%2C%20Paul%20The%20power%20of%20linear%20estimators%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valiant%2C%20Gregory%20Valiant%2C%20Paul%20The%20power%20of%20linear%20estimators%202011"
        },
        {
            "id": "43",
            "entry": "[43] Paul Valiant and Gregory Valiant. Estimating the unseen: improved estimators for entropy and other properties. In Advances in Neural Information Processing Systems, pages 2157\u20132165, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valiant%2C%20Paul%20Valiant%2C%20Gregory%20Estimating%20the%20unseen%3A%20improved%20estimators%20for%20entropy%20and%20other%20properties%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valiant%2C%20Paul%20Valiant%2C%20Gregory%20Estimating%20the%20unseen%3A%20improved%20estimators%20for%20entropy%20and%20other%20properties%202013"
        },
        {
            "id": "44",
            "entry": "[44] Chunyan Wang and Bernardo A Huberman. How random are online social interactions? Scientific reports, 2, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Chunyan%20Huberman%2C%20Bernardo%20A.%20How%20random%20are%20online%20social%20interactions%3F%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Chunyan%20Huberman%2C%20Bernardo%20A.%20How%20random%20are%20online%20social%20interactions%3F%202012"
        },
        {
            "id": "45",
            "entry": "[45] Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best polynomial approximation. IEEE Transactions on Information Theory, 62(6):3702\u20133720, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yihong%20Yang%2C%20Pengkun%20Minimax%20rates%20of%20entropy%20estimation%20on%20large%20alphabets%20via%20best%20polynomial%20approximation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yihong%20Yang%2C%20Pengkun%20Minimax%20rates%20of%20entropy%20estimation%20on%20large%20alphabets%20via%20best%20polynomial%20approximation%202016"
        },
        {
            "id": "46",
            "entry": "[46] Aaron D. Wyner and Jacob Ziv. Some asymptotic properties of the entropy of a stationary ergodic data source with applications to data compression. IEEE Trans. Inf. Theory, 35(6):1250\u2013 1258, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wyner%2C%20Aaron%20D.%20Ziv%2C%20Jacob%20Some%20asymptotic%20properties%20of%20the%20entropy%20of%20a%20stationary%20ergodic%20data%20source%20with%20applications%20to%20data%20compression%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wyner%2C%20Aaron%20D.%20Ziv%2C%20Jacob%20Some%20asymptotic%20properties%20of%20the%20entropy%20of%20a%20stationary%20ergodic%20data%20source%20with%20applications%20to%20data%20compression%201989"
        },
        {
            "id": "47",
            "entry": "[47] Jacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding. Information Theory, IEEE Transactions on, 24(5):530\u2013536, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziv%2C%20Jacob%20Lempel%2C%20Abraham%20Compression%20of%20individual%20sequences%20via%20variable-rate%20coding%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziv%2C%20Jacob%20Lempel%2C%20Abraham%20Compression%20of%20individual%20sequences%20via%20variable-rate%20coding%201978"
        },
        {
            "id": "48",
            "entry": "[48] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR, abs/1611.01578, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        }
    ]
}
