{
    "filename": "7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication.pdf",
    "metadata": {
        "title": "A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication",
        "author": "Peng Jiang, Gagan Agrawal",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks. Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost. However, there is still a lack of understanding about how sparse and quantized communication affects the convergence rate of the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization. We p show that O(1/ M K) convergence rate can be achieved if the sparsification and quantization hyperparameters are configured properly. We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communip cation cost while preserving the O(1/ M K) convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-communication SGD with only 3% 5% communication data size."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "The explosion of data and an increase in model size has led to great interest in training deep neural networks on distributed systems",
        "We prove that if all of the parameter components are exchanged in p consecutive iterations, p distributed Stochastic Gradient Descent will converge at rate O(1/ M K) if K is large enough and p is limited",
        "We focus on full precision neural network with quantized gradients, and we show that convergence rate is guaranteed on non-convex optimization if a good quantization function is used.\n3 Analysis of Sparse Parameter Averaging and Gradient Quantization",
        "We show that sparse parameter averaging and gradient quantization can p achieve O(1/ M K) convergence rate for distributed Stochastic Gradient Descent",
        "We studied the convergence rate of distributed Stochastic Gradient Descent with two communication reducing strategies: sparse parameter averaging and gradient quantization",
        "We propose a strategy p called Periodic Quantized Averaging SGD that combines sparsification and quantization while preserving the O(1/ M K)"
    ],
    "key_statements": [
        "The explosion of data and an increase in model size has led to great interest in training deep neural networks on distributed systems",
        "In today\u2019s mainstream deep learning frameworks such as Tensorflow, Torch, MXNet, Caffe, and CNTK [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], data-parallel distributed Stochastic Gradient Descent is widely adopted to exploit the compute capacity of multiple machines",
        "The idea of data-parallel distributed Stochastic Gradient Descent is that each machine holds a copy of the entire model and computes stochastic gradients with local mini-batches, and the local model parameters or gradients are frequently synchronized to achieve a global consensus of the learned model",
        "To fill these gaps in theory, this paper studies the convergence rate of distributed Stochastic Gradient Descent with sparse and quantized communication for the following non-convex stochastic optimization problem: min x2RN",
        "We prove that if all of the parameter components are exchanged in p consecutive iterations, p distributed Stochastic Gradient Descent will converge at rate O(1/ M K) if K is large enough and p is limited",
        "We prove that distributed Stochastic Gradient Descent that averages the model parameters only once every p iterations can converge at rate p O(1/ M K)",
        "We focus on full precision neural network with quantized gradients, and we show that convergence rate is guaranteed on non-convex optimization if a good quantization function is used.\n3 Analysis of Sparse Parameter Averaging and Gradient Quantization",
        "Underpthe assumptions in \u00a73.1, if using a quantization function with an error bound of q and setting = \u2713 M/K where \u2713 > 0 is a constant, we have the following convergence rate for distributed Stochastic Gradient Descent: K",
        "We show that sparse parameter averaging and gradient quantization can p achieve O(1/ M K) convergence rate for distributed Stochastic Gradient Descent",
        "There are three aspects in our evaluation: 1) We evaluate two sparse parameter averaging strategies \u2013 periodic averaging (PSGD) and rotate averaging (RSGD), on partitioned training data",
        "We studied the convergence rate of distributed Stochastic Gradient Descent with two communication reducing strategies: sparse parameter averaging and gradient quantization",
        "We propose a strategy p called Periodic Quantized Averaging SGD that combines sparsification and quantization while preserving the O(1/ M K)"
    ],
    "summary": [
        "The explosion of data and an increase in model size has led to great interest in training deep neural networks on distributed systems.",
        "To fill these gaps in theory, this paper studies the convergence rate of distributed SGD with sparse and quantized communication for the following non-convex stochastic optimization problem: min x2RN",
        "Contributions We first analyze the convergence rate of distributed SGD with two communication reducing techniques: sparse parameter averaging and gradient quantization.",
        "We prove that distributed SGD that averages the model parameters only once every p iterations can converge at rate p O(1/ M K).",
        "SGD will converge at rate O((1 + q)/ M K) or O((1 + qm)/ M K), depending on if the training data are shared or partitioned among nodes.",
        "This result suggests that choosing a p quantization function that ensures q = \u21e5(1) and q = \u21e5(1/m) can achieve O(1/ M K) convergence rate for distributed SGD in the two scenarios.",
        "Many gradient sparsification and quantization techniques have been proposed to reduce the communication overhead in distributed training.",
        "None of the previous works have considered the impact of partitioned training data to the convergence rate of distributed SGD with gradient quantization.",
        "We analyze the convergence rates of distributed SGD with two communication reducing techniques: sparse parameter averaging and gradient quantization.",
        "We have the following convergence rate for distributed SGD with any gradient sparsification method: Theorem 1.",
        "If we periodically apverage the model parameters once every p iterations, distributed SGD still converges at rate O(1/ M K) when K is large enough.",
        "We have the following convergence result for distributed SGD with gradient quantization: Theorem 2.",
        "Underpthe assumptions in \u00a73.1, if using a quantization function with an error bound of q and setting = \u2713 M/K where \u2713 > 0 is a constant, we have the following convergence rate for distributed SGD: K",
        "We show that sparse parameter averaging and gradient quantization can p achieve O(1/ M K) convergence rate for distributed SGD.",
        "Each node computes a stochastic gradient based on a local mini-batch and updates its local model parameters.",
        "Under the assumptions in \u00a73.1, psuppose the quantization function is unbiased with an error bound q and the learning rate = \u2713 M/K where \u2713 is a constant, Algorithm 2 has the following convergence rate: 1 K",
        "We studied the convergence rate of distributed SGD with two communication reducing strategies: sparse parameter averaging and gradient quantization.",
        "We prove that both strategies p can achieve O(1/<br/><br/>M K) convergence rate if configured properly.<br/><br/>We propose a strategy p called PQASGD that combines sparsification and quantization while preserving the O(1/ M K)<br/><br/>matches the convergence rate of full-communication SGD with only 3%-5% communication data size"
    ],
    "headline": "We study the convergence rate of distributed Stochastic Gradient Descent for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR, abs/1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "2",
            "entry": "[2] A. Acero. Acoustical and environmental robustness in automatic speech recognition. 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Acero%2C%20A.%20Acoustical%20and%20environmental%20robustness%20in%20automatic%20speech%20recognition%201990"
        },
        {
            "id": "3",
            "entry": "[3] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 873\u2013881. Curran Associates, Inc., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Duchi%2C%20J.C.%20Distributed%20delayed%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Duchi%2C%20J.C.%20Distributed%20delayed%20stochastic%20optimization%202011"
        },
        {
            "id": "4",
            "entry": "[4] A. F. Aji and K. Heafield. Sparse communication for distributed gradient descent. CoRR, abs/1704.05021, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05021"
        },
        {
            "id": "5",
            "entry": "[5] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1709\u20131720. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alistarh%2C%20D.%20Grubic%2C%20D.%20Li%2C%20J.%20Tomioka%2C%20R.%20Qsgd%3A%20Communication-efficient%20sgd%20via%20gradient%20quantization%20and%20encoding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alistarh%2C%20D.%20Grubic%2C%20D.%20Li%2C%20J.%20Tomioka%2C%20R.%20Qsgd%3A%20Communication-efficient%20sgd%20via%20gradient%20quantization%20and%20encoding%202017"
        },
        {
            "id": "6",
            "entry": "[6] A. A. Awan, K. Hamidouche, J. M. Hashmi, and D. K. Panda. S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable Deep Learning on Modern GPU Clusters. In Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP \u201917, pages 193\u2013205, New York, NY, USA, 2017. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Awan%2C%20A.A.%20Hamidouche%2C%20K.%20Hashmi%2C%20J.M.%20Panda%2C%20D.K.%20S-Caffe%3A%20Co-designing%20MPI%20Runtimes%20and%20Caffe%20for%20Scalable%20Deep%20Learning%20on%20Modern%20GPU%20Clusters%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Awan%2C%20A.A.%20Hamidouche%2C%20K.%20Hashmi%2C%20J.M.%20Panda%2C%20D.K.%20S-Caffe%3A%20Co-designing%20MPI%20Runtimes%20and%20Caffe%20for%20Scalable%20Deep%20Learning%20on%20Modern%20GPU%20Clusters%202017"
        },
        {
            "id": "7",
            "entry": "[7] C.-Y. Chen, J. Choi, D. Brand, A. Agrawal, W. Zhang, and K. Gopalakrishnan. Adacomp : Adaptive residual gradient compression for data-parallel distributed training. CoRR, abs/1712.02679, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02679"
        },
        {
            "id": "8",
            "entry": "[8] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. CoRR, abs/1512.01274, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.01274"
        },
        {
            "id": "9",
            "entry": "[9] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project adam: Building an efficient and scalable deep learning training system. In Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI\u201914, pages 571\u2013582, Berkeley, CA, USA, 2014. USENIX Association.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chilimbi%2C%20T.%20Suzue%2C%20Y.%20Apacible%2C%20J.%20Kalyanaraman%2C%20K.%20Project%20adam%3A%20Building%20an%20efficient%20and%20scalable%20deep%20learning%20training%20system",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chilimbi%2C%20T.%20Suzue%2C%20Y.%20Apacible%2C%20J.%20Kalyanaraman%2C%20K.%20Project%20adam%3A%20Building%20an%20efficient%20and%20scalable%20deep%20learning%20training%20system"
        },
        {
            "id": "10",
            "entry": "[10] A. Coates, B. Huval, T. Wang, D. J. Wu, A. Y. Ng, and B. Catanzaro. Deep learning with cots hpc systems. In Proceedings of the 30th International Conference on International Conference on Machine Learning Volume 28, ICML\u201913, pages III\u20131337\u2013III\u20131345. JMLR.org, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20A.%20Huval%2C%20B.%20Wang%2C%20T.%20Wu%2C%20D.J.%20Deep%20learning%20with%20cots%20hpc%20systems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20A.%20Huval%2C%20B.%20Wang%2C%20T.%20Wu%2C%20D.J.%20Deep%20learning%20with%20cots%20hpc%20systems%202013"
        },
        {
            "id": "11",
            "entry": "[11] R. Collobert, S. Bengio, and J. Marithoz. Torch: A modular machine learning software library, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collobert%2C%20R.%20Bengio%2C%20S.%20Marithoz%2C%20J.%20Torch%3A%20A%20modular%20machine%20learning%20software%20library%202002"
        },
        {
            "id": "12",
            "entry": "[12] C. M. De Sa, C. Zhang, K. Olukotun, C. R\u00e9, and C. R\u00e9. Taming the wild: A unified analysis of hogwildstyle algorithms. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2674\u20132682. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20C.M.De%20Zhang%2C%20C.%20Olukotun%2C%20K.%20R%C3%A9%2C%20C.%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20hogwildstyle%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20C.M.De%20Zhang%2C%20C.%20Olukotun%2C%20K.%20R%C3%A9%2C%20C.%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20hogwildstyle%20algorithms%202015"
        },
        {
            "id": "13",
            "entry": "[13] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201912, pages 1223\u2013 1231, USA, 2012. Curran Associates Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20J.%20Corrado%2C%20G.S.%20Monga%2C%20R.%20Chen%2C%20K.%20Large%20scale%20distributed%20deep%20networks%201231",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20J.%20Corrado%2C%20G.S.%20Monga%2C%20R.%20Chen%2C%20K.%20Large%20scale%20distributed%20deep%20networks%201231"
        },
        {
            "id": "14",
            "entry": "[14] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches. J. Mach. Learn. Res., 13(1):165\u2013202, Jan. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20O.%20Gilad-Bachrach%2C%20R.%20Shamir%2C%20O.%20Xiao%2C%20L.%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20O.%20Gilad-Bachrach%2C%20R.%20Shamir%2C%20O.%20Xiao%2C%20L.%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012-01"
        },
        {
            "id": "15",
            "entry": "[15] L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1019\u20131028, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20L.%20Pascanu%2C%20R.%20Bengio%2C%20S.%20Bengio%2C%20Y.%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20L.%20Pascanu%2C%20R.%20Bengio%2C%20S.%20Bengio%2C%20Y.%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017-08"
        },
        {
            "id": "16",
            "entry": "[16] R. Garg and R. Khandekar. Gradient descent with sparsification: An iterative algorithm for sparse recovery with restricted isometry property. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 337\u2013344, New York, NY, USA, 2009. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garg%2C%20R.%20Khandekar%2C%20R.%20Gradient%20descent%20with%20sparsification%3A%20An%20iterative%20algorithm%20for%20sparse%20recovery%20with%20restricted%20isometry%20property%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garg%2C%20R.%20Khandekar%2C%20R.%20Gradient%20descent%20with%20sparsification%3A%20An%20iterative%20algorithm%20for%20sparse%20recovery%20with%20restricted%20isometry%20property%202009"
        },
        {
            "id": "17",
            "entry": "[17] S. Ghadimi and G. Lan. Stochastic firstand zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "18",
            "entry": "[18] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep learning with limited numerical precision. CoRR, abs/1502.02551, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.02551"
        },
        {
            "id": "19",
            "entry": "[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "20",
            "entry": "[20] Q. Ho, J. Cipar, H. Cui, J. K. Kim, S. Lee, P. B. Gibbons, G. A. Gibson, G. R. Ganger, and E. P. Xing. More effective distributed ml via a stale synchronous parallel parameter server. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201913, pages 1223\u20131231, USA, 2013. Curran Associates Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Q.%20Cipar%2C%20J.%20Cui%2C%20H.%20Kim%2C%20J.K.%20More%20effective%20distributed%20ml%20via%20a%20stale%20synchronous%20parallel%20parameter%20server%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Q.%20Cipar%2C%20J.%20Cui%2C%20H.%20Kim%2C%20J.K.%20More%20effective%20distributed%20ml%20via%20a%20stale%20synchronous%20parallel%20parameter%20server%202013"
        },
        {
            "id": "21",
            "entry": "[21] E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1731\u20131741. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20E.%20Hubara%2C%20I.%20Soudry%2C%20D.%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20E.%20Hubara%2C%20I.%20Soudry%2C%20D.%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "22",
            "entry": "[22] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22Nd ACM International Conference on Multimedia, MM \u201914, pages 675\u2013678, New York, NY, USA, 2014. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20Y.%20Shelhamer%2C%20E.%20Donahue%2C%20J.%20Karayev%2C%20S.%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20Y.%20Shelhamer%2C%20E.%20Donahue%2C%20J.%20Karayev%2C%20S.%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014"
        },
        {
            "id": "23",
            "entry": "[23] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "24",
            "entry": "[24] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "25",
            "entry": "[25] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436 EP \u2013, 05 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015"
        },
        {
            "id": "26",
            "entry": "[26] H. Li, S. De, Z. Xu, C. Studer, H. Samet, and T. Goldstein. Training quantized nets: A deeper understanding. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5811\u20135821. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20H.%20De%2C%20S.%20Xu%2C%20Z.%20Studer%2C%20C.%20Training%20quantized%20nets%3A%20A%20deeper%20understanding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20H.%20De%2C%20S.%20Xu%2C%20Z.%20Studer%2C%20C.%20Training%20quantized%20nets%3A%20A%20deeper%20understanding%202017"
        },
        {
            "id": "27",
            "entry": "[27] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling distributed machine learning with the parameter server. In Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI\u201914, pages 583\u2013598, Berkeley, CA, USA, 2014. USENIX Association.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20M.%20Andersen%2C%20D.G.%20Park%2C%20J.W.%20Smola%2C%20A.J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20M.%20Andersen%2C%20D.G.%20Park%2C%20J.W.%20Smola%2C%20A.J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server"
        },
        {
            "id": "28",
            "entry": "[28] M. Li, D. G. Andersen, A. J. Smola, and K. Yu. Communication efficient distributed machine learning with the parameter server. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 19\u201327. Curran Associates, Inc., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20M.%20Andersen%2C%20D.G.%20Smola%2C%20A.J.%20Yu%2C%20K.%20Communication%20efficient%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20M.%20Andersen%2C%20D.G.%20Smola%2C%20A.J.%20Yu%2C%20K.%20Communication%20efficient%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "29",
            "entry": "[29] M. Li, J. Dean, B. P\u00f3czos, R. Salakhutdinov, and A. J. Smola. Scaling distributed machine learning with system and algorithm co-design. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20M.%20Dean%2C%20J.%20P%C3%B3czos%2C%20B.%20Salakhutdinov%2C%20R.%20Scaling%20distributed%20machine%20learning%20with%20system%20and%20algorithm%20co-design%202016"
        },
        {
            "id": "30",
            "entry": "[30] X. Lian, Y. Huang, Y. Li, and J. Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915, pages 2737\u20132745, Cambridge, MA, USA, 2015. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Huang%2C%20Y.%20Li%2C%20Y.%20Liu%2C%20J.%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20X.%20Huang%2C%20Y.%20Li%2C%20Y.%20Liu%2C%20J.%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015"
        },
        {
            "id": "31",
            "entry": "[31] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5330\u20135340. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Zhang%2C%20C.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Can%20decentralized%20algorithms%20outperform%20centralized%20algorithms%3F%20a%20case%20study%20for%20decentralized%20parallel%20stochastic%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20X.%20Zhang%2C%20C.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Can%20decentralized%20algorithms%20outperform%20centralized%20algorithms%3F%20a%20case%20study%20for%20decentralized%20parallel%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "32",
            "entry": "[32] X. Lian, H. Zhang, C.-J. Hsieh, Y. Huang, and J. Liu. A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order. In Advances in Neural Information Processing Systems 29, pages 3054\u20133062. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Huang%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20X.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Huang%2C%20Y.%20A%20comprehensive%20linear%20speedup%20analysis%20for%20asynchronous%20stochastic%20parallel%20optimization%20from%20zeroth-order%20to%20first-order%202016"
        },
        {
            "id": "33",
            "entry": "[33] Y. Lin, S. Han, H. Mao, Y. Wang, and B. Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Y.%20Han%2C%20S.%20Mao%2C%20H.%20Wang%2C%20Y.%20Deep%20gradient%20compression%3A%20Reducing%20the%20communication%20bandwidth%20for%20distributed%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Y.%20Han%2C%20S.%20Mao%2C%20H.%20Wang%2C%20Y.%20Deep%20gradient%20compression%3A%20Reducing%20the%20communication%20bandwidth%20for%20distributed%20training%202018"
        },
        {
            "id": "34",
            "entry": "[34] D. Povey, X. Zhang, and S. Khudanpur. Parallel training of deep neural networks with natural gradient and parameter averaging. CoRR, abs/1410.7455, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.7455"
        },
        {
            "id": "35",
            "entry": "[35] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 693\u2013701. Curran Associates, Inc., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20B.%20Re%2C%20C.%20Wright%2C%20S.%20Niu%2C%20F.%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20B.%20Re%2C%20C.%20Wright%2C%20S.%20Niu%2C%20F.%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "36",
            "entry": "[36] F. Seide and A. Agarwal. Cntk: Microsoft\u2019s open-source deep-learning toolkit. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pages 2135\u20132135, New York, NY, USA, 2016. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seide%2C%20F.%20Agarwal%2C%20A.%20Cntk%3A%20Microsoft%E2%80%99s%20open-source%20deep-learning%20toolkit%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seide%2C%20F.%20Agarwal%2C%20A.%20Cntk%3A%20Microsoft%E2%80%99s%20open-source%20deep-learning%20toolkit%202016"
        },
        {
            "id": "37",
            "entry": "[37] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns, September 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seide%2C%20F.%20Fu%2C%20H.%20Droppo%2C%20J.%20Li%2C%20G.%201-bit%20stochastic%20gradient%20descent%20and%20application%20to%20data-parallel%20distributed%20training%20of%20speech%20dnns%202014-09"
        },
        {
            "id": "38",
            "entry": "[38] R. Shi, Y. Gan, and Y. Wang. Evaluating scalability bottlenecks by workload extrapolation. In 2018 IEEE 26th International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS), Milwaukee, WI, USA, Sept 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20R.%20Gan%2C%20Y.%20Wang%2C%20Y.%20Evaluating%20scalability%20bottlenecks%20by%20workload%20extrapolation%202018-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20R.%20Gan%2C%20Y.%20Wang%2C%20Y.%20Evaluating%20scalability%20bottlenecks%20by%20workload%20extrapolation%202018-09"
        },
        {
            "id": "39",
            "entry": "[39] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "40",
            "entry": "[40] S. L. Smith, P.-J. Kindermans, and Q. V. Le. Don\u2019t decay the learning rate, increase the batch size. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20S.L.%20Kindermans%2C%20P.-J.%20Le%2C%20Q.V.%20Don%E2%80%99t%20decay%20the%20learning%20rate%2C%20increase%20the%20batch%20size%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20S.L.%20Kindermans%2C%20P.-J.%20Le%2C%20Q.V.%20Don%E2%80%99t%20decay%20the%20learning%20rate%2C%20increase%20the%20batch%20size%202018"
        },
        {
            "id": "41",
            "entry": "[41] S. L. Smith and Q. V. Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20S.L.%20Le%2C%20Q.V.%20A%20bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20S.L.%20Le%2C%20Q.V.%20A%20bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "42",
            "entry": "[42] N. Strom. Scalable distributed dnn training using commodity gpu cloud computing. In INTERSPEECH, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strom%2C%20N.%20Scalable%20distributed%20dnn%20training%20using%20commodity%20gpu%20cloud%20computing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strom%2C%20N.%20Scalable%20distributed%20dnn%20training%20using%20commodity%20gpu%20cloud%20computing%202015"
        },
        {
            "id": "43",
            "entry": "[43] H. Su and H. Chen. Experiments on parallel training of deep neural network using model averaging. CoRR, abs/1507.01239, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.01239"
        },
        {
            "id": "44",
            "entry": "[44] L. Wang, J. Ye, Y. Zhao, W. Wu, A. Li, S. L. Song, Z. Xu, and T. Kraska. Superneurons: Dynamic gpu memory management for training deep neural networks. In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP \u201918, pages 41\u201353, New York, NY, USA, 2018. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20L.%20Ye%2C%20J.%20Zhao%2C%20Y.%20Wu%2C%20W.%20Superneurons%3A%20Dynamic%20gpu%20memory%20management%20for%20training%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20L.%20Ye%2C%20J.%20Zhao%2C%20Y.%20Wu%2C%20W.%20Superneurons%3A%20Dynamic%20gpu%20memory%20management%20for%20training%20deep%20neural%20networks%202018"
        },
        {
            "id": "45",
            "entry": "[45] J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for communication-efficient distributed optimization. CoRR, abs/1710.09854, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09854"
        },
        {
            "id": "46",
            "entry": "[46] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1509\u20131519. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20W.%20Xu%2C%20C.%20Yan%2C%20F.%20Wu%2C%20C.%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20W.%20Xu%2C%20C.%20Yan%2C%20F.%20Wu%2C%20C.%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017"
        },
        {
            "id": "47",
            "entry": "[47] S. Zhang, A. Choromanska, and Y. LeCun. Deep learning with elastic averaging sgd. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201915, pages 685\u2013693, Cambridge, MA, USA, 2015. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20S.%20Choromanska%2C%20A.%20LeCun%2C%20Y.%20Deep%20learning%20with%20elastic%20averaging%20sgd%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20S.%20Choromanska%2C%20A.%20LeCun%2C%20Y.%20Deep%20learning%20with%20elastic%20averaging%20sgd%202015"
        },
        {
            "id": "48",
            "entry": "[48] S. Zhou, Z. Ni, X. Zhou, H. Wen, Y. Wu, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. CoRR, abs/1606.06160, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1606.06160"
        }
    ]
}
