{
    "filename": "8173-multi-agent-reinforcement-learning-via-double-averaging-primal-dual-optimization.pdf",
    "metadata": {
        "title": "Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization",
        "author": "Hoi-To Wai, Zhuoran Yang, Princeton Zhaoran Wang, Mingyi Hong",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8173-multi-agent-reinforcement-learning-via-double-averaging-primal-dual-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Despite the success of single-agent reinforcement learning, multi-agent reinforcement learning (MARL) remains challenging due to complex interactions between agents. Motivated by decentralized applications such as sensor networks, swarm robotics, and power grids, we study policy evaluation in MARL, where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy. In this paper, we propose a double averaging scheme, where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information, respectively. We prove that the proposed algorithm converges to the optimal solution at a global geometric rate. In particular, such an algorithm is built upon a primal-dual reformulation of the mean squared projected Bellman error minimization problem, which gives rise to a decentralized convex-concave saddle-point problem. To the best of our knowledge, the proposed double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "MARL",
            "url": "https://en.wikipedia.org/wiki/Marl"
        }
    ],
    "highlights": [
        "Reinforcement learning combined with deep neural networks recently achieves superhuman performance on various challenging tasks such as video games and board games [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]",
        "We study collaborative multi-agent reinforcement learning with local rewards",
        "To make multi-agent reinforcement learning more scalable and robust, we propose a decentralized scheme for exchanging local information, where each agent only communicates with its neighbors over a network",
        "In the single-agent setting, our algorithm is closely related to stochastic average gradient (SAG) [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] and stochastic incremental gradient (SAGA) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], with the difference that our objective function is a finite sum convex-concave saddle-point problem",
        "We introduce the background of multi-agent reinforcement learning, which is modeled as a multi-agent Markov decision process (MDP)",
        "Conclusion In this paper, we have studied the policy evaluation problem in multi-agent reinforcement learning"
    ],
    "key_statements": [
        "Reinforcement learning combined with deep neural networks recently achieves superhuman performance on various challenging tasks such as video games and board games [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]",
        "We study collaborative multi-agent reinforcement learning with local rewards",
        "To collaboratively make globally optimal decisions, the agents need to exchange local information. Such a setting of multi-agent reinforcement learning is ubiquitous in large-scale applications such as sensor networks [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], swarm robotics [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], and power grids [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "To make multi-agent reinforcement learning more scalable and robust, we propose a decentralized scheme for exchanging local information, where each agent only communicates with its neighbors over a network",
        "In the single-agent setting, our algorithm is closely related to stochastic average gradient (SAG) [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] and stochastic incremental gradient (SAGA) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], with the difference that our objective function is a finite sum convex-concave saddle-point problem",
        "Organization In \u00a72 we introduce the problem formulation of multi-agent reinforcement learning",
        "We introduce the background of multi-agent reinforcement learning, which is modeled as a multi-agent Markov decision process (MDP)",
        "For any given joint policy \u03c0, the value function of \u03c0, denoted by V \u03c0 : S \u2192 R, is defined as the expected value of the discounted cumulative reward when the multi-agent Markov decision process is initialized with a given state and the agents follows policy \u03c0 afterwards",
        "Conclusion In this paper, we have studied the policy evaluation problem in multi-agent reinforcement learning"
    ],
    "summary": [
        "Reinforcement learning combined with deep neural networks recently achieves superhuman performance on various challenging tasks such as video games and board games [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>].",
        "In the single-agent setting, our algorithm is closely related to stochastic average gradient (SAG) [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] and stochastic incremental gradient (SAGA) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], with the difference that our objective function is a finite sum convex-concave saddle-point problem.",
        "Contribution Our contribution is threefold: (i) We reformulate the multi-agent policy evaluation problem using Fenchel duality and propose a decentralized primal-dual optimization algorithm with a double averaging update scheme.",
        "For any given joint policy \u03c0, the value function of \u03c0, denoted by V \u03c0 : S \u2192 R, is defined as the expected value of the discounted cumulative reward when the multi-agent MDP is initialized with a given state and the agents follows policy \u03c0 afterwards.",
        "Multi-agent, Primal-dual, Finite-sum Optimization Recall that under the multi-agent MDP, the agents are able to observe the states and the joint actions, but can only observe their local rewards.",
        "Algorithm 1 PD-DistIAG Method for Multi-agent, Primal-dual, Finite-sum Optimization",
        "Communication Overhead The PD-DistIAG method described in Algorithm 1 requires an information exchange round [of sti and \u03b8it] among the agents at every iteration.",
        "The PD-DistIAG method is built using the techniques of (a) primal-dual batch gradient descent, (b) gradient tracking for distributed optimization and (c) stochastic average gradient, where each of them has been independently shown to attain linear convergence under certain conditions; see [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "Extension Our analysis and algorithm may be applied to solve general problems that involves multi-agent and finite-sum optimization, e.g., min\u03b8\u2208Rd",
        "Especially with convergence for inequality systems of the form (20), can be applied to study a similar double averaging algorithm with just the primal variable.",
        "To verify the performance of our proposed method, we conduct an experiment on the mountaincar dataset [<a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>] under a setting similar to [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] \u2013 to collect the dataset, we ran Sarsa with d = 300 features to obtain the policy, we generate the trajectories of actions and states according to the policy with M samples.",
        "We generate the local reward, Ri by assigning a random portion for the reward to each agent such that the average of the local rewards equals Rc. We compare our method to several centralized methods \u2013 PDBG is the primal-dual gradient descent method in (11), GTD2 [<a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>], and SAGA [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "Utilizing Fenchel duality, a double averaging scheme is proposed to tackle the primal-dual, multi-agent, and finite-sum optimization arises.",
        "Future work includes characterizing the impact of different sampling schemes in the distributed algorithm"
    ],
    "headline": "Motivated by decentralized applications such as sensor networks, swarm robotics, and power grids, we study policy evaluation in multi-agent reinforcement learning, where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] G. Arslan and S. Y\u00fcksel. Decentralized Q-learning for stochastic teams and games. IEEE Transactions on Automatic Control, 62(4):1545\u20131558, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arslan%2C%20G.%20Y%C3%BCksel%2C%20S.%20Decentralized%20Q-learning%20for%20stochastic%20teams%20and%20games%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arslan%2C%20G.%20Y%C3%BCksel%2C%20S.%20Decentralized%20Q-learning%20for%20stochastic%20teams%20and%20games%202017"
        },
        {
            "id": "2",
            "entry": "[2] V. S. Borkar. Stochastic approximation: A dynamical systems viewpoint. Cambridge University Press, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borkar%2C%20V.S.%20Stochastic%20approximation%3A%20A%20dynamical%20systems%20viewpoint%202008"
        },
        {
            "id": "3",
            "entry": "[3] D. S. Callaway and I. A. Hiskens. Achieving controllability of electric loads. Proceedings of the IEEE, 99(1):184\u2013199, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Callaway%2C%20D.S.%20Hiskens%2C%20I.A.%20Achieving%20controllability%20of%20electric%20loads%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Callaway%2C%20D.S.%20Hiskens%2C%20I.A.%20Achieving%20controllability%20of%20electric%20loads%202011"
        },
        {
            "id": "4",
            "entry": "[4] A. Chambolle and T. Pock. On the ergodic convergence rates of a first-order primal\u2013dual algorithm. Mathematical Programming, 159(1-2):253\u2013287, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20A.%20Pock%2C%20T.%20On%20the%20ergodic%20convergence%20rates%20of%20a%20first-order%20primal%E2%80%93dual%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20A.%20Pock%2C%20T.%20On%20the%20ergodic%20convergence%20rates%20of%20a%20first-order%20primal%E2%80%93dual%20algorithm%202016"
        },
        {
            "id": "5",
            "entry": "[5] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(Mar):1069\u20131109, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20K.%20Monteleoni%2C%20C.%20Sarwate%2C%20A.D.%20Differentially%20private%20empirical%20risk%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20K.%20Monteleoni%2C%20C.%20Sarwate%2C%20A.D.%20Differentially%20private%20empirical%20risk%20minimization%202011"
        },
        {
            "id": "6",
            "entry": "[6] J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and learning over networks. IEEE Transactions on Signal Processing, 60(8):4289\u20134305, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20J.%20Sayed%2C%20A.H.%20Diffusion%20adaptation%20strategies%20for%20distributed%20optimization%20and%20learning%20over%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20J.%20Sayed%2C%20A.H.%20Diffusion%20adaptation%20strategies%20for%20distributed%20optimization%20and%20learning%20over%20networks%202012"
        },
        {
            "id": "7",
            "entry": "[7] Y. Chen and M. Wang. Stochastic primal-dual methods and sample complexity of reinforcement learning. arXiv preprint arXiv:1612.02516, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.02516"
        },
        {
            "id": "8",
            "entry": "[8] P. Corke, R. Peterson, and D. Rus. Networked robots: Flying robot navigation using a sensor net. Robotics Research, pages 234\u2013243, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Corke%2C%20P.%20Peterson%2C%20R.%20Rus%2C%20D.%20Networked%20robots%3A%20Flying%20robot%20navigation%20using%20a%20sensor%20net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Corke%2C%20P.%20Peterson%2C%20R.%20Rus%2C%20D.%20Networked%20robots%3A%20Flying%20robot%20navigation%20using%20a%20sensor%20net%202005"
        },
        {
            "id": "9",
            "entry": "[9] J. Cortes, S. Martinez, T. Karatas, and F. Bullo. Coverage control for mobile sensing networks. IEEE Transactions on Robotics and Automation, 20(2):243\u2013255, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20J.%20Martinez%2C%20S.%20Karatas%2C%20T.%20Bullo%2C%20F.%20Coverage%20control%20for%20mobile%20sensing%20networks%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20J.%20Martinez%2C%20S.%20Karatas%2C%20T.%20Bullo%2C%20F.%20Coverage%20control%20for%20mobile%20sensing%20networks%202004"
        },
        {
            "id": "10",
            "entry": "[10] B. Dai, N. He, Y. Pan, B. Boots, and L. Song. Learning from conditional distributions via dual embeddings. arXiv preprint arXiv:1607.04579, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04579"
        },
        {
            "id": "11",
            "entry": "[11] B. Dai, A. Shaw, N. He, L. Li, and L. Song. Boosting the actor with dual critic. arXiv preprint arXiv:1712.10282, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.10282"
        },
        {
            "id": "12",
            "entry": "[12] B. Dai, A. Shaw, L. Li, L. Xiao, N. He, J. Chen, and L. Song. Smoothed dual embedding control. arXiv preprint arXiv:1712.10285, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.10285"
        },
        {
            "id": "13",
            "entry": "[13] E. Dall\u2019Anese, H. Zhu, and G. B. Giannakis. Distributed optimal power flow for smart microgrids. IEEE Transactions on Smart Grid, 4(3):1464\u20131475, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dall%E2%80%99Anese%2C%20E.%20Zhu%2C%20H.%20Giannakis%2C%20G.B.%20Distributed%20optimal%20power%20flow%20for%20smart%20microgrids%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dall%E2%80%99Anese%2C%20E.%20Zhu%2C%20H.%20Giannakis%2C%20G.B.%20Distributed%20optimal%20power%20flow%20for%20smart%20microgrids%202013"
        },
        {
            "id": "14",
            "entry": "[14] A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "15",
            "entry": "[15] S. S. Du, J. Chen, L. Li, L. Xiao, and D. Zhou. Stochastic variance reduction methods for policy evaluation. arXiv preprint arXiv:1702.07944, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07944"
        },
        {
            "id": "16",
            "entry": "[16] H. R. Feyzmahdavian, A. Aytekin, and M. Johansson. A delayed proximal gradient method with linear convergence rate. In Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on, pages 1\u20136. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feyzmahdavian%2C%20H.R.%20Aytekin%2C%20A.%20Johansson%2C%20M.%20A%20delayed%20proximal%20gradient%20method%20with%20linear%20convergence%20rate%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feyzmahdavian%2C%20H.R.%20Aytekin%2C%20A.%20Johansson%2C%20M.%20A%20delayed%20proximal%20gradient%20method%20with%20linear%20convergence%20rate%202014"
        },
        {
            "id": "17",
            "entry": "[17] J. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, pages 2137\u20132145, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20J.%20Assael%2C%20Y.M.%20de%20Freitas%2C%20N.%20Whiteson%2C%20S.%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20J.%20Assael%2C%20Y.M.%20de%20Freitas%2C%20N.%20Whiteson%2C%20S.%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016"
        },
        {
            "id": "18",
            "entry": "[18] J. Foerster, N. Nardelli, G. Farquhar, P. Torr, P. Kohli, S. Whiteson, et al. Stabilising experience replay for deep multi-agent reinforcement learning. arXiv preprint arXiv:1702.08887, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08887"
        },
        {
            "id": "19",
            "entry": "[19] J. K. Gupta, M. Egorov, and M. Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multi-agent Systems, pages 66\u201383, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20J.K.%20Egorov%2C%20M.%20Kochenderfer%2C%20M.%20Cooperative%20multi-agent%20control%20using%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20J.K.%20Egorov%2C%20M.%20Kochenderfer%2C%20M.%20Cooperative%20multi-agent%20control%20using%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] M. Gurbuzbalaban, A. Ozdaglar, and P. A. Parrilo. On the convergence rate of incremental aggregated gradient algorithms. SIAM Journal on Optimization, 27(2):1035\u20131048, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gurbuzbalaban%2C%20M.%20Ozdaglar%2C%20A.%20Parrilo%2C%20P.A.%20On%20the%20convergence%20rate%20of%20incremental%20aggregated%20gradient%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gurbuzbalaban%2C%20M.%20Ozdaglar%2C%20A.%20Parrilo%2C%20P.A.%20On%20the%20convergence%20rate%20of%20incremental%20aggregated%20gradient%20algorithms%202017"
        },
        {
            "id": "21",
            "entry": "[21] J. Hu and M. P. Wellman. Nash Q-learning for general-sum stochastic games. Journal of Machine Learning Research, 4(Nov):1039\u20131069, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20J.%20Wellman%2C%20M.P.%20Nash%20Q-learning%20for%20general-sum%20stochastic%20games%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20J.%20Wellman%2C%20M.P.%20Nash%20Q-learning%20for%20general-sum%20stochastic%20games%202003"
        },
        {
            "id": "22",
            "entry": "[22] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "23",
            "entry": "[23] J. Kober and J. Peters. Reinforcement learning in robotics: A survey. In Reinforcement Learning, pages 579\u2013610.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20J.%20Peters%2C%20J.%20Reinforcement%20learning%20in%20robotics%3A%20A%20survey",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20J.%20Peters%2C%20J.%20Reinforcement%20learning%20in%20robotics%3A%20A%20survey"
        },
        {
            "id": "24",
            "entry": "[24] M. Lauer and M. Riedmiller. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In International Conference on Machine Learning, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lauer%2C%20M.%20Riedmiller%2C%20M.%20An%20algorithm%20for%20distributed%20reinforcement%20learning%20in%20cooperative%20multi-agent%20systems%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lauer%2C%20M.%20Riedmiller%2C%20M.%20An%20algorithm%20for%20distributed%20reinforcement%20learning%20in%20cooperative%20multi-agent%20systems%202000"
        },
        {
            "id": "25",
            "entry": "[25] D. Lee, H. Yoon, and N. Hovakimyan. Primal-dual algorithm for distributed reinforcement learning: Distributed gtd2. arXiv preprint arXiv:1803.08031, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08031"
        },
        {
            "id": "26",
            "entry": "[26] X. Lian, M. Wang, and J. Liu. Finite-sum composition optimization via variance reduced gradient descent. arXiv preprint arXiv:1610.04674, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04674"
        },
        {
            "id": "27",
            "entry": "[27] A. Lin and Q. Ling. Decentralized and privacy-preserving low-rank matrix completion. 2014. Preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20A.%20Ling%2C%20Q.%20Decentralized%20and%20privacy-preserving%20low-rank%20matrix%20completion%202014"
        },
        {
            "id": "28",
            "entry": "[28] M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In International Conference on Machine Learning, pages 157\u2013163, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20M.L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20M.L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994"
        },
        {
            "id": "29",
            "entry": "[29] M. L. Littman. Value-function reinforcement learning in Markov games. Cognitive Systems Research, 2(1):55\u201366, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20M.L.%20Value-function%20reinforcement%20learning%20in%20Markov%20games%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20M.L.%20Value-function%20reinforcement%20learning%20in%20Markov%20games%202001"
        },
        {
            "id": "30",
            "entry": "[30] B. Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik. Finite-sample analysis of proximal gradient td algorithms. In UAI, pages 504\u2013513, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20B.%20Liu%2C%20J.%20Ghavamzadeh%2C%20M.%20Mahadevan%2C%20S.%20Finite-sample%20analysis%20of%20proximal%20gradient%20td%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20B.%20Liu%2C%20J.%20Ghavamzadeh%2C%20M.%20Mahadevan%2C%20S.%20Finite-sample%20analysis%20of%20proximal%20gradient%20td%20algorithms%202015"
        },
        {
            "id": "31",
            "entry": "[31] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02275"
        },
        {
            "id": "32",
            "entry": "[32] S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed. Distributed policy evaluation under multiple behavior strategies. IEEE Transactions on Automatic Control, 60(5):1260\u20131274, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Macua%2C%20S.V.%20Chen%2C%20J.%20Zazo%2C%20S.%20Sayed%2C%20A.H.%20Distributed%20policy%20evaluation%20under%20multiple%20behavior%20strategies%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Macua%2C%20S.V.%20Chen%2C%20J.%20Zazo%2C%20S.%20Sayed%2C%20A.H.%20Distributed%20policy%20evaluation%20under%20multiple%20behavior%20strategies%202015"
        },
        {
            "id": "33",
            "entry": "[33] S. V. Macua, A. Tukiainen, D. G.-O. Hern\u00e1ndez, D. Baldazo, E. M. de Cote, and S. Zazo. Diff-dac: Distributed actor-critic for multitask deep reinforcement learning. arXiv preprint arXiv:1710.10363, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10363"
        },
        {
            "id": "34",
            "entry": "[34] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "35",
            "entry": "[35] A. Nedicand D. P. Bertsekas. Least squares policy evaluation algorithms with linear function approximation. Discrete Event Dynamic Systems, 13(1-2):79\u2013110, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20A.Nedicand%20D.P.%20Least%20squares%20policy%20evaluation%20algorithms%20with%20linear%20function%20approximation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20A.Nedicand%20D.P.%20Least%20squares%20policy%20evaluation%20algorithms%20with%20linear%20function%20approximation%202003"
        },
        {
            "id": "36",
            "entry": "[36] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48\u201361, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20A.%20Ozdaglar%2C%20A.%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20A.%20Ozdaglar%2C%20A.%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009"
        },
        {
            "id": "37",
            "entry": "[37] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In International Conference on Machine Learning, pages 2681\u20132690, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omidshafiei%2C%20S.%20Pazis%2C%20J.%20Amato%2C%20C.%20How%2C%20J.P.%20Deep%20decentralized%20multi-task%20multi-agent%20reinforcement%20learning%20under%20partial%20observability%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Omidshafiei%2C%20S.%20Pazis%2C%20J.%20Amato%2C%20C.%20How%2C%20J.P.%20Deep%20decentralized%20multi-task%20multi-agent%20reinforcement%20learning%20under%20partial%20observability%202017"
        },
        {
            "id": "38",
            "entry": "[38] B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In Advances in Neural Information Processing Systems, pages 1416\u20131424, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Palaniappan%2C%20B.%20Bach%2C%20F.%20Stochastic%20variance%20reduction%20methods%20for%20saddle-point%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Palaniappan%2C%20B.%20Bach%2C%20F.%20Stochastic%20variance%20reduction%20methods%20for%20saddle-point%20problems%202016"
        },
        {
            "id": "39",
            "entry": "[39] E. Parisotto, J. L. Ba, and R. Salakhutdinov. Actor-mimic: Deep multi-task and transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06342"
        },
        {
            "id": "40",
            "entry": "[40] S. Pu and A. Nedic. Distributed stochastic gradient tracking methods. arXiv preprint arXiv:1805.11454, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11454"
        },
        {
            "id": "41",
            "entry": "[41] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20G.%20Li%2C%20N.%20Harnessing%20smoothness%20to%20accelerate%20distributed%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20G.%20Li%2C%20N.%20Harnessing%20smoothness%20to%20accelerate%20distributed%20optimization%202017"
        },
        {
            "id": "42",
            "entry": "[42] M. Rabbat and R. Nowak. Distributed optimization in sensor networks. In International Symposium on Information Processing in Sensor Networks, pages 20\u201327, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Rabbat%20and%20R%20Nowak%20Distributed%20optimization%20in%20sensor%20networks%20In%20International%20Symposium%20on%20Information%20Processing%20in%20Sensor%20Networks%20pages%202027%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Rabbat%20and%20R%20Nowak%20Distributed%20optimization%20in%20sensor%20networks%20In%20International%20Symposium%20on%20Information%20Processing%20in%20Sensor%20Networks%20pages%202027%202004"
        },
        {
            "id": "43",
            "entry": "[43] M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83\u2013112, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017"
        },
        {
            "id": "44",
            "entry": "[44] W. Shi, Q. Ling, G. Wu, and W. Yin. Extra: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944\u2013966, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20W.%20Ling%2C%20Q.%20Wu%2C%20G.%20Yin%2C%20W.%20Extra%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20W.%20Ling%2C%20Q.%20Wu%2C%20G.%20Yin%2C%20W.%20Extra%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015"
        },
        {
            "id": "45",
            "entry": "[45] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550 (7676):354\u2013359, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20Go%20without%20human%20knowledge%207676",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20Go%20without%20human%20knowledge%207676"
        },
        {
            "id": "46",
            "entry": "[46] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "47",
            "entry": "[47] R. S. Sutton, H. R. Maei, and C. Szepesv\u00e1ri. A convergent o(n) temporal-difference algorithm for off-policy learning with linear function approximation. In Advances in neural information processing systems, pages 1609\u20131616, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Maei%2C%20H.R.%20Szepesv%C3%A1ri%2C%20C.%20A%20convergent%20o%28n%29%20temporal-difference%20algorithm%20for%20off-policy%20learning%20with%20linear%20function%20approximation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Maei%2C%20H.R.%20Szepesv%C3%A1ri%2C%20C.%20A%20convergent%20o%28n%29%20temporal-difference%20algorithm%20for%20off-policy%20learning%20with%20linear%20function%20approximation%202009"
        },
        {
            "id": "48",
            "entry": "[48] Y. W. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pascanu. Distral: Robust multi-task reinforcement learning. arXiv preprint arXiv:1707.04175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04175"
        },
        {
            "id": "49",
            "entry": "[49] J. Tsitsiklis, D. Bertsekas, and M. Athans. Distributed asynchronous deterministic and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control, 31(9):803\u2013812, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20J.%20Bertsekas%2C%20D.%20Athans%2C%20M.%20Distributed%20asynchronous%20deterministic%20and%20stochastic%20gradient%20optimization%20algorithms%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20J.%20Bertsekas%2C%20D.%20Athans%2C%20M.%20Distributed%20asynchronous%20deterministic%20and%20stochastic%20gradient%20optimization%20algorithms%201986"
        },
        {
            "id": "50",
            "entry": "[50] M. Wang. Primal-dual \u03c0 learning: Sample complexity and sublinear run time for ergodic markov decision problems. arXiv preprint arXiv:1710.06100, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06100"
        },
        {
            "id": "51",
            "entry": "[51] X. Wang and T. Sandholm. Reinforcement learning to play an optimal Nash equilibrium in team Markov games. In Advances in Neural Information Processing Systems, pages 1603\u20131610, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Sandholm%2C%20T.%20Reinforcement%20learning%20to%20play%20an%20optimal%20Nash%20equilibrium%20in%20team%20Markov%20games%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Sandholm%2C%20T.%20Reinforcement%20learning%20to%20play%20an%20optimal%20Nash%20equilibrium%20in%20team%20Markov%20games%202003"
        },
        {
            "id": "52",
            "entry": "[52] A. Wilson, A. Fern, S. Ray, and P. Tadepalli. Multi-task reinforcement learning: A hierarchical Bayesian approach. In International Conference on Machine Learning, pages 1015\u20131022, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.%20Fern%2C%20A.%20Ray%2C%20S.%20Tadepalli%2C%20P.%20Multi-task%20reinforcement%20learning%3A%20A%20hierarchical%20Bayesian%20approach%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.%20Fern%2C%20A.%20Ray%2C%20S.%20Tadepalli%2C%20P.%20Multi-task%20reinforcement%20learning%3A%20A%20hierarchical%20Bayesian%20approach%202007"
        },
        {
            "id": "53",
            "entry": "[53] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar. Fully decentralized multi-agent reinforcement learning with networked agents. arXiv preprint arXiv:1802.08757, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08757"
        },
        {
            "id": "54",
            "entry": "[54] M. Zhu and S. Mart\u00ednez. Discrete-time dynamic average consensus. Automatica, 46(2):322\u2013329, 2010. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20M.%20Mart%C3%ADnez%2C%20S.%20Discrete-time%20dynamic%20average%20consensus%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20M.%20Mart%C3%ADnez%2C%20S.%20Discrete-time%20dynamic%20average%20consensus%202010"
        }
    ]
}
