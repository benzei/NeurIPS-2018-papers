{
    "filename": "7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.pdf",
    "metadata": {
        "date": 2018,
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
        "author": "Kurtland Chua",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g. 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task)."
    },
    "keywords": [
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Gaussian processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_processes"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "dynamic model",
            "url": "https://en.wikipedia.org/wiki/dynamic_model"
        },
        {
            "term": "Deep deterministic policy gradient",
            "url": "https://en.wikipedia.org/wiki/Deep_deterministic_policy_gradient"
        },
        {
            "term": "bayesian neural network",
            "url": "https://en.wikipedia.org/wiki/bayesian_neural_network"
        },
        {
            "term": "MBRL",
            "url": "https://en.wikipedia.org/wiki/MBRL"
        },
        {
            "term": "model predictive control",
            "url": "https://en.wikipedia.org/wiki/model_predictive_control"
        }
    ],
    "highlights": [
        "Reinforcement learning (RL) algorithms provide for an automated framework for decision making and control: by specifying a high-level objective function, an Reinforcement learning algorithm can, in principle, automatically learn a control policy that satisfies this objective",
        "We study how expressive neural network can be incorporated into model-based reinforcement learning",
        "Our experiments suggest several conclusions that are relevant for further investigation in model-based reinforcement learning",
        "Our results show that model-based reinforcement learning with neural network dynamics models can achieve results that are competitive not only with Bayesian nonparametric models such as Gaussian processes, but on par with model-free algorithms such as policy optimization and Soft actor critic in terms of asymptotic performance, while attaining substantially more efficient convergence",
        "The individual components of our model-based reinforcement learning algorithms are not individually new \u2013 prior works have suggested both ensembling and outputting Gaussian distribution parameters [<a class=\"ref-link\" id=\"cLakshminarayanan_et+al_2017_a\" href=\"#rLakshminarayanan_et+al_2017_a\">Lakshminarayanan et al, 2017</a>], as well as the use of model predictive control for model-based Reinforcement learning [<a class=\"ref-link\" id=\"cNagabandi_et+al_2017_a\" href=\"#rNagabandi_et+al_2017_a\">Nagabandi et al, 2017</a>] \u2013 the particular combination of these components into a model-based reinforcement learning algorithm is, to our knowledge, novel, and the results provide a new state-of-the-art for model-based reinforcement learning algorithms based on high-capacity parametric models such as neural networks",
        "The systematic investigation in our experiments was a critical ingredient in determining the precise combination of these components that attains the best performance"
    ],
    "key_statements": [
        "Reinforcement learning (RL) algorithms provide for an automated framework for decision making and control: by specifying a high-level objective function, an Reinforcement learning algorithm can, in principle, automatically learn a control policy that satisfies this objective",
        "A promising direction for reducing sample complexity is to explore model-based reinforcement learning (MBRL) methods, which proceed by first acquiring a predictive model of the world, and using that model to make decisions [<a class=\"ref-link\" id=\"cAtkeson_1997_a\" href=\"#rAtkeson_1997_a\">Atkeson and Santamar\u00eda, 1997</a>, <a class=\"ref-link\" id=\"cKocijan_et+al_2004_a\" href=\"#rKocijan_et+al_2004_a\">Kocijan et al, 2004</a>, <a class=\"ref-link\" id=\"cDeisenroth_et+al_2014_a\" href=\"#rDeisenroth_et+al_2014_a\">Deisenroth et al, 2014</a>]",
        "The asymptotic performance of model-based reinforcement learning methods on common benchmark tasks generally lags behind model-free methods",
        "We study how expressive neural network can be incorporated into model-based reinforcement learning",
        "We evaluate the performance of our proposed model-based reinforcement learning algorithm called probabilistic ensembles with trajectory sampling using a deep neural network probabilistic dynamics model",
        "Gaussian process dynamics model (GP): we compare against three model-based reinforcement learning algorithms based on Gaussian processes",
        "The quality of the model and the use of uncertainty at learning time significantly affect the performance of the model-based reinforcement learning algorithms tested, while the use of more advanced uncertainty propagation techniques seem to offers only minor improvements",
        "As our experiments here and in the previous section show, the particular choice of these components in our algorithm achieves substantially improved results over previous state-of-the-art model-based and model-free methods, experimentally confirming both the importance of uncertainty estimation in model-based reinforcement learning and the potential for model-based reinforcement learning to achieve asymptotic performance that is comparable to the best model-free methods at a fraction of the sample complexity.\n8 Discussion & conclusion",
        "Our experiments suggest several conclusions that are relevant for further investigation in model-based reinforcement learning",
        "Our results show that model-based reinforcement learning with neural network dynamics models can achieve results that are competitive not only with Bayesian nonparametric models such as Gaussian processes, but on par with model-free algorithms such as policy optimization and Soft actor critic in terms of asymptotic performance, while attaining substantially more efficient convergence",
        "The individual components of our model-based reinforcement learning algorithms are not individually new \u2013 prior works have suggested both ensembling and outputting Gaussian distribution parameters [<a class=\"ref-link\" id=\"cLakshminarayanan_et+al_2017_a\" href=\"#rLakshminarayanan_et+al_2017_a\">Lakshminarayanan et al, 2017</a>], as well as the use of model predictive control for model-based Reinforcement learning [<a class=\"ref-link\" id=\"cNagabandi_et+al_2017_a\" href=\"#rNagabandi_et+al_2017_a\">Nagabandi et al, 2017</a>] \u2013 the particular combination of these components into a model-based reinforcement learning algorithm is, to our knowledge, novel, and the results provide a new state-of-the-art for model-based reinforcement learning algorithms based on high-capacity parametric models such as neural networks",
        "The systematic investigation in our experiments was a critical ingredient in determining the precise combination of these components that attains the best performance",
        "The observation that model-based Reinforcement learning can match the performance of model-free algorithms suggests that substantial further investigation of such of methods is in order, as a potential avenue for effective, sample-efficient, and practical general-purpose reinforcement learning"
    ],
    "summary": [
        "Reinforcement learning (RL) algorithms provide for an automated framework for decision making and control: by specifying a high-level objective function, an RL algorithm can, in principle, automatically learn a control policy that satisfies this objective.",
        "Once a dynamics model f is learned, we use f to predict the distribution over state-trajectories resulting from applying a sequence of actions.",
        "Gaussian process dynamics model (GP): we compare against three MBRL algorithms based on GPs. GP-E learns a GP model, but only propagate the expectation.",
        "Our results demonstrate that a purely model-based deep RL algorithm that only learns a dynamics model, omitting even a parameterized policy, can achieve comparable performance when properly incorporating uncertainty estimation during modeling and planning.",
        "These observations shed some light on the role of uncertainty in MBRL, particularly as it relates to discriminatively trained, expressive parametric models such as NNs. Our results suggest that, the quality of the model and the use of uncertainty at learning time significantly affect the performance of the MBRL algorithms tested, while the use of more advanced uncertainty propagation techniques seem to offers only minor improvements.",
        "As our experiments here and in the previous section show, the particular choice of these components in our algorithm achieves substantially improved results over previous state-of-the-art model-based and model-free methods, experimentally confirming both the importance of uncertainty estimation in MBRL and the potential for MBRL to achieve asymptotic performance that is comparable to the best model-free methods at a fraction of the sample complexity.",
        "Our results show that model-based reinforcement learning with neural network dynamics models can achieve results that are competitive not only with Bayesian nonparametric models such as GPs, but on par with model-free algorithms such as PPO and SAC in terms of asymptotic performance, while attaining substantially more efficient convergence.",
        "The individual components of our model-based reinforcement learning algorithms are not individually new \u2013 prior works have suggested both ensembling and outputting Gaussian distribution parameters [<a class=\"ref-link\" id=\"cLakshminarayanan_et+al_2017_a\" href=\"#rLakshminarayanan_et+al_2017_a\">Lakshminarayanan et al, 2017</a>], as well as the use of MPC for model-based RL [<a class=\"ref-link\" id=\"cNagabandi_et+al_2017_a\" href=\"#rNagabandi_et+al_2017_a\">Nagabandi et al, 2017</a>] \u2013 the particular combination of these components into a model-based reinforcement learning algorithm is, to our knowledge, novel, and the results provide a new state-of-the-art for model-based reinforcement learning algorithms based on high-capacity parametric models such as neural networks.",
        "The observation that model-based RL can match the performance of model-free algorithms suggests that substantial further investigation of such of methods is in order, as a potential avenue for effective, sample-efficient, and practical general-purpose reinforcement learning."
    ],
    "headline": "We study how to bridge this gap, by employing uncertainty-aware dynamics models",
    "reference_links": [
        {
            "id": "Abbeel_et+al_2006_a",
            "entry": "P. Abbeel, M. Quigley, and A. Y. Ng. Using inaccurate models in reinforcement learning. In International Conference on Machine Learning (ICML), pages 1\u20138, 2006. ISBN 1-59593-383-2. doi: 10.1145/1143844.1143845.",
            "crossref": "https://dx.doi.org/10.1145/1143844.1143845",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1143844.1143845"
        },
        {
            "id": "Agrawal_et+al_2016_a",
            "entry": "P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. Neural Information Processing Systems (NIPS), pages 5074\u20135082, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20P.%20Nair%2C%20A.V.%20Abbeel%2C%20P.%20Malik%2C%20J.%20Learning%20to%20poke%20by%20poking%3A%20Experiential%20learning%20of%20intuitive%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20P.%20Nair%2C%20A.V.%20Abbeel%2C%20P.%20Malik%2C%20J.%20Learning%20to%20poke%20by%20poking%3A%20Experiential%20learning%20of%20intuitive%20physics%202016"
        },
        {
            "id": "Atkeson_1997_a",
            "entry": "C. G. Atkeson and J. C. Santamar\u00eda. A comparison of direct and model-based reinforcement learning. In International Conference on Robotics and Automation (ICRA), 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Atkeson%2C%20C.G.%20Santamar%C3%ADa%2C%20J.C.%20A%20comparison%20of%20direct%20and%20model-based%20reinforcement%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Atkeson%2C%20C.G.%20Santamar%C3%ADa%2C%20J.C.%20A%20comparison%20of%20direct%20and%20model-based%20reinforcement%20learning%201997"
        },
        {
            "id": "Baranes_2013_a",
            "entry": "A. Baranes and P.-Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49\u201373, 2013. ISSN 0921-8890. doi: 10.1016/j.robot.2012.05.008.",
            "crossref": "https://dx.doi.org/10.1016/j.robot.2012.05.008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.robot.2012.05.008"
        },
        {
            "id": "Bellman_1957_a",
            "entry": "R. Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, pages 679\u2013684, 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellman%2C%20R.%20A%20Markovian%20decision%20process%201957",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellman%2C%20R.%20A%20Markovian%20decision%20process%201957"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks. International Conference on Machine Learning (ICML), 37:1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20C.%20Cornebise%2C%20J.%20Kavukcuoglu%2C%20K.%20Wierstra%2C%20D.%20Weight%20uncertainty%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20C.%20Cornebise%2C%20J.%20Kavukcuoglu%2C%20K.%20Wierstra%2C%20D.%20Weight%20uncertainty%20in%20neural%20networks%202015"
        },
        {
            "id": "Calandra_et+al_2016_a",
            "entry": "R. Calandra, J. Peters, C. E. Rasmussen, and M. P. Deisenroth. Manifold Gaussian processes for regression. In International Joint Conference on Neural Networks (IJCNN), pages 3338\u20133345, 2016. doi: 10.1109/IJCNN.2016.7727626.",
            "crossref": "https://dx.doi.org/10.1109/IJCNN.2016.7727626",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/IJCNN.2016.7727626"
        },
        {
            "id": "Camacho_2013_a",
            "entry": "E. F. Camacho and C. B. Alba. Model predictive control. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camacho%2C%20E.F.%20Alba%2C%20C.B.%20Model%20predictive%20control%202013"
        },
        {
            "id": "Chebotar_et+al_2017_a",
            "entry": "Y. Chebotar, K. Hausman, M. Zhang, G. Sukhatme, S. Schaal, and S. Levine. Combining model-based and model-free updates for trajectory-centric reinforcement learning. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chebotar%2C%20Y.%20Hausman%2C%20K.%20Zhang%2C%20M.%20Sukhatme%2C%20G.%20Combining%20model-based%20and%20model-free%20updates%20for%20trajectory-centric%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chebotar%2C%20Y.%20Hausman%2C%20K.%20Zhang%2C%20M.%20Sukhatme%2C%20G.%20Combining%20model-based%20and%20model-free%20updates%20for%20trajectory-centric%20reinforcement%20learning%202017"
        },
        {
            "id": "Deisenroth_et+al_2014_a",
            "entry": "M. Deisenroth, D. Fox, and C. Rasmussen. Gaussian processes for data-efficient learning in robotics and control. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 37(2): 408\u2013423, 2014. ISSN 0162-8828. doi: 10.1109/TPAMI.2013.218.",
            "crossref": "https://dx.doi.org/10.1109/TPAMI.2013.218",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TPAMI.2013.218"
        },
        {
            "id": "Depeweg_et+al_2016_a",
            "entry": "S. Depeweg, J. M. Hern\u00e1ndez-Lobato, F. Doshi-Velez, and S. Udluft. Learning and policy search in stochastic dynamical systems with Bayesian neural networks. ArXiv e-prints, May 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Depeweg%2C%20S.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Doshi-Velez%2C%20F.%20Udluft%2C%20S.%20Learning%20and%20policy%20search%20in%20stochastic%20dynamical%20systems%20with%20Bayesian%20neural%20networks.%20ArXiv%20e-prints%202016-05"
        },
        {
            "id": "Depeweg_et+al_2018_a",
            "entry": "S. Depeweg, J.-M. Hernandez-Lobato, F. Doshi-Velez, and S. Udluft. Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning. In International Conference on Machine Learning (ICML), pages 1192\u20131201, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Depeweg%2C%20S.%20Hernandez-Lobato%2C%20J.-M.%20Doshi-Velez%2C%20F.%20Udluft%2C%20S.%20Decomposition%20of%20uncertainty%20in%20Bayesian%20deep%20learning%20for%20efficient%20and%20risk-sensitive%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Depeweg%2C%20S.%20Hernandez-Lobato%2C%20J.-M.%20Doshi-Velez%2C%20F.%20Udluft%2C%20S.%20Decomposition%20of%20uncertainty%20in%20Bayesian%20deep%20learning%20for%20efficient%20and%20risk-sensitive%20learning%202018"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Draeger_et+al_1995_a",
            "entry": "A. Draeger, S. Engell, and H. Ranke. Model predictive control using neural networks. IEEE Control Systems, 15(5):61\u201366, Oct 1995. ISSN 1066-033X. doi: 10.1109/37.466261.",
            "crossref": "https://dx.doi.org/10.1109/37.466261",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/37.466261"
        },
        {
            "id": "Efron_1994_a",
            "entry": "B. Efron and R. Tibshirani. An introduction to the bootstrap. CRC press, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Efron%2C%20B.%20Tibshirani%2C%20R.%20An%20introduction%20to%20the%20bootstrap%201994"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "C. Finn, X. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel. Deep spatial autoencoders for visuomotor learning. In International Conference on Robotics and Automation (ICRA), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Tan%2C%20X.%20Duan%2C%20Y.%20Darrell%2C%20T.%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Tan%2C%20X.%20Duan%2C%20Y.%20Darrell%2C%20T.%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016"
        },
        {
            "id": "Fu_et+al_2016_a",
            "entry": "J. Fu, S. Levine, and P. Abbeel. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4019\u20134026, 2016. doi: 10.1109/IROS.2016.7759592.",
            "crossref": "https://dx.doi.org/10.1109/IROS.2016.7759592",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/IROS.2016.7759592"
        },
        {
            "id": "Gal_et+al_2016_a",
            "entry": "Y. Gal, R. McAllister, and C. Rasmussen. Improving PILCO with Bayesian neural network dynamics models. ICML Workshop on Data-Efficient Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20McAllister%2C%20R.%20Rasmussen%2C%20C.%20Improving%20PILCO%20with%20Bayesian%20neural%20network%20dynamics%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Y.%20McAllister%2C%20R.%20Rasmussen%2C%20C.%20Improving%20PILCO%20with%20Bayesian%20neural%20network%20dynamics%20models%202016"
        },
        {
            "id": "Gal_et+al_2017_a",
            "entry": "Y. Gal, J. Hron, and A. Kendall. Concrete dropout. In Neural Information Processing Systems (NIPS), pages 3584\u20133593, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%20Gal%20J%20Hron%20and%20A%20Kendall%20Concrete%20dropout%20In%20Neural%20Information%20Processing%20Systems%20NIPS%20pages%2035843593%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%20Gal%20J%20Hron%20and%20A%20Kendall%20Concrete%20dropout%20In%20Neural%20Information%20Processing%20Systems%20NIPS%20pages%2035843593%202017"
        },
        {
            "id": "Girard_et+al_2002_a",
            "entry": "A. Girard, C. E. Rasmussen, J. Quinonero-Candela, R. Murray-Smith, O. Winther, and J. Larsen. Multiple-step ahead prediction for non linear dynamic systems\u2013a Gaussian process treatment with propagation of the uncertainty. Neural Information Processing Systems (NIPS), 15:529\u2013536, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girard%2C%20A.%20Rasmussen%2C%20C.E.%20Quinonero-Candela%2C%20J.%20Murray-Smith%2C%20R.%20Multiple-step%20ahead%20prediction%20for%20non%20linear%20dynamic%20systems%E2%80%93a%20Gaussian%20process%20treatment%20with%20propagation%20of%20the%20uncertainty%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girard%2C%20A.%20Rasmussen%2C%20C.E.%20Quinonero-Candela%2C%20J.%20Murray-Smith%2C%20R.%20Multiple-step%20ahead%20prediction%20for%20non%20linear%20dynamic%20systems%E2%80%93a%20Gaussian%20process%20treatment%20with%20propagation%20of%20the%20uncertainty%202002"
        },
        {
            "id": "Grancharova_et+al_2008_a",
            "entry": "A. Grancharova, J. Kocijan, and T. A. Johansen. Explicit stochastic predictive control of combustion plants based on Gaussian process models. Automatica, 44(6):1621\u20131631, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grancharova%2C%20A.%20Kocijan%2C%20J.%20Johansen%2C%20T.A.%20Explicit%20stochastic%20predictive%20control%20of%20combustion%20plants%20based%20on%20Gaussian%20process%20models%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grancharova%2C%20A.%20Kocijan%2C%20J.%20Johansen%2C%20T.A.%20Explicit%20stochastic%20predictive%20control%20of%20combustion%20plants%20based%20on%20Gaussian%20process%20models%202008"
        },
        {
            "id": "Gu_et+al_2016_a",
            "entry": "S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep Q-learning with model-based acceleration. In International Conference on Machine Learning (ICML), pages 2829\u20132838, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20S.%20Lillicrap%2C%20T.%20Sutskever%2C%20I.%20Levine%2C%20S.%20Continuous%20deep%20Q-learning%20with%20model-based%20acceleration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20S.%20Lillicrap%2C%20T.%20Sutskever%2C%20I.%20Levine%2C%20S.%20Continuous%20deep%20Q-learning%20with%20model-based%20acceleration%202016"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20C.%20Pleiss%2C%20G.%20Sun%2C%20Y.%20Weinberger%2C%20K.Q.%20On%20calibration%20of%20modern%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20C.%20Pleiss%2C%20G.%20Sun%2C%20Y.%20Weinberger%2C%20K.Q.%20On%20calibration%20of%20modern%20neural%20networks%202017"
        },
        {
            "id": "Haarnoja_et+al_2018_a",
            "entry": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning (ICML), volume 80, pages 1856\u20131865, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20T.%20Zhou%2C%20A.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20T.%20Zhou%2C%20A.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018"
        },
        {
            "id": "Hernandaz_1990_a",
            "entry": "E. Hernandaz and Y. Arkun. Neural network modeling and an extended DMC algorithm to control nonlinear systems. In American Control Conference, pages 2454\u20132459, May 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandaz%2C%20E.%20Arkun%2C%20Y.%20Neural%20network%20modeling%20and%20an%20extended%20DMC%20algorithm%20to%20control%20nonlinear%20systems%201990-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandaz%2C%20E.%20Arkun%2C%20Y.%20Neural%20network%20modeling%20and%20an%20extended%20DMC%20algorithm%20to%20control%20nonlinear%20systems%201990-05"
        },
        {
            "id": "Hern_2015_a",
            "entry": "J. M. Hern\u00e1ndez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In International Conference on Machine Learning, pages 1861\u20131869, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20J.M.%20Adams%2C%20R.%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20Bayesian%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20J.M.%20Adams%2C%20R.%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20Bayesian%20neural%20networks%202015"
        },
        {
            "id": "J_2016_a",
            "entry": "J. M. Hern\u00e1ndez-Lobato, Y. Li, M. Rowland, D. Hern\u00e1ndez-Lobato, T. Bui, and R. E. Turner. Blackbox \u03b1-divergence minimization. International Conference on Machine Learning (ICML), 48: 1511\u20131520, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blackbox%20%CE%B1-divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blackbox%20%CE%B1-divergence%20minimization%202016"
        },
        {
            "id": "Higuera_et+al_2018_a",
            "entry": "J. C. G. Higuera, D. Meger, and G. Dudek. Synthesizing neural network controllers with probabilistic model based reinforcement learning. arXiv preprint arXiv:1803.02291, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02291"
        },
        {
            "id": "Kamthe_2018_a",
            "entry": "S. Kamthe and M. P. Deisenroth. Data-efficient reinforcement learning with probabilistic model predictive control. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamthe%2C%20S.%20Deisenroth%2C%20M.P.%20Data-efficient%20reinforcement%20learning%20with%20probabilistic%20model%20predictive%20control%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamthe%2C%20S.%20Deisenroth%2C%20M.P.%20Data-efficient%20reinforcement%20learning%20with%20probabilistic%20model%20predictive%20control%202018"
        },
        {
            "id": "Ko_et+al_2007_a",
            "entry": "J. Ko, D. J. Klein, D. Fox, and D. Haehnel. Gaussian processes and reinforcement learning for identification and control of an autonomous blimp. In IEEE International Conference on Robotics and Automation (ICRA), pages 742\u2013747. IEEE, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ko%2C%20J.%20Klein%2C%20D.J.%20Fox%2C%20D.%20Haehnel%2C%20D.%20Gaussian%20processes%20and%20reinforcement%20learning%20for%20identification%20and%20control%20of%20an%20autonomous%20blimp%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ko%2C%20J.%20Klein%2C%20D.J.%20Fox%2C%20D.%20Haehnel%2C%20D.%20Gaussian%20processes%20and%20reinforcement%20learning%20for%20identification%20and%20control%20of%20an%20autonomous%20blimp%202007"
        },
        {
            "id": "Kober_2009_a",
            "entry": "J. Kober and J. Peters. Policy search for motor primitives in robotics. In Neural information processing systems (NIPS), pages 849\u2013856, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20J.%20Peters%2C%20J.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20J.%20Peters%2C%20J.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009"
        },
        {
            "id": "Kocijan_et+al_2004_a",
            "entry": "J. Kocijan, R. Murray-Smith, C. E. Rasmussen, and A. Girard. Gaussian process model based predictive control. In American Control Conference, volume 3, pages 2214\u20132219. IEEE, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kocijan%2C%20J.%20Murray-Smith%2C%20R.%20Rasmussen%2C%20C.E.%20Girard%2C%20A.%20Gaussian%20process%20model%20based%20predictive%20control%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kocijan%2C%20J.%20Murray-Smith%2C%20R.%20Rasmussen%2C%20C.E.%20Girard%2C%20A.%20Gaussian%20process%20model%20based%20predictive%20control%202004"
        },
        {
            "id": "Kupcsik_et+al_2013_a",
            "entry": "A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-efficient generalization of robot skills with contextual policy search. In Conference on Artificial Intelligence (AAAI), pages 1401\u20131407, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kupcsik%2C%20A.G.%20Deisenroth%2C%20M.P.%20Peters%2C%20J.%20Neumann%2C%20G.%20Data-efficient%20generalization%20of%20robot%20skills%20with%20contextual%20policy%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kupcsik%2C%20A.G.%20Deisenroth%2C%20M.P.%20Peters%2C%20J.%20Neumann%2C%20G.%20Data-efficient%20generalization%20of%20robot%20skills%20with%20contextual%20policy%20search%202013"
        },
        {
            "id": "Kurutach_et+al_2018_a",
            "entry": "T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10592"
        },
        {
            "id": "Lakshminarayanan_et+al_2017_a",
            "entry": "B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Neural Information Processing Systems (NIPS), pages 6405\u20136416. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20B.%20Pritzel%2C%20A.%20Blundell%2C%20C.%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshminarayanan%2C%20B.%20Pritzel%2C%20A.%20Blundell%2C%20C.%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017"
        },
        {
            "id": "Lenz_et+al_2015_a",
            "entry": "I. Lenz, R. Knepper, and A. Saxena. DeepMPC: Learning deep latent features for model predictive control. In Robotics Science and Systems (RSS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lenz%2C%20I.%20Knepper%2C%20R.%20Saxena%2C%20A.%20DeepMPC%3A%20Learning%20deep%20latent%20features%20for%20model%20predictive%20control%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lenz%2C%20I.%20Knepper%2C%20R.%20Saxena%2C%20A.%20DeepMPC%3A%20Learning%20deep%20latent%20features%20for%20model%20predictive%20control%202015"
        },
        {
            "id": "Levine_et+al_2016_a",
            "entry": "S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. J. Mach. Learn. Res., 17(1):1334\u20131373, Jan. 2016. ISSN 1532-4435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016-01"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Lin_1992_a",
            "entry": "L.-J. Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie Mellon University, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20L.-J.%20Reinforcement%20Learning%20for%20Robots%20Using%20Neural%20Networks%201992"
        },
        {
            "id": "Mackay_1992_a",
            "entry": "D. J. MacKay. A practical Bayesian framework for backpropagation networks. Neural computation, 4(3):448\u2013472, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20D.J.%20A%20practical%20Bayesian%20framework%20for%20backpropagation%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacKay%2C%20D.J.%20A%20practical%20Bayesian%20framework%20for%20backpropagation%20networks%201992"
        },
        {
            "id": "Mcallister_2017_a",
            "entry": "R. McAllister and C. E. Rasmussen. Data-efficient reinforcement learning in continuous state-action Gaussian-POMDPs. In Neural Information Processing Systems (NIPS), pages 2037\u20132046. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllister%2C%20R.%20Rasmussen%2C%20C.E.%20Data-efficient%20reinforcement%20learning%20in%20continuous%20state-action%20Gaussian-POMDPs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllister%2C%20R.%20Rasmussen%2C%20C.E.%20Data-efficient%20reinforcement%20learning%20in%20continuous%20state-action%20Gaussian-POMDPs%202017"
        },
        {
            "id": "Miller_et+al_1990_a",
            "entry": "W. T. Miller, R. P. Hewes, F. H. Glanz, and L. G. Kraft. Real-time dynamic control of an industrial manipulator using a neural network-based learning controller. IEEE Transactions on Robotics and Automation, 6(1):1\u20139, Feb 1990. ISSN 1042-296X. doi: 10.1109/70.88112.",
            "crossref": "https://dx.doi.org/10.1109/70.88112",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/70.88112"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mordatch_et+al_2016_a",
            "entry": "I. Mordatch, N. Mishra, C. Eppner, and P. Abbeel. Combining model-based policy search with online model learning for control of physical humanoids. In IEEE International Conference on Robotics and Automation (ICRA), pages 242\u2013248, May 2016. doi: 10.1109/ICRA.2016.7487140.",
            "crossref": "https://dx.doi.org/10.1109/ICRA.2016.7487140",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICRA.2016.7487140"
        },
        {
            "id": "Nagabandi_et+al_2017_a",
            "entry": "A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. ArXiv e-prints, Aug. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagabandi%2C%20A.%20Kahn%2C%20G.%20Fearing%2C%20R.S.%20Levine%2C%20S.%20Neural%20network%20dynamics%20for%20model-based%20deep%20reinforcement%20learning%20with%20model-free%20fine-tuning.%20ArXiv%20e-prints%202017-08"
        },
        {
            "id": "Neal_1995_a",
            "entry": "R. Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20R.%20Bayesian%20learning%20for%20neural%20networks%201995"
        },
        {
            "id": "Nguyen-Tuong_et+al_2008_a",
            "entry": "D. Nguyen-Tuong, J. Peters, and M. Seeger. Local Gaussian process regression for real time online model learning. In Neural Information Processing Systems (NIPS), pages 1193\u20131200, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen-Tuong%2C%20D.%20Peters%2C%20J.%20Seeger%2C%20M.%20Local%20Gaussian%20process%20regression%20for%20real%20time%20online%20model%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen-Tuong%2C%20D.%20Peters%2C%20J.%20Seeger%2C%20M.%20Local%20Gaussian%20process%20regression%20for%20real%20time%20online%20model%20learning%202008"
        },
        {
            "id": "Osband_2016_a",
            "entry": "I. Osband. Risk versus uncertainty in deep learning: Bayes, bootstrap and the dangers of dropout. NIPS Workshop on Bayesian Deep Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20I.%20Risk%20versus%20uncertainty%20in%20deep%20learning%3A%20Bayes%2C%20bootstrap%20and%20the%20dangers%20of%20dropout%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20I.%20Risk%20versus%20uncertainty%20in%20deep%20learning%3A%20Bayes%2C%20bootstrap%20and%20the%20dangers%20of%20dropout%202016"
        },
        {
            "id": "Osband_et+al_2016_b",
            "entry": "I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped DQN. In Neural Information Processing Systems (NIPS), pages 4026\u20134034, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20I.%20Blundell%2C%20C.%20Pritzel%2C%20A.%20Roy%2C%20B.Van%20Deep%20exploration%20via%20bootstrapped%20DQN%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20I.%20Blundell%2C%20C.%20Pritzel%2C%20A.%20Roy%2C%20B.Van%20Deep%20exploration%20via%20bootstrapped%20DQN%202016"
        },
        {
            "id": "Parmas_et+al_2018_a",
            "entry": "P. Parmas, C. E. Rasmussen, J. Peters, and K. Doya. PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (ICML), volume 80, pages 4062\u20134071, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parmas%2C%20P.%20Rasmussen%2C%20C.E.%20Peters%2C%20J.%20Doya%2C%20K.%20PIPPS%3A%20Flexible%20model-based%20policy%20search%20robust%20to%20the%20curse%20of%20chaos%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parmas%2C%20P.%20Rasmussen%2C%20C.E.%20Peters%2C%20J.%20Doya%2C%20K.%20PIPPS%3A%20Flexible%20model-based%20policy%20search%20robust%20to%20the%20curse%20of%20chaos%202018"
        },
        {
            "id": "Punjani_2015_a",
            "entry": "A. Punjani and P. Abbeel. Deep learning helicopter dynamics models. In IEEE International Conference on Robotics and Automation (ICRA), pages 3223\u20133230, May 2015. doi: 10.1109/ ICRA.2015.7139643.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Punjani%2C%20A.%20Abbeel%2C%20P.%20Deep%20learning%20helicopter%20dynamics%20models%202015-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Punjani%2C%20A.%20Abbeel%2C%20P.%20Deep%20learning%20helicopter%20dynamics%20models%202015-05"
        },
        {
            "id": "Qui_et+al_2003_a",
            "entry": "J. Qui\u00f1onero-Candela, A. Girard, J. Larsen, and C. E. Rasmussen. Propagation of uncertainty in Bayesian kernel models\u2014application to multiple-step ahead forecasting. In IEEE International Conference on Acoustics, Speech and Signal Processing, volume 2, pages 701\u2013704, April 2003. doi: 10.1109/ICASSP.2003.1202463.",
            "crossref": "https://dx.doi.org/10.1109/ICASSP.2003.1202463",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICASSP.2003.1202463"
        },
        {
            "id": "Ramachandran_et+al_2017_a",
            "entry": "P. Ramachandran, B. Zoph, and Q. V. Le. Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941.",
            "url": "http://arxiv.org/abs/1710.05941",
            "arxiv_url": "https://arxiv.org/pdf/1710.05941"
        },
        {
            "id": "Rasmussen_2003_a",
            "entry": "C. E. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning. In Neural Information Processing Systems (NIPS), volume 4, page 1, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Kuss%2C%20M.%20Gaussian%20processes%20in%20reinforcement%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20C.E.%20Kuss%2C%20M.%20Gaussian%20processes%20in%20reinforcement%20learning%202003"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Thrun_1992_a",
            "entry": "S. Thrun. Efficient exploration in reinforcement learning. Technical Report CMU-CS-92-102, Carnegie Mellon University, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20S.%20Efficient%20exploration%20in%20reinforcement%20learning%201992"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5026\u20135033, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Williams_et+al_2017_a",
            "entry": "G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou. Information theoretic MPC for model-based reinforcement learning. In International Conference on Robotics and Automation (ICRA), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20G.%20Wagener%2C%20N.%20Goldfain%2C%20B.%20Drews%2C%20P.%20Theodorou.%20Information%20theoretic%20MPC%20for%20model-based%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20G.%20Wagener%2C%20N.%20Goldfain%2C%20B.%20Drews%2C%20P.%20Theodorou.%20Information%20theoretic%20MPC%20for%20model-based%20reinforcement%20learning%202017"
        }
    ]
}
