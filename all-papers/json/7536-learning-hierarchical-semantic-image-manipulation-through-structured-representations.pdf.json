{
    "filename": "7536-learning-hierarchical-semantic-image-manipulation-through-structured-representations.pdf",
    "metadata": {
        "title": "Learning Hierarchical Semantic Image Manipulation through Structured Representations",
        "author": "Seunghoon Hong, Xinchen Yan, Honglak Lee, Thomas S. Huang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7536-learning-hierarchical-semantic-image-manipulation-through-structured-representations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation on natural image manifold through color strokes, keypoints, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ structured semantic layout as our intermediate representation for manipulation. Initialized with coarse-level bounding boxes, our structure generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation."
    },
    "keywords": [
        {
            "term": "image editing",
            "url": "https://en.wikipedia.org/wiki/image_editing"
        },
        {
            "term": "image manipulation",
            "url": "https://en.wikipedia.org/wiki/image_manipulation"
        },
        {
            "term": "bounding box",
            "url": "https://en.wikipedia.org/wiki/bounding_box"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        }
    ],
    "highlights": [
        "To facilitate the image manipulation from coarse-level semantic bounding boxes, we introduce a hierarchical generation model that predicts the image in multiple abstraction levels",
        "In the experiment on interactive image editing, we show that our method allows users to manipulate images using object bounding boxes, while the structure and style of the generated objects are adaptively determined depending on the context and naturally blended into the scene",
        "We show that our simple object-level manipulation interface can be naturally extended to data-driven image manipulation, by automatically sampling the object boxes and generating the novel images",
        "Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively",
        "We further demonstrate its practical benefits in semantic object manipulation, interactive image editing and data-driven image editing"
    ],
    "key_statements": [
        "To facilitate the image manipulation from coarse-level semantic bounding boxes, we introduce a hierarchical generation model that predicts the image in multiple abstraction levels",
        "We present two applications of the proposed method on interactive and data-driven image editing",
        "In the experiment on interactive image editing, we show that our method allows users to manipulate images using object bounding boxes, while the structure and style of the generated objects are adaptively determined depending on the context and naturally blended into the scene",
        "We show that our simple object-level manipulation interface can be naturally extended to data-driven image manipulation, by automatically sampling the object boxes and generating the novel images",
        "Contrary to the previous works that manipulate images based on low-level visual controls such as visual context [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] or color strokes [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], our model allows semantic control over manipulation process through labeled bounding box and the inferred semantic layout",
        "Contrary to Hong et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], our work focuses on manipulation of parts of an image, which requires incorporation of both semantic and visual context of surrounding regions in the hierarchical generation process in such a way that structure and style of the generated object are aligned with the other parts of an image.\n3 Hierarchical Image Manipulation",
        "Ladv(Mobj, Mo\u2217bj) is the conditional adversarial loss defined on object mask ensuring the perceptual quality of predicted object shape, which is given by",
        "Given an image I and the manipulated layout Mobtained by the structure generator, the image generator outputs a pixel-level prediction of the contents inside the regions defined by B",
        "We present the manipulation results of different methods together with the input image and the class label of the bounding box and ask users to choose the best method based on how natural the manipulated images are",
        "In Figure 5, we present the manipulation results by moving the same bounding box of a car to different locations in an image",
        "We present an application of data-driven image manipulation in Figure 9",
        "Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively",
        "We further demonstrate its practical benefits in semantic object manipulation, interactive image editing and data-driven image editing"
    ],
    "summary": [
        "To facilitate the image manipulation from coarse-level semantic bounding boxes, we introduce a hierarchical generation model that predicts the image in multiple abstraction levels.",
        "Our structure generator first infers the fine-grained semantic label maps from the coarse object bounding boxes, which produces structure of the manipulated object aligned with the context.",
        "Our image generator infers the style of the object considering the perceptual consistency to the surroundings.",
        "In the experiment on interactive image editing, we show that our method allows users to manipulate images using object bounding boxes, while the structure and style of the generated objects are adaptively determined depending on the context and naturally blended into the scene.",
        "Contrary to the previous works that manipulate images based on low-level visual controls such as visual context [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] or color strokes [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], our model allows semantic control over manipulation process through labeled bounding box and the inferred semantic layout.",
        "Contrary to Hong et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], our work focuses on manipulation of parts of an image, which requires incorporation of both semantic and visual context of surrounding regions in the hierarchical generation process in such a way that structure and style of the generated object are aligned with the other parts of an image.",
        "Given an object-level specification by B, the image manipulation is posed as a hierarchical generation task from a coarse bounding box to pixel-level predictions of structure and style.",
        "Given an image I and the manipulated layout Mobtained by the structure generator, the image generator outputs a pixel-level prediction of the contents inside the regions defined by B.",
        "By combining information from both encoders, the model learns to manipulate images that reflect the underlying image structure defined by the label map with appearance patterns naturally blending into the surrounding context, which is semantically meaningful and perceptually plausible.",
        "To measure the quality of layoutconditional image generation, we apply a pre-trained semantic segmentation model DeepLab v3 [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to the generated images, and measure the consistency between the input layout and the predicted segmentation labels in terms of pixel-wise accuracy.",
        "A comparison between SingleStream-Concat and TwoStream shows that modeling the image and layout conditions using separate encoding pathways further improves the generation quality.",
        "Our model with predicted layout TwoStream-Pred generates objects different from the ground-truth layout but still match the bounding box condition.",
        "Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively.",
        "We further demonstrate its practical benefits in semantic object manipulation, interactive image editing and data-driven image editing"
    ],
    "headline": "We present a novel hierarchical framework for semantic image manipulation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing. ACM Transactions on Graphics, 28(3): 24, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barnes%2C%20C.%20Shechtman%2C%20E.%20Finkelstein%2C%20A.%20Goldman%2C%20D.B.%20Patchmatch%3A%20A%20randomized%20correspondence%20algorithm%20for%20structural%20image%20editing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barnes%2C%20C.%20Shechtman%2C%20E.%20Finkelstein%2C%20A.%20Goldman%2C%20D.B.%20Patchmatch%3A%20A%20randomized%20correspondence%20algorithm%20for%20structural%20image%20editing%202009"
        },
        {
            "id": "2",
            "entry": "[2] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. arXiv:1802.02611, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02611"
        },
        {
            "id": "3",
            "entry": "[3] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Q.%20Koltun%2C%20V.%20Photographic%20image%20synthesis%20with%20cascaded%20refinement%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Q.%20Koltun%2C%20V.%20Photographic%20image%20synthesis%20with%20cascaded%20refinement%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016"
        },
        {
            "id": "5",
            "entry": "[5] E. Denton, S. Gross, and R. Fergus. Semi-supervised learning with context-conditional generative adversarial networks. arXiv preprint arXiv:1611.06430, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06430"
        },
        {
            "id": "6",
            "entry": "[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "7",
            "entry": "[7] A. Gupta, A. A. Efros, and M. Hebert. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In ECCV, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20A.%20Efros%2C%20A.A.%20Hebert%2C%20M.%20Blocks%20world%20revisited%3A%20Image%20understanding%20using%20qualitative%20geometry%20and%20mechanics%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20A.%20Efros%2C%20A.A.%20Hebert%2C%20M.%20Blocks%20world%20revisited%3A%20Image%20understanding%20using%20qualitative%20geometry%20and%20mechanics%202010"
        },
        {
            "id": "8",
            "entry": "[8] D. Hoiem, A. A. Efros, and M. Hebert. Geometric context from a single image. In ICCV, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoiem%2C%20D.%20Efros%2C%20A.A.%20Hebert%2C%20M.%20Geometric%20context%20from%20a%20single%20image%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoiem%2C%20D.%20Efros%2C%20A.A.%20Hebert%2C%20M.%20Geometric%20context%20from%20a%20single%20image%202005"
        },
        {
            "id": "9",
            "entry": "[9] D. Hoiem, A. A. Efros, and M. Hebert. Putting objects in perspective. International Journal of Computer Vision, 80(1):3\u201315, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoiem%2C%20D.%20Efros%2C%20A.A.%20Hebert%2C%20M.%20Putting%20objects%20in%20perspective%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoiem%2C%20D.%20Efros%2C%20A.A.%20Hebert%2C%20M.%20Putting%20objects%20in%20perspective%202008"
        },
        {
            "id": "10",
            "entry": "[10] S. Hong, D. Yang, J. Choi, and H. Lee. Inferring semantic layout for hierarchical text-to-image synthesis. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20S.%20Yang%2C%20D.%20Choi%2C%20J.%20Lee%2C%20H.%20Inferring%20semantic%20layout%20for%20hierarchical%20text-to-image%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20S.%20Yang%2C%20D.%20Choi%2C%20J.%20Lee%2C%20H.%20Inferring%20semantic%20layout%20for%20hierarchical%20text-to-image%20synthesis%202018"
        },
        {
            "id": "11",
            "entry": "[11] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "12",
            "entry": "[12] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "13",
            "entry": "[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "14",
            "entry": "[14] Y. Li, S. Liu, J. Yang, and M.-H. Yang. Generative face completion. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Liu%2C%20S.%20Yang%2C%20J.%20Yang%2C%20M.-H.%20Generative%20face%20completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Liu%2C%20S.%20Yang%2C%20J.%20Yang%2C%20M.-H.%20Generative%20face%20completion%202017"
        },
        {
            "id": "15",
            "entry": "[15] C. Liu, J. Yuen, and A. Torralba. Sift flow: Dense correspondence across scenes and its applications. In Dense Image Correspondences for Computer Vision, pages 15\u201349. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20C.%20Yuen%2C%20J.%20Torralba%2C%20A.%20Sift%20flow%3A%20Dense%20correspondence%20across%20scenes%20and%20its%20applications%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20C.%20Yuen%2C%20J.%20Torralba%2C%20A.%20Sift%20flow%3A%20Dense%20correspondence%20across%20scenes%20and%20its%20applications%202016"
        },
        {
            "id": "16",
            "entry": "[16] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "17",
            "entry": "[17] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Krahenbuhl%2C%20P.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Krahenbuhl%2C%20P.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "18",
            "entry": "[18] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "19",
            "entry": "[19] S. E. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee. Learning what and where to draw. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.E.%20Akata%2C%20Z.%20Mohan%2C%20S.%20Tenka%2C%20S.%20Learning%20what%20and%20where%20to%20draw%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.E.%20Akata%2C%20Z.%20Mohan%2C%20S.%20Tenka%2C%20S.%20Learning%20what%20and%20where%20to%20draw%202016"
        },
        {
            "id": "20",
            "entry": "[20] P. Sangkloy, J. Lu, C. Fang, F. Yu, and J. Hays. Scribbler: Controlling deep image synthesis with sketch and color. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sangkloy%2C%20P.%20Lu%2C%20J.%20Fang%2C%20C.%20Yu%2C%20F.%20Scribbler%3A%20Controlling%20deep%20image%20synthesis%20with%20sketch%20and%20color%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sangkloy%2C%20P.%20Lu%2C%20J.%20Fang%2C%20C.%20Yu%2C%20F.%20Scribbler%3A%20Controlling%20deep%20image%20synthesis%20with%20sketch%20and%20color%202017"
        },
        {
            "id": "21",
            "entry": "[21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "22",
            "entry": "[22] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, et al. Going deeper with convolutions. CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "23",
            "entry": "[23] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016"
        },
        {
            "id": "24",
            "entry": "[24] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Tao%2C%20A.%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Tao%2C%20A.%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20gans%202017"
        },
        {
            "id": "25",
            "entry": "[25] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on Image Processing, 13(4):600\u2013612, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004"
        },
        {
            "id": "26",
            "entry": "[26] W. Xian, P. Sangkloy, J. Lu, C. Fang, F. Yu, and J. Hays. Texturegan: Controlling deep image synthesis with texture patches. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xian%2C%20W.%20Sangkloy%2C%20P.%20Lu%2C%20J.%20Fang%2C%20C.%20Texturegan%3A%20Controlling%20deep%20image%20synthesis%20with%20texture%20patches%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xian%2C%20W.%20Sangkloy%2C%20P.%20Lu%2C%20J.%20Fang%2C%20C.%20Texturegan%3A%20Controlling%20deep%20image%20synthesis%20with%20texture%20patches%202018"
        },
        {
            "id": "27",
            "entry": "[27] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Conditional image generation from visual attributes. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20X.%20Yang%2C%20J.%20Sohn%2C%20K.%20Lee%2C%20H.%20Attribute2image%3A%20Conditional%20image%20generation%20from%20visual%20attributes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20X.%20Yang%2C%20J.%20Sohn%2C%20K.%20Lee%2C%20H.%20Attribute2image%3A%20Conditional%20image%20generation%20from%20visual%20attributes%202016"
        },
        {
            "id": "28",
            "entry": "[28] J. Yang, A. Kannan, D. Batra, and D. Parikh. Lr-gan: Layered recursive generative adversarial networks for image generation. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20J.%20Kannan%2C%20A.%20Batra%2C%20D.%20Parikh%2C%20D.%20Lr-gan%3A%20Layered%20recursive%20generative%20adversarial%20networks%20for%20image%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20J.%20Kannan%2C%20A.%20Batra%2C%20D.%20Parikh%2C%20D.%20Lr-gan%3A%20Layered%20recursive%20generative%20adversarial%20networks%20for%20image%20generation%202017"
        },
        {
            "id": "29",
            "entry": "[29] Y. Yang, S. Hallman, D. Ramanan, and C. C. Fowlkes. Layered object models for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(9):1731\u2013 1743, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Y.%20Hallman%2C%20S.%20Ramanan%2C%20D.%20Fowlkes%2C%20C.C.%20Layered%20object%20models%20for%20image%20segmentation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Y.%20Hallman%2C%20S.%20Ramanan%2C%20D.%20Fowlkes%2C%20C.C.%20Layered%20object%20models%20for%20image%20segmentation%202012"
        },
        {
            "id": "30",
            "entry": "[30] R. A. Yeh, C. Chen, T. Y. Lim, A. G. Schwing, M. Hasegawa-Johnson, and M. N. Do. Semantic image inpainting with deep generative models. In CVPR, pages 5485\u20135493, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20R.A.%20Chen%2C%20C.%20Lim%2C%20T.Y.%20Schwing%2C%20A.G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20R.A.%20Chen%2C%20C.%20Lim%2C%20T.Y.%20Schwing%2C%20A.G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017"
        },
        {
            "id": "31",
            "entry": "[31] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Zhao%2C%20H.%20Puig%2C%20X.%20Fidler%2C%20S.%20Scene%20parsing%20through%20ade20k%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20B.%20Zhao%2C%20H.%20Puig%2C%20X.%20Fidler%2C%20S.%20Scene%20parsing%20through%20ade20k%20dataset%202017"
        },
        {
            "id": "32",
            "entry": "[32] J.-Y. Zhu, P. Kr\u00e4henb\u00fchl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image manifold. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Kr%C3%A4henb%C3%BChl%2C%20P.%20Shechtman%2C%20E.%20Efros%2C%20A.A.%20Generative%20visual%20manipulation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Kr%C3%A4henb%C3%BChl%2C%20P.%20Shechtman%2C%20E.%20Efros%2C%20A.A.%20Generative%20visual%20manipulation%202016"
        },
        {
            "id": "33",
            "entry": "[33] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        }
    ]
}
