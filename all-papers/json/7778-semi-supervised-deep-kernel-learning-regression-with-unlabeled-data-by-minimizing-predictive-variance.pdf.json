{
    "filename": "7778-semi-supervised-deep-kernel-learning-regression-with-unlabeled-data-by-minimizing-predictive-variance.pdf",
    "metadata": {
        "title": "Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance",
        "author": "Neal Jean, Sang Michael Xie, Stefano Ermon",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7778-semi-supervised-deep-kernel-learning-regression-with-unlabeled-data-by-minimizing-predictive-variance.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression."
    },
    "keywords": [
        {
            "term": "Department of Defense",
            "url": "https://en.wikipedia.org/wiki/Department_of_Defense"
        },
        {
            "term": "k-nearest neighbor",
            "url": "https://en.wikipedia.org/wiki/k-nearest_neighbor"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "mean teacher",
            "url": "https://en.wikipedia.org/wiki/Mean_Teacher"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "The prevailing trend in machine learning is to automatically discover good feature representations through end-to-end optimization of neural networks",
        "We present semi-supervised deep kernel learning, which addresses the challenge of semisupervised regression by building on previous work combining the feature learning capabilities of deep neural networks with the ability of Gaussian processes to capture uncertainty [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "We demonstrate that supervised deep kernel learning can use unlabeled data to learn more generalizable features and improve performance on a range of regression tasks, outperforming the supervised deep kernel learning method and semi-supervised methods such as virtual adversarial training (VAT) and mean teacher [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "Representation learning To gain some intuition about how the unlabeled data helps in the learning process, we visualize the neural network embeddings learned by the Deep kernel learning and supervised deep kernel learning models on the Skillcraft dataset",
        "supervised deep kernel learning models are naturally suited for many real-world problems, as spatial and temporal structure can be explicitly modeled through the composition of kernel functions",
        "While our focus is on regression problems, we believe the supervised deep kernel learning framework is applicable to classification problems\u2014we leave this to future work"
    ],
    "key_statements": [
        "The prevailing trend in machine learning is to automatically discover good feature representations through end-to-end optimization of neural networks",
        "We present semi-supervised deep kernel learning, which addresses the challenge of semisupervised regression by building on previous work combining the feature learning capabilities of deep neural networks with the ability of Gaussian processes to capture uncertainty [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "We demonstrate that supervised deep kernel learning can use unlabeled data to learn more generalizable features and improve performance on a range of regression tasks, outperforming the supervised deep kernel learning method and semi-supervised methods such as virtual adversarial training (VAT) and mean teacher [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "Representation learning To gain some intuition about how the unlabeled data helps in the learning process, we visualize the neural network embeddings learned by the Deep kernel learning and supervised deep kernel learning models on the Skillcraft dataset",
        "supervised deep kernel learning models are naturally suited for many real-world problems, as spatial and temporal structure can be explicitly modeled through the composition of kernel functions",
        "While our focus is on regression problems, we believe the supervised deep kernel learning framework is applicable to classification problems\u2014we leave this to future work"
    ],
    "summary": [
        "The prevailing trend in machine learning is to automatically discover good feature representations through end-to-end optimization of neural networks.",
        "Semi-supervised learning approaches offer promise when few labels are available by allowing models to supplement their training with unlabeled data [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "SSDKL incorporates unlabeled training data by minimizing predictive variance in the posterior regularization framework, a flexible way of encoding prior knowledge in Bayesian models [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "We demonstrate that SSDKL can use unlabeled data to learn more generalizable features and improve performance on a range of regression tasks, outperforming the supervised deep kernel learning method and semi-supervised methods such as virtual adversarial training (VAT) and mean teacher [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "We introduce semi-supervised deep kernel learning (SSDKL) for problems where labeled data is limited but unlabeled data is plentiful.",
        "Since the deep kernel parameters are jointly learned, the neural net is encouraged to learn a feature representation in which the unlabeled examples are closer to the labeled examples, thereby reducing the variance on our predictions.",
        "The model is discouraged from learning features from labeled data that are not useful for making low-variance predictions at unlabeled data points.",
        "Training and inference Semi-supervised deep kernel learning scales well with large amounts of unlabeled data since the unsupervised objective Lvariance naturally decomposes into a sum over conditionally independent terms.",
        "Existing approximation methods based on kernel interpolation and structured matrices used in DKL can be directly incorporated in SSDKL and would reduce the training complexity to close to linear in labeled dataset size and inference to constant time per test point [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "In addition to the supervised DKL method, we compare against semi-supervised methods including co-training, consistency regularization, generative modeling, and label propagation.",
        "Following [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], we use this same base model for all deep models, including SSDKL, DKL, VAT, mean teacher, and the VAE encoder, in order to make results comparable across methods.",
        "Representation learning To gain some intuition about how the unlabeled data helps in the learning process, we visualize the neural network embeddings learned by the DKL and SSDKL models on the Skillcraft dataset.",
        "Label propagation is subject to memory constraints since it forms a kernel matrix over all data points, requiring quadratic space in general, sparser graph structures can reduce this to a linear scaling.",
        "In experiments with UCI datasets and a real-world poverty prediction task, we find that minimizing posterior variance can be an effective way to incorporate unlabeled data when labeled training data is scarce.",
        "While our focus is on regression problems, we believe the SSDKL framework is applicable to classification problems\u2014we leave this to future work"
    ],
    "headline": "We present semi-supervised deep kernel learning , a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "2",
            "entry": "[2] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 353(6301):790\u2013 794, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jean%2C%20Neal%20Burke%2C%20Marshall%20Michael%20Xie%2C%20W.Matthew%20Davis%20Lobell%2C%20David%20B.%20Combining%20satellite%20imagery%20and%20machine%20learning%20to%20predict%20poverty%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jean%2C%20Neal%20Burke%2C%20Marshall%20Michael%20Xie%2C%20W.Matthew%20Davis%20Lobell%2C%20David%20B.%20Combining%20satellite%20imagery%20and%20machine%20learning%20to%20predict%20poverty%202016"
        },
        {
            "id": "3",
            "entry": "[3] Jiaxuan You, Xiaocheng Li, Melvin Low, David Lobell, and Stefano Ermon. Deep gaussian process for crop yield prediction based on remote sensing data. In AAAI, pages 4559\u20134566, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=You%2C%20Jiaxuan%20Li%2C%20Xiaocheng%20Low%2C%20Melvin%20Lobell%2C%20David%20Deep%20gaussian%20process%20for%20crop%20yield%20prediction%20based%20on%20remote%20sensing%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=You%2C%20Jiaxuan%20Li%2C%20Xiaocheng%20Low%2C%20Melvin%20Lobell%2C%20David%20Deep%20gaussian%20process%20for%20crop%20yield%20prediction%20based%20on%20remote%20sensing%20data%202017"
        },
        {
            "id": "4",
            "entry": "[4] Barak Oshri, Annie Hu, Peter Adelson, Xiao Chen, Pascaline Dupas, Jeremy Weinstein, Marshall Burke, David Lobell, and Stefano Ermon. Infrastructure quality assessment in africa using satellite imagery and deep learning. Proc. 24th ACM SIGKDD Conference, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oshri%2C%20Barak%20Hu%2C%20Annie%20Adelson%2C%20Peter%20Chen%2C%20Xiao%20Infrastructure%20quality%20assessment%20in%20africa%20using%20satellite%20imagery%20and%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oshri%2C%20Barak%20Hu%2C%20Annie%20Adelson%2C%20Peter%20Chen%2C%20Xiao%20Infrastructure%20quality%20assessment%20in%20africa%20using%20satellite%20imagery%20and%20deep%20learning%202018"
        },
        {
            "id": "5",
            "entry": "[5] Xiaojin Zhu and Andrew B Goldberg. Introduction to semi-supervised learning. Synthesis lectures on artificial intelligence and machine learning, 3(1):1\u2013130, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Xiaojin%20Goldberg%2C%20Andrew%20B.%20Introduction%20to%20semi-supervised%20learning.%20Synthesis%20lectures%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Xiaojin%20Goldberg%2C%20Andrew%20B.%20Introduction%20to%20semi-supervised%20learning.%20Synthesis%20lectures%202009"
        },
        {
            "id": "6",
            "entry": "[6] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T approach to unsupervised domain adaptation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shu%2C%20Rui%20Bui%2C%20Hung%20H.%20Narui%2C%20Hirokazu%20Ermon%2C%20Stefano%20A%20DIRT-T%20approach%20to%20unsupervised%20domain%20adaptation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shu%2C%20Rui%20Bui%2C%20Hung%20H.%20Narui%2C%20Hirokazu%20Ermon%2C%20Stefano%20A%20DIRT-T%20approach%20to%20unsupervised%20domain%20adaptation%202018"
        },
        {
            "id": "7",
            "entry": "[7] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural information processing systems, pages 529\u2013536, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grandvalet%2C%20Yves%20Bengio%2C%20Yoshua%20Semi-supervised%20learning%20by%20entropy%20minimization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grandvalet%2C%20Yves%20Bengio%2C%20Yoshua%20Semi-supervised%20learning%20by%20entropy%20minimization%202004"
        },
        {
            "id": "8",
            "entry": "[8] Olivier Chapelle and Alexander Zien. Semi-supervised classification by low density separation. In AISTATS, pages 57\u201364, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Zien%2C%20Alexander%20Semi-supervised%20classification%20by%20low%20density%20separation%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Zien%2C%20Alexander%20Semi-supervised%20classification%20by%20low%20density%20separation%202005"
        },
        {
            "id": "9",
            "entry": "[9] Aarti Singh, Robert Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesn\u2019t. In Advances in neural information processing systems, pages 1513\u20131520, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Aarti%20Nowak%2C%20Robert%20Zhu%2C%20Xiaojin%20Unlabeled%20data%3A%20Now%20it%20helps%2C%20now%20it%20doesn%E2%80%99t%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Aarti%20Nowak%2C%20Robert%20Zhu%2C%20Xiaojin%20Unlabeled%20data%3A%20Now%20it%20helps%2C%20now%20it%20doesn%E2%80%99t%202009"
        },
        {
            "id": "10",
            "entry": "[10] Volodymyr Kuleshov and Stefano Ermon. Deep hybrid models: Bridging discriminative and generative approaches. In Proceedings of the Conference on Uncertainty in AI (UAI), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuleshov%2C%20Volodymyr%20Ermon%2C%20Stefano%20Deep%20hybrid%20models%3A%20Bridging%20discriminative%20and%20generative%20approaches%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuleshov%2C%20Volodymyr%20Ermon%2C%20Stefano%20Deep%20hybrid%20models%3A%20Bridging%20discriminative%20and%20generative%20approaches%202017"
        },
        {
            "id": "11",
            "entry": "[11] Russell Ren, Hongyu Stewart, Jiaming Song, Volodymyr Kuleshov, and Stefano Ermon. Adversarial constraint learning for structured prediction. Proc. 27th International Joint Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Russell%20Stewart%2C%20Hongyu%20Song%2C%20Jiaming%20Kuleshov%2C%20Volodymyr%20Adversarial%20constraint%20learning%20for%20structured%20prediction%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Russell%20Stewart%2C%20Hongyu%20Song%2C%20Jiaming%20Kuleshov%2C%20Volodymyr%20Adversarial%20constraint%20learning%20for%20structured%20prediction%202018"
        },
        {
            "id": "12",
            "entry": "[12] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. The Journal of Machine Learning Research, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20Xing%2C%20Eric%20P.%20Deep%20kernel%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20Xing%2C%20Eric%20P.%20Deep%20kernel%20learning%202015"
        },
        {
            "id": "13",
            "entry": "[13] Stephan Eissman and Stefano Ermon. Bayesian optimization and attribute adjustment. Proc. 34th Conference on Uncertainty in Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eissman%2C%20Stephan%20Ermon%2C%20Stefano%20Bayesian%20optimization%20and%20attribute%20adjustment%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eissman%2C%20Stephan%20Ermon%2C%20Stefano%20Bayesian%20optimization%20and%20attribute%20adjustment%202018"
        },
        {
            "id": "14",
            "entry": "[14] Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, et al. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11(Jul):2001\u20132049, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganchev%2C%20Kuzman%20Gillenwater%2C%20Jennifer%20Taskar%2C%20Ben%20Posterior%20regularization%20for%20structured%20latent%20variable%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganchev%2C%20Kuzman%20Gillenwater%2C%20Jennifer%20Taskar%2C%20Ben%20Posterior%20regularization%20for%20structured%20latent%20variable%20models%202010"
        },
        {
            "id": "15",
            "entry": "[15] Jun Zhu, Ning Chen, and Eric P Xing. Bayesian inference with posterior regularization and applications to infinite latent svms. Journal of Machine Learning Research, 15(1):1799\u20131847, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun%20Chen%2C%20Ning%20and%20Eric%20P%20Xing.%20Bayesian%20inference%20with%20posterior%20regularization%20and%20applications%20to%20infinite%20latent%20svms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun%20Chen%2C%20Ning%20and%20Eric%20P%20Xing.%20Bayesian%20inference%20with%20posterior%20regularization%20and%20applications%20to%20infinite%20latent%20svms%202014"
        },
        {
            "id": "16",
            "entry": "[16] Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon. Amortized inference regularization. NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shu%2C%20Rui%20Bui%2C%20Hung%20H.%20Zhao%2C%20Shengjia%20Kochenderfer%2C%20Mykel%20J.%20Amortized%20inference%20regularization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shu%2C%20Rui%20Bui%2C%20Hung%20H.%20Zhao%2C%20Shengjia%20Kochenderfer%2C%20Mykel%20J.%20Amortized%20inference%20regularization%202018"
        },
        {
            "id": "17",
            "entry": "[17] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00677"
        },
        {
            "id": "18",
            "entry": "[18] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1195\u20131204. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tarvainen%2C%20Antti%20Valpola%2C%20Harri%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tarvainen%2C%20Antti%20Valpola%2C%20Harri%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017"
        },
        {
            "id": "19",
            "entry": "[19] Andrew Arnold, Ramesh Nallapati, and William W. Cohen. A comparative study of methods for transductive transfer learning. Proc. Seventh IEEE Int\u2019,l Conf. Data Mining Workshops, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arnold%2C%20Andrew%20Nallapati%2C%20Ramesh%20Cohen%2C%20William%20W.%20A%20comparative%20study%20of%20methods%20for%20transductive%20transfer%20learning%202007"
        },
        {
            "id": "20",
            "entry": "[20] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 370\u2013378, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Ruslan%20Salakhutdinov%2C%20and%20Eric%20P%20Xing.%20Deep%20kernel%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20Gordon%20Hu%2C%20Zhiting%20Ruslan%20Salakhutdinov%2C%20and%20Eric%20P%20Xing.%20Deep%20kernel%20learning%202016"
        },
        {
            "id": "21",
            "entry": "[21] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning. The MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20Carl%20Edward%20Williams%2C%20Christopher%20K.I.%20Gaussian%20processes%20for%20machine%20learning%202006"
        },
        {
            "id": "22",
            "entry": "[22] Andrew Gordon Wilson and Hannes Nickisch. Kernel interpolation for scalable structured gaussian processes (KISS-GP). In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 1775\u20131784, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20Gordon%20Nickisch%2C%20Hannes%20Kernel%20interpolation%20for%20scalable%20structured%20gaussian%20processes%20%28KISS-GP%29%202015-07-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20Gordon%20Nickisch%2C%20Hannes%20Kernel%20interpolation%20for%20scalable%20structured%20gaussian%20processes%20%28KISS-GP%29%202015-07-06"
        },
        {
            "id": "23",
            "entry": "[23] M. Lichman. UCI machine learning repository, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lichman%2C%20M.%20UCI%20machine%20learning%20repository%202013"
        },
        {
            "id": "24",
            "entry": "[24] Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Transfer learning from deep features for remote sensing and poverty mapping. AAAI Conference on Artificial Intelligence, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Michael%20Jean%2C%20Neal%20Burke%2C%20Marshall%20Lobell%2C%20David%20Transfer%20learning%20from%20deep%20features%20for%20remote%20sensing%20and%20poverty%20mapping%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Michael%20Jean%2C%20Neal%20Burke%2C%20Marshall%20Lobell%2C%20David%20Transfer%20learning%20from%20deep%20features%20for%20remote%20sensing%20and%20poverty%20mapping%202016"
        },
        {
            "id": "25",
            "entry": "[25] Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "26",
            "entry": "[26] Zhi-Hua Zhou and Ming Li. Semi-supervised regression with co-training. In IJCAI, volume 5, pages 908\u2013913, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Zhi-Hua%20Li%2C%20Ming%20Semi-supervised%20regression%20with%20co-training%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Zhi-Hua%20Li%2C%20Ming%20Semi-supervised%20regression%20with%20co-training%202005"
        },
        {
            "id": "27",
            "entry": "[27] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92\u2013100. ACM, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Mitchell%2C%20Tom%20Combining%20labeled%20and%20unlabeled%20data%20with%20co-training%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Mitchell%2C%20Tom%20Combining%20labeled%20and%20unlabeled%20data%20with%20co-training%201998"
        },
        {
            "id": "28",
            "entry": "[28] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. ICLR 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20Samuli%20Aila%2C%20Timo%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20Samuli%20Aila%2C%20Timo%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "29",
            "entry": "[29] Augustus Odena, Avital Oliver, Colin Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of semi-supervised learning algorithms. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Oliver%2C%20Avital%20Raffel%2C%20Colin%20Cubuk%2C%20Ekin%20Dogus%20Realistic%20evaluation%20of%20semi-supervised%20learning%20algorithms%202018"
        },
        {
            "id": "30",
            "entry": "[30] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03976"
        },
        {
            "id": "31",
            "entry": "[31] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "32",
            "entry": "[32] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical report, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Learning%20from%20labeled%20and%20unlabeled%20data%20with%20label%20propagation%202002"
        },
        {
            "id": "33",
            "entry": "[33] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "34",
            "entry": "[34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "35",
            "entry": "[35] Lars Maal\u00f8e, Casper Kaae S\u00f8nderby, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Auxiliary deep generative models. arXiv preprint arXiv:1602.05473, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.05473"
        },
        {
            "id": "36",
            "entry": "[36] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semisupervised learning with ladder networks. In Advances in Neural Information Processing Systems, pages 3546\u20133554, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semisupervised%20learning%20with%20ladder%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semisupervised%20learning%20with%20ladder%20networks%202015"
        },
        {
            "id": "37",
            "entry": "[37] Andreas C. Damianou and Neil D. Lawrence. Deep gaussian processes. The Journal of Machine Learning Research, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Damianou%2C%20Andreas%20C.%20Lawrence%2C%20Neil%20D.%20Deep%20gaussian%20processes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Damianou%2C%20Andreas%20C.%20Lawrence%2C%20Neil%20D.%20Deep%20gaussian%20processes%202013"
        },
        {
            "id": "38",
            "entry": "[38] Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic variational deep kernel learning. In Advances in Neural Information Processing Systems, pages 2586\u20132594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20G.%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20R.%20and%20Eric%20P%20Xing.%20Stochastic%20variational%20deep%20kernel%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20G.%20Hu%2C%20Zhiting%20Salakhutdinov%2C%20Ruslan%20R.%20and%20Eric%20P%20Xing.%20Stochastic%20variational%20deep%20kernel%20learning%202016"
        },
        {
            "id": "39",
            "entry": "[39] Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, and Eric P Xing. Learning scalable deep kernels with recurrent structure. arXiv preprint arXiv:1610.08936, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.08936"
        },
        {
            "id": "40",
            "entry": "[40] Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. The International Conference on Machine Learning (ICML), 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Kai%20Bi%2C%20Jinbo%20Tresp%2C%20Volker%20Active%20learning%20via%20transductive%20experimental%20design%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Kai%20Bi%2C%20Jinbo%20Tresp%2C%20Volker%20Active%20learning%20via%20transductive%20experimental%20design%202006"
        },
        {
            "id": "41",
            "entry": "[41] Chenyang Zhao and Shaodan Zhai. Minimum variance semi-supervised boosting for multilabel classification. In 2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pages 1342\u20131346. IEEE, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Chenyang%20Zhai%2C%20Shaodan%20Minimum%20variance%20semi-supervised%20boosting%20for%20multilabel%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Chenyang%20Zhai%2C%20Shaodan%20Minimum%20variance%20semi-supervised%20boosting%20for%20multilabel%20classification%202015"
        }
    ]
}
