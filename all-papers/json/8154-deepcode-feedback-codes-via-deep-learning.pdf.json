{
    "filename": "8154-deepcode-feedback-codes-via-deep-learning.pdf",
    "metadata": {
        "title": "Deepcode: Feedback Codes via Deep Learning",
        "author": "Hyeji Kim, Yihan Jiang, Sreeram Kannan, Sewoong Oh, Pramod Viswanath",
        "date": 1948,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8154-deepcode-feedback-codes-via-deep-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wideranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed. We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability. We also demonstrate several desirable properties in the codes: (a) generalization to larger block lengths; (b) composability with known codes; (c) adaptation to practical constraints. This result also presents broader ramifications to coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel-specific information-theoretic insights, can potentially beat state-of-the-art codes, constructed over decades of mathematical research."
    },
    "keywords": [
        {
            "term": "mathematical research",
            "url": "https://en.wikipedia.org/wiki/mathematical_research"
        },
        {
            "term": "channel model",
            "url": "https://en.wikipedia.org/wiki/channel_model"
        },
        {
            "term": "wireless communication",
            "url": "https://en.wikipedia.org/wiki/wireless_communication"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "bit error rate",
            "url": "https://en.wikipedia.org/wiki/bit_error_rate"
        },
        {
            "term": "Signal-to-Noise Ratio",
            "url": "https://en.wikipedia.org/wiki/Signal-To-Noise_Ratio"
        },
        {
            "term": "output feedback",
            "url": "https://en.wikipedia.org/wiki/output_feedback"
        },
        {
            "term": "digital communication",
            "url": "https://en.wikipedia.org/wiki/digital_communication"
        },
        {
            "term": "information theory",
            "url": "https://en.wikipedia.org/wiki/information_theory"
        },
        {
            "term": "block length",
            "url": "https://en.wikipedia.org/wiki/block_length"
        },
        {
            "term": "gaussian noise",
            "url": "https://en.wikipedia.org/wiki/gaussian_noise"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "open problem",
            "url": "https://en.wikipedia.org/wiki/open_problem"
        }
    ],
    "highlights": [
        "The ubiquitous digital communication enabled via wireless (e.g. WiFi, mobile, satellite) and wired media has been the plumbing underlying the current information age",
        "We demonstrate first family of codes obtained via deep learning which beats state-of-the-art codes, signaling a potential shift in code design, which historically has been driven by individual human ingenuity with sporadic progress over the decades",
        "We show sRthtaNatteN(-aoef)n-Dtchoeede-eapdrctoosducehtpeaumntdeinsististnevacadhraionafnnitetsltshrawatwiathloluontwopisusyta,faKened-dsblteeaapcrknd;ieytbehydisaflReleoNdwNbianecgnktchaoerdereemrc, eowirveeearrectlhoiiaefbevleeedatbhfaaucnrkththaener improvement in reliability, demonstrating the power of encoding in the feedback link; (c) Deepcode concatenated with turbo code achieves superior error rate decay as block length increases with noisy feedback",
        "We show that Deepcode trained under AWGN channels with noisy output feedback, achieves a significantly smaller bit error rate than both Schalkwijk and Kailath and C-L schemes under AWGN channels with noisy output feedback",
        "In this paper we have shown that appropriately designed and trained recurrent neural network codes, which we call Deepcode, outperform the state-of-the-art codes by a significant margin on the challenging problem of communicating over AWGN channels with noisy output feedback, both on the theoretical model and with practical considerations taken into account",
        "The encoding and decoding capabilities of the recurrent neural network architectures suggest that new codes could be found in other open problems in information theory, where practical codes are sorely missing"
    ],
    "key_statements": [
        "The ubiquitous digital communication enabled via wireless (e.g. WiFi, mobile, satellite) and wired media has been the plumbing underlying the current information age",
        "There are different models for channels with feedback, and among them, the AWGN channel with output feedback is a model that captures the essence of channels with feedback; this model is classical, introduced by Shannon in 1956 [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We demonstrate new neural network-driven encoders that operate significantly better (100\u20131000 times) than state of the art, on the AWGN channel with output feedback",
        "We show that architectural insights from simple communication channels with feedback when coupled with recurrent neural network architectures can discover novel codes",
        "We demonstrate first family of codes obtained via deep learning which beats state-of-the-art codes, signaling a potential shift in code design, which historically has been driven by individual human ingenuity with sporadic progress over the decades",
        "We demonstrate Deepcode \u2013 a new family of recurrent neural network-driven neural codes that have three orders of magnitude better reliability than state of the art with both noiseless and noisy feedback",
        "We propose representing the encoder and the decoder as recurrent neural network, training them jointly under AWGN channels with noisy feedback, and minimizing the error in decoding the information bits",
        "We considered so far the AWGN channel with noiseless output feedback with a unit time-step delay",
        "We show sRthtaNatteN(-aoef)n-Dtchoeede-eapdrctoosducehtpeaumntdeinsististnevacadhraionafnnitetsltshrawatwiathloluontwopisusyta,faKened-dsblteeaapcrknd;ieytbehydisaflReleoNdwNbianecgnktchaoerdereemrc, eowirveeearrectlhoiiaefbevleeedatbhfaaucnrkththaener improvement in reliability, demonstrating the power of encoding in the feedback link; (c) Deepcode concatenated with turbo code achieves superior error rate decay as block length increases with noisy feedback",
        "We show that Deepcode trained under AWGN channels with noisy output feedback, achieves a significantly smaller bit error rate than both Schalkwijk and Kailath and C-L schemes under AWGN channels with noisy output feedback",
        "Figure 4 demonstrates the powerful encoding of the received output, as learnt by the neural architecture; the bit error rate is improved two-three times",
        "We see that even with noisy feedback, bit error rate drops almost exponentially as block length increases, and the slope is sharper than the one for turbo codes",
        "In this paper we have shown that appropriately designed and trained recurrent neural network codes, which we call Deepcode, outperform the state-of-the-art codes by a significant margin on the challenging problem of communicating over AWGN channels with noisy output feedback, both on the theoretical model and with practical considerations taken into account",
        "The encoding and decoding capabilities of the recurrent neural network architectures suggest that new codes could be found in other open problems in information theory, where practical codes are sorely missing"
    ],
    "summary": [
        "The ubiquitous digital communication enabled via wireless (e.g. WiFi, mobile, satellite) and wired media has been the plumbing underlying the current information age.",
        "We propose representing the encoder and the decoder as RNNs, training them jointly under AWGN channels with noisy feedback, and minimizing the error in decoding the information bits.",
        "2K coded bits are generated based on the information bits and output feedback and sequentially transmitted.",
        "Encoder achieves performance close to Turbo code that does not use the feedback information at all as shown in Figure 2.",
        "For an AWGN channel without feedback, it is known that the optimal decoder under a symmetric code is robust to the distribution of noise [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]; the BER does not increase if we keep the power of noise and only change the distribution.",
        "We demonstrate the robustness of Deepcode under two variations on the feedback channel, noise and delay, and present generalization to longer block lengths.",
        "We show sRthtaNatteN(-aoef)n-Dtchoeede-eapdrctoosducehtpeaumntdeinsististnevacadhraionafnnitetsltshrawatwiathloluontwopisusyta,faKened-dsblteeaapcrknd;ieytbehydisaflReleoNdwNbianecgnktchaoerdereemrc, eowirveeearrectlhoiiaefbevleeedatbhfaaucnrkththaener improvement in reliability, demonstrating the power of encoding in the feedback link; (c) Deepcode concatenated with turbo code achieves superior error rate decay as block length increases with noisy feedback.",
        "In Figure 4 (Left), we plot the BER as a function of the feedback SNR for S-K scheme, C-L scheme, and Deepcode for a rate 1/3, 50 information bits, where we fix the forward channel SNR to be 0dB.",
        "Deepcode outperforms these two baseline codes by a large margin, with decaying error as feedback SNR increases, showing that Deepcode harnesses noisy feedback information to make communication more reliable.",
        "We propose using RNN as an encoder that maps noisy output to the transmitted feedback, with implementation details in Appendix B.5.",
        "Figure 4 demonstrates the powerful encoding of the received output, as learnt by the neural architecture; the BER is improved two-three times.",
        "In Figure 4 (Right), we plot the BERs of the concatenated code, under both noiseless and noisy feedback, and turbo code, both at rate 1/9 at SNR 6.5dB.",
        "We see that even with noisy feedback, BER drops almost exponentially as block length increases, and the slope is sharper than the one for turbo codes.",
        "In this paper we have shown that appropriately designed and trained RNN codes, which we call Deepcode, outperform the state-of-the-art codes by a significant margin on the challenging problem of communicating over AWGN channels with noisy output feedback, both on the theoretical model and with practical considerations taken into account.",
        "By concatenating Deepcode with a traditional outer code, the BER curve drops significantly with increasing block lengths, allowing generalizations of the learned neural network architectures.",
        "The encoding and decoding capabilities of the RNN architectures suggest that new codes could be found in other open problems in information theory, where practical codes are sorely missing"
    ],
    "headline": "We present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. E. Shannon, \u201cA mathematical theory of communication, part i, part ii,\u201d Bell Syst. Tech. J., vol. 27, pp. 623\u2013656, 1948.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shannon%2C%20C.E.%20%E2%80%9CA%20mathematical%20theory%201948",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shannon%2C%20C.E.%20%E2%80%9CA%20mathematical%20theory%201948"
        },
        {
            "id": "2",
            "entry": "[2] C. J. Il, J. Mayank, S. Kannan, L. Philip, and K. Sachin, \u201cAchieving single channel, full duplex wireless communication,\u201d in Proceedings of the 16th Annual International Conference on Mobile Computing and Networking, MOBICOM 2010, Chicago, Illinois, USA, September 20-24, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Il%2C%20C.J.%20Mayank%2C%20J.%20Kannan%2C%20S.%20Philip%2C%20L.%20Achieving%20single%20channel%2C%20full%20duplex%20wireless%20communication%2C%202010-09-20",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Il%2C%20C.J.%20Mayank%2C%20J.%20Kannan%2C%20S.%20Philip%2C%20L.%20Achieving%20single%20channel%2C%20full%20duplex%20wireless%20communication%2C%202010-09-20"
        },
        {
            "id": "3",
            "entry": "[3] C. Shannon, \u201cThe zero error capacity of a noisy channel,\u201d IRE Transactions on Information Theory, vol. 2, no. 3, pp. 8\u201319, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shannon%2C%20C.%20The%20zero%20error%20capacity%20of%20a%20noisy%20channel%2C%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shannon%2C%20C.%20The%20zero%20error%20capacity%20of%20a%20noisy%20channel%2C%201956"
        },
        {
            "id": "4",
            "entry": "[4] J. Schalkwijk and T. Kailath, \u201cA coding scheme for additive noise channels with feedback\u2013i: No bandwidth constraint,\u201d IEEE Transactions on Information Theory, vol. 12, no. 2, pp. 172\u2013182, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schalkwijk%2C%20J.%20Kailath%2C%20T.%20A%20coding%20scheme%20for%20additive%20noise%20channels%20with%20feedback%E2%80%93i%3A%20No%20bandwidth%20constraint%2C%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schalkwijk%2C%20J.%20Kailath%2C%20T.%20A%20coding%20scheme%20for%20additive%20noise%20channels%20with%20feedback%E2%80%93i%3A%20No%20bandwidth%20constraint%2C%201966"
        },
        {
            "id": "5",
            "entry": "[5] J. Schalkwijk, \u201cA coding scheme for additive noise channels with feedback\u2013ii: Band-limited signals,\u201d IEEE Transactions on Information Theory, vol. 12, no. 2, pp. 183\u2013189, April 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schalkwijk%2C%20J.%20A%20coding%20scheme%20for%20additive%20noise%20channels%20with%20feedback%E2%80%93ii%3A%20Band-limited%20signals%2C%201966-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schalkwijk%2C%20J.%20A%20coding%20scheme%20for%20additive%20noise%20channels%20with%20feedback%E2%80%93ii%3A%20Band-limited%20signals%2C%201966-04"
        },
        {
            "id": "6",
            "entry": "[6] R. G. Gallager and B. Nakiboglu, \u201cVariations on a theme by schalkwijk and kailath,\u201d IEEE Transactions on Information Theory, vol. 56, no. 1, pp. 6\u201317, Jan 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gallager%2C%20R.G.%20Nakiboglu%2C%20B.%20Variations%20on%20a%20theme%20by%20schalkwijk%20and%20kailath%2C%202010-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gallager%2C%20R.G.%20Nakiboglu%2C%20B.%20Variations%20on%20a%20theme%20by%20schalkwijk%20and%20kailath%2C%202010-01"
        },
        {
            "id": "7",
            "entry": "[7] Z. Chance and D. J. Love, \u201cConcatenated coding for the AWGN channel with noisy feedback,\u201d IEEE Transactions on Information Theory, vol. 57, no. 10, pp. 6633\u20136649, Oct 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chance%2C%20Z.%20Love%2C%20D.J.%20Concatenated%20coding%20for%20the%20AWGN%20channel%20with%20noisy%20feedback%2C%202011-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chance%2C%20Z.%20Love%2C%20D.J.%20Concatenated%20coding%20for%20the%20AWGN%20channel%20with%20noisy%20feedback%2C%202011-10"
        },
        {
            "id": "8",
            "entry": "[8] Y.-H. Kim, A. Lapidoth, and T. Weissman, \u201cThe Gaussian channel with noisy feedback,\u201d in Information Theory, 2007. ISIT 2007. IEEE International Symposium on. IEEE, 2007, pp. 1416\u20131420.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.-H.%20Lapidoth%2C%20A.%20Weissman%2C%20T.%20%E2%80%9CThe%20Gaussian%20channel%20with%20noisy%20feedback%2C%E2%80%9D%20in%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.-H.%20Lapidoth%2C%20A.%20Weissman%2C%20T.%20%E2%80%9CThe%20Gaussian%20channel%20with%20noisy%20feedback%2C%E2%80%9D%20in%202007"
        },
        {
            "id": "9",
            "entry": "[9] P. Elias, \u201cCoding for noisy channels,\u201d in IRE Convention record, vol. 4, 1955, pp. 37\u201346.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elias%2C%20P.%20%E2%80%9CCoding%20for%20noisy%20channels%2C%E2%80%9D%20in%20IRE%20Convention",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elias%2C%20P.%20%E2%80%9CCoding%20for%20noisy%20channels%2C%E2%80%9D%20in%20IRE%20Convention"
        },
        {
            "id": "10",
            "entry": "[10] T. J. O\u2019Shea and J. Hoydis, \u201cAn introduction to machine learning communications systems,\u201d arXiv preprint arXiv:1702.00832, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.00832"
        },
        {
            "id": "11",
            "entry": "[11] A. Felix, S. Cammerer, S. D\u00f6rner, J. Hoydis, and S. t. Brink, \u201cOfdm-autoencoder for end-to-end learning of communications systems,\u201d arXiv preprint arXiv:1803.05815, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05815"
        },
        {
            "id": "12",
            "entry": "[12] S. Cammerer, S. D\u00f6rner, J. Hoydis, and S. ten Brink, \u201cEnd-to-end learning for physical layer communications,\u201d in The International Zurich Seminar on Information and Communication (IZS 2018) Proceedings. ETH Zurich, 2018, pp. 51\u201352.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cammerer%2C%20S.%20D%C3%B6rner%2C%20S.%20Hoydis%2C%20J.%20ten%20Brink%2C%20S.%20End-to-end%20learning%20for%20physical%20layer%20communications%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cammerer%2C%20S.%20D%C3%B6rner%2C%20S.%20Hoydis%2C%20J.%20ten%20Brink%2C%20S.%20End-to-end%20learning%20for%20physical%20layer%20communications%2C%202018"
        },
        {
            "id": "13",
            "entry": "[13] T. J. O\u2019Shea, T. Erpek, and T. C. Clancy, \u201cDeep learning based MIMO communications,\u201d CoRR, vol. abs/1707.07980, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07980"
        },
        {
            "id": "14",
            "entry": "[14] N. Farsad, M. Rao, and A. Goldsmith, \u201cDeep learning for joint source-channel coding of text,\u201d IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farsad%2C%20N.%20Rao%2C%20M.%20Goldsmith%2C%20A.%20Deep%20learning%20for%20joint%20source-channel%20coding%20of%20text%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farsad%2C%20N.%20Rao%2C%20M.%20Goldsmith%2C%20A.%20Deep%20learning%20for%20joint%20source-channel%20coding%20of%20text%2C%202018"
        },
        {
            "id": "15",
            "entry": "[15] H. Kim, Y. Jiang, R. Rana, S. Kannan, S. Oh, and P. Viswanath, \u201cCommunication algorithms via deep learning,\u201d in The International Conference on Representation Learning (ICLR 2018) Proceedings. Vancouver, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20H.%20Jiang%2C%20Y.%20Rana%2C%20R.%20Kannan%2C%20S.%20Communication%20algorithms%20via%20deep%20learning%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20H.%20Jiang%2C%20Y.%20Rana%2C%20R.%20Kannan%2C%20S.%20Communication%20algorithms%20via%20deep%20learning%2C%202018"
        },
        {
            "id": "16",
            "entry": "[16] E. Nachmani, Y. Be\u2019ery, and D. Burshtein, \u201cLearning to decode linear codes using deep learning,\u201d in Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on. IEEE, 2016, pp. 341\u2013346.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=E%20Nachmani%20Y%20Beery%20and%20D%20Burshtein%20Learning%20to%20decode%20linear%20codes%20using%20deep%20learning%20in%20Communication%20Control%20and%20Computing%20Allerton%202016%2054th%20Annual%20Allerton%20Conference%20on%20IEEE%202016%20pp%20341346",
            "oa_query": "https://api.scholarcy.com/oa_version?query=E%20Nachmani%20Y%20Beery%20and%20D%20Burshtein%20Learning%20to%20decode%20linear%20codes%20using%20deep%20learning%20in%20Communication%20Control%20and%20Computing%20Allerton%202016%2054th%20Annual%20Allerton%20Conference%20on%20IEEE%202016%20pp%20341346"
        },
        {
            "id": "17",
            "entry": "[17] E. Nachmani, E. Marciano, L. Lugosch, W. J. Gross, D. Burshtein, and Y. Be\u2019ery, \u201cDeep learning methods for improved decoding of linear codes,\u201d IEEE Journal of Selected Topics in Signal Processing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nachmani%2C%20E.%20Marciano%2C%20E.%20Lugosch%2C%20L.%20Gross%2C%20W.J.%20Deep%20learning%20methods%20for%20improved%20decoding%20of%20linear%20codes%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nachmani%2C%20E.%20Marciano%2C%20E.%20Lugosch%2C%20L.%20Gross%2C%20W.J.%20Deep%20learning%20methods%20for%20improved%20decoding%20of%20linear%20codes%2C%202018"
        },
        {
            "id": "18",
            "entry": "[18] X. Tan, W. Xu, Y. Be\u2019ery, Z. Zhang, X. You, and C. Zhang, \u201cImproving massive MIMO belief propagation detector with deep neural network,\u201d arXiv preprint arXiv:1804.01002, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01002"
        },
        {
            "id": "19",
            "entry": "[19] J. Seo, J. Lee, and K. Kim, \u201cDecoding of polar code by using deep feed-forward neural networks,\u201d March 2018, pp. 238\u2013242.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20J.%20Lee%2C%20J.%20Kim%2C%20K.%20Decoding%20of%20polar%20code%20by%20using%20deep%20feed-forward%20neural%20networks%2C%202018-03"
        },
        {
            "id": "20",
            "entry": "[20] J. Kosaian, K. Rashmi, and S. Venkataraman, \u201cLearning a code: Machine learning for approximate non-linear coded computation,\u201d arXiv preprint arXiv:1806.01259, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01259"
        },
        {
            "id": "21",
            "entry": "[21] J. Zhao and Z. Gao, \u201cResearch on the blind equalization technology based on the complex BP neural network with tunable activation functions,\u201d March 2017, pp. 813\u2013817.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20J.%20Gao%2C%20Z.%20Research%20on%20the%20blind%20equalization%20technology%20based%20on%20the%20complex%20BP%20neural%20network%20with%20tunable%20activation%20functions%2C%202017-03"
        },
        {
            "id": "22",
            "entry": "[22] N. Farsad and A. Goldsmith, \u201cNeural network detection of data sequences in communication systems,\u201d IEEE Transactions on Signal Processing, vol. PP, January 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farsad%2C%20N.%20Goldsmith%2C%20A.%20Neural%20network%20detection%20of%20data%20sequences%20in%20communication%20systems%2C%202018-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farsad%2C%20N.%20Goldsmith%2C%20A.%20Neural%20network%20detection%20of%20data%20sequences%20in%20communication%20systems%2C%202018-01"
        },
        {
            "id": "23",
            "entry": "[23] H. He, C. Wen, S. Jin, and G. Y. Li, \u201cDeep learning-based channel estimation for beamspace mmWave massive MIMO systems,\u201d CoRR, vol. abs/1802.01290, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01290"
        },
        {
            "id": "24",
            "entry": "[24] H. Sun, X. Chen, M. Hong, Q. Shi, X. Fu, and N. D. Sidiropoulos, \u201cLearning to optimize: Training deep neural networks for interference management,\u201d IEEE Transactions on Signal Processing, August 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20H.%20Chen%2C%20X.%20Hong%2C%20M.%20Shi%2C%20Q.%20Learning%20to%20optimize%3A%20Training%20deep%20neural%20networks%20for%20interference%20management%2C%202018-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20H.%20Chen%2C%20X.%20Hong%2C%20M.%20Shi%2C%20Q.%20Learning%20to%20optimize%3A%20Training%20deep%20neural%20networks%20for%20interference%20management%2C%202018-08"
        },
        {
            "id": "25",
            "entry": "[25] C.-K. Wen, W.-T. Shih, and S. Jin, \u201cDeep learning for massive MIMO CSI feedback,\u201d IEEE Wireless Communications Letters, December 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20C.-K.%20Shih%2C%20W.-T.%20Jin%2C%20S.%20Deep%20learning%20for%20massive%20MIMO%20CSI%20feedback%2C%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20C.-K.%20Shih%2C%20W.-T.%20Jin%2C%20S.%20Deep%20learning%20for%20massive%20MIMO%20CSI%20feedback%2C%202017-12"
        },
        {
            "id": "26",
            "entry": "[26] T. J. O\u2019Shea, K. Karra, and T. C. Clancy, \u201cLearning to communicate: Channel auto-encoders, domain specific regularizers, and attention,\u201d in Signal Processing and Information Technology (ISSPIT), 2016 IEEE International Symposium on. IEEE, 2016, pp. 223\u2013228.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Shea%2C%20T.J.%20Karra%2C%20K.%20Clancy%2C%20T.C.%20Learning%20to%20communicate%3A%20Channel%20auto-encoders%2C%20domain%20specific%20regularizers%2C%20and%20attention%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=O%E2%80%99Shea%2C%20T.J.%20Karra%2C%20K.%20Clancy%2C%20T.C.%20Learning%20to%20communicate%3A%20Channel%20auto-encoders%2C%20domain%20specific%20regularizers%2C%20and%20attention%2C%202016"
        },
        {
            "id": "27",
            "entry": "[27] T. M. Cover and J. A. Thomas, Elements of information theory. John Wiley & Sons, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20T.M.%20Thomas%2C%20J.A.%20Elements%20of%20information%20theory%202012"
        },
        {
            "id": "28",
            "entry": "[28] Y. Polyanskiy, H. V. Poor, and S. Verd\u00fa, \u201cChannel coding rate in the finite blocklength regime,\u201d IEEE Transactions on Information Theory, vol. 56, no. 5, pp. 2307\u20132359, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyanskiy%2C%20Y.%20Poor%2C%20H.V.%20Verd%C3%BA%2C%20S.%20Channel%20coding%20rate%20in%20the%20finite%20blocklength%20regime%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyanskiy%2C%20Y.%20Poor%2C%20H.V.%20Verd%C3%BA%2C%20S.%20Channel%20coding%20rate%20in%20the%20finite%20blocklength%20regime%2C%202010"
        },
        {
            "id": "29",
            "entry": "[29] H. Huawei, \u201cPerformance evaluation of channel codes for control channel,\u201d 3GPP TSGRAN WG1 #87 Reno, U.S.A., November 14-18, 2016, vol. R1-1611257. [Online]. Available: www.3gpp.org/ftp/tsg_ran/WG1_RL1/TSGR1_87/Docs/R1-1611257.zip",
            "url": "http://www.3gpp.org/ftp/tsg_ran/WG1_RL1/TSGR1_87/Docs/R1-1611257.zip"
        },
        {
            "id": "30",
            "entry": "[30] T. Erseghe, \u201cOn the evaluation of the polyanskiy-poor-verdu converse bound for finite block-length coding in awgn,\u201d vol. 61, 01 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erseghe%2C%20T.%20On%20the%20evaluation%20of%20the%20polyanskiy-poor-verdu%20converse%20bound%20for%20finite%20block-length%20coding%20in%20awgn%2C%202014"
        },
        {
            "id": "31",
            "entry": "[31] Y. H. Kim, A. Lapidoth, and T. Weissman, \u201cThe gaussian channel with noisy feedback,\u201d in 2007 IEEE International Symposium on Information Theory, June 2007, pp. 1416\u20131420.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.H.%20Lapidoth%2C%20A.%20Weissman%2C%20T.%20The%20gaussian%20channel%20with%20noisy%20feedback%2C%202007-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.H.%20Lapidoth%2C%20A.%20Weissman%2C%20T.%20The%20gaussian%20channel%20with%20noisy%20feedback%2C%202007-06"
        },
        {
            "id": "32",
            "entry": "[32] T. Duman and M. Salehi, \u201cOn optimal power allocation for turbo codes,\u201d in IEEE International Symposium on Information Theory - Proceedings. IEEE, 1997, p. 104.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duman%2C%20T.%20Salehi%2C%20M.%20On%20optimal%20power%20allocation%20for%20turbo%20codes%2C%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duman%2C%20T.%20Salehi%2C%20M.%20On%20optimal%20power%20allocation%20for%20turbo%20codes%2C%201997"
        },
        {
            "id": "33",
            "entry": "[33] H. Qi, D. Malone, and V. Subramanian, \u201cDoes every bit need the same power? an investigation on unequal power allocation for irregular ldpc codes,\u201d in 2009 International Conference on Wireless Communications Signal Processing, Nov 2009, pp. 1\u20135.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20H.%20Malone%2C%20D.%20Subramanian%2C%20V.%20Does%20every%20bit%20need%20the%20same%20power%3F%20an%20investigation%20on%20unequal%20power%20allocation%20for%20irregular%20ldpc%20codes%2C%202009-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20H.%20Malone%2C%20D.%20Subramanian%2C%20V.%20Does%20every%20bit%20need%20the%20same%20power%3F%20an%20investigation%20on%20unequal%20power%20allocation%20for%20irregular%20ldpc%20codes%2C%202009-11"
        },
        {
            "id": "34",
            "entry": "[34] A. Lapidoth, \u201cNearest neighbor decoding for additive non-Gaussian noise channels,\u201d IEEE Transactions on Information Theory, vol. 42, no. 5, pp. 1520\u20131529, Sep 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lapidoth%2C%20A.%20Nearest%20neighbor%20decoding%20for%20additive%20non-Gaussian%20noise%20channels%2C%201996-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lapidoth%2C%20A.%20Nearest%20neighbor%20decoding%20for%20additive%20non-Gaussian%20noise%20channels%2C%201996-09"
        },
        {
            "id": "35",
            "entry": "[35] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "36",
            "entry": "[36] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, \u201cXnor-net: Imagenet classification using binary convolutional neural networks,\u201d in European Conference on Computer Vision. Springer, 2016, pp. 525\u2013542.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20M.%20Ordonez%2C%20V.%20Redmon%2C%20J.%20Farhadi%2C%20A.%20%E2%80%9CXnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%2C%E2%80%9D%20in%20European%20Conference%20on%20Computer%20Vision%202016"
        },
        {
            "id": "37",
            "entry": "[37] A. Ben-Yishai and O. Shayevitz, \u201cInteractive schemes for the awgn channel with noisy feedback,\u201d IEEE Transactions on Information Theory, vol. 63, no. 4, pp. 2409\u20132427, April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-Yishai%2C%20A.%20Shayevitz%2C%20O.%20Interactive%20schemes%20for%20the%20awgn%20channel%20with%20noisy%20feedback%2C%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-Yishai%2C%20A.%20Shayevitz%2C%20O.%20Interactive%20schemes%20for%20the%20awgn%20channel%20with%20noisy%20feedback%2C%202017-04"
        },
        {
            "id": "38",
            "entry": "[38] G. D. Forney, Jr, MIT Press, Cambridge, MA., 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forney%2C%20G.D.%20Jr%201966"
        },
        {
            "id": "39",
            "entry": "[39] K. Miwa, N. Miki, T. Kawamura, and M. Sawahashi, \u201cPerformance of decision-directed channel estimation using low-rate turbo codes for dft-precoded ofdma,\u201d in 2012 IEEE 75th Vehicular Technology Conference (VTC Spring), May 2012, pp. 1\u20135. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miwa%2C%20K.%20Miki%2C%20N.%20Kawamura%2C%20T.%20Sawahashi%2C%20M.%20Performance%20of%20decision-directed%20channel%20estimation%20using%20low-rate%20turbo%20codes%20for%20dft-precoded%20ofdma%2C%202012-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miwa%2C%20K.%20Miki%2C%20N.%20Kawamura%2C%20T.%20Sawahashi%2C%20M.%20Performance%20of%20decision-directed%20channel%20estimation%20using%20low-rate%20turbo%20codes%20for%20dft-precoded%20ofdma%2C%202012-05"
        }
    ]
}
