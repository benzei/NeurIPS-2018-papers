{
    "filename": "8062-almost-optimal-algorithms-for-linear-stochastic-bandits-with-heavy-tailed-payoffs.pdf",
    "metadata": {
        "title": "Almost Optimal Algorithms for Linear Stochastic Bandits with Heavy-Tailed Payoffs",
        "author": "Han Shao, Xiaotian Yu, Irwin King, Michael R. Lyu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8062-almost-optimal-algorithms-for-linear-stochastic-bandits-with-heavy-tailed-payoffs.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In linear stochastic bandits, it is commonly assumed that payoffs are with sub-\nGaussian noises. In this paper, under a weaker assumption on noises, we study the problem of linear stochastic bandits with heavy-tailed payoffs (LinBET), where the distributions have finite moments of order 1 + \u01eb, for some \u01eb \u2208 (0, 1]. We rigorously"
    },
    "keywords": [
        {
            "term": "resource allocation",
            "url": "https://en.wikipedia.org/wiki/resource_allocation"
        },
        {
            "term": "gaussian noise",
            "url": "https://en.wikipedia.org/wiki/gaussian_noise"
        },
        {
            "term": "linear mapping",
            "url": "https://en.wikipedia.org/wiki/linear_mapping"
        }
    ],
    "highlights": [
        "The decision-making model named Multi-Armed Bandits (MAB), where at each time step an algorithm chooses an arm among a given set of arms and receives a stochastic payoff with respect to the chosen arm, elegantly characterizes the tradeoff between exploration and exploitation in sequential learning",
        "A natural and important variant of Multi-Armed Bandits is linear stochastic bandits with the expected payoff of each arm satisfying a linear mapping from the arm information to a real number",
        "Both the algorithms adopt the optimism in the face of uncertainty principle, which is common in bandit problems (<a class=\"ref-link\" id=\"cAbbasi-Yadkori_et+al_2011_a\" href=\"#rAbbasi-Yadkori_et+al_2011_a\">Abbasi-Yadkori et al, 2011</a>; Munos et al, 2014)",
        "We develop two novel bandit algorithms, which are named as means under OFU and Truncation under OFU",
        "We evaluate our algorithms with other synthetic datasets, as well as different \u03bb and \u03b4, and observe similar superiority of means under OFU and Truncation under OFU",
        "We broke the traditional assumption of sub-Gaussian noises in payoffs of bandits, and derived theoretical guarantees based on the prior information of bounds on finite moments"
    ],
    "key_statements": [
        "The decision-making model named Multi-Armed Bandits (MAB), where at each time step an algorithm chooses an arm among a given set of arms and receives a stochastic payoff with respect to the chosen arm, elegantly characterizes the tradeoff between exploration and exploitation in sequential learning",
        "A natural and important variant of Multi-Armed Bandits is linear stochastic bandits with the expected payoff of each arm satisfying a linear mapping from the arm information to a real number",
        "Both the algorithms adopt the optimism in the face of uncertainty principle, which is common in bandit problems (<a class=\"ref-link\" id=\"cAbbasi-Yadkori_et+al_2011_a\" href=\"#rAbbasi-Yadkori_et+al_2011_a\">Abbasi-Yadkori et al, 2011</a>; Munos et al, 2014)",
        "We develop two novel bandit algorithms, which are named as means under OFU and Truncation under OFU",
        "Whe\u221an \u01eb is finite for LinBET, the algorithms by <a class=\"ref-link\" id=\"cMedina_2016_a\" href=\"#rMedina_2016_a\">Medina and Yang (2016</a>) cannot recover the bound of O( T ) which is the regret of the state-of-the-art algorithms in linear stochastic bandits under \u221athe sub-Gaussian assumption",
        "We notice that martingale differences to prove the lower bound for linear stochastic bandits in (Dani et al, 2008a) are not directly feasible for the proof of lower bound in LinBET, because under our construction of heavy-tailed payoffs (i.e., Eq (2)), the information of \u01eb is excluded",
        "We evaluate our algorithms with other synthetic datasets, as well as different \u03bb and \u03b4, and observe similar superiority of means under OFU and Truncation under OFU",
        "We have studied the problem of LinBET, where stochastic payoffs are characterized by finite (1 + \u01eb)-th moments with \u01eb \u2208 (0, 1]",
        "We broke the traditional assumption of sub-Gaussian noises in payoffs of bandits, and derived theoretical guarantees based on the prior information of bounds on finite moments"
    ],
    "summary": [
        "The decision-making model named Multi-Armed Bandits (MAB), where at each time step an algorithm chooses an arm among a given set of arms and receives a stochastic payoff with respect to the chosen arm, elegantly characterizes the tradeoff between exploration and exploitation in sequential learning.",
        "By adopting median of means with a well-designed allocation of decisions and truncation based on historical information, we develop two novel bandit algorithms, where the regret upper bounds match the lower bound up to polylogarithmic factors.",
        "We consider a general characterization of heavy-tailed payoffs in bandits, where the distributions have finite moments of order 1 + \u01eb, where \u01eb \u2208 (0, 1].",
        "For each round t = 1, \u00b7 \u00b7 \u00b7 , T , the bandit algorithm A is given a decision set Dt \u2286 Rd such that x 2 \u2264 D for any x \u2208 Dt. A has to choose an arm xt \u2208 Dt and observes a stochastic payoff yt.",
        "Given a decision set Dt for time step t = 1, \u00b7 \u00b7 \u00b7 , T , an algorithm A, of which the goal is to maximize cumulative payoffs over T rounds, chooses an arm xt \u2208 Dt. With Ft\u22121, the observed stochastic payoff yt is conditionally heavy-tailed, i.e., E |yt|1+\u01eb|Ft\u22121 \u2264 b or E |yt \u2212 xt, \u03b8\u2217 |1+\u01eb|Ft\u22121 \u2264 c, where \u01eb \u2208 (0, 1], and b, c \u2208 (0, +\u221e).",
        "Whe\u221an \u01eb is finite for LinBET, the algorithms by <a class=\"ref-link\" id=\"cMedina_2016_a\" href=\"#rMedina_2016_a\">Medina and Yang (2016</a>) cannot recover the bound of O( T ) which is the regret of the state-of-the-art algorithms in linear stochastic bandits under \u221athe sub-Gaussian assumption.",
        "We notice that martingale differences to prove the lower bound for linear stochastic bandits in (Dani et al, 2008a) are not directly feasible for the proof of lower bound in LinBET, because under our construction of heavy-tailed payoffs (i.e., Eq (2)), the information of \u01eb is excluded.",
        "For MENU, we adopt the assumption of heavy-tailed payoffs on central moments, which is required in the basic technique of median of means (<a class=\"ref-link\" id=\"cBubeck_et+al_2013_a\" href=\"#rBubeck_et+al_2013_a\">Bubeck et al, 2013</a>).",
        "We conduct experiments based on synthetic datasets to evaluate the performance of our proposed bandit algorithms: MENU and TOFU.",
        "To show effectiveness of bandit algorithms, we will demonstrate cumulative payoffs with respect to number of rounds for playing bandits over a fixed finite-arm decision set.",
        "For S1 and S2, which contain different numbers of arms and different dimensions for the contextual information, we adopt standard Student\u2019s t-distribution to generate heavy-tailed noises.",
        "We broke the traditional assumption of sub-Gaussian noises in payoffs of bandits, and derived theoretical guarantees based on the prior information of bounds on finite moments.",
        "We rigorously analyzed the lower bound of LinBET, and developed two novel bandit algorithms with regret upper bounds matching the lower bound up to polylogarithmic factors.",
        "Since both algorithms in this paper require a priori knowledge of \u01eb, future directions in this line of research include automatic learning of LinBET without information of distributional moments, and evaluation of our proposed algorithms in real-world scenarios"
    ],
    "headline": "Under a weaker assumption on noises, we study the problem of linear stochastic bandits with heavy-tailed payoffs , where the distributions have finite moments of order 1 + \u01eb, for some \u01eb \u2208 (0, 1]",
    "reference_links": [
        {
            "id": "Abbasi-Yadkori_et+al_2011_a",
            "entry": "Y. Abbasi-Yadkori, D. P\u00e1l, and C. Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312\u20132320, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Y.%20P%C3%A1l%2C%20D.%20Szepesv%C3%A1ri%2C%20C.%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Y.%20P%C3%A1l%2C%20D.%20Szepesv%C3%A1ri%2C%20C.%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011"
        },
        {
            "id": "Agrawal_1995_a",
            "entry": "R. Agrawal. Sample mean based index policies by O(log n) regret for the multi-armed bandit problem. Advances in Applied Probability, 27(4):1054\u20131078, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20R.%20Sample%20mean%20based%20index%20policies%20by%20O%28log%20n%29%20regret%20for%20the%20multi-armed%20bandit%20problem%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20R.%20Sample%20mean%20based%20index%20policies%20by%20O%28log%20n%29%20regret%20for%20the%20multi-armed%20bandit%20problem%201995"
        },
        {
            "id": "Agrawal_2012_a",
            "entry": "S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In Conference on Learning Theory, pages 39\u20131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20S.%20Goyal%2C%20N.%20Analysis%20of%20Thompson%20sampling%20for%20the%20multi-armed%20bandit%20problem%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20S.%20Goyal%2C%20N.%20Analysis%20of%20Thompson%20sampling%20for%20the%20multi-armed%20bandit%20problem%202012"
        },
        {
            "id": "Audibert_2011_a",
            "entry": "J.-Y. Audibert, O. Catoni, et al. Robust linear least squares regression. The Annals of Statistics, 39 (5):2766\u20132794, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.-Y.%20Catoni%2C%20O.%20Robust%20linear%20least%20squares%20regression%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.-Y.%20Catoni%2C%20O.%20Robust%20linear%20least%20squares%20regression%202011"
        },
        {
            "id": "Auer_2002_a",
            "entry": "P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Using%20confidence%20bounds%20for%20exploitation-exploration%20trade-offs%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Using%20confidence%20bounds%20for%20exploitation-exploration%20trade-offs%202002"
        },
        {
            "id": "Bubeck_2010_a",
            "entry": "S. Bubeck. Bandits games and clustering foundations. PhD thesis, Universit\u00e9 des Sciences et Technologie de Lille-Lille I, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Bandits%20games%20and%20clustering%20foundations%202010"
        },
        {
            "id": "Bubeck_2012_a",
            "entry": "S. Bubeck, N. Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "Bubeck_et+al_2013_a",
            "entry": "S. Bubeck, N. Cesa-Bianchi, and G. Lugosi. Bandits with heavy tail. IEEE Transactions on Information Theory, 59(11):7711\u20137717, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Lugosi%2C%20G.%20Bandits%20with%20heavy%20tail%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Lugosi%2C%20G.%20Bandits%20with%20heavy%20tail%202013"
        },
        {
            "id": "Carpentier_2014_a",
            "entry": "A. Carpentier and M. Valko. Extreme bandits. In Advances in Neural Information Processing Systems, pages 1089\u20131097, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carpentier%2C%20A.%20Valko%2C%20M.%20Extreme%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carpentier%2C%20A.%20Valko%2C%20M.%20Extreme%20bandits%202014"
        },
        {
            "id": "Chapelle_2011_a",
            "entry": "O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Advances in Neural Information Processing Systems, pages 2249\u20132257, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20O.%20Li%2C%20L.%20An%20empirical%20evaluation%20of%20Thompson%20sampling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20O.%20Li%2C%20L.%20An%20empirical%20evaluation%20of%20Thompson%20sampling%202011"
        },
        {
            "id": "Chu_et+al_2011_a",
            "entry": "W. Chu, L. Li, L. Reyzin, and R. Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208\u2013214, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chu%2C%20W.%20Li%2C%20L.%20Reyzin%2C%20L.%20Schapire%2C%20R.%20Contextual%20bandits%20with%20linear%20payoff%20functions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chu%2C%20W.%20Li%2C%20L.%20Reyzin%2C%20L.%20Schapire%2C%20R.%20Contextual%20bandits%20with%20linear%20payoff%20functions%202011"
        },
        {
            "id": "Cont_2000_a",
            "entry": "R. Cont and J.-P. Bouchaud. Herd behavior and aggregate fluctuations in financial markets. Macroeconomic Dynamics, 4(2):170\u2013196, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cont%2C%20R.%20Bouchaud%2C%20J.-P.%20Herd%20behavior%20and%20aggregate%20fluctuations%20in%20financial%20markets%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cont%2C%20R.%20Bouchaud%2C%20J.-P.%20Herd%20behavior%20and%20aggregate%20fluctuations%20in%20financial%20markets%202000"
        },
        {
            "id": "Dani_et+al_0000_a",
            "entry": "V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In Conference on Learning Theory, pages 355\u2013366, 2008a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dani%2C%20V.%20Hayes%2C%20T.P.%20Kakade%2C%20S.M.%20Stochastic%20linear%20optimization%20under%20bandit%20feedback",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dani%2C%20V.%20Hayes%2C%20T.P.%20Kakade%2C%20S.M.%20Stochastic%20linear%20optimization%20under%20bandit%20feedback"
        },
        {
            "id": "Dani_et+al_0000_b",
            "entry": "V. Dani, S. M. Kakade, and T. P. Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345\u2013352, 2008b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dani%2C%20V.%20Kakade%2C%20S.M.%20Hayes%2C%20T.P.%20The%20price%20of%20bandit%20information%20for%20online%20optimization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dani%2C%20V.%20Kakade%2C%20S.M.%20Hayes%2C%20T.P.%20The%20price%20of%20bandit%20information%20for%20online%20optimization"
        },
        {
            "id": "Gittins_et+al_2011_a",
            "entry": "J. Gittins, K. Glazebrook, and R. Weber. Multi-armed bandit allocation indices. John Wiley & Sons, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gittins%2C%20J.%20Glazebrook%2C%20K.%20Weber%2C%20R.%20Multi-armed%20bandit%20allocation%20indices%202011"
        },
        {
            "id": "Hsu_2016_a",
            "entry": "D. Hsu and S. Sabato. Loss minimization and parameter estimation with heavy tails. The Journal of Machine Learning Research, 17(1):543\u2013582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20D.%20Sabato%2C%20S.%20Loss%20minimization%20and%20parameter%20estimation%20with%20heavy%20tails%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20D.%20Sabato%2C%20S.%20Loss%20minimization%20and%20parameter%20estimation%20with%20heavy%20tails%202016"
        },
        {
            "id": "Lai_1985_a",
            "entry": "T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4\u201322, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985"
        },
        {
            "id": "Lattimore_2017_a",
            "entry": "T. Lattimore. A scale free algorithm for stochastic bandits with bounded kurtosis. In Advances in Neural Information Processing Systems, pages 1583\u20131592, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lattimore%2C%20T.%20A%20scale%20free%20algorithm%20for%20stochastic%20bandits%20with%20bounded%20kurtosis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lattimore%2C%20T.%20A%20scale%20free%20algorithm%20for%20stochastic%20bandits%20with%20bounded%20kurtosis%202017"
        },
        {
            "id": "Lattimore_et+al_2014_a",
            "entry": "T. Lattimore, K. Crammer, and C. Szepesv\u00e1ri. Optimal resource allocation with semi-bandit feedback. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, pages 477\u2013486. AUAI Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lattimore%2C%20T.%20Crammer%2C%20K.%20Szepesv%C3%A1ri%2C%20C.%20Optimal%20resource%20allocation%20with%20semi-bandit%20feedback%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lattimore%2C%20T.%20Crammer%2C%20K.%20Szepesv%C3%A1ri%2C%20C.%20Optimal%20resource%20allocation%20with%20semi-bandit%20feedback%202014"
        },
        {
            "id": "Li_et+al_2010_a",
            "entry": "L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the Nineteenth International Conference on World Wide Web, pages 661\u2013670. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Schapire%2C%20R.E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Schapire%2C%20R.E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010"
        },
        {
            "id": "Liebeherr_et+al_2012_a",
            "entry": "J. Liebeherr, A. Burchard, and F. Ciucu. Delay bounds in communication networks with heavy-tailed and self-similar traffic. IEEE Transactions on Information Theory, 58(2):1010\u20131024, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liebeherr%2C%20J.%20Burchard%2C%20A.%20Ciucu%2C%20F.%20Delay%20bounds%20in%20communication%20networks%20with%20heavy-tailed%20and%20self-similar%20traffic%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liebeherr%2C%20J.%20Burchard%2C%20A.%20Ciucu%2C%20F.%20Delay%20bounds%20in%20communication%20networks%20with%20heavy-tailed%20and%20self-similar%20traffic%202012"
        },
        {
            "id": "Medina_2016_a",
            "entry": "A. M. Medina and S. Yang. No-regret algorithms for heavy-tailed linear bandits. In International Conference on Machine Learning, pages 1642\u20131650, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Medina%2C%20A.M.%20Yang%2C%20S.%20No-regret%20algorithms%20for%20heavy-tailed%20linear%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Medina%2C%20A.M.%20Yang%2C%20S.%20No-regret%20algorithms%20for%20heavy-tailed%20linear%20bandits%202016"
        },
        {
            "id": "Munos_2014_a",
            "entry": "R. Munos et al. From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning. Foundations and Trends R in Machine Learning, 7(1):1\u2013129, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R.%20From%20bandits%20to%20monte-carlo%20tree%20search%3A%20The%20optimistic%20principle%20applied%20to%20optimization%20and%20planning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R.%20From%20bandits%20to%20monte-carlo%20tree%20search%3A%20The%20optimistic%20principle%20applied%20to%20optimization%20and%20planning%202014"
        },
        {
            "id": "Robbins_1952_a",
            "entry": "H. Robbins et al. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527\u2013535, 1952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952"
        },
        {
            "id": "Roberts_et+al_2015_a",
            "entry": "J. A. Roberts, T. W. Boonstra, and M. Breakspear. The heavy tail of the human brain. Current Opinion in Neurobiology, 31:164\u2013172, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roberts%2C%20J.A.%20Boonstra%2C%20T.W.%20Breakspear%2C%20M.%20The%20heavy%20tail%20of%20the%20human%20brain%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roberts%2C%20J.A.%20Boonstra%2C%20T.W.%20Breakspear%2C%20M.%20The%20heavy%20tail%20of%20the%20human%20brain%202015"
        },
        {
            "id": "Shao_1993_a",
            "entry": "M. Shao and C. L. Nikias. Signal processing with fractional lower order moments: stable processes and their applications. Proceedings of the IEEE, 81(7):986\u20131010, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shao%2C%20M.%20Nikias%2C%20C.L.%20Signal%20processing%20with%20fractional%20lower%20order%20moments%3A%20stable%20processes%20and%20their%20applications%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shao%2C%20M.%20Nikias%2C%20C.L.%20Signal%20processing%20with%20fractional%20lower%20order%20moments%3A%20stable%20processes%20and%20their%20applications%201993"
        },
        {
            "id": "Thompson_1933_a",
            "entry": "W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "Vakili_et+al_2013_a",
            "entry": "S. Vakili, K. Liu, and Q. Zhao. Deterministic sequencing of exploration and exploitation for multiarmed bandit problems. IEEE Journal of Selected Topics in Signal Processing, 7(5):759\u2013767, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vakili%2C%20S.%20Liu%2C%20K.%20Zhao%2C%20Q.%20Deterministic%20sequencing%20of%20exploration%20and%20exploitation%20for%20multiarmed%20bandit%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vakili%2C%20S.%20Liu%2C%20K.%20Zhao%2C%20Q.%20Deterministic%20sequencing%20of%20exploration%20and%20exploitation%20for%20multiarmed%20bandit%20problems%202013"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "X. Yu, H. Shao, M. R. Lyu, and I. King. Pure exploration of multi-armed bandits with heavy-tailed payoffs. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, pages 937\u2013946. AUAI Press, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20X.%20Shao%2C%20H.%20Lyu%2C%20M.R.%20King%2C%20I.%20Pure%20exploration%20of%20multi-armed%20bandits%20with%20heavy-tailed%20payoffs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20X.%20Shao%2C%20H.%20Lyu%2C%20M.R.%20King%2C%20I.%20Pure%20exploration%20of%20multi-armed%20bandits%20with%20heavy-tailed%20payoffs%202018"
        }
    ]
}
