{
    "filename": "7427-overcoming-language-priors-in-visual-question-answering-with-adversarial-regularization.pdf",
    "metadata": {
        "title": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization",
        "author": "Sainandan Ramakrishnan, Aishwarya Agrawal, Stefan Lee",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7427-overcoming-language-priors-in-visual-question-answering-with-adversarial-regularization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training \u2013 e.g. overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "strong prior",
            "url": "https://en.wikipedia.org/wiki/strong_prior"
        },
        {
            "term": "conditional mutual information",
            "url": "https://en.wikipedia.org/wiki/conditional_mutual_information"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "long short-term memory",
            "url": "https://en.wikipedia.org/wiki/long_short-term_memory"
        },
        {
            "term": "gated recurrent unit",
            "url": "https://en.wikipedia.org/wiki/gated_recurrent_unit"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        }
    ],
    "highlights": [
        "The task of answering questions about visual content \u2013 called Visual Question Answering (VQA) \u2013 presents a rich set of artificial intelligence challenges spanning computer vision and natural language processing",
        "In response to these challenges, there has been extensive work on Visual Question Answering in recent years both in terms of dataset curation [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] and modeling [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]. This widespread interest in Visual Question Answering has resulted in increasingly sophisticated models achieving higher and higher performance on increasingly large benchmark datasets; recent studies have demonstrated that many models tend to have poor image grounding, instead heavily leveraging superficial correlations between questions and answers in the training dataset to answer questions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "We consider unwanted language bias in Visual Question Answering to be overly-specific relationships between questions and their likely answers learned from the training dataset \u2013 i.e. those that could enable a question-only model to achieve relatively high performance without ever seeing an image \u2013 and we explicitly optimize the question representation within a base Visual Question Answering model to be uninformative to a question-only adversary model",
        "We find this Q-only(SAN) model achieves 24.84% on the VQACP v2 training set compared to 13.85% for our Stacked Attention Network+Q-only+Difference of entropies model, demonstrating that our approach has effectively restricted the discriminative information in the question encoding",
        "We propose a novel adversarial regularization scheme for reducing the memorization of dataset biases in Visual Question Answering based on a question-only adversary and the difference of model confidences after processing the image",
        "Our approach can be implemented as a simple, drop-in module on top of existing Visual Question Answering models and trained end-to-end from scratch"
    ],
    "key_statements": [
        "The task of answering questions about visual content \u2013 called Visual Question Answering (VQA) \u2013 presents a rich set of artificial intelligence challenges spanning computer vision and natural language processing",
        "We introduce a question-only model that takes as input the question encoding from the Visual Question Answering model and must leverage language biases in order to succeed",
        "Successful Visual Question Answering models must understand the question posed in natural language, identify relevant entities, object, and relationships in the image, and perform grounded reasoning to deduce the correct answer",
        "In response to these challenges, there has been extensive work on Visual Question Answering in recent years both in terms of dataset curation [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] and modeling [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]. This widespread interest in Visual Question Answering has resulted in increasingly sophisticated models achieving higher and higher performance on increasingly large benchmark datasets; recent studies have demonstrated that many models tend to have poor image grounding, instead heavily leveraging superficial correlations between questions and answers in the training dataset to answer questions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "Agrawal et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] introduced the Visual Question Answering-CP (Visual Question Answering under Changing Priors) diagnostic split of the Visual Question Answering [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] dataset to measure the effect of language bias",
        "We codify this intuition, introducing a novel regularization scheme that sets a base Visual Question Answering model against a question-only adversary to reduce the impact of language biases",
        "We consider unwanted language bias in Visual Question Answering to be overly-specific relationships between questions and their likely answers learned from the training dataset \u2013 i.e. those that could enable a question-only model to achieve relatively high performance without ever seeing an image \u2013 and we explicitly optimize the question representation within a base Visual Question Answering model to be uninformative to a question-only adversary model",
        "For any model of the form presented in (1), we can introduce a simple adversarial regularizer that explicitly reduces the effect of language biases by modifying the question encoder to minimize the performance of this question-only adversary",
        "Deriving inspiration from this baseline, we introduce a question-only adversary to explicitly reduce the ability of the question-only baseline to predict answers from questions alone",
        "We take this notion further by leveraging the question-only adversary to estimate and directly maximize the change in confidence after observing the image, which we show provides substantial benefits when paired with the question-only adversary",
        "We find this Q-only(SAN) model achieves 24.84% on the VQACP v2 training set compared to 13.85% for our Stacked Attention Network+Q-only+Difference of entropies model, demonstrating that our approach has effectively restricted the discriminative information in the question encoding",
        "We propose a novel adversarial regularization scheme for reducing the memorization of dataset biases in Visual Question Answering based on a question-only adversary and the difference of model confidences after processing the image",
        "Our approach can be implemented as a simple, drop-in module on top of existing Visual Question Answering models and trained end-to-end from scratch",
        "Acknowledgements This work was supported in part by NSF, AFRL, DARPA, Siemens, Google, Amazon, ONR YIPs and ONR Grants N00014-16-1-{2713,2793}"
    ],
    "summary": [
        "The task of answering questions about visual content \u2013 called Visual Question Answering (VQA) \u2013 presents a rich set of artificial intelligence challenges spanning computer vision and natural language processing.",
        "Agrawal et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] introduced the VQA-CP (Visual Question Answering under Changing Priors) diagnostic split of the VQA [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] dataset to measure the effect of language bias.",
        "One intuitive measure of the strength of language priors in VQA is the performance of a \u2018blind\u2019 model that produces answers given only the question and not the associated image.",
        "We codify this intuition, introducing a novel regularization scheme that sets a base VQA model against a question-only adversary to reduce the impact of language biases.",
        "We consider unwanted language bias in VQA to be overly-specific relationships between questions and their likely answers learned from the training dataset \u2013 i.e. those that could enable a question-only model to achieve relatively high performance without ever seeing an image \u2013 and we explicitly optimize the question representation within a base VQA model to be uninformative to a question-only adversary model.",
        "For any model of the form presented in (1), we can introduce a simple adversarial regularizer that explicitly reduces the effect of language biases by modifying the question encoder to minimize the performance of this question-only adversary.",
        "We introduce another adversarial regularizer corresponding to the difference in entropies between the base model prediction given the image and the question-only model which we write as",
        "We use VQA-CP as a testbed for our adversarial regularization approach and show consistent improvements over base models and existing work.",
        "This model isolates the answering module from the input question, mitigating the effect of language biases, but at a cost of relatively low standard VQA performance and multi-stage training.",
        "The combination of the Q-Adv and DoE regularizers further boosts the performance, resulting in 8.33% improvement over SAN and 1.43% over UpDn. Comparing our SAN + Q-Adv + DoE model to GVQA which is built on top of SAN, we outperform GVQA significantly (1.99%).",
        "When trained and tested on the VQA v2 dataset, the addition of the proposed regularizers results in a insignificant drop in the performance for SAN (0.1%) and a minor drop in performance for UpDn (0.73%) compared to prior work.",
        "We propose a novel adversarial regularization scheme for reducing the memorization of dataset biases in VQA based on a question-only adversary and the difference of model confidences after processing the image.",
        "The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of any sponsor"
    ],
    "headline": "We present a novel regularization scheme for Visual Question Answering that reduces this effect",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. Analyzing the behavior of visual question answering models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1955\u20131960, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Aishwarya%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Analyzing%20the%20behavior%20of%20visual%20question%20answering%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Aishwarya%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Analyzing%20the%20behavior%20of%20visual%20question%20answering%20models%202016"
        },
        {
            "id": "2",
            "entry": "[2] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don\u2019t just assume; look and answer: Overcoming priors for visual question answering, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Aishwarya%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Kembhavi%2C%20Aniruddha%20Don%E2%80%99t%20just%20assume%3B%20look%20and%20answer%3A%20Overcoming%20priors%20for%20visual%20question%20answering%202017"
        },
        {
            "id": "3",
            "entry": "[3] Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. C-vqa: A compositional split of the visual question answering (vqa) v1.0 dataset, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Aishwarya%20Kembhavi%2C%20Aniruddha%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20C-vqa%3A%20A%20compositional%20split%20of%20the%20visual%20question%20answering%20%28vqa%29%20v1.0%20dataset%202017"
        },
        {
            "id": "4",
            "entry": "[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202017"
        },
        {
            "id": "5",
            "entry": "[5] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%20Andreas%20Marcus%20Rohrbach%20Trevor%20Darrell%20and%20Dan%20Klein%20Neural%20module%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%20Andreas%20Marcus%20Rohrbach%20Trevor%20Darrell%20and%20Dan%20Klein%20Neural%20module%20networks%202015"
        },
        {
            "id": "6",
            "entry": "[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antol%2C%20Stanislaw%20Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Mitchell%2C%20Margaret%20VQA%3A%20Visual%20Question%20Answering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antol%2C%20Stanislaw%20Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Mitchell%2C%20Margaret%20VQA%3A%20Visual%20Question%20Answering%202015"
        },
        {
            "id": "7",
            "entry": "[7] Kaylee Burns, Lisa Anne Hendricks, Trevor Darrell, and Anna Rohrbach. Women also snowboard: Overcoming bias in captioning models. arXiv preprint arXiv:1803.09797, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09797"
        },
        {
            "id": "8",
            "entry": "[8] Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. Towards diverse and natural image descriptions via a conditional gan. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Bo%20Fidler%2C%20Sanja%20Urtasun%2C%20Raquel%20Lin%2C%20Dahua%20Towards%20diverse%20and%20natural%20image%20descriptions%20via%20a%20conditional%20gan%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Bo%20Fidler%2C%20Sanja%20Urtasun%2C%20Raquel%20Lin%2C%20Dahua%20Towards%20diverse%20and%20natural%20image%20descriptions%20via%20a%20conditional%20gan%202017"
        },
        {
            "id": "9",
            "entry": "[9] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M.F. Moura, Devi Parikh, and Dhruv Batra. Visual Dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abhishek%20Das%20Satwik%20Kottur%20Khushi%20Gupta%20Avi%20Singh%20Deshraj%20Yadav%20Jos%C3%A9%20MF%20Moura%20Devi%20Parikh%20and%20Dhruv%20Batra%20Visual%20Dialog%20In%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abhishek%20Das%20Satwik%20Kottur%20Khushi%20Gupta%20Avi%20Singh%20Deshraj%20Yadav%20Jos%C3%A9%20MF%20Moura%20Devi%20Parikh%20and%20Dhruv%20Batra%20Visual%20Dialog%20In%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202017"
        },
        {
            "id": "10",
            "entry": "[10] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20and%20Yoshua%20Bengio%202014"
        },
        {
            "id": "11",
            "entry": "[11] Jonathan Gordon and Benjamin Van Durme. Reporting bias and knowledge acquisition. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25\u201330. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordon%2C%20Jonathan%20Durme%2C%20Benjamin%20Van%20Reporting%20bias%20and%20knowledge%20acquisition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordon%2C%20Jonathan%20Durme%2C%20Benjamin%20Van%20Reporting%20bias%20and%20knowledge%20acquisition%202013"
        },
        {
            "id": "12",
            "entry": "[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Yash%20Khot%2C%20Tejas%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Making%20the%20v%20in%20vqa%20matter%3A%20Elevating%20the%20role%20of%20image%20understanding%20in%20visual%20question%20answering%202016"
        },
        {
            "id": "13",
            "entry": "[13] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gurari%2C%20Danna%20Li%2C%20Qing%20Stangl%2C%20Abigale%20J.%20Guo%2C%20Anhong%20Vizwiz%20grand%20challenge%3A%20Answering%20visual%20questions%20from%20blind%20people%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gurari%2C%20Danna%20Li%2C%20Qing%20Stangl%2C%20Abigale%20J.%20Guo%2C%20Anhong%20Vizwiz%20grand%20challenge%3A%20Answering%20visual%20questions%20from%20blind%20people%202018"
        },
        {
            "id": "14",
            "entry": "[14] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Ronghang%20Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Learning%20to%20reason%3A%20End-to-end%20module%20networks%20for%20visual%20question%20answering%202017"
        },
        {
            "id": "15",
            "entry": "[15] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 1988\u20131997. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Li%20Fei-Fei%2C%20C.Lawrence%20Zitnick%20Clevr%3A%20A%20diagnostic%20dataset%20for%20compositional%20language%20and%20elementary%20visual%20reasoning%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Li%20Fei-Fei%2C%20C.Lawrence%20Zitnick%20Clevr%3A%20A%20diagnostic%20dataset%20for%20compositional%20language%20and%20elementary%20visual%20reasoning%201988"
        },
        {
            "id": "16",
            "entry": "[16] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Hoffman%2C%20Judy%20Inferring%20and%20executing%20programs%20for%20visual%20reasoning%202017"
        },
        {
            "id": "17",
            "entry": "[17] Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kafle%2C%20Kushal%20Kanan%2C%20Christopher%20An%20analysis%20of%20visual%20question%20answering%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kafle%2C%20Kushal%20Kanan%2C%20Christopher%20An%20analysis%20of%20visual%20question%20answering%20algorithms%202017"
        },
        {
            "id": "18",
            "entry": "[18] Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Fader networks: Manipulating images by sliding attributes, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20Guillaume%20Zeghidour%2C%20Neil%20Usunier%2C%20Nicolas%20Bordes%2C%20Antoine%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%202017"
        },
        {
            "id": "19",
            "entry": "[19] Gilles Louppe, Michael Kagan, and Kyle Cranmer. Learning to pivot with adversarial networks. In NIPS, pages 982\u2013991, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louppe%2C%20Gilles%20Kagan%2C%20Michael%20Cranmer%2C%20Kyle%20Learning%20to%20pivot%20with%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louppe%2C%20Gilles%20Kagan%2C%20Michael%20Cranmer%2C%20Kyle%20Learning%20to%20pivot%20with%20adversarial%20networks%202017"
        },
        {
            "id": "20",
            "entry": "[20] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image coattention for visual question answering, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Jiasen%20Yang%2C%20Jianwei%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Hierarchical%20question-image%20coattention%20for%20visual%20question%20answering%202016"
        },
        {
            "id": "21",
            "entry": "[21] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "22",
            "entry": "[22] Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, and Aaron Courville. Learning visual reasoning without strong priors, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perez%2C%20Ethan%20de%20Vries%2C%20Harm%20Strub%2C%20Florian%20Dumoulin%2C%20Vincent%20Learning%20visual%20reasoning%20without%20strong%20priors%202017"
        },
        {
            "id": "23",
            "entry": "[23] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202015"
        },
        {
            "id": "24",
            "entry": "[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015"
        },
        {
            "id": "25",
            "entry": "[25] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "26",
            "entry": "[26] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1521\u20131528. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Antonio%20Torralba%20and%20Alexei%20A%20Efros.%20Unbiased%20look%20at%20dataset%20bias%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Antonio%20Torralba%20and%20Alexei%20A%20Efros.%20Unbiased%20look%20at%20dataset%20bias%202011"
        },
        {
            "id": "27",
            "entry": "[27] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Adversarial discriminative domain adaptation. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tzeng%2C%20Eric%20Hoffman%2C%20Judy%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Adversarial%20discriminative%20domain%20adaptation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tzeng%2C%20Eric%20Hoffman%2C%20Judy%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Adversarial%20discriminative%20domain%20adaptation%202017"
        },
        {
            "id": "28",
            "entry": "[28] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zichao%20He%2C%20Xiaodong%20Gao%2C%20Jianfeng%20Deng%2C%20Li%20Stacked%20attention%20networks%20for%20image%20question%20answering%202015"
        },
        {
            "id": "29",
            "entry": "[29] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "30",
            "entry": "[30] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and yang: Balancing and answering binary visual questions. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pages 5014\u20135022. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Peng%20Goyal%2C%20Yash%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Yin%20and%20yang%3A%20Balancing%20and%20answering%20binary%20visual%20questions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Peng%20Goyal%2C%20Yash%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Yin%20and%20yang%3A%20Balancing%20and%20answering%20binary%20visual%20questions%202016"
        },
        {
            "id": "31",
            "entry": "[31] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias amplification using corpus-level constraints, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Jieyu%20Wang%2C%20Tianlu%20Yatskar%2C%20Mark%20Ordonez%2C%20Vicente%20Men%20also%20like%20shopping%3A%20Reducing%20gender%20bias%20amplification%20using%20corpus-level%20constraints%202017"
        },
        {
            "id": "32",
            "entry": "[32] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In CVPR, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Yuke%20Groth%2C%20Oliver%20Bernstein%2C%20Michael%20Fei-Fei%2C%20Li%20Visual7w%3A%20Grounded%20question%20answering%20in%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Yuke%20Groth%2C%20Oliver%20Bernstein%2C%20Michael%20Fei-Fei%2C%20Li%20Visual7w%3A%20Grounded%20question%20answering%20in%20images%202016"
        }
    ]
}
