{
    "filename": "7391-video-to-video-synthesis.pdf",
    "metadata": {
        "title": "Video-to-Video Synthesis",
        "author": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Nikolai Yakovenko, Andrew Tao, Jan Kautz, Bryan Catanzaro",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website."
    },
    "keywords": [
        {
            "term": "high resolution",
            "url": "https://en.wikipedia.org/wiki/high_resolution"
        },
        {
            "term": "image synthesis",
            "url": "https://en.wikipedia.org/wiki/image_synthesis"
        },
        {
            "term": "image translation",
            "url": "https://en.wikipedia.org/wiki/image_translation"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "real time",
            "url": "https://en.wikipedia.org/wiki/real_time"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "video synthesis",
            "url": "https://en.wikipedia.org/wiki/video_synthesis"
        },
        {
            "term": "Amazon Mechanical Turk",
            "url": "https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"
        }
    ],
    "highlights": [
        "The capability to model and recreate the dynamics of our visual world is essential to building intelligent agents",
        "We study a new form: video-to-video synthesis",
        "Our method belongs to the category of conditional video generation with Generative Adversarial Networks",
        "We propose a conditional Generative Adversarial Networks framework for this conditional video distribution matching task",
        "We evaluate the algorithm by the ratio that the algorithm outputs are preferred. Fr\u00e9chet Inception Distance (FID) [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] is a widely used metric for implicit generative models, as it correlates well with the visual quality of generated samples",
        "We present a general video-to-video synthesis framework based on conditional Generative Adversarial Networks"
    ],
    "key_statements": [
        "The capability to model and recreate the dynamics of our visual world is essential to building intelligent agents",
        "We study a new form: video-to-video synthesis",
        "Our method belongs to the category of conditional video generation with Generative Adversarial Networks",
        "We propose a conditional Generative Adversarial Networks framework for this conditional video distribution matching task",
        "We evaluate the algorithm by the ratio that the algorithm outputs are preferred. Fr\u00e9chet Inception Distance (FID) [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] is a widely used metric for implicit generative models, as it correlates well with the visual quality of generated samples",
        "We show an extension of our approach to the future video prediction task: learning to predict the future video given a few observed frames",
        "We present a general video-to-video synthesis framework based on conditional Generative Adversarial Networks"
    ],
    "summary": [
        "The capability to model and recreate the dynamics of our visual world is essential to building intelligent agents.",
        "Given paired input and output videos, With carefully-designed generators and discriminators, and a new spatio-temporal learning objective, our method can learn to synthesize high-resolution, photorealistic, temporally coherent videos.",
        "Our method belongs to the category of conditional video generation with GANs. instead of predicting future videos conditioning on the current observed frames [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>, <a class=\"ref-link\" id=\"c68\" href=\"#r68\">68</a>], our method synthesizes photorealistic videos conditioning on manipulable semantic representations, such as segmentation masks, sketches, and poses.",
        "Through matching the conditional video distributions, the model learns to generate photorealistic, temporally coherent output sequences as if they were captured by a video camera.",
        "The purpose of DV is to ensure that consecutive output frames resemble the temporal dynamics of a real video given the same optical flow.",
        "To achieve multimodal synthesis [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c72\" href=\"#r72\">72</a>, <a class=\"ref-link\" id=\"c82\" href=\"#r82\">82</a>], we adopt a feature embedding scheme [<a class=\"ref-link\" id=\"c72\" href=\"#r72\">72</a>] for the source video that consists of instance-level semantic segmentation masks.",
        "The generator F can synthesize videos with different visual appearances.",
        "A subset of images in the videos contains ground truth semantic segmentation masks.",
        "We compare the proposed approach to the baselines on the Cityscapes benchmark, where we apply the learned models to synthesize 500 short video clips in the validation set.",
        "Our approach produces a high-resolution, photorealistic, temporally consistent video output.",
        "We remove the optical flow prediction network W and the mask prediction network M from the generator F in Equation 4 and only use H for synthesis.",
        "We train a sketch-to-face synthesis video model using the real face videos in the FaceForensics dataset [<a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>].",
        "As shown in Figure 5, our model can convert sequences of sketches to photorealistic output videos.",
        "We apply our method to the task of converting sequences of human poses to photorealistic output videos.",
        "As shown in Figure 6, our model learns to synthesize high-resolution photorealistic output dance videos that contain unseen body shapes and",
        "After extracting the segmentation masks from the observed frames, we train a generator to predict future semantic masks.",
        "We use the proposed video-to-video synthesis approach to convert the predicted segmentation masks to a future video.",
        "As shown in Table 3, our model produces smaller FIDs, and the human subjects favor our resulting videos.",
        "We present a general video-to-video synthesis framework based on conditional GANs. Through carefully-designed generators and discriminators as well as a spatio-temporal adversarial objective, we can synthesize high-resolution, photorealistic, and temporally consistent videos."
    ],
    "headline": "We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video  to an output photorealistic video that precisely depicts the content of the source video",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] K. Aberman, M. Shi, J. Liao, D. Lischinski, B. Chen, and D. Cohen-Or. Deep video-based performance cloning. arXiv preprint arXiv:1808.06847, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.06847"
        },
        {
            "id": "2",
            "entry": "[2] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26\u201338, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arulkumaran%2C%20K.%20Deisenroth%2C%20M.P.%20Brundage%2C%20M.%20Bharath%2C%20A.A.%20Deep%20reinforcement%20learning%3A%20A%20brief%20survey%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arulkumaran%2C%20K.%20Deisenroth%2C%20M.P.%20Brundage%2C%20M.%20Bharath%2C%20A.A.%20Deep%20reinforcement%20learning%3A%20A%20brief%20survey%202017"
        },
        {
            "id": "3",
            "entry": "[3] X. Bai, J. Wang, D. Simons, and G. Sapiro. Video snapcut: robust video object cutout using localized classifiers. ACM Transactions on Graphics (TOG), 28(3):70, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bai%2C%20X.%20Wang%2C%20J.%20Simons%2C%20D.%20Sapiro%2C%20G.%20Video%20snapcut%3A%20robust%20video%20object%20cutout%20using%20localized%20classifiers%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bai%2C%20X.%20Wang%2C%20J.%20Simons%2C%20D.%20Sapiro%2C%20G.%20Video%20snapcut%3A%20robust%20video%20object%20cutout%20using%20localized%20classifiers%202009"
        },
        {
            "id": "4",
            "entry": "[4] G. Balakrishnan, A. Zhao, A. V. Dalca, F. Durand, and J. Guttag. Synthesizing images of humans in unseen poses. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balakrishnan%2C%20G.%20Zhao%2C%20A.%20Dalca%2C%20A.V.%20Durand%2C%20F.%20Synthesizing%20images%20of%20humans%20in%20unseen%20poses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balakrishnan%2C%20G.%20Zhao%2C%20A.%20Dalca%2C%20A.V.%20Durand%2C%20F.%20Synthesizing%20images%20of%20humans%20in%20unseen%20poses%202018"
        },
        {
            "id": "5",
            "entry": "[5] D. Bitouk, N. Kumar, S. Dhillon, P. Belhumeur, and S. K. Nayar. Face swapping: automatically replacing faces in photographs. In ACM SIGGRAPH, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bitouk%2C%20D.%20Kumar%2C%20N.%20Dhillon%2C%20S.%20Belhumeur%2C%20P.%20Face%20swapping%3A%20automatically%20replacing%20faces%20in%20photographs%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bitouk%2C%20D.%20Kumar%2C%20N.%20Dhillon%2C%20S.%20Belhumeur%2C%20P.%20Face%20swapping%3A%20automatically%20replacing%20faces%20in%20photographs%202008"
        },
        {
            "id": "6",
            "entry": "[6] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousmalis%2C%20K.%20Silberman%2C%20N.%20Dohan%2C%20D.%20Erhan%2C%20D.%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousmalis%2C%20K.%20Silberman%2C%20N.%20Dohan%2C%20D.%20Erhan%2C%20D.%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "7",
            "entry": "[7] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-person 2D pose estimation using part affinity fields. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20Z.%20Simon%2C%20T.%20Wei%2C%20S.-E.%20Sheikh%2C%20Y.%20Realtime%20multi-person%202D%20pose%20estimation%20using%20part%20affinity%20fields%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20Z.%20Simon%2C%20T.%20Wei%2C%20S.-E.%20Sheikh%2C%20Y.%20Realtime%20multi-person%202D%20pose%20estimation%20using%20part%20affinity%20fields%202017"
        },
        {
            "id": "8",
            "entry": "[8] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20J.%20Zisserman%2C%20A.%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20J.%20Zisserman%2C%20A.%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017"
        },
        {
            "id": "9",
            "entry": "[9] C. Chan, S. Ginosar, T. Zhou, and A. A. Efros. Everybody dance now. In European Conference on Computer Vision (ECCV) Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20C.%20Ginosar%2C%20S.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Everybody%20dance%20now%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20C.%20Ginosar%2C%20S.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Everybody%20dance%20now%202018"
        },
        {
            "id": "10",
            "entry": "[10] D. Chen, J. Liao, L. Yuan, N. Yu, and G. Hua. Coherent online video style transfer. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20D.%20Liao%2C%20J.%20Yuan%2C%20L.%20Yu%2C%20N.%20Coherent%20online%20video%20style%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20D.%20Liao%2C%20J.%20Yuan%2C%20L.%20Yu%2C%20N.%20Coherent%20online%20video%20style%20transfer%202017"
        },
        {
            "id": "11",
            "entry": "[11] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05587"
        },
        {
            "id": "12",
            "entry": "[12] T. Chen, J.-Y. Zhu, A. Shamir, and S.-M. Hu. Motion-aware gradient domain video composition. IEEE Trans. Image Processing, 22(7):2532\u20132544, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20T.%20Zhu%2C%20J.-Y.%20Shamir%2C%20A.%20Hu%2C%20S.-M.%20Motion-aware%20gradient%20domain%20video%20composition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20T.%20Zhu%2C%20J.-Y.%20Shamir%2C%20A.%20Hu%2C%20S.-M.%20Motion-aware%20gradient%20domain%20video%20composition%202013"
        },
        {
            "id": "13",
            "entry": "[13] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20Cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20Cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016"
        },
        {
            "id": "14",
            "entry": "[14] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a Laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.%20Chintala%2C%20S.%20Szlam%2C%20A.%20Fergus%2C%20R.%20Deep%20generative%20image%20models%20using%20a%20Laplacian%20pyramid%20of%20adversarial%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.%20Chintala%2C%20S.%20Szlam%2C%20A.%20Fergus%2C%20R.%20Deep%20generative%20image%20models%20using%20a%20Laplacian%20pyramid%20of%20adversarial%20networks%202015"
        },
        {
            "id": "15",
            "entry": "[15] E. L. Denton and V. Birodkar. Unsupervised learning of disentangled representations from video. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.L.%20Birodkar%2C%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.L.%20Birodkar%2C%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017"
        },
        {
            "id": "16",
            "entry": "[16] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on deep networks. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Generating%20images%20with%20perceptual%20similarity%20metrics%20based%20on%20deep%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Generating%20images%20with%20perceptual%20similarity%20metrics%20based%20on%20deep%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] P. Esser, E. Sutter, and B. Ommer. A variational u-net for conditional appearance and shape generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esser%2C%20P.%20Sutter%2C%20E.%20Ommer%2C%20B.%20A%20variational%20u-net%20for%20conditional%20appearance%20and%20shape%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esser%2C%20P.%20Sutter%2C%20E.%20Ommer%2C%20B.%20A%20variational%20u-net%20for%20conditional%20appearance%20and%20shape%20generation%202018"
        },
        {
            "id": "18",
            "entry": "[18] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through video prediction. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Goodfellow%2C%20I.%20Levine%2C%20S.%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Goodfellow%2C%20I.%20Levine%2C%20S.%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016"
        },
        {
            "id": "19",
            "entry": "[19] A. Ghosh, V. Kulharia, V. Namboodiri, P. H. Torr, and P. K. Dokania. Multi-agent diverse generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20A.%20Kulharia%2C%20V.%20Namboodiri%2C%20V.%20Torr%2C%20P.H.%20Multi-agent%20diverse%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20A.%20Kulharia%2C%20V.%20Namboodiri%2C%20V.%20Torr%2C%20P.H.%20Multi-agent%20diverse%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "20",
            "entry": "[20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20networks%202014"
        },
        {
            "id": "21",
            "entry": "[21] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein GANs. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20GANs%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. Gupta, J. Johnson, A. Alahi, and L. Fei-Fei. Characterizing and improving stability in neural style transfer. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20A.%20Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Characterizing%20and%20improving%20stability%20in%20neural%20style%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20A.%20Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Characterizing%20and%20improving%20stability%20in%20neural%20style%20transfer%202017"
        },
        {
            "id": "23",
            "entry": "[23] R. A. G\u00fcler, N. Neverova, and I. Kokkinos. Densepose: Dense human pose estimation in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%C3%BCler%2C%20R.A.%20Neverova%2C%20N.%20Kokkinos%2C%20I.%20Densepose%3A%20Dense%20human%20pose%20estimation%20in%20the%20wild%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%C3%BCler%2C%20R.A.%20Neverova%2C%20N.%20Kokkinos%2C%20I.%20Densepose%3A%20Dense%20human%20pose%20estimation%20in%20the%20wild%202018"
        },
        {
            "id": "24",
            "entry": "[24] D. Ha and J. Schmidhuber. World models. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ha%2C%20D.%20Schmidhuber%2C%20J.%20World%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ha%2C%20D.%20Schmidhuber%2C%20J.%20World%20models%202018"
        },
        {
            "id": "25",
            "entry": "[25] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20He%20G%20Gkioxari%20P%20Doll%C3%A1r%20and%20R%20Girshick%20Mask%20RCNN%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20He%20G%20Gkioxari%20P%20Doll%C3%A1r%20and%20R%20Girshick%20Mask%20RCNN%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%202017"
        },
        {
            "id": "26",
            "entry": "[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "27",
            "entry": "[27] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017"
        },
        {
            "id": "28",
            "entry": "[28] H. Huang, H. Wang, W. Luo, L. Ma, W. Jiang, X. Zhu, Z. Li, and W. Liu. Real-time neural style transfer for videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20H.%20Wang%2C%20H.%20Luo%2C%20W.%20Ma%2C%20L.%20Real-time%20neural%20style%20transfer%20for%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20H.%20Wang%2C%20H.%20Luo%2C%20W.%20Ma%2C%20L.%20Real-time%20neural%20style%20transfer%20for%20videos%202017"
        },
        {
            "id": "29",
            "entry": "[29] X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin, and R. Yang. The ApolloScape dataset for autonomous driving. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Cheng%2C%20X.%20Geng%2C%20Q.%20Cao%2C%20B.%20The%20ApolloScape%20dataset%20for%20autonomous%20driving%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Cheng%2C%20X.%20Geng%2C%20Q.%20Cao%2C%20B.%20The%20ApolloScape%20dataset%20for%20autonomous%20driving%202018"
        },
        {
            "id": "30",
            "entry": "[30] X. Huang, Y. Li, O. Poursaeed, J. E. Hopcroft, and S. J. Belongie. Stacked generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Li%2C%20Y.%20Poursaeed%2C%20O.%20Hopcroft%2C%20J.E.%20Stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Li%2C%20Y.%20Poursaeed%2C%20O.%20Hopcroft%2C%20J.E.%20Stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "31",
            "entry": "[31] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018"
        },
        {
            "id": "32",
            "entry": "[32] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ilg%2C%20E.%20Mayer%2C%20N.%20Saikia%2C%20T.%20Keuper%2C%20M.%20Flownet%202.0%3A%20Evolution%20of%20optical%20flow%20estimation%20with%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ilg%2C%20E.%20Mayer%2C%20N.%20Saikia%2C%20T.%20Keuper%2C%20M.%20Flownet%202.0%3A%20Evolution%20of%20optical%20flow%20estimation%20with%20deep%20networks%202017"
        },
        {
            "id": "33",
            "entry": "[33] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "34",
            "entry": "[34] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and superresolution. In European Conference on Computer Vision (ECCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20superresolution%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20superresolution%202016"
        },
        {
            "id": "35",
            "entry": "[35] J. T. Kajiya. The rendering equation. In ACM SIGGRAPH, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kajiya%2C%20J.T.%20The%20rendering%20equation%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kajiya%2C%20J.T.%20The%20rendering%20equation%201986"
        },
        {
            "id": "36",
            "entry": "[36] N. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.00527"
        },
        {
            "id": "37",
            "entry": "[37] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "38",
            "entry": "[38] D. E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=King%2C%20D.E.%20Dlib-ml%3A%20A%20machine%20learning%20toolkit%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=King%2C%20D.E.%20Dlib-ml%3A%20A%20machine%20learning%20toolkit%202009"
        },
        {
            "id": "39",
            "entry": "[39] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "40",
            "entry": "[40] A. B. L. Larsen, S. K. S\u00f8nderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20B.%20L.%20Larsen%2C%20S.%20K.%20S%C3%B8nderby%2C%20H.%20Larochelle%20Winther%2C%20O.%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20B.%20L.%20Larsen%2C%20S.%20K.%20S%C3%B8nderby%2C%20H.%20Larochelle%20Winther%2C%20O.%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016"
        },
        {
            "id": "41",
            "entry": "[41] A. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and S. Levine. Stochastic adversarial video prediction. arXiv preprint arXiv:1804.01523, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01523"
        },
        {
            "id": "42",
            "entry": "[42] X. Liang, L. Lee, W. Dai, and E. P. Xing. Dual motion GAN for future-flow embedded video prediction. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20X.%20Lee%2C%20L.%20Dai%2C%20W.%20Xing%2C%20E.P.%20Dual%20motion%20GAN%20for%20future-flow%20embedded%20video%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20X.%20Lee%2C%20L.%20Dai%2C%20W.%20Xing%2C%20E.P.%20Dual%20motion%20GAN%20for%20future-flow%20embedded%20video%20prediction%202017"
        },
        {
            "id": "43",
            "entry": "[43] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "44",
            "entry": "[44] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.-Y.%20Tuzel%2C%20O.%20Coupled%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.-Y.%20Tuzel%2C%20O.%20Coupled%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "45",
            "entry": "[45] W. Lotter, G. Kreiman, and D. Cox. Deep predictive coding networks for video prediction and unsupervised learning. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lotter%2C%20W.%20Kreiman%2C%20G.%20Cox%2C%20D.%20Deep%20predictive%20coding%20networks%20for%20video%20prediction%20and%20unsupervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lotter%2C%20W.%20Kreiman%2C%20G.%20Cox%2C%20D.%20Deep%20predictive%20coding%20networks%20for%20video%20prediction%20and%20unsupervised%20learning%202017"
        },
        {
            "id": "46",
            "entry": "[46] B. D. Lucas, T. Kanade, et al. An iterative image registration technique with an application to stereo vision. International Joint Conference on Artificial Intelligence (IJCAI), 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucas%2C%20B.D.%20Kanade%2C%20T.%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucas%2C%20B.D.%20Kanade%2C%20T.%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981"
        },
        {
            "id": "47",
            "entry": "[47] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool. Pose guided person image generation. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20L.%20Jia%2C%20X.%20Sun%2C%20Q.%20Schiele%2C%20B.%20Pose%20guided%20person%20image%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20L.%20Jia%2C%20X.%20Sun%2C%20Q.%20Schiele%2C%20B.%20Pose%20guided%20person%20image%20generation%202017"
        },
        {
            "id": "48",
            "entry": "[48] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and M. Fritz. Disentangled person image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20L.%20Sun%2C%20Q.%20Georgoulis%2C%20S.%20Gool%2C%20L.Van%20Disentangled%20person%20image%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20L.%20Sun%2C%20Q.%20Georgoulis%2C%20S.%20Gool%2C%20L.Van%20Disentangled%20person%20image%20generation%202018"
        },
        {
            "id": "49",
            "entry": "[49] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley. Least squares generative adversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.Y.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.Y.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "50",
            "entry": "[50] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016"
        },
        {
            "id": "51",
            "entry": "[51] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Kataoka%2C%20T.%20Koyama%2C%20M.%20Yoshida%2C%20Y.%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Kataoka%2C%20T.%20Koyama%2C%20M.%20Yoshida%2C%20Y.%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "52",
            "entry": "[52] T. Miyato and M. Koyama. cGANs with projection discriminator. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Koyama%2C%20M.%20cGANs%20with%20projection%20discriminator%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Koyama%2C%20M.%20cGANs%20with%20projection%20discriminator%202018"
        },
        {
            "id": "53",
            "entry": "[53] A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier GANs. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20A.%20Olah%2C%20C.%20Shlens%2C%20J.%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20A.%20Olah%2C%20C.%20Shlens%2C%20J.%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017"
        },
        {
            "id": "54",
            "entry": "[54] K. Ohnishi, S. Yamamoto, Y. Ushiku, and T. Harada. Hierarchical video generation from orthogonal information: Optical flow and texture. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ohnishi%2C%20K.%20Yamamoto%2C%20S.%20Ushiku%2C%20Y.%20Harada%2C%20T.%20Hierarchical%20video%20generation%20from%20orthogonal%20information%3A%20Optical%20flow%20and%20texture%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ohnishi%2C%20K.%20Yamamoto%2C%20S.%20Ushiku%2C%20Y.%20Harada%2C%20T.%20Hierarchical%20video%20generation%20from%20orthogonal%20information%3A%20Optical%20flow%20and%20texture%202018"
        },
        {
            "id": "55",
            "entry": "[55] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202015"
        },
        {
            "id": "56",
            "entry": "[56] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image synthesis. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Akata%2C%20Z.%20Yan%2C%20X.%20Logeswaran%2C%20L.%20Generative%20adversarial%20text%20to%20image%20synthesis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Akata%2C%20Z.%20Yan%2C%20X.%20Logeswaran%2C%20L.%20Generative%20adversarial%20text%20to%20image%20synthesis%202016"
        },
        {
            "id": "57",
            "entry": "[57] A. R\u00f6ssler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nie\u00dfner. Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09179"
        },
        {
            "id": "58",
            "entry": "[58] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer for videos. In German Conference on Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruder%2C%20M.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Artistic%20style%20transfer%20for%20videos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruder%2C%20M.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Artistic%20style%20transfer%20for%20videos%202016"
        },
        {
            "id": "59",
            "entry": "[59] M. Saito, E. Matsumoto, and S. Saito. Temporal generative adversarial nets with singular value clipping. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saito%2C%20M.%20Matsumoto%2C%20E.%20Saito%2C%20S.%20Temporal%20generative%20adversarial%20nets%20with%20singular%20value%20clipping%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saito%2C%20M.%20Matsumoto%2C%20E.%20Saito%2C%20S.%20Temporal%20generative%20adversarial%20nets%20with%20singular%20value%20clipping%202017"
        },
        {
            "id": "60",
            "entry": "[60] A. Sch\u00f6dl, R. Szeliski, D. H. Salesin, and I. Essa. Video textures. ACM Transactions on Graphics (TOG), 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6dl%2C%20A.%20Szeliski%2C%20R.%20Salesin%2C%20D.H.%20Essa%2C%20I.%20Video%20textures%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6dl%2C%20A.%20Szeliski%2C%20R.%20Salesin%2C%20D.H.%20Essa%2C%20I.%20Video%20textures%202000"
        },
        {
            "id": "61",
            "entry": "[61] E. Shechtman, Y. Caspi, and M. Irani. Space-time super-resolution. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 27(4):531\u2013545, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shechtman%2C%20E.%20Caspi%2C%20Y.%20Irani%2C%20M.%20Space-time%20super-resolution%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shechtman%2C%20E.%20Caspi%2C%20Y.%20Irani%2C%20M.%20Space-time%20super-resolution%202005"
        },
        {
            "id": "62",
            "entry": "[62] W. Shi, J. Caballero, F. Husz\u00e1r, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Real-time%20single%20image%20and%20video%20super-resolution%20using%20an%20efficient%20sub-pixel%20convolutional%20neural%20network%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Real-time%20single%20image%20and%20video%20super-resolution%20using%20an%20efficient%20sub-pixel%20convolutional%20neural%20network%202016"
        },
        {
            "id": "63",
            "entry": "[63] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrivastava%2C%20A.%20Pfister%2C%20T.%20Tuzel%2C%20O.%20Susskind%2C%20J.%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrivastava%2C%20A.%20Pfister%2C%20T.%20Tuzel%2C%20O.%20Susskind%2C%20J.%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017"
        },
        {
            "id": "64",
            "entry": "[64] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsupervised learning of video representations using lstms. In International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhudinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhudinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015"
        },
        {
            "id": "65",
            "entry": "[65] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-domain image generation. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taigman%2C%20Y.%20Polyak%2C%20A.%20Wolf%2C%20L.%20Unsupervised%20cross-domain%20image%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taigman%2C%20Y.%20Polyak%2C%20A.%20Wolf%2C%20L.%20Unsupervised%20cross-domain%20image%20generation%202017"
        },
        {
            "id": "66",
            "entry": "[66] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz. MoCoGAN: Decomposing motion and content for video generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tulyakov%2C%20S.%20Liu%2C%20M.-Y.%20Yang%2C%20X.%20Kautz%2C%20J.%20MoCoGAN%3A%20Decomposing%20motion%20and%20content%20for%20video%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tulyakov%2C%20S.%20Liu%2C%20M.-Y.%20Yang%2C%20X.%20Kautz%2C%20J.%20MoCoGAN%3A%20Decomposing%20motion%20and%20content%20for%20video%20generation%202018"
        },
        {
            "id": "67",
            "entry": "[67] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. Decomposing motion and content for natural video sequence prediction. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20R.%20Yang%2C%20J.%20Hong%2C%20S.%20Lin%2C%20X.%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20R.%20Yang%2C%20J.%20Hong%2C%20S.%20Lin%2C%20X.%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017"
        },
        {
            "id": "68",
            "entry": "[68] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016"
        },
        {
            "id": "69",
            "entry": "[69] C. Vondrick and A. Torralba. Generating the future with adversarial transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20C.%20Torralba%2C%20A.%20Generating%20the%20future%20with%20adversarial%20transformers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20C.%20Torralba%2C%20A.%20Generating%20the%20future%20with%20adversarial%20transformers%202017"
        },
        {
            "id": "70",
            "entry": "[70] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An uncertain future: Forecasting from static images using variational autoencoders. In European Conference on Computer Vision (ECCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20J.%20Doersch%2C%20C.%20Gupta%2C%20A.%20Hebert%2C%20M.%20An%20uncertain%20future%3A%20Forecasting%20from%20static%20images%20using%20variational%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20J.%20Doersch%2C%20C.%20Gupta%2C%20A.%20Hebert%2C%20M.%20An%20uncertain%20future%3A%20Forecasting%20from%20static%20images%20using%20variational%20autoencoders%202016"
        },
        {
            "id": "71",
            "entry": "[71] J. Walker, K. Marino, A. Gupta, and M. Hebert. The pose knows: Video forecasting by generating pose futures. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20J.%20Marino%2C%20K.%20Gupta%2C%20A.%20Hebert%2C%20M.%20The%20pose%20knows%3A%20Video%20forecasting%20by%20generating%20pose%20futures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20J.%20Marino%2C%20K.%20Gupta%2C%20A.%20Hebert%2C%20M.%20The%20pose%20knows%3A%20Video%20forecasting%20by%20generating%20pose%20futures%202017"
        },
        {
            "id": "72",
            "entry": "[72] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Tao%2C%20A.%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Tao%2C%20A.%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20GANs%202018"
        },
        {
            "id": "73",
            "entry": "[73] Y. Wexler, E. Shechtman, and M. Irani. Space-time video completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wexler%2C%20Y.%20Shechtman%2C%20E.%20Irani%2C%20M.%20Space-time%20video%20completion%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wexler%2C%20Y.%20Shechtman%2C%20E.%20Irani%2C%20M.%20Space-time%20video%20completion%202004"
        },
        {
            "id": "74",
            "entry": "[74] Y. Wexler, E. Shechtman, and M. Irani. Space-time completion of video. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 29(3), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wexler%2C%20Y.%20Shechtman%2C%20E.%20Irani%2C%20M.%20Space-time%20completion%20of%20video%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wexler%2C%20Y.%20Shechtman%2C%20E.%20Irani%2C%20M.%20Space-time%20completion%20of%20video%202007"
        },
        {
            "id": "75",
            "entry": "[75] S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "76",
            "entry": "[76] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xue%2C%20T.%20Wu%2C%20J.%20Bouman%2C%20K.%20Freeman%2C%20B.%20Visual%20dynamics%3A%20Probabilistic%20future%20frame%20synthesis%20via%20cross%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xue%2C%20T.%20Wu%2C%20J.%20Bouman%2C%20K.%20Freeman%2C%20B.%20Visual%20dynamics%3A%20Probabilistic%20future%20frame%20synthesis%20via%20cross%20convolutional%20networks%202016"
        },
        {
            "id": "77",
            "entry": "[77] C. Yang, Z. Wang, X. Zhu, C. Huang, J. Shi, and D. Lin. Pose guided human video generation. In European Conference on Computer Vision (ECCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20C.%20Wang%2C%20Z.%20Zhu%2C%20X.%20Huang%2C%20C.%20Pose%20guided%20human%20video%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20C.%20Wang%2C%20Z.%20Zhu%2C%20X.%20Huang%2C%20C.%20Pose%20guided%20human%20video%20generation%202018"
        },
        {
            "id": "78",
            "entry": "[78] S. Yang, T. Ambert, Z. Pan, K. Wang, L. Yu, T. Berg, and M. C. Lin. Detailed garment recovery from a single-view image. arXiv preprint arXiv:1608.01250, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.01250"
        },
        {
            "id": "79",
            "entry": "[79] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Xu%2C%20T.%20Li%2C%20H.%20Zhang%2C%20S.%20Metaxas.%20StackGAN%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Xu%2C%20T.%20Li%2C%20H.%20Zhang%2C%20S.%20Metaxas.%20StackGAN%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "80",
            "entry": "[80] Z.-H. Zheng, H.-T. Zhang, F.-L. Zhang, and T.-J. Mu. Image-based clothes changing system. Computational Visual Media, 3(4):337\u2013347, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Z.-H.%20Zhang%2C%20H.-T.%20Zhang%2C%20F.-L.%20Mu%2C%20T.-J.%20Image-based%20clothes%20changing%20system%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Z.-H.%20Zhang%2C%20H.-T.%20Zhang%2C%20F.-L.%20Mu%2C%20T.-J.%20Image-based%20clothes%20changing%20system%202017"
        },
        {
            "id": "81",
            "entry": "[81] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycleconsistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycleconsistent%20adversarial%20networks%202017"
        },
        {
            "id": "82",
            "entry": "[82] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems (NIPS), 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017"
        }
    ]
}
