{
    "filename": "8004-deep-state-space-models-for-time-series-forecasting.pdf",
    "metadata": {
        "title": "Deep State Space Models for Time Series Forecasting",
        "author": "Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, Tim Januschowski",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from large collection of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art."
    },
    "keywords": [
        {
            "term": "time series forecasting",
            "url": "https://en.wikipedia.org/wiki/Time_Series_Forecasting"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "matrix factorization",
            "url": "https://en.wikipedia.org/wiki/matrix_factorization"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "state space model",
            "url": "https://en.wikipedia.org/wiki/State_Space_Model"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "probabilistic forecasting",
            "url": "https://en.wikipedia.org/wiki/probabilistic_forecasting"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        }
    ],
    "highlights": [
        "State space models [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] (SSMs) provide a principled framework for modeling and learning time series patterns such as trend and seasonality",
        "We present a forecasting method that parametrizes a particular linear state-space models using a recurrent neural network (RNN)",
        "In this paper we propose a new approach to time series forecasting by marrying state space models with deep recurrent neural networks",
        "Our experiments on synthetic data suggest that the model is capable of accurately recovering the parameters of the state space model from which the data is generated",
        "On real-world datasets, that the proposed method achieves stateof-the-art performance by comparing it against a recent recurrent neural network-based method, a matrix factorization method, as well as classical approaches such as ARIMA and ETS",
        "Under regimes of limited data our method clearly outperforms the other methods by explicitly modelling seasonal structure"
    ],
    "key_statements": [
        "State space models [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] (SSMs) provide a principled framework for modeling and learning time series patterns such as trend and seasonality",
        "In this paper we propose to bridge the gap between these two approaches by fusing state-space models with deep neural networks",
        "We present a forecasting method that parametrizes a particular linear state-space models using a recurrent neural network (RNN)",
        "Gthiveesntattheesppoascteerpiaorraomfettheersla\u21e5te1Tt istoabtep, prediction samples are generated by recursively applying the transition equation and the are observation obtained by model (Eq 8) where the state space unrolling the recurrent neural network in the prediction parameters range",
        "Note that in our model, in contrast to classical and deep learning-based auto-regressive models (e.g. DeepAR [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]), target values are not used as inputs directly. This is a key feature of our method, and brings several advantages: (i) It makes the model more robust to noise, as target values are only incorporated through the likelihood term, where noise is properly accounted for; Missing target values can be handled by dropping the corresponding likelihood terms; Forecast sample path generation is computationally more efficient, as the recurrent neural network needs to be unrolled only once for the entire prediction, whereas auto-regressive models (e.g. [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]) have to be unrolled for each sample",
        "Since MatFact only produces point forecasts, we report normalized deviation as in [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], which in our case is equal to p50-loss",
        "In this paper we propose a new approach to time series forecasting by marrying state space models with deep recurrent neural networks",
        "Our experiments on synthetic data suggest that the model is capable of accurately recovering the parameters of the state space model from which the data is generated",
        "On real-world datasets, that the proposed method achieves stateof-the-art performance by comparing it against a recent recurrent neural network-based method, a matrix factorization method, as well as classical approaches such as ARIMA and ETS",
        "Under regimes of limited data our method clearly outperforms the other methods by explicitly modelling seasonal structure",
        "Some ideas of extending our method to non-Gaussian likelihoods are discussed in Appendix A.4"
    ],
    "summary": [
        "State space models [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] (SSMs) provide a principled framework for modeling and learning time series patterns such as trend and seasonality.",
        "The parameters of the RNN are learned jointly from a dataset of raw time series and associated covariates, allowing the model to automatically extract features and learn complex temporal patterns.",
        "Denotes the set of learnable parameters of the model, which are shared between and learned jointly from all N time series.",
        "SSMs model the temporal structure of the data via a latent state lt 2 RL that can be used to encode time series components such as level, trend, and seasonality patterns.",
        "Where pSS denotes the marginal likelihood under a linear state space model as defined in Eq 4, given its",
        "We parameterize the mapping from covariates to state space model parameters using a deep recurrent neural network (RNN).",
        "For both training and prediction ranges, forecasts are produced as follows: (i) first the posterior of the latent state p for the last time step Ti in the training range is computed using the tained by unrolling the RNN network in tohbesetrraviantiinognsrazn1ei; and",
        "Gthiveesntattheesppoascteerpiaorraomfettheersla\u21e5te1Tt istoabtep, prediction samples are generated by recursively applying the transition equation and the are observation obtained by model (Eq 8) where the state space unrolling the RNN in the prediction parameters range.",
        "Essentially uses the same basic primitives as that of classical methods that learn parameters per time series independently.",
        "Once the network parameters are learned, we can use them to address our original problem specified in Eq 1, i.e., to make probabilistic forecasts for each given time series.",
        "This is a key feature of our method, and brings several advantages: (i) It makes the model more robust to noise, as target values are only incorporated through the likelihood term, where noise is properly accounted for; Missing target values can be handled by dropping the corresponding likelihood terms; Forecast sample path generation is computationally more efficient, as the RNN needs to be unrolled only once for the entire prediction, whereas auto-regressive models (e.g.",
        "We test whether our model effectively recovers the state space parameters if trained on synthetic data.",
        "We generate five groups of time series from day-of-week seasonality model but with different initial states and innovation parameters per group.",
        "In this paper we propose a new approach to time series forecasting by marrying state space models with deep recurrent neural networks.",
        "Some ideas of extending our method to non-Gaussian likelihoods are discussed in Appendix A.4"
    ],
    "headline": "We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Amazon Sagemaker: DeepAR Forecasting. sagemaker/latest/dg/deepar.html. https://docs.aws.amazon.com/",
            "url": "https://docs.aws.amazon.com/"
        },
        {
            "id": "2",
            "entry": "[2] G. Athanasopoulos, R. Hyndman, H. Song, and D. Wu. The tourism forecasting competition. International Journal of Forecasting, 27(3):822\u2013844, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Athanasopoulos%2C%20G.%20Hyndman%2C%20R.%20Song%2C%20H.%20Wu%2C%20D.%20The%20tourism%20forecasting%20competition%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Athanasopoulos%2C%20G.%20Hyndman%2C%20R.%20Song%2C%20H.%20Wu%2C%20D.%20The%20tourism%20forecasting%20competition%202011"
        },
        {
            "id": "3",
            "entry": "[3] D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barber%2C%20D.%20Bayesian%20Reasoning%20and%20Machine%20Learning%202012"
        },
        {
            "id": "4",
            "entry": "[4] George E. P. Box and Gwilym M. Jenkins. Some recent advances in forecasting and control. Journal of the Royal Statistical Society. Series C (Applied Statistics), 17(2):91\u2013109, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Box%2C%20George%20E.P.%20Jenkins%2C%20Gwilym%20M.%20Some%20recent%20advances%20in%20forecasting%20and%20control%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Box%2C%20George%20E.P.%20Jenkins%2C%20Gwilym%20M.%20Some%20recent%20advances%20in%20forecasting%20and%20control%201968"
        },
        {
            "id": "5",
            "entry": "[5] George EP Box and David R Cox. An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), pages 211\u2013252, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Box%2C%20George%20E.P.%20Cox%2C%20David%20R.%20An%20analysis%20of%20transformations%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Box%2C%20George%20E.P.%20Cox%2C%20David%20R.%20An%20analysis%20of%20transformations%201964"
        },
        {
            "id": "6",
            "entry": "[6] Nicolas Chapados. Effective bayesian modeling of groups of related count time series. In International Conference on Machine Learning, pages 1395\u20131403, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapados%2C%20Nicolas%20Effective%20bayesian%20modeling%20of%20groups%20of%20related%20count%20time%20series%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapados%2C%20Nicolas%20Effective%20bayesian%20modeling%20of%20groups%20of%20related%20count%20time%20series%202014"
        },
        {
            "id": "7",
            "entry": "[7] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pages 2980\u20132988, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Junyoung%20Kastner%2C%20Kyle%20Dinh%2C%20Laurent%20Goel%2C%20Kratarth%20and%20Yoshua%20Bengio.%20A%20recurrent%20latent%20variable%20model%20for%20sequential%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20Junyoung%20Kastner%2C%20Kyle%20Dinh%2C%20Laurent%20Goel%2C%20Kratarth%20and%20Yoshua%20Bengio.%20A%20recurrent%20latent%20variable%20model%20for%20sequential%20data%202015"
        },
        {
            "id": "8",
            "entry": "[8] James Durbin and Siem Jan Koopman. Time series analysis by state space methods, volume 38. OUP Oxford, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durbin%2C%20James%20Koopman%2C%20Siem%20Jan%20Time%20series%20analysis%20by%20state%20space%20methods%2C%20volume%2038%202012"
        },
        {
            "id": "9",
            "entry": "[9] Valentin Flunkert, David Salinas, and Jan Gasthaus. DeepAR: Probabilistic forecasting with autoregressive recurrent networks. CoRR, abs/1704.04110, 2017. URL http://arxiv.org/abs/1704.04110.",
            "url": "http://arxiv.org/abs/1704.04110",
            "arxiv_url": "https://arxiv.org/pdf/1704.04110"
        },
        {
            "id": "10",
            "entry": "[10] Marco Fraccaro, S\u00f8ren Kaae S\u00f8nderby, Ulrich Paquet, and Ole Winther. Sequential neural models with stochastic layers. In Advances in neural information processing systems, pages 2199\u20132207, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fraccaro%2C%20Marco%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Paquet%2C%20Ulrich%20Winther%2C%20Ole%20Sequential%20neural%20models%20with%20stochastic%20layers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fraccaro%2C%20Marco%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Paquet%2C%20Ulrich%20Winther%2C%20Ole%20Sequential%20neural%20models%20with%20stochastic%20layers%202016"
        },
        {
            "id": "11",
            "entry": "[11] Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information Processing Systems, pages 3604\u20133613, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fraccaro%2C%20Marco%20Kamronn%2C%20Simon%20Paquet%2C%20Ulrich%20Winther%2C%20Ole%20A%20disentangled%20recognition%20and%20nonlinear%20dynamics%20model%20for%20unsupervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fraccaro%2C%20Marco%20Kamronn%2C%20Simon%20Paquet%2C%20Ulrich%20Winther%2C%20Ole%20A%20disentangled%20recognition%20and%20nonlinear%20dynamics%20model%20for%20unsupervised%20learning%202017"
        },
        {
            "id": "12",
            "entry": "[12] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.0850"
        },
        {
            "id": "13",
            "entry": "[13] R. Hyndman, A. B. Koehler, J. K. Ord, and R. D. Snyder. Forecasting with Exponential Smoothing: The State Space Approach. Springer Series in Statistics. Springer, 2008. ISBN 9783540719182.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyndman%2C%20R.%20Koehler%2C%20A.B.%20Ord%2C%20J.K.%20Snyder%2C%20R.D.%20Forecasting%20with%20Exponential%20Smoothing%3A%20The%20State%20Space%20Approach.%20Springer%20Series%20in%20Statistics%202008"
        },
        {
            "id": "14",
            "entry": "[14] Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. , pages 2946\u20132954, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthew%20Johnson%20David%20K%20Duvenaud%20Alex%20Wiltschko%20Ryan%20P%20Adams%20and%20Sandeep%20R%20Datta%20%20pages%2029462954%202016"
        },
        {
            "id": "15",
            "entry": "[15] Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karl%2C%20Maximilian%20Soelch%2C%20Maximilian%20Bayer%2C%20Justin%20van%20der%20Smagt%2C%20Patrick%20Deep%20variational%20bayes%20filters%3A%20Unsupervised%20learning%20of%20state%20space%20models%20from%20raw%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karl%2C%20Maximilian%20Soelch%2C%20Maximilian%20Bayer%2C%20Justin%20van%20der%20Smagt%2C%20Patrick%20Deep%20variational%20bayes%20filters%3A%20Unsupervised%20learning%20of%20state%20space%20models%20from%20raw%20data%202017"
        },
        {
            "id": "16",
            "entry": "[16] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "17",
            "entry": "[17] Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05121"
        },
        {
            "id": "18",
            "entry": "[18] Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space models. In AAAI, pages 2101\u20132109, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnan%2C%20Rahul%20G.%20Shalit%2C%20Uri%20Sontag%2C%20David%20Structured%20inference%20networks%20for%20nonlinear%20state%20space%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishnan%2C%20Rahul%20G.%20Shalit%2C%20Uri%20Sontag%2C%20David%20Structured%20inference%20networks%20for%20nonlinear%20state%20space%20models%202017"
        },
        {
            "id": "19",
            "entry": "[19] Nikolay Laptev, Jason Yosinsk, Li Li Erran, and Slawek Smyl. Time-series extreme event forecasting with neural networks at Uber. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laptev%2C%20Nikolay%20Yosinsk%2C%20Jason%20Erran%2C%20Li%20Li%20Smyl%2C%20Slawek%20Time-series%20extreme%20event%20forecasting%20with%20neural%20networks%20at%20Uber",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laptev%2C%20Nikolay%20Yosinsk%2C%20Jason%20Erran%2C%20Li%20Li%20Smyl%2C%20Slawek%20Time-series%20extreme%20event%20forecasting%20with%20neural%20networks%20at%20Uber"
        },
        {
            "id": "20",
            "entry": "[20] S. Makridakis, E. Spiliotis, and V. Assimakopoulos. The M4 competition: Results, findings, conclusion and way forward. International Journal of Forecasting, 34(4):802\u2013808, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Makridakis%2C%20S.%20Spiliotis%2C%20E.%20Assimakopoulos%2C%20V.%20The%20M4%20competition%3A%20Results%2C%20findings%2C%20conclusion%20and%20way%20forward%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Makridakis%2C%20S.%20Spiliotis%2C%20E.%20Assimakopoulos%2C%20V.%20The%20M4%20competition%3A%20Results%2C%20findings%2C%20conclusion%20and%20way%20forward%202018"
        },
        {
            "id": "21",
            "entry": "[21] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pages 1278\u20131286, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "22",
            "entry": "[22] Matthias Seeger, Syama Rangapuram, Yuyang Wang, David Salinas, Jan Gasthaus, Tim Januschowski, and Valentin Flunkert. Approximate Bayesian inference in linear state space models for intermittent demand forecasting at scale. CoRR, abs/1704.04110, 2017. URL http://arxiv.org/abs/1704.04110.",
            "url": "http://arxiv.org/abs/1704.04110",
            "arxiv_url": "https://arxiv.org/pdf/1704.04110"
        },
        {
            "id": "23",
            "entry": "[23] Matthias W Seeger, David Salinas, and Valentin Flunkert. Bayesian intermittent demand forecasting for large inventories. In Advances in Neural Information Processing Systems, pages 4646\u20134654, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20Matthias%20W.%20Salinas%2C%20David%20Flunkert%2C%20Valentin%20Bayesian%20intermittent%20demand%20forecasting%20for%20large%20inventories%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20Matthias%20W.%20Salinas%2C%20David%20Flunkert%2C%20Valentin%20Bayesian%20intermittent%20demand%20forecasting%20for%20large%20inventories%202016"
        },
        {
            "id": "24",
            "entry": "[24] Matthias W Seeger, David Salinas, and Valentin Flunkert. Bayesian intermittent demand forecasting for large inventories. In Advances in Neural Information Processing Systems, pages 4646\u20134654, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20Matthias%20W.%20Salinas%2C%20David%20Flunkert%2C%20Valentin%20Bayesian%20intermittent%20demand%20forecasting%20for%20large%20inventories%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20Matthias%20W.%20Salinas%2C%20David%20Flunkert%2C%20Valentin%20Bayesian%20intermittent%20demand%20forecasting%20for%20large%20inventories%202016"
        },
        {
            "id": "25",
            "entry": "[25] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "26",
            "entry": "[26] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. CoRR, abs/1609.03499, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "27",
            "entry": "[27] Ruofeng Wen Wen, Kari Torkkola, and Balakrishnan Narayanaswamy. A multi-horizon quantile recurrent forecaster. In NIPS Time Series Workshop. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Ruofeng%20Wen%20Torkkola%2C%20Kari%20Narayanaswamy%2C%20Balakrishnan%20A%20multi-horizon%20quantile%20recurrent%20forecaster%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Ruofeng%20Wen%20Torkkola%2C%20Kari%20Narayanaswamy%2C%20Balakrishnan%20A%20multi-horizon%20quantile%20recurrent%20forecaster%202017"
        },
        {
            "id": "28",
            "entry": "[28] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for high-dimensional time series prediction. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 847\u2013855. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Hsiang-Fu%20Rao%2C%20Nikhil%20Dhillon%2C%20Inderjit%20S.%20Temporal%20regularized%20matrix%20factorization%20for%20high-dimensional%20time%20series%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Hsiang-Fu%20Rao%2C%20Nikhil%20Dhillon%2C%20Inderjit%20S.%20Temporal%20regularized%20matrix%20factorization%20for%20high-dimensional%20time%20series%20prediction%202016"
        },
        {
            "id": "29",
            "entry": "[29] Manzil Zaheer, Amr Ahmed, and Alexander J Smola. Latent LSTM allocation: Joint clustering and non-linear dynamic modeling of sequence data. In International Conference on Machine Learning, pages 3967\u20133976, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaheer%2C%20Manzil%20Ahmed%2C%20Amr%20Smola%2C%20Alexander%20J.%20Latent%20LSTM%20allocation%3A%20Joint%20clustering%20and%20non-linear%20dynamic%20modeling%20of%20sequence%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaheer%2C%20Manzil%20Ahmed%2C%20Amr%20Smola%2C%20Alexander%20J.%20Latent%20LSTM%20allocation%3A%20Joint%20clustering%20and%20non-linear%20dynamic%20modeling%20of%20sequence%20data%202017"
        },
        {
            "id": "30",
            "entry": "[30] Xun Zheng, Manzil Zaheer, Amr Ahmed, Yuan Wang, Eric P Xing, and Alexander J Smola. State space LSTM models with particle MCMC inference. arXiv preprint arXiv:1711.11179, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1711.11179"
        }
    ]
}
