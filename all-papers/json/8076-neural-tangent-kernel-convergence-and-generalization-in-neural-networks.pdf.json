{
    "filename": "8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf",
    "metadata": {
        "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
        "author": "Arthur Jacot-Guillarmod, Clement Hongler, Franck Gabriel",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function f\u03b8 (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinitewidth limit, the network function f\u03b8 follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit."
    },
    "keywords": [
        {
            "term": "Artificial neural networks",
            "url": "https://en.wikipedia.org/wiki/Artificial_neural_networks"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "positive definiteness",
            "url": "https://en.wikipedia.org/wiki/positive_definiteness"
        },
        {
            "term": "function space",
            "url": "https://en.wikipedia.org/wiki/function_space"
        },
        {
            "term": "network function",
            "url": "https://en.wikipedia.org/wiki/network_function"
        }
    ],
    "highlights": [
        "Artificial neural networks (ANNs) have achieved impressive results in numerous areas of machine learning",
        "During gradient descent, we show that the dynamics of f\u03b8 follows that of the so-called kernel gradient descent in function space with respect to a limiting kernel, which only depends on the depth of the network, the choice of nonlinearity and the initialization variance",
        "We will show that during training, the network function f\u03b8 follows a descent along the kernel gradient with respect to the Neural Tangent Kernel (NTK) which we introduce in Section 4",
        "This paper introduces a new tool to study Artificial neural networks, the Neural Tangent Kernel (NTK), which describes the local dynamics of an Artificial neural networks during gradient descent",
        "This leads to a new connection between Artificial neural networks training and kernel methods: in the infinite-width limit, an Artificial neural networks can be described in the function space directly by the limit of the Neural Tangent Kernel, an explicit constant kernel \u0398(\u221eL), which only depends on its depth, nonlinearity and parameter initialization variance",
        "In this limit, Artificial neural networks gradient descent is shown to be equivalent to a kernel gradient descent with respect to \u0398\u221e (L)"
    ],
    "key_statements": [
        "Artificial neural networks (ANNs) have achieved impressive results in numerous areas of machine learning",
        "During gradient descent, we show that the dynamics of f\u03b8 follows that of the so-called kernel gradient descent in function space with respect to a limiting kernel, which only depends on the depth of the network, the choice of nonlinearity and the initialization variance",
        "We will show that during training, the network function f\u03b8 follows a descent along the kernel gradient with respect to the Neural Tangent Kernel (NTK) which we introduce in Section 4",
        "As a starting point to understand the convergence of Artificial neural networks gradient descent to kernel gradient descent in the infinite-width limit, we introduce a simple model, inspired by the approach of [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "This paper introduces a new tool to study Artificial neural networks, the Neural Tangent Kernel (NTK), which describes the local dynamics of an Artificial neural networks during gradient descent",
        "This leads to a new connection between Artificial neural networks training and kernel methods: in the infinite-width limit, an Artificial neural networks can be described in the function space directly by the limit of the Neural Tangent Kernel, an explicit constant kernel \u0398(\u221eL), which only depends on its depth, nonlinearity and parameter initialization variance",
        "In this limit, Artificial neural networks gradient descent is shown to be equivalent to a kernel gradient descent with respect to \u0398\u221e (L)"
    ],
    "summary": [
        "Artificial neural networks (ANNs) have achieved impressive results in numerous areas of machine learning.",
        "We will see that in the same limit, the behavior of ANNs during training is described by a related kernel, which we call the neural tangent network (NTK).",
        "In the limit as the widths of the hidden layers tend to infinity, the network function at initialization, f\u03b8 converges to a Gaussian distribution [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>].",
        "For a least-squares regression loss, the network function f\u03b8 follows a linear differential equation in the infinite-width limit, and the eigenfunctions of the Jacobian are the kernel principal components of the input data.",
        "We will show that during training, the network function f\u03b8 follows a descent along the kernel gradient with respect to the Neural Tangent Kernel (NTK) which we introduce in Section 4.",
        "For a network of depth L at initialization, with a Lipschitz nonlinearity \u03c3, and in the limit as n1, ..., nL\u22121 \u2192 \u221e, the output functions f\u03b8,k, for k = 1, ..., nL, tend to iid centered Gaussian processes of covariance \u03a3(L), where \u03a3(L) is defined recursively by: \u03a3(1)(x, x ) = 1 xT x + \u03b22 n0",
        "For a network of depth L at initialization, with a Lipschitz nonlinearity \u03c3, and in the limit as the layers width n1, ..., nL\u22121 \u2192 \u221e, the NTK \u0398(L) converges in probability to a deterministic limiting kernel: \u0398(L) \u2192 \u0398(\u221eL) \u2297 IdnL .",
        "The infinite-width limit network function f\u03b8(t) has a Gaussian distribution for all times t and in particular at convergence t \u2192 \u221e.",
        "For two different widths n = 50, 1000 and for 10 random initializations each, a network is trained on a least-squares cost on 4 points of the unit circle for 1000 steps with learning rate 1.0 and plotted in Figure 2.",
        "In the infinite-width limit, the first component decays exponentially fast ||gt||pin = 0.5e\u2212\u03bb2t while the second is null, as the function converges along a straight line.",
        "This paper introduces a new tool to study ANNs, the Neural Tangent Kernel (NTK), which describes the local dynamics of an ANN during gradient descent.",
        "This leads to a new connection between ANN training and kernel methods: in the infinite-width limit, an ANN can be described in the function space directly by the limit of the NTK, an explicit constant kernel \u0398(\u221eL), which only depends on its depth, nonlinearity and parameter initialization variance.",
        "The analysis of training using NTK allows one to relate convergence of ANN training with the positive-definiteness of the limiting NTK and leads to a characterization of the directions favored by early stopping methods"
    ],
    "headline": "We prove that the evolution of an Artificial neural networks during training can be described by a kernel: during gradient descent on the parameters of an Artificial neural networks, the network function f\u03b8  follows the kernel gradient of the functional cost  w.r.t. a new kernel: the Neural Tangent Kernel ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel learning. arXiv preprint, Feb 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20M.%20Ma%2C%20S.%20Mandal%2C%20S.%20To%20understand%20deep%20learning%20we%20need%20to%20understand%20kernel%20learning.%20arXiv%20p%202018-02"
        },
        {
            "id": "2",
            "entry": "[2] Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information Processing Systems 22, pages 342\u2013350. Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Y.%20Saul%2C%20L.K.%20Kernel%20methods%20for%20deep%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Y.%20Saul%2C%20L.K.%20Kernel%20methods%20for%20deep%20learning%202009"
        },
        {
            "id": "3",
            "entry": "[3] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The Loss Surfaces of Multilayer Networks. Journal of Machine Learning Research, 38:192\u2013204, nov 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20Loss%20Surfaces%20of%20Multilayer%20Networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20Loss%20Surfaces%20of%20Multilayer%20Networks"
        },
        {
            "id": "4",
            "entry": "[4] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Proceedings of the 27th International Conference on Neural Information Processing Systems Volume 2, NIPS\u201914, pages 2933\u20132941, Cambridge, MA, USA, 2014. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "5",
            "entry": "[5] S. S. Dragomir. Some Gronwall Type Inequalities and Applications. Nova Science Publishers, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dragomir%2C%20S.S.%20Some%20Gronwall%20Type%20Inequalities%20and%20Applications%202003"
        },
        {
            "id": "6",
            "entry": "[6] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Networks. NIPS\u201914 Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, pages 2672\u20132680, jun 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20Adversarial%20Networks.%20NIPS%E2%80%9914%202014-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20Adversarial%20Networks.%20NIPS%E2%80%9914%202014-06"
        },
        {
            "id": "7",
            "entry": "[7] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359 \u2013 366, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989"
        },
        {
            "id": "8",
            "entry": "[8] R. Karakida, S. Akaho, and S.-i. Amari. Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach. jun 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karakida%2C%20R.%20Akaho%2C%20S.%20Amari%2C%20S.-i%20Universal%20Statistics%20of%20Fisher%20Information%20in%20Deep%20Neural%20Networks%3A%20Mean%20Field%20Approach%202018-06"
        },
        {
            "id": "9",
            "entry": "[9] J. H. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20J.H.%20Bahri%2C%20Y.%20Novak%2C%20R.%20Schoenholz%2C%20S.S.%20Deep%20neural%20networks%20as%20gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20J.H.%20Bahri%2C%20Y.%20Novak%2C%20R.%20Schoenholz%2C%20S.S.%20Deep%20neural%20networks%20as%20gaussian%20processes%202018"
        },
        {
            "id": "10",
            "entry": "[10] M. Leshno, V. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6(6):861\u2013867, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leshno%2C%20M.%20Lin%2C%20V.%20Pinkus%2C%20A.%20Schocken%2C%20S.%20Multilayer%20feedforward%20networks%20with%20a%20nonpolynomial%20activation%20function%20can%20approximate%20any%20function%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leshno%2C%20M.%20Lin%2C%20V.%20Pinkus%2C%20A.%20Schocken%2C%20S.%20Multilayer%20feedforward%20networks%20with%20a%20nonpolynomial%20activation%20function%20can%20approximate%20any%20function%201993"
        },
        {
            "id": "11",
            "entry": "[11] S. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20S.%20Montanari%2C%20A.%20Nguyen%2C%20P.-M.%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20S.%20Montanari%2C%20A.%20Nguyen%2C%20P.-M.%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018"
        },
        {
            "id": "12",
            "entry": "[12] R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20R.M.%20Bayesian%20Learning%20for%20Neural%20Networks%201996"
        },
        {
            "id": "13",
            "entry": "[13] R. Pascanu, Y. N. Dauphin, S. Ganguli, and Y. Bengio. On the saddle point problem for non-convex optimization. arXiv preprint, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20R.%20Dauphin%2C%20Y.N.%20Ganguli%2C%20S.%20Bengio%2C%20Y.%20On%20the%20saddle%20point%20problem%20for%20non-convex%20optimization.%20arXiv%20p%202014"
        },
        {
            "id": "14",
            "entry": "[14] J. Pennington and Y. Bahri. Geometry of neural network loss surfaces via random matrix theory. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2798\u20132806, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Bahri%2C%20Y.%20Geometry%20of%20neural%20network%20loss%20surfaces%20via%20random%20matrix%20theory%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20J.%20Bahri%2C%20Y.%20Geometry%20of%20neural%20network%20loss%20surfaces%20via%20random%20matrix%20theory%202017-08"
        },
        {
            "id": "15",
            "entry": "[15] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems 20, pages 1177\u20131184. Curran Associates, Inc., 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "16",
            "entry": "[16] L. Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. Empirical analysis of the hessian of over-parametrized neural networks. CoRR, abs/1706.04454, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04454"
        },
        {
            "id": "17",
            "entry": "[17] B. Scholkopf, A. Smola, and K.-R. Muller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299\u20131319, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholkopf%2C%20B.%20Smola%2C%20A.%20Muller%2C%20K.-R.%20Nonlinear%20component%20analysis%20as%20a%20kernel%20eigenvalue%20problem%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scholkopf%2C%20B.%20Smola%2C%20A.%20Muller%2C%20K.-R.%20Nonlinear%20component%20analysis%20as%20a%20kernel%20eigenvalue%20problem%201998"
        },
        {
            "id": "18",
            "entry": "[18] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shawe-Taylor%2C%20J.%20Cristianini%2C%20N.%20Kernel%20Methods%20for%20Pattern%20Analysis%202004"
        },
        {
            "id": "19",
            "entry": "[19] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. ICLR 2017 proceedings, Feb 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017-02"
        }
    ]
}
