{
    "filename": "7579-deep-reinforcement-learning-of-marked-temporal-point-processes.pdf",
    "metadata": {
        "title": "Deep Reinforcement Learning of Marked Temporal Point Processes",
        "author": "Utkarsh Upadhyay, Abir De, Manuel Gomez Rodriguez",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7579-deep-reinforcement-learning-of-marked-temporal-point-processes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent\u2019s policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent\u2019s actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives."
    },
    "keywords": [
        {
            "term": "stochastic differential equations",
            "url": "https://en.wikipedia.org/wiki/stochastic_differential_equations"
        },
        {
            "term": "policy gradient method",
            "url": "https://en.wikipedia.org/wiki/policy_gradient_method"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "functional form",
            "url": "https://en.wikipedia.org/wiki/functional_form"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "continuous time",
            "url": "https://en.wikipedia.org/wiki/continuous_time"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        }
    ],
    "highlights": [
        "The framework of marked temporal point processes (MTPPs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] has become increasingly popular for modeling asynchronous event data in continuous time, which is ubiquitous in a wide range of application domains, from social and information networks to finance or health informatics",
        "We first briefly revisit the theoretical framework of marked temporal point processes [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and use it to formally define our novel reinforcement learning problem, where an agent interacts with a complex environment by means of asynchronous stochastic discrete events in continuous time.\n1Our setting should not be confused with the asynchronous setting of Mnih et al [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], where the gradient descent is asynchronous but the action/observations are synchronous and the system evolves at discrete time steps.\n2 https://github.com/Networks-Learning/tpprl",
        "We tackle the reinforcement learning problem defined by Eq 3 using a novel policy gradient method for marked temporal point processes",
        "Note that the proposition formally shows that the REINFORCE trick [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] is still valid if the expectation is taken over realizations of marked temporal point processes, which are a type of random elements [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] whose values are discrete events localized in continuous time",
        "We derived a flexible policy gradient method, which does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions"
    ],
    "key_statements": [
        "The framework of marked temporal point processes (MTPPs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] has become increasingly popular for modeling asynchronous event data in continuous time, which is ubiquitous in a wide range of application domains, from social and information networks to finance or health informatics",
        "They make strong assumptions about the functional form of the conditional intensities and mark distributions of the marked temporal point processes, which in turn prevent them from using state of the art marked temporal point processes models based on deep learning [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "We first introduce a novel reinforcement learning problem where both the actions taken by an agent and the feedback it receives from its environment are asynchronous stochastic events in continuous time, which are characterized using marked temporal point processes",
        "We first briefly revisit the theoretical framework of marked temporal point processes [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and use it to formally define our novel reinforcement learning problem, where an agent interacts with a complex environment by means of asynchronous stochastic discrete events in continuous time.\n1Our setting should not be confused with the asynchronous setting of Mnih et al [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], where the gradient descent is asynchronous but the action/observations are synchronous and the system evolves at discrete time steps.\n2 https://github.com/Networks-Learning/tpprl",
        "Both the actions and the feedback are asynchronous stochastic events localized in time and we characterize them using marked temporal point processes (MTPPs), i.e.,",
        "We tackle the reinforcement learning problem defined by Eq 3 using a novel policy gradient method for marked temporal point processes",
        "To implement the above policy p\u21e4A;\u2713 = ( \u2713\u21e4, m\u2713\u21e4), we need to be able to sample the action times t and marks y from the intensity function defined by Eq 5 and the mark distribution defined by Eq 6, respectively",
        "Note that the proposition formally shows that the REINFORCE trick [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] is still valid if the expectation is taken over realizations of marked temporal point processes, which are a type of random elements [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] whose values are discrete events localized in continuous time",
        "We derived a flexible policy gradient method, which does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions"
    ],
    "summary": [
        "The framework of marked temporal point processes (MTPPs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] has become increasingly popular for modeling asynchronous event data in continuous time, which is ubiquitous in a wide range of application domains, from social and information networks to finance or health informatics.",
        "We first briefly revisit the theoretical framework of marked temporal point processes [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and use it to formally define our novel reinforcement learning problem, where an agent interacts with a complex environment by means of asynchronous stochastic discrete events in continuous time.",
        "Both the actions and the feedback are asynchronous stochastic events localized in time and we characterize them using marked temporal point processes (MTPPs), i.e.,",
        "We tackle the reinforcement learning problem defined by Eq 3 using a novel policy gradient method for marked temporal point processes.",
        "At any time t, the policy pA\u21e4 ;\u2713 that maximizes the reward may depend on the previous history of the action events and the feedback events, Ht = At [ Ft, in an unknown and complex way.",
        "To implement the above policy p\u21e4A;\u2713 = ( \u2713\u21e4, m\u2713\u21e4), we need to be able to sample the action times t and marks y from the intensity function defined by Eq 5 and the mark distribution defined by Eq 6, respectively.",
        "The gradient of the log-likelihood of the times and marks of a realization of the marked temporal point process associated to the agent\u2019s actions, r\u2713 log P\u21e4A;\u2713(HT ), can be computed using the policy parametrization defined by Eqs. 5 and 6.",
        "Note that the proposition formally shows that the REINFORCE trick [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] is still valid if the expectation is taken over realizations of marked temporal point processes, which are a type of random elements [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] whose values are discrete events localized in continuous time.",
        "Feeds are sorted in reverse chronological order, previous work has derived optimal offline [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] and online [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] algorithms for (i) and, respectively, under the additional assumption that the posting intensity of other users her followers follow adopts certain functional form.",
        "Since our feed sorting algorithm does only depends on the time of the post and the identity of the user who posts, not marks, the optimal policy only comprises an intensity function, i.e., pA\u21e4 ;\u2713 = \u2713\u21e4(t).",
        "We approached a novel reinforcement learning problem where both actions and feedback are asynchronous stochastic events in continuous time, characterized using marked temporal point processes (MTPPs).",
        "We derived a flexible policy gradient method, which does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions.",
        "Experiments on two different applications in personalized teaching and viral marketing show that our method beats competing methods"
    ],
    "headline": "Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] O. Aalen, O. Borgan, and H. Gjessing. Survival and event history analysis: a process point of view. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aalen%2C%20O.%20Borgan%2C%20O.%20Gjessing%2C%20H.%20Survival%20and%20event%20history%20analysis%3A%20a%20process%20point%20of%20view%202008"
        },
        {
            "id": "2",
            "entry": "[2] M. Cha, H. Haddadi, F. Benevenuto, and P. K. Gummadi. Measuring user influence in twitter: The million follower fallacy. ICWSM, 10(10-17):30, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cha%2C%20M.%20Haddadi%2C%20H.%20Benevenuto%2C%20F.%20Gummadi%2C%20P.K.%20Measuring%20user%20influence%20in%20twitter%3A%20The%20million%20follower%20fallacy%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cha%2C%20M.%20Haddadi%2C%20H.%20Benevenuto%2C%20F.%20Gummadi%2C%20P.K.%20Measuring%20user%20influence%20in%20twitter%3A%20The%20million%20follower%20fallacy%202010"
        },
        {
            "id": "3",
            "entry": "[3] D. J. Daley and D. Vere-Jones. An introduction to the theory of point processes: volume II: general theory and structure. Springer Science & Business Media, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daley%2C%20D.J.%20Vere-Jones%2C%20D.%20An%20introduction%20to%20the%20theory%20of%20point%20processes%3A%20volume%20II%3A%20general%20theory%20and%20structure%202007"
        },
        {
            "id": "4",
            "entry": "[4] K. Doya. Reinforcement learning in continuous time and space. Neural computation, 12(1):219\u2013245, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doya%2C%20K.%20Reinforcement%20learning%20in%20continuous%20time%20and%20space%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doya%2C%20K.%20Reinforcement%20learning%20in%20continuous%20time%20and%20space%202000"
        },
        {
            "id": "5",
            "entry": "[5] N. Du, H. Dai, R. Trivedi, U. Upadhyay, M. Gomez-Rodriguez, and L. Song. Recurrent marked temporal point processes: Embedding event history to vector. In KDD, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20N.%20Dai%2C%20H.%20Trivedi%2C%20R.%20Upadhyay%2C%20U.%20Recurrent%20marked%20temporal%20point%20processes%3A%20Embedding%20event%20history%20to%20vector%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20N.%20Dai%2C%20H.%20Trivedi%2C%20R.%20Upadhyay%2C%20U.%20Recurrent%20marked%20temporal%20point%20processes%3A%20Embedding%20event%20history%20to%20vector%202016"
        },
        {
            "id": "6",
            "entry": "[6] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "7",
            "entry": "[7] H. Ebbinghaus. Memory: a contribution to experimental psychology. Teachers College, Columbia University, 1885.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ebbinghaus%2C%20H.%20Memory%3A%20a%20contribution%20to%20experimental%20psychology%201885"
        },
        {
            "id": "8",
            "entry": "[8] M. Farajtabar, J. Yang, X. Ye, H. Xu, R. Trivedi, E. Khalil, S. Li, L. Song, and H. Zha. Fake news mitigation via point process based intervention. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farajtabar%2C%20M.%20Yang%2C%20J.%20Ye%2C%20X.%20Xu%2C%20H.%20Fake%20news%20mitigation%20via%20point%20process%20based%20intervention%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farajtabar%2C%20M.%20Yang%2C%20J.%20Ye%2C%20X.%20Xu%2C%20H.%20Fake%20news%20mitigation%20via%20point%20process%20based%20intervention%202017"
        },
        {
            "id": "9",
            "entry": "[9] N. Fr\u00e9maux, H. Sprekeler, and W. Gerstner. Reinforcement learning using a continuous time actor-critic framework with spiking neurons. PLoS computational biology, 9(4):e1003024, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fr%C3%A9maux%2C%20N.%20Sprekeler%2C%20H.%20Gerstner%2C%20W.%20Reinforcement%20learning%20using%20a%20continuous%20time%20actor-critic%20framework%20with%20spiking%20neurons%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fr%C3%A9maux%2C%20N.%20Sprekeler%2C%20H.%20Gerstner%2C%20W.%20Reinforcement%20learning%20using%20a%20continuous%20time%20actor-critic%20framework%20with%20spiking%20neurons%202013"
        },
        {
            "id": "11",
            "entry": "[11] H. Jing and A. J. Smola. Neural survival recommender. In WSDM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H%20Jing%20and%20A%20J%20Smola%20Neural%20survival%20recommender%20In%20WSDM%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H%20Jing%20and%20A%20J%20Smola%20Neural%20survival%20recommender%20In%20WSDM%202017"
        },
        {
            "id": "12",
            "entry": "[12] M. R. Karimi, E. Tavakoli, M. Farajtabar, L. Song, and M. Gomez Rodriguez. Smart broadcasting: Do you want to be seen? In KDD, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20M.R.%20Tavakoli%2C%20E.%20Farajtabar%2C%20M.%20Song%2C%20L.%20Smart%20broadcasting%3A%20Do%20you%20want%20to%20be%20seen%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karimi%2C%20M.R.%20Tavakoli%2C%20E.%20Farajtabar%2C%20M.%20Song%2C%20L.%20Smart%20broadcasting%3A%20Do%20you%20want%20to%20be%20seen%3F%202016"
        },
        {
            "id": "13",
            "entry": "[13] J. Kim, B. Tabibian, A. Oh, B. Sch\u00f6lkopf, and M. Gomez-Rodriguez. Leveraging the crowd to detect and reduce the spread of fake news and misinformation. In WSDM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20J.%20Tabibian%2C%20B.%20Oh%2C%20A.%20Sch%C3%B6lkopf%2C%20B.%20Leveraging%20the%20crowd%20to%20detect%20and%20reduce%20the%20spread%20of%20fake%20news%20and%20misinformation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20J.%20Tabibian%2C%20B.%20Oh%2C%20A.%20Sch%C3%B6lkopf%2C%20B.%20Leveraging%20the%20crowd%20to%20detect%20and%20reduce%20the%20spread%20of%20fake%20news%20and%20misinformation%202018"
        },
        {
            "id": "14",
            "entry": "[14] S. Leitner. So lernt man lernen: Der weg zum erfolg. Herder, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leitner%2C%20S.%20So%20lernt%20man%20lernen%3A%20Der%20weg%20zum%20erfolg%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leitner%2C%20S.%20So%20lernt%20man%20lernen%3A%20Der%20weg%20zum%20erfolg%201972"
        },
        {
            "id": "15",
            "entry": "[15] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "16",
            "entry": "[16] R. V. Lindsey, J. D. Shroyer, H. Pashler, and M. C. Mozer. Improving students\u2019 long-term knowledge retention through personalized review. Psychological science, 25(3):639\u2013647, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lindsey%2C%20R.V.%20Shroyer%2C%20J.D.%20Pashler%2C%20H.%20Mozer%2C%20M.C.%20Improving%20students%E2%80%99%20long-term%20knowledge%20retention%20through%20personalized%20review%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lindsey%2C%20R.V.%20Shroyer%2C%20J.D.%20Pashler%2C%20H.%20Mozer%2C%20M.C.%20Improving%20students%E2%80%99%20long-term%20knowledge%20retention%20through%20personalized%20review%202014"
        },
        {
            "id": "17",
            "entry": "[17] H. Mei and J. M. Eisner. The neural hawkes process: A neurally self-modulating multivariate point process. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20H.%20Eisner%2C%20J.M.%20The%20neural%20hawkes%20process%3A%20A%20neurally%20self-modulating%20multivariate%20point%20process%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20H.%20Eisner%2C%20J.M.%20The%20neural%20hawkes%20process%3A%20A%20neurally%20self-modulating%20multivariate%20point%20process%202017"
        },
        {
            "id": "18",
            "entry": "[18] E. Mettler, C. M. Massey, and P. J. Kellman. A comparison of adaptive and fixed schedules of practice. Journal of Experimental Psychology: General, 145(7):897, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mettler%2C%20E.%20Massey%2C%20C.M.%20Kellman%2C%20P.J.%20A%20comparison%20of%20adaptive%20and%20fixed%20schedules%20of%20practice%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mettler%2C%20E.%20Massey%2C%20C.M.%20Kellman%2C%20P.J.%20A%20comparison%20of%20adaptive%20and%20fixed%20schedules%20of%20practice%202016"
        },
        {
            "id": "19",
            "entry": "[19] C. Metzler-Baddeley and R. J. Baddeley. Does adaptive training work? Applied Cognitive Psychology, 23(2):254\u2013266, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzler-Baddeley%2C%20C.%20Baddeley%2C%20R.J.%20Does%20adaptive%20training%20work%3F%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzler-Baddeley%2C%20C.%20Baddeley%2C%20R.J.%20Does%20adaptive%20training%20work%3F%202009"
        },
        {
            "id": "20",
            "entry": "[20] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "21",
            "entry": "[21] H. Pashler, N. Cepeda, R. V. Lindsey, E. Vul, and M. C. Mozer. Predicting the optimal spacing of study: A multiscale context model of memory. In NIPS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pashler%2C%20H.%20Cepeda%2C%20N.%20Lindsey%2C%20R.V.%20Vul%2C%20E.%20Predicting%20the%20optimal%20spacing%20of%20study%3A%20A%20multiscale%20context%20model%20of%20memory%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pashler%2C%20H.%20Cepeda%2C%20N.%20Lindsey%2C%20R.V.%20Vul%2C%20E.%20Predicting%20the%20optimal%20spacing%20of%20study%3A%20A%20multiscale%20context%20model%20of%20memory%202009"
        },
        {
            "id": "22",
            "entry": "[22] S. Reddy, I. Labutov, S. Banerjee, and T. Joachims. Unbounded human learning: Optimal scheduling for spaced repetition. In KDD, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddy%2C%20S.%20Labutov%2C%20I.%20Banerjee%2C%20S.%20Joachims%2C%20T.%20Unbounded%20human%20learning%3A%20Optimal%20scheduling%20for%20spaced%20repetition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddy%2C%20S.%20Labutov%2C%20I.%20Banerjee%2C%20S.%20Joachims%2C%20T.%20Unbounded%20human%20learning%3A%20Optimal%20scheduling%20for%20spaced%20repetition%202016"
        },
        {
            "id": "23",
            "entry": "[23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        },
        {
            "id": "24",
            "entry": "[24] B. Settles and B. Meeder. A trainable spaced repetition model for language learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1848\u20131858, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Settles%2C%20B.%20Meeder%2C%20B.%20A%20trainable%20spaced%20repetition%20model%20for%20language%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Settles%2C%20B.%20Meeder%2C%20B.%20A%20trainable%20spaced%20repetition%20model%20for%20language%20learning%202016"
        },
        {
            "id": "25",
            "entry": "[25] N. Spasojevic, Z. Li, A. Rao, and P. Bhattacharyya. When-to-post on social networks. In KDD, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spasojevic%2C%20N.%20Li%2C%20Z.%20Rao%2C%20A.%20Bhattacharyya%2C%20P.%20When-to-post%20on%20social%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spasojevic%2C%20N.%20Li%2C%20Z.%20Rao%2C%20A.%20Bhattacharyya%2C%20P.%20When-to-post%20on%20social%20networks%202015"
        },
        {
            "id": "26",
            "entry": "[26] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "27",
            "entry": "[27] B. Tabibian, U. Upadhyay, A. De, A. Zarezade, B. Schoelkopf, and M. Gomez-Rodriguez. Optimizing human learning. arXiv preprint arXiv:1712.01856, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01856"
        },
        {
            "id": "28",
            "entry": "[28] E. Vasilaki, N. Fr\u00e9maux, R. Urbanczik, W. Senn, and W. Gerstner. Spike-based reinforcement learning in continuous state and action space: when policy gradient methods fail. PLoS computational biology, 5(12):e1000586, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vasilaki%2C%20E.%20Fr%C3%A9maux%2C%20N.%20Urbanczik%2C%20R.%20Senn%2C%20W.%20Spike-based%20reinforcement%20learning%20in%20continuous%20state%20and%20action%20space%3A%20when%20policy%20gradient%20methods%20fail%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vasilaki%2C%20E.%20Fr%C3%A9maux%2C%20N.%20Urbanczik%2C%20R.%20Senn%2C%20W.%20Spike-based%20reinforcement%20learning%20in%20continuous%20state%20and%20action%20space%3A%20when%20policy%20gradient%20methods%20fail%202009"
        },
        {
            "id": "29",
            "entry": "[29] Y. Wang, E. Theodorou, A. Verma, and L. Song. A stochastic differential equation framework for guiding online user activities in closed loop. In AISTATS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Theodorou%2C%20E.%20Verma%2C%20A.%20Song%2C%20L.%20A%20stochastic%20differential%20equation%20framework%20for%20guiding%20online%20user%20activities%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Theodorou%2C%20E.%20Verma%2C%20A.%20Song%2C%20L.%20A%20stochastic%20differential%20equation%20framework%20for%20guiding%20online%20user%20activities%202018"
        },
        {
            "id": "30",
            "entry": "[30] Y. Wang, G. Williams, E. Theodorou, and L. Song. Variational policy for guiding point processes. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Williams%2C%20G.%20Theodorou%2C%20E.%20Song%2C%20L.%20Variational%20policy%20for%20guiding%20point%20processes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Williams%2C%20G.%20Theodorou%2C%20E.%20Song%2C%20L.%20Variational%20policy%20for%20guiding%20point%20processes%202017"
        },
        {
            "id": "31",
            "entry": "[31] D. Wierstra, A. Foerster, J. Peters, and J. Schmidhuber. Solving deep memory POMDPs with recurrent policy gradients. In ICANN, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wierstra%2C%20D.%20Foerster%2C%20A.%20Peters%2C%20J.%20Schmidhuber%2C%20J.%20Solving%20deep%20memory%20POMDPs%20with%20recurrent%20policy%20gradients%202007"
        },
        {
            "id": "32",
            "entry": "[32] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "33",
            "entry": "[33] A. Zarezade, A. De, U. Upadhyay, H. Rabiee, and M. Gomez-Rodriguez. Steering social activity: A stochastic optimal control point of view. JMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zarezade%2C%20A.%20De%2C%20A.%20Upadhyay%2C%20U.%20Rabiee%2C%20H.%20Steering%20social%20activity%3A%20A%20stochastic%20optimal%20control%20point%20of%20view%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zarezade%2C%20A.%20De%2C%20A.%20Upadhyay%2C%20U.%20Rabiee%2C%20H.%20Steering%20social%20activity%3A%20A%20stochastic%20optimal%20control%20point%20of%20view%202018"
        },
        {
            "id": "34",
            "entry": "[34] A. Zarezade, U. Upadhyay, H. Rabiee, and M. Gomez-Rodriguez. Redqueen: An online algorithm for smart broadcasting in social networks. In WSDM, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zarezade%2C%20A.%20Upadhyay%2C%20U.%20Rabiee%2C%20H.%20Gomez-Rodriguez%2C%20M.%20Redqueen%3A%20An%20online%20algorithm%20for%20smart%20broadcasting%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zarezade%2C%20A.%20Upadhyay%2C%20U.%20Rabiee%2C%20H.%20Gomez-Rodriguez%2C%20M.%20Redqueen%3A%20An%20online%20algorithm%20for%20smart%20broadcasting%202017"
        }
    ]
}
