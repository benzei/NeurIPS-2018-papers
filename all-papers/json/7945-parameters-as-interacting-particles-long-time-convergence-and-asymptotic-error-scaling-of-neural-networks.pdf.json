{
    "filename": "7945-parameters-as-interacting-particles-long-time-convergence-and-asymptotic-error-scaling-of-neural-networks.pdf",
    "metadata": {
        "title": "Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks",
        "author": "Grant Rotskoff, Eric Vanden-Eijnden",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7945-parameters-as-interacting-particles-long-time-convergence-and-asymptotic-error-scaling-of-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The performance of neural networks on high-dimensional data distributions suggests that it may be possible to parameterize a representation of a given highdimensional function with controllably small errors, potentially outperforming standard interpolation methods. We demonstrate, both theoretically and numerically, that this is indeed the case. We map the parameters of a neural network to a system of particles relaxing with an interaction potential determined by the loss function. We show that in the limit that the number of parameters n is large, the landscape of the mean-squared error becomes convex and the representation error in the function scales as O(n\u22121). In this limit, we prove a dynamical variant of the universal approximation theorem showing that the optimal representation can be attained by stochastic gradient descent, the algorithm ubiquitously used for parameter optimization in machine learning. In the asymptotic regime, we study the fluctuations around the optimal representation and show that they arise at a scale O(n\u22121). These fluctuations in the landscape identify the natural scale for the noise in stochastic gradient descent. Our results apply to both single and multi-layer neural networks, as well as standard kernel methods like radial basis functions."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "central limit theorem",
            "url": "https://en.wikipedia.org/wiki/central_limit_theorem"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "law of large numbers",
            "url": "https://en.wikipedia.org/wiki/law_of_large_numbers"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "partial differential equation",
            "url": "https://en.wikipedia.org/wiki/partial_differential_equation"
        }
    ],
    "highlights": [
        "The methods and models of machine learning are rapidly becoming de facto tools for the analysis and interpretation of large data sets",
        "Parameter optimization in machine learning typically relies on the stochastic gradient descent algorithm (SGD), which makes an empirical estimate of the gradient of the objective function over a small number of sample points [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "These statements reveal the scaling in the representation error and the analysis has synergies which are useful in deriving law of large numbers and central limit theorem for stochastic gradient descent",
        "We have introduced a perspective based on particle distribution functions that enables asymptotic analysis of the optimization dynamics of neural networks",
        "We have focused on the limit where the number of parameters n \u2192 \u221e, in which the objective function becomes convex and a stochastic partial differential equation describes the time evolution of the parameters",
        "Our results emphasize that the optimal parameters in this limit are accessible via stochastic gradient descent (Proposition 4.1) and that fluctuations around the optimum can be controlled by modulating the batch size (Proposition 4.2)"
    ],
    "key_statements": [
        "The methods and models of machine learning are rapidly becoming de facto tools for the analysis and interpretation of large data sets",
        "Parameter optimization in machine learning typically relies on the stochastic gradient descent algorithm (SGD), which makes an empirical estimate of the gradient of the objective function over a small number of sample points [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "Parameters as particles\u2014In order to study the properties of stochastic gradient descent for neural network optimization, we recast the standard training procedure in terms of a system of interacting particles [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "Convergence and asymptotic dynamics of stochastic gradient descent\u2014We demonstrate that the optimization problem becomes convex in the limit n \u2192 \u221e and we show that both gradient descent and stochastic gradient descent algorithm convergence to the global minimum [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "Writing the loss function in this language gives us a perspective that can be exploited to derive the scaling of the error in arbitrary neural networks trained with stochastic gradient descent.\n3 Gradient descent",
        "We first discuss the case of gradient descent for which we provide derivations of a law of large numbers (LLN) and central limit theorem (CLT) for the optimization dynamics",
        "These statements reveal the scaling in the representation error and the analysis has synergies which are useful in deriving law of large numbers and central limit theorem for stochastic gradient descent",
        "We have introduced a perspective based on particle distribution functions that enables asymptotic analysis of the optimization dynamics of neural networks",
        "We have focused on the limit where the number of parameters n \u2192 \u221e, in which the objective function becomes convex and a stochastic partial differential equation describes the time evolution of the parameters",
        "Our results emphasize that the optimal parameters in this limit are accessible via stochastic gradient descent (Proposition 4.1) and that fluctuations around the optimum can be controlled by modulating the batch size (Proposition 4.2)"
    ],
    "summary": [
        "The methods and models of machine learning are rapidly becoming de facto tools for the analysis and interpretation of large data sets.",
        "Determining the optimal set of parameters or \u201ctraining\u201d a given neural network remains one of the central challenges in applications due to the slow dynamics of training [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] and the complexity of the objective function [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "Parameter optimization in machine learning typically relies on the stochastic gradient descent algorithm (SGD), which makes an empirical estimate of the gradient of the objective function over a small number of sample points [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "Parameters as particles\u2014In order to study the properties of stochastic gradient descent for neural network optimization, we recast the standard training procedure in terms of a system of interacting particles [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "The partial differential equation for gradient descent represents the evolution of the parameters on the true loss landscape, i.e., the large data limit.",
        "Writing the loss function in this language gives us a perspective that can be exploited to derive the scaling of the error in arbitrary neural networks trained with stochastic gradient descent.",
        "We first discuss the case of gradient descent for which we provide derivations of a law of large numbers (LLN) and central limit theorem (CLT) for the optimization dynamics.",
        "These statements reveal the scaling in the representation error and the analysis has synergies which are useful in deriving LLN and CLT for stochastic gradient descent.",
        "The gradient descent dynamics is given by coupled ordinary differential equations for the weight and the parameters of the kernel,",
        "The LLN should be understood as a guarantee that gradient descent reaches the optimal representation for initial conditions sampled iid from a smooth distribution with full support on D \u00d7 R.",
        "The dynamics can be analyzed as a stochastic differential equation with a multiplicative noise term arising from the approximate evaluation of the gradient of the loss function.",
        "We have introduced a perspective based on particle distribution functions that enables asymptotic analysis of the optimization dynamics of neural networks.",
        "We have focused on the limit where the number of parameters n \u2192 \u221e, in which the objective function becomes convex and a stochastic partial differential equation describes the time evolution of the parameters.",
        "Our results emphasize that the optimal parameters in this limit are accessible via stochastic gradient descent (Proposition 4.1) and that fluctuations around the optimum can be controlled by modulating the batch size (Proposition 4.2).",
        "The dynamical evolution does not depend on n, suggesting that the rate of convergence should be asymptotically independent of the number of parameters"
    ],
    "headline": "That this is the case",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Elia Schneider, Luke Dai, Robert Q Topper, Christof Drechsel-Grau, and Mark E Tuckerman. Stochastic Neural Network Approach for Learning High-Dimensional Free Energy Surfaces. Physical Review Letters, 119(15):150601, October 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schneider%2C%20Elia%20Dai%2C%20Luke%20Topper%2C%20Robert%20Q.%20Drechsel-Grau%2C%20Christof%20Stochastic%20Neural%20Network%20Approach%20for%20Learning%20High-Dimensional%20Free%20Energy%20Surfaces%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schneider%2C%20Elia%20Dai%2C%20Luke%20Topper%2C%20Robert%20Q.%20Drechsel-Grau%2C%20Christof%20Stochastic%20Neural%20Network%20Approach%20for%20Learning%20High-Dimensional%20Free%20Energy%20Surfaces%202017-10"
        },
        {
            "id": "2",
            "entry": "[2] Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving for high dimensional committor functions using artificial neural networks. arXiv:1802.10275, February 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10275"
        },
        {
            "id": "3",
            "entry": "[3] Jens Berg and Kaj Nystr\u00f6m. A unified deep artificial neural network approach to partial differential equations in complex geometries. arXiv:1711.06464, November 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06464"
        },
        {
            "id": "4",
            "entry": "[4] J\u00f6rg Behler and Michele Parrinello. Generalized Neural-Network Representation of HighDimensional Potential-Energy Surfaces. Physical Review Letters, 98(14):583, April 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Behler%2C%20J%C3%B6rg%20Parrinello%2C%20Michele%20Generalized%20Neural-Network%20Representation%20of%20HighDimensional%20Potential-Energy%20Surfaces%202007-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Behler%2C%20J%C3%B6rg%20Parrinello%2C%20Michele%20Generalized%20Neural-Network%20Representation%20of%20HighDimensional%20Potential-Energy%20Surfaces%202007-04"
        },
        {
            "id": "5",
            "entry": "[5] L\u00e9on Bottou and Yann L. Cun. Large Scale Online Learning. In S. Thrun, L. K. Saul, and B. Sch\u00f6lkopf, editors, Advances in Neural Information Processing Systems 16, pages 217\u2013224. MIT Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Cun%2C%20Yann%20L.%20Large%20Scale%20Online%20Learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Cun%2C%20Yann%20L.%20Large%20Scale%20Online%20Learning%202004"
        },
        {
            "id": "6",
            "entry": "[6] Levent Sagun, V Ugur Guney, G\u00e9rard Ben Arous, and Yann LeCun. Explorations on high dimensional landscapes. arXiv:1412.6615, December 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6615"
        },
        {
            "id": "7",
            "entry": "[7] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The Loss Surfaces of Multilayer Networks. arXiv:1412.0233, November 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.0233"
        },
        {
            "id": "8",
            "entry": "[8] C Daniel Freeman and Joan Bruna. Topology and Geometry of Half-Rectified Network Optimization. arXiv:1611.01540, November 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01540"
        },
        {
            "id": "9",
            "entry": "[9] Luca Venturi, Afonso S Bandeira, and Joan Bruna. Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys. arXiv:1802.06384, February 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06384"
        },
        {
            "id": "10",
            "entry": "[10] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv:1605.08361, May 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08361"
        },
        {
            "id": "11",
            "entry": "[11] K Fukumizu and S Amari. Local minima and plateaus in hierarchical structures of multilayer perceptrons. Neural Networks, 13(3):317\u2013327, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fukumizu%2C%20K.%20Amari%2C%20S.%20Local%20minima%20and%20plateaus%20in%20hierarchical%20structures%20of%20multilayer%20perceptrons%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fukumizu%2C%20K.%20Amari%2C%20S.%20Local%20minima%20and%20plateaus%20in%20hierarchical%20structures%20of%20multilayer%20perceptrons%202000"
        },
        {
            "id": "12",
            "entry": "[12] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine Learning. arXiv:1606.04838, June 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04838"
        },
        {
            "id": "13",
            "entry": "[13] G Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303\u2013314, December 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989-12"
        },
        {
            "id": "14",
            "entry": "[14] A R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930\u2013945, May 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20A.R.%20Universal%20approximation%20bounds%20for%20superpositions%20of%20a%20sigmoidal%20function%201993-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20A.R.%20Universal%20approximation%20bounds%20for%20superpositions%20of%20a%20sigmoidal%20function%201993-05"
        },
        {
            "id": "15",
            "entry": "[15] Francis Bach. Breaking the Curse of Dimensionality with Convex Neural Networks. Journal of Machine Learning Research, 18(19):1\u201353, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Breaking%20the%20Curse%20of%20Dimensionality%20with%20Convex%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Breaking%20the%20Curse%20of%20Dimensionality%20with%20Convex%20Neural%20Networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] J Park and I W Sandberg. Universal Approximation Using Radial-Basis-Function Networks. Neural Computation, 3(2):246\u2013257, June 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Park%2C%20J.%20Sandberg%2C%20I.W.%20Universal%20Approximation%20Using%20Radial-Basis-Function%20Networks%201991-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Park%2C%20J.%20Sandberg%2C%20I.W.%20Universal%20Approximation%20Using%20Radial-Basis-Function%20Networks%201991-06"
        },
        {
            "id": "17",
            "entry": "[17] Sylvia Serfaty. Systems of Points with Coulomb Interactions. arXiv:1712.04095, December 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04095"
        },
        {
            "id": "18",
            "entry": "[18] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex stochastic gradient descent. arXiv:1705.07562, May 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07562"
        },
        {
            "id": "19",
            "entry": "[19] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradient algorithms. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2101\u20132110, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Qianxiao%20Tai%2C%20Cheng%20E%2C%20Weinan%20Stochastic%20modified%20equations%20and%20adaptive%20stochastic%20gradient%20algorithms%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Qianxiao%20Tai%2C%20Cheng%20E%2C%20Weinan%20Stochastic%20modified%20equations%20and%20adaptive%20stochastic%20gradient%20algorithms%202017-08"
        },
        {
            "id": "20",
            "entry": "[20] David S Dean. Langevin equation for the density of a system of interacting Langevin processes. Journal of Physics A: Mathematical and Theoretical, 29(24):L613\u2013L617, January 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20David%20S.%20Langevin%20equation%20for%20the%20density%20of%20a%20system%20of%20interacting%20Langevin%20processes%201999-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20David%20S.%20Langevin%20equation%20for%20the%20density%20of%20a%20system%20of%20interacting%20Langevin%20processes%201999-01"
        },
        {
            "id": "21",
            "entry": "[21] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013 E7671, August 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018-08"
        },
        {
            "id": "22",
            "entry": "[22] L\u00e9na\u00efc Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Overparameterized Models using Optimal Transport. arXiv:1805.09545, May 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09545"
        },
        {
            "id": "23",
            "entry": "[23] Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Neural Networks. arXiv:1805.01053, May 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01053"
        },
        {
            "id": "24",
            "entry": "[24] Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. In Y. Weiss, B. Sch\u00f6lkopf, and J. C. Platt, editors, Advances in Neural Information Processing Systems 18, pages 123\u2013130. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Roux%2C%20Nicolas%20L.%20Vincent%2C%20Pascal%20Delalleau%2C%20Olivier%20Convex%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Roux%2C%20Nicolas%20L.%20Vincent%2C%20Pascal%20Delalleau%2C%20Olivier%20Convex%20neural%20networks%202006"
        },
        {
            "id": "25",
            "entry": "[25] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. arXiv:1609.04836, September 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "26",
            "entry": "[26] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. arXiv:1705.08741, May 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08741"
        },
        {
            "id": "27",
            "entry": "[27] Antonio Auffinger and G\u00e9rard Ben Arous. Complexity of random smooth functions on the high-dimensional sphere. The Annals of Probability, 41(6):4214\u20134247, November 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auffinger%2C%20Antonio%20Arous%2C%20G%C3%A9rard%20Ben%20Complexity%20of%20random%20smooth%20functions%20on%20the%20high-dimensional%20sphere%202013-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auffinger%2C%20Antonio%20Arous%2C%20G%C3%A9rard%20Ben%20Complexity%20of%20random%20smooth%20functions%20on%20the%20high-dimensional%20sphere%202013-11"
        },
        {
            "id": "28",
            "entry": "[28] Antonio Auffinger, G\u00e9rard Ben Arous, and Jir\u00ed Cern\u00fd. Random Matrices and Complexity of Spin Glasses. Communications on Pure and Applied Mathematics, 66(2):165\u2013201, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antonio%20Auffinger%2C%20G%C3%A9rard%20Ben%20Arous%20Cern%C3%BD%2C%20Jir%C3%AD%20Random%20Matrices%20and%20Complexity%20of%20Spin%20Glasses%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antonio%20Auffinger%2C%20G%C3%A9rard%20Ben%20Arous%20Cern%C3%BD%2C%20Jir%C3%AD%20Random%20Matrices%20and%20Complexity%20of%20Spin%20Glasses%202012"
        }
    ]
}
