{
    "filename": "8107-improving-neural-program-synthesis-with-inferred-execution-traces.pdf",
    "metadata": {
        "title": "Improving Neural Program Synthesis with Inferred Execution Traces",
        "author": "Richard Shin, Illia Polosukhin, Dawn Song",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8107-improving-neural-program-synthesis-with-inferred-execution-traces.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, compared to other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces. While execution traces can provide highly detailed guidance for a program synthesis method, they are more difficult to obtain than more basic forms of specification such as input/output pairs. Therefore, we use the insight that we can split the process into two parts: infer traces from input/output examples, then infer programs from traces. Our application of this idea leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3% from the 77.12% of prior work."
    },
    "keywords": [
        {
            "term": "TRACE",
            "url": "https://en.wikipedia.org/wiki/TRACE"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "programming language",
            "url": "https://en.wikipedia.org/wiki/programming_language"
        },
        {
            "term": "input output",
            "url": "https://en.wikipedia.org/wiki/input_output"
        },
        {
            "term": "CODE",
            "url": "https://en.wikipedia.org/wiki/CODE"
        },
        {
            "term": "program synthesis",
            "url": "https://en.wikipedia.org/wiki/program_synthesis"
        }
    ],
    "highlights": [
        "Prog p := def main():s Stmt s := while(b) : s | repeat(r) : s | s1; s2<br/><br/>| a | if(b) : s | if(b) : s1else : s2 Cond b := markersPresent() | leftIsClear()<br/><br/>| rightIsClear() | frontIsClear() | not(b) Action a := move() | turnLeft() | turnRight()<br/><br/>| pickMarker() | putMarker() Cste r := 0 | 1 | \u00b7 \u00b7 \u00b7 | 19<br/><br/>Problem domain",
        "Our work is based on <a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\">Bunel et al [2018</a>], which applied a neural encoder-decoder approach to Karel program synthesis, similar to the work of Devlin et al [2017b] and <a class=\"ref-link\" id=\"cParisotto_et+al_2017_a\" href=\"#rParisotto_et+al_2017_a\">Parisotto et al [2017</a>] which was for a string-editing domain",
        "For the first part of our approach (I/O \u2192 TRACE), in Table 3 we show results of predicting the 5 execution traces from the 5 input/output examples used to specify a Karel program synthesis task",
        "We consider confirmed our hypothesis that it is beneficial to use traces for explicitly training a model that learns the semantics of interpreter, separately from the task of synthesizing the code for the correct program",
        "When we evaluated TRACE \u2192 CODE on gold traces at validation time, it outperformed the model trained on inferred traces",
        "Since I/O \u2192 TRACE independently predicts traces for each I/O pair, we hypothesize that they lack consistency with each other compared to the gold traces"
    ],
    "key_statements": [
        "Prog p := def main():s Stmt s := while(b) : s | repeat(r) : s | s1; s2<br/><br/>| a | if(b) : s | if(b) : s1else : s2 Cond b := markersPresent() | leftIsClear()<br/><br/>| rightIsClear() | frontIsClear() | not(b) Action a := move() | turnLeft() | turnRight()<br/><br/>| pickMarker() | putMarker() Cste r := 0 | 1 | \u00b7 \u00b7 \u00b7 | 19<br/><br/>Problem domain",
        "Our work is based on <a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\">Bunel et al [2018</a>], which applied a neural encoder-decoder approach to Karel program synthesis, similar to the work of Devlin et al [2017b] and <a class=\"ref-link\" id=\"cParisotto_et+al_2017_a\" href=\"#rParisotto_et+al_2017_a\">Parisotto et al [2017</a>] which was for a string-editing domain",
        "For the first part of our approach (I/O \u2192 TRACE), in Table 3 we show results of predicting the 5 execution traces from the 5 input/output examples used to specify a Karel program synthesis task",
        "We consider confirmed our hypothesis that it is beneficial to use traces for explicitly training a model that learns the semantics of interpreter, separately from the task of synthesizing the code for the correct program",
        "When we evaluated TRACE \u2192 CODE on gold traces at validation time, it outperformed the model trained on inferred traces",
        "Since I/O \u2192 TRACE independently predicts traces for each I/O pair, we hypothesize that they lack consistency with each other compared to the gold traces"
    ],
    "summary": [
        "Prog p := def main():s Stmt s := while(b) : s | repeat(r) : s | s1; s2<br/><br/>| a | if(b) : s | if(b) : s1else : s2 Cond b := markersPresent() | leftIsClear()<br/><br/>| rightIsClear() | frontIsClear() | not(b) Action a := move() | turnLeft() | turnRight()<br/><br/>| pickMarker() | putMarker() Cste r := 0 | 1 | \u00b7 \u00b7 \u00b7 | 19<br/><br/>Problem domain.",
        "A different set of approaches, neural program synthesis, instead learn to generate explicit discrete programs in a domain-specific language from a specification that consists of as few as 5 input/output example pairs.",
        "Several recent papers have proposed neural network-based approaches to program synthesis from input/output examples [<a class=\"ref-link\" id=\"cParisotto_et+al_2017_a\" href=\"#rParisotto_et+al_2017_a\">Parisotto et al, 2017</a>, Devlin et al, 2017b, <a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\">Bunel et al, 2018</a>].",
        "These methods use an end-to-end encoder-decoder approach, where a neural network learns to generate a program from an encoding of a program specification (a set of input/output examples) from a large synthetic training dataset.",
        "We use the insight that if encoder-decoder neural networks can synthesize programs from input/output examples, they should be able to infer execution traces.",
        "<a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\"><a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\">Bunel et al [2018</a></a>] used both supervised learning with a randomly-generated synthetic dataset to train their model, as well as a reinforcement learning-based approach to further improve the model\u2019s program synthesis accuracy.",
        "To create a model which uses both a set of input/output examples and execution traces for generating the desired program, we started with the architecture from <a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\"><a class=\"ref-link\" id=\"cBunel_et+al_2018_a\" href=\"#rBunel_et+al_2018_a\">Bunel et al [2018</a></a>] and extend it to take the execution trace as an input.",
        "Given an execution trace and an initial state state1,n := In for the nth input/output example, we can use the Karel interpreter to replay the actions to obtain state2,n, \u00b7 \u00b7 \u00b7 , stateT +1,n.",
        "We will not have access to the gold trace when we wish to use this model for program synthesis from input/output examples.",
        "For the first part of our approach (I/O \u2192 TRACE), in Table 3 we show results of predicting the 5 execution traces from the 5 input/output examples used to specify a Karel program synthesis task.",
        "We consider a result to be correct if all 5 predicted traces transform the corresponding input states to the output states when executed in the Karel interpreter.",
        "As discussed in Section 4.2, there exists many possible execution traces which transform a given input state to the output state; the exact match accuracy is much lower than the correctness metric.",
        "We consider confirmed our hypothesis that it is beneficial to use traces for explicitly training a model that learns the semantics of interpreter, separately from the task of synthesizing the code for the correct program."
    ],
    "headline": "As our method is largely orthogonal to prior techniques like reinforcement learning, our research suggests useful future directions for further improving the accuracy of neural program synthesis. 2 Related work",
    "reference_links": [
        {
            "id": "Balog_et+al_2016_a",
            "entry": "Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. CoRR, abs/1611.01989, 2016. URL http://arxiv.org/abs/1611.01989.",
            "url": "http://arxiv.org/abs/1611.01989",
            "arxiv_url": "https://arxiv.org/pdf/1611.01989"
        },
        {
            "id": "Bhupatiraju_et+al_2017_a",
            "entry": "Surya Bhupatiraju, Rishabh Singh, Abdel rahman Mohamed, and Pushmeet Kohli. Deep api programmer: Learning to program with apis. CoRR, abs/1704.04327, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04327"
        },
        {
            "id": "Bunel_et+al_2018_a",
            "entry": "Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=H1Xw62kRZ.",
            "url": "https://openreview.net/forum?id=H1Xw62kRZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bunel%2C%20Rudy%20Hausknecht%2C%20Matthew%20Devlin%2C%20Jacob%20Singh%2C%20Rishabh%20Leveraging%20grammar%20and%20reinforcement%20learning%20for%20neural%20program%20synthesis%202018"
        },
        {
            "id": "Cai_et+al_2017_a",
            "entry": "Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. In International Conference on Learning Representations, 2017. URL http://arxiv.org/abs/1511.06279.",
            "url": "http://arxiv.org/abs/1511.06279",
            "arxiv_url": "https://arxiv.org/pdf/1511.06279"
        },
        {
            "id": "Devlin_et+al_2017_a",
            "entry": "Jacob Devlin, Rudy R Bunel, Rishabh Singh, Matthew Hausknecht, and Pushmeet Kohli. Neural program meta-induction. In Advances in Neural Information Processing Systems, pages 2077\u20132085, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%20Devlin%20Rudy%20R%20Bunel%20Rishabh%20Singh%20Matthew%20Hausknecht%20and%20Pushmeet%20Kohli%20Neural%20program%20metainduction%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2020772085%202017a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%20Devlin%20Rudy%20R%20Bunel%20Rishabh%20Singh%20Matthew%20Hausknecht%20and%20Pushmeet%20Kohli%20Neural%20program%20metainduction%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2020772085%202017a"
        },
        {
            "id": "Devlin_et+al_0000_a",
            "entry": "Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy i/o. In International Conference on Machine Learning, pages 990\u2013998, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20Robustfill%3A%20Neural%20program%20learning%20under%20noisy%20i/o",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20Robustfill%3A%20Neural%20program%20learning%20under%20noisy%20i/o"
        },
        {
            "id": "Ellis_et+al_2017_a",
            "entry": "Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Joshua B. Tenenbaum. Learning to infer graphics programs from hand-drawn images. CoRR, abs/1707.09627, 2017. URL http://arxiv.org/abs/1707.09627.",
            "url": "http://arxiv.org/abs/1707.09627",
            "arxiv_url": "https://arxiv.org/pdf/1707.09627"
        },
        {
            "id": "Ganin_et+al_2018_a",
            "entry": "Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, and Oriol Vinyals. Synthesizing programs for images using reinforced adversarial learning. CoRR, abs/1804.01118, 2018. URL http://arxiv.org/abs/1804.01118.",
            "url": "http://arxiv.org/abs/1804.01118",
            "arxiv_url": "https://arxiv.org/pdf/1804.01118"
        },
        {
            "id": "Gaunt_et+al_2016_a",
            "entry": "Alexander L Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction. arXiv preprint arXiv:1608.04428, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04428"
        },
        {
            "id": "Graves_et+al_0000_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014a.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Graves_et+al_2014_a",
            "entry": "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014b. URL http://arxiv.org/abs/1410.5401.",
            "url": "http://arxiv.org/abs/1410.5401",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "Graves_et+al_2016_a",
            "entry": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio G\u00f3mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): 471\u2013476, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016"
        },
        {
            "id": "Grefenstette_et+al_2015_a",
            "entry": "Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems, pages 1828\u20131836, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Suleyman%2C%20Mustafa%20Blunsom%2C%20Phil%20Learning%20to%20transduce%20with%20unbounded%20memory%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grefenstette%2C%20Edward%20Hermann%2C%20Karl%20Moritz%20Suleyman%2C%20Mustafa%20Blunsom%2C%20Phil%20Learning%20to%20transduce%20with%20unbounded%20memory%202015"
        },
        {
            "id": "Gregor_et+al_2015_a",
            "entry": "Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623, 2015. URL http://arxiv.org/abs/1502.04623.",
            "url": "http://arxiv.org/abs/1502.04623",
            "arxiv_url": "https://arxiv.org/pdf/1502.04623"
        },
        {
            "id": "Gulwani_2011_a",
            "entry": "Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. In Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL \u201911, pages 317\u2013330, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-04900. doi: 10.1145/1926385.1926423. URL http://doi.acm.org/10.1145/1926385.1926423.",
            "crossref": "https://dx.doi.org/10.1145/1926385.1926423",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1926385.1926423"
        },
        {
            "id": "Joulin_2015_a",
            "entry": "Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joulin%2C%20Armand%20Mikolov%2C%20Tomas%20Inferring%20algorithmic%20patterns%20with%20stack-augmented%20recurrent%20nets%202015"
        },
        {
            "id": "Kaiser_2015_a",
            "entry": "\u0141ukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.08228"
        },
        {
            "id": "Kaiser_2016_a",
            "entry": "Lukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In International Conference on Learning Representations, 2016. URL http://arxiv.org/abs/1511.08228.",
            "url": "http://arxiv.org/abs/1511.08228",
            "arxiv_url": "https://arxiv.org/pdf/1511.08228"
        },
        {
            "id": "Kurach_et+al_2016_a",
            "entry": "Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random access machines. ICLR, 2016. URL http://arxiv.org/abs/1511.06392.",
            "url": "http://arxiv.org/abs/1511.06392",
            "arxiv_url": "https://arxiv.org/pdf/1511.06392"
        },
        {
            "id": "Manna_1975_a",
            "entry": "Zohar Manna and Richard Waldinger. Knowledge and reasoning in program synthesis. In Proceedings of the 4th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI\u201975, pages 288\u2013295, San Francisco, CA, USA, 1975. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/citation.cfm?id=1624626.1624670.",
            "url": "http://dl.acm.org/citation.cfm?id=1624626.1624670",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manna%2C%20Zohar%20Waldinger%2C%20Richard%20Knowledge%20and%20reasoning%20in%20program%20synthesis%201975"
        },
        {
            "id": "Parisotto_et+al_2017_a",
            "entry": "Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20Emilio%20Mohamed%2C%20Abdel-rahman%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20Emilio%20Mohamed%2C%20Abdel-rahman%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202017"
        },
        {
            "id": "Pattis_1981_a",
            "entry": "Richard E Pattis. Karel the robot: a gentle introduction to the art of programming. John Wiley & Sons, Inc., 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pattis%2C%20Richard%20E.%20Karel%20the%20robot%3A%20a%20gentle%20introduction%20to%20the%20art%20of%20programming%201981"
        },
        {
            "id": "Reed_2015_a",
            "entry": "Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06279"
        },
        {
            "id": "Reed_2016_a",
            "entry": "Scott Reed and Nando de Freitas. Neural programmer-interpreters. In International Conference on Learning Representations, 2016. URL http://arxiv.org/abs/1511.06279.",
            "url": "http://arxiv.org/abs/1511.06279",
            "arxiv_url": "https://arxiv.org/pdf/1511.06279"
        },
        {
            "id": "Waldinger_0000_a",
            "entry": "Richard J. Waldinger and Richard C. T. Lee. Prow: A step toward automatic program writing. In Proceedings of the 1st International Joint Conference on Artificial Intelligence, IJCAI\u201969, pages 241\u2013252, San Francisco, CA, USA, 1969. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/citation.cfm?id=1624562.1624586.",
            "url": "http://dl.acm.org/citation.cfm?id=1624562.1624586",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Waldinger%2C%20Richard%20J.%20Lee%2C%20Richard%20C.T.%20Prow%3A%20A%20step%20toward%20automatic%20program%20writing"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Ke Wang, Zhendong Su, and Rishabh Singh. Dynamic neural program embeddings for program repair. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BJuWrGW0Z.",
            "url": "https://openreview.net/forum?id=BJuWrGW0Z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ke%20Su%2C%20Zhendong%20Singh%2C%20Rishabh%20Dynamic%20neural%20program%20embeddings%20for%20program%20repair%202018"
        }
    ]
}
