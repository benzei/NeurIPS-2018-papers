{
    "filename": "7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks.pdf",
    "metadata": {
        "title": "Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks",
        "author": "Hang Gao, Zheng Shou, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation on novel classes can be inductively biased, we explicitly preserve covariance information as the \u201cvariability\u201d of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "base class",
            "url": "https://en.wikipedia.org/wiki/base_class"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        }
    ],
    "highlights": [
        "The hallmark of learning new concepts from very few examples characterizes human intelligence",
        "This usually leads to catastrophic forgetting [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], i.e., fine-tuning makes the model over-fitting on novel classes, and agnostic to the majority of base classes [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], deteriorating overall performance",
        "After constraining our translation process to selected class pairs, we develop a baseline based on Conditional Generative Adversarial Networks (c-Generative Adversarial Networks) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We report a variant called cDeLi-Generative Adversarial Networks which uses the same objective as cCyc-Generative Adversarial Networks, but sample the noise vector z from a mixture of C different Gaussian distributions, Z =d 1 C",
        "We have presented a novel approach to low-shot learning that augments data for novel classes by training a cyclic Generative Adversarial Networks model, while shaping intra-class variability through similar base classes",
        "Our proposed model significantly outperforms the state of the art on the challenging ImageNet benchmark in various settings"
    ],
    "key_statements": [
        "The hallmark of learning new concepts from very few examples characterizes human intelligence",
        "A straightforward idea to learn new concepts is to fine-tune a model pre-trained on base categories, using limited data from another set of novel categories",
        "This usually leads to catastrophic forgetting [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], i.e., fine-tuning makes the model over-fitting on novel classes, and agnostic to the majority of base classes [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], deteriorating overall performance",
        "We propose Covariance-Preserving Adversarial Augmentation Networks (CP-AAN), a",
        "We thoroughly investigate conditional Generative Adversarial Networks variants inspired by previous works [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] to enable low-shot generation",
        "After constraining our translation process to selected class pairs, we develop a baseline based on Conditional Generative Adversarial Networks (c-Generative Adversarial Networks) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We extend this method for our conditional scenario and derive cCyc-Generative Adversarial Networks",
        "We report a variant called cDeLi-Generative Adversarial Networks which uses the same objective as cCyc-Generative Adversarial Networks, but sample the noise vector z from a mixture of C different Gaussian distributions, Z =d 1 C",
        "While cycle-consistency alone can transfer certain degrees of intra-class variance from base classes, we find it rather weak and unreliable since there are still infinite candidate distributions that cannot be discriminated based on limited observations (See Figure 4)",
        "We provide two extra baselines to exclude the bias induced by different embedding methods: P-Feature Hallucination builds on Feature Hallucinating by substituting their non-episodic representation with learned prototypical features",
        "We provide four models constructed with different Generative Adversarial Networks choices as justified in Section 3",
        "We believe such performance gain can be attributed to our advanced generation methods since at low shots, Feature Imagination applies discrete transformations that its generator has previously learned while we can sample through a smooth distribution combining all related base classes\u2019 covariance information",
        "We carefully examine our design choices for the final version of our Covariance-Preserving Adversarial Augmentation Networks",
        "We have presented a novel approach to low-shot learning that augments data for novel classes by training a cyclic Generative Adversarial Networks model, while shaping intra-class variability through similar base classes",
        "We introduced and compared several Generative Adversarial Networks variants in a logical process and demonstrated the increasing performance of each model variant",
        "Our proposed model significantly outperforms the state of the art on the challenging ImageNet benchmark in various settings",
        "Quantitative and qualitative evaluations show the effectiveness of our method in generating realistic and diverse data for low-shot learning, given very few examples"
    ],
    "summary": [
        "The hallmark of learning new concepts from very few examples characterizes human intelligence.",
        "Base set new class of Generative Adversarial Networks (GAN) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] for feature augmentation.",
        "Our model aims to learn the latent distribution of each novel class given its base counterparts.",
        "Diverse feature augmentation is feasible through conditional translation given a pair of related novel and base classes.",
        "A final classifier is trained using both original examples of the base classes and all examples of the novel classes.",
        "Their performances are degraded by naive generation methods without modeling the latent distribution of each novel class.",
        "We address two challenges that impede good translation under imbalanced scenarios: (1) through which base-novel class pairs we can translate; and more fundamentally, (2) through what objectives for GAN training we can estimate latent distribution of novel classes with limited observations.",
        "As illustrated in Figure 3e, preserving covariance information from relevant base classes to a novel class can improve low-shot generation quality.",
        "Evaluation We repeat sampling novel examples five times for held-out novel sets and report results of mean top-5 accuracy in both conventional low-shot learning (LSL, to test on novel classes only) and its generalized setting (GLSL, to test on all categories including base classes).",
        "Our best method consistently achieves significant improvement over previous augmentation-based approaches for different values of K under both LSL and GLSL settings, achieving almost 2% performance gain compared to baselines.",
        "We believe such performance gain can be attributed to our advanced generation methods since at low shots, FI applies discrete transformations that its generator has previously learned while we can sample through a smooth distribution combining all related base classes\u2019 covariance information.",
        "Note that in the LSL setting, all generative methods assume we still have access to original base examples when learning final classifiers while non-generative baselines usually don\u2019t have this constraint.",
        "T-SNE might visually drag similar high dimensional points towards one mode, our model shows more diverse generation that is better aligned with the latent distribution, improving overall recognition performance by spreading seed examples in meaningful directions.",
        "We have presented a novel approach to low-shot learning that augments data for novel classes by training a cyclic GAN model, while shaping intra-class variability through similar base classes.",
        "Quantitative and qualitative evaluations show the effectiveness of our method in generating realistic and diverse data for low-shot learning, given very few examples."
    ],
    "headline": "We propose Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6211"
        },
        {
            "id": "2",
            "entry": "[2] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. Incremental learning of object detectors without catastrophic forgetting. arXiv preprint arXiv:1708.06977, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.06977"
        },
        {
            "id": "4",
            "entry": "[4] Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. arXiv preprint arXiv:1712.00981, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00981"
        },
        {
            "id": "5",
            "entry": "[5] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "6",
            "entry": "[6] Relja Arandjelovicand Andrew Zisserman. Three things everyone should know to improve object retrieval. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2911\u20132918. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zisserman%2C%20Relja%20Arandjelovicand%20Andrew%20Three%20things%20everyone%20should%20know%20to%20improve%20object%20retrieval%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zisserman%2C%20Relja%20Arandjelovicand%20Andrew%20Three%20things%20everyone%20should%20know%20to%20improve%20object%20retrieval%202012"
        },
        {
            "id": "7",
            "entry": "[7] Ying-Cong Chen, Xiatian Zhu, Wei-Shi Zheng, and Jian-Huang Lai. Person re-identification by camera correlation aware feature augmentation. IEEE transactions on pattern analysis and machine intelligence, 40(2):392\u2013408, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Ying-Cong%20Zhu%2C%20Xiatian%20Zheng%2C%20Wei-Shi%20Lai%2C%20Jian-Huang%20Person%20re-identification%20by%20camera%20correlation%20aware%20feature%20augmentation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Ying-Cong%20Zhu%2C%20Xiatian%20Zheng%2C%20Wei-Shi%20Lai%2C%20Jian-Huang%20Person%20re-identification%20by%20camera%20correlation%20aware%20feature%20augmentation%202018"
        },
        {
            "id": "8",
            "entry": "[8] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pages 3630\u20133638, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "9",
            "entry": "[9] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pages 4080\u20134090, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Prototypical%20networks%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Prototypical%20networks%20for%20few-shot%20learning%202017"
        },
        {
            "id": "10",
            "entry": "[10] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. arXiv preprint arXiv:1711.06025, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06025"
        },
        {
            "id": "11",
            "entry": "[11] Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating features. arXiv preprint arXiv:1606.02819, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.02819"
        },
        {
            "id": "12",
            "entry": "[12] Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary data. arXiv preprint arXiv:1801.05401, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05401"
        },
        {
            "id": "13",
            "entry": "[13] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-toimage translation networks. arXiv preprint arXiv:1804.04732, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.04732"
        },
        {
            "id": "14",
            "entry": "[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "15",
            "entry": "[15] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "16",
            "entry": "[16] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        },
        {
            "id": "17",
            "entry": "[17] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, pages 465\u2013476, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Zhang%2C%20Richard%20Pathak%2C%20Deepak%20Darrell%2C%20Trevor%20Alexei%20A%20Efros%2C%20Oliver%20Wang%2C%20and%20Eli%20Shechtman.%20Toward%20multimodal%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Zhang%2C%20Richard%20Pathak%2C%20Deepak%20Darrell%2C%20Trevor%20Alexei%20A%20Efros%2C%20Oliver%20Wang%2C%20and%20Eli%20Shechtman.%20Toward%20multimodal%20image-to-image%20translation%202017"
        },
        {
            "id": "18",
            "entry": "[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "19",
            "entry": "[19] David Lei, Michael A Hitt, and Richard Bettis. Dynamic core competences through metalearning and strategic context. Journal of management, 22(4):549\u2013569, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20David%20Hitt%2C%20Michael%20A.%20Bettis%2C%20Richard%20Dynamic%20core%20competences%20through%20metalearning%20and%20strategic%20context%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20David%20Hitt%2C%20Michael%20A.%20Bettis%2C%20Richard%20Dynamic%20core%20competences%20through%20metalearning%20and%20strategic%20context%201996"
        },
        {
            "id": "20",
            "entry": "[20] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09585"
        },
        {
            "id": "21",
            "entry": "[21] James Tobin. Estimation of relationships for limited dependent variables. Econometrica: journal of the Econometric Society, pages 24\u201336, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tobin%2C%20James%20Estimation%20of%20relationships%20for%20limited%20dependent%20variables%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tobin%2C%20James%20Estimation%20of%20relationships%20for%20limited%20dependent%20variables%201958"
        },
        {
            "id": "22",
            "entry": "[22] Emmanuel Candes, Terence Tao, et al. The dantzig selector: Statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20Tao%2C%20Terence%20The%20dantzig%20selector%3A%20Statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20Tao%2C%20Terence%20The%20dantzig%20selector%3A%20Statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007"
        },
        {
            "id": "23",
            "entry": "[23] Ruslan Salakhutdinov, Joshua Tenenbaum, and Antonio Torralba. One-shot learning with a hierarchical nonparametric bayesian model. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pages 195\u2013206, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20Torralba%2C%20Antonio%20One-shot%20learning%20with%20a%20hierarchical%20nonparametric%20bayesian%20model%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20Torralba%2C%20Antonio%20One-shot%20learning%20with%20a%20hierarchical%20nonparametric%20bayesian%20model%202012"
        },
        {
            "id": "24",
            "entry": "[24] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra. One-shot generalization in deep generative models. arXiv preprint arXiv:1603.05106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.05106"
        },
        {
            "id": "25",
            "entry": "[25] Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and V Babu Radhakrishnan. Deligan: Generative adversarial networks for diverse and limited data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gurumurthy%2C%20Swaminathan%20Sarvadevabhatla%2C%20Ravi%20Kiran%20Radhakrishnan%2C%20V.Babu%20Deligan%3A%20Generative%20adversarial%20networks%20for%20diverse%20and%20limited%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gurumurthy%2C%20Swaminathan%20Sarvadevabhatla%2C%20Ravi%20Kiran%20Radhakrishnan%2C%20V.Babu%20Deligan%3A%20Generative%20adversarial%20networks%20for%20diverse%20and%20limited%20data%202017"
        },
        {
            "id": "26",
            "entry": "[26] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching gan. arXiv preprint arXiv:1702.08398, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08398"
        },
        {
            "id": "27",
            "entry": "[27] Youssef Mroueh and Tom Sercu. Fisher gan. In Advances in Neural Information Processing Systems, pages 2510\u20132520, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mroueh%2C%20Youssef%20Sercu%2C%20Tom%20Fisher%20gan%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mroueh%2C%20Youssef%20Sercu%2C%20Tom%20Fisher%20gan%202017"
        },
        {
            "id": "28",
            "entry": "[28] Jacob Goldberger, Geoffrey E Hinton, Sam T Roweis, and Ruslan R Salakhutdinov. Neighbourhood components analysis. In Advances in neural information processing systems, pages 513\u2013520, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberger%2C%20Jacob%20Hinton%2C%20Geoffrey%20E.%20Roweis%2C%20Sam%20T.%20Salakhutdinov%2C%20Ruslan%20R.%20Neighbourhood%20components%20analysis%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldberger%2C%20Jacob%20Hinton%2C%20Geoffrey%20E.%20Roweis%2C%20Sam%20T.%20Salakhutdinov%2C%20Ruslan%20R.%20Neighbourhood%20components%20analysis%202005"
        },
        {
            "id": "29",
            "entry": "[29] Peiliang Xu. Truncated svd methods for discrete linear ill-posed problems. Geophysical Journal International, 135(2):505\u2013514, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Peiliang%20Truncated%20svd%20methods%20for%20discrete%20linear%20ill-posed%20problems%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Peiliang%20Truncated%20svd%20methods%20for%20discrete%20linear%20ill-posed%20problems%201998"
        },
        {
            "id": "30",
            "entry": "[30] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.06065"
        },
        {
            "id": "31",
            "entry": "[31] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop, volume 2, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015"
        },
        {
            "id": "32",
            "entry": "[32] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "33",
            "entry": "[33] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4367\u20134375, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidaris%2C%20Spyros%20Komodakis%2C%20Nikos%20Dynamic%20few-shot%20visual%20learning%20without%20forgetting%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidaris%2C%20Spyros%20Komodakis%2C%20Nikos%20Dynamic%20few-shot%20visual%20learning%20without%20forgetting%202018"
        },
        {
            "id": "34",
            "entry": "[34] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "35",
            "entry": "[35] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "36",
            "entry": "[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "37",
            "entry": "[37] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        }
    ]
}
