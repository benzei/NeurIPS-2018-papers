{
    "filename": "8063-data-dependent-pac-bayes-priors-via-differential-privacy.pdf",
    "metadata": {
        "title": "Data-dependent PAC-Bayes priors via differential privacy",
        "author": "Gintare Karolina Dziugaite, Daniel M. Roy",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8063-data-dependent-pac-bayes-priors-via-differential-privacy.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999) can incorporate knowledge about the learning algorithm and (data) distribution through the use of distribution-dependent priors, yielding tighter generalization bounds on data-dependent posteriors. Using this flexibility, however, is difficult, especially when the data distribution is presumed to be unknown. We show how an \u03b5-differentially private data-dependent prior yields a valid PAC-Bayes bound, and then show how non-private mechanisms for choosing priors can also yield generalization bounds. As an application of this result, we show that a Gaussian prior mean chosen via stochastic gradient Langevin dynamics (SGLD; <a class=\"ref-link\" id=\"cWelling_2011_a\" href=\"#rWelling_2011_a\">Welling and Teh, 2011</a>) leads to a valid PAC-Bayes bound due to control of the 2-Wasserstein distance to the \u03b5-differentially private stationary distribution. We study our datadependent bounds empirically, and show that they can be nonvacuous even when other distribution-dependent bounds are vacuous."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "stochastic gradient Langevin dynamics",
            "url": "https://en.wikipedia.org/wiki/Stochastic_Gradient_Langevin_Dynamics"
        },
        {
            "term": "differential privacy",
            "url": "https://en.wikipedia.org/wiki/differential_privacy"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "algorithmic learning theory",
            "url": "https://en.wikipedia.org/wiki/algorithmic_learning_theory"
        }
    ],
    "highlights": [
        "There has been a resurgence of interest in Probably Approximately Correct-Bayes bounds, especially towards explaining generalization in large-scale neural networks trained by stochastic gradient descent (<a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\"><a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite and Roy, 2017</a></a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\">Neyshabur et al, 2017b</a></a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017a</a></a>)",
        "There has been a resurgence of interest in Probably Approximately Correct-Bayes bounds, especially towards explaining generalization in large-scale neural networks trained by stochastic gradient descent (<a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite and Roy, 2017</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\">Neyshabur et al, 2017b</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017a</a>)",
        "We show that an \u03b5-differentially private prior mean yields a valid, though necessarily looser, PACBayes generalization bound. (See Theorem 4.2.) The result is a straightforward application of results connecting privacy and adaptive data analysis (<a class=\"ref-link\" id=\"cDwork_et+al_2015_b\" href=\"#rDwork_et+al_2015_b\">Dwork et al, 2015b</a>; <a class=\"ref-link\" id=\"cDwork_et+al_2015_a\" href=\"#rDwork_et+al_2015_a\">Dwork et al, 2015a</a>)",
        "We only evaluate the differentially private Probably Approximately Correct-Bayes bounds on the small neural network and SYNTH dataset, The parameter settings for SYNTH experiments are: T 1 = 100, T 2 = 1000, \u03b3 = 2; for MNIST: T 1 = 500, T 2 = 1000, \u03b3 = 5",
        "We never observe a violation of the Probably Approximately Correct-Bayes bounds for Gibbs distributions",
        "This suggests that our assumption that stochastic gradient Langevin dynamics has nearly converged is accurate enough or the bounds are sufficiently loose that any effect from nonconvergence was masked"
    ],
    "key_statements": [
        "There has been a resurgence of interest in Probably Approximately Correct-Bayes bounds, especially towards explaining generalization in large-scale neural networks trained by stochastic gradient descent (<a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\"><a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite and Roy, 2017</a></a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\">Neyshabur et al, 2017b</a></a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017a</a></a>)",
        "There has been a resurgence of interest in Probably Approximately Correct-Bayes bounds, especially towards explaining generalization in large-scale neural networks trained by stochastic gradient descent (<a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite and Roy, 2017</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\">Neyshabur et al, 2017b</a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017a</a>)",
        "We show that an \u03b5-differentially private prior mean yields a valid, though necessarily looser, PACBayes generalization bound. (See Theorem 4.2.) The result is a straightforward application of results connecting privacy and adaptive data analysis (<a class=\"ref-link\" id=\"cDwork_et+al_2015_b\" href=\"#rDwork_et+al_2015_b\">Dwork et al, 2015b</a>; <a class=\"ref-link\" id=\"cDwork_et+al_2015_a\" href=\"#rDwork_et+al_2015_a\">Dwork et al, 2015a</a>)",
        "Theorem 3.3 allows us to formalize this intuition: we can obtain new Probably Approximately Correct-Bayes bounds that use data-dependent priors, provided they are \u03b5-differentially private: We provide an example using the bound of Maurer (Theorem 4.1)",
        "We are justified in taking this step due to our main result (Corollary 5.7), which shows that a slight weakening of the Probably Approximately Correct-Bayes generalization bound is valid provided that stochastic gradient Langevin dynamics converges weakly to its stationary distribution",
        "We evaluate our private Probably Approximately Correct-Bayes bound (Theorem 4.2) for the randomized classifier Q\u03c42 and the private data-dependent prior Pw0 , where the privacy parameter depends on \u03c41",
        "We only evaluate the differentially private Probably Approximately Correct-Bayes bounds on the small neural network and SYNTH dataset, The parameter settings for SYNTH experiments are: T 1 = 100, T 2 = 1000, \u03b3 = 2; for MNIST: T 1 = 500, T 2 = 1000, \u03b3 = 5",
        "We never observe a violation of the Probably Approximately Correct-Bayes bounds for Gibbs distributions",
        "This suggests that our assumption that stochastic gradient Langevin dynamics has nearly converged is accurate enough or the bounds are sufficiently loose that any effect from nonconvergence was masked",
        "We evaluate our private Probably Approximately Correct-Bayes bound on MNIST dataset only for a combination of \u03c41 = 103 and \u03c42 \u2208 [3 \u2217 103, 3 \u2217 104, 105, 3 \u2217 105]"
    ],
    "summary": [
        "There has been a resurgence of interest in PAC-Bayes bounds, especially towards explaining generalization in large-scale neural networks trained by stochastic gradient descent (<a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\"><a class=\"ref-link\" id=\"cDziugaite_2017_a\" href=\"#rDziugaite_2017_a\">Dziugaite and Roy, 2017</a></a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_b\" href=\"#rNeyshabur_et+al_2017_b\">Neyshabur et al, 2017b</a></a>; <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017a</a></a>).",
        "PAC-Bayes bounds control the generalization error of Gibbs classifiers in terms of the Kullback\u2013Leibler (KL) divergence to a fixed probability measure on the space of classifiers.",
        "PAC-Bayes bounds depend on a tradeoff between the empirical risk of the sample of S",
        "Our focus is on generalization bounds that use all the data, and in this setting, Lever, Laviolette, and Shawe-Taylor (2013) prove a remarkable result for Gibbs posteriors.",
        "As an application of this result, SGLD can be used to optimize a data-dependent prior and still yield a valid PAC-Bayes bound.",
        "The prior P that appears in the PAC-Bayes generalization bound must be chosen independently of the data S \u223c Dm, but can depend on the data distribution D itself.",
        "Theorem 3.3 allows us to formalize this intuition: we can obtain new PAC-Bayes bounds that use data-dependent priors, provided they are \u03b5-differentially private: We provide an example using the bound of Maurer (Theorem 4.1).",
        "We formalize this intuition here by deriving bounds on KL(Q||P(S)) in terms of a non-private data-dependent prior PS.",
        "We are justified in taking this step due to our main result (Corollary 5.7), which shows that a slight weakening of the PAC-Bayes generalization bound is valid provided that SGLD converges weakly to its stationary distribution.",
        "The k-step transitions of SGLD converge weakly towards a Gibbs distribution with a uniform base measure, producing a random vector w0 \u2208 Rp. The private data-dependent prior",
        "We evaluate our private PAC-Bayes bound (Theorem 4.2) for the randomized classifier Q\u03c42 and the private data-dependent prior Pw0 , where the privacy parameter depends on \u03c41.",
        "We only evaluate the differentially private PAC-Bayes bounds on the small neural network and SYNTH dataset, The parameter settings for SYNTH experiments are: T 1 = 100, T 2 = 1000, \u03b3 = 2; for MNIST: T 1 = 500, T 2 = 1000, \u03b3 = 5.",
        "This is due to the fact that it retains the KL term, which is sensitive to the data distribution via Q, and it can be much lower than the upper bound on the KL in Lever, Laviolette, and Shawe-Taylor (2013) for datasets with small true Bayes risk.",
        "We do not plot DP PAC-Bayes bound for MNIST experiments due to the computational challenges approximating the KL term for a high-dimensional parameter space, as discussed in Appendix E."
    ],
    "headline": "We show how an \u03b5-differentially private data-dependent prior yields a valid Probably Approximately Correct-Bayes bound, and show how non-private mechanisms for choosing priors can yield generalization bounds",
    "reference_links": [
        {
            "id": "Alquier_2018_a",
            "entry": "P. Alquier and B. Guedj (May 2018). \u201cSimpler PAC-Bayesian Bounds for Hostile Data\u201d. Mach. Learn. 107.5, pp. 887\u2013902. DOI: 10.1007/s10994-017-5690-0.",
            "crossref": "https://dx.doi.org/10.1007/s10994-017-5690-0",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10994-017-5690-0"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "P. L. Bartlett, D. J. Foster, and M. J. Telgarsky (2017). \u201cSpectrally-normalized margin bounds for neural networks\u201d. Advances in Neural Information Processing Systems (NIPS), pp. 6241\u20136250.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20J%2C%20M.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20J%2C%20M.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Bassily_et+al_2014_a",
            "entry": "R. Bassily, A. Smith, and A. Thakurta (2014). Differentially private empirical risk minimization: Efficient algorithms and tight error bounds. arXiv: 1405.7085v2 [cs.LG].",
            "arxiv_url": "https://arxiv.org/pdf/1405.7085v2"
        },
        {
            "id": "Bassily_et+al_2016_a",
            "entry": "R. Bassily, K. Nissim, A. Smith, T. Steinke, U. Stemmer, and J. Ullman (2016). \u201cAlgorithmic stability for adaptive data analysis\u201d. Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing. ACM, pp. 1046\u20131059.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bassily%2C%20R.%20Nissim%2C%20K.%20Smith%2C%20A.%20Steinke%2C%20T.%20Algorithmic%20stability%20for%20adaptive%20data%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bassily%2C%20R.%20Nissim%2C%20K.%20Smith%2C%20A.%20Steinke%2C%20T.%20Algorithmic%20stability%20for%20adaptive%20data%20analysis%202016"
        },
        {
            "id": "B_et+al_2016_a",
            "entry": "L. B\u00e9gin, P. Germain, F. Laviolette, and J.-F. Roy (2016). \u201cPAC-Bayesian bounds based on the R\u00e9nyi divergence\u201d. Proc. Artificial Intelligence and Statistics (AISTATS), pp. 435\u2013444.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%A9gin%2C%20L.%20Germain%2C%20P.%20Laviolette%2C%20F.%20J.-F.%20PAC-Bayesian%20bounds%20based%20on%20the%20R%C3%A9nyi%20divergence%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=B%C3%A9gin%2C%20L.%20Germain%2C%20P.%20Laviolette%2C%20F.%20J.-F.%20PAC-Bayesian%20bounds%20based%20on%20the%20R%C3%A9nyi%20divergence%202016"
        },
        {
            "id": "Bousquet_2002_a",
            "entry": "O. Bousquet and A. Elisseeff (2002). \u201cStability and generalization\u201d. Journal of Machine Learning Research 2.Mar, pp. 499\u2013526.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20O.%20A.%20Stability%20and%20generalization%202002-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20O.%20A.%20Stability%20and%20generalization%202002-03"
        },
        {
            "id": "O_2007_a",
            "entry": "O. Catoni (2007). PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning. Lecture Notes-Monograph Series. Institute of Mathematical Statistics. DOI: 10.1214/ 074921707000000391. arXiv: 0712.0248 [stat.ML].",
            "crossref": "https://dx.doi.org/10.1214/074921707000000391.arXiv",
            "arxiv_url": "https://arxiv.org/pdf/0712.0248"
        },
        {
            "id": "Dimitrakakis_et+al_2014_a",
            "entry": "C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. I. Rubinstein (2014). \u201cRobust and private Bayesian inference\u201d. International Conference on Algorithmic Learning Theory. Springer, pp. 291\u2013305.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dimitrakakis%2C%20C.%20Nelson%2C%20B.%20Mitrokotsa%2C%20A.%20I%2C%20B.%20%E2%80%9CRobust%20and%20private%20Bayesian%20inference%E2%80%9D.%20International%20Conference%20on%20Algorithmic%20Learning%20Theory%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dimitrakakis%2C%20C.%20Nelson%2C%20B.%20Mitrokotsa%2C%20A.%20I%2C%20B.%20%E2%80%9CRobust%20and%20private%20Bayesian%20inference%E2%80%9D.%20International%20Conference%20on%20Algorithmic%20Learning%20Theory%202014"
        },
        {
            "id": "Dwork_2006_a",
            "entry": "C. Dwork (2006). \u201cDifferential Privacy\u201d. Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10\u201314, 2006, Proceedings, Part II. Ed. by M. Bugliesi, B. Preneel, V. Sassone, and I. Wegener. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 1\u201312. DOI: 10.1007/11787006_1.",
            "crossref": "https://dx.doi.org/10.1007/11787006_1",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/11787006_1"
        },
        {
            "id": "\u2013_0000_a",
            "entry": "\u2013 (2008). \u201cDifferential privacy: A survey of results\u201d. International Conference on Theory and Applications of Models of Computation. Springer, pp. 1\u201319.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Differential%20privacy%3A%20A%20survey%20of%20results%E2%80%9D",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Differential%20privacy%3A%20A%20survey%20of%20results%E2%80%9D"
        },
        {
            "id": "Dwork_2014_a",
            "entry": "C. Dwork, A. Roth, et al. (2014). \u201cThe algorithmic foundations of differential privacy\u201d. Foundations and Trends in Theoretical Computer Science 9.3\u20134, pp. 211\u2013407.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20C.%20Roth%2C%20A.%20The%20algorithmic%20foundations%20of%20differential%20privacy%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20C.%20Roth%2C%20A.%20The%20algorithmic%20foundations%20of%20differential%20privacy%202014"
        },
        {
            "id": "Dwork_et+al_2015_a",
            "entry": "C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth (2015a). \u201cGeneralization in adaptive data analysis and holdout reuse\u201d. Advances in Neural Information Processing Systems, pp. 2350\u20132358.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20C.%20Feldman%2C%20V.%20Hardt%2C%20M.%20Pitassi%2C%20T.%20Generalization%20in%20adaptive%20data%20analysis%20and%20holdout%20reuse%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20C.%20Feldman%2C%20V.%20Hardt%2C%20M.%20Pitassi%2C%20T.%20Generalization%20in%20adaptive%20data%20analysis%20and%20holdout%20reuse%202015"
        },
        {
            "id": "Dwork_et+al_2015_b",
            "entry": "C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. L. Roth (2015b). \u201cPreserving statistical validity in adaptive data analysis\u201d. Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing. ACM, pp. 117\u2013126.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20C.%20Feldman%2C%20V.%20Hardt%2C%20M.%20Pitassi%2C%20T.%20Preserving%20statistical%20validity%20in%20adaptive%20data%20analysis%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20C.%20Feldman%2C%20V.%20Hardt%2C%20M.%20Pitassi%2C%20T.%20Preserving%20statistical%20validity%20in%20adaptive%20data%20analysis%202015"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "G. K. Dziugaite and D. M. Roy (2017). \u201cComputing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data\u201d. Proc. 33rd Conf. Uncertainty in Artificial Intelligence (UAI). arXiv: 1703.11008.",
            "arxiv_url": "https://arxiv.org/pdf/1703.11008"
        },
        {
            "id": "Germain_et+al_2016_a",
            "entry": "P. Germain, F. Bach, A. Lacoste, and S. Lacoste-Julien (2016). \u201cPAC-Bayesian Theory Meets Bayesian Inference\u201d. Advances in Neural Information Processing Systems, pp. 1884\u20131892.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20P.%20Bach%2C%20F.%20Lacoste%2C%20A.%20S.%20PAC-Bayesian%20Theory%20Meets%20Bayesian%20Inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20P.%20Bach%2C%20F.%20Lacoste%2C%20A.%20S.%20PAC-Bayesian%20Theory%20Meets%20Bayesian%20Inference%202016"
        },
        {
            "id": "Gruenwald_2016_a",
            "entry": "P. D. Gr\u00fcnwald and N. A. Mehta (2016). \u201cFast Rates for General Unbounded Loss Functions: from ERM to Generalized Bayes\u201d. arXiv: 1605.00252.",
            "arxiv_url": "https://arxiv.org/pdf/1605.00252"
        },
        {
            "id": "Gruenwald_2017_a",
            "entry": "P. D. Gr\u00fcnwald and N. A. Mehta (2017). A Tight Excess Risk Bound via a Unified PAC-BayesianRademacher-Shtarkov-MDL Complexity. arXiv: 1710.07732.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07732"
        },
        {
            "id": "Kifer_et+al_2012_a",
            "entry": "D. Kifer, A. Smith, and A. Thakurta (2012). \u201cPrivate convex empirical risk minimization and highdimensional regression\u201d. Journal of Machine Learning Research 1.41, pp. 1\u201340.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kifer%2C%20D.%20Smith%2C%20A.%20A.%20Private%20convex%20empirical%20risk%20minimization%20and%20highdimensional%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kifer%2C%20D.%20Smith%2C%20A.%20A.%20Private%20convex%20empirical%20risk%20minimization%20and%20highdimensional%20regression%202012"
        },
        {
            "id": "Langford_2002_a",
            "entry": "J. Langford (2002). \u201cQuantitatively tight sample complexity bounds\u201d. PhD thesis. Carnegie Mellon University.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20Quantitatively%20tight%20sample%20complexity%20bounds%202002"
        },
        {
            "id": "Langford_2001_a",
            "entry": "J. Langford and M. Seeger (2001). Bounds for Averaging Classifiers. Tech. rep. CMU-CS-01-102. Carnegie Mellon University.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20M.%20Bounds%20for%20Averaging%20Classifiers%202001"
        },
        {
            "id": "Lever_et+al_2013_a",
            "entry": "G. Lever, F. Laviolette, and J. Shawe-Taylor (2013). \u201cTighter PAC-Bayes bounds through distribution-dependent priors\u201d. Theoretical Computer Science 473, pp. 4\u201328. DOI: 10 . 1016 / j.tcs.2012.10.013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lever%2C%20G.%20Laviolette%2C%20F.%20J.%20Tighter%20PAC-Bayes%20bounds%20through%20distribution-dependent%20priors%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lever%2C%20G.%20Laviolette%2C%20F.%20J.%20Tighter%20PAC-Bayes%20bounds%20through%20distribution-dependent%20priors%202013"
        },
        {
            "id": "Maurer_2004_a",
            "entry": "A. Maurer (2004). A note on the PAC-Bayesian theorem. arXiv: cs/0411099 [cs.LG].",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20A%20note%20on%20the%20PAC-Bayesian%20theorem%202004"
        },
        {
            "id": "Mcallester_1999_a",
            "entry": "D. A. McAllester (1999). \u201cPAC-Bayesian Model Averaging\u201d. Proceedings of the Twelfth Annual Conference on Computational Learning Theory. COLT \u201999. Santa Cruz, California, USA: ACM, pp. 164\u2013170. DOI: 10.1145/307400.307435.",
            "crossref": "https://dx.doi.org/10.1145/307400.307435",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/307400.307435"
        },
        {
            "id": "Mcsherry_2007_a",
            "entry": "F. McSherry and K. Talwar (2007). \u201cMechanism Design via Differential Privacy\u201d. Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science. FOCS \u201907. Washington, DC, USA: IEEE Computer Society, pp. 94\u2013103. DOI: 10.1109/FOCS.2007.41.",
            "crossref": "https://dx.doi.org/10.1109/FOCS.2007.41",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/FOCS.2007.41"
        },
        {
            "id": "Minami_et+al_2016_a",
            "entry": "K. Minami, H. Arai, I. Sato, and H. Nakagawa (2016). \u201cDifferential Privacy without Sensitivity\u201d. Advances in Neural Information Processing Systems 29. Ed. by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, pp. 956\u2013964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minami%2C%20K.%20Arai%2C%20H.%20Sato%2C%20I.%20H.%20Differential%20Privacy%20without%20Sensitivity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minami%2C%20K.%20Arai%2C%20H.%20Sato%2C%20I.%20H.%20Differential%20Privacy%20without%20Sensitivity%202016"
        },
        {
            "id": "D_2013_a",
            "entry": "D. J. Mir (2013). \u201cDifferential privacy: an exploration of the privacy-utility landscape\u201d. PhD thesis. Rutgers University.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%2C%20D.%20Differential%20privacy%3A%20an%20exploration%20of%20the%20privacy-utility%20landscape%202013"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro (2017a). \u201cA PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks\u201d. Proc. Int. Conf. on Learning Representation (ICLR). arXiv: 1707.09564.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "Neyshabur_et+al_2017_b",
            "entry": "B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro (2017b). \u201cExploring generalization in deep learning\u201d. Advances in Neural Information Processing Systems (NIPS), pp. 5949\u20135958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20N.%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20N.%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "Oneto_et+al_2017_a",
            "entry": "L. Oneto, S. Ridella, and D. Anguita (2017). \u201cDifferential privacy and generalization: Sharper bounds with applications\u201d. Pattern Recognition Letters 89, pp. 31\u201338. DOI: 10.1016/j.patrec. 2017.02.006.",
            "crossref": "https://dx.doi.org/10.1016/j.patrec.2017.02.006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.patrec.2017.02.006"
        },
        {
            "id": "Parrado-Hern_et+al_2012_a",
            "entry": "E. Parrado-Hern\u00e1ndez, A. Ambroladze, J. Shawe-Taylor, and S. Sun (2012). \u201cPAC-Bayes bounds with data dependent priors\u201d. Journal of Machine Learning Research 13.Dec, pp. 3507\u20133531.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parrado-Hern%C3%A1ndez%2C%20E.%20Ambroladze%2C%20A.%20Shawe-Taylor%2C%20J.%20S.%20PAC-Bayes%20bounds%20with%20data%20dependent%20priors%202012-12-13",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parrado-Hern%C3%A1ndez%2C%20E.%20Ambroladze%2C%20A.%20Shawe-Taylor%2C%20J.%20S.%20PAC-Bayes%20bounds%20with%20data%20dependent%20priors%202012-12-13"
        },
        {
            "id": "Raginsky_et+al_2017_a",
            "entry": "M. Raginsky, A. Rakhlin, and M. Telgarsky (2017). \u201cNon-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis\u201d. Proc. Conf. on Learning Theory (COLT). arXiv: 1702.03849.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03849"
        },
        {
            "id": "Smith_2018_a",
            "entry": "S. L. Smith and Q. V. Le (2018). \u201cA Bayesian perspective on generalization and stochastic gradient descent\u201d. Proc. Int. Conf. on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20S.L.%20V%2C%20Q.%20A%20Bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20S.L.%20V%2C%20Q.%20A%20Bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "Teh_et+al_2016_a",
            "entry": "Y. W. Teh, A. H. Thiery, and S. J. Vollmer (2016). \u201cConsistency and fluctuations for stochastic gradient Langevin dynamics\u201d. Journal of Machine Learning Research 17, pp. 1\u201333.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Y.W.%20Thiery%2C%20A.H.%20J%2C%20S.%20Consistency%20and%20fluctuations%20for%20stochastic%20gradient%20Langevin%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Y.W.%20Thiery%2C%20A.H.%20J%2C%20S.%20Consistency%20and%20fluctuations%20for%20stochastic%20gradient%20Langevin%20dynamics%202016"
        },
        {
            "id": "Thiemann_et+al_2017_a",
            "entry": "N. Thiemann, C. Igel, O. Wintenberger, and Y. Seldin (2017). \u201cA Strongly Quasiconvex PACBayesian Bound\u201d. Int. Conf. on Algorithmic Learning Theory (ALT), pp. 466\u2013492. arXiv: 1608. 05610.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thiemann%2C%20N.%20Igel%2C%20C.%20Wintenberger%2C%20O.%20Y.%20%E2%80%9CA%20Strongly%20Quasiconvex%20PACBayesian%20Bound%E2%80%9D.%20Int.%20Conf.%20on%20Algorithmic%20Learning%20Theory%20%28ALT%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thiemann%2C%20N.%20Igel%2C%20C.%20Wintenberger%2C%20O.%20Y.%20%E2%80%9CA%20Strongly%20Quasiconvex%20PACBayesian%20Bound%E2%80%9D.%20Int.%20Conf.%20on%20Algorithmic%20Learning%20Theory%20%28ALT%29%202017"
        },
        {
            "id": "Wang_et+al_2015_a",
            "entry": "Y.-X. Wang, S. E. Fienberg, and A. J. Smola (2015). \u201cPrivacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo\u201d. Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37. ICML\u201915. Lille, France: JMLR.org, pp. 2493\u20132502.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.-X.%20Fienberg%2C%20S.E.%20Smola%2C%20A.J.%20Privacy%20for%20Free%3A%20Posterior%20Sampling%20and%20Stochastic%20Gradient%20Monte%20Carlo%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.-X.%20Fienberg%2C%20S.E.%20Smola%2C%20A.J.%20Privacy%20for%20Free%3A%20Posterior%20Sampling%20and%20Stochastic%20Gradient%20Monte%20Carlo%202015"
        },
        {
            "id": "Welling_2011_a",
            "entry": "M. Welling and Y. W. Teh (2011). \u201cBayesian learning via stochastic gradient Langevin dynamics\u201d. Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681\u2013688.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20learning%20via%20stochastic%20gradient%20Langevin%20dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20learning%20via%20stochastic%20gradient%20Langevin%20dynamics%202011"
        }
    ]
}
