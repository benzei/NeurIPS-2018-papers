{
    "filename": "7316-image-inpainting-via-generative-multi-column-convolutional-neural-networks.pdf",
    "metadata": {
        "title": "Image Inpainting via Generative Multi-column Convolutional Neural Networks",
        "author": "Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7316-image-inpainting-via-generative-multi-column-convolutional-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we propose a generative multi-column network for image inpainting. This network synthesizes different image components in a parallel manner within one stage. To better characterize global structures, we design a confidence-driven reconstruction loss while an implicit diversified MRF regularization is adopted to enhance local details. The multi-column network combined with the reconstruction and MRF loss propagates local and global information derived from context to the target inpainting regions. Extensive experiments on challenging street view, face, natural objects and scenes manifest that our method produces visual compelling results even without previously common post-processing."
    },
    "keywords": [
        {
            "term": "image denoising",
            "url": "https://en.wikipedia.org/wiki/image_denoising"
        },
        {
            "term": "structural similarity",
            "url": "https://en.wikipedia.org/wiki/structural_similarity"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "similarity measure",
            "url": "https://en.wikipedia.org/wiki/similarity_measure"
        },
        {
            "term": "peak signal-to-noise ratio",
            "url": "https://en.wikipedia.org/wiki/peak_signal-to-noise_ratio"
        }
    ],
    "highlights": [
        "Image inpainting aims to estimate suitable pixel information to fill holes in images",
        "In order to generate realistic structures and textures, researchers resort to auxiliary information, from either surrounding image areas or external data",
        "The generation task is not suitable to be evaluated by peak signal-to-noise ratio (PSNR) or structural similarity (SSIM), for completeness, we still give them on the testing or validation sets of four used datasets for reference",
        "With and without implicit diversified Markov random fields Regularization We train a complete Generative Multi-column Convolutional Neural Network on the Paris street view dataset with all losses and one model that does not involve implicit diversified Markov random fields",
        "We have primarily addressed the important problems of representing visual context and using it to generate and constrain unknown regions in inpainting",
        "We have proposed a generative multi-column neural network for this task and showed its ability to model different image components and extract multi-level features"
    ],
    "key_statements": [
        "Image inpainting aims to estimate suitable pixel information to fill holes in images",
        "In order to generate realistic structures and textures, researchers resort to auxiliary information, from either surrounding image areas or external data",
        "The generation task is not suitable to be evaluated by peak signal-to-noise ratio (PSNR) or structural similarity (SSIM), for completeness, we still give them on the testing or validation sets of four used datasets for reference",
        "Our Generative Multi-column Convolutional Neural Network structure with varied receptive fields in each branch predicts reasonable image structure and texture compared with single encoder-decoder and coarse-to-fine structure",
        "With and without implicit diversified Markov random fields Regularization We train a complete Generative Multi-column Convolutional Neural Network on the Paris street view dataset with all losses and one model that does not involve implicit diversified Markov random fields",
        "We have primarily addressed the important problems of representing visual context and using it to generate and constrain unknown regions in inpainting",
        "We have proposed a generative multi-column neural network for this task and showed its ability to model different image components and extract multi-level features"
    ],
    "summary": [
        "Image inpainting aims to estimate suitable pixel information to fill holes in images.",
        "The proposed method can produce high quality results considering boundary consistency, structure suitability and texture similarity, without any post-processing operations.",
        "Our proposed Generative Multi-column Convolutional Neural Network (GMCNN) shown in Figure 2 consists of three sub-networks: a generator to produce results, global&local discriminators for adversarial training, and a pretrained VGG network [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] to calculate ID-MRF loss.",
        "The generator network consists of n (n = 3) parallel encoder-decoder branches to extract different levels of features from input X with mask M, and a shared decoder module to transform deep features into natural image space Y .",
        "The encoder-decoder transforms the image into a common feature space with the same-size receptive field, ignoring the fact that inpainting involves different levels of representations.",
        "The proposed network is optimized to minimize the difference between generated content and corresponding nearest-neighbors from ground truth in the feature space.",
        "By minimizing the ID-MRF loss, not only local neural patches in YgL find corresponding candidates from YL, but the feature distributions come near, helping capture variation in complicated texture.",
        "Our ID-MRF regularization exploits both reference and contextual information inside and out of the filling regions, and causes high diversity in inpainting structure generation.",
        "As shown in Figures 8 and 10, compared with other methods, ours gives obvious visual improvement on plausible image structures and crisp textures.",
        "The generation task is not suitable to be evaluated by peak signal-to-noise ratio (PSNR) or structural similarity (SSIM), for completeness, we still give them on the testing or validation sets of four used datasets for reference.",
        "Our GMCNN structure with varied receptive fields in each branch predicts reasonable image structure and texture compared with single encoder-decoder and coarse-to-fine structure.",
        "With and without ID-MRF Regularization We train a complete GMCNN on the Paris street view dataset with all losses and one model that does not involve ID-MRF.",
        "We have proposed a generative multi-column neural network for this task and showed its ability to model different image components and extract multi-level features.",
        "Limitations Similar to other generative neural networks [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] for inpainting, our method still has difficulties dealing with large-scale datasets with thousands of diverse object and scene categories, such as ImageNet. When data falls into a few categories, our method works best, since the ambiguity removal in terms of structure and texture can be achieved in these cases.",
        "Our future work will be to explore other constraints with location and content"
    ],
    "headline": "We propose a generative multi-column network for image inpainting",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] F. Agostinelli, M. R. Anderson, and H. Lee. Adaptive multi-column deep neural networks with application to robust image denoising. In NIPS, pages 1493\u20131501, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agostinelli%2C%20F.%20Anderson%2C%20M.R.%20Lee%2C%20H.%20Adaptive%20multi-column%20deep%20neural%20networks%20with%20application%20to%20robust%20image%20denoising%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agostinelli%2C%20F.%20Anderson%2C%20M.R.%20Lee%2C%20H.%20Adaptive%20multi-column%20deep%20neural%20networks%20with%20application%20to%20robust%20image%20denoising%202013"
        },
        {
            "id": "2",
            "entry": "[2] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing. TOG, 28(3):24, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barnes%2C%20C.%20Shechtman%2C%20E.%20Finkelstein%2C%20A.%20Goldman%2C%20D.B.%20Patchmatch%3A%20A%20randomized%20correspondence%20algorithm%20for%20structural%20image%20editing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barnes%2C%20C.%20Shechtman%2C%20E.%20Finkelstein%2C%20A.%20Goldman%2C%20D.B.%20Patchmatch%3A%20A%20randomized%20correspondence%20algorithm%20for%20structural%20image%20editing%202009"
        },
        {
            "id": "3",
            "entry": "[3] D. Ciregan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. In CVPR, pages 3642\u20133649. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciregan%2C%20D.%20Meier%2C%20U.%20Schmidhuber%2C%20J.%20Multi-column%20deep%20neural%20networks%20for%20image%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciregan%2C%20D.%20Meier%2C%20U.%20Schmidhuber%2C%20J.%20Multi-column%20deep%20neural%20networks%20for%20image%20classification%202012"
        },
        {
            "id": "4",
            "entry": "[4] A. Criminisi, P. P\u00e9rez, and K. Toyama. Region filling and object removal by exemplar-based image inpainting. TIP, 13(9):1200\u20131212, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Criminisi%2C%20A.%20P%C3%A9rez%2C%20P.%20Toyama%2C%20K.%20Region%20filling%20and%20object%20removal%20by%20exemplar-based%20image%20inpainting%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Criminisi%2C%20A.%20P%C3%A9rez%2C%20P.%20Toyama%2C%20K.%20Region%20filling%20and%20object%20removal%20by%20exemplar-based%20image%20inpainting%202004"
        },
        {
            "id": "5",
            "entry": "[5] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In CVPR, pages 2414\u20132423. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "6",
            "entry": "[6] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. In NIPS, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "7",
            "entry": "[7] K. He and J. Sun. Statistics of patch offsets for image completion. In ECCV, pages 16\u201329.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Sun%2C%20J.%20Statistics%20of%20patch%20offsets%20for%20image%20completion",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Sun%2C%20J.%20Statistics%20of%20patch%20offsets%20for%20image%20completion"
        },
        {
            "id": "8",
            "entry": "[8] K. He and J. Sun. Image completion approaches using the statistics of similar patches. TPAMI, 36(12):2423\u20132435, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Sun%2C%20J.%20Image%20completion%20approaches%20using%20the%20statistics%20of%20similar%20patches%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Sun%2C%20J.%20Image%20completion%20approaches%20using%20the%20statistics%20of%20similar%20patches%202014"
        },
        {
            "id": "9",
            "entry": "[9] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Globally and locally consistent image completion. TOG, 36(4):107, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Globally%20and%20locally%20consistent%20image%20completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Globally%20and%20locally%20consistent%20image%20completion%202017"
        },
        {
            "id": "10",
            "entry": "[10] J. Jia and C.-K. Tang. Image repairing: Robust image synthesis by adaptive nd tensor voting. In CVPR, volume 1, pages I\u2013I. IEEE, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20J.%20Tang%2C%20C.-K.%20Image%20repairing%3A%20Robust%20image%20synthesis%20by%20adaptive%20nd%20tensor%20voting%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20J.%20Tang%2C%20C.-K.%20Image%20repairing%3A%20Robust%20image%20synthesis%20by%20adaptive%20nd%20tensor%20voting%202003"
        },
        {
            "id": "11",
            "entry": "[11] J. Jia and C.-K. Tang. Inference of segmented color and texture description by tensor voting. TPAMI, 26(6):771\u2013786, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20J.%20Tang%2C%20C.-K.%20Inference%20of%20segmented%20color%20and%20texture%20description%20by%20tensor%20voting%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20J.%20Tang%2C%20C.-K.%20Inference%20of%20segmented%20color%20and%20texture%20description%20by%20tensor%20voting%202004"
        },
        {
            "id": "12",
            "entry": "[12] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "13",
            "entry": "[13] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "14",
            "entry": "[14] C. Li and M. Wand. Combining markov random fields and convolutional neural networks for image synthesis. In CVPR, pages 2479\u20132486, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Wand%2C%20M.%20Combining%20markov%20random%20fields%20and%20convolutional%20neural%20networks%20for%20image%20synthesis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Wand%2C%20M.%20Combining%20markov%20random%20fields%20and%20convolutional%20neural%20networks%20for%20image%20synthesis%202016"
        },
        {
            "id": "15",
            "entry": "[15] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In ICCV, pages 3730\u20133738, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Z.%20Luo%2C%20P.%20Wang%2C%20X.%20Tang%2C%20X.%20Deep%20learning%20face%20attributes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Z.%20Luo%2C%20P.%20Wang%2C%20X.%20Tang%2C%20X.%20Deep%20learning%20face%20attributes%202015"
        },
        {
            "id": "16",
            "entry": "[16] R. Mechrez, I. Talmi, F. Shama, and L. Zelnik-Manor. Learning to maintain natural image statistics. arXiv preprint arXiv:1803.04626, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.04626"
        },
        {
            "id": "17",
            "entry": "[17] R. Mechrez, I. Talmi, and L. Zelnik-Manor. The contextual loss for image transformation with non-aligned data. arXiv preprint arXiv:1803.02077, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02077"
        },
        {
            "id": "18",
            "entry": "[18] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In CVPR, pages 2536\u20132544, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Krahenbuhl%2C%20P.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Krahenbuhl%2C%20P.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "19",
            "entry": "[19] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "20",
            "entry": "[20] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "21",
            "entry": "[21] J. Sun, L. Yuan, J. Jia, and H.-Y. Shum. Image completion with structure propagation. In TOG, volume 24, pages 861\u2013868. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20J.%20Yuan%2C%20L.%20Jia%2C%20J.%20Shum%2C%20H.-Y.%20Image%20completion%20with%20structure%20propagation%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20J.%20Yuan%2C%20L.%20Jia%2C%20J.%20Shum%2C%20H.-Y.%20Image%20completion%20with%20structure%20propagation%202005"
        },
        {
            "id": "22",
            "entry": "[22] I. Talmi, R. Mechrez, and L. Zelnik-Manor. Template matching with deformable diversity similarity. In CVPR, pages 175\u2013183, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talmi%2C%20I.%20Mechrez%2C%20R.%20Zelnik-Manor%2C%20L.%20Template%20matching%20with%20deformable%20diversity%20similarity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Talmi%2C%20I.%20Mechrez%2C%20R.%20Zelnik-Manor%2C%20L.%20Template%20matching%20with%20deformable%20diversity%20similarity%202017"
        },
        {
            "id": "23",
            "entry": "[23] Z. Yan, X. Li, M. Li, W. Zuo, and S. Shan. Shift-net: Image inpainting via deep feature rearrangement. arXiv preprint arXiv:1801.09392, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09392"
        },
        {
            "id": "24",
            "entry": "[24] C. Yang, X. Lu, Z. Lin, E. Shechtman, O. Wang, and H. Li. High-resolution image inpainting using multi-scale neural patch synthesis. In CVPR, volume 1, page 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20C.%20Lu%2C%20X.%20Lin%2C%20Z.%20Shechtman%2C%20E.%20High-resolution%20image%20inpainting%20using%20multi-scale%20neural%20patch%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20C.%20Lu%2C%20X.%20Lin%2C%20Z.%20Shechtman%2C%20E.%20High-resolution%20image%20inpainting%20using%20multi-scale%20neural%20patch%20synthesis%202017"
        },
        {
            "id": "25",
            "entry": "[25] R. A. Yeh, C. Chen, T. Y. Lim, A. G. Schwing, M. Hasegawa-Johnson, and M. N. Do. Semantic image inpainting with deep generative models. In CVPR, pages 5485\u20135493, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20R.A.%20Chen%2C%20C.%20Lim%2C%20T.Y.%20Schwing%2C%20A.G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20R.A.%20Chen%2C%20C.%20Lim%2C%20T.Y.%20Schwing%2C%20A.G.%20Semantic%20image%20inpainting%20with%20deep%20generative%20models%202017"
        },
        {
            "id": "26",
            "entry": "[26] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang. Generative image inpainting with contextual attention. arXiv preprint arXiv:1801.07892, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07892"
        },
        {
            "id": "27",
            "entry": "[27] Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma. Single-image crowd counting via multi-column convolutional neural network. In CVPR, pages 589\u2013597, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Zhou%2C%20D.%20Chen%2C%20S.%20Gao%2C%20S.%20Single-image%20crowd%20counting%20via%20multi-column%20convolutional%20neural%20network%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Zhou%2C%20D.%20Chen%2C%20S.%20Gao%2C%20S.%20Single-image%20crowd%20counting%20via%20multi-column%20convolutional%20neural%20network%202016"
        },
        {
            "id": "28",
            "entry": "[28] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image database for scene recognition. TPAMI, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Lapedriza%2C%20A.%20Khosla%2C%20A.%20Oliva%2C%20A.%20Places%3A%20A%2010%20million%20image%20database%20for%20scene%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20B.%20Lapedriza%2C%20A.%20Khosla%2C%20A.%20Oliva%2C%20A.%20Places%3A%20A%2010%20million%20image%20database%20for%20scene%20recognition%202017"
        }
    ]
}
