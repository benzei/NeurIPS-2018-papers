{
    "filename": "8141-representer-point-selection-for-explaining-deep-neural-networks.pdf",
    "metadata": {
        "title": "Representer Point Selection for Explaining Deep Neural Networks",
        "author": "Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, Pradeep K. Ravikumar",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8141-representer-point-selection-for-explaining-deep-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions."
    },
    "keywords": [
        {
            "term": "reproducing kernel Hilbert spaces",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_spaces"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "representer theorem",
            "url": "https://en.wikipedia.org/wiki/representer_theorem"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "influence function",
            "url": "https://en.wikipedia.org/wiki/influence_function"
        }
    ],
    "highlights": [
        "As machine learning systems start to be more widely used, we are starting to care not just about the accuracy and speed of the predictions, but why it made its specific predictions",
        "We show that we can decompose the pre-activation prediction values into a linear combination of training point activations, with the weights corresponding to what we call representer values, which can be used to measure the importance of each training point has on the learned parameter of the model",
        "Tory, while a negative representer value indicates that a similarity to that training point is inhibitory, to the prediction at the given test point",
        "We provide a representer theorem that addresses these two points: it holds for deep neural networks, and for any stationary point solution",
        "In this work we proposed a novel method of selecting representer points, the training examples that are influential to the model\u2019s prediction",
        "To do so we introduced the modified representer theorem that could be generalized to most deep neural networks, which allows us to linearly decompose the prediction value into a sum of representer values"
    ],
    "key_statements": [
        "As machine learning systems start to be more widely used, we are starting to care not just about the accuracy and speed of the predictions, but why it made its specific predictions",
        "We show that we can decompose the pre-activation prediction values into a linear combination of training point activations, with the weights corresponding to what we call representer values, which can be used to measure the importance of each training point has on the learned parameter of the model",
        "Tory, while a negative representer value indicates that a similarity to that training point is inhibitory, to the prediction at the given test point",
        "Looking at its most inhibitory training points, we see that the dataset is rife with training images where there are antelopes in the image, but some other animals, and the image is labeled with the other animal. These contribute to inhibitory effects of small antelopes with other big objects: an insight that as machine learning experts, we found deeply useful, and which is difficult to obtain via other explanatory approaches",
        "We provide a representer theorem that addresses these two points: it holds for deep neural networks, and for any stationary point solution",
        "In Figure 2 we report the results for four different metrics: \u201cours\u201d picks the points with bigger |\u03b1ij| for training instance i and its corresponding label j; \u201cinfluence\u201d prioritizes the training points with bigger influence function value; and \u201crandom\u201d picks random points",
        "In Figure 4, for the image of a rhino the influence function could not recover useful training points, while ours does, including the similar-looking elephants or zebras which might be confused as rhinos, as negatives",
        "From Theorem 3.1, we have seen that the pre-softmax output of the neural network can be decomposed as the weighted sum of the product of the training point feature and the test point feature, or",
        "In this work we proposed a novel method of selecting representer points, the training examples that are influential to the model\u2019s prediction",
        "To do so we introduced the modified representer theorem that could be generalized to most deep neural networks, which allows us to linearly decompose the prediction value into a sum of representer values"
    ],
    "summary": [
        "As machine learning systems start to be more widely used, we are starting to care not just about the accuracy and speed of the predictions, but why it made its specific predictions.",
        "Tory, while a negative representer value indicates that a similarity to that training point is inhibitory, to the prediction at the given test point.",
        "The second class of approaches are sample-based, and they identify training samples that have the most influence on the model\u2019s prediction on a test point.",
        "By introducing a L2 regularizer on the weight with a fixed \u03bb > 0, we can decompose the presoftmax prediction value as some finite linear combinations of a function between the test and train data.",
        "We visualize the training points with high representer values for some test points in Animals with Attributes (AwA) dataset [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] and compare the results with those of the influence functions.",
        "Just like the influence functions, our method returns images that look like the test image but are labeled as a different class.",
        "In Figure 4, for the image of a rhino the influence function could not recover useful training points, while ours does, including the similar-looking elephants or zebras which might be confused as rhinos, as negatives.",
        "The negative examples work as inhibitory examples for the model \u2013 they suppress the activation values for a particular class of a given test point because they are in a different class despite their striking similarity to the test image.",
        "Such inhibitory points provide a richer understanding, even to machine learning experts, of the behavior of deep neural networks, since they explicitly indicate training points that lead the network away from a particular label for the given test point.",
        "From Theorem 3.1, we have seen that the pre-softmax output of the neural network can be decomposed as the weighted sum of the product of the training point feature and the test point feature, or",
        "4.5 Computational Cost and Numerical Instabilities Computation time is particularly an issue for computing the influence function values [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] for a large dataset, which is very costly to compute for each test point.",
        "Note that the influence function does not need the fine-tuning step when given a pre-trained model, the values being 0, while our method first optimizes for \u0398\u2217 using line-search computes the representer values.",
        "For CIFAR-10, over 30 percent of the test images had all zero training point influences, so influence function was unable to provide positive or negative influential examples.",
        "In this work we proposed a novel method of selecting representer points, the training examples that are influential to the model\u2019s prediction.",
        "We have demonstrated our method\u2019s advantages and performances on several large-scale models and image datasets, along with some insights on how these values allow the users to understand the behaviors of the model"
    ],
    "headline": "We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135\u20131144. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016"
        },
        {
            "id": "2",
            "entry": "[2] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision modelagnostic explanations. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Anchors%3A%20High-precision%20modelagnostic%20explanations%202018"
        },
        {
            "id": "3",
            "entry": "[3] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "4",
            "entry": "[4] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. arXiv preprint arXiv:1704.02685, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02685"
        },
        {
            "id": "5",
            "entry": "[5] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01365"
        },
        {
            "id": "6",
            "entry": "[6] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Sebastian%20Binder%2C%20Alexander%20Montavon%2C%20Gr%C3%A9goire%20Klauschen%2C%20Frederick%20On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "7",
            "entry": "[7] Jacob Bien and Robert Tibshirani. Prototype selection for interpretable classification. The Annals of Applied Statistics, pages 2403\u20132424, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bien%2C%20Jacob%20Tibshirani%2C%20Robert%20Prototype%20selection%20for%20interpretable%20classification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bien%2C%20Jacob%20Tibshirani%2C%20Robert%20Prototype%20selection%20for%20interpretable%20classification%202011"
        },
        {
            "id": "8",
            "entry": "[8] Been Kim, Cynthia Rudin, and Julie A Shah. The bayesian case model: A generative approach for case-based reasoning and prototype classification. In Advances in Neural Information Processing Systems, pages 1952\u20131960, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Been%20Rudin%2C%20Cynthia%20Shah%2C%20Julie%20A.%20The%20bayesian%20case%20model%3A%20A%20generative%20approach%20for%20case-based%20reasoning%20and%20prototype%20classification%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Been%20Rudin%2C%20Cynthia%20Shah%2C%20Julie%20A.%20The%20bayesian%20case%20model%3A%20A%20generative%20approach%20for%20case-based%20reasoning%20and%20prototype%20classification%201952"
        },
        {
            "id": "9",
            "entry": "[9] Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism for interpretability. In Advances in Neural Information Processing Systems, pages 2280\u20132288, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Been%20Khanna%2C%20Rajiv%20Koyejo%2C%20Oluwasanmi%20O.%20Examples%20are%20not%20enough%2C%20learn%20to%20criticize%21%20criticism%20for%20interpretability%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Been%20Khanna%2C%20Rajiv%20Koyejo%2C%20Oluwasanmi%20O.%20Examples%20are%20not%20enough%2C%20learn%20to%20criticize%21%20criticism%20for%20interpretability%202016"
        },
        {
            "id": "10",
            "entry": "[10] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning, pages 1885\u20131894, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koh%2C%20Pang%20Wei%20Liang%2C%20Percy%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koh%2C%20Pang%20Wei%20Liang%2C%20Percy%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017"
        },
        {
            "id": "11",
            "entry": "[11] Rushil Anirudh, Jayaraman J Thiagarajan, Rahul Sridhar, and Timo Bremer. Influential sample selection: A graph signal processing approach. arXiv preprint arXiv:1711.05407, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05407"
        },
        {
            "id": "12",
            "entry": "[12] Bernhard Sch\u00f6lkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In International conference on computational learning theory, pages 416\u2013426.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20Bernhard%20Herbrich%2C%20Ralf%20Smola%2C%20Alex%20J.%20A%20generalized%20representer%20theorem",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6lkopf%2C%20Bernhard%20Herbrich%2C%20Ralf%20Smola%2C%20Alex%20J.%20A%20generalized%20representer%20theorem"
        },
        {
            "id": "13",
            "entry": "[13] Bastian Bohn, Michael Griebel, and Christian Rieger. A representer theorem for deep kernel learning. arXiv preprint arXiv:1709.10441, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10441"
        },
        {
            "id": "14",
            "entry": "[14] Michael Unser. A representer theorem for deep neural networks. arXiv preprint arXiv:1802.09210, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09210"
        },
        {
            "id": "15",
            "entry": "[15] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "16",
            "entry": "[16] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "17",
            "entry": "[17] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "18",
            "entry": "[18] Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly. arXiv preprint arXiv:1707.00600, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00600"
        },
        {
            "id": "19",
            "entry": "[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "20",
            "entry": "[20] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03825"
        },
        {
            "id": "21",
            "entry": "[21] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142\u2013150. Association for Computational Linguistics, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011"
        }
    ]
}
