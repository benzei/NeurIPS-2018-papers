{
    "filename": "8137-coordinate-descent-with-bandit-sampling.pdf",
    "metadata": {
        "title": "Coordinate Descent with Bandit Sampling",
        "author": "Farnood Salehi, Patrick Thiran, Elisa Celis",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8137-coordinate-descent-with-bandit-sampling.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Coordinate descent methods usually minimize a cost function by updating a random decision variable (corresponding to one coordinate) at a time. Ideally, we would update the decision variable that yields the largest decrease in the cost function. However, finding this coordinate would require checking all of them, which would effectively negate the improvement in computational tractability that coordinate descent is intended to afford. To address this, we propose a new adaptive method for selecting a coordinate. First, we find a lower bound on the amount the cost function decreases when a coordinate is updated. We then use a multi-armed bandit algorithm to learn which coordinates result in the largest lower bound by interleaving this learning with conventional coordinate descent updates except that the coordinate is selected proportionately to the expected decrease. We show that our approach improves the convergence of coordinate descent methods both theoretically and experimentally."
    },
    "keywords": [
        {
            "term": "cost function",
            "url": "https://en.wikipedia.org/wiki/cost_function"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "convex function",
            "url": "https://en.wikipedia.org/wiki/convex_function"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "multi-armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi-armed_bandit"
        }
    ],
    "highlights": [
        "Most supervised learning algorithms minimize an empirical risk cost function over a dataset",
        "I=1 where f (\u00b7) : Rn ! R is a smooth convex function, d is the number of decision variables on which the cost function is minimized, which are gathered in vector x 2 Rd, gi(\u00b7) : R ! R are convex functions for all i 2 [d], and A 2 Rn\u21e5d is the data matrix",
        "Figure 2 shows the result for Lasso",
        "We propose a new approach to select the coordinates to update in Coordinate Descent methods",
        "We derive a lower bound on the decrease of the cost function in Lemma 1, i.e., the marginal decrease, when a coordinate is updated, for a large class of update methods H",
        "We show that the approach converges faster than state-of-the-art approaches both theoretically and empirically"
    ],
    "key_statements": [
        "Most supervised learning algorithms minimize an empirical risk cost function over a dataset",
        "I=1 where f (\u00b7) : Rn ! R is a smooth convex function, d is the number of decision variables on which the cost function is minimized, which are gathered in vector x 2 Rd, gi(\u00b7) : R ! R are convex functions for all i 2 [d], and A 2 Rn\u21e5d is the data matrix",
        "Figure 2 shows the result for Lasso",
        "We propose a new approach to select the coordinates to update in Coordinate Descent methods",
        "We derive a lower bound on the decrease of the cost function in Lemma 1, i.e., the marginal decrease, when a coordinate is updated, for a large class of update methods H",
        "We show that the approach converges faster than state-of-the-art approaches both theoretically and empirically",
        "The bandit algorithm B_max_r uses only the marginal decrease of the selected coordinate to update the estimations of the marginal decreases"
    ],
    "summary": [
        "Most supervised learning algorithms minimize an empirical risk cost function over a dataset.",
        "Our principled approach leverages a bandit algorithm to learn a good estimate of rit; this allows for theoretical guarantees and outperforms the state-of-the-art methods, as we will see in Section 4.",
        "Bandit approaches have very recently been used to accelerate various stochastic optimization algorithms; among these works [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] focus on improving the convergence of SGD by reducing the variance of the estimator for the gradient.",
        "We compare the algorithm for full information setting as in Section 2.3 against other state-ofthe-art methods that use O(d \u00b7 z) computations per epoch of size d, where zdenotes the number of non-zero elements of A.",
        "We compare the algorithm for partial information setting as in Section 2.4 (B_max_r) against other methods with appropriate heuristic modifications that allow them to use O(z) computations per epoch.",
        "Benchmarks for Adaptive-Bandit Algorithm (B_max_r): For comparison, in addition to the uniform sampling, we consider the coordinate selection method that has the best performance empirically in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and two accelerated CD methods NUACDM in [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and Approx in [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>].",
        "NUACDM [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]: Sample a coordinate i 2 [d] with probability proportional to the square root of smoothness of the cost function along the th i coordinate, use an unbiased estimator for the gradient to update the decision variables.",
        "We see that the accelerated CD method Approx converges faster than uniform sampling and gap_per_epoch, but using B_max_r improves the convergence rate and reaches a lower sub-optimality gap \u270f with the same number of iterations.",
        "To test the effect of \" and E on the convergence rate, we choose a9a dataset and perform a binary classification on it by using the logistic regression cost function.",
        "A smaller bin size E results in a larger clock time, because we need to compute the marginal decreases for all coordinates more often.",
        "We derive a lower bound on the decrease of the cost function in Lemma 1, i.e., the marginal decrease, when a coordinate is updated, for a large class of update methods H.",
        "We emphasize that our coordinate selection approach is quite general and works for a large class of update rules H, which includes Lasso, SVM, ridge and logistic regression, and a large class of bandit algorithms that select the coordinate to update.",
        "The bandit algorithm B_max_r uses only the marginal decrease of the selected coordinate to update the estimations of the marginal decreases."
    ],
    "headline": "We propose a new adaptive method for selecting a coordinate",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Z Allen-Zhu, Z Qu, P Richt\u00e1rik, and Y Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In International Conference on Machine Learning, pages 1110\u20131119, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Z%20Allen-Zhu%2C%20Z%20Qu%2C%20P%20Richt%C3%A1rik%20Yuan%2C%20Y%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Z%20Allen-Zhu%2C%20Z%20Qu%2C%20P%20Richt%C3%A1rik%20Yuan%2C%20Y%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016"
        },
        {
            "id": "2",
            "entry": "[2] Y Arjevani and O Shamir. Dimension-free iteration complexity of finite sum optimization problems. In Advances in Neural Information Processing Systems, pages 3540\u20133548, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjevani%2C%20Y.%20Shamir%2C%20O.%20Dimension-free%20iteration%20complexity%20of%20finite%20sum%20optimization%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjevani%2C%20Y.%20Shamir%2C%20O.%20Dimension-free%20iteration%20complexity%20of%20finite%20sum%20optimization%20problems%202016"
        },
        {
            "id": "3",
            "entry": "[3] P Auer, N Cesa-Bianchi, Y Freund, and R Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "4",
            "entry": "[4] Z Borsos, A Krause, and K Levy. Online variance reduction for stochastic optimization. In International Conference on Learning Theory, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borsos%2C%20Z.%20Krause%2C%20A.%20Levy%2C%20K.%20Online%20variance%20reduction%20for%20stochastic%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borsos%2C%20Z.%20Krause%2C%20A.%20Levy%2C%20K.%20Online%20variance%20reduction%20for%20stochastic%20optimization%202018"
        },
        {
            "id": "5",
            "entry": "[5] C Chang and C Lin. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20C.%20Lin%2C%20C.%20Libsvm%3A%20a%20library%20for%20support%20vector%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20C.%20Lin%2C%20C.%20Libsvm%3A%20a%20library%20for%20support%20vector%20machines%202011"
        },
        {
            "id": "6",
            "entry": "[6] D Csiba, Z Qu, and P Richt\u00e1rik. Stochastic dual coordinate ascent with adaptive probabilities. In International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Csiba%2C%20D.%20Qu%2C%20Z.%20Richt%C3%A1rik%2C%20P.%20Stochastic%20dual%20coordinate%20ascent%20with%20adaptive%20probabilities%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Csiba%2C%20D.%20Qu%2C%20Z.%20Richt%C3%A1rik%2C%20P.%20Stochastic%20dual%20coordinate%20ascent%20with%20adaptive%20probabilities%202015"
        },
        {
            "id": "7",
            "entry": "[7] C D\u00fcnner, S Forte, M Tak\u00e1c, and M Jaggi. Primal-dual rates and certificates. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%C3%BCnner%2C%20C.%20Forte%2C%20S.%20Tak%C3%A1c%2C%20M.%20Jaggi%2C%20M.%20Primal-dual%20rates%20and%20certificates%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%C3%BCnner%2C%20C.%20Forte%2C%20S.%20Tak%C3%A1c%2C%20M.%20Jaggi%2C%20M.%20Primal-dual%20rates%20and%20certificates%202016"
        },
        {
            "id": "8",
            "entry": "[8] C D\u00fcnner, T Parnell, and M Jaggi. Efficient use of limited-memory accelerators for linear learning on heterogeneous systems. In Advances in Neural Information Processing Systems, pages 4261\u20134270, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%C3%BCnner%2C%20C.%20Parnell%2C%20T.%20Jaggi%2C%20M.%20Efficient%20use%20of%20limited-memory%20accelerators%20for%20linear%20learning%20on%20heterogeneous%20systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%C3%BCnner%2C%20C.%20Parnell%2C%20T.%20Jaggi%2C%20M.%20Efficient%20use%20of%20limited-memory%20accelerators%20for%20linear%20learning%20on%20heterogeneous%20systems%202017"
        },
        {
            "id": "9",
            "entry": "[9] O Fercoq and P Richt\u00e1rik. Accelerated, parallel, and proximal coordinate descent. SIAM Journal on Optimization, 25(4):1997\u20132023, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fercoq%2C%20O.%20Accelerated%2C%20P.Richt%C3%A1rik%20parallel%20and%20proximal%20coordinate%20descent%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fercoq%2C%20O.%20Accelerated%2C%20P.Richt%C3%A1rik%20parallel%20and%20proximal%20coordinate%20descent%202015"
        },
        {
            "id": "10",
            "entry": "[10] T Glasmachers and U Dogan. Accelerated coordinate descent with adaptive coordinate frequencies. In Asian Conference on Machine Learning, pages 72\u201386, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glasmachers%2C%20T.%20Dogan%2C%20U.%20Accelerated%20coordinate%20descent%20with%20adaptive%20coordinate%20frequencies%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glasmachers%2C%20T.%20Dogan%2C%20U.%20Accelerated%20coordinate%20descent%20with%20adaptive%20coordinate%20frequencies%202013"
        },
        {
            "id": "11",
            "entry": "[11] T Johnson and C Guestrin. Stingycd: Safely avoiding wasteful updates in coordinate descent. In International Conference on Machine Learning, pages 1752\u20131760, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20T.%20Guestrin%2C%20C.%20Stingycd%3A%20Safely%20avoiding%20wasteful%20updates%20in%20coordinate%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20T.%20Guestrin%2C%20C.%20Stingycd%3A%20Safely%20avoiding%20wasteful%20updates%20in%20coordinate%20descent%202017"
        },
        {
            "id": "12",
            "entry": "[12] H Namkoong, A Sinha, S Yadlowsky, and J Duchi. Adaptive sampling probabilities for non-smooth optimization. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Namkoong%2C%20H.%20Sinha%2C%20A.%20Yadlowsky%2C%20S.%20Duchi%2C%20J.%20Adaptive%20sampling%20probabilities%20for%20non-smooth%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Namkoong%2C%20H.%20Sinha%2C%20A.%20Yadlowsky%2C%20S.%20Duchi%2C%20J.%20Adaptive%20sampling%20probabilities%20for%20non-smooth%20optimization%202017"
        },
        {
            "id": "13",
            "entry": "[13] J Nutini, M Schmidt, I Laradji, M Friedlander, and H Koepke. Coordinate descent converges faster with the gauss-southwell rule than random selection. In International Conference on Machine Learning, pages 1632\u20131641, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nutini%2C%20J.%20Schmidt%2C%20M.%20Laradji%2C%20I.%20Friedlander%2C%20M.%20Coordinate%20descent%20converges%20faster%20with%20the%20gauss-southwell%20rule%20than%20random%20selection%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nutini%2C%20J.%20Schmidt%2C%20M.%20Laradji%2C%20I.%20Friedlander%2C%20M.%20Coordinate%20descent%20converges%20faster%20with%20the%20gauss-southwell%20rule%20than%20random%20selection%202015"
        },
        {
            "id": "14",
            "entry": "[14] A Osokin, J Alayrac, I Lukasewitz, P Dokania, and S Lacoste-Julien. Minding the gaps for block frank-wolfe optimization of structured svms. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20A.%20Alayrac%2C%20J.%20Lukasewitz%2C%20I.%20Dokania%2C%20P.%20Minding%20the%20gaps%20for%20block%20frank-wolfe%20optimization%20of%20structured%20svms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20A.%20Alayrac%2C%20J.%20Lukasewitz%2C%20I.%20Dokania%2C%20P.%20Minding%20the%20gaps%20for%20block%20frank-wolfe%20optimization%20of%20structured%20svms%202016"
        },
        {
            "id": "15",
            "entry": "[15] D Perekrestenko, V Cevher, and M Jaggi. Faster coordinate descent via adaptive importance sampling. In International Conference on Artificial Intelligence and Statistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perekrestenko%2C%20D.%20Cevher%2C%20V.%20Jaggi%2C%20M.%20Faster%20coordinate%20descent%20via%20adaptive%20importance%20sampling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perekrestenko%2C%20D.%20Cevher%2C%20V.%20Jaggi%2C%20M.%20Faster%20coordinate%20descent%20via%20adaptive%20importance%20sampling%202017"
        },
        {
            "id": "16",
            "entry": "[16] A Rakotomamonjy, S Ko\u00e7o, and L Ralaivola. Greedy methods, randomization approaches, and multiarm bandit algorithms for efficient sparsity-constrained optimization. IEEE transactions on neural networks and learning systems, 28(11):2789\u20132802, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakotomamonjy%2C%20A.%20Ko%C3%A7o%2C%20S.%20Ralaivola%2C%20L.%20Greedy%20methods%2C%20randomization%20approaches%2C%20and%20multiarm%20bandit%20algorithms%20for%20efficient%20sparsity-constrained%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakotomamonjy%2C%20A.%20Ko%C3%A7o%2C%20S.%20Ralaivola%2C%20L.%20Greedy%20methods%2C%20randomization%20approaches%2C%20and%20multiarm%20bandit%20algorithms%20for%20efficient%20sparsity-constrained%20optimization%202017"
        },
        {
            "id": "17",
            "entry": "[17] F Salehi, L.E Celis, and P Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544v2, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02544v2"
        },
        {
            "id": "18",
            "entry": "[18] S Shalev-Shwartz and A Tewari. Stochastic methods for l1-regularized loss minimization. Journal of Machine Learning Research, 12(Jun):1865\u20131892, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Tewari%2C%20A.%20Stochastic%20methods%20for%20l1-regularized%20loss%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Tewari%2C%20A.%20Stochastic%20methods%20for%20l1-regularized%20loss%20minimization%202011"
        },
        {
            "id": "19",
            "entry": "[19] S Shalev-Shwartz and T Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 378\u2013385, 2013a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Accelerated%20mini-batch%20stochastic%20dual%20coordinate%20ascent",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Accelerated%20mini-batch%20stochastic%20dual%20coordinate%20ascent"
        },
        {
            "id": "20",
            "entry": "[20] S Shalev-Shwartz and T Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567\u2013599, 2013b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "21",
            "entry": "[21] H Shi, S Tu, Y Xu, and W Yin. A primer on coordinate descent algorithms. arXiv preprint arXiv:1610.00040, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.00040"
        },
        {
            "id": "22",
            "entry": "[22] S Stich, At Raj, and M Jaggi. Approximate steepest coordinate descent. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stich%2C%20S.%20Raj%2C%20At%20Jaggi%2C%20M.%20Approximate%20steepest%20coordinate%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stich%2C%20S.%20Raj%2C%20At%20Jaggi%2C%20M.%20Approximate%20steepest%20coordinate%20descent%202017"
        },
        {
            "id": "23",
            "entry": "[23] A Zhang and Q Gu. Accelerated stochastic block coordinate descent with optimal sampling. In International Conference on Knowledge Discovery and Data Mining, pages 2035\u20132044. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20A.%20Gu%2C%20Q.%20Accelerated%20stochastic%20block%20coordinate%20descent%20with%20optimal%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20A.%20Gu%2C%20Q.%20Accelerated%20stochastic%20block%20coordinate%20descent%20with%20optimal%20sampling%202016"
        },
        {
            "id": "24",
            "entry": "[24] P Zhao and T Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In International Conference on Machine Learning, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20P.%20Zhang%2C%20T.%20Stochastic%20optimization%20with%20importance%20sampling%20for%20regularized%20loss%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20P.%20Zhang%2C%20T.%20Stochastic%20optimization%20with%20importance%20sampling%20for%20regularized%20loss%20minimization%202015"
        }
    ]
}
