{
    "filename": "7541-paraphrasing-complex-network-network-compression-via-factor-transfer.pdf",
    "metadata": {
        "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer",
        "author": "Jangho Kim, Seonguk Park, Nojun Kwak",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher\u2019s knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods."
    },
    "keywords": [
        {
            "term": "transfer method",
            "url": "https://en.wikipedia.org/wiki/Transfer_Method"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "object detection",
            "url": "https://en.wikipedia.org/wiki/object_detection"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        }
    ],
    "highlights": [
        "Deep neural nets (DNNs) have shown their remarkable capabilities in various parts of computer vision and pattern recognition tasks such as image classification, object detection, localization and segmentation",
        "Teacher factors are extracted from the teacher network by a paraphraser.\n2) In the second step, these teacher factors are transferred to the student factors such that the student network learns from them.\n3.1",
        "We propose the factor transfer which is a novel method for knowledge transfer",
        "We introduce factors which contain paraphrased information of the teacher network, extracted from the paraphraser",
        "The other reason is that the translator of the student can help the student network to understand teacher factors by mimicking the teacher factors",
        "We showed the effectiveness of the factor transfer on various image classification datasets"
    ],
    "key_statements": [
        "Deep neural nets (DNNs) have shown their remarkable capabilities in various parts of computer vision and pattern recognition tasks such as image classification, object detection, localization and segmentation",
        "Many researchers have studied deep neural network for their application in various fields, high-performance deep neural network generally require a vast amount of computational power and storage, which makes them difficult to be used in embedded systems that have limited resources",
        "These studies can be roughly classified into four categories: 1) network pruning, 2) network quantization, 3) building efficient small networks, and 4) knowledge transfer",
        "We propose a convolutional translator in the student side that learns the factors of the teacher network",
        "Teacher factors are extracted from the teacher network by a paraphraser.\n2) In the second step, these teacher factors are transferred to the student factors such that the student network learns from them.\n3.1",
        "We have described that the teacher network provides more paraphrased information to the student network via factors, and described a need for a translator to act as a buffer to better understand factors in the student network",
        "We propose the factor transfer which is a novel method for knowledge transfer",
        "We introduce factors which contain paraphrased information of the teacher network, extracted from the paraphraser",
        "One reason is that the factors can relieve the inherent differences between the teacher and student network",
        "The other reason is that the translator of the student can help the student network to understand teacher factors by mimicking the teacher factors",
        "A downside of the proposed method is that the factor transfer requires the training of a paraphraser to extract factors and needs more parameters of the paraphraser and the translator",
        "We showed the effectiveness of the factor transfer on various image classification datasets",
        "We verified that factor transfer can be applied to other domains than classification"
    ],
    "summary": [
        "Deep neural nets (DNNs) have shown their remarkable capabilities in various parts of computer vision and pattern recognition tasks such as image classification, object detection, localization and segmentation.",
        "While these methods provide fairly good performance improvements, directly transferring the teacher\u2019s outputs overlooks the inherent differences between the teacher network and the student network, such as the network structure, the number of channels, and initial conditions.",
        "We propose a novel knowledge transferring method that leads both the student and teacher networks to make transportable features, which we call \u2018factors\u2019 in this paper.",
        "We trained the student network with the translator to assimilate the factors extracted from the paraphraser.",
        "We succeeded in training the student network to perform better than the ones with the same architecture trained by the conventional knowledge transfer methods.",
        "We propose a convolutional translator in the student side that learns the factors of the teacher network.",
        "Since the teacher network and the student network are focusing on the same task, we extracted factors from the feature maps of the last group as clearly can be seen in Figure 1 because the last layer of a trained network must contain enough information for the task.",
        "In order to extract the factor from the teacher network, we train the paraphraser in an unsupervised way by assigning the reconstruction loss between the input feature maps x and the output feature maps P (x) of the paraphraser.",
        "Once the teacher network has extracted the factors which are the paraphrased teacher\u2019s knowledge, the student network should be able to absorb and digest them on its own way.",
        "The student network is trained with the translator using the sum of two loss terms, i.e. the classification loss and the factor transfer loss: Lstudent = Lcls + \u03b2LF T , (2)",
        "In terms of paraphrasing the information of the teacher network, the paraphraser which maintains the spatial dimension outperformed autoencoders based methods which use CAE or RAE.",
        "We have described that the teacher network provides more paraphrased information to the student network via factors, and described a need for a translator to act as a buffer to better understand factors in the student network.",
        "There are mainly two reasons that the student can understand information from the teacher network more by the factor transfer than other methods.",
        "A downside of the proposed method is that the factor transfer requires the training of a paraphraser to extract factors and needs more parameters of the paraphraser and the translator.",
        "We think that our method will help further researches in knowledge transfer"
    ],
    "headline": "We propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher\u2019s knowledge and to translate it for the student",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the datagenerating distribution. The Journal of Machine Learning Research, 15(1):3563\u20133593, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alain%2C%20Guillaume%20Bengio%2C%20Yoshua%20What%20regularized%20auto-encoders%20learn%20from%20the%20datagenerating%20distribution%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alain%2C%20Guillaume%20Bengio%2C%20Yoshua%20What%20regularized%20auto-encoders%20learn%20from%20the%20datagenerating%20distribution%202014"
        },
        {
            "id": "2",
            "entry": "[2] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02167"
        },
        {
            "id": "3",
            "entry": "[3] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123\u20133131, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015"
        },
        {
            "id": "4",
            "entry": "[4] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "5",
            "entry": "[5] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.",
            "url": "http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Everingham%2C%20M.%20Gool%2C%20L.Van%20Williams%2C%20C.K.I.%20Winn%2C%20J.%20Results%202007"
        },
        {
            "id": "6",
            "entry": "[6] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pages 1737\u20131746, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015"
        },
        {
            "id": "7",
            "entry": "[7] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "8",
            "entry": "[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "9",
            "entry": "[9] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "10",
            "entry": "[10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "11",
            "entry": "[11] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "12",
            "entry": "[12] Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Condensenet: An efficient densenet using learned group convolutions. CoRR, abs/1711.09224, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09224"
        },
        {
            "id": "13",
            "entry": "[13] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "14",
            "entry": "[14] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alex%20Krizhevsky%20Vinod%20Nair%20and%20Geoffrey%20Hinton%20Cifar10%20canadian%20institute%20for%20advanced%20research"
        },
        {
            "id": "15",
            "entry": "[15] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alex%20Krizhevsky%20Vinod%20Nair%20and%20Geoffrey%20Hinton%20Cifar100%20canadian%20institute%20for%20advanced%20research"
        },
        {
            "id": "16",
            "entry": "[16] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473\u2013480. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larochelle%2C%20Hugo%20Erhan%2C%20Dumitru%20Courville%2C%20Aaron%20Bergstra%2C%20James%20An%20empirical%20evaluation%20of%20deep%20architectures%20on%20problems%20with%20many%20factors%20of%20variation%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20Hugo%20Erhan%2C%20Dumitru%20Courville%2C%20Aaron%20Bergstra%2C%20James%20An%20empirical%20evaluation%20of%20deep%20architectures%20on%20problems%20with%20many%20factors%20of%20variation%202007"
        },
        {
            "id": "17",
            "entry": "[17] Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pages 2554\u2013 2564. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lebedev%2C%20Vadim%20Lempitsky%2C%20Victor%20Fast%20convnets%20using%20group-wise%20brain%20damage%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lebedev%2C%20Vadim%20Lempitsky%2C%20Victor%20Fast%20convnets%20using%20group-wise%20brain%20damage%202016"
        },
        {
            "id": "18",
            "entry": "[18] Jonathan Masci, Ueli Meier, Dan Ciresan, and J\u00fcrgen Schmidhuber. Stacked convolutional auto-encoders for hierarchical feature extraction. In International Conference on Artificial Neural Networks, pages 52\u201359.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jonathan%20Masci%2C%20Ueli%20Meier%2C%20Dan%20Ciresan%20Schmidhuber%2C%20J%C3%BCrgen%20Stacked%20convolutional%20auto-encoders%20for%20hierarchical%20feature%20extraction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jonathan%20Masci%2C%20Ueli%20Meier%2C%20Dan%20Ciresan%20Schmidhuber%2C%20J%C3%BCrgen%20Stacked%20convolutional%20auto-encoders%20for%20hierarchical%20feature%20extraction"
        },
        {
            "id": "19",
            "entry": "[19] Wing WY Ng, Guangjun Zeng, Jiangjun Zhang, Daniel S Yeung, and Witold Pedrycz. Dual autoencoders features for imbalance classification problem. Pattern Recognition, 60:875\u2013889, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Wing%20W.Y.%20Zeng%2C%20Guangjun%20Zhang%2C%20Jiangjun%20Yeung%2C%20Daniel%20S.%20Dual%20autoencoders%20features%20for%20imbalance%20classification%20problem%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Wing%20W.Y.%20Zeng%2C%20Guangjun%20Zhang%2C%20Jiangjun%20Yeung%2C%20Daniel%20S.%20Dual%20autoencoders%20features%20for%20imbalance%20classification%20problem%202016"
        },
        {
            "id": "20",
            "entry": "[20] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525\u2013542.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks"
        },
        {
            "id": "21",
            "entry": "[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91\u201399, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015"
        },
        {
            "id": "22",
            "entry": "[22] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6550"
        },
        {
            "id": "23",
            "entry": "[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015"
        },
        {
            "id": "24",
            "entry": "[24] Hoo-Chang Shin, Matthew R Orton, David J Collins, Simon J Doran, and Martin O Leach. Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4d patient data. IEEE transactions on pattern analysis and machine intelligence, 35(8):1930\u20131943, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shin%2C%20Hoo-Chang%20Orton%2C%20Matthew%20R.%20Collins%2C%20David%20J.%20Doran%2C%20Simon%20J.%20Stacked%20autoencoders%20for%20unsupervised%20feature%20learning%20and%20multiple%20organ%20detection%20in%20a%20pilot%20study%20using%204d%20patient%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shin%2C%20Hoo-Chang%20Orton%2C%20Matthew%20R.%20Collins%2C%20David%20J.%20Doran%2C%20Simon%20J.%20Stacked%20autoencoders%20for%20unsupervised%20feature%20learning%20and%20multiple%20organ%20detection%20in%20a%20pilot%20study%20using%204d%20patient%20data%202013"
        },
        {
            "id": "25",
            "entry": "[25] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "26",
            "entry": "[26] Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv preprint arXiv:1507.06149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.06149"
        },
        {
            "id": "27",
            "entry": "[27] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4820\u20134828, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20convolutional%20neural%20networks%20for%20mobile%20devices%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20convolutional%20neural%20networks%20for%20mobile%20devices%202016"
        },
        {
            "id": "28",
            "entry": "[28] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yim%2C%20Junho%20Joo%2C%20Donggyu%20Bae%2C%20Jihoon%20Kim%2C%20Junmo%20A%20gift%20from%20knowledge%20distillation%3A%20Fast%20optimization%2C%20network%20minimization%20and%20transfer%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yim%2C%20Junho%20Joo%2C%20Donggyu%20Bae%2C%20Jihoon%20Kim%2C%20Junmo%20A%20gift%20from%20knowledge%20distillation%3A%20Fast%20optimization%2C%20network%20minimization%20and%20transfer%20learning%202017"
        },
        {
            "id": "29",
            "entry": "[29] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "30",
            "entry": "[30] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03928"
        },
        {
            "id": "31",
            "entry": "[31] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "32",
            "entry": "[32] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03126"
        },
        {
            "id": "33",
            "entry": "[33] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        }
    ]
}
