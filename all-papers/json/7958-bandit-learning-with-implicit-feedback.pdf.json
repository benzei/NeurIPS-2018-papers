{
    "filename": "7958-bandit-learning-with-implicit-feedback.pdf",
    "metadata": {
        "date": 2018,
        "title": "Bandit Learning with Implicit Feedback",
        "author": "Yi Qi1, Qingyun Wu2, Hongning Wang2, Jie Tang1, Maosong Sun1 1 State Key Lab of Intell. Tech. & Sys., Institute for Artificial Intelligence,",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7958-bandit-learning-with-implicit-feedback.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Implicit feedback, such as user clicks, although abundant in online information service systems, does not provide substantial evidence on users\u2019 evaluation of system\u2019s output. Without proper modeling, such incomplete supervision inevitably misleads model estimation, especially in a bandit learning setting where the feedback is acquired on the fly. In this work, we perform contextual bandit learning with implicit feedback by modeling the feedback as a composition of user result examination and relevance judgment. Since users\u2019 examination behavior is unobserved, we introduce latent variables to model it. We perform Thompson sampling on top of variational Bayesian inference for arm selection and model update. Our upper regret bound analysis of the proposed algorithm proves its feasibility of learning from implicit feedback in a bandit setting; and extensive empirical evaluations on click logs collected from a major MOOC platform further demonstrate its learning effectiveness in practice."
    },
    "keywords": [
        {
            "term": "click through rate",
            "url": "https://en.wikipedia.org/wiki/click_through_rate"
        },
        {
            "term": "contextual bandit algorithm",
            "url": "https://en.wikipedia.org/wiki/contextual_bandit_algorithm"
        },
        {
            "term": "Massive Open Online Course",
            "url": "https://en.wikipedia.org/wiki/Massive_Open_Online_Course"
        },
        {
            "term": "information service",
            "url": "https://en.wikipedia.org/wiki/information_service"
        }
    ],
    "highlights": [
        "Contextual bandit algorithms [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] provide modern information service systems an effective solution to adaptively find good mappings between available items and users",
        "We focus on learning contextual bandits with user click feedback, and model such implicit feedback as a composition of user result examination and relevance judgment",
        "Thanks to the tight approximation achieved by Bayesian variational inference presented in Section 4, truly online model update is feasible in this algorithm",
        "Li et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] proposed a method to calculate a near-unbiased estimate of click through rate of any bandit algorithm based on the collected history data, so that offline evaluation and performance comparison are possible",
        "Motivated by the examination hypothesis in user click modeling, in this paper we developed E-C Bandit, which differentiates result examination and content relevance in user clicks and actively learns from such implicit feedback",
        "It is important to study a tighter upper regret bound under our approximated posterior in general"
    ],
    "key_statements": [
        "Contextual bandit algorithms [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] provide modern information service systems an effective solution to adaptively find good mappings between available items and users",
        "We focus on learning contextual bandits with user click feedback, and model such implicit feedback as a composition of user result examination and relevance judgment",
        "Thanks to the tight approximation achieved by Bayesian variational inference presented in Section 4, truly online model update is feasible in this algorithm",
        "Li et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] proposed a method to calculate a near-unbiased estimate of click through rate of any bandit algorithm based on the collected history data, so that offline evaluation and performance comparison are possible",
        "Motivated by the examination hypothesis in user click modeling, in this paper we developed E-C Bandit, which differentiates result examination and content relevance in user clicks and actively learns from such implicit feedback",
        "It is important to study a tighter upper regret bound under our approximated posterior in general"
    ],
    "summary": [
        "Contextual bandit algorithms [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] provide modern information service systems an effective solution to adaptively find good mappings between available items and users.",
        "Our finite time analysis proves that, despite the increased complexity in parameter estimation introduced by the latent variables, our Thompson sampling policy based on the true posterior is guaranteed to achieve a sub-linear Bayesian regret with a high probability.",
        "Based on the unbiased offline evaluation policy [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], our algorithm achieved a 8.9% CTR lift compared to standard contextual bandits [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] which do not model users\u2019 examination behavior.",
        "There is a line of related research that develops click model based bandit algorithms for learning to rank problems.",
        "We turn to provide an upper regret bound of our E-C Bandit, based on the above generic Bayeisan regret analysis under a log-loss estimator.",
        "We want to highlight that despite mountains of works focusing on generalized linear bandits, most of them are not truly online algorithms, because the estimation of their parameters at each iteration has to involve all historical observations iteratively.",
        "Thanks to the tight approximation achieved by Bayesian variational inference presented in Section 4, truly online model update is feasible in this algorithm.",
        "Bandit suffers from a linear regret with respect to time t, as it mistakenly treats no click as negative feedback.",
        "We construct a more general case in Appendix F to illustrate the scenario that failing to model examination condition would lead to a linear regret.",
        "Li et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] proposed a method to calculate a near-unbiased estimate of CTR of any bandit algorithm based on the collected history data, so that offline evaluation and performance comparison are possible.",
        "The improvement of our model compared to Logistic Bandit clearly suggests the necessity of modeling examination condition in user clicks for improving the online recommendation performance, and the improvement against PBMUCB provides strong evidence of the importance of modelling examination with available contextual information.",
        "Motivated by the examination hypothesis in user click modeling, in this paper we developed E-C Bandit, which differentiates result examination and content relevance in user clicks and actively learns from such implicit feedback.",
        "We proved that despite the complexity of underlying reward generation assumption and the resulting parameter estimation procedure, the proposed learning algorithm enjoys a sub-linear regret bound.",
        "We only studied click feedback on single items; it is important for us to study it in a more general setting, e.g., a list of ranked items, where sequential result examination and relevance judgment introduce richer inter-dependency.",
        "It is important to study a tighter upper regret bound under our approximated posterior in general"
    ],
    "headline": "Since users\u2019 examination behavior is unobserved, we introduce latent variables to model it",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Yasin Abbasi-yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. In NIPS, pages 2312\u20132320. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-yadkori%2C%20Yasin%20P%C3%A1l%2C%20D%C3%A1vid%20Szepesv%C3%A1ri%2C%20Csaba%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-yadkori%2C%20Yasin%20P%C3%A1l%2C%20D%C3%A1vid%20Szepesv%C3%A1ri%2C%20Csaba%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011"
        },
        {
            "id": "2",
            "entry": "[2] Marc Abeille, Alessandro Lazaric, et al. Linear thompson sampling revisited. Electronic Journal of Statistics, 11(2):5165\u20135197, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abeille%2C%20Marc%20Lazaric%2C%20Alessandro%20Linear%20thompson%20sampling%20revisited%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abeille%2C%20Marc%20Lazaric%2C%20Alessandro%20Linear%20thompson%20sampling%20revisited%202017"
        },
        {
            "id": "3",
            "entry": "[3] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In International Conference on Machine Learning, pages 127\u2013135, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Thompson%20sampling%20for%20contextual%20bandits%20with%20linear%20payoffs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Thompson%20sampling%20for%20contextual%20bandits%20with%20linear%20payoffs%202013"
        },
        {
            "id": "4",
            "entry": "[4] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3:397\u2013422, 2002. URL http://www.jmlr.org/papers/v3/auer02a.html.",
            "url": "http://www.jmlr.org/papers/v3/auer02a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Using%20confidence%20bounds%20for%20exploitation-exploration%20trade-offs%202002"
        },
        {
            "id": "5",
            "entry": "[5] Djallel Bouneffouf, Amel Bouzeghoub, and Alda Lopes Gan\u00e7arski. A contextual-bandit algorithm for mobile context-aware recommender system. In Neural Information Processing, pages 324\u2013331. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bouneffouf%2C%20Djallel%20Bouzeghoub%2C%20Amel%20Gan%C3%A7arski%2C%20Alda%20Lopes%20A%20contextual-bandit%20algorithm%20for%20mobile%20context-aware%20recommender%20system%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bouneffouf%2C%20Djallel%20Bouzeghoub%2C%20Amel%20Gan%C3%A7arski%2C%20Alda%20Lopes%20A%20contextual-bandit%20algorithm%20for%20mobile%20context-aware%20recommender%20system%202012"
        },
        {
            "id": "6",
            "entry": "[6] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pages 2249\u20132257, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Li%2C%20Lihong%20An%20empirical%20evaluation%20of%20thompson%20sampling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Li%2C%20Lihong%20An%20empirical%20evaluation%20of%20thompson%20sampling%202011"
        },
        {
            "id": "7",
            "entry": "[7] Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke. Click models for web search. Synthesis Lectures on Information Concepts, Retrieval, and Services, 7(3):1\u2013115, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chuklin%2C%20Aleksandr%20Markov%2C%20Ilya%20de%20Rijke%2C%20Maarten%20Click%20models%20for%20web%20search%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chuklin%2C%20Aleksandr%20Markov%2C%20Ilya%20de%20Rijke%2C%20Maarten%20Click%20models%20for%20web%20search%202015"
        },
        {
            "id": "8",
            "entry": "[8] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison of click position-bias models. In Proceedings of the 2008 international conference on web search and data mining, pages 87\u201394. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Craswell%2C%20Nick%20Zoeter%2C%20Onno%20Taylor%2C%20Michael%20Ramsey%2C%20Bill%20An%20experimental%20comparison%20of%20click%20position-bias%20models%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Craswell%2C%20Nick%20Zoeter%2C%20Onno%20Taylor%2C%20Michael%20Ramsey%2C%20Bill%20An%20experimental%20comparison%20of%20click%20position-bias%20models%202008"
        },
        {
            "id": "9",
            "entry": "[9] Sarah Filippi, Olivier Cappe, Aur\u00e9lien Garivier, and Csaba Szepesv\u00e1ri. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586\u2013594, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Filippi%2C%20Sarah%20Cappe%2C%20Olivier%20Garivier%2C%20Aur%C3%A9lien%20Szepesv%C3%A1ri%2C%20Csaba%20Parametric%20bandits%3A%20The%20generalized%20linear%20case%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Filippi%2C%20Sarah%20Cappe%2C%20Olivier%20Garivier%2C%20Aur%C3%A9lien%20Szepesv%C3%A1ri%2C%20Csaba%20Parametric%20bandits%3A%20The%20generalized%20linear%20case%202010"
        },
        {
            "id": "10",
            "entry": "[10] Fan Guo, Chao Liu, and Yi Min Wang. Efficient multiple-click models in web search. In Proceedings of the Second ACM International Conference on WSDM, pages 124\u2013131. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Fan%20Liu%2C%20Chao%20Wang%2C%20Yi%20Min%20Efficient%20multiple-click%20models%20in%20web%20search%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Fan%20Liu%2C%20Chao%20Wang%2C%20Yi%20Min%20Efficient%20multiple-click%20models%20in%20web%20search%202009"
        },
        {
            "id": "11",
            "entry": "[11] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM\u201908. Eighth IEEE International Conference on, pages 263\u2013272.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yifan%20Hu%20Yehuda%20Koren%20and%20Chris%20Volinsky%20Collaborative%20filtering%20for%20implicit%20feedback%20datasets%20In%20Data%20Mining%202008%20ICDM08%20Eighth%20IEEE%20International%20Conference%20on%20pages%20263272",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yifan%20Hu%20Yehuda%20Koren%20and%20Chris%20Volinsky%20Collaborative%20filtering%20for%20implicit%20feedback%20datasets%20In%20Data%20Mining%202008%20ICDM08%20Eighth%20IEEE%20International%20Conference%20on%20pages%20263272"
        },
        {
            "id": "12",
            "entry": "[12] Tommi S Jaakkola and Michael I Jordan. Bayesian parameter estimation via variational methods. Statistics and Computing, 10(1):25\u201337, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaakkola%2C%20Tommi%20S.%20Jordan%2C%20Michael%20I.%20Bayesian%20parameter%20estimation%20via%20variational%20methods%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaakkola%2C%20Tommi%20S.%20Jordan%2C%20Michael%20I.%20Bayesian%20parameter%20estimation%20via%20variational%20methods%202000"
        },
        {
            "id": "13",
            "entry": "[13] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. Accurately interpreting clickthrough data as implicit feedback. In ACM SIGIR Forum, volume 51, pages 4\u201311.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joachims%2C%20Thorsten%20Granka%2C%20Laura%20Pan%2C%20Bing%20Hembrooke%2C%20Helene%20Accurately%20interpreting%20clickthrough%20data%20as%20implicit%20feedback",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joachims%2C%20Thorsten%20Granka%2C%20Laura%20Pan%2C%20Bing%20Hembrooke%2C%20Helene%20Accurately%20interpreting%20clickthrough%20data%20as%20implicit%20feedback"
        },
        {
            "id": "14",
            "entry": "[14] Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, and Zheng Wen. Dcm bandits: Learning to rank with multiple clicks. In International Conference on Machine Learning, pages 1215\u2013 1224, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katariya%2C%20Sumeet%20Kveton%2C%20Branislav%20Szepesvari%2C%20Csaba%20Wen%2C%20Zheng%20Dcm%20bandits%3A%20Learning%20to%20rank%20with%20multiple%20clicks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katariya%2C%20Sumeet%20Kveton%2C%20Branislav%20Szepesvari%2C%20Csaba%20Wen%2C%20Zheng%20Dcm%20bandits%3A%20Learning%20to%20rank%20with%20multiple%20clicks%202016"
        },
        {
            "id": "15",
            "entry": "[15] Jaya Kawale, Hung H Bui, Branislav Kveton, Long Tran-Thanh, and Sanjay Chawla. Efficient thompson sampling for online matrix-factorization recommendation. In NIPS, pages 1297\u20131305, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawale%2C%20Jaya%20Bui%2C%20Hung%20H.%20Kveton%2C%20Branislav%20Tran-Thanh%2C%20Long%20Efficient%20thompson%20sampling%20for%20online%20matrix-factorization%20recommendation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawale%2C%20Jaya%20Bui%2C%20Hung%20H.%20Kveton%2C%20Branislav%20Tran-Thanh%2C%20Long%20Efficient%20thompson%20sampling%20for%20online%20matrix-factorization%20recommendation%202015"
        },
        {
            "id": "16",
            "entry": "[16] Diane Kelly and Jaime Teevan. Implicit feedback for inferring user preference: a bibliography. In Acm Sigir Forum, volume 37, pages 18\u201328. ACM, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelly%2C%20Diane%20Teevan%2C%20Jaime%20Implicit%20feedback%20for%20inferring%20user%20preference%3A%20a%20bibliography%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kelly%2C%20Diane%20Teevan%2C%20Jaime%20Implicit%20feedback%20for%20inferring%20user%20preference%3A%20a%20bibliography%202003"
        },
        {
            "id": "17",
            "entry": "[17] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In International Conference on Machine Learning, pages 767\u2013776, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kveton%2C%20Branislav%20Szepesvari%2C%20Csaba%20Wen%2C%20Zheng%20Ashkan%2C%20Azin%20Cascading%20bandits%3A%20Learning%20to%20rank%20in%20the%20cascade%20model%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kveton%2C%20Branislav%20Szepesvari%2C%20Csaba%20Wen%2C%20Zheng%20Ashkan%2C%20Azin%20Cascading%20bandits%3A%20Learning%20to%20rank%20in%20the%20cascade%20model%202015"
        },
        {
            "id": "18",
            "entry": "[18] Paul Lagr\u00e9e, Claire Vernade, and Olivier Cappe. Multiple-play bandits in the position-based model. In Advances in Neural Information Processing Systems, pages 1597\u20131605, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lagr%C3%A9e%2C%20Paul%20Vernade%2C%20Claire%20Cappe%2C%20Olivier%20Multiple-play%20bandits%20in%20the%20position-based%20model%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lagr%C3%A9e%2C%20Paul%20Vernade%2C%20Claire%20Cappe%2C%20Olivier%20Multiple-play%20bandits%20in%20the%20position-based%20model%202016"
        },
        {
            "id": "19",
            "entry": "[19] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In NIPS, pages 817\u2013824, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Zhang%2C%20Tong%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Zhang%2C%20Tong%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008"
        },
        {
            "id": "20",
            "entry": "[20] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Schapire%2C%20Robert%20E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Schapire%2C%20Robert%20E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010"
        },
        {
            "id": "21",
            "entry": "[21] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 297\u2013306. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Wang%2C%20Xuanhui%20Unbiased%20offline%20evaluation%20of%20contextual-bandit-based%20news%20article%20recommendation%20algorithms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Wang%2C%20Xuanhui%20Unbiased%20offline%20evaluation%20of%20contextual-bandit-based%20news%20article%20recommendation%20algorithms%202011"
        },
        {
            "id": "22",
            "entry": "[22] Wei Li, Xuerui Wang, Ruofei Zhang, Ying Cui, Jianchang Mao, and Rong Jin. Exploitation and exploration in a performance based contextual advertising system. In Proceedings of 16th SIGKDD, pages 27\u201336. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Wei%20Wang%2C%20Xuerui%20Zhang%2C%20Ruofei%20Cui%2C%20Ying%20Exploitation%20and%20exploration%20in%20a%20performance%20based%20contextual%20advertising%20system%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Wei%20Wang%2C%20Xuerui%20Zhang%2C%20Ruofei%20Cui%2C%20Ying%20Exploitation%20and%20exploration%20in%20a%20performance%20based%20contextual%20advertising%20system%202010"
        },
        {
            "id": "23",
            "entry": "[23] Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In International Conference on Machine Learning, pages 136\u2013144, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maillard%2C%20Odalric-Ambrym%20Mannor%2C%20Shie%20Latent%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maillard%2C%20Odalric-Ambrym%20Mannor%2C%20Shie%20Latent%20bandits%202014"
        },
        {
            "id": "24",
            "entry": "[24] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221\u20131243, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20Learning%20to%20optimize%20via%20posterior%20sampling%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20Learning%20to%20optimize%20via%20posterior%20sampling%202014"
        },
        {
            "id": "25",
            "entry": "[25] Huazheng Wang, Qingyun Wu, and Hongning Wang. Learning hidden features for contextual bandits. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 1633\u20131642. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Huazheng%20Wu%2C%20Qingyun%20Wang%2C%20Hongning%20Learning%20hidden%20features%20for%20contextual%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Huazheng%20Wu%2C%20Qingyun%20Wang%2C%20Hongning%20Learning%20hidden%20features%20for%20contextual%20bandits%202016"
        },
        {
            "id": "26",
            "entry": "[26] Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversified retrieval. In NIPS, pages 2483\u20132491, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yue%2C%20Yisong%20Guestrin%2C%20Carlos%20Linear%20submodular%20bandits%20and%20their%20application%20to%20diversified%20retrieval%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yue%2C%20Yisong%20Guestrin%2C%20Carlos%20Linear%20submodular%20bandits%20and%20their%20application%20to%20diversified%20retrieval%202011"
        },
        {
            "id": "27",
            "entry": "[27] Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton, Csaba Szepesvari, and Zheng Wen. Online learning to rank in stochastic click models. In International Conference on Machine Learning, pages 4199\u20134208, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoghi%2C%20Masrour%20Tunys%2C%20Tomas%20Ghavamzadeh%2C%20Mohammad%20Kveton%2C%20Branislav%20Online%20learning%20to%20rank%20in%20stochastic%20click%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoghi%2C%20Masrour%20Tunys%2C%20Tomas%20Ghavamzadeh%2C%20Mohammad%20Kveton%2C%20Branislav%20Online%20learning%20to%20rank%20in%20stochastic%20click%20models%202017"
        }
    ]
}
