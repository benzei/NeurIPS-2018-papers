{
    "filename": "7623-optimal-subsampling-with-influence-functions.pdf",
    "metadata": {
        "title": "Optimal Subsampling with Influence Functions",
        "author": "Daniel Ting, Eric Brochu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7623-optimal-subsampling-with-influence-functions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Subsampling is a common and often effective method to deal with the computational challenges of large datasets. However, for most statistical models, there is no well-motivated approach for drawing a non-uniform subsample. We show that the concept of an asymptotically linear estimator and the associated influence function leads to asymptotically optimal sampling probabilities for a wide class of popular models. This is the only tight optimality result for subsampling we are aware of as other methods only provide probabilistic error bounds or optimal rates. We also show that these optimal weights can differ depending on whether the task is parameter estimation or prediction. Furthermore, for linear regression models, which have well-studied procedures for non-uniform subsampling, we empirically show our optimal influence function based method outperforms previous approaches even when using approximations to the optimal probabilities."
    },
    "keywords": [
        {
            "term": "generalized linear models",
            "url": "https://en.wikipedia.org/wiki/generalized_linear_models"
        },
        {
            "term": "logistic regression",
            "url": "https://en.wikipedia.org/wiki/logistic_regression"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "robust linear regression",
            "url": "https://en.wikipedia.org/wiki/robust_linear_regression"
        },
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "regression model",
            "url": "https://en.wikipedia.org/wiki/regression_model"
        },
        {
            "term": "influence function",
            "url": "https://en.wikipedia.org/wiki/influence_function"
        }
    ],
    "highlights": [
        "As the amount of data increases, the question arises as to how best to deal with the large datasets",
        "Our work shows that the influence function can be used to improve the sensitivity measure used in many coreset designs",
        "[<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] defines a sensitivity metric on the log-likelihood for logistic regression while our work shows that choice is inefficient, but that appropriately scaled derivatives of the log-likelihood define an asymptotically optimal metric",
        "We examine the influence functions for linear least squares regression, logistic regression and generalized linear models, and quantile regression",
        "We demonstrate both theoretically and empirically that influence functions yield good and principled subsampling procedures, achieving the best possible asymptotic variance over all independent, without replacement sampling designs with the same size and regularization",
        "We address computational difficulties with influence based methods and show fast approximations perform well"
    ],
    "key_statements": [
        "As the amount of data increases, the question arises as to how best to deal with the large datasets",
        "Our work shows that the influence function can be used to improve the sensitivity measure used in many coreset designs",
        "[<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] defines a sensitivity metric on the log-likelihood for logistic regression while our work shows that choice is inefficient, but that appropriately scaled derivatives of the log-likelihood define an asymptotically optimal metric",
        "A proportional to size design with size equal to the norm of the influence function is optimal for minimizing the trace of the asymptotic variance",
        "We examine the influence functions for linear least squares regression, logistic regression and generalized linear models, and quantile regression",
        "To implement our method practically, we must address three main issues: the fact that the influence function depends on the unknown true parameter \u03b8; the problem that very small sampling probabilities can lead to high variance; and the computational expense of exactly computing the influence function",
        "Let \u03c0n be estimated regularized inclusion probabilities for a sample of expected size nsub based on proportional to size sampling with size measure equal to the norm of the estimated influence function",
        "We demonstrate both theoretically and empirically that influence functions yield good and principled subsampling procedures, achieving the best possible asymptotic variance over all independent, without replacement sampling designs with the same size and regularization",
        "We address computational difficulties with influence based methods and show fast approximations perform well"
    ],
    "summary": [
        "As the amount of data increases, the question arises as to how best to deal with the large datasets.",
        "Given a model and objective, derive the influence function \u03c8\u03b8 or an approximation of it Compute a pilot estimate of the parameters \u03b8.",
        "The gradient-based approach of [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] for linear models and local case-control sampling [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] for logistic models generate sampling probabilities based on the residuals given a pilot estimate of the coefficients.",
        "The theory of asymptotically linear estimators addresses the problem of mapping the quantity of interest to optimal sampling probabilities in two ways.",
        "A PPS design with size equal to the norm of the influence function is optimal for minimizing the trace of the asymptotic variance.",
        "While asymptotically linear estimators encompass a wider class of estimators, we focus on this particular subset as we can demonstrate they can be computed efficiently and they share some structure that allows us to understand the problem by decomposing the influence into informative residual and design components.",
        "In the classical statistical setting where the experimenter does not have knowledge of the response Yi while setting the regression design X, a sensible measure of influence takes the expectation\u221aover the unknown Yi. In this case, one obtains a sampling weight proportional to the root leverage Hii. If only the influence on the single prediction yi is considered, leverage score based sampling is recovered.",
        "To implement our method practically, we must address three main issues: the fact that the influence function depends on the unknown true parameter \u03b8; the problem that very small sampling probabilities can lead to high variance; and the computational expense of exactly computing the influence function.",
        "Theorem 2 shows that simple consistency of the pilot is sufficient to ensure asymptotically optimal estimation of regularized sampling probabilities.",
        "Our main theoretical result is the optimality of influence based sampling weights and derivation of its asymptotic variance.",
        "Let \u03c0n be estimated regularized inclusion probabilities for a sample of expected size nsub based on PPS sampling with size measure equal to the norm of the estimated influence function.",
        "We compared our sampling procedures with existing methods on three real datasets for least squares and quantile regression models.",
        "We use a uniform random sample with size equal to the size of the smallest subsample considered to derive a pilot estimate and drew a weighted subsample from the remainder.",
        "We demonstrate both theoretically and empirically that influence functions yield good and principled subsampling procedures, achieving the best possible asymptotic variance over all independent, without replacement sampling designs with the same size and regularization."
    ],
    "headline": "We show that the concept of an asymptotically linear estimator and the associated influence function leads to asymptotically optimal sampling probabilities for a wide class of popular models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] O. Bachem, M. Lucic, and A. Krause. Practical coreset constructions for machine learning. arXiv preprint arXiv:1703.06476, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06476"
        },
        {
            "id": "2",
            "entry": "[2] A. Bordes, L. Bottou, and P. Gallinari. SGD-QN: Careful quasi-Newton stochastic gradient descent. Journal of Machine Learning Research, 10:1737\u20131754, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bordes%2C%20A.%20Bottou%2C%20L.%20Gallinari%2C%20P.%20SGD-QN%3A%20Careful%20quasi-Newton%20stochastic%20gradient%20descent%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bordes%2C%20A.%20Bottou%2C%20L.%20Gallinari%2C%20P.%20SGD-QN%3A%20Careful%20quasi-Newton%20stochastic%20gradient%20descent%202009"
        },
        {
            "id": "3",
            "entry": "[3] L. Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT. 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010"
        },
        {
            "id": "4",
            "entry": "[4] K. Clarkson, P. Drineas, M. Magdon-Ismail, M. Mahoney, X. Meng, and D. Woodruff. The fast Cauchy transform and faster robust linear regression. SIAM Journal on Computing, 45(3):763\u2013810, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.%20Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.%20The%20fast%20Cauchy%20transform%20and%20faster%20robust%20linear%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.%20Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.%20The%20fast%20Cauchy%20transform%20and%20faster%20robust%20linear%20regression%202016"
        },
        {
            "id": "5",
            "entry": "[5] K. L. Clarkson. Subgradient and sampling algorithms for 1 regression. In SODA, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.L.%20Subgradient%20and%20sampling%20algorithms%20for%201%20regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.L.%20Subgradient%20and%20sampling%20algorithms%20for%201%20regression%202005"
        },
        {
            "id": "6",
            "entry": "[6] E. Cohen. Multi-objective weighted sampling. In HotWeb, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20E.%20Multi-objective%20weighted%20sampling%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20E.%20Multi-objective%20weighted%20sampling%202015"
        },
        {
            "id": "7",
            "entry": "[7] E. Cohen and H. Kaplan. What you can do with coordinated samples. In RANDOM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20E.%20Kaplan%2C%20H.%20What%20you%20can%20do%20with%20coordinated%20samples%202013"
        },
        {
            "id": "8",
            "entry": "[8] A. Dasgupta, P. Drineas, B. Harb, R. Kumar, and M. Mahoney. Sampling algorithms and coresets for p regression. SIAM Journal on Computing, 38(5):2060\u20132078, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20A.%20Drineas%2C%20P.%20Harb%2C%20B.%20Kumar%2C%20R.%20Sampling%20algorithms%20and%20coresets%20for%20p%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20A.%20Drineas%2C%20P.%20Harb%2C%20B.%20Kumar%2C%20R.%20Sampling%20algorithms%20and%20coresets%20for%20p%20regression%202009"
        },
        {
            "id": "9",
            "entry": "[9] P. Dhillon, Y. Lu, D. Foster, and L. Ungar. New subsampling algorithms for fast least squares regression. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhillon%2C%20P.%20Lu%2C%20Y.%20Foster%2C%20D.%20Ungar%2C%20L.%20New%20subsampling%20algorithms%20for%20fast%20least%20squares%20regression%202013"
        },
        {
            "id": "10",
            "entry": "[10] P. Drineas, M. Magdon-Ismail, M. Mahoney, and D. Woodruff. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13:3475\u20133506, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.%20Woodruff%2C%20D.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20P.%20Magdon-Ismail%2C%20M.%20Mahoney%2C%20M.%20Woodruff%2C%20D.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012"
        },
        {
            "id": "11",
            "entry": "[11] W. DuMouchel, C. Volinsky, T. Johnson, C. Cortes, and D. Pregibon. Squashing flat files flatter. In KDD, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DuMouchel%2C%20W.%20Volinsky%2C%20C.%20Johnson%2C%20T.%20Cortes%2C%20C.%20Squashing%20flat%20files%20flatter%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=DuMouchel%2C%20W.%20Volinsky%2C%20C.%20Johnson%2C%20T.%20Cortes%2C%20C.%20Squashing%20flat%20files%20flatter%201999"
        },
        {
            "id": "12",
            "entry": "[12] U.S. EPA. U.S. EPA risk-screening environmental indicators (RSEI) chemical weights, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%2C%20U.S.E.P.A.U.%20EPA%20risk-screening%20environmental%20indicators%20%28RSEI%29%20chemical%20weights%202015"
        },
        {
            "id": "13",
            "entry": "[13] W. Fithian and T. Hastie. Local case-control sampling: Efficient subsampling in imbalanced data sets. Annals of Statistics, 42(5):1693, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fithian%2C%20W.%20Hastie%2C%20T.%20Local%20case-control%20sampling%3A%20Efficient%20subsampling%20in%20imbalanced%20data%20sets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fithian%2C%20W.%20Hastie%2C%20T.%20Local%20case-control%20sampling%3A%20Efficient%20subsampling%20in%20imbalanced%20data%20sets%202014"
        },
        {
            "id": "14",
            "entry": "[14] Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383\u2013393, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hampel%2C%20Frank%20R.%20The%20influence%20curve%20and%20its%20role%20in%20robust%20estimation%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hampel%2C%20Frank%20R.%20The%20influence%20curve%20and%20its%20role%20in%20robust%20estimation%201974"
        },
        {
            "id": "15",
            "entry": "[15] J. Huggins, T. Campbell, and T. Broderick. Coresets for scalable Bayesian logistic regression. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huggins%2C%20J.%20Campbell%2C%20T.%20Broderick%2C%20T.%20Coresets%20for%20scalable%20Bayesian%20logistic%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huggins%2C%20J.%20Campbell%2C%20T.%20Broderick%2C%20T.%20Coresets%20for%20scalable%20Bayesian%20logistic%20regression%202016"
        },
        {
            "id": "16",
            "entry": "[16] R. Koenker. Quantile Regression. Cambridge university press, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koenker%2C%20R.%20Quantile%20Regression%202005"
        },
        {
            "id": "17",
            "entry": "[17] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017"
        },
        {
            "id": "18",
            "entry": "[18] M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.",
            "url": "http://archive.ics.uci.edu/ml"
        },
        {
            "id": "19",
            "entry": "[19] P. Ma, M. Mahoney, and B. Yu. A statistical perspective on algorithmic leveraging. Journal of Machine Learning Research, 16:861\u2013911, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20P.%20Mahoney%2C%20M.%20Yu%2C%20B.%20A%20statistical%20perspective%20on%20algorithmic%20leveraging%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20P.%20Mahoney%2C%20M.%20Yu%2C%20B.%20A%20statistical%20perspective%20on%20algorithmic%20leveraging%202015"
        },
        {
            "id": "20",
            "entry": "[20] D. Madigan, N. Raghavan, W. Dumouchel, M. Nason, C. Posse, and G. Ridgeway. Likelihood-based data squashing: A modeling approach to instance construction. Data Mining and Knowledge Discovery, 6(2): 173\u2013190, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madigan%2C%20D.%20Raghavan%2C%20N.%20Dumouchel%2C%20W.%20Nason%2C%20M.%20Likelihood-based%20data%20squashing%3A%20A%20modeling%20approach%20to%20instance%20construction%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madigan%2C%20D.%20Raghavan%2C%20N.%20Dumouchel%2C%20W.%20Nason%2C%20M.%20Likelihood-based%20data%20squashing%3A%20A%20modeling%20approach%20to%20instance%20construction%202002"
        },
        {
            "id": "21",
            "entry": "[21] X. Meng and M. Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In STOC, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20X.%20Mahoney%2C%20M.%20Low-distortion%20subspace%20embeddings%20in%20input-sparsity%20time%20and%20applications%20to%20robust%20linear%20regression%202013"
        },
        {
            "id": "22",
            "entry": "[22] X. Meng, M. Saunders, and M. Mahoney. LSRN: A parallel iterative solver for strongly over-or underdetermined systems. SIAM Journal on Scientific Computing, 36(2):C95\u2013C118, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20X.%20Saunders%2C%20M.%20Mahoney%2C%20M.%20LSRN%3A%20A%20parallel%20iterative%20solver%20for%20strongly%20over-or%20underdetermined%20systems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20X.%20Saunders%2C%20M.%20Mahoney%2C%20M.%20LSRN%3A%20A%20parallel%20iterative%20solver%20for%20strongly%20over-or%20underdetermined%20systems%202014"
        },
        {
            "id": "23",
            "entry": "[23] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, W. Paul, M. I. Jordan, and I. Stoica. Ray: A distributed framework for emerging ai applications. arXiv preprint arXiv:1712.05889, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05889"
        },
        {
            "id": "24",
            "entry": "[24] Garvesh Raskutti and Michael W Mahoney. A statistical perspective on randomized sketching for ordinary least-squares. The Journal of Machine Learning Research, 17(1):7508\u20137538, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Garvesh%20Raskutti%20and%20Michael%20W%20Mahoney.%20A%20statistical%20perspective%20on%20randomized%20sketching%20for%20ordinary%20least-squares%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Garvesh%20Raskutti%20and%20Michael%20W%20Mahoney.%20A%20statistical%20perspective%20on%20randomized%20sketching%20for%20ordinary%20least-squares%202016"
        },
        {
            "id": "25",
            "entry": "[25] D. Ting. Adaptive threshold sampling and estimation. arXiv preprint arXiv:1708.04970, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04970"
        },
        {
            "id": "26",
            "entry": "[26] A.W. van der Vaart. Asymptotic Statistics. Cambridge University Press, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Vaart%2C%20A.W.%20Asymptotic%20Statistics%202000"
        },
        {
            "id": "27",
            "entry": "[27] HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. Journal of the American Statistical Association, 113(522):829\u2013844, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20HaiYing%20Zhu%2C%20Rong%20Ma%2C%20Ping%20Optimal%20subsampling%20for%20large%20sample%20logistic%20regression%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20HaiYing%20Zhu%2C%20Rong%20Ma%2C%20Ping%20Optimal%20subsampling%20for%20large%20sample%20logistic%20regression%202018"
        },
        {
            "id": "28",
            "entry": "[28] M. Zaharia, M. Chowdhury, M. Franklin, S. Shenker, and I. Stoica. Spark: Cluster computing with working sets. In HotCloud, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaharia%2C%20M.%20Chowdhury%2C%20M.%20Franklin%2C%20M.%20Shenker%2C%20S.%20Spark%3A%20Cluster%20computing%20with%20working%20sets%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaharia%2C%20M.%20Chowdhury%2C%20M.%20Franklin%2C%20M.%20Shenker%2C%20S.%20Spark%3A%20Cluster%20computing%20with%20working%20sets%202010"
        },
        {
            "id": "29",
            "entry": "[29] R. Zhu. Gradient-based sampling: An adaptive importance sampling for least-squares. In NIPS, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20R.%20Gradient-based%20sampling%3A%20An%20adaptive%20importance%20sampling%20for%20least-squares%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20R.%20Gradient-based%20sampling%3A%20An%20adaptive%20importance%20sampling%20for%20least-squares%202016"
        }
    ]
}
