{
    "filename": "7526-online-learning-with-an-unknown-fairness-metric.pdf",
    "metadata": {
        "title": "Online Learning with an Unknown Fairness Metric",
        "author": "Stephen Gillen, Christopher Jung, Michael Kearns, Aaron Roth",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7526-online-learning-with-an-unknown-fairness-metric.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability [<a class=\"ref-link\" id=\"cDwork_et+al_2012_a\" href=\"#rDwork_et+al_2012_a\">Dwork et al., 2012</a>], which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who \u201cknows unfairness when he sees it\u201d but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairnes\u221as violations that depends only logarithmically on T , while obtaining an optimal O( T ) regret bound to the best fair policy."
    },
    "keywords": [
        {
            "term": "online learning",
            "url": "https://en.wikipedia.org/wiki/online_learning"
        },
        {
            "term": "decision making",
            "url": "https://en.wikipedia.org/wiki/decision_making"
        }
    ],
    "highlights": [
        "The last several years have seen an explosion of work studying the problem of fairness in machine learning",
        "The literature can be divided into two families of fairness definitions: those aiming at group fairness, and those aiming at individual fairness",
        "Group fairness definitions are aggegrate in nature: they partition individuals into some collection of protected groups, specify some statistic of interest, and require that a learning algorithm equalize this quantity across the protected groups",
        "Individual fairness definitions ask for some constraint that binds on the individual level, rather than only over averages of people",
        "We study the standard linear contextual bandit setting",
        "The interesting question is: how much can we further relax the information about the fairness metric available to the algorithm? For instance, what if the fairness feedback is only partial, identifying some but not all fairness violations? What if it only indicates whether or not there were any violations, but does not identify them? What if the feedback is not guaranteed to be exactly consistent with any metric? Or what if the feedback is consistent with some distance function, but not one in a known class: for example, what if the distance is not exactly Mahalanobis, but is approximately so? In general, it is very interesting to continue to push to close the wide gap between the study of individual fairness notions and the study of group fairness notions"
    ],
    "key_statements": [
        "The last several years have seen an explosion of work studying the problem of fairness in machine learning",
        "The literature can be divided into two families of fairness definitions: those aiming at group fairness, and those aiming at individual fairness",
        "Group fairness definitions are aggegrate in nature: they partition individuals into some collection of protected groups, specify some statistic of interest, and require that a learning algorithm equalize this quantity across the protected groups",
        "Individual fairness definitions ask for some constraint that binds on the individual level, rather than only over averages of people",
        "We investigate the extent to which it is possible to satisfy this fairness constraint while simultaneously solving an online learning problem, when the underlying fairness metric is Mahalanobis but not known to the learning algorithm, and may be in tension with the learning problem",
        "We study the standard linear contextual bandit setting",
        "The closest paper to our work focuses on online learning of Mahalanobis distances (<a class=\"ref-link\" id=\"cJain_et+al_2009_a\" href=\"#rJain_et+al_2009_a\">Jain et al [2009</a>])",
        "We study algorithms that operate in the linear contextual bandits setting",
        "We show via a reduction to an online learning algorithm of <a class=\"ref-link\" id=\"cLobel_et+al_2017_a\" href=\"#rLobel_et+al_2017_a\">Lobel et al [2017</a>], how to simultaneously obtain a logarithmic regret bound and a logarithmic number of fairness violations",
        "The interesting question is: how much can we further relax the information about the fairness metric available to the algorithm? For instance, what if the fairness feedback is only partial, identifying some but not all fairness violations? What if it only indicates whether or not there were any violations, but does not identify them? What if the feedback is not guaranteed to be exactly consistent with any metric? Or what if the feedback is consistent with some distance function, but not one in a known class: for example, what if the distance is not exactly Mahalanobis, but is approximately so? In general, it is very interesting to continue to push to close the wide gap between the study of individual fairness notions and the study of group fairness notions"
    ],
    "summary": [
        "The last several years have seen an explosion of work studying the problem of fairness in machine learning.",
        "After the learner makes its decisions according to some probability distribution \u03c0t at round t, it receives feedback specifying for which pairs of contexts the fairness constraint was violated.",
        "We show how to obtain a bound on the number of fairness violations using a linear-programming based reduction to a recent algorithm which has a mistake bound for learning a linear function with a particularly weak form of feedback <a class=\"ref-link\" id=\"cLobel_et+al_2017_a\" href=\"#rLobel_et+al_2017_a\"><a class=\"ref-link\" id=\"cLobel_et+al_2017_a\" href=\"#rLobel_et+al_2017_a\">Lobel et al [2017</a></a>].",
        "<a class=\"ref-link\" id=\"cJoseph_et+al_2016_a\" href=\"#rJoseph_et+al_2016_a\">Joseph et al [2016a</a>] study a notion of \u201cmeritocratic\u201d fairness in the contextual bandit setting, and prove upper and lower bounds on the regret achievable by algorithms that must be \u201cfair\u201d at every round.",
        "In our paper, the main objective of the learning algorithm is orthogonal to the metric learning problem \u2014 i.e. to minimize regret in the linear contextual bandit problem, but while simultaneously learning and obeying a fairness constraint, and only from weak feedback noting violations of fairness.",
        "Algorithm L is Lipschitz-fair on round t with respect to distance function d if for all pairs of individuals i, j: |\u03c0it \u2212 \u03c0jt| \u2264 d.",
        "Given a distance function d and a history hT +1, the -fairness loss of an algorithm L is the total number of pairs on which it is -unfair: FairnessLoss(L, hT +1, ) =",
        "Observe that \u03c0 corresponds to the distribution over actions that maximizes expected reward at round t, subject to satisfying the fairness constraints \u2014 i.e. the distribution that an optimal player, with advance knowledge of \u03b8 would play, if he were not allowed to violate the fairness constraints at all.",
        "We consider an easier case of the problem in which the linear objective function \u03b8 is known to the algorithm, and the distance function d is all that is unknown.",
        "We show via a reduction to an online learning algorithm of <a class=\"ref-link\" id=\"cLobel_et+al_2017_a\" href=\"#rLobel_et+al_2017_a\"><a class=\"ref-link\" id=\"cLobel_et+al_2017_a\" href=\"#rLobel_et+al_2017_a\">Lobel et al [2017</a></a>], how to simultaneously obtain a logarithmic regret bound and a logarithmic number of fairness violations.",
        "Our algorithm will play at each round t the distribution \u03c0 that results from solving the linear program LP, where dt is a \u201cguess\u201d for the pairwise distances between each context dt.",
        "1. In any round in which we are -unfair for some pair of contexts xti and xtj, it must be that dit,j \u2265 d+ , and so we can always update the (i, j)th copy of DistanceEstimator and charge our fairness loss to its mistake bound.",
        "When can we obtain the strong semantics of individual fairness without making correspondingly strong assumptions?"
    ],
    "headline": "We investigate the extent to which it is possible to satisfy this fairness constraint while simultaneously solving an online learning problem, when the underlying fairness metric is Mahalanobis but not known to the learning algorithm, and may be in tension with the learning problem",
    "reference_links": [
        {
            "id": "Abbasi-Yadkori_et+al_2011_a",
            "entry": "Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain., pages 2312\u20132320, 2011. URL http://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits.",
            "url": "http://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Yasin%20P%C3%A1l%2C%20D%C3%A1vid%20Szepesv%C3%A1ri%2C%20Csaba%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011-12"
        },
        {
            "id": "Berk_et+al_2017_a",
            "entry": "Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. arXiv preprint arXiv:1703.09207, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09207"
        },
        {
            "id": "Celis_et+al_2018_a",
            "entry": "L Elisa Celis, Sayash Kapoor, Farnood Salehi, and Nisheeth K Vishnoi. An algorithmic framework to control bias in bandit-based personalization. arXiv preprint arXiv:1802.08674, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08674"
        },
        {
            "id": "Chouldechova_2017_a",
            "entry": "Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. arXiv preprint arXiv:1703.00056, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00056"
        },
        {
            "id": "Dwork_et+al_2012_a",
            "entry": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214\u2013226. ACM, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "Friedler_et+al_2016_a",
            "entry": "Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility of fairness. arXiv preprint arXiv:1609.07236, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07236"
        },
        {
            "id": "Gillen_et+al_2018_a",
            "entry": "Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online learning with an unknown fairness metric. arXiv preprint arXiv:1802.06936, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06936"
        },
        {
            "id": "Hajian_2013_a",
            "entry": "Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination prevention in data mining. IEEE transactions on knowledge and data engineering, 25(7):1445\u2013 1459, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hajian%2C%20Sara%20Domingo-Ferrer%2C%20Josep%20A%20methodology%20for%20direct%20and%20indirect%20discrimination%20prevention%20in%20data%20mining%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hajian%2C%20Sara%20Domingo-Ferrer%2C%20Josep%20A%20methodology%20for%20direct%20and%20indirect%20discrimination%20prevention%20in%20data%20mining%202013"
        },
        {
            "id": "Hardt_et+al_2016_a",
            "entry": "Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nathan%20Equality%20of%20opportunity%20in%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nathan%20Equality%20of%20opportunity%20in%20supervised%20learning%202016"
        },
        {
            "id": "H_et+al_2017_a",
            "entry": "Ursula H\u00e9bert-Johnson, Michael P Kim, Omer Reingold, and Guy N Rothblum. Calibration for the (computationally-identifiable) masses. arXiv preprint arXiv:1711.08513, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.08513"
        },
        {
            "id": "Jabbari_et+al_2017_a",
            "entry": "Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in reinforcement learning. In International Conference on Machine Learning, pages 1617\u20131626, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shahin%20Jabbari%20Matthew%20Joseph%20Michael%20Kearns%20Jamie%20Morgenstern%20and%20Aaron%20Roth%20Fairness%20in%20reinforcement%20learning%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2016171626%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shahin%20Jabbari%20Matthew%20Joseph%20Michael%20Kearns%20Jamie%20Morgenstern%20and%20Aaron%20Roth%20Fairness%20in%20reinforcement%20learning%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2016171626%202017"
        },
        {
            "id": "Jain_et+al_2009_a",
            "entry": "Prateek Jain, Brian Kulis, Inderjit S Dhillon, and Kristen Grauman. Online metric learning and fast similarity search. In Advances in neural information processing systems, pages 761\u2013768, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Prateek%20Kulis%2C%20Brian%20Dhillon%2C%20Inderjit%20S.%20Grauman%2C%20Kristen%20Online%20metric%20learning%20and%20fast%20similarity%20search%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Prateek%20Kulis%2C%20Brian%20Dhillon%2C%20Inderjit%20S.%20Grauman%2C%20Kristen%20Online%20metric%20learning%20and%20fast%20similarity%20search%202009"
        },
        {
            "id": "Joseph_et+al_0000_a",
            "entry": "Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. pages 325\u2013333, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits"
        },
        {
            "id": "Joseph_et+al_2016_a",
            "entry": "Matthew Joseph, Michael J. Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Fair algorithms for infinite and contextual bandits. CoRR, abs/1610.09559, 2016b. URL http://arxiv.org/abs/1610.09559.",
            "url": "http://arxiv.org/abs/1610.09559",
            "arxiv_url": "https://arxiv.org/pdf/1610.09559"
        },
        {
            "id": "Kamiran_2012_a",
            "entry": "Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1\u201333, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202012"
        },
        {
            "id": "Kearns_et+al_2017_a",
            "entry": "Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. arXiv preprint arXiv:1711.05144, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05144"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Michael P Kim, Omer Reingold, and Guy N Rothblum. Fairness through computationally-bounded awareness. arXiv preprint arXiv:1803.03239, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03239"
        },
        {
            "id": "Kleinberg_et+al_2017_a",
            "entry": "Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In Proceedings of the 2017 ACM Conference on Innovations in Theoretical Computer Science, Berkeley, CA, USA, 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20Jon%20Mullainathan%2C%20Sendhil%20Raghavan%2C%20Manish%20Inherent%20trade-offs%20in%20the%20fair%20determination%20of%20risk%20scores%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20Jon%20Mullainathan%2C%20Sendhil%20Raghavan%2C%20Manish%20Inherent%20trade-offs%20in%20the%20fair%20determination%20of%20risk%20scores%202017"
        },
        {
            "id": "Kulis_2013_a",
            "entry": "Brian Kulis et al. Metric learning: A survey. Foundations and Trends R in Machine Learning, 5(4): 287\u2013364, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulis%2C%20Brian%20Metric%20learning%3A%20A%20survey.%20Foundations%20and%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulis%2C%20Brian%20Metric%20learning%3A%20A%20survey.%20Foundations%20and%202013"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C Parkes. Calibrated fairness in bandits. arXiv preprint arXiv:1707.01875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01875"
        },
        {
            "id": "Lobel_et+al_2017_a",
            "entry": "Ilan Lobel, Renato Paes Leme, and Adrian Vladu. Multidimensional binary search for contextual decision-making. In Proceedings of the 2017 ACM Conference on Economics and Computation, EC \u201917, Cambridge, MA, USA, June 26-30, 2017, page 585, 2017. doi: 10.1145/3033274.3085100. URL http://doi.acm.org/10.1145/3033274.3085100.",
            "crossref": "https://dx.doi.org/10.1145/3033274.3085100",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/3033274.3085100"
        },
        {
            "id": "Rothblum_2018_a",
            "entry": "Guy N Rothblum and Gal Yona. Probably approximately metric-fair learning. arXiv preprint arXiv:1803.03242, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03242"
        },
        {
            "id": "Zafar_et+al_2017_a",
            "entry": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pages 1171\u20131180. International World Wide Web Conferences Steering Committee, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20beyond%20disparate%20treatment%20%26%20disparate%20impact%3A%20Learning%20classification%20without%20disparate%20mistreatment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20beyond%20disparate%20treatment%20%26%20disparate%20impact%3A%20Learning%20classification%20without%20disparate%20mistreatment%202017"
        },
        {
            "id": "Zemel_et+al_2013_a",
            "entry": "Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325\u2013333, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zemel%2C%20Rich%20Wu%2C%20Yu%20Swersky%2C%20Kevin%20Pitassi%2C%20Toni%20Learning%20fair%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20Rich%20Wu%2C%20Yu%20Swersky%2C%20Kevin%20Pitassi%2C%20Toni%20Learning%20fair%20representations%202013"
        }
    ]
}
