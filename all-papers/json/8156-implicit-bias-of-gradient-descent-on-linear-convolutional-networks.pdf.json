{
    "filename": "8156-implicit-bias-of-gradient-descent-on-linear-convolutional-networks.pdf",
    "metadata": {
        "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
        "author": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nati Srebro",
        "date": 1997,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8156-implicit-bias-of-gradient-descent-on-linear-convolutional-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We show that gradient descent on full width linear convolutional networks of depth L converges to a linear predictor related to the 2/L bridge penalty in the frequency domain. This is in contrast to fully connected linear networks, where regardless of depth, gradient descent converges to the 2 maximum margin solution."
    },
    "keywords": [
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "frequency domain",
            "url": "https://en.wikipedia.org/wiki/frequency_domain"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "circular convolution",
            "url": "https://en.wikipedia.org/wiki/circular_convolution"
        },
        {
            "term": "implicit bias",
            "url": "https://en.wikipedia.org/wiki/implicit_bias"
        },
        {
            "term": "inductive bias",
            "url": "https://en.wikipedia.org/wiki/inductive_bias"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Implicit biases introduced by optimization algorithms play an crucial role in learning deep neural networks [<a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\">Neyshabur et al, 2015b</a></a>,a, <a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a></a>, <a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\"><a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\">Keskar et al, 2016</a></a>, <a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\"><a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\">Chaudhari et al, 2016</a></a>, <a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\">Dinh et al, 2017</a></a>, <a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\">Andrychowicz et al, 2016</a></a>, <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017</a></a>, <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>, <a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\"><a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\">Wilson et al, 2017</a></a>, <a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\"><a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\">Hoffer et al, 2017</a></a>, <a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\"><a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\">Smith, 2018</a></a>]",
        "Implicit biases introduced by optimization algorithms play an crucial role in learning deep neural networks [<a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\">Neyshabur et al, 2015b</a>,a, <a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a>, <a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\">Keskar et al, 2016</a>, <a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\">Chaudhari et al, 2016</a>, <a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\">Dinh et al, 2017</a>, <a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\">Andrychowicz et al, 2016</a>, <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017</a>, <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a>, <a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\">Wilson et al, 2017</a>, <a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\">Hoffer et al, 2017</a>, <a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\">Smith, 2018</a>]",
        "In over-parameterized models, specially deep neural networks, much, if not most, of the inductive bias of the learned model comes from this implicit regularization from the optimization algorithm",
        "Our main results characterize the implicit bias of gradient descent for multi-layer fully connected and convolutional networks with linear activations",
        "For fully connected networks with single output, Theorem 1 shows that there is no effect of depth on the implicit bias of gradient descent",
        "We showed that even in the case of linear activations and a full width convolution, wherein the convolutional network defines the exact same model class as fully connected networks, merely changing to a convolutional parameterization introduces radically different, and very interesting, bias when training with gradient descent"
    ],
    "key_statements": [
        "Implicit biases introduced by optimization algorithms play an crucial role in learning deep neural networks [<a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\">Neyshabur et al, 2015b</a></a>,a, <a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a></a>, <a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\"><a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\">Keskar et al, 2016</a></a>, <a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\"><a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\">Chaudhari et al, 2016</a></a>, <a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\">Dinh et al, 2017</a></a>, <a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\">Andrychowicz et al, 2016</a></a>, <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017</a></a>, <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>, <a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\"><a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\">Wilson et al, 2017</a></a>, <a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\"><a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\">Hoffer et al, 2017</a></a>, <a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\"><a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\">Smith, 2018</a></a>]",
        "Implicit biases introduced by optimization algorithms play an crucial role in learning deep neural networks [<a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\">Neyshabur et al, 2015b</a>,a, <a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a>, <a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\">Keskar et al, 2016</a>, <a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\">Chaudhari et al, 2016</a>, <a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\">Dinh et al, 2017</a>, <a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\">Andrychowicz et al, 2016</a>, <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017</a>, <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a>, <a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\">Wilson et al, 2017</a>, <a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\">Hoffer et al, 2017</a>, <a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\">Smith, 2018</a>]",
        "In over-parameterized models, specially deep neural networks, much, if not most, of the inductive bias of the learned model comes from this implicit regularization from the optimization algorithm",
        "Our main results characterize the implicit bias of gradient descent for multi-layer fully connected and convolutional networks with linear activations",
        "For fully connected networks with single output, Theorem 1 shows that there is no effect of depth on the implicit bias of gradient descent",
        "We showed that even in the case of linear activations and a full width convolution, wherein the convolutional network defines the exact same model class as fully connected networks, merely changing to a convolutional parameterization introduces radically different, and very interesting, bias when training with gradient descent",
        "For convenience and simplicity of presentation, we studied one dimensional circular convolutions",
        "There are of course many other implicit and explicit sources of inductive bias\u2014here we show that merely parameterizing transformations via convolutions and using gradient descent for training already induces sparsity in the frequency domain"
    ],
    "summary": [
        "Implicit biases introduced by optimization algorithms play an crucial role in learning deep neural networks [<a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2015_b\" href=\"#rNeyshabur_et+al_2015_b\">Neyshabur et al, 2015b</a></a>,a, <a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a></a>, <a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\"><a class=\"ref-link\" id=\"cKeskar_et+al_2016_a\" href=\"#rKeskar_et+al_2016_a\">Keskar et al, 2016</a></a>, <a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\"><a class=\"ref-link\" id=\"cChaudhari_et+al_2016_a\" href=\"#rChaudhari_et+al_2016_a\">Chaudhari et al, 2016</a></a>, <a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2017_a\" href=\"#rDinh_et+al_2017_a\">Dinh et al, 2017</a></a>, <a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\"><a class=\"ref-link\" id=\"cAndrychowicz_et+al_2016_a\" href=\"#rAndrychowicz_et+al_2016_a\">Andrychowicz et al, 2016</a></a>, <a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\"><a class=\"ref-link\" id=\"cNeyshabur_et+al_2017_a\" href=\"#rNeyshabur_et+al_2017_a\">Neyshabur et al, 2017</a></a>, <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\"><a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a></a>, <a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\"><a class=\"ref-link\" id=\"cWilson_et+al_2017_a\" href=\"#rWilson_et+al_2017_a\">Wilson et al, 2017</a></a>, <a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\"><a class=\"ref-link\" id=\"cHoffer_et+al_2017_a\" href=\"#rHoffer_et+al_2017_a\">Hoffer et al, 2017</a></a>, <a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\"><a class=\"ref-link\" id=\"cSmith_2018_a\" href=\"#rSmith_2018_a\">Smith, 2018</a></a>].",
        "We study the implicit bias of optimizing multi-layer fully connected linear networks, and linear convolutional networks using gradient descent.",
        "We show that for fully connected networks with single output, optimizing the exponential loss over linearly separable data using gradient loss again converges to the homogeneous hard margin support vector machine solution.",
        "In this paper we study the behavior of gradient descent on the problem (4) w.r.t different parameterizations of the model class of linear predictors.",
        "Our main results characterize the implicit bias of gradient descent for multi-layer fully connected and convolutional networks with linear activations",
        "For any depth L, almost all linearly separable datasets {xn, yn}nN=1, almost all initializations w(0), and any bounded sequence of step sizes {\u03b7t}t, consider the sequence gradient descent iterates w(t) in eq (7) for minimizing LPfull (w) in eq (4) with exponential loss over L\u2013layer fully connected linear networks.",
        "For fully connected networks with single output, Theorem 1 shows that there is no effect of depth on the implicit bias of gradient descent.",
        "Regardless of the depth of the network, the asymptotic classifier is always the hard margin support vector machine classifier, which is the limit direction of gradient descent for linear logistic regression in the direct parameterization of \u03b2 = w.",
        "For any homogeneous polynomial map P : RP \u2192 RD from parameters w \u2208 RD to linear predictors, almost all datasets {xn, yn}nN=1 separable by B := {P(w) : w \u2208 RP }, almost all initializations w(0), and any bounded sequence of step sizes {\u03b7t}t, consider the sequence of gradient descent updates w(t) from eq (7) for minimizing the empirical risk objective LP (w) in (4) with exponential loss (u, y) = exp(\u2212uy).",
        "Instead of using Theorem 4, for the specific networks in Section 3, we directly show that gradient descent updates converge in direction to a first order stationary point of the problem in eq (15).",
        "We showed that even in the case of linear activations and a full width convolution, wherein the convolutional network defines the exact same model class as fully connected networks, merely changing to a convolutional parameterization introduces radically different, and very interesting, bias when training with gradient descent.",
        "Results for matrix sensing in <a class=\"ref-link\" id=\"cGunasekar_et+al_2018_a\" href=\"#rGunasekar_et+al_2018_a\">Gunasekar et al [2018</a>] imply that for two layer fully connected networks with multiple outputs, the implicit bias is to a maximum margin solution with respect to the nuclear norm \u03b2 ."
    ],
    "headline": "We show that gradient descent on full width linear convolutional networks of depth L converges to a linear predictor related to the 2/L bridge penalty in the frequency domain",
    "reference_links": [
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "Bartlett_2003_a",
            "entry": "P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202003"
        },
        {
            "id": "Burer_2003_a",
            "entry": "Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329\u2013357, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burer%2C%20Samuel%20Monteiro%2C%20Renato%20D.C.%20A%20nonlinear%20programming%20algorithm%20for%20solving%20semidefinite%20programs%20via%20low-rank%20factorization%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burer%2C%20Samuel%20Monteiro%2C%20Renato%20D.C.%20A%20nonlinear%20programming%20algorithm%20for%20solving%20semidefinite%20programs%20via%20low-rank%20factorization%202003"
        },
        {
            "id": "Chaudhari_et+al_2016_a",
            "entry": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01838"
        },
        {
            "id": "Dinh_et+al_2017_a",
            "entry": "Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017"
        },
        {
            "id": "Ge_et+al_2011_a",
            "entry": "Dongdong Ge, Xiaoye Jiang, and Yinyu Ye. A note on the complexity of lp minimization. Mathematical programming, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Dongdong%20Jiang%2C%20Xiaoye%20Ye%2C%20Yinyu%20A%20note%20on%20the%20complexity%20of%20lp%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Dongdong%20Jiang%2C%20Xiaoye%20Ye%2C%20Yinyu%20A%20note%20on%20the%20complexity%20of%20lp%20minimization%202011"
        },
        {
            "id": "Gunasekar_et+al_2017_a",
            "entry": "Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017"
        },
        {
            "id": "Gunasekar_et+al_2018_a",
            "entry": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. arXiv preprint, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20Suriya%20Lee%2C%20Jason%20D.%20Soudry%2C%20Daniel%20Srebro%2C%20Nathan%20Characterizing%20implicit%20bias%20in%20terms%20of%20optimization%20geometry.%20arXiv%20p%202018"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural Computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997"
        },
        {
            "id": "Hoffer_et+al_2017_a",
            "entry": "Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "Ji_2018_a",
            "entry": "Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07300"
        },
        {
            "id": "Journ_et+al_2010_a",
            "entry": "Michel Journ\u00e9e, Francis Bach, P-A Absil, and Rodolphe Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327\u20132351, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Journ%C3%A9e%2C%20Michel%20Francis%20Bach%2C%20P.-A.Absil%20Sepulchre%2C%20Rodolphe%20Low-rank%20optimization%20on%20the%20cone%20of%20positive%20semidefinite%20matrices%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Journ%C3%A9e%2C%20Michel%20Francis%20Bach%2C%20P.-A.Absil%20Sepulchre%2C%20Rodolphe%20Low-rank%20optimization%20on%20the%20cone%20of%20positive%20semidefinite%20matrices%202010"
        },
        {
            "id": "Kakade_et+al_2009_a",
            "entry": "Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Keskar_et+al_2016_a",
            "entry": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202016"
        },
        {
            "id": "Lee_et+al_2016_a",
            "entry": "Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In 29th Annual Conference on Learning Theory, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix recovery. arXiv preprint arXiv:1712.09203, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09203"
        },
        {
            "id": "Nacson_et+al_2018_a",
            "entry": "Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01905"
        },
        {
            "id": "Neyshabur_et+al_2015_a",
            "entry": "Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pages 2422\u20132430, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Behnam%20Neyshabur%20Ruslan%20R%20Salakhutdinov%20and%20Nati%20Srebro%20Pathsgd%20Pathnormalized%20optimization%20in%20deep%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2024222430%202015a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Behnam%20Neyshabur%20Ruslan%20R%20Salakhutdinov%20and%20Nati%20Srebro%20Pathsgd%20Pathnormalized%20optimization%20in%20deep%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2024222430%202015a"
        },
        {
            "id": "Neyshabur_et+al_2015_b",
            "entry": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. In International Conference on Learning Representations, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20In%20search%20of%20the%20real%20inductive%20bias%3A%20On%20the%20role%20of%20implicit%20regularization%20in%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20In%20search%20of%20the%20real%20inductive%20bias%3A%20On%20the%20role%20of%20implicit%20regularization%20in%20deep%20learning%202015"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of optimization and implicit regularization in deep learning. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Salakhutdinov%2C%20Ruslan%20Srebro%2C%20Nathan%20Geometry%20of%20optimization%20and%20implicit%20regularization%20in%20deep%20learning.%20arXiv%20p%202017"
        },
        {
            "id": "Nguyen_2017_a",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08045"
        },
        {
            "id": "Rockafellar_1979_a",
            "entry": "R Tyrrell Rockafellar. Directionally lipschitzian functions and subdifferential calculus. Proceedings of the London Mathematical Society, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.Tyrrell%20Directionally%20lipschitzian%20functions%20and%20subdifferential%20calculus%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rockafellar%2C%20R.Tyrrell%20Directionally%20lipschitzian%20functions%20and%20subdifferential%20calculus%201979"
        },
        {
            "id": "Smith_2018_a",
            "entry": "Le Smith, Kindermans. Don\u2019t Decay the Learning Rate, Increase the Batch Size. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Le%20Kindermans%20Don%E2%80%99t%20Decay%20the%20Learning%20Rate%2C%20Increase%20the%20Batch%20Size%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20Le%20Kindermans%20Don%E2%80%99t%20Decay%20the%20Learning%20Rate%2C%20Increase%20the%20Batch%20Size%202018"
        },
        {
            "id": "Soudry_et+al_2017_a",
            "entry": "Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345"
        },
        {
            "id": "Margins_2013_a",
            "entry": "Matus Telgarsky. Margins, shrinkage and boosting. In Proceedings of the 30th International Conference on International Conference on Machine Learning-Volume 28, pages II\u2013307. JMLR. org, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Margins%2C%20Matus%20Telgarsky%20shrinkage%20and%20boosting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Margins%2C%20Matus%20Telgarsky%20shrinkage%20and%20boosting%202013"
        },
        {
            "id": "Wilson_et+al_2017_a",
            "entry": "Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202017"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        }
    ]
}
