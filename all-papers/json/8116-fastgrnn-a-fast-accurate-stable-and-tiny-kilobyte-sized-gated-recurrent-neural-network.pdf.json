{
    "filename": "8116-fastgrnn-a-fast-accurate-stable-and-tiny-kilobyte-sized-gated-recurrent-neural-network.pdf",
    "metadata": {
        "date": 2018,
        "title": "FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network",
        "author": "Aditya Kusupati\u2020, Manish Singh\u00a7, Kush Bhatia\u2021, Ashish Kumar\u2021, Prateek Jain\u2020 and Manik Varma\u2020",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8116-fastgrnn-a-fast-accurate-stable-and-tiny-kilobyte-sized-gated-recurrent-neural-network.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix\u2019s singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN\u2019s matrices to be low-rank, sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the \"Hey Cortana\" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN\u2019s code is available at [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]."
    },
    "keywords": [
        {
            "term": "Internet of Things",
            "url": "https://en.wikipedia.org/wiki/Internet_of_Things"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "real time",
            "url": "https://en.wikipedia.org/wiki/real_time"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "floating point unit",
            "url": "https://en.wikipedia.org/wiki/floating_point_unit"
        }
    ],
    "highlights": [
        "FastGRNN almost matches the accuracies and training times of state-of-the-art unitary and gated RNNs but has significantly lower prediction costs with models ranging from 1 to 6 Kilobytes for real-world applications",
        "Squeezing the RNN model and code into a few Kilobytes could allow RNNs to be deployed on billions of Internet of Things (IoT) endpoints having just 2 KB RAM and 32 KB flash memory [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "FastRNN\u2019s training speedups over all unitary and gated RNNs could range from 1.2x over UGRNN on the Yelp-5 and DSA-19 datasets to 196x over Efficient Unitary Recurrent Neural Networks on the Google-12 dataset. This demonstrates that the vanishing and exploding gradient problem could be overcome by the addition of a simple weighted residual connection to the standard RNN architecture thereby allowing FastRNN to train efficiently and stablely",
        "This demonstrates that the vanishing and exploding gradient problem could be overcome by the addition of a simple weighted residual connection to the standard RNN architecture thereby allowing FastRNN to train efficiently and stablely. This demonstrates that the residual connection offers a theoretically principled architecture that can often result in accuracy gains without limiting the expressive power of the hidden state transition matrix",
        "FastRNN could lead to provably stable training by incorporating a residual connection with two scalar parameters into the standard RNN architecture",
        "FastGRNN extended the residual connection to a gate reusing the RNN matrices and was able to match the accuracies of state-of-the-art gated RNNs but with significantly lower prediction costs"
    ],
    "key_statements": [
        "FastGRNN almost matches the accuracies and training times of state-of-the-art unitary and gated RNNs but has significantly lower prediction costs with models ranging from 1 to 6 Kilobytes for real-world applications",
        "Objective: This paper develops the FastGRNN algorithm to address the twin RNN limitations of inaccurate training and inefficient prediction",
        "Squeezing the RNN model and code into a few Kilobytes could allow RNNs to be deployed on billions of Internet of Things (IoT) endpoints having just 2 KB RAM and 32 KB flash memory [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "Gated RNNs [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] have stabilized training by adding extra parameters leading to state-of-the-art prediction accuracies but with models that might sometimes be even larger than unitary RNNs",
        "FastRNN: This paper demonstrates that standard RNN training could be stabilized with the addition of a residual connection [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] having just 2 additional scalar parameters",
        "This paper studies the FastRNN architecture that is inspired by weighted residual connections [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and shows that FastRNN can be significantly more stable and accurate than the standard RNN while preserving its prediction complexity",
        "FastRNN\u2019s training speedups over all unitary and gated RNNs could range from 1.2x over UGRNN on the Yelp-5 and DSA-19 datasets to 196x over Efficient Unitary Recurrent Neural Networks on the Google-12 dataset. This demonstrates that the vanishing and exploding gradient problem could be overcome by the addition of a simple weighted residual connection to the standard RNN architecture thereby allowing FastRNN to train efficiently and stablely",
        "This demonstrates that the vanishing and exploding gradient problem could be overcome by the addition of a simple weighted residual connection to the standard RNN architecture thereby allowing FastRNN to train efficiently and stablely. This demonstrates that the residual connection offers a theoretically principled architecture that can often result in accuracy gains without limiting the expressive power of the hidden state transition matrix",
        "FastRNN could lead to provably stable training by incorporating a residual connection with two scalar parameters into the standard RNN architecture",
        "FastGRNN extended the residual connection to a gate reusing the RNN matrices and was able to match the accuracies of state-of-the-art gated RNNs but with significantly lower prediction costs"
    ],
    "summary": [
        "FastGRNN almost matches the accuracies and training times of state-of-the-art unitary and gated RNNs but has significantly lower prediction costs with models ranging from 1 to 6 Kilobytes for real-world applications.",
        "Gated RNNs [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] have stabilized training by adding extra parameters leading to state-of-the-art prediction accuracies but with models that might sometimes be even larger than unitary RNNs. FastRNN: This paper demonstrates that standard RNN training could be stabilized with the addition of a residual connection [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] having just 2 additional scalar parameters.",
        "This allowed FastGRNN to match, and sometimes exceed, state-of-the-art prediction accuracies of LSTMs, GRUs, UGRNNs and other leading gated RNN techniques while having 2-4x fewer parameters.",
        "Gated RNNs: Gated architectures [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] achieve state-of-the-art classification accuracies by adding extra parameters but increase model size and prediction time.",
        "This paper further strengthens FastRNN to develop the FastGRNN architecture that is more accurate than unitary methods [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>] and provides comparable accuracy to the state-of-the-art gated RNNs at 35x less computational cost.",
        "FastRNN is able to address this issue, and have faster training and more accurate model than the state-of-the-art unitary RNNs. by using the above observations and a careful perturbation analysis, we can provide the following convergence and generalization error bounds for FastRNN: Theorem 3.1 (Convergence Bound).",
        "FastGRNN\u2019s inference complexity is almost same as that of the standard RNN but its accuracy and training stability is on par with expensive gated architectures like GRUs and LSTMs. Sparse low-rank representation: FastGRNN further compresses the model size by using a low-rank and a sparse representation of the parameter matrices W, U.",
        "This demonstrates that the vanishing and exploding gradient problem could be overcome by the addition of a simple weighted residual connection to the standard RNN architecture thereby allowing FastRNN to train efficiently and stablely.",
        "Figures 3 and 4 in the Appendix show that FastGRNN-LSQ and FastGRNN\u2019s classification accuracies could be higher than those obtained by the best unitary and gated RNNs for any given model size in the 0-128 KB range.",
        "Table 16 in the Appendix shows that regularization and layering techniques [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] that have been proposed to increase the prediction accuracy of other gated RNNs are effective for FastGRNN and can lead to reductions in perplexity on the PTB dataset.",
        "This allowed FastGRNN to make accurate predictions efficiently on severely resource-constrained IoT devices too tiny to hold other RNN models"
    ],
    "headline": "This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Ahmad, A. Lavin, S. Purdy, and Z. Agha. Unsupervised real-time anomaly detection for streaming data. Neurocomputing, 262:134\u2013147, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahmad%2C%20S.%20Lavin%2C%20A.%20Purdy%2C%20S.%20Agha%2C%20Z.%20Unsupervised%20real-time%20anomaly%20detection%20for%20streaming%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahmad%2C%20S.%20Lavin%2C%20A.%20Purdy%2C%20S.%20Agha%2C%20Z.%20Unsupervised%20real-time%20anomaly%20detection%20for%20streaming%20data%202017"
        },
        {
            "id": "2",
            "entry": "[2] K. Altun, B. Barshan, and O. Tun\u00e7el. Comparative study on classifying human activities with miniature inertial and magnetic sensors. Pattern Recognition, 43(10):3605\u20133620, 2010. URL https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities.",
            "url": "https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altun%2C%20K.%20Barshan%2C%20B.%20Tun%C3%A7el%2C%20O.%20Comparative%20study%20on%20classifying%20human%20activities%20with%20miniature%20inertial%20and%20magnetic%20sensors%202010"
        },
        {
            "id": "3",
            "entry": "[3] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz. Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. In International Workshop on Ambient Assisted Living, pages 216\u2013223. Springer, 2012. URL https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones.",
            "url": "https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anguita%2C%20D.%20Ghio%2C%20A.%20Oneto%2C%20L.%20Parra%2C%20X.%20Human%20activity%20recognition%20on%20smartphones%20using%20a%20multiclass%20hardware-friendly%20support%20vector%20machine%202012"
        },
        {
            "id": "4",
            "entry": "[4] M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20M.%20Bartlett%2C%20P.L.%20Neural%20network%20learning%3A%20Theoretical%20foundations%202009"
        },
        {
            "id": "5",
            "entry": "[5] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pages 1120\u20131128, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Shah%2C%20A.%20Bengio%2C%20Y.%20Unitary%20evolution%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Shah%2C%20A.%20Bengio%2C%20Y.%20Unitary%20evolution%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "6",
            "entry": "[6] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "7",
            "entry": "[7] Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8624\u20138628. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Boulanger-Lewandowski%2C%20N.%20Pascanu%2C%20R.%20Advances%20in%20optimizing%20recurrent%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Boulanger-Lewandowski%2C%20N.%20Pascanu%2C%20R.%20Advances%20in%20optimizing%20recurrent%20networks%202013"
        },
        {
            "id": "8",
            "entry": "[8] J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. arXiv preprint arXiv:1611.01576, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01576"
        },
        {
            "id": "9",
            "entry": "[9] V. Campos, B. Jou, X. G. i Nieto, J. Torres, and S.-F. Chang. Skip RNN: Learning to skip state updates in recurrent neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Campos%2C%20V.%20Jou%2C%20B.%20G%2C%20X.%20Skip%20RNN%3A%20Learning%20to%20skip%20state%20updates%20in%20recurrent%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Campos%2C%20V.%20Jou%2C%20B.%20G%2C%20X.%20Skip%20RNN%3A%20Learning%20to%20skip%20state%20updates%20in%20recurrent%20neural%20networks%202018"
        },
        {
            "id": "10",
            "entry": "[10] G. Chen, C. Parada, and G. Heigold. Small-footprint keyword spotting using deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 4087\u20134091. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20G.%20Parada%2C%20C.%20Heigold%2C%20G.%20Small-footprint%20keyword%20spotting%20using%20deep%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20G.%20Parada%2C%20C.%20Heigold%2C%20G.%20Small-footprint%20keyword%20spotting%20using%20deep%20neural%20networks%202014"
        },
        {
            "id": "11",
            "entry": "[11] G. Chen, C. Parada, and T. N. Sainath. Query-by-example keyword spotting using long shortterm memory networks. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5236\u20135240. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20G.%20Parada%2C%20C.%20Sainath%2C%20T.N.%20Query-by-example%20keyword%20spotting%20using%20long%20shortterm%20memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20G.%20Parada%2C%20C.%20Sainath%2C%20T.N.%20Query-by-example%20keyword%20spotting%20using%20long%20shortterm%20memory%20networks%202015"
        },
        {
            "id": "12",
            "entry": "[12] K. Cho, B. Van Merri\u00ebnboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1259"
        },
        {
            "id": "13",
            "entry": "[13] J. Collins, J. Sohl-Dickstein, and D. Sussillo. Capacity and trainability in recurrent neural networks. arXiv preprint arXiv:1611.09913, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09913"
        },
        {
            "id": "14",
            "entry": "[14] S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "15",
            "entry": "[15] N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. arXiv preprint arXiv:1712.06541, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06541"
        },
        {
            "id": "16",
            "entry": "[16] C. Gupta, A. S. Suggala, A. Gupta, H. V. Simhadri, B. Paranjape, A. Kumar, S. Goyal, M. Udupa, R. amd Varma, and P. Jain. Protonn: Compressed and accurate knn for resource-scarce devices. In Proceedings of the International Conference on Machine Learning, August 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20C.%20Suggala%2C%20A.S.%20Gupta%2C%20A.%20Simhadri%2C%20H.V.%20Protonn%3A%20Compressed%20and%20accurate%20knn%20for%20resource-scarce%20devices%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20C.%20Suggala%2C%20A.S.%20Gupta%2C%20A.%20Simhadri%2C%20H.V.%20Protonn%3A%20Compressed%20and%20accurate%20knn%20for%20resource-scarce%20devices%202017-08"
        },
        {
            "id": "17",
            "entry": "[17] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20S.%20Mao%2C%20H.%20Dally%2C%20W.J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20S.%20Mao%2C%20H.%20Dally%2C%20W.J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016"
        },
        {
            "id": "18",
            "entry": "[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "19",
            "entry": "[19] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "20",
            "entry": "[20] H. Inan, K. Khosravi, and R. Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01462"
        },
        {
            "id": "21",
            "entry": "[21] H. Jaeger, M. Lukosevicius, D. Popovici, and U. Siewert. Optimization and applications of echo state networks with leaky-integrator neurons. Neural Networks, 20(3):335\u2013352, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaeger%2C%20H.%20Lukosevicius%2C%20M.%20Popovici%2C%20D.%20Siewert%2C%20U.%20Optimization%20and%20applications%20of%20echo%20state%20networks%20with%20leaky-integrator%20neurons%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaeger%2C%20H.%20Lukosevicius%2C%20M.%20Popovici%2C%20D.%20Siewert%2C%20U.%20Optimization%20and%20applications%20of%20echo%20state%20networks%20with%20leaky-integrator%20neurons%202007"
        },
        {
            "id": "22",
            "entry": "[22] L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and Y. Bengio. Gated orthogonal recurrent units: On learning to forget. arXiv preprint arXiv:1706.02761, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02761"
        },
        {
            "id": "23",
            "entry": "[23] L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, M. Tegmark, and M. Soljacic. Tunable efficient unitary neural networks (eunn) and their application to RNN. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jing%2C%20L.%20Shen%2C%20Y.%20Dubcek%2C%20T.%20Peurifoy%2C%20J.%20Soljacic.%20Tunable%20efficient%20unitary%20neural%20networks%20%28eunn%29%20and%20their%20application%20to%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jing%2C%20L.%20Shen%2C%20Y.%20Dubcek%2C%20T.%20Peurifoy%2C%20J.%20Soljacic.%20Tunable%20efficient%20unitary%20neural%20networks%20%28eunn%29%20and%20their%20application%20to%202017"
        },
        {
            "id": "24",
            "entry": "[24] C. Jose, M. Cisse, and F. Fleuret. Kronecker recurrent units. In J. Dy and A. Krause, editors, International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2380\u20132389, Stockholmsm\u00e4ssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jose%2C%20C.%20Cisse%2C%20M.%20Fleuret%2C%20F.%20Kronecker%20recurrent%20units%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jose%2C%20C.%20Cisse%2C%20M.%20Fleuret%2C%20F.%20Kronecker%20recurrent%20units%202018-07"
        },
        {
            "id": "25",
            "entry": "[25] S. Kanai, Y. Fujiwara, and S. Iwamura. Preventing gradient explosions in gated recurrent units. In Advances in Neural Information Processing Systems, pages 435\u2013444, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kanai%2C%20S.%20Fujiwara%2C%20Y.%20Iwamura%2C%20S.%20Preventing%20gradient%20explosions%20in%20gated%20recurrent%20units%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kanai%2C%20S.%20Fujiwara%2C%20Y.%20Iwamura%2C%20S.%20Preventing%20gradient%20explosions%20in%20gated%20recurrent%20units%202017"
        },
        {
            "id": "26",
            "entry": "[26] V. K\u00ebpuska and T. Klein. A novel wake-up-word speech recognition system, wake-up-word recognition task, technology and evaluation. Nonlinear Analysis: Theory, Methods & Applications, 71(12):e2772\u2013e2789, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%C3%ABpuska%2C%20V.%20Klein%2C%20T.%20A%20novel%20wake-up-word%20speech%20recognition%20system%2C%20wake-up-word%20recognition%20task%2C%20technology%20and%20evaluation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%C3%ABpuska%2C%20V.%20Klein%2C%20T.%20A%20novel%20wake-up-word%20speech%20recognition%20system%2C%20wake-up-word%20recognition%20task%2C%20technology%20and%20evaluation%202009"
        },
        {
            "id": "27",
            "entry": "[27] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "28",
            "entry": "[28] A. Kumar, S. Goyal, and M. Varma. Resource-efficient machine learning in 2 kb ram for the internet of things. In Proceedings of the International Conference on Machine Learning, August 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20A.%20Goyal%2C%20S.%20Varma%2C%20M.%20Resource-efficient%20machine%20learning%20in%202%20kb%20ram%20for%20the%20internet%20of%20things%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20A.%20Goyal%2C%20S.%20Varma%2C%20M.%20Resource-efficient%20machine%20learning%20in%202%20kb%20ram%20for%20the%20internet%20of%20things%202017-08"
        },
        {
            "id": "29",
            "entry": "[29] A. Kusupati, M. Singh, K. Bhatia, A. Kumar, P. Jain, and M. Varma. The EdgeML Library: Code for FastRNN and FastGRNN, 2018. URL https://github.com/Microsoft/EdgeML.",
            "url": "https://github.com/Microsoft/EdgeML",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kusupati%2C%20A.%20Singh%2C%20M.%20Bhatia%2C%20K.%20Kumar%2C%20A.%20The%20EdgeML%20Library%3A%20Code%20for%20FastRNN%20and%202018"
        },
        {
            "id": "30",
            "entry": "[30] Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00941"
        },
        {
            "id": "31",
            "entry": "[31] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "32",
            "entry": "[32] M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20M.P.%20Marcinkiewicz%2C%20M.A.%20Santorini%2C%20B.%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20The%20penn%20treebank%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20M.P.%20Marcinkiewicz%2C%20M.A.%20Santorini%2C%20B.%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20The%20penn%20treebank%201993"
        },
        {
            "id": "33",
            "entry": "[33] J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAuley%2C%20J.%20Leskovec%2C%20J.%20Hidden%20factors%20and%20hidden%20topics%3A%20understanding%20rating%20dimensions%20with%20review%20text%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAuley%2C%20J.%20Leskovec%2C%20J.%20Hidden%20factors%20and%20hidden%20topics%3A%20understanding%20rating%20dimensions%20with%20review%20text%202013"
        },
        {
            "id": "34",
            "entry": "[34] G. Melis, C. Dyer, and P. Blunsom. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.05589"
        },
        {
            "id": "35",
            "entry": "[35] S. Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02182"
        },
        {
            "id": "36",
            "entry": "[36] Z. Mhammedi, A. Hellicar, A. Rahman, and J. Bailey. Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhammedi%2C%20Z.%20Hellicar%2C%20A.%20Rahman%2C%20A.%20Bailey%2C%20J.%20Efficient%20orthogonal%20parametrisation%20of%20recurrent%20neural%20networks%20using%20householder%20reflections%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhammedi%2C%20Z.%20Hellicar%2C%20A.%20Rahman%2C%20A.%20Bailey%2C%20J.%20Efficient%20orthogonal%20parametrisation%20of%20recurrent%20neural%20networks%20using%20householder%20reflections%202017"
        },
        {
            "id": "37",
            "entry": "[37] T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. SLT, 12(234-239):8, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Zweig%2C%20G.%20Context%20dependent%20recurrent%20neural%20network%20language%20model%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20T.%20Zweig%2C%20G.%20Context%20dependent%20recurrent%20neural%20network%20language%20model%202012"
        },
        {
            "id": "38",
            "entry": "[38] S. Narang, E. Elsen, G. Diamos, and S. Sengupta. Exploring sparsity in recurrent neural networks. arXiv preprint arXiv:1704.05119, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05119"
        },
        {
            "id": "39",
            "entry": "[39] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pages 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20R.%20Mikolov%2C%20T.%20Bengio%2C%20Y.%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20R.%20Mikolov%2C%20T.%20Bengio%2C%20Y.%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "40",
            "entry": "[40] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        },
        {
            "id": "41",
            "entry": "[41] T. N. Sainath and C. Parada. Convolutional neural networks for small-footprint keyword spotting. In Sixteenth Annual Conference of the International Speech Communication Association, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20T.N.%20Parada%2C%20C.%20Convolutional%20neural%20networks%20for%20small-footprint%20keyword%20spotting%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20T.N.%20Parada%2C%20C.%20Convolutional%20neural%20networks%20for%20small-footprint%20keyword%20spotting%202015"
        },
        {
            "id": "42",
            "entry": "[42] Siri Team, Apple. Hey Siri: An on-device dnn-powered voice trigger for apple\u2019s personal assistant, 2017. URL https://machinelearning.apple.com/2017/10/01/hey-siri.html.",
            "url": "https://machinelearning.apple.com/2017/10/01/hey-siri.html"
        },
        {
            "id": "43",
            "entry": "[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.00387"
        },
        {
            "id": "44",
            "entry": "[44] STCI, Microsoft. Wakeword dataset.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Wakeword%20dataset"
        },
        {
            "id": "45",
            "entry": "[45] G. A. Susto, A. Schirru, S. Pampuri, S. McLoone, and A. Beghi. Machine learning for predictive maintenance: A multiple classifier approach. IEEE Transactions on Industrial Informatics, 11 (3):812\u2013820, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Susto%2C%20G.A.%20Schirru%2C%20A.%20Pampuri%2C%20S.%20McLoone%2C%20S.%20Machine%20learning%20for%20predictive%20maintenance%3A%20A%20multiple%20classifier%20approach%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Susto%2C%20G.A.%20Schirru%2C%20A.%20Pampuri%2C%20S.%20McLoone%2C%20S.%20Machine%20learning%20for%20predictive%20maintenance%3A%20A%20multiple%20classifier%20approach%202015"
        },
        {
            "id": "46",
            "entry": "[46] E. Vorontsov, C. Trabelsi, S. Kadoury, and C. Pal. On orthogonality and learning recurrent networks with long term dependencies. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vorontsov%2C%20E.%20Trabelsi%2C%20C.%20Kadoury%2C%20S.%20Pal%2C%20C.%20On%20orthogonality%20and%20learning%20recurrent%20networks%20with%20long%20term%20dependencies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vorontsov%2C%20E.%20Trabelsi%2C%20C.%20Kadoury%2C%20S.%20Pal%2C%20C.%20On%20orthogonality%20and%20learning%20recurrent%20networks%20with%20long%20term%20dependencies%202017"
        },
        {
            "id": "47",
            "entry": "[47] Z. Wang, J. Lin, and Z. Wang. Accelerating recurrent neural networks: A memory-efficient approach. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 25(10):2763\u2013 2775, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Lin%2C%20J.%20Wang%2C%20Z.%20Accelerating%20recurrent%20neural%20networks%3A%20A%20memory-efficient%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Lin%2C%20J.%20Wang%2C%20Z.%20Accelerating%20recurrent%20neural%20networks%3A%20A%20memory-efficient%20approach%202017"
        },
        {
            "id": "48",
            "entry": "[48] P. Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018. URL http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz.",
            "url": "http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz",
            "arxiv_url": "https://arxiv.org/pdf/1804.03209"
        },
        {
            "id": "49",
            "entry": "[49] S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent neural networks. In Advances in Neural Information Processing Systems, pages 4880\u20134888, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wisdom%2C%20S.%20Powers%2C%20T.%20Hershey%2C%20J.%20Roux%2C%20J.Le%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wisdom%2C%20S.%20Powers%2C%20T.%20Hershey%2C%20J.%20Roux%2C%20J.Le%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "50",
            "entry": "[50] J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu. Learning compact recurrent neural networks with block-term tensor decomposition. arXiv preprint arXiv:1712.05134, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05134"
        },
        {
            "id": "51",
            "entry": "[51] Yelp Inc. Yelp dataset challenge, 2017. URL https://www.yelp.com/dataset/challenge.",
            "url": "https://www.yelp.com/dataset/challenge"
        },
        {
            "id": "52",
            "entry": "[52] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.2329"
        },
        {
            "id": "53",
            "entry": "[53] J. Zhang, Q. Lei, and I. S. Dhillon. Stabilizing gradients for deep neural networks via efficient SVD parameterization. In International Conference on Machine Learning, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20J.%20Lei%2C%20Q.%20Dhillon%2C%20I.S.%20Stabilizing%20gradients%20for%20deep%20neural%20networks%20via%20efficient%20SVD%20parameterization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20J.%20Lei%2C%20Q.%20Dhillon%2C%20I.S.%20Stabilizing%20gradients%20for%20deep%20neural%20networks%20via%20efficient%20SVD%20parameterization%202018"
        }
    ]
}
