{
    "filename": "8294-the-price-of-fair-pca-one-extra-dimension.pdf",
    "metadata": {
        "title": "The Price of Fair PCA: One Extra dimension",
        "author": "Samira Samadi, Uthaipon Tantipongpipat, Jamie H. Morgenstern, Mohit Singh, Santosh Vempala",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8294-the-price-of-fair-pca-one-extra-dimension.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations. We show on several real-world data sets, PCA has higher reconstruction error on population A than on B (for example, women versus men or lowerversus higher-educated individuals). This can happen even when the data set has a similar number of samples from A and B. This motivates our study of dimensionality reduction techniques which maintain similar fidelity for A and B. We define the notion of Fair PCA and give a polynomial-time algorithm for finding a low dimensional representation of the data which is nearly-optimal with respect to this measure. Finally, we show on real-world data sets that our algorithm can be used to efficiently generate a fair low dimensional representation of the data."
    },
    "keywords": [
        {
            "term": "dimensionality reduction",
            "url": "https://en.wikipedia.org/wiki/dimensionality_reduction"
        },
        {
            "term": "disparate impact",
            "url": "https://en.wikipedia.org/wiki/disparate_impact"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "feedback loop",
            "url": "https://en.wikipedia.org/wiki/feedback_loop"
        },
        {
            "term": "data set",
            "url": "https://en.wikipedia.org/wiki/data_set"
        }
    ],
    "highlights": [
        "The ML community has witnessed an onslaught of charges that real-world machine learning algorithms have produced \u201cbiased\u201d outcomes",
        "We show several real-world data sets for which PCA incurs much higher average reconstruction error for one population than another, even when the populations are of similar sizes",
        "We propose a new dimensionality reduction problem which focuses on representing A and B with similar additional error over projecting A or B individually",
        "Our algorithm relies on solving a semidefinite program (SDP), which can be prohibitively slow for practical applications",
        "We note that it is possible to solve an semidefinite program with a much faster multiplicative-weights style algorithm, whose running time in practice is equivalent to solving standard PCA at most 10-15 times",
        "We propose another algorithm of solving semidefinite program using the multiplicative weight (MW) update method"
    ],
    "key_statements": [
        "The ML community has witnessed an onslaught of charges that real-world machine learning algorithms have produced \u201cbiased\u201d outcomes",
        "We show several real-world data sets for which PCA incurs much higher average reconstruction error for one population than another, even when the populations are of similar sizes",
        "We propose a new dimensionality reduction problem which focuses on representing A and B with similar additional error over projecting A or B individually",
        "Our algorithm relies on solving a semidefinite program (SDP), which can be prohibitively slow for practical applications",
        "We note that it is possible to solve an semidefinite program with a much faster multiplicative-weights style algorithm, whose running time in practice is equivalent to solving standard PCA at most 10-15 times",
        "We introduce our approach to fair dimensionality reduction by giving two compelling examples of settings where dimensionality reduction inherently makes a tradeoff between groups A and B",
        "The algorithm has two steps: first, relax fair PCA to a semidefinite optimization problem and solve the semidefinite program; second, solve an LP designed to reduce the rank of said solution",
        "We propose another algorithm of solving semidefinite program using the multiplicative weight (MW) update method",
        "Figure 4 shows the average loss of each population as the result of applying vanilla PCA and Fair PCA on both data sets"
    ],
    "summary": [
        "The ML community has witnessed an onslaught of charges that real-world machine learning algorithms have produced \u201cbiased\u201d outcomes.",
        "Given the n-dimensional data with two subgroups A and B, let Mc, Ab, Bb be optimal rank-d PCA approximations for M, A, and B, respectively.",
        "We introduce our notation for measuring a group\u2019s loss when being projected to Z rather than to its optimal d-dimensional representation: Definition 4.2 (Reconstruction error).",
        "Given m data points in Rn with subgroups A and B, we define the problem of finding a fair PCA projection into d-dimensions as optimizing",
        "To take a step in characterizing solutions to this optimization, Theorem 4.5 states that a fair PCA low dimensional approximation of the data results in the same loss for both groups.",
        "Our algorithm outputs a matrix of rank at most d + 1 and guarantees that it achieves the fair PCA objective value equal to the optimal d-dimensional fair PCA value.",
        "The algorithm has two steps: first, relax fair PCA to a semidefinite optimization problem and solve the SDP; second, solve an LP designed to reduce the rank of said solution.",
        "There is a polynomial-time algorithm that outputs an approximation matrix of the data such that it is either of rank d and is an optimal solution to the fair PCA problem OR it is of rank d + 1, has equal losses for the two populations and achieves the optimal fair PCA objective value for dimension d.",
        "N the desired number of dimensions of k projected space, we generalize Definition 4.4 of fair PCA problem as optimizing",
        "There is a polynomial-time algorithm to find a projection such that it is of dimension at most d + k 1 and achieves the optimal fairness objective value for dimension d.",
        "In contrast to the case of two groups, when there are more than two groups in the data, it is possible that all optimal solutions to fair PCA will not assign the same loss to all groups.",
        "Figure 3 shows the average reconstruction error of each population (Male/Female, Higher/Lower education) as the result of running vanilla PCA and Fair PCA on LFW and Credit data.",
        "Figure 4 shows the average loss of each population as the result of applying vanilla PCA and Fair PCA on both data sets.",
        "Note that at the optimal solution of Fair PCA, the average loss of two populations are the same, we have one line for \u201cFair loss\u201d.",
        "We evaluate the empirical performance of this algorithm on several human-centric data sets"
    ],
    "headline": "We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations",
    "reference_links": [
        {
            "id": "Adler_et+al_2016_a",
            "entry": "Philip Adler, Casey Falk, Sorelle Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. Auditing black-box models for indirect influence. In Proceedings of the 16th International Conference on Data Mining, pages 1\u201310, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adler%2C%20Philip%20Falk%2C%20Casey%20Friedler%2C%20Sorelle%20Rybeck%2C%20Gabriel%20Auditing%20black-box%20models%20for%20indirect%20influence%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adler%2C%20Philip%20Falk%2C%20Casey%20Friedler%2C%20Sorelle%20Rybeck%2C%20Gabriel%20Auditing%20black-box%20models%20for%20indirect%20influence%202016"
        },
        {
            "id": "Afifi_2017_a",
            "entry": "Mahmoud Afifi and Abdelrahman Abdelhamed. Afif4: Deep gender classification based on adaboostbased fusion of isolated facial features and foggy faces. arXiv preprint arXiv:1706.04277, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04277"
        },
        {
            "id": "https://www.propublica.org/article/machine-bias-riskassessments-in-criminal-sentencing,_2018_a",
            "entry": "https://www.propublica.org/article/machine-bias-riskassessments-in-criminal-sentencing, 2018.",
            "url": "https://www.propublica.org/article/machine-bias-riskassessments-in-criminal-sentencing"
        },
        {
            "id": "Arora_et+al_2012_a",
            "entry": "Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a metaalgorithm and applications. Theory of Computing, 8(1):121\u2013164, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20The%20multiplicative%20weights%20update%20method%3A%20a%20metaalgorithm%20and%20applications%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20The%20multiplicative%20weights%20update%20method%3A%20a%20metaalgorithm%20and%20applications%202012"
        },
        {
            "id": "Beutel_2017_a",
            "entry": "Alex Beutel, Jilin Chen, Zhe Zhao, and Ed Huai-hsin Chi. Data decisions and theoretical implications when adversarially learning fair representations. CoRR, abs/1707.00075, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00075"
        },
        {
            "id": "Buolamwini_2018_a",
            "entry": "Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, pages 77\u201391, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buolamwini%2C%20Joy%20Gebru%2C%20Timnit%20Gender%20shades%3A%20Intersectional%20accuracy%20disparities%20in%20commercial%20gender%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buolamwini%2C%20Joy%20Gebru%2C%20Timnit%20Gender%20shades%3A%20Intersectional%20accuracy%20disparities%20in%20commercial%20gender%20classification%202018"
        },
        {
            "id": "Calders_2010_a",
            "entry": "Toon Calders and Sicco Verwer. Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2):277\u2013292, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calders%2C%20Toon%20Verwer%2C%20Sicco%20Three%20naive%20Bayes%20approaches%20for%20discrimination-free%20classification%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calders%2C%20Toon%20Verwer%2C%20Sicco%20Three%20naive%20Bayes%20approaches%20for%20discrimination-free%20classification%202010"
        },
        {
            "id": "Calmon_et+al_2017_a",
            "entry": "Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Information Processing Systems, pages 3992\u20134001, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calmon%2C%20Flavio%20Wei%2C%20Dennis%20Vinzamuri%2C%20Bhanukiran%20Ramamurthy%2C%20Karthikeyan%20Natesan%20Optimized%20pre-processing%20for%20discrimination%20prevention%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calmon%2C%20Flavio%20Wei%2C%20Dennis%20Vinzamuri%2C%20Bhanukiran%20Ramamurthy%2C%20Karthikeyan%20Natesan%20Optimized%20pre-processing%20for%20discrimination%20prevention%202017"
        },
        {
            "id": "Chouldechova_2017_a",
            "entry": "Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2):153\u2013163, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017"
        },
        {
            "id": "Crawford_2017_a",
            "entry": "Kate Crawford. The trouble with bias, 2017. URL http://blog.revolutionanalytics.. Invited Talk com/2017/12/the-trouble-with-bias-by-kate-crawford.html by Kate Crawford at NIPS 2017, Long Beach, CA.",
            "url": "http://blog.revolutionanalytics"
        },
        {
            "id": "Dwork_et+al_2012_a",
            "entry": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214\u2013226. ACM, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "Ensign_et+al_0000_a",
            "entry": "Danielle Ensign, Sorelle A Friedler, Scott Neville, Carlos Scheidegger, and Suresh Venkatasubramanian. Runaway feedback loops in predictive policing. arXiv preprint arXiv:1706.09847, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1706.09847"
        },
        {
            "id": "Ensign_et+al_2017_a",
            "entry": "Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Eduardo Scheidegger, and Suresh Venkatasubramanian. Runaway feedback loops in predictive policing. Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ensign%2C%20Danielle%20Friedler%2C%20Sorelle%20A.%20Neville%2C%20Scott%20Scheidegger%2C%20Carlos%20Eduardo%20Runaway%20feedback%20loops%20in%20predictive%20policing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ensign%2C%20Danielle%20Friedler%2C%20Sorelle%20A.%20Neville%2C%20Scott%20Scheidegger%2C%20Carlos%20Eduardo%20Runaway%20feedback%20loops%20in%20predictive%20policing%202017"
        },
        {
            "id": "Feldman_et+al_2015_a",
            "entry": "Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259\u2013268, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015"
        },
        {
            "id": "Benjamin_2016_a",
            "entry": "Benjamin Fish, Jeremy Kun, and \u00c1d\u00e1m D\u00e1niel Lelkes. A confidence-based approach for balancing fairness and accuracy. In Proceedings of the 16th SIAM International Conference on Data Mining, pages 144\u2013152, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benjamin%20Fish%2C%20Jeremy%20Kun%2C%20and%20%C3%81d%C3%A1m%20D%C3%A1niel%20Lelkes.%20A%20confidence-based%20approach%20for%20balancing%20fairness%20and%20accuracy%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benjamin%20Fish%2C%20Jeremy%20Kun%2C%20and%20%C3%81d%C3%A1m%20D%C3%A1niel%20Lelkes.%20A%20confidence-based%20approach%20for%20balancing%20fairness%20and%20accuracy%202016"
        },
        {
            "id": "Gebru_et+al_2018_a",
            "entry": "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume\u00e9 III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09010"
        },
        {
            "id": "Hajian_2013_a",
            "entry": "Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination prevention in data mining. IEEE Transactions on Knowledge and Data Engineering, 25(7): 1445\u20131459, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hajian%2C%20Sara%20Domingo-Ferrer%2C%20Josep%20A%20methodology%20for%20direct%20and%20indirect%20discrimination%20prevention%20in%20data%20mining%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hajian%2C%20Sara%20Domingo-Ferrer%2C%20Josep%20A%20methodology%20for%20direct%20and%20indirect%20discrimination%20prevention%20in%20data%20mining%202013"
        },
        {
            "id": "Hardt_et+al_2016_a",
            "entry": "Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In Advances in neural information processing systems, pages 3315\u20133323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016"
        },
        {
            "id": "Hotelling_1933_a",
            "entry": "Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hotelling%2C%20Harold%20Analysis%20of%20a%20complex%20of%20statistical%20variables%20into%20principal%20components%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hotelling%2C%20Harold%20Analysis%20of%20a%20complex%20of%20statistical%20variables%20into%20principal%20components%201933"
        },
        {
            "id": "Huang_et+al_2007_a",
            "entry": "Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gary%20B.%20Ramesh%2C%20Manu%20Berg%2C%20Tamara%20Learned-Miller%2C%20Erik%20Labeled%20faces%20in%20the%20wild%3A%20A%20database%20for%20studying%20face%20recognition%20in%20unconstrained%20environments%202007-10"
        },
        {
            "id": "Joseph_et+al_2016_a",
            "entry": "Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325\u2013333, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016"
        },
        {
            "id": "Kamiran_2011_a",
            "entry": "Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1\u201333, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202011"
        },
        {
            "id": "Kamiran_et+al_2010_a",
            "entry": "Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree learning. In Proceedings of the 10th IEEE International Conference on Data Mining, pages 869\u2013874, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Pechenizkiy%2C%20Mykola%20Discrimination%20aware%20decision%20tree%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Pechenizkiy%2C%20Mykola%20Discrimination%20aware%20decision%20tree%20learning%202010"
        },
        {
            "id": "Kamiran_et+al_2012_a",
            "entry": "Faisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision theory for discrimination-aware classification. In Proceedings of the 12th IEEE International Conference on Data Mining, pages 924\u2013929, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20Faisal%20Karim%2C%20Asim%20Zhang%2C%20Xiangliang%20Decision%20theory%20for%20discrimination-aware%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20Faisal%20Karim%2C%20Asim%20Zhang%2C%20Xiangliang%20Decision%20theory%20for%20discrimination-aware%20classification%202012"
        },
        {
            "id": "Kamishima_et+al_2012_a",
            "entry": "Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prejudice remover regularizer. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, pages 35\u201350, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamishima%2C%20Toshihiro%20Akaho%2C%20Shotaro%20Asoh%2C%20Hideki%20Sakuma%2C%20Jun%20Fairness-aware%20classifier%20with%20prejudice%20remover%20regularizer%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamishima%2C%20Toshihiro%20Akaho%2C%20Shotaro%20Asoh%2C%20Hideki%20Sakuma%2C%20Jun%20Fairness-aware%20classifier%20with%20prejudice%20remover%20regularizer%202012"
        },
        {
            "id": "Kannan_et+al_2017_a",
            "entry": "Sampath Kannan, Michael Kearns, Jamie Morgenstern, Mallesh M. Pai, Aaron Roth, Rakesh V. Vohra, and Zhiwei Steven Wu. Fairness incentives for myopic agents. In Proceedings of the 2017 ACM Conference on Economics and Computation, pages 369\u2013386, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kannan%2C%20Sampath%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20Pai%2C%20Mallesh%20M.%20Fairness%20incentives%20for%20myopic%20agents%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kannan%2C%20Sampath%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20Pai%2C%20Mallesh%20M.%20Fairness%20incentives%20for%20myopic%20agents%202017"
        },
        {
            "id": "Kay_et+al_2015_a",
            "entry": "Matthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereotypes in image search results for occupations. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 3819\u20133828. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kay%2C%20Matthew%20Matuszek%2C%20Cynthia%20Munson%2C%20Sean%20A.%20Unequal%20representation%20and%20gender%20stereotypes%20in%20image%20search%20results%20for%20occupations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kay%2C%20Matthew%20Matuszek%2C%20Cynthia%20Munson%2C%20Sean%20A.%20Unequal%20representation%20and%20gender%20stereotypes%20in%20image%20search%20results%20for%20occupations%202015"
        },
        {
            "id": "Kleinberg_et+al_2016_a",
            "entry": "Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.05807"
        },
        {
            "id": "Lau_et+al_2011_a",
            "entry": "Lap Chi Lau, Ramamoorthi Ravi, and Mohit Singh. Iterative methods in combinatorial optimization, volume 46. Cambridge University Press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lau%2C%20Lap%20Chi%20Ravi%2C%20Ramamoorthi%20Singh%2C%20Mohit%20Iterative%20methods%20in%20combinatorial%20optimization%2C%20volume%2046%202011"
        },
        {
            "id": "Lipton_et+al_2017_a",
            "entry": "Zachary C. Lipton, Alexandra Chouldechova, and Julian McAuley. Does mitigating ML\u2019s disparate impact require disparate treatment? arXiv preprint arXiv:1711.07076, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07076"
        },
        {
            "id": "Luong_et+al_2011_a",
            "entry": "Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. k-NN as an implementation of situation testing for discrimination discovery and prevention. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 502\u2013510. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Binh%20Thanh%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20k-NN%20as%20an%20implementation%20of%20situation%20testing%20for%20discrimination%20discovery%20and%20prevention%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Binh%20Thanh%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20k-NN%20as%20an%20implementation%20of%20situation%20testing%20for%20discrimination%20discovery%20and%20prevention%202011"
        },
        {
            "id": "Madras_et+al_2018_a",
            "entry": "David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In Proceedings of the 35th International Conference on Machine Learning, pages 3384\u20133393, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madras%2C%20David%20Creager%2C%20Elliot%20Pitassi%2C%20Toniann%20Zemel%2C%20Richard%20Learning%20adversarially%20fair%20and%20transferable%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madras%2C%20David%20Creager%2C%20Elliot%20Pitassi%2C%20Toniann%20Zemel%2C%20Richard%20Learning%20adversarially%20fair%20and%20transferable%20representations%202018"
        },
        {
            "id": "Olfat_2018_a",
            "entry": "Matt Olfat and Anil Aswani. Convex formulations for fair principal component analysis. arXiv preprint arXiv:1802.03765, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03765"
        },
        {
            "id": "Pearson_1901_a",
            "entry": "Karl Pearson. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559\u2013572, 1901.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearson%2C%20Karl%20On%20lines%20and%20planes%20of%20closest%20fit%20to%20systems%20of%20points%20in%20space%201901",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pearson%2C%20Karl%20On%20lines%20and%20planes%20of%20closest%20fit%20to%20systems%20of%20points%20in%20space%201901"
        },
        {
            "id": "Pedreshi_et+al_2008_a",
            "entry": "Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560\u2013568. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedreshi%2C%20Dino%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20Discrimination-aware%20data%20mining%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedreshi%2C%20Dino%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20Discrimination-aware%20data%20mining%202008"
        },
        {
            "id": "Schrijver_1998_a",
            "entry": "Alexander Schrijver. Theory of linear and integer programming. John Wiley & Sons, 1998. Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014. Tom Simonite. When it comes to gorillas, google photos remains blind. https:",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schrijver%2C%20Alexander%20Theory%20of%20linear%20and%20integer%20programming%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schrijver%2C%20Alexander%20Theory%20of%20linear%20and%20integer%20programming%201998"
        },
        {
            "id": "Wwwwiredcom_2018_a",
            "entry": "//www.wired.com/story/when-it-comes-to-gorillas-google-photosremains-blind/, Jan 2018. Latanya Sweeney. Discrimination in online ad delivery. Communications of the ACM, 56(5):44\u201354, 2013. Twitter. Jacky lives: Google photos, y\u2019all fucked up. My friend\u2019s not a gorilla.",
            "url": "http://www.wired.com/story/when-it-comes-to-gorillas-google-photosremains-blind/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=wwwwiredcom/story/when-it-comes-to-gorillas-google-photosremains-blind%20Latanya%20Sweeney.%20Discrimination%20in%20online%20ad%20delivery%202018-01"
        },
        {
            "id": "Zafar_et+al_2015_a",
            "entry": "https://twitter.com/jackyalcine/status/615329515909156865, June 2015. I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2):2473\u20132480, 2009. Muhammad Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna Gummadi. Fairness constraints: A mechanism for fair classification. CoRR, abs/1507.05259, 2015. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325\u2013333, 2013. Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning.arXiv preprint arXiv:1801.07593, 2018.",
            "url": "https://twitter.com/jackyalcine/status/615329515909156865",
            "arxiv_url": "https://arxiv.org/pdf/1507.05259"
        }
    ]
}
