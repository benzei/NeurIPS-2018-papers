{
    "filename": "7451-simple-random-search-of-static-linear-policies-is-competitive-for-reinforcement-learning.pdf",
    "metadata": {
        "title": "Simple random search of static linear policies is competitive for reinforcement learning",
        "author": "Horia Mania, Aurelia Guy, Benjamin Recht",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7451-simple-random-search-of-static-linear-policies-is-competitive-for-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Model-free reinforcement learning aims to offer off-the-shelf solutions for controlling dynamical systems without requiring models of the system dynamics. We introduce a model-free random search algorithm for training static, linear policies for continuous control problems. Common evaluation methodology shows that our method matches state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Nonetheless, more rigorous evaluation reveals that the assessment of performance on these benchmarks is optimistic. We evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. This extensive evaluation is possible because of the small computational footprint of our method. Our simulations highlight a high variability in performance in these benchmark tasks, indicating that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms. Our results stress the need for new baselines, benchmarks and evaluation methodology for RL algorithms."
    },
    "keywords": [
        {
            "term": "physical system",
            "url": "https://en.wikipedia.org/wiki/physical_system"
        },
        {
            "term": "random search",
            "url": "https://en.wikipedia.org/wiki/random_search"
        },
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        },
        {
            "term": "Evolution Strategies",
            "url": "https://en.wikipedia.org/wiki/Evolution_Strategies"
        },
        {
            "term": "dynamical system",
            "url": "https://en.wikipedia.org/wiki/dynamical_system"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "system dynamic",
            "url": "https://en.wikipedia.org/wiki/system_dynamic"
        }
    ],
    "highlights": [
        "Model-free reinforcement learning (RL) aims to offer off-the-shelf solutions for controlling dynamical systems without requiring models of the system dynamics",
        "There are several factors prohibiting the adoption of model-free reinforcement learning methods for controlling physical systems: the methods require too much data to achieve reasonable performance, the ever-increasing assortment of reinforcement learning methods makes it difficult to choose what is the best method for a specific task, and many candidate algorithms are difficult to implement and deploy [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "We introduce the Augmented Random Search (ARS) method, which relies on three augmentations of BRS that build on successful heuristics employed in deep reinforcement learning",
        "Table 1 shows the average number of episodes required by Augmented Random Search, Natural Gradients, and Trust Region Policy Optimization to reach a prescribed reward threshold, using the values reported by Rajeswaran et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] for Natural Gradients and Trust Region Policy Optimization",
        "The presence of local optima is inherent to non-convex optimization, and our results show that reinforcement learning algorithms should be evaluated on many random seeds for determining the frequency with which local optima are found",
        "No special nonlinear controllers are needed to match the performance recorded in the reinforcement learning literature"
    ],
    "key_statements": [
        "Model-free reinforcement learning (RL) aims to offer off-the-shelf solutions for controlling dynamical systems without requiring models of the system dynamics",
        "There are several factors prohibiting the adoption of model-free reinforcement learning methods for controlling physical systems: the methods require too much data to achieve reasonable performance, the ever-increasing assortment of reinforcement learning methods makes it difficult to choose what is the best method for a specific task, and many candidate algorithms are difficult to implement and deploy [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "As a second simplification to model-free reinforcement learning, Rajeswaran et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] have shown that linear policies can be trained via natural policy gradients to obtain competitive performance on the MuJoCo locomotion tasks, showing that complicated neural network policies are not needed to solve these continuous control problems",
        "We demonstrate that a simple random search method can match or exceed state-of-the-art sample efficiency on the MuJoCo locomotion tasks, included in the OpenAI Gym",
        "To put Augmented Random Search on equal footing with competing methods, we evaluate its sample complexity over three random seeds and compare it to results reported in the literature [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "We introduce the Augmented Random Search (ARS) method, which relies on three augmentations of BRS that build on successful heuristics employed in deep reinforcement learning",
        "The first version, Augmented Random Search V1, is obtained from BRS by scaling the update steps by the standard deviation R of the rewards collected at each iteration; see Line 7 of Algorithm 1",
        "Relies on policies parametrized by neural networks with virtual batch normalization, while we show that Augmented Random Search achieves state-of-the-art performance with linear policies.\n4 Empirical results on the MuJoCo locomotion tasks",
        "We evaluate the performance of Augmented Random Search on the MuJoCo locomotion tasks included in the OpenAI Gymv0.9.3 [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "Table 1 shows the average number of episodes required by Augmented Random Search, Natural Gradients, and Trust Region Policy Optimization to reach a prescribed reward threshold, using the values reported by Rajeswaran et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] for Natural Gradients and Trust Region Policy Optimization",
        "Salimans et al [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] reported the average number of episodes required by Evolution Strategies to reach a prescribed reward threshold, on four of the locomotion tasks",
        "The presence of local optima is inherent to non-convex optimization, and our results show that reinforcement learning algorithms should be evaluated on many random seeds for determining the frequency with which local optima are found",
        "No special nonlinear controllers are needed to match the performance recorded in the reinforcement learning literature",
        "We propose the Linear Quadratic Regulator as a reasonable testbed for reinforcement learning algorithms"
    ],
    "summary": [
        "Model-free reinforcement learning (RL) aims to offer off-the-shelf solutions for controlling dynamical systems without requiring models of the system dynamics.",
        "As a second simplification to model-free RL, Rajeswaran et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] have shown that linear policies can be trained via natural policy gradients to obtain competitive performance on the MuJoCo locomotion tasks, showing that complicated neural network policies are not needed to solve these continuous control problems.",
        "In Section 4, we evaluate the performance of ARS on the benchmark MuJoCo locomotion tasks, included in the OpenAI Gym. Our method learns static, linear policies that achieve high rewards on all MuJoCo tasks.",
        "The first version, ARS V1, is obtained from BRS by scaling the update steps by the standard deviation R of the rewards collected at each iteration; see Line 7 of Algorithm 1.",
        "As shown in Section 4, ARS V1 can train linear policies, which achieve the reward thresholds previously proposed in the literature, for five MuJoCo benchmarks.",
        "This version of ARS trains policies which are linear maps of states normalized by a mean and standard deviation computed online.",
        "This algorithmic enhancement intuitively improves the performance of ARS because it ensures that the update steps are an average over directions that obtained high rewards.",
        "We used these default reward functions for evaluating the performance of the linear policies trained with ARS.",
        "Three random seeds evaluation: We compare the different versions of ARS to the following methods: Trust Region Policy Optimization (TRPO), Deep Deterministic Policy Gradient (DDPG), Natural Gradients (NG), Evolution Strategies (ES), Proximal Policy Optimization (PPO), Soft Actor Critic (SAC), Soft Q-Learning (SQL), A2C, and the Cross Entropy Method (CEM).",
        "Table 1 shows the average number of episodes required by ARS, NG, and TRPO to reach a prescribed reward threshold, using the values reported by Rajeswaran et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] for NG and TRPO.",
        "For each version of ARS and each MuJoCo task we chose the hyperparameters which minimize the average number of episodes required to reach the reward threshold.",
        "Salimans et al [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] reported the average number of episodes required by ES to reach a prescribed reward threshold, on four of the locomotion tasks.",
        "With a few algorithmic augmentations, basic random search of static, linear policies achieves stateof-the-art sample efficiency on the MuJoCo locomotion tasks.",
        "The \u201csample complexity\" of the method is reported as the number of samples required to reach a target reward threshold, with the given hyperparameter configuration."
    ],
    "headline": "We introduce a model-free random search algorithm for training static, linear policies for continuous control problems",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. pages 28\u201340, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Dekel%2C%20O.%20Xiao%2C%20L.%20Optimal%20algorithms%20for%20online%20convex%20optimization%20with%20multi-point%20bandit%20feedback%202010"
        },
        {
            "id": "2",
            "entry": "[2] F. Bach and V. Perchet. Highly-smooth zero-th order online optimization. Conference on Learning Theory, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Perchet%2C%20V.%20Highly-smooth%20zero-th%20order%20online%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Perchet%2C%20V.%20Highly-smooth%20zero-th%20order%20online%20optimization%202016"
        },
        {
            "id": "3",
            "entry": "[3] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%20Brockman%20V%20Cheung%20L%20Pettersson%20J%20Schneider%20J%20Schulman%20J%20Tang%20and%20W%20Zaremba%20OpenAI%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%20Brockman%20V%20Cheung%20L%20Pettersson%20J%20Schneider%20J%20Schulman%20J%20Tang%20and%20W%20Zaremba%20OpenAI%20gym%202016"
        },
        {
            "id": "4",
            "entry": "[4] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the sample complexity of the linear quadratic regulator. arxiv:1710.01688, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.01688"
        },
        {
            "id": "5",
            "entry": "[5] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. Proceedings of the International Conference on Machine Learning, pages 1329\u2013 1338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "6",
            "entry": "[6] A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. Proceedings of the ACM-SIAM symposium on Discrete algorithms, pages 385\u2013394, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flaxman%2C%20A.D.%20Kalai%2C%20A.T.%20McMahan%2C%20H.B.%20Online%20convex%20optimization%20in%20the%20bandit%20setting%3A%20gradient%20descent%20without%20a%20gradient%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flaxman%2C%20A.D.%20Kalai%2C%20A.T.%20McMahan%2C%20H.B.%20Online%20convex%20optimization%20in%20the%20bandit%20setting%3A%20gradient%20descent%20without%20a%20gradient%202005"
        },
        {
            "id": "7",
            "entry": "[7] S. Gu, T. Lillicrap, Z. Ghahramani, R. E. Turner, and S. Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20S.%20Lillicrap%2C%20T.%20Ghahramani%2C%20Z.%20Turner%2C%20R.E.%20Q-prop%3A%20Sample-efficient%20policy%20gradient%20with%20an%20off-policy%20critic%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20S.%20Lillicrap%2C%20T.%20Ghahramani%2C%20Z.%20Turner%2C%20R.E.%20Q-prop%3A%20Sample-efficient%20policy%20gradient%20with%20an%20off-policy%20critic%202016"
        },
        {
            "id": "8",
            "entry": "[8] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based policies. Proceedings of the International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20T.%20Tang%2C%20H.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Reinforcement%20learning%20with%20deep%20energy-based%20policies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20T.%20Tang%2C%20H.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Reinforcement%20learning%20with%20deep%20energy-based%20policies%202017"
        },
        {
            "id": "9",
            "entry": "[9] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arxiv:1801.01290, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01290"
        },
        {
            "id": "10",
            "entry": "[10] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, A. Eslami, M. Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arxiv:1707.02286, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02286"
        },
        {
            "id": "11",
            "entry": "[11] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. arXiv:1709.06560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06560"
        },
        {
            "id": "12",
            "entry": "[12] R. Islam, P. Henderson, M. Gomrokchi, and D. Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arxiv:1708.04133, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04133"
        },
        {
            "id": "13",
            "entry": "[13] K. G. Jamieson, R. Nowak, and B. Recht. Query complexity of derivative-free optimization. pages 2672\u20132680, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jamieson%2C%20K.G.%20Nowak%2C%20R.%20Recht%2C%20B.%20Query%20complexity%20of%20derivative-free%20optimization%202012"
        },
        {
            "id": "14",
            "entry": "[14] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20T.P.%20Hunt%2C%20J.J.%20Pritzel%2C%20A.%20Heess%2C%20N.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "15",
            "entry": "[15] J. Matyas. Random optimization. Automation and Remote control, 26(2):246\u2013253, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Matyas%20Random%20optimization%20Automation%20and%20Remote%20control%20262246253%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Matyas%20Random%20optimization%20Automation%20and%20Remote%20control%20262246253%201965"
        },
        {
            "id": "16",
            "entry": "[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "17",
            "entry": "[17] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. Proceedings of the International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "18",
            "entry": "[18] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, W. Paul, M. I. Jordan, and I. Stoica. Ray: A distributed framework for emerging ai applications. arxiv:1712.05889, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05889"
        },
        {
            "id": "19",
            "entry": "[19] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. arxiv:1708.02596, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02596"
        },
        {
            "id": "20",
            "entry": "[20] Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527\u2013566, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Spokoiny%2C%20V.%20Random%20gradient-free%20minimization%20of%20convex%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Spokoiny%2C%20V.%20Random%20gradient-free%20minimization%20of%20convex%20functions%202017"
        },
        {
            "id": "21",
            "entry": "[21] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen, T. Asfour, P. Abbeel, and M. Andrychowicz. Parameter space noise for exploration. arxiv:1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "22",
            "entry": "[22] A. Rajeswaran, K. Lowrey, E. Todorov, and S. Kakade. Towards generalization and simplicity in continuous control. Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajeswaran%2C%20A.%20Lowrey%2C%20K.%20Todorov%2C%20E.%20Kakade%2C%20S.%20Towards%20generalization%20and%20simplicity%20in%20continuous%20control%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajeswaran%2C%20A.%20Lowrey%2C%20K.%20Todorov%2C%20E.%20Kakade%2C%20S.%20Towards%20generalization%20and%20simplicity%20in%20continuous%20control%202017"
        },
        {
            "id": "23",
            "entry": "[23] T. Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arxiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "24",
            "entry": "[24] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "25",
            "entry": "[25] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Moritz%2C%20P.%20Levine%2C%20S.%20Jordan%2C%20M.%20High-dimensional%20continuous%20control%20using%20generalized%20advantage%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Moritz%2C%20P.%20Levine%2C%20S.%20Jordan%2C%20M.%20High-dimensional%20continuous%20control%20using%20generalized%20advantage%20estimation%202015"
        },
        {
            "id": "26",
            "entry": "[26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arxiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "27",
            "entry": "[27] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. Proceedings of the International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Lever%2C%20G.%20Heess%2C%20N.%20Degris%2C%20T.%20Deterministic%20policy%20gradient%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Lever%2C%20G.%20Heess%2C%20N.%20Degris%2C%20T.%20Deterministic%20policy%20gradient%20algorithms%202014"
        },
        {
            "id": "28",
            "entry": "[28] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "29",
            "entry": "[29] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "30",
            "entry": "[30] S. Tu and B. Recht. Least-squares temporal difference learning for the linear quadratic regulator. arxiv:1712.08642, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08642"
        },
        {
            "id": "31",
            "entry": "[31] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. Sample efficient actor-critic with experience replay. Interational Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Bapst%2C%20V.%20Heess%2C%20N.%20Mnih%2C%20V.%20Sample%20efficient%20actor-critic%20with%20experience%20replay%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Bapst%2C%20V.%20Heess%2C%20N.%20Mnih%2C%20V.%20Sample%20efficient%20actor-critic%20with%20experience%20replay%202016"
        },
        {
            "id": "32",
            "entry": "[32] Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. Advances in Neural Information Processing Systems, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Mansimov%2C%20E.%20Liao%2C%20S.%20Grosse%2C%20R.%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Y.%20Mansimov%2C%20E.%20Liao%2C%20S.%20Grosse%2C%20R.%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017"
        }
    ]
}
