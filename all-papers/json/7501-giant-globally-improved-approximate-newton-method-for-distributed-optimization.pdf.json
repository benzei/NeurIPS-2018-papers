{
    "filename": "7501-giant-globally-improved-approximate-newton-method-for-distributed-optimization.pdf",
    "metadata": {
        "title": "GIANT: Globally Improved Approximate Newton Method for Distributed Optimization",
        "author": "Shusen Wang, Farbod Roosta-Khorasani, Peng Xu, Michael W. Mahoney",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7501-giant-globally-improved-approximate-newton-method-for-distributed-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "For distributed computing environment, we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, which is sent to the main driver. The main driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically, we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further, and in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and, empirically, demonstrate the superior performance of GIANT."
    },
    "keywords": [
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "newton method",
            "url": "https://en.wikipedia.org/wiki/newton_method"
        },
        {
            "term": "conjugate gradient",
            "url": "https://en.wikipedia.org/wiki/conjugate_gradient"
        },
        {
            "term": "GIANT",
            "url": "https://en.wikipedia.org/wiki/GIANT"
        }
    ],
    "highlights": [
        "The large-scale nature of many modern \u201cbig-data\u201d problems, arising routinely in science, engineering, financial markets, Internet and social media, etc., poses significant computational as well as storage challenges for machine learning procedures",
        "We propose a Globally Improved Approximate NewTon (GIANT) method and establish its theoretical and empirical properties as follows",
        "In each round of Globally Improved ANT, the local computational cost of a worker machine is O q \u00b7 nnz(At,i) , where q is the number of conjugate gradient iterations specified by the users and typically set to tens.\n3If the samples themselves are i.i.d. drawn from some distribution, a data-independent partition is equivalent to uniform sampling",
        "\u03972, the convergence rate of such inexact variant of Globally Improved ANT remains similar to the exact algorithm in which the local linear system is solved exactly",
        "We have proposed Globally Improved ANT, a practical Newton-type method, for empirical risk minimization in distributed computing environments",
        "Globally Improved ANT is guaranteed to converge to high precision in a small number of iterations, provided that the number of training samples, n, is sufficiently large, relative to dm, where d is the number of features and m is the number of partitions"
    ],
    "key_statements": [
        "The large-scale nature of many modern \u201cbig-data\u201d problems, arising routinely in science, engineering, financial markets, Internet and social media, etc., poses significant computational as well as storage challenges for machine learning procedures",
        "We propose a Globally Improved Approximate NewTon (GIANT) method and establish its theoretical and empirical properties as follows",
        "In each round of Globally Improved ANT, the local computational cost of a worker machine is O q \u00b7 nnz(At,i) , where q is the number of conjugate gradient iterations specified by the users and typically set to tens.\n3If the samples themselves are i.i.d. drawn from some distribution, a data-independent partition is equivalent to uniform sampling",
        "\u03972, the convergence rate of such inexact variant of Globally Improved ANT remains similar to the exact algorithm in which the local linear system is solved exactly",
        "We have proposed Globally Improved ANT, a practical Newton-type method, for empirical risk minimization in distributed computing environments",
        "Globally Improved ANT is guaranteed to converge to high precision in a small number of iterations, provided that the number of training samples, n, is sufficiently large, relative to dm, where d is the number of features and m is the number of partitions"
    ],
    "summary": [
        "The large-scale nature of many modern \u201cbig-data\u201d problems, arising routinely in science, engineering, financial markets, Internet and social media, etc., poses significant computational as well as storage challenges for machine learning procedures.",
        "Such overheads are increasingly exacerbated by the growing number of compute nodes in the network, limiting the scalability of any distributed optimization method that requires many communication-intensive iterations.",
        "\u039a some condition number these methods are designed to perform as much local computation as possible before making any communications across the network.",
        "Improved Approximate NewTon (GIANT) method and establish its improved theoretical convergence properties as compared with other similar second-order methods.",
        "We propose a Globally Improved Approximate NewTon (GIANT) method and establish its theoretical and empirical properties as follows.",
        "When the number of data points is much larger than the number of features, the theoretical convergence of GIANT enjoys significant improvement over other similar methods.",
        "In each round of GIANT, the local computational cost of a worker machine is O q \u00b7 nnz(At,i) , where q is the number of CG iterations specified by the users and typically set to tens.",
        "This is followed by local convergence properties of GIANT for more general non-quadratic loss in Section 4.2.",
        "The prior works, DANE, AIDE, and DiSCO, did not use the notation of incoherence; instead, they assume \u22072wlj |w=wt = ajaTj is upper bounded for all j \u2208 [n] and wt \u2208 Rd, where aj \u2208 Rd is the j-th row of At. Such an assumption is different from but has similar implication as our incoherence assumption; under either of the two assumptions, it can be shown that the Hessian matrix can be approximated using a subset of samples selected uniformly at random.",
        "For more general but smooth loss, GIANT has linear-quadratic local convergence, which is formally stated in Theorem 2 and Corollary 3.",
        "Note that in Theorem 2 the convergence depends on the condition numbers of the Hessian at every point.",
        "\u03972, the convergence rate of such inexact variant of GIANT remains similar to the exact algorithm in which the local linear system is solved exactly.",
        "We implement GIANT, Accelerated Gradient Descent (AGD) [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], Limited memory BFGS (LBFGS) [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], and Distributed Approximate NEwton (DANE) [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>] in Scala and Apache Spark [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>].",
        "We have proposed GIANT, a practical Newton-type method, for empirical risk minimization in distributed computing environments.",
        "GIANT is guaranteed to converge to high precision in a small number of iterations, provided that the number of training samples, n, is sufficiently large, relative to dm, where d is the number of features and m is the number of partitions.",
        "Empirical studies showed the superior performance of GIANT as compared several other methods"
    ],
    "headline": "We show that Globally Improved ANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy preserving machine learning. IACR Cryptology ePrint Archive, 2017:281, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonawitz%2C%20Keith%20Ivanov%2C%20Vladimir%20Kreuter%2C%20Ben%20Antonio%20Marcedone%2C%20H.Brendan%20McMahan%20Practical%20secure%20aggregation%20for%20privacy%20preserving%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bonawitz%2C%20Keith%20Ivanov%2C%20Vladimir%20Kreuter%2C%20Ben%20Antonio%20Marcedone%2C%20H.Brendan%20McMahan%20Practical%20secure%20aggregation%20for%20privacy%20preserving%20machine%20learning%202017"
        },
        {
            "id": "2",
            "entry": "[2] Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009"
        },
        {
            "id": "3",
            "entry": "[3] Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406\u20135425, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Near-optimal%20signal%20recovery%20from%20random%20projections%3A%20Universal%20encoding%20strategies%3F%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Near-optimal%20signal%20recovery%20from%20random%20projections%3A%20Universal%20encoding%20strategies%3F%202006"
        },
        {
            "id": "4",
            "entry": "[4] Jeffrey Dean and Sanjay Ghemawat. MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107\u2013113, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20Jeffrey%20Ghemawat%2C%20Sanjay%20MapReduce%3A%20simplified%20data%20processing%20on%20large%20clusters%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20Jeffrey%20Ghemawat%2C%20Sanjay%20MapReduce%3A%20simplified%20data%20processing%20on%20large%20clusters%202008"
        },
        {
            "id": "5",
            "entry": "[5] Olivier Fercoq and Peter Richt\u00e1rik. Optimization in high dimensions via accelerated, parallel, and proximal coordinate descent. SIAM Review, 58(4):739\u2013771, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fercoq%2C%20Olivier%20Richt%C3%A1rik%2C%20Peter%20Optimization%20in%20high%20dimensions%20via%20accelerated%2C%20parallel%2C%20and%20proximal%20coordinate%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fercoq%2C%20Olivier%20Richt%C3%A1rik%2C%20Peter%20Optimization%20in%20high%20dimensions%20via%20accelerated%2C%20parallel%2C%20and%20proximal%20coordinate%20descent%202016"
        },
        {
            "id": "6",
            "entry": "[6] Alex Gittens and Michael W Mahoney. Revisiting the Nystr\u00f6m method for improved large-scale machine learning. Journal of Machine Learning Research, 17(1):3977\u20134041, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gittens%2C%20Alex%20Mahoney%2C%20Michael%20W.%20Revisiting%20the%20Nystr%C3%B6m%20method%20for%20improved%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gittens%2C%20Alex%20Mahoney%2C%20Michael%20W.%20Revisiting%20the%20Nystr%C3%B6m%20method%20for%20improved%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "7",
            "entry": "[7] Martin Jaggi, Virginia Smith, Martin Tak\u00e1c, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Tak%C3%A1c%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Tak%C3%A1c%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014"
        },
        {
            "id": "8",
            "entry": "[8] Jakub Konecny, H Brendan McMahan, Daniel Ramage, and Peter Richt\u00e1rik. Federated optimization: distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.02527"
        },
        {
            "id": "9",
            "entry": "[9] Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05492"
        },
        {
            "id": "10",
            "entry": "[10] Jason D Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang. Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity. arXiv preprint arXiv:1507.07595, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.07595"
        },
        {
            "id": "11",
            "entry": "[11] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "12",
            "entry": "[12] Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503\u2013528, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Dong%20C.%20Nocedal%2C%20Jorge%20On%20the%20limited%20memory%20BFGS%20method%20for%20large%20scale%20optimization%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Dong%20C.%20Nocedal%2C%20Jorge%20On%20the%20limited%20memory%20BFGS%20method%20for%20large%20scale%20optimization%201989"
        },
        {
            "id": "13",
            "entry": "[13] Ji Liu, Stephen J Wright, Christopher R\u00e9, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. Journal of Machine Learning Research, 16(285-322):1\u20135, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015"
        },
        {
            "id": "14",
            "entry": "[14] Yucheng Low, Danny Bickson, Joseph Gonzalez, Carlos Guestrin, Aapo Kyrola, and Joseph M. Hellerstein. Distributed GraphLab: A framework for machine learning and data mining in the cloud. Proceedings of the VLDB Endowment, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Low%2C%20Yucheng%20Bickson%2C%20Danny%20Gonzalez%2C%20Joseph%20Guestrin%2C%20Carlos%20Distributed%20GraphLab%3A%20A%20framework%20for%20machine%20learning%20and%20data%20mining%20in%20the%20cloud%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Low%2C%20Yucheng%20Bickson%2C%20Danny%20Gonzalez%2C%20Joseph%20Guestrin%2C%20Carlos%20Distributed%20GraphLab%3A%20A%20framework%20for%20machine%20learning%20and%20data%20mining%20in%20the%20cloud%202012"
        },
        {
            "id": "15",
            "entry": "[15] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan, Peter Richtarik, and Martin Takac. Adding vs. averaging in distributed primal-dual optimization. In International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Chenxin%20Smith%2C%20Virginia%20Jaggi%2C%20Martin%20Jordan%2C%20Michael%20Adding%20vs.%20averaging%20in%20distributed%20primal-dual%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Chenxin%20Smith%2C%20Virginia%20Jaggi%2C%20Martin%20Jordan%2C%20Michael%20Adding%20vs.%20averaging%20in%20distributed%20primal-dual%20optimization%202015"
        },
        {
            "id": "16",
            "entry": "[16] Dhruv Mahajan, Nikunj Agrawal, S Sathiya Keerthi, S Sundararajan, and L\u00e9on Bottou. An efficient distributed learning algorithm based on effective local functional approximations. arXiv preprint arXiv:1310.8418, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1310.8418"
        },
        {
            "id": "17",
            "entry": "[17] Dhruv Mahajan, S Sathiya Keerthi, S Sundararajan, and L\u00e9on Bottou. A parallel SGD method with strong convergence. arXiv preprint arXiv:1311.0636, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1311.0636"
        },
        {
            "id": "18",
            "entry": "[18] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202017"
        },
        {
            "id": "19",
            "entry": "[19] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al. MLlib: machine learning in Apache Spark. Journal of Machine Learning Research, 17(34):1\u20137, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20Xiangrui%20Bradley%2C%20Joseph%20Yavuz%2C%20Burak%20Sparks%2C%20Evan%20MLlib%3A%20machine%20learning%20in%20Apache%20Spark%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20Xiangrui%20Bradley%2C%20Joseph%20Yavuz%2C%20Burak%20Sparks%2C%20Evan%20MLlib%3A%20machine%20learning%20in%20Apache%20Spark%202016"
        },
        {
            "id": "20",
            "entry": "[20] Ion Necoara and Dragos Clipici. Parallel random coordinate descent method for composite minimization: Convergence analysis and error bounds. SIAM Journal on Optimization, 26(1):197\u2013226, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Necoara%2C%20Ion%20Clipici%2C%20Dragos%20Parallel%20random%20coordinate%20descent%20method%20for%20composite%20minimization%3A%20Convergence%20analysis%20and%20error%20bounds%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Necoara%2C%20Ion%20Clipici%2C%20Dragos%20Parallel%20random%20coordinate%20descent%20method%20for%20composite%20minimization%3A%20Convergence%20analysis%20and%20error%20bounds%202016"
        },
        {
            "id": "21",
            "entry": "[21] A.S. Nemirovskii and D.B. Yudin. Problem Complexity and Method Efficiency in Optimization. A Wiley-Interscience publication. Wiley, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovskii%2C%20A.S.%20Yudin%2C%20D.B.%20Problem%20Complexity%20and%20Method%20Efficiency%20in%20Optimization.%20A%20Wiley-Interscience%20publication%201983"
        },
        {
            "id": "22",
            "entry": "[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In Soviet Mathematics Doklady, volume 27, pages 372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983"
        },
        {
            "id": "23",
            "entry": "[23] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "24",
            "entry": "[24] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nocedal%2C%20Jorge%20Wright%2C%20Stephen%20Numerical%20optimization%202006"
        },
        {
            "id": "25",
            "entry": "[25] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205\u2013245, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilanci%2C%20Mert%20Wainwright%2C%20Martin%20J.%20Newton%20sketch%3A%20A%20near%20linear-time%20optimization%20algorithm%20with%20linear-quadratic%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilanci%2C%20Mert%20Wainwright%2C%20Martin%20J.%20Newton%20sketch%3A%20A%20near%20linear-time%20optimization%20algorithm%20with%20linear-quadratic%20convergence%202017"
        },
        {
            "id": "26",
            "entry": "[26] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NIPS), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "27",
            "entry": "[27] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: a lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems (NIPS), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20a%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20a%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "28",
            "entry": "[28] Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. On variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in Neural Information Processing Systems (NIPS). 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20On%20variance%20reduction%20in%20stochastic%20gradient%20descent%20and%20its%20asynchronous%20variants%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20On%20variance%20reduction%20in%20stochastic%20gradient%20descent%20and%20its%20asynchronous%20variants%202015"
        },
        {
            "id": "29",
            "entry": "[29] Sashank J Reddi, Jakub Konecny, Peter Richt\u00e1rik, Barnab\u00e1s P\u00f3cz\u00f3s, and Alex Smola. AIDE: fast and communication efficient distributed optimization. arXiv preprint arXiv:1608.06879, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06879"
        },
        {
            "id": "30",
            "entry": "[30] Peter Richt\u00e1rik and Martin Tak\u00e1c. Distributed coordinate descent method for learning with big data. Journal of Machine Learning Research, 17(1):2657\u20132681, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20Distributed%20coordinate%20descent%20method%20for%20learning%20with%20big%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20Distributed%20coordinate%20descent%20method%20for%20learning%20with%20big%20data%202016"
        },
        {
            "id": "31",
            "entry": "[31] Peter Richt\u00e1rik and Martin Tak\u00e1vc. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1-2):433\u2013484, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1vc%2C%20Martin%20Parallel%20coordinate%20descent%20methods%20for%20big%20data%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1vc%2C%20Martin%20Parallel%20coordinate%20descent%20methods%20for%20big%20data%20optimization%202016"
        },
        {
            "id": "32",
            "entry": "[32] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled Newton methods I: globally convergent algorithms. arXiv preprint arXiv:1601.04737, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.04737"
        },
        {
            "id": "33",
            "entry": "[33] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled Newton methods II: Local convergence rates. arXiv preprint arXiv:1601.04738, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.04738"
        },
        {
            "id": "34",
            "entry": "[34] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from theory to algorithms. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Ben-David%2C%20Shai%20Understanding%20machine%20learning%3A%20from%20theory%20to%20algorithms%202014"
        },
        {
            "id": "35",
            "entry": "[35] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In Annual Allerton Conference on Communication, Control, and Computing, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Srebro%2C%20Nathan%20Distributed%20stochastic%20optimization%20and%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Srebro%2C%20Nathan%20Distributed%20stochastic%20optimization%20and%20learning%202014"
        },
        {
            "id": "36",
            "entry": "[36] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate Newton-type method. In International conference on machine learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Zhang%2C%20Tong%20Communication-efficient%20distributed%20optimization%20using%20an%20approximate%20Newton-type%20method%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Zhang%2C%20Tong%20Communication-efficient%20distributed%20optimization%20using%20an%20approximate%20Newton-type%20method%202014"
        },
        {
            "id": "37",
            "entry": "[37] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learning. arXiv preprint arXiv:1705.10467, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10467"
        },
        {
            "id": "38",
            "entry": "[38] Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I Jordan, and Martin Jaggi. CoCoA: A general framework for communication-efficient distributed optimization. arXiv preprint arXiv:1611.02189, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02189"
        },
        {
            "id": "39",
            "entry": "[39] Shusen Wang, Alex Gittens, and Michael W. Mahoney. Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shusen%20Gittens%2C%20Alex%20Mahoney%2C%20Michael%20W.%20Sketched%20ridge%20regression%3A%20Optimization%20perspective%2C%20statistical%20perspective%2C%20and%20model%20averaging%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shusen%20Gittens%2C%20Alex%20Mahoney%2C%20Michael%20W.%20Sketched%20ridge%20regression%3A%20Optimization%20perspective%2C%20statistical%20perspective%2C%20and%20model%20averaging%202017"
        },
        {
            "id": "40",
            "entry": "[40] Shusen Wang, Luo Luo, and Zhihua Zhang. SPSD matrix approximation vis column selection: Theories, algorithms, and extensions. Journal of Machine Learning Research, 17(49):1\u201349, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shusen%20Luo%2C%20Luo%20Zhang%2C%20Zhihua%20SPSD%20matrix%20approximation%20vis%20column%20selection%3A%20Theories%2C%20algorithms%2C%20and%20extensions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shusen%20Luo%2C%20Luo%20Zhang%2C%20Zhihua%20SPSD%20matrix%20approximation%20vis%20column%20selection%3A%20Theories%2C%20algorithms%2C%20and%20extensions%202016"
        },
        {
            "id": "41",
            "entry": "[41] Shusen Wang, Farbod Roosta-Khorasani, Peng Xu, and Michael W. Mahoney. GIANT: Globally improved approximate Newton method for distributed optimization. arXiv:1709.03528, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03528"
        },
        {
            "id": "42",
            "entry": "[42] Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher R\u00e9, and Michael W Mahoney. Sub-sampled Newton methods with non-uniform sampling. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Peng%20Yang%2C%20Jiyan%20Roosta-Khorasani%2C%20Farbod%20R%C3%A9%2C%20Christopher%20Sub-sampled%20Newton%20methods%20with%20non-uniform%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Peng%20Yang%2C%20Jiyan%20Roosta-Khorasani%2C%20Farbod%20R%C3%A9%2C%20Christopher%20Sub-sampled%20Newton%20methods%20with%20non-uniform%20sampling%202016"
        },
        {
            "id": "43",
            "entry": "[43] Tianbao Yang. Trading computation for communication: distributed stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems (NIPS), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Tianbao%20Trading%20computation%20for%20communication%3A%20distributed%20stochastic%20dual%20coordinate%20ascent%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Tianbao%20Trading%20computation%20for%20communication%3A%20distributed%20stochastic%20dual%20coordinate%20ascent%202013"
        },
        {
            "id": "44",
            "entry": "[44] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and Ion Stoica. Spark: Cluster computing with working sets. HotCloud, 10(10-10):95, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaharia%2C%20Matei%20Chowdhury%2C%20Mosharaf%20Franklin%2C%20Michael%20J.%20Shenker%2C%20Scott%20Spark%3A%20Cluster%20computing%20with%20working%20sets%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaharia%2C%20Matei%20Chowdhury%2C%20Mosharaf%20Franklin%2C%20Michael%20J.%20Shenker%2C%20Scott%20Spark%3A%20Cluster%20computing%20with%20working%20sets%202010"
        },
        {
            "id": "45",
            "entry": "[45] Yuchen Zhang and Xiao Lin. DiSCO: distributed optimization for self-concordant empirical loss. In International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20DiSCO%3A%20distributed%20optimization%20for%20self-concordant%20empirical%20loss%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20DiSCO%3A%20distributed%20optimization%20for%20self-concordant%20empirical%20loss%202015"
        },
        {
            "id": "46",
            "entry": "[46] Shun Zheng, Fen Xia, Wei Xu, and Tong Zhang. A general distributed dual coordinate optimization framework for regularized loss minimization. arXiv preprint arXiv:1604.03763, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.03763"
        },
        {
            "id": "47",
            "entry": "[47] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In Advances in Neural Information Processing Systems (NIPS), 2010. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Weimer%2C%20Markus%20Li%2C%20Lihong%20Smola%2C%20Alex%20J.%20Parallelized%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Weimer%2C%20Markus%20Li%2C%20Lihong%20Smola%2C%20Alex%20J.%20Parallelized%20stochastic%20gradient%20descent%202010"
        }
    ]
}
