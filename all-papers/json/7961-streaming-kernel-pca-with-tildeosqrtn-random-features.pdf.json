{
    "filename": "7961-streaming-kernel-pca-with-tildeosqrtn-random-features.pdf",
    "metadata": {
        "title": "Streaming Kernel PCA with $\\tilde{O}(\\sqrt{n})$ Random Features",
        "author": "Md Enayat Ullah, Poorya Mianjy, Teodor Vanislavov Marinov, Raman Arora",
        "date": 1998,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7961-streaming-kernel-pca-with-tildeosqrtn-random-features.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the statistical and computational aspects of kernel principal component ana\u221alysis using O( n log (n))\nrandom features\nFourier suffice features and show that under mild assumptions, to achieve O(1/ 2) sample complexity. Furthermore, we give a memory efficient streaming algorithm based on classical Oja\u2019s algorithm that achieves this rate."
    },
    "keywords": [
        {
            "term": "principal component analysis",
            "url": "https://en.wikipedia.org/wiki/principal_component_analysis"
        },
        {
            "term": "nystr\u00f6m method",
            "url": "https://en.wikipedia.org/wiki/nystr\u00f6m_method"
        }
    ],
    "highlights": [
        "Kernel methods represent an important class of machine learning algorithms that simultaneously enjoy strong theoretical guarantees as well as empirical performance",
        "We focus on Kernel Principal Component Analysis (KPCA) [Sch\u00f6lkopf et al, 1998], which is a popular technique for unsupervised nonlinear representation learning",
        "We study kernel principal component analysis as stochastic optimization problem and show that under mild distributional assumptions, for a wide range of kernels, the em\u221apirical risk minimizer (E\u221aRM) in the random feature space converges in objective as O(1/ n) whenever m = \u03a9(k n log (n)), with overall runtime of",
        "We overcome a key challenge associated with kernel principal component analysis using random features which is to ensure that the output of the algorithm corresponds to a projection operator in the Reproducing Kernel Hilbert Space",
        "In order to better understand the computational benefits of using random features, we consider the Kernel Principal Component Analysis problem in a streaming setting, where at each iteration, the algorithm is provided with a fresh sample drawn i.i.d. from the underlying distribution and is required to output a solution based on the samples observed so far"
    ],
    "key_statements": [
        "Kernel methods represent an important class of machine learning algorithms that simultaneously enjoy strong theoretical guarantees as well as empirical performance",
        "We focus on Kernel Principal Component Analysis (KPCA) [Sch\u00f6lkopf et al, 1998], which is a popular technique for unsupervised nonlinear representation learning",
        "We study kernel principal component analysis as stochastic optimization problem and show that under mild distributional assumptions, for a wide range of kernels, the em\u221apirical risk minimizer (E\u221aRM) in the random feature space converges in objective as O(1/ n) whenever m = \u03a9(k n log (n)), with overall runtime of",
        "We overcome a key challenge associated with kernel principal component analysis using random features which is to ensure that the output of the algorithm corresponds to a projection operator in the Reproducing Kernel Hilbert Space",
        "In order to better understand the computational benefits of using random features, we consider the Kernel Principal Component Analysis problem in a streaming setting, where at each iteration, the algorithm is provided with a fresh sample drawn i.i.d. from the underlying distribution and is required to output a solution based on the samples observed so far"
    ],
    "summary": [
        "Kernel methods represent an important class of machine learning algorithms that simultaneously enjoy strong theoretical guarantees as well as empirical performance.",
        "1. We study kernel PCA as stochastic optimization problem and show that under mild distributional assumptions, for a wide range of kernels, the em\u221apirical risk minimizer (E\u221aRM) in the random feature space converges in objective as O(1/ n) whenever m = \u03a9(k n log (n)), with overall runtime of",
        "2. We propose a stochastic approximation algorithm based on classical Oja\u2019s updates on random features which enjoys the same statistical guarantees as the ERM above but with better runtime and space requirements.",
        "3. We overcome a key challenge associated with kernel PCA using random features which is to ensure that the output of the algorithm corresponds to a projection operator in the RKHS.",
        "4. In order to better understand the computational benefits of using random features, we consider the KPCA problem in a streaming setting, where at each iteration, the algorithm is provided with a fresh sample drawn i.i.d. from the underlying distribution and is required to output a solution based on the samples observed so far.",
        "At the kernel approximation using random features as an estimation problem in the space of square integrable functions, where we appeal to recent results in local Rademacher complexity [<a class=\"ref-link\" id=\"cMassart_2000_a\" href=\"#rMassart_2000_a\">Massart, 2000</a>, Bartlett et al, 2002, <a class=\"ref-link\" id=\"cBlanchard_et+al_2007_a\" href=\"#rBlanchard_et+al_2007_a\">Blanchard et al, 2007</a>] to yield faster rates.",
        "Given a vector space X , let PXk denote the set of rank-k projection operators on X .",
        "Let C1 denote the random linear operator on L2(X , \u03c1) given by C1 = z\u03c9 \u2297\u03c1 z\u03c9.",
        "The above mappings allow us to embed operators in the common space, i.e., HS(\u03c1) and to bound estimation and approximation errors.",
        "An ESL is an algorithm which returns a projection onto a k-dimensional subspace such that the angle between the subspace and the space spanned by the top k eigenvectors of Cm decays at a sub-linear rate with the number of samples.",
        "Bounding the Approximation error: Using simple algebraic manipulations, we can show that the approximation error is exactly the error incurred by the ERM in estimating the top k eigenfunctions of the kernel integral operator L using m samples drawn from \u03c0.",
        "Empirical risk minimization (RF-ERM) and Oja\u2019s algorithm (RF-Oja) are used with random features to verify the theoretical results presented in Corollary 4.4.",
        "As guaranteed by Corollary 4.4, the output of both algorithms will converge to a projection operator as more training samples are introduced."
    ],
    "headline": "We study the statistical and computational aspects of kernel principal component ana\u221alysis using O)",
    "reference_links": [
        {
            "id": "Allen-Zhu_0000_a",
            "entry": "Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap-free, and near-optimal rate. arXiv preprint arXiv:1607.07837, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1607.07837"
        },
        {
            "id": "Allen-Zhu_0000_b",
            "entry": "Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet without agonizing pain. In Advances in Neural Information Processing Systems, pages 974\u2013982, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Lazysvd%3A%20Even%20faster%20svd%20decomposition%20yet%20without%20agonizing%20pain",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Lazysvd%3A%20Even%20faster%20svd%20decomposition%20yet%20without%20agonizing%20pain"
        },
        {
            "id": "Aronszajn_1950_a",
            "entry": "Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337\u2013404, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950"
        },
        {
            "id": "Arora_et+al_2013_a",
            "entry": "Raman Arora, Andy Cotter, and Nati Srebro. Stochastic optimization of PCA with capped MSG. In Advances in Neural Information Processing Systems, NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Raman%20Cotter%2C%20Andy%20Srebro%2C%20Nati%20Stochastic%20optimization%20of%20PCA%20with%20capped%20MSG%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Raman%20Cotter%2C%20Andy%20Srebro%2C%20Nati%20Stochastic%20optimization%20of%20PCA%20with%20capped%20MSG%202013"
        },
        {
            "id": "Blanchard_et+al_2007_a",
            "entry": "Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel principal component analysis. Machine Learning, 66(2-3):259\u2013294, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blanchard%2C%20Gilles%20Bousquet%2C%20Olivier%20Zwald%2C%20Laurent%20Statistical%20properties%20of%20kernel%20principal%20component%20analysis%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blanchard%2C%20Gilles%20Bousquet%2C%20Olivier%20Zwald%2C%20Laurent%20Statistical%20properties%20of%20kernel%20principal%20component%20analysis%202007"
        },
        {
            "id": "Drineas_2005_a",
            "entry": "Petros Drineas and Michael W Mahoney. On the nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning. journal of machine learning research, 6(Dec):2153\u20132175, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20On%20the%20nystr%C3%B6m%20method%20for%20approximating%20a%20gram%20matrix%20for%20improved%20kernel-based%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20On%20the%20nystr%C3%B6m%20method%20for%20approximating%20a%20gram%20matrix%20for%20improved%20kernel-based%20learning%202005"
        },
        {
            "id": "Fine_2001_a",
            "entry": "Shai Fine and Katya Scheinberg. Efficient svm training using low-rank kernel representations. Journal of Machine Learning Research, 2(Dec):243\u2013264, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fine%2C%20Shai%20Scheinberg%2C%20Katya%20Efficient%20svm%20training%20using%20low-rank%20kernel%20representations%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fine%2C%20Shai%20Scheinberg%2C%20Katya%20Efficient%20svm%20training%20using%20low-rank%20kernel%20representations%202001"
        },
        {
            "id": "Ge_et+al_2017_a",
            "entry": "Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00708"
        },
        {
            "id": "Ghashami_et+al_2016_a",
            "entry": "Mina Ghashami, Daniel J Perry, and Jeff Phillips. Streaming kernel principal component analysis. In Artificial Intelligence and Statistics, pages 1365\u20131374, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghashami%2C%20Mina%20Perry%2C%20Daniel%20J.%20Phillips%2C%20Jeff%20Streaming%20kernel%20principal%20component%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghashami%2C%20Mina%20Perry%2C%20Daniel%20J.%20Phillips%2C%20Jeff%20Streaming%20kernel%20principal%20component%20analysis%202016"
        },
        {
            "id": "Liberty_2013_a",
            "entry": "Edo Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 581\u2013588. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liberty%2C%20Edo%20Simple%20and%20deterministic%20matrix%20sketching%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liberty%2C%20Edo%20Simple%20and%20deterministic%20matrix%20sketching%202013"
        },
        {
            "id": "Lopez-Paz_et+al_2014_a",
            "entry": "David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Ghahramani, and Bernhard Sch\u00f6lkopf. Randomized nonlinear component analysis. arXiv preprint arXiv:1402.0119, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0119"
        },
        {
            "id": "Massart_2000_a",
            "entry": "Pascal Massart. Some applications of concentration inequalities to statistics. In Annales-Faculte des Sciences Toulouse Mathematiques, volume 9, pages 245\u2013303. Universit\u00e9 Paul Sabatier, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Massart%2C%20Pascal%20Some%20applications%20of%20concentration%20inequalities%20to%20statistics%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Massart%2C%20Pascal%20Some%20applications%20of%20concentration%20inequalities%20to%20statistics%202000"
        },
        {
            "id": "Mianjy_2018_a",
            "entry": "Poorya Mianjy and Raman Arora. Stochastic PCA with l 2 and l 1 regularization. In International Conference on Machine Learning, pages 3528\u20133536, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mianjy%2C%20Poorya%20Arora%2C%20Raman%20Stochastic%20PCA%20with%20l%202%20and%20l%201%20regularization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mianjy%2C%20Poorya%20Arora%2C%20Raman%20Stochastic%20PCA%20with%20l%202%20and%20l%201%20regularization%202018"
        },
        {
            "id": "Oja_1982_a",
            "entry": "Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical biology, 15(3):267\u2013273, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oja%2C%20Erkki%20Simplified%20neuron%20model%20as%20a%20principal%20component%20analyzer%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oja%2C%20Erkki%20Simplified%20neuron%20model%20as%20a%20principal%20component%20analyzer%201982"
        },
        {
            "id": "Rahimi_2007_a",
            "entry": "Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177\u20131184, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "Rahimi_2009_a",
            "entry": "Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in neural information processing systems, pages 1313\u20131320, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Weighted%20sums%20of%20random%20kitchen%20sinks%3A%20Replacing%20minimization%20with%20randomization%20in%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Weighted%20sums%20of%20random%20kitchen%20sinks%3A%20Replacing%20minimization%20with%20randomization%20in%20learning%202009"
        },
        {
            "id": "Reed_1972_a",
            "entry": "M Reed and B Simon. Methods of modern mathematical physics i: Functional analysis (academic, new york). Google Scholar, page 151, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20M.%20Simon%2C%20B.%20Methods%20of%20modern%20mathematical%20physics%20i%3A%20Functional%20analysis%201972"
        },
        {
            "id": "Rudi_2017_a",
            "entry": "Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pages 3218\u20133228, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "Rudin_2017_a",
            "entry": "Walter Rudin. Fourier analysis on groups. Courier Dover Publications, 2017. Bernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299\u20131319, 1998. Dino Sejdinovic and Arthur Gretton. What is an rkhs?, 2012. John Shawe-Taylor, Christopher KI Williams, Nello Cristianini, and Jaz Kandola. On the eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory, 51(7):2510\u20132522, 2005. Alex J Smola and Bernhard Sch\u00f6lkopf. Learning with kernels, volume 4.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudin%2C%20Walter%20Fourier%20analysis%20on%20groups%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudin%2C%20Walter%20Fourier%20analysis%20on%20groups%202017"
        }
    ]
}
