{
    "filename": "7704-third-order-smoothness-helps-faster-stochastic-optimization-algorithms-for-finding-local-minima.pdf",
    "metadata": {
        "title": "Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima",
        "author": "Yaodong Yu, Pan Xu, Quanquan Gu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7704-third-order-smoothness-helps-faster-stochastic-optimization-algorithms-for-finding-local-minima.pdf"
        },
        "abstract": "We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently."
    },
    "keywords": [
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "global minimum",
            "url": "https://en.wikipedia.org/wiki/global_minimum"
        },
        {
            "term": "linear time",
            "url": "https://en.wikipedia.org/wiki/linear_time"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "We aim to design efficient stochastic optimization algorithms that can find an approximate local minimum of (1.1), i.e., an (\u270f, \u270fH )-second-order stationary point x defined as follows krf (x)k2",
        "We show that the third-order smoothness of the nonconvex function can lead to a faster escape from saddle points in the stochastic optimization",
        "We present our main algorithm to find approximate local minima for nonconvex stochastic optimization problems, based on the negative curvature descent algorithms proposed in previous section",
        "We evaluate our algorithm FLASH-Stochastic (FLASH for short) together with the following stateof-the-art stochastic optimization algorithms for nonconvex problems: (1) stochastic gradient descent (SGD); (2) stochastic gradient descent with momentum (SGD-m); (3) noisy stochastic gradient descent (NSGD) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]; (4) Stochastically Controlled Stochastic Gradient (SCSG) [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]; (5) NEgative-curvature-Originated-fromNoise (Neon) [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]; (6) NEgative-curvature-Originated-from-Noise 2 (Neon2) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We investigated the benefit of third-order smoothness of nonconvex objective functions in stochastic optimization",
        "Based on the proposed negative curvature descent algorithm, we further proposed a practical stochastic optimization algorithm with improved run time complexity that finds local minima for stochastic nonconvex optimization problems"
    ],
    "key_statements": [
        "We aim to design efficient stochastic optimization algorithms that can find an approximate local minimum of (1.1), i.e., an (\u270f, \u270fH )-second-order stationary point x defined as follows krf (x)k2",
        "We are interested in nonconvex optimization where the expected function f (x) is not convex. This kind of nonconvex optimization is ubiquitous in machine learning, especially deep learning [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We show that the third-order smoothness of the nonconvex function can lead to a faster escape from saddle points in the stochastic optimization",
        "We will present an algorithm for stochastic nonconvex optimization which exploits the benefits of third-order smoothness to escape from saddle points",
        "We present our main algorithm to find approximate local minima for nonconvex stochastic optimization problems, based on the negative curvature descent algorithms proposed in previous section",
        "It is worth noting that the runtime complexity of Algorithm 2 matches that of the state-of-the-art stochastic optimization algorithm (SCSG) [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] which only finds first-order stationary points but does not impose the third-order smoothness assumption",
        "We evaluate our algorithm FLASH-Stochastic (FLASH for short) together with the following stateof-the-art stochastic optimization algorithms for nonconvex problems: (1) stochastic gradient descent (SGD); (2) stochastic gradient descent with momentum (SGD-m); (3) noisy stochastic gradient descent (NSGD) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]; (4) Stochastically Controlled Stochastic Gradient (SCSG) [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]; (5) NEgative-curvature-Originated-fromNoise (Neon) [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]; (6) NEgative-curvature-Originated-from-Noise 2 (Neon2) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We investigated the benefit of third-order smoothness of nonconvex objective functions in stochastic optimization",
        "Based on the proposed negative curvature descent algorithm, we further proposed a practical stochastic optimization algorithm with improved run time complexity that finds local minima for stochastic nonconvex optimization problems"
    ],
    "summary": [
        "We aim to design efficient stochastic optimization algorithms that can find an approximate local minimum of (1.1), i.e., an (\u270f, \u270fH )-second-order stationary point x defined as follows krf (x)k2",
        "Note that our gradient complexity matches that of the state-of-the-art stochastic optimization algorithm SCSG [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] for finding first-order stationary points.",
        "In the general stochastic optimization setting, Allen-Zhu [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] proposed an algorithm named Npatasha2, which is find (\u270f, \u270f)-second-order based on variance reduction and negative curvature descent, and is able to stationary points with at most Oe(\u270f 7/2) stochastic gradient and stochastic",
        "We will present an algorithm for stochastic nonconvex optimization which exploits the benefits of third-order smoothness to escape from saddle points .",
        "By using either Oja\u2019s algorithm or Neon2online, there exists an algorithm, denoted by ApproxNCStochastic, which uses stochastic gradient evaluations or stochastic Hessian-vector product evaluations to find the negative curvature direction for general stochastic nonconvex optimization problem (1.1).",
        "As we cannot access the full objective function value in stochastic setting, we use a Rademacher variable (\u21e3 = 1 or \u21e3 = 1 with probability 1/2) in our algorithm to decide the direction of negative curvature descent step.",
        "We present our main algorithm to find approximate local minima for nonconvex stochastic optimization problems, based on the negative curvature descent algorithms proposed in previous section.",
        "To find the local minimum, we use SCSG [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], which is the state-of-the-art stochastic optimization algorithm, to find a first-order stationary point and apply Algorithm 1 to escape the saddle point using negative curvature direction.",
        "It is worth noting that the runtime complexity of Algorithm 2 matches that of the state-of-the-art stochastic optimization algorithm (SCSG) [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] which only finds first-order stationary points but does not impose the third-order smoothness assumption.",
        "We can see from Figure 1(c) and 1(d) that our algorithm FLASH converges faster with larger step sizes for negative curvature descent, which validates our theories on third-order smoothness can be helpful in the nonconvex stochastic optimization.",
        "We illustrated that third-order smoothness can help faster escape saddle points, by proposing a new negative curvature descent algorithms with improved theoretical guarantee.",
        "Based on the proposed negative curvature descent algorithm, we further proposed a practical stochastic optimization algorithm with improved run time complexity that finds local minima for stochastic nonconvex optimization problems."
    ],
    "headline": "We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01146"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arXiv:1708.08694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International Conference on Machine Learning, pages 699\u2013707, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster algorithms for matrix multiplicative weight updates. arXiv preprint arXiv:1701.01722, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.01722"
        },
        {
            "id": "5",
            "entry": "[5] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv preprint arXiv:1711.06673, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06673"
        },
        {
            "id": "6",
            "entry": "[6] Animashree Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in non-convex optimization. In Conference on Learning Theory, pages 81\u2013102, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Efficient%20approaches%20for%20escaping%20higher%20order%20saddle%20points%20in%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Efficient%20approaches%20for%20escaping%20higher%20order%20saddle%20points%20in%20non-convex%20optimization%202016"
        },
        {
            "id": "7",
            "entry": "[7] Yair Carmon and John C Duchi. Gradient descent efficiently finds the cubic-regularized non-convex newton step. arXiv preprint arXiv:1612.00547, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00547"
        },
        {
            "id": "8",
            "entry": "[8] Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization. arXiv preprint arXiv:1611.00756, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "9",
            "entry": "[9] Yair Carmon, Oliver Hinder, John C Duchi, and Aaron Sidford. \" convex until proven guilty\": Dimension-free acceleration of gradient descent on non-convex functions. arXiv preprint arXiv:1705.02766, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02766"
        },
        {
            "id": "10",
            "entry": "[10] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "11",
            "entry": "[11] Frank E Curtis and Daniel P Robinson. Exploiting negative curvature in deterministic and stochastic optimization. arXiv preprint arXiv:1703.00412, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00412"
        },
        {
            "id": "12",
            "entry": "[12] Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a worst-case iteration complexity of\\mathcal {O}(\\epsilon{-3/2}) for nonconvex optimization. Mathematical Programming, 162(1-2):1\u201332, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Curtis%2C%20Frank%20E.%20Robinson%2C%20Daniel%20P.%20Samadi%2C%20Mohammadreza%20A%20trust%20region%20algorithm%20with%20a%20worst-case%20iteration%20complexity%20of%5Cmathcal%20%7BO%7D%28%5Cepsilon%7B-3/2%7D%29%20for%20nonconvex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Curtis%2C%20Frank%20E.%20Robinson%2C%20Daniel%20P.%20Samadi%2C%20Mohammadreza%20A%20trust%20region%20algorithm%20with%20a%20worst-case%20iteration%20complexity%20of%5Cmathcal%20%7BO%7D%28%5Cepsilon%7B-3/2%7D%29%20for%20nonconvex%20optimization%202017"
        },
        {
            "id": "13",
            "entry": "[13] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. In Advances in neural information processing systems, pages 2933\u20132941, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014"
        },
        {
            "id": "14",
            "entry": "[14] Dan Garber, Elad Hazan, Chi Jin, Cameron Musco, Praneeth Netrapalli, Aaron Sidford, et al. Faster eigenvector computation via shift-and-invert preconditioning. In International Conference on Machine Learning, pages 2626\u20132634, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Musco%2C%20Cameron%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Musco%2C%20Cameron%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016"
        },
        {
            "id": "15",
            "entry": "[15] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797\u2013 842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "16",
            "entry": "[16] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "17",
            "entry": "[17] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155 (1-2):267\u2013305, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Zhang%2C%20Hongchao%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Zhang%2C%20Hongchao%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016"
        },
        {
            "id": "18",
            "entry": "[18] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM (JACM), 60(6):45, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hillar%2C%20Christopher%20J.%20Lim%2C%20Lek-Heng%20Most%20tensor%20problems%20are%20np-hard%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hillar%2C%20Christopher%20J.%20Lim%2C%20Lek-Heng%20Most%20tensor%20problems%20are%20np-hard%202013"
        },
        {
            "id": "19",
            "entry": "[19] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "20",
            "entry": "[20] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00887"
        },
        {
            "id": "21",
            "entry": "[21] Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10456"
        },
        {
            "id": "22",
            "entry": "[22] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "23",
            "entry": "[23] Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex optimization. arXiv preprint arXiv:1705.05933, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.05933"
        },
        {
            "id": "24",
            "entry": "[24] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015"
        },
        {
            "id": "25",
            "entry": "[25] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Nonconvex finite-sum optimization via scsg methods. arXiv preprint arXiv:1706.09156, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.09156"
        },
        {
            "id": "26",
            "entry": "[26] Kfir Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04831"
        },
        {
            "id": "27",
            "entry": "[27] Jorge J Mor\u00e9 and Danny C Sorensen. On the use of directions of negative curvature in a modified newton method. Mathematical Programming, 16(1):1\u201320, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mor%C3%A9%2C%20Jorge%20J.%20Sorensen%2C%20Danny%20C.%20On%20the%20use%20of%20directions%20of%20negative%20curvature%20in%20a%20modified%20newton%20method%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mor%C3%A9%2C%20Jorge%20J.%20Sorensen%2C%20Danny%20C.%20On%20the%20use%20of%20directions%20of%20negative%20curvature%20in%20a%20modified%20newton%20method%201979"
        },
        {
            "id": "28",
            "entry": "[28] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "29",
            "entry": "[29] Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pages 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "30",
            "entry": "[30] Sashank J Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov, and Alexander J Smola. A generic approach for escaping saddle points. arXiv preprint arXiv:1709.01434, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01434"
        },
        {
            "id": "31",
            "entry": "[31] Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic regularization for fast nonconvex optimization. arXiv preprint arXiv:1711.02838, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02838"
        },
        {
            "id": "32",
            "entry": "[32] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.",
            "arxiv_url": "https://arxiv.org/pdf/1011.3027"
        },
        {
            "id": "33",
            "entry": "[33] Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Newton-type methods for non-convex optimization under inexact hessian information. arXiv preprint arXiv:1708.07164, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07164"
        },
        {
            "id": "34",
            "entry": "[34] Yi Xu and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. arXiv preprint arXiv:1711.01944, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01944"
        },
        {
            "id": "35",
            "entry": "[35] Yaodong Yu, Difan Zou, and Quanquan Gu. Saving gradient and negative curvature computations: Finding local minima more efficiently. arXiv preprint arXiv:1712.03950, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03950"
        },
        {
            "id": "36",
            "entry": "[36] Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic variance-reduced cubic regularized Newton methods. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 5990\u20135999, 10\u201315 Jul 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Dongruo%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Stochastic%20variance-reduced%20cubic%20regularized%20Newton%20methods%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Dongruo%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Stochastic%20variance-reduced%20cubic%20regularized%20Newton%20methods%202018-07"
        }
    ]
}
