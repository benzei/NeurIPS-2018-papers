{
    "filename": "7935-gilbo-one-metric-to-measure-them-all.pdf",
    "metadata": {
        "title": "How much Do People Remember? Some Estimates of the Quantity of Learned Information in Long-term Memory",
        "author": "Thomas K. Landauer",
        "date": 2005,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7935-gilbo-one-metric-to-measure-them-all.pdf",
            "doi": "10.1207/s15516709cog1004_4"
        },
        "journal": "Cognitive Science",
        "volume": "10",
        "abstract": "We propose a simple, tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO (Generative Information Lower BOund). It offers a data-independent measure of the complexity of the learned latent variable description, giving the log of the effective description length. It is well-defined for both VAEs and GANs. We compute the GILBO for 800 GANs and VAEs each trained on four datasets (MNIST, FashionMNIST, CIFAR-10 and CelebA) and discuss the results.",
        "pages": "477-493"
    },
    "keywords": [
        {
            "term": "long term memory",
            "url": "https://en.wikipedia.org/wiki/long_term_memory"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "highlights": [
        "GANs (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) and VAEs (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\"><a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Welling, 2014</a></a>) are the most popular latent variable generative models because of their relative ease of training and high expressivity",
        "Optimizing the GILBO for the parameters of the encoder gives a lower bound on the true generative mutual information in the GAN or VAE",
        "We\u2019ve observed that GILBO gives us different information than is currently available in sample-quality based metrics like FID, both signifying a qualitative difference in the performance of most GANs on MNIST versus richer datasets, as well as being able to distinguish between GANs with qualitatively different latent representations even if they have the same FID score",
        "GANs show a wider array of complexities in their trained generative models",
        "These complexities cannot be discerned by existing sample-quality based metrics, but would have important implications for any use of the trained generative models for auxiliary tasks, such as compression or representation learning",
        "Since GANs are implemented as a feed forward neural network, the fact that we can measure finite and distinct values for the GILBO for different architectures suggest not only are they fundamentally not perfectly invertible, but the degree of invertibility is an interesting signal of the complexity of the learned generative model"
    ],
    "key_statements": [
        "GANs (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) and VAEs (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\"><a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Welling, 2014</a></a>) are the most popular latent variable generative models because of their relative ease of training and high expressivity",
        "Optimizing the GILBO for the parameters of the encoder gives a lower bound on the true generative mutual information in the GAN or VAE",
        "All three checkpoints for CelebA have the same FID score of 49, making them each competitive amongst the GANs studied; they have GILBO values that span a range of 63 nats (91 bits), which indicates a massive difference in model complexity",
        "We\u2019ve observed that GILBO gives us different information than is currently available in sample-quality based metrics like FID, both signifying a qualitative difference in the performance of most GANs on MNIST versus richer datasets, as well as being able to distinguish between GANs with qualitatively different latent representations even if they have the same FID score",
        "GANs show a wider array of complexities in their trained generative models",
        "These complexities cannot be discerned by existing sample-quality based metrics, but would have important implications for any use of the trained generative models for auxiliary tasks, such as compression or representation learning",
        "Since GANs are implemented as a feed forward neural network, the fact that we can measure finite and distinct values for the GILBO for different architectures suggest not only are they fundamentally not perfectly invertible, but the degree of invertibility is an interesting signal of the complexity of the learned generative model",
        "We believe it is important to consider the complexity in information-theoretic terms of the generative models we train, and the GILBO offers a relatively cheap comparative measure"
    ],
    "summary": [
        "GANs (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\"><a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a></a>) and VAEs (<a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\"><a class=\"ref-link\" id=\"cKingma_2014_a\" href=\"#rKingma_2014_a\">Kingma & Welling, 2014</a></a>) are the most popular latent variable generative models because of their relative ease of training and high expressivity.",
        "Optimizing the GILBO for the parameters of the encoder gives a lower bound on the true generative mutual information in the GAN or VAE.",
        "While the GILBO is a valid lower bound regardless of the accuracy of the learned encoder, its utility as a metric naturally requires it to be comparable across models.",
        "These approximations are valid lower bounds, and demonstrate that our amortized GILBO calculations above might be off by as much as a factor of 2 in their values from the true generative information, but again highlights that the comparisons between different models appear to be real.",
        "All three checkpoints for CelebA have the same FID score of 49, making them each competitive amongst the GANs studied; they have GILBO values that span a range of 63 nats (91 bits), which indicates a massive difference in model complexity.",
        "We\u2019ve observed that GILBO gives us different information than is currently available in sample-quality based metrics like FID, both signifying a qualitative difference in the performance of most GANs on MNIST versus richer datasets, as well as being able to distinguish between GANs with qualitatively different latent representations even if they have the same FID score.",
        "In an information-theoretic sense we cannot distinguish what GANs with the best FIDs are doing from models that are limited to making some local deformations of the training set.",
        "These complexities cannot be discerned by existing sample-quality based metrics, but would have important implications for any use of the trained generative models for auxiliary tasks, such as compression or representation learning.",
        "Since GANs are implemented as a feed forward neural network, the fact that we can measure finite and distinct values for the GILBO for different architectures suggest not only are they fundamentally not perfectly invertible, but the degree of invertibility is an interesting signal of the complexity of the learned generative model.",
        "One could imagine adding the GILBO as an auxiliary objective to ordinary GAN training, though as a lower bound, it may not prove useful for helping to keep the generative information low.",
        "We believe it is important to consider the complexity in information-theoretic terms of the generative models we train, and the GILBO offers a relatively cheap comparative measure."
    ],
    "headline": "Tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO ",
    "reference_links": [
        {
            "id": "Agakov_2006_a",
            "entry": "Felix Vsevolodovich Agakov. Variational Information Maximization in Stochastic Environments. PhD thesis, University of Edinburgh, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agakov%2C%20Felix%20Vsevolodovich%20Variational%20Information%20Maximization%20in%20Stochastic%20Environments%202006"
        },
        {
            "id": "Alemi_et+al_2017_a",
            "entry": "Alex Alemi, Ben Poole, Ian Fischer, Josh Dillon, Rif A. Saurus, and Kevin Murphy. Fixing a Broken ELBO. ICML, 2017. URL https://arxiv.org/abs/1711.00464.",
            "url": "https://arxiv.org/abs/1711.00464",
            "arxiv_url": "https://arxiv.org/pdf/1711.00464"
        },
        {
            "id": "Arora_2017_a",
            "entry": "Sanjeev Arora and Yi Zhang. Do GANs actually learn the distribution? An empirical study. CoRR, abs/1706.08224, 2017. URL http://arxiv.org/abs/1706.08224.",
            "url": "http://arxiv.org/abs/1706.08224",
            "arxiv_url": "https://arxiv.org/pdf/1706.08224"
        },
        {
            "id": "Belghazi_et+al_2018_a",
            "entry": "Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Devon Hjelm, and Aaron Courville. Mutual Information Neural Estimation. In International Conference on Machine Learning, pp. 530\u2013539, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohamed%20Ishmael%20Belghazi%20Aristide%20Baratin%20Sai%20Rajeshwar%20Sherjil%20Ozair%20Yoshua%20Bengio%20Devon%20Hjelm%20and%20Aaron%20Courville%20Mutual%20Information%20Neural%20Estimation%20In%20International%20Conference%20on%20Machine%20Learning%20pp%20530539%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohamed%20Ishmael%20Belghazi%20Aristide%20Baratin%20Sai%20Rajeshwar%20Sherjil%20Ozair%20Yoshua%20Bengio%20Devon%20Hjelm%20and%20Aaron%20Courville%20Mutual%20Information%20Neural%20Estimation%20In%20International%20Conference%20on%20Machine%20Learning%20pp%20530539%202018"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. In NIPS, 2016. URL https://arxiv.org/pdf/1606.03657.pdf.",
            "url": "https://arxiv.org/pdf/1606.03657.pdf",
            "arxiv_url": "https://arxiv.org/pdf/1606.03657"
        },
        {
            "id": "Danihelka_et+al_2017_a",
            "entry": "I. Danihelka, B. Lakshminarayanan, B. Uria, D. Wierstra, and P. Dayan. Comparison of Maximum Likelihood and GAN-based training of Real NVPs. arXiv 1705.05263, 2017. URL https://arxiv.org/abs/1705.05263.",
            "url": "https://arxiv.org/abs/1705.05263",
            "arxiv_url": "https://arxiv.org/pdf/1705.05263"
        },
        {
            "id": "Gao_et+al_2017_a",
            "entry": "Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath. Estimating Mutual Information for Discrete-Continuous Mixtures. In NIPS, 2017. URL http://papers.nips.cc/paper/7180-estimating-mutual-information-for-discrete-continuous-mixtures.pdf.",
            "url": "http://papers.nips.cc/paper/7180-estimating-mutual-information-for-discrete-continuous-mixtures.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Weihao%20Kannan%2C%20Sreeram%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Estimating%20Mutual%20Information%20for%20Discrete-Continuous%20Mixtures%202017"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In NIPS, 2014. URL https://arxiv.org/abs/1406.2661.",
            "url": "https://arxiv.org/abs/1406.2661",
            "arxiv_url": "https://arxiv.org/pdf/1406.2661"
        },
        {
            "id": "Heusel_et+al_2017_a",
            "entry": "M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. arXiv 1806.08500, 2017. URL https://arxiv.org/abs/1806.08500.",
            "url": "https://arxiv.org/abs/1806.08500",
            "arxiv_url": "https://arxiv.org/pdf/1806.08500"
        },
        {
            "id": "Im_et+al_2018_a",
            "entry": "Daniel Jiwoong Im, He Ma, Graham Taylor, and Kristin Branson. Quantitatively evaluating GANs with divergences proposed for training. In International Conference on Learning Representations, 2018. URL https://arxiv.org/abs/1803.01045.",
            "url": "https://arxiv.org/abs/1803.01045",
            "arxiv_url": "https://arxiv.org/pdf/1803.01045"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. URL https://arxiv.org/abs/1412.6980.",
            "url": "https://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. URL https://arxiv.org/abs/1312.6114.",
            "url": "https://arxiv.org/abs/1312.6114",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Landauer_1986_a",
            "entry": "Thomas K. Landauer. How much Do People Remember? Some Estimates of the Quantity of Learned Information in Long-term Memory. Cognitive Science, 10(4):477\u2013493, 1986. doi: 10.1207/s15516709cog1004\\_4. URL https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1004_4.",
            "crossref": "https://dx.doi.org/10.1207/s15516709cog1004\\_4",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1207/s15516709cog1004%5C_4"
        },
        {
            "id": "Lipton_2017_a",
            "entry": "Zachary C Lipton and Subarna Tripathi. Precise Recovery of Latent Vectors From Generative Adversarial Networks, 2017. URL https://arxiv.org/abs/1702.04782.",
            "url": "https://arxiv.org/abs/1702.04782",
            "arxiv_url": "https://arxiv.org/pdf/1702.04782"
        },
        {
            "id": "Lucic_et+al_2017_a",
            "entry": "M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are GANs Created Equal? A Large-Scale Study. arXiv 1711.10337, 2017. URL https://arxiv.org/abs/1711.10337.",
            "url": "https://arxiv.org/abs/1711.10337",
            "arxiv_url": "https://arxiv.org/pdf/1711.10337"
        },
        {
            "id": "Marsh_2013_a",
            "entry": "Charles Marsh. Introduction to Continuous Entropy, 2013. URL http://www.crmarsh.com/static/pdf/Charles_Marsh_Continuous_Entropy.pdf.",
            "url": "http://www.crmarsh.com/static/pdf/Charles_Marsh_Continuous_Entropy.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marsh%2C%20Charles%20Introduction%20to%20Continuous%202013"
        },
        {
            "id": "Van_et+al_2018_a",
            "entry": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03748"
        },
        {
            "id": "Talts_et+al_2018_a",
            "entry": "S. Talts, M. Betancourt, D. Simpson, A. Vehtari, and A. Gelman. Validating Bayesian Inference Algorithms with Simulation-Based Calibration. arXiv 1804.06788, April 2018. URL https://arxiv.org/abs/1804.06788.",
            "url": "https://arxiv.org/abs/1804.06788",
            "arxiv_url": "https://arxiv.org/pdf/1804.06788"
        },
        {
            "id": "Tishby_2015_a",
            "entry": "N. Tishby and N. Zaslavsky. Deep Learning and the Information Bottleneck Principle. arXiv 1503.02406, 2015. URL https://arxiv.org/abs/1503.02406.",
            "url": "https://arxiv.org/abs/1503.02406",
            "arxiv_url": "https://arxiv.org/pdf/1503.02406"
        },
        {
            "id": "Wu_et+al_2017_a",
            "entry": "Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. In International Conference on Learning Representations, 2017. URL https://arxiv.org/abs/1611.04273.",
            "url": "https://arxiv.org/abs/1611.04273",
            "arxiv_url": "https://arxiv.org/pdf/1611.04273"
        }
    ]
}
