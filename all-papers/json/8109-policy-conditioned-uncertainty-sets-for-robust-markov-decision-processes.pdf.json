{
    "filename": "8109-policy-conditioned-uncertainty-sets-for-robust-markov-decision-processes.pdf",
    "metadata": {
        "date": 2018,
        "title": "Policy-Conditioned Uncertainty Sets for Robust Markov Decision Processes",
        "author": "Andrea Tirinzoni Politecnico di Milano andrea.tirinzoni@polimi.it",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8109-policy-conditioned-uncertainty-sets-for-robust-markov-decision-processes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "What policy should be employed in a Markov decision process with uncertain parameters? Robust optimization\u2019s answer to this question is to use rectangular uncertainty sets, which independently reflect available knowledge about each state, and then to obtain a decision policy that maximizes the expected reward for the worst-case decision process parameters from these uncertainty sets. While this rectangularity is convenient computationally and leads to tractable solutions, it often produces policies that are too conservative in practice, and does not facilitate knowledge transfer between portions of the state space or across related decision processes. In this work, we propose non-rectangular uncertainty sets that bound marginal moments of state-action features defined over entire trajectories through a decision process. This enables generalization to different portions of the state space while retaining appropriate uncertainty of the decision process. We develop algorithms for solving the resulting robust decision problems, which reduce to finding an optimal policy for a mixture of decision processes, and demonstrate the benefits of our approach experimentally."
    },
    "keywords": [
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "markov decision process",
            "url": "https://en.wikipedia.org/wiki/markov_decision_process"
        },
        {
            "term": "logistic regression",
            "url": "https://en.wikipedia.org/wiki/logistic_regression"
        },
        {
            "term": "markov decision processes",
            "url": "https://en.wikipedia.org/wiki/markov_decision_processes"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "maximum likelihood estimation",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimation"
        },
        {
            "term": "robust control",
            "url": "https://en.wikipedia.org/wiki/robust_control"
        },
        {
            "term": "robust optimization",
            "url": "https://en.wikipedia.org/wiki/robust_optimization"
        }
    ],
    "highlights": [
        "Robust control<br/><br/>The Markov Decision Process (MDP) with state set S and action set A povides a common formulation of discrete control problems",
        "In the remainder of this paper, we review existing robust control methods and directed information theory concepts in Section 2",
        "Choosing state transition dynamics to optimize a mixture of expected returns under different control policies, \u03c0 and \u03c0 (Definition 3)2 is an important subproblem arising from our formulation of robust control as a process estimation task with robustness properties and uncertainty sets defined by different control policies",
        "We have proposed a new approach to robust control based on causally conditioned probability distribution estimation that defines uncertainty sets using features of the interaction with the decision process with a different policy",
        "Though the solution to the corresponding robust control problem is non-Markovian, we show that it can be closely approximated by augmenting the typical Markovian robust Markov Decision Process formulation [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] with a continuous-valued \u201cbelief state\u201d that can be discretized",
        "We plan to extend our formulation to incorporate constraints that are obtained from multiple separate reference control policies. This could allow episodic reinforcement learning [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] where the robust optimal control policy is employed and updated based on the trajectories that are observed from its application"
    ],
    "key_statements": [
        "Robust control<br/><br/>The Markov Decision Process (MDP) with state set S and action set A povides a common formulation of discrete control problems",
        "In the remainder of this paper, we review existing robust control methods and directed information theory concepts in Section 2",
        "We evaluate our approach in Section 4 to demonstrate its comparative benefits over rectangular robust control methods",
        "Choosing state transition dynamics to optimize a mixture of expected returns under different control policies, \u03c0 and \u03c0 (Definition 3)2 is an important subproblem arising from our formulation of robust control as a process estimation task with robustness properties and uncertainty sets defined by different control policies",
        "Augmenting with the belief state of Equation (7), we prove that it is possible to compute a Markovian solution to the inner zero-sum game of Equation (6) and, to the optimization problem of",
        "For the sake of completeness, we show how such a policy can be used in a regular Markov Decision Process with dynamics \u03c4",
        "We have proposed a new approach to robust control based on causally conditioned probability distribution estimation that defines uncertainty sets using features of the interaction with the decision process with a different policy",
        "Though the solution to the corresponding robust control problem is non-Markovian, we show that it can be closely approximated by augmenting the typical Markovian robust Markov Decision Process formulation [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] with a continuous-valued \u201cbelief state\u201d that can be discretized",
        "We plan to extend our formulation to incorporate constraints that are obtained from multiple separate reference control policies. This could allow episodic reinforcement learning [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] where the robust optimal control policy is employed and updated based on the trajectories that are observed from its application"
    ],
    "summary": [
        "Robust control<br/><br/>The Markov Decision Process (MDP) with state set S and action set A povides a common formulation of discrete control problems.",
        "We consider state-action feature-based constraints over the marginals of stateaction sequences to define our uncertainty sets.",
        "We can define the robust control problem with constraints using marginal statistics of the state-action sequence to define the uncertainty set \u039e.",
        "Even when we consider a different constraint for each triple, our solution implicitly couples the transition probabilities of different state-action pairs and differs from a rectangular formulation which focuses on matching the conditional distribution p(St+1|St, At).",
        "We instead consider optimizing the control policy and state transition dynamics as causally conditioned probability distributions .",
        "We re-express the optimization problem of Definition 2 using processes\u2014the causally conditioned probabilities of Section 2.2\u2014for the control policy \u03c0(a1:T \u22121||s1:T \u22121) and state transition dynamics \u03c4 (s1:T ||a1:T \u22121), which conveniently combine the individual conditional probabilities over the stateaction sequence.",
        "Choosing state transition dynamics to optimize a mixture of expected returns under different control policies, \u03c0 and \u03c0 (Definition 3)2 is an important subproblem arising from our formulation of robust control as a process estimation task with robustness properties and uncertainty sets defined by different control policies.",
        "Given two control policies \u03c0 and \u03c0, and two reward functions R and R, the mixed objective optimization problem seeks state transition dynamics \u03c4 that minimizes a mixture of these weighted by \u03b8 \u2265 0 : min\u03c4 {\u03b8\u03c1R(\u03c0, \u03c4 ) + (1 \u2212 \u03b8)\u03c1R(\u03c0, \u03c4 )}.",
        "Any such method is required to repeatedly solve the inner minimax problem of Equation (6) as specified in the previous section, obtaining (\u03c0\u2217, \u03c4 \u2217), compute the feature expectations of the reference policy \u03c0 under \u03c4 \u2217, and use these to update \u03c9.",
        "We empirically evaluate our robust approach for control using uncertainty sets defined by marginal state-action statistics.",
        "We compare our marginallyconstrained approach (MC) to three other methods for estimating the state-transition dynamics: (1) a supervised approach using logistic regression (LR); (2) a robust MDP with s,a-rectangular uncertainty sets (RECT); and (3) a simple maximum likelihood estimation (MLE) of the conditional transition probabilities for all state-action pairs.",
        "We have proposed a new approach to robust control based on causally conditioned probability distribution estimation that defines uncertainty sets using features of the interaction with the decision process with a different policy.",
        "Incorporating more sophisticated ideas for solving POMDPs using belief state compression will likely be required, since discretizing the belief space scales poorly with the number of different reference policies"
    ],
    "headline": "We propose non-rectangular uncertainty sets that bound marginal moments of state-action features defined over entire trajectories through a decision process",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780\u2013798, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nilim%2C%20A.%20Ghaoui%2C%20L.El%20Robust%20control%20of%20Markov%20decision%20processes%20with%20uncertain%20transition%20matrices%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nilim%2C%20A.%20Ghaoui%2C%20L.El%20Robust%20control%20of%20Markov%20decision%20processes%20with%20uncertain%20transition%20matrices%202005"
        },
        {
            "id": "2",
            "entry": "[2] Grani Adiwena Hanasusanto and Daniel Kuhn. Robust data-driven dynamic programming. In Advances in Neural Information Processing Systems, pages 827\u2013835, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanasusanto%2C%20Grani%20Adiwena%20Kuhn%2C%20Daniel%20Robust%20data-driven%20dynamic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanasusanto%2C%20Grani%20Adiwena%20Kuhn%2C%20Daniel%20Robust%20data-driven%20dynamic%20programming%202013"
        },
        {
            "id": "3",
            "entry": "[3] Wolfram Wiesemann, Daniel Kuhn, and Ber\u00e7 Rustem. Robust markov decision processes. Mathematics of Operations Research, 38(1):153\u2013183, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolfram%20Wiesemann%2C%20Daniel%20Kuhn%20Rustem%2C%20Ber%C3%A7%20Robust%20markov%20decision%20processes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolfram%20Wiesemann%2C%20Daniel%20Kuhn%20Rustem%2C%20Ber%C3%A7%20Robust%20markov%20decision%20processes%202013"
        },
        {
            "id": "4",
            "entry": "[4] Shie Mannor, Ofir Mebel, and Huan Xu. Lightning does not strike twice: Robust mdps with coupled uncertainty. arXiv preprint arXiv:1206.4643, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.4643"
        },
        {
            "id": "5",
            "entry": "[5] Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):257\u2013280, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyengar%2C%20Garud%20N.%20Robust%20dynamic%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iyengar%2C%20Garud%20N.%20Robust%20dynamic%20programming%202005"
        },
        {
            "id": "6",
            "entry": "[6] Yann Le Tallec. Robust, Risk-Sensitive, and Data-driven Control of Markov Decision Processes. PhD thesis, MIT, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yann%20Le%20Tallec.%20Robust%2C%20Risk-Sensitive%2C%20and%20Data-driven%20Control%20of%20Markov%20Decision%20Processes%202007"
        },
        {
            "id": "7",
            "entry": "[7] Marek Petrik, Mohammad Ghavamzadeh, and Yinlam Chow. Safe Policy Improvement by Minimizing Robust Baseline Regret. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petrik%2C%20Marek%20Ghavamzadeh%2C%20Mohammad%20Chow%2C%20Yinlam%20Safe%20Policy%20Improvement%20by%20Minimizing%20Robust%20Baseline%20Regret%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petrik%2C%20Marek%20Ghavamzadeh%2C%20Mohammad%20Chow%2C%20Yinlam%20Safe%20Policy%20Improvement%20by%20Minimizing%20Robust%20Baseline%20Regret%202016"
        },
        {
            "id": "8",
            "entry": "[8] Erick Delage. Distributionally Robust Optimization under Moment Uncertainty with Application to Data-Driven Problems. 00(0):1\u201326, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Delage%2C%20Erick%20Distributionally%20Robust%20Optimization%20under%20Moment%20Uncertainty%20with%20Application%20to%20Data-Driven%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Delage%2C%20Erick%20Distributionally%20Robust%20Optimization%20under%20Moment%20Uncertainty%20with%20Application%20to%20Data-Driven%202000"
        },
        {
            "id": "9",
            "entry": "[9] Huan Xu and Shie Mannor. Distributionally robust markov decision processes. In Advances in Neural Information Processing Systems, pages 2505\u20132513, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huan%20Mannor%2C%20Shie%20Distributionally%20robust%20markov%20decision%20processes%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huan%20Mannor%2C%20Shie%20Distributionally%20robust%20markov%20decision%20processes%202010"
        },
        {
            "id": "10",
            "entry": "[10] Wolfram Wiesemann, Daniel Kuhn, and Melvyn Sim. Distributionally robust convex optimization. Operations Research, 62(6):1358\u20131376, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiesemann%2C%20Wolfram%20Kuhn%2C%20Daniel%20Sim%2C%20Melvyn%20Distributionally%20robust%20convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiesemann%2C%20Wolfram%20Kuhn%2C%20Daniel%20Sim%2C%20Melvyn%20Distributionally%20robust%20convex%20optimization%202014"
        },
        {
            "id": "11",
            "entry": "[11] Marek Petrik and Dharmashankar Subramanian. RAAM : The benefits of robustness in approximating aggregated MDPs in reinforcement learning. In Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petrik%2C%20Marek%20Subramanian%2C%20Dharmashankar%20RAAM%20%3A%20The%20benefits%20of%20robustness%20in%20approximating%20aggregated%20MDPs%20in%20reinforcement%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petrik%2C%20Marek%20Subramanian%2C%20Dharmashankar%20RAAM%20%3A%20The%20benefits%20of%20robustness%20in%20approximating%20aggregated%20MDPs%20in%20reinforcement%20learning%202014"
        },
        {
            "id": "12",
            "entry": "[12] Martin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc., 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20Martin%20L.%20Markov%20decision%20processes%3A%20Discrete%20stochastic%20dynamic%20programming%202005"
        },
        {
            "id": "13",
            "entry": "[13] G. Kramer. Directed Information for Channels with Feedback. PhD thesis, Swiss Federal Institute of Technology (ETH) Zurich, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kramer%2C%20G.%20Directed%20Information%20for%20Channels%20with%20Feedback%201998"
        },
        {
            "id": "14",
            "entry": "[14] Hans Marko. The bidirectional communication theory \u2013 a generalization of information theory. In IEEE Transactions on Communications, pages 1345\u20131351, 1973.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marko%2C%20Hans%20The%20bidirectional%20communication%20theory%20%E2%80%93%20a%20generalization%20of%20information%20theory%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marko%2C%20Hans%20The%20bidirectional%20communication%20theory%20%E2%80%93%20a%20generalization%20of%20information%20theory%201973"
        },
        {
            "id": "15",
            "entry": "[15] James L. Massey. Causality, feedback and directed information. In Proc. IEEE International Symposium on Information Theory and Its Applications, pages 27\u201330, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Massey%2C%20James%20L.%20Causality%2C%20feedback%20and%20directed%20information%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Massey%2C%20James%20L.%20Causality%2C%20feedback%20and%20directed%20information%201990"
        },
        {
            "id": "16",
            "entry": "[16] Haim H. Permuter, Young-Han Kim, and Tsachy Weissman. On directed information and gambling. In Proc. IEEE International Symposium on Information Theory, pages 1403\u20131407, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Permuter%2C%20Haim%20H.%20Kim%2C%20Young-Han%20Weissman%2C%20Tsachy%20On%20directed%20information%20and%20gambling%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Permuter%2C%20Haim%20H.%20Kim%2C%20Young-Han%20Weissman%2C%20Tsachy%20On%20directed%20information%20and%20gambling%202008"
        },
        {
            "id": "17",
            "entry": "[17] S. Tatikonda. Control under Communication Constraints. PhD thesis, Massachusetts Institute of Technology, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tatikonda%2C%20S.%20Control%20under%20Communication%20Constraints%202000"
        },
        {
            "id": "18",
            "entry": "[18] G. Kramer. Capacity results for the discrete memoryless network. Proc. IEEE Transactions on Information Theory, 49(1):4\u201321, Jan 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kramer%2C%20G.%20Capacity%20results%20for%20the%20discrete%20memoryless%20network%202003-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kramer%2C%20G.%20Capacity%20results%20for%20the%20discrete%20memoryless%20network%202003-01"
        },
        {
            "id": "19",
            "entry": "[19] Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling interaction via the principle of maximum causal entropy. In Proc. International Conference on Machine Learning, pages 1255\u20131262, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Modeling%20interaction%20via%20the%20principle%20of%20maximum%20causal%20entropy%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Modeling%20interaction%20via%20the%20principle%20of%20maximum%20causal%20entropy%202010"
        },
        {
            "id": "20",
            "entry": "[20] Bita Analui and Georg Ch Pflug. On distributionally robust multiperiod stochastic optimization. Computational Management Science, 11(3):197\u2013220, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Analui%2C%20Bita%20Pflug%2C%20Georg%20Ch%20On%20distributionally%20robust%20multiperiod%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Analui%2C%20Bita%20Pflug%2C%20Georg%20Ch%20On%20distributionally%20robust%20multiperiod%20stochastic%20optimization%202014"
        },
        {
            "id": "21",
            "entry": "[21] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. International Conference on Machine Learning, pages 1\u20138, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "22",
            "entry": "[22] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proc. AAAI Conference on Artificial Intelligence, pages 1433\u20131438, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        },
        {
            "id": "23",
            "entry": "[23] Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust optimization.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Zhaolin%20Hong%2C%20L.Jeff%20Kullback-leibler%20divergence%20constrained%20distributionally%20robust%20optimization"
        },
        {
            "id": "24",
            "entry": "[24] B. D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, Carnegie Mellon University, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20B.D.%20Modeling%20Purposeful%20Adaptive%20Behavior%20with%20the%20Principle%20of%20Maximum%20Causal%20Entropy%202010"
        },
        {
            "id": "25",
            "entry": "[25] Jun\u2019ichi Kazama and Jun\u2019ichi Tsujii. Evaluation and extension of maximum entropy models with inequality constraints. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 137\u2013144. Association for Computational Linguistics, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kazama%2C%20Jun%E2%80%99ichi%20Tsujii%2C%20Jun%E2%80%99ichi%20Evaluation%20and%20extension%20of%20maximum%20entropy%20models%20with%20inequality%20constraints%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kazama%2C%20Jun%E2%80%99ichi%20Tsujii%2C%20Jun%E2%80%99ichi%20Evaluation%20and%20extension%20of%20maximum%20entropy%20models%20with%20inequality%20constraints%202003"
        },
        {
            "id": "26",
            "entry": "[26] Miroslav Dud\u00edk, Steven J. Phillips, and Robert E. Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. J. Mach. Learn. Res., 8:1217\u20131260, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dud%C3%ADk%2C%20Miroslav%20Phillips%2C%20Steven%20J.%20Schapire%2C%20Robert%20E.%20Maximum%20entropy%20density%20estimation%20with%20generalized%20regularization%20and%20an%20application%20to%20species%20distribution%20modeling%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dud%C3%ADk%2C%20Miroslav%20Phillips%2C%20Steven%20J.%20Schapire%2C%20Robert%20E.%20Maximum%20entropy%20density%20estimation%20with%20generalized%20regularization%20and%20an%20application%20to%20species%20distribution%20modeling%202007"
        },
        {
            "id": "27",
            "entry": "[27] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-Based Batch Mode Reinforcement Learning. Journal of Machine Learning Research, 6(1):503\u2013556, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-Based%20Batch%20Mode%20Reinforcement%20Learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-Based%20Batch%20Mode%20Reinforcement%20Learning%202005"
        },
        {
            "id": "28",
            "entry": "[28] M. Kery and M. Schaub. Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective. Elsevier Science, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kery%2C%20M.%20Schaub%2C%20M.%20Bayesian%20Population%20Analysis%20using%20WinBUGS%3A%20A%20Hierarchical%20Perspective%202011"
        },
        {
            "id": "29",
            "entry": "[29] A B Philpott, V de Matos, and V L De Matos. Dynamic sampling algorithms for multi-stage stochastic programs with risk aversion. European Journal of Operations Research, 218(2):470\u2013 483, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Philpott%2C%20A.B.%20de%20Matos%2C%20V.%20Matos%2C%20V.L.De%20Dynamic%20sampling%20algorithms%20for%20multi-stage%20stochastic%20programs%20with%20risk%20aversion%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Philpott%2C%20A.B.%20de%20Matos%2C%20V.%20Matos%2C%20V.L.De%20Dynamic%20sampling%20algorithms%20for%20multi-stage%20stochastic%20programs%20with%20risk%20aversion%202012"
        },
        {
            "id": "30",
            "entry": "[30] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201"
        },
        {
            "id": "31",
            "entry": "[31] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Convex%20optimization%202004"
        }
    ]
}
