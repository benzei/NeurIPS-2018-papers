{
    "filename": "8253-complex-gated-recurrent-neural-networks.pdf",
    "metadata": {
        "title": "Complex Gated Recurrent Neural Networks",
        "author": "Moritz Wolter, Angela Yao",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8253-complex-gated-recurrent-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction."
    },
    "keywords": [
        {
            "term": "complex representation",
            "url": "https://en.wikipedia.org/wiki/complex_representation"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        },
        {
            "term": "time series",
            "url": "https://en.wikipedia.org/wiki/time_series"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "signal processing",
            "url": "https://en.wikipedia.org/wiki/signal_processing"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "state transition matrix",
            "url": "https://en.wikipedia.org/wiki/state_transition_matrix"
        },
        {
            "term": "short term memory",
            "url": "https://en.wikipedia.org/wiki/short_term_memory"
        },
        {
            "term": "long short-term memory",
            "url": "https://en.wikipedia.org/wiki/long_short-term_memory"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Recurrent neural networks (RNNs) are widely used for processing time series and sequential information",
        "We introduce a novel complex-gated recurrent unit; to the best of our knowledge, we are the first to explore such a structure using complex number representations",
        "We introduce our gates into the Recurrent neural networks in a similar fashion as the classic gated recurrent units [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]: zt = W + Vxt + b, (9)",
        "We find that gating has no impact for the memory problem, i.e. the gateless unitary RNN [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>] always converges, but is necessary for the adding problem",
        "We have proposed a novel complex gated recurrent unit which we use together with unitary state transition matrices to form a stable and fast to train recurrent neural network",
        "We optimize the state transition matrices on the Stiefel manifold, which we show to work well with the modReLU"
    ],
    "key_statements": [
        "Recurrent neural networks (RNNs) are widely used for processing time series and sequential information",
        "We introduce a novel complex-gated recurrent unit; to the best of our knowledge, we are the first to explore such a structure using complex number representations",
        "We introduce our gates into the Recurrent neural networks in a similar fashion as the classic gated recurrent units [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]: zt = W + Vxt + b, (9)",
        "We find that our complex gated RNN successfully solves both the memory problem as well as the adding problem",
        "We find that gating has no impact for the memory problem, i.e. the gateless unitary RNN [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>] always converges, but is necessary for the adding problem",
        "We note that over 20 runs, our complex gated RNN converged only on 15-16 runs; adding the gates introduces instabilities, we find the ability to solve the adding problem a reasonable trade-off",
        "We have proposed a novel complex gated recurrent unit which we use together with unitary state transition matrices to form a stable and fast to train recurrent neural network",
        "We optimize the state transition matrices on the Stiefel manifold, which we show to work well with the modReLU"
    ],
    "summary": [
        "Recurrent neural networks (RNNs) are widely used for processing time series and sequential information.",
        "Our proposed complex gated RNN builds on these works in that we use unitary state transition matrices.",
        "We work with a split-complex approach, where real-valued non-linear activations are applied separately to the real and imaginary parts of the complex number.",
        "Fa is a point-wise non-linear activation function, and W and V are the hidden and input state transition matrices respectively.",
        "To map the complex state h into a real output or, we use a linear combination of the real and imaginary components, similar to [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], with Wo and bo as weights and offset: or = Wo[ (h) (h)] + bo.",
        "Both networks use complex representations and unitary state transition matrices.",
        "The standard GRU baseline, without any norm-preserving state transition matrices works very well on the adding problem; it marginally outperforms our cgRNN.",
        "All experiments use weight normalized recurrent weights, a cell size of nh = 80, and have networks with approximately 44k parameters; to keep approximately the same number of parameters, we set nh = 140 for the uRNN and two independent gates each with nh = 90 for the real free real case.",
        "We compare the cgRNN to a free real variant, which is the most similar architecture in R, i.e.normalized hidden transition matrices, same gate formulation, and two independently real-valued versions of Equations 11 and 12.",
        "We compare the bounded Hirose tanh non-linearity versus the unbounded modReLU in our cgRNN in Figure 3 and discover a strong interaction effect from the norm-preservation.",
        "We find that optimizing on the Stiefel manifold to preserve norms for the state transition matrices significantly improves learning, regardless of the non-linearity.",
        "In both the memory and the adding problem, keeping the state transition matrices unitary ensures faster and smoother convergence of the learning curve.",
        "Without unitary state transition matrices, the bounded tanh non-linearity, i.e.the conventional choice is better than the unbounded modReLU.",
        "We have proposed a novel complex gated recurrent unit which we use together with unitary state transition matrices to form a stable and fast to train recurrent neural network.",
        "Our complex gated RNN achieves state-of-the-art performance on the adding problem while remaining competitive on the memory problem.",
        "The experimental success of the cgRNN leads us to believe that complex representations have significant potential and advocate for their use not only in recurrent networks but in deep learning as a whole."
    ],
    "headline": "We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Shah%2C%20A.%20Bengio%2C%20Y.%20Unitary%20evolution%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Shah%2C%20A.%20Bengio%2C%20Y.%20Unitary%20evolution%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "2",
            "entry": "[2] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Trans. on Neural Networks, 5(2):157\u2013166, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Simard%2C%20P.%20Frasconi%2C%20P.%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Simard%2C%20P.%20Frasconi%2C%20P.%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994"
        },
        {
            "id": "3",
            "entry": "[3] N. Benvenuto and F. Piazza. On the complex backpropagation algorithm. IEEE Trans. Signal Processing, 40(4):967\u2013969, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benvenuto%2C%20N.%20Piazza%2C%20F.%20On%20the%20complex%20backpropagation%20algorithm%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benvenuto%2C%20N.%20Piazza%2C%20F.%20On%20the%20complex%20backpropagation%20algorithm%201992"
        },
        {
            "id": "4",
            "entry": "[4] K. Cho, B. van Merri\u00ebnboer, \u00c7. G\u00fcl\u00e7ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20K.%20van%20Merri%C3%ABnboer%2C%20B.%20G%C3%BCl%C3%A7ehre%2C%20%C3%87.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20RNN%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20K.%20van%20Merri%C3%ABnboer%2C%20B.%20G%C3%BCl%C3%A7ehre%2C%20%C3%87.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20RNN%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "5",
            "entry": "[5] I. Danihelka, G. Wayne, B. Uria, N. Kalchbrenner, and A. Graves. Associative long short-term memory. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Danihelka%20G%20Wayne%20B%20Uria%20N%20Kalchbrenner%20and%20A%20Graves%20Associative%20long%20shortterm%20memory%20In%20ICML%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Danihelka%20G%20Wayne%20B%20Uria%20N%20Kalchbrenner%20and%20A%20Graves%20Associative%20long%20shortterm%20memory%20In%20ICML%202016"
        },
        {
            "id": "6",
            "entry": "[6] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y.N. Dauphin. Convolutional sequence to sequence learning. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20J.%20Auli%2C%20M.%20Grangier%2C%20D.%20Yarats%2C%20D.%20Convolutional%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20J.%20Auli%2C%20M.%20Grangier%2C%20D.%20Yarats%2C%20D.%20Convolutional%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "7",
            "entry": "[7] G. Georgiou and C. Koutsougeras. Complex domain backpropagation. IEEE Trans. Circuits and systems II: Analog and Digital Signal Processing, 39(5):330\u2013334, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Georgiou%2C%20G.%20Koutsougeras%2C%20C.%20Complex%20domain%20backpropagation%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Georgiou%2C%20G.%20Koutsougeras%2C%20C.%20Complex%20domain%20backpropagation%201992"
        },
        {
            "id": "8",
            "entry": "[8] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "9",
            "entry": "[9] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning. MIT press Cambridge, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Bengio%2C%20Y.%20Deep%20learning%202016"
        },
        {
            "id": "10",
            "entry": "[10] N. Guberman. On complex valued convolutional neural networks. Technical report, The Hebrew University of Jerusalem Israel, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guberman%2C%20N.%20On%20complex%20valued%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "11",
            "entry": "[11] A. Hirose. Complex-valued neural networks: Advances and applications. John Wiley & Sons, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hirose%2C%20A.%20Complex-valued%20neural%20networks%3A%20Advances%20and%20applications%202013"
        },
        {
            "id": "12",
            "entry": "[12] S. Hochreiter and J. Schmidhuber. Long short term memory. Neural Computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short%20term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short%20term%20memory%201997"
        },
        {
            "id": "13",
            "entry": "[13] S. Hyland and G. R\u00e4tsch. Learning unitary operators with help from u(n). In AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyland%2C%20S.%20R%C3%A4tsch%2C%20G.%20Learning%20unitary%20operators%20with%20help%20from%20u%28n%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyland%2C%20S.%20R%C3%A4tsch%2C%20G.%20Learning%20unitary%20operators%20with%20help%20from%20u%28n%29%202017"
        },
        {
            "id": "14",
            "entry": "[14] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Trans. Pattern Analysis and Machine Intelligence, 36(7):1325\u20131339, jul 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ionescu%2C%20C.%20Papava%2C%20D.%20Olaru%2C%20V.%20Sminchisescu%2C%20C.%20Human3.%206M%3A%20Large%20Scale%20Datasets%20and%20Predictive%20Methods%20for%203D%20Human%20Sensing%20in%202014-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ionescu%2C%20C.%20Papava%2C%20D.%20Olaru%2C%20V.%20Sminchisescu%2C%20C.%20Human3.%206M%3A%20Large%20Scale%20Datasets%20and%20Predictive%20Methods%20for%203D%20Human%20Sensing%20in%202014-07"
        },
        {
            "id": "15",
            "entry": "[15] A. Jain, A.R. Zamir, S. Savarese, and A. Saxena. Structural-RNN: Deep learning on spatio-temporal graphs. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20A.%20Zamir%2C%20A.R.%20Savarese%2C%20S.%20Saxena%2C%20A.%20Structural-RNN%3A%20Deep%20learning%20on%20spatio-temporal%20graphs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20A.%20Zamir%2C%20A.R.%20Savarese%2C%20S.%20Saxena%2C%20A.%20Structural-RNN%3A%20Deep%20learning%20on%20spatio-temporal%20graphs%202016"
        },
        {
            "id": "16",
            "entry": "[16] L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Solja, and Y. Bengio. Gated orthogonal recurrent units: On learning to forget. In AAAI Workshops, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jing%2C%20L.%20Gulcehre%2C%20C.%20Peurifoy%2C%20J.%20Shen%2C%20Y.%20Gated%20orthogonal%20recurrent%20units%3A%20On%20learning%20to%20forget%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jing%2C%20L.%20Gulcehre%2C%20C.%20Peurifoy%2C%20J.%20Shen%2C%20Y.%20Gated%20orthogonal%20recurrent%20units%3A%20On%20learning%20to%20forget%202018"
        },
        {
            "id": "17",
            "entry": "[17] L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun, M. Tegmark, and M. Soljacic. Tunable efficient unitary neural networks (EUNN) and their application to RNNs. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jing%2C%20L.%20Shen%2C%20Y.%20Dubcek%2C%20T.%20Peurifoy%2C%20J.%20Soljacic.%20Tunable%20efficient%20unitary%20neural%20networks%20%28EUNN%29%20and%20their%20application%20to%20RNNs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jing%2C%20L.%20Shen%2C%20Y.%20Dubcek%2C%20T.%20Peurifoy%2C%20J.%20Soljacic.%20Tunable%20efficient%20unitary%20neural%20networks%20%28EUNN%29%20and%20their%20application%20to%20RNNs%202017"
        },
        {
            "id": "18",
            "entry": "[18] T. Kim and T. Adali. Complex backpropagation neural network using elementary transcendental activation functions. In ICASSP, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20T.%20Adali%2C%20T.%20Complex%20backpropagation%20neural%20network%20using%20elementary%20transcendental%20activation%20functions%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20T.%20Adali%2C%20T.%20Complex%20backpropagation%20neural%20network%20using%20elementary%20transcendental%20activation%20functions%202001"
        },
        {
            "id": "19",
            "entry": "[19] H. Koppula and A. Saxena. Anticipating human activities using object affordances for reactive robotic response. IEEE Trans. Pattern Analysis and Machine Intelligence, 38(1):14\u201329, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koppula%2C%20H.%20Saxena%2C%20A.%20Anticipating%20human%20activities%20using%20object%20affordances%20for%20reactive%20robotic%20response%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koppula%2C%20H.%20Saxena%2C%20A.%20Anticipating%20human%20activities%20using%20object%20affordances%20for%20reactive%20robotic%20response%202016"
        },
        {
            "id": "20",
            "entry": "[20] L. Kovar, M. Gleicher, and F. Pighin. Motion graphs. ACM Tran. Graphics (TOG), 21(3):473\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kovar%2C%20L.%20Gleicher%2C%20M.%20Pighin%2C%20F.%20Motion%20graphs%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kovar%2C%20L.%20Gleicher%2C%20M.%20Pighin%2C%20F.%20Motion%20graphs%202002"
        },
        {
            "id": "21",
            "entry": "[21] K. Kreutz-Delgado. The complex gradient operator and the CR-calculus. arXiv preprint:0906.4835, 2009.",
            "arxiv_url": "https://arxiv.org/pdf/0906.4835"
        },
        {
            "id": "22",
            "entry": "[22] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "23",
            "entry": "[23] H. Leung and S. Haykin. The complex backpropagation algorithm. IEEE Trans. Signal Processing, 39(9):2101\u20132104, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leung%2C%20H.%20Haykin%2C%20S.%20The%20complex%20backpropagation%20algorithm%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leung%2C%20H.%20Haykin%2C%20S.%20The%20complex%20backpropagation%20algorithm%201991"
        },
        {
            "id": "24",
            "entry": "[24] H. Li and T. Adali. Complex-valued adaptive signal processing using nonlinear functions. EURASIP Journal on Advances in Signal Processing, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20H.%20Adali%2C%20T.%20Complex-valued%20adaptive%20signal%20processing%20using%20nonlinear%20functions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20H.%20Adali%2C%20T.%20Complex-valued%20adaptive%20signal%20processing%20using%20nonlinear%20functions%202008"
        },
        {
            "id": "25",
            "entry": "[25] J. Liouville. Le\u00e7ons sur les fonctions doublement p\u00e9riodiques. Journal f\u00fcr die reine und angewandte Mathematik / Zeitschriftenband (1879), 1879.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liouville%2C%20J.%20Le%C3%A7ons%20sur%20les%20fonctions%20doublement%20p%C3%A9riodiques",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liouville%2C%20J.%20Le%C3%A7ons%20sur%20les%20fonctions%20doublement%20p%C3%A9riodiques"
        },
        {
            "id": "26",
            "entry": "[26] A. Maas, A. Hannun, and A. Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20A.%20Hannun%2C%20A.%20Ng%2C%20A.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models.%20In%20ICML%202013"
        },
        {
            "id": "27",
            "entry": "[27] D.P. Mandic and V.S.L. Goh. Complex valued nonlinear adaptive filters: noncircularity, widely linear and neural models. John Wiley & Sons, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandic%2C%20D.P.%20Goh%2C%20V.S.L.%20Complex%20valued%20nonlinear%20adaptive%20filters%3A%20noncircularity%2C%20widely%20linear%20and%20neural%20models%202009"
        },
        {
            "id": "28",
            "entry": "[28] J. Martinez, M.J. Black, and J. Romero. On human motion prediction using recurrent neural networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martinez%2C%20J.%20Black%2C%20M.J.%20Romero%2C%20J.%20On%20human%20motion%20prediction%20using%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martinez%2C%20J.%20Black%2C%20M.J.%20Romero%2C%20J.%20On%20human%20motion%20prediction%20using%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "29",
            "entry": "[29] T. Mikolov. Statistical language models based on neural networks. Technical report, Brno University of Technology, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Statistical%20language%20models%20based%20on%20neural%20networks%202012"
        },
        {
            "id": "30",
            "entry": "[30] V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20Boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20Boltzmann%20machines%202010"
        },
        {
            "id": "31",
            "entry": "[31] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20R.%20Mikolov%2C%20T.%20Bengio%2C%20Y.%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20R.%20Mikolov%2C%20T.%20Bengio%2C%20Y.%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "32",
            "entry": "[32] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio. Improving speech recognition by revising gated recurrent units. In INTERSPEECH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravanelli%2C%20M.%20Brakel%2C%20P.%20Omologo%2C%20M.%20Bengio%2C%20Y.%20Improving%20speech%20recognition%20by%20revising%20gated%20recurrent%20units%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravanelli%2C%20M.%20Brakel%2C%20P.%20Omologo%2C%20M.%20Bengio%2C%20Y.%20Improving%20speech%20recognition%20by%20revising%20gated%20recurrent%20units%202017"
        },
        {
            "id": "33",
            "entry": "[33] D.P. Reichert and T. Serre. Neuronal synchrony in complex-valued deep networks. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reichert%2C%20D.P.%20Serre%2C%20T.%20Neuronal%20synchrony%20in%20complex-valued%20deep%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reichert%2C%20D.P.%20Serre%2C%20T.%20Neuronal%20synchrony%20in%20complex-valued%20deep%20networks%202014"
        },
        {
            "id": "34",
            "entry": "[34] H. Tagare. Notes on optimization on stiefel manifolds. Technical report, Yale University, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tagare%2C%20H.%20Notes%20on%20optimization%20on%20stiefel%20manifolds%202011"
        },
        {
            "id": "35",
            "entry": "[35] J. Thickstun, Z. Harchaoui, and S.M. Kakade. Learning features of music from scratch. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thickstun%2C%20J.%20Harchaoui%2C%20Z.%20Kakade%2C%20S.M.%20Learning%20features%20of%20music%20from%20scratch%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thickstun%2C%20J.%20Harchaoui%2C%20Z.%20Kakade%2C%20S.M.%20Learning%20features%20of%20music%20from%20scratch%202017"
        },
        {
            "id": "36",
            "entry": "[36] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J. F. Santos, S. Mehri, N. Rostamzadeh, Y. Bengio, and C.J. Pal. Deep complex networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trabelsi%2C%20C.%20Bilaniuk%2C%20O.%20Zhang%2C%20Y.%20Serdyuk%2C%20D.%20Deep%20complex%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trabelsi%2C%20C.%20Bilaniuk%2C%20O.%20Zhang%2C%20Y.%20Serdyuk%2C%20D.%20Deep%20complex%20networks%202018"
        },
        {
            "id": "37",
            "entry": "[37] A Van Den Bos. Complex gradient and hessian. IEE Proceedings-Vision, Image and Signal Processing, 141(6):380\u2013382, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Van%20Den%20Bos%20Complex%20gradient%20and%20hessian%20IEE%20ProceedingsVision%20Image%20and%20Signal%20Processing%201416380382%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Van%20Den%20Bos%20Complex%20gradient%20and%20hessian%20IEE%20ProceedingsVision%20Image%20and%20Signal%20Processing%201416380382%201994"
        },
        {
            "id": "38",
            "entry": "[38] P. Virtue, S.X. Yu, and M. Lustig. Better than real: Complex-valued neural nets for MRI fingerprinting. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Virtue%2C%20P.%20Yu%2C%20S.X.%20Lustig%2C%20M.%20Better%20than%20real%3A%20Complex-valued%20neural%20nets%20for%20MRI%20fingerprinting",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Virtue%2C%20P.%20Yu%2C%20S.X.%20Lustig%2C%20M.%20Better%20than%20real%3A%20Complex-valued%20neural%20nets%20for%20MRI%20fingerprinting"
        },
        {
            "id": "40",
            "entry": "[40] S. Wisdom, T. Powers, J.R. Hershey, J. Le Roux, , and L. Atlas. Full-capacity unitary recurrent neural networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Full-capacity%20unitary%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Full-capacity%20unitary%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "41",
            "entry": "[41] A. Yao, J. Gall, and L. Van Gool. Coupled action recognition and pose estimation from multiple views. International Journal of Computer Vision, 100(1):16\u201337, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20A.%20Gall%2C%20J.%20Gool%2C%20L.Van%20Coupled%20action%20recognition%20and%20pose%20estimation%20from%20multiple%20views%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20A.%20Gall%2C%20J.%20Gool%2C%20L.Van%20Coupled%20action%20recognition%20and%20pose%20estimation%20from%20multiple%20views%202012"
        },
        {
            "id": "42",
            "entry": "[42] M. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and G. E. Hinton. On rectified linear units for speech processing. In ICASSP, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20M.%20Ranzato%2C%20M.%20Monga%2C%20R.%20Mao%2C%20M.%20On%20rectified%20linear%20units%20for%20speech%20processing%202013"
        }
    ]
}
