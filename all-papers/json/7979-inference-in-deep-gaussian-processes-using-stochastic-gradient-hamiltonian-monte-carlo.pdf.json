{
    "filename": "7979-inference-in-deep-gaussian-processes-using-stochastic-gradient-hamiltonian-monte-carlo.pdf",
    "metadata": {
        "title": "Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo",
        "author": "Marton Havasi, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Juan Jos\u00e9 Murillo-Fuentes",
        "date": 2013,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7979-inference-in-deep-gaussian-processes-using-stochastic-gradient-hamiltonian-monte-carlo.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs."
    },
    "keywords": [
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "likelihood function",
            "url": "https://en.wikipedia.org/wiki/likelihood_function"
        },
        {
            "term": "exact inference",
            "url": "https://en.wikipedia.org/wiki/exact_inference"
        },
        {
            "term": "Gaussian Processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_Processes"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "computational cost",
            "url": "https://en.wikipedia.org/wiki/computational_cost"
        },
        {
            "term": "MCEM",
            "url": "https://en.wikipedia.org/wiki/MCEM"
        },
        {
            "term": "new state",
            "url": "https://en.wikipedia.org/wiki/New_State"
        },
        {
            "term": "markov chain monte carlo",
            "url": "https://en.wikipedia.org/wiki/markov_chain_monte_carlo"
        },
        {
            "term": "Maximum Likelihood",
            "url": "https://en.wikipedia.org/wiki/Maximum_Likelihood"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        }
    ],
    "highlights": [
        "We provide the background on Gaussian Processes and Deep Gaussian Processes for regression and establishes the notation used in the paper. 2.1",
        "Stochastic Variation Inference Doubly Stochastic Variation Inference is an extension of Variational Inference to Deep Gaussian Processes [<a class=\"ref-link\" id=\"cSalimbeni_2017_a\" href=\"#rSalimbeni_2017_a\">Salimbeni and Deisenroth, 2017</a>] that approximates the posterior of the inducing outputs ul with independent multivariate Gaussians q = N",
        "We examine the approximate posterior samples generated by Stochastic Gradient Hamiltonian Monte Carlo for each inducing output, using the implementation of Stochastic Gradient Hamiltonian Monte Carlo for Deep Gaussian Processes described",
        "To produce the plot in the center of Figure 3, we plotted the predictive log-likelihood on the test set against the number of iterations of the algorithm to demonstrate the superior performance of Moving Window Monte Carlo Expectation Maximization over Monte Carlo Expectation Maximization",
        "This paper described and demonstrated an inference method new to Deep Gaussian Processes, Stochastic Gradient Hamiltonian Monte Carlo, that samples from the posterior distribution in the usual inducing point framework",
        "We described a novel Moving Window Monte Carlo Expectation Maximization algorithm that was demonstrably able to optimize hyperparameters in a fast and efficient manner"
    ],
    "key_statements": [
        "We provide the background on Gaussian Processes and Deep Gaussian Processes for regression and establishes the notation used in the paper. 2.1",
        "We propose Moving Window Monte Carlo Expectation Maximization, a novel method for obtaining the Maximum Likelihood (ML) estimate of the hyperparameters",
        "We provide the background on Gaussian Processes and Deep Gaussian Processes for regression and establishes the notation used in the paper.\n2.1",
        "Stochastic Variation Inference Doubly Stochastic Variation Inference is an extension of Variational Inference to Deep Gaussian Processes [<a class=\"ref-link\" id=\"cSalimbeni_2017_a\" href=\"#rSalimbeni_2017_a\">Salimbeni and Deisenroth, 2017</a>] that approximates the posterior of the inducing outputs ul with independent multivariate Gaussians q = N",
        "We examine the approximate posterior samples generated by Stochastic Gradient Hamiltonian Monte Carlo for each inducing output, using the implementation of Stochastic Gradient Hamiltonian Monte Carlo for Deep Gaussian Processes described",
        "Monte Carlo Expectation Maximization (MCEM) [<a class=\"ref-link\" id=\"cWei_1990_a\" href=\"#rWei_1990_a\">Wei and Tanner, 1990</a>] is the natural extension of the Expectation Maximization algorithm that works with posterior samples to obtain the Maximum Likelihood estimate of the hyperparameters",
        "We introduce a novel extension of Monte Carlo Expectation Maximization called Moving Window Monte Carlo Expectation Maximization",
        "To produce the plot in the center of Figure 3, we plotted the predictive log-likelihood on the test set against the number of iterations of the algorithm to demonstrate the superior performance of Moving Window Monte Carlo Expectation Maximization over Monte Carlo Expectation Maximization",
        "This section describes an extension to Deep Gaussian Processes that enables using a large number of inducing points without significantly impacting performance. This method is only applicable in the case of Doubly Stochastic Variation Inference, so we considered it as a baseline model in our experiments",
        "This paper described and demonstrated an inference method new to Deep Gaussian Processes, Stochastic Gradient Hamiltonian Monte Carlo, that samples from the posterior distribution in the usual inducing point framework",
        "We described a novel Moving Window Monte Carlo Expectation Maximization algorithm that was demonstrably able to optimize hyperparameters in a fast and efficient manner"
    ],
    "summary": [
        "We provide the background on Gaussian Processes and Deep Gaussian Processes for regression and establishes the notation used in the paper. 2.1.",
        "We apply an inference method new to DGPs, Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), a sampling method that accurately and efficiently captures the posterior distribution.",
        "Gaussian processes define a posterior distribution over functions f : RD \u2192 R given a set of input-output pairs x = {x1, .",
        "Stochastic Variation Inference DSVI is an extension of Variational Inference to DGPs [<a class=\"ref-link\" id=\"cSalimbeni_2017_a\" href=\"#rSalimbeni_2017_a\">Salimbeni and Deisenroth, 2017</a>] that approximates the posterior of the inducing outputs ul with independent multivariate Gaussians q = N.",
        "Sampling-based inference for Gaussian Processes In a related work, <a class=\"ref-link\" id=\"cHensman_et+al_2015_a\" href=\"#rHensman_et+al_2015_a\">Hensman et al [2015</a>] use Hybrid MC sampling in single layer GPs. They consider joint sampling of the GP hyperparameters and the inducing outputs.",
        "It uses a costly method, Bayesian Optimization, to tune the parameters of the sampler which further limits its applicability to DGPs. 3 Analysis of the Deep Gaussian Process Posterior",
        "We provide evidence that every regression dataset that we consider in this work results in a non-Gaussian posterior distribution.",
        "We examine the approximate posterior samples generated by SGHMC for each inducing output, using the implementation of SGHMC for DGPs described .",
        "SGHMC [<a class=\"ref-link\" id=\"cChen_et+al_2014_a\" href=\"#rChen_et+al_2014_a\">Chen et al, 2014</a>] is a Markov Chain Monte Carlo sampling method [<a class=\"ref-link\" id=\"cNeal_1993_a\" href=\"#rNeal_1993_a\">Neal, 1993</a>] for producing samples from the intractable posterior distribution of the inducing outputs p(u|y) purely from stochastic gradient estimates.",
        "The naive approach consisting in optimizing them as the sampler progresses fails because subsequent samples are highly correlated and as a result, the hyperparameters fit this moving, point-estimate of the posterior.",
        "Monte Carlo Expectation Maximization (MCEM) [<a class=\"ref-link\" id=\"cWei_1990_a\" href=\"#rWei_1990_a\">Wei and Tanner, 1990</a>] is the natural extension of the Expectation Maximization algorithm that works with posterior samples to obtain the Maximum Likelihood estimate of the hyperparameters.",
        "To produce the plot in the center of Figure 3, we plotted the predictive log-likelihood on the test set against the number of iterations of the algorithm to demonstrate the superior performance of Moving Window MCEM over MCEM.",
        "This paper described and demonstrated an inference method new to DGPs, SGHMC, that samples from the posterior distribution in the usual inducing point framework.",
        "This significantly improved performance on medium-large datasets at a reduced computational cost and established a new state-of-the-art for inference in DGPs. We want to thank Adria Gariga-Alonso, John Bronskill, Robert Peharz and Siddharth Swaroop for their helpful comments and thank Intel and EPSRC for their generous support."
    ],
    "headline": "We provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples",
    "reference_links": [
        {
            "id": "Abadi_et+al_2015_a",
            "entry": "M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Largescale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Brooks_et+al_2011_a",
            "entry": "S. Brooks, A. Gelman, G. Jones, and X.-L. Meng. Handbook of Markov chain Monte Carlo. CRC press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brooks%2C%20S.%20Gelman%2C%20A.%20Jones%2C%20G.%20Meng%2C%20X.-L.%20Handbook%20of%20Markov%20chain%20Monte%20Carlo%202011"
        },
        {
            "id": "Bui_et+al_2016_a",
            "entry": "T. Bui, D. Hernandez-Lobato, J. Hernandez-Lobato, Y. Li, and R. Turner. Deep Gaussian processes for regression using approximate expectation propagation. In International Conference on Machine Learning, pages 1472\u20131481, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bui%2C%20T.%20Hernandez-Lobato%2C%20D.%20Hernandez-Lobato%2C%20J.%20Li%2C%20Y.%20Deep%20Gaussian%20processes%20for%20regression%20using%20approximate%20expectation%20propagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bui%2C%20T.%20Hernandez-Lobato%2C%20D.%20Hernandez-Lobato%2C%20J.%20Li%2C%20Y.%20Deep%20Gaussian%20processes%20for%20regression%20using%20approximate%20expectation%20propagation%202016"
        },
        {
            "id": "Chen_et+al_2014_a",
            "entry": "T. Chen, E. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In International Conference on Machine Learning, pages 1683\u20131691, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202014"
        },
        {
            "id": "Cheng_2016_a",
            "entry": "C.-A. Cheng and B. Boots. Incremental variational sparse Gaussian process regression. In Advances in Neural Information Processing Systems, pages 4410\u20134418, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Incremental%20variational%20sparse%20Gaussian%20process%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Incremental%20variational%20sparse%20Gaussian%20process%20regression%202016"
        },
        {
            "id": "Cheng_2017_a",
            "entry": "C.-A. Cheng and B. Boots. Variational inference for Gaussian process models with linear complexity. In Advances in Neural Information Processing Systems, pages 5190\u20135200, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Variational%20inference%20for%20Gaussian%20process%20models%20with%20linear%20complexity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Variational%20inference%20for%20Gaussian%20process%20models%20with%20linear%20complexity%202017"
        },
        {
            "id": "Cramer_1998_a",
            "entry": "D. Cramer. Fundamental Statistics for Social Research: Step-by-Step Calculations and Computer Techniques Using SPSS for Windows. Routledge, New York, NY, 10001, 1998. ISBN 0415172039.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cramer%2C%20D.%20Fundamental%20Statistics%20for%20Social%20Research%3A%20Step-by-Step%20Calculations%20and%20Computer%20Techniques%20Using%20SPSS%20for%20Windows%201998"
        },
        {
            "id": "Cutajar_et+al_2016_a",
            "entry": "K. Cutajar, E. V. Bonilla, P. Michiardi, and M. Filippone. Random feature expansions for deep Gaussian processes. arXiv preprint arXiv:1610.04386, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04386"
        },
        {
            "id": "Damianou_2015_a",
            "entry": "A. Damianou. Deep Gaussian processes and variational propagation of uncertainty. PhD thesis, University of Sheffield, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Damianou%2C%20A.%20Deep%20Gaussian%20processes%20and%20variational%20propagation%20of%20uncertainty%202015"
        },
        {
            "id": "Damianou_2013_a",
            "entry": "A. Damianou and N. Lawrence. Deep Gaussian processes. In Artificial Intelligence and Statistics, pages 207\u2013215, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Damianou%2C%20A.%20Lawrence%2C%20N.%20Deep%20Gaussian%20processes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Damianou%2C%20A.%20Lawrence%2C%20N.%20Deep%20Gaussian%20processes%202013"
        },
        {
            "id": "Dunlop_et+al_2017_a",
            "entry": "M. M. Dunlop, M. Girolami, A. M. Stuart, and A. L. Teckentrup. How deep are deep Gaussian processes? arXiv preprint arXiv:1711.11280, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11280"
        },
        {
            "id": "Graves_2011_a",
            "entry": "A. Graves. Practical variational inference for neural networks. In Advances in Neural Information Processing Systems, pages 2348\u20132356, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20A.%20Practical%20variational%20inference%20for%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20A.%20Practical%20variational%20inference%20for%20neural%20networks%202011"
        },
        {
            "id": "Hachmann_et+al_2011_a",
            "entry": "J. Hachmann, R. Olivares-Amaya, S. Atahan-Evrenk, C. Amador-Bedolla, R. S. Sanchez-Carrera, A. Gold-Parker, L. Vogt, A. M. Brockway, and A. Aspuru-Guzik. The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2(17):2241\u20132251, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hachmann%2C%20J.%20Olivares-Amaya%2C%20R.%20Atahan-Evrenk%2C%20S.%20Amador-Bedolla%2C%20C.%20The%20Harvard%20clean%20energy%20project%3A%20large-scale%20computational%20screening%20and%20design%20of%20organic%20photovoltaics%20on%20the%20world%20community%20grid%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hachmann%2C%20J.%20Olivares-Amaya%2C%20R.%20Atahan-Evrenk%2C%20S.%20Amador-Bedolla%2C%20C.%20The%20Harvard%20clean%20energy%20project%3A%20large-scale%20computational%20screening%20and%20design%20of%20organic%20photovoltaics%20on%20the%20world%20community%20grid%202011"
        },
        {
            "id": "Hensman_et+al_2015_a",
            "entry": "J. Hensman, A. G. Matthews, M. Filippone, and Z. Ghahramani. MCMC for variationally sparse Gaussian processes. In Advances in Neural Information Processing Systems, pages 1648\u20131656, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Filippone%2C%20M.%20Ghahramani%2C%20Z.%20MCMC%20for%20variationally%20sparse%20Gaussian%20processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Filippone%2C%20M.%20Ghahramani%2C%20Z.%20MCMC%20for%20variationally%20sparse%20Gaussian%20processes%202015"
        },
        {
            "id": "Hernandez-Lobato_et+al_2011_a",
            "entry": "D. Hernandez-Lobato, J. M. Hernandez-Lobato, and P. Dupont. Robust multi-class Gaussian process classification. In Advances in neural information processing systems, pages 280\u2013288, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandez-Lobato%2C%20D.%20Hernandez-Lobato%2C%20J.M.%20Dupont%2C%20P.%20Robust%20multi-class%20Gaussian%20process%20classification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandez-Lobato%2C%20D.%20Hernandez-Lobato%2C%20J.M.%20Dupont%2C%20P.%20Robust%20multi-class%20Gaussian%20process%20classification%202011"
        },
        {
            "id": "Hernandez-Lobato_2015_a",
            "entry": "J. M. Hernandez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In International Conference on Machine Learning, pages 1861\u20131869, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandez-Lobato%2C%20J.M.%20Adams%2C%20R.%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20Bayesian%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandez-Lobato%2C%20J.M.%20Adams%2C%20R.%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20Bayesian%20neural%20networks%202015"
        },
        {
            "id": "Hoffman_2017_a",
            "entry": "M. D. Hoffman. Learning deep latent Gaussian models with Markov chain Monte Carlo. In International Conference on Machine Learning, pages 1510\u20131519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20M.D.%20Learning%20deep%20latent%20Gaussian%20models%20with%20Markov%20chain%20Monte%20Carlo%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20M.D.%20Learning%20deep%20latent%20Gaussian%20models%20with%20Markov%20chain%20Monte%20Carlo%202017"
        },
        {
            "id": "Minka_2001_a",
            "entry": "T. P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pages 362\u2013369. Morgan Kaufmann Publishers Inc., 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20T.P.%20Expectation%20propagation%20for%20approximate%20Bayesian%20inference%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minka%2C%20T.P.%20Expectation%20propagation%20for%20approximate%20Bayesian%20inference%202001"
        },
        {
            "id": "Neal_1993_a",
            "entry": "R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20R.M.%20Probabilistic%20inference%20using%20Markov%20chain%20Monte%20Carlo%20methods%201993"
        },
        {
            "id": "Neath_2013_a",
            "entry": "R. C. Neath et al. On convergence properties of the Monte Carlo EM algorithm. In Advances in Modern Statistical Theory and Applications: A Festschrift in Honor of Morris L. Eaton, pages 43\u201362. Institute of Mathematical Statistics, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neath%2C%20R.C.%20On%20convergence%20properties%20of%20the%20Monte%20Carlo%20EM%20algorithm.%20In%20Advances%20in%20Modern%20Statistical%20Theory%20and%20Applications%3A%20A%20Festschrift%20in%20Honor%20of%20Morris%20L%202013"
        },
        {
            "id": "Quinonero-Candela_2005_a",
            "entry": "J. Quinonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939\u20131959, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quinonero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quinonero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005"
        },
        {
            "id": "Salimbeni_2017_a",
            "entry": "H. Salimbeni and M. Deisenroth. Doubly stochastic variational inference for deep Gaussian processes. In Advances in Neural Information Processing Systems, pages 4591\u20134602, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimbeni%2C%20H.%20Deisenroth%2C%20M.%20Doubly%20stochastic%20variational%20inference%20for%20deep%20Gaussian%20processes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimbeni%2C%20H.%20Deisenroth%2C%20M.%20Doubly%20stochastic%20variational%20inference%20for%20deep%20Gaussian%20processes%202017"
        },
        {
            "id": "Snelson_2006_a",
            "entry": "E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Y. Weiss, B. Scholkopf, and J. C. Platt, editors, Advances in Neural Information Processing Systems 18, pages 1257\u20131264. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs.%20In%20Y%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs.%20In%20Y%202006"
        },
        {
            "id": "Springenberg_et+al_2016_a",
            "entry": "J. T. Springenberg, A. Klein, S. Falkner, and F. Hutter. Bayesian optimization with robust Bayesian neural networks. In Advances in Neural Information Processing Systems, pages 4134\u20134142, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20J.T.%20Klein%2C%20A.%20Falkner%2C%20S.%20Hutter%2C%20F.%20Bayesian%20optimization%20with%20robust%20Bayesian%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20J.T.%20Klein%2C%20A.%20Falkner%2C%20S.%20Hutter%2C%20F.%20Bayesian%20optimization%20with%20robust%20Bayesian%20neural%20networks%202016"
        },
        {
            "id": "Titsias_2009_a",
            "entry": "M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In D. van Dyk and M. Welling, editors, Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, volume 5 of Proceedings of Machine Learning Research, pages 567\u2013574, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 16\u201318 Apr 2009. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20M.%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009-04-16",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20M.%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009-04-16"
        },
        {
            "id": "Turner_2011_a",
            "entry": "R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series models. Bayesian Time series models, 1(3.1):3\u20131, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Turner%2C%20R.E.%20Sahani%2C%20M.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Turner%2C%20R.E.%20Sahani%2C%20M.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011"
        },
        {
            "id": "Wei_1990_a",
            "entry": "G. C. Wei and M. A. Tanner. A Monte Carlo implementation of the EM algorithm and the poor man\u2019s data augmentation algorithms. Journal of the American statistical Association, 85(411):699\u2013704, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20G.C.%20Tanner%2C%20M.A.%20A%20Monte%20Carlo%20implementation%20of%20the%20EM%20algorithm%20and%20the%20poor%20man%E2%80%99s%20data%20augmentation%20algorithms%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20G.C.%20Tanner%2C%20M.A.%20A%20Monte%20Carlo%20implementation%20of%20the%20EM%20algorithm%20and%20the%20poor%20man%E2%80%99s%20data%20augmentation%20algorithms%201990"
        },
        {
            "id": "Welling_2011_a",
            "entry": "M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681\u2013 688, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20learning%20via%20stochastic%20gradient%20Langevin%20dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20learning%20via%20stochastic%20gradient%20Langevin%20dynamics%202011"
        },
        {
            "id": "Williams_1996_a",
            "entry": "C. K. Williams and C. E. Rasmussen. Gaussian processes for regression. In Advances in neural information processing systems, pages 514\u2013520, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20C.K.%20Rasmussen%2C%20C.E.%20Gaussian%20processes%20for%20regression%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20C.K.%20Rasmussen%2C%20C.E.%20Gaussian%20processes%20for%20regression%201996"
        }
    ]
}
