{
    "filename": "8016-robust-detection-of-adversarial-attacks-by-modeling-the-intrinsic-properties-of-deep-neural-networks.pdf",
    "metadata": {
        "title": "Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks",
        "author": "Zhihao Zheng, Pengyu Hong",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8016-robust-detection-of-adversarial-attacks-by-modeling-the-intrinsic-properties-of-deep-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "It has been shown that deep neural network (DNN) based classifiers are vulnerable to human-imperceptive adversarial perturbations which can cause DNN classifiers to output wrong predictions with high confidence. We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers. Our approach tries to capture the intrinsic properties of a DNN classifier and uses them to detect adversarial inputs. The intrinsic properties used in this study are the output distributions of the hidden neurons in a DNN classifier presented with natural images. Our approach can be easily applied to any DNN classifiers or combined with other defense strategies to improve robustness. Experimental results show that our approach demonstrates state-of-the-art robustness in defending black-box and gray-box attacks."
    },
    "keywords": [
        {
            "term": "gray box",
            "url": "https://en.wikipedia.org/wiki/gray_box"
        },
        {
            "term": "Gradient Descent",
            "url": "https://en.wikipedia.org/wiki/Gradient_Descent"
        },
        {
            "term": "high confidence",
            "url": "https://en.wikipedia.org/wiki/high_confidence"
        },
        {
            "term": "face recognition",
            "url": "https://en.wikipedia.org/wiki/face_recognition"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "adversarial input",
            "url": "https://en.wikipedia.org/wiki/adversarial_input"
        },
        {
            "term": "intrinsic property",
            "url": "https://en.wikipedia.org/wiki/intrinsic_property"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "image recognition",
            "url": "https://en.wikipedia.org/wiki/image_recognition"
        },
        {
            "term": "defense strategy",
            "url": "https://en.wikipedia.org/wiki/defense_strategy"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Since the successful application of deep convolutional neural network to large-scale image recognition [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] by Krizhevsky et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], neural network based Deep Learning has gained significant attentions",
        "Gray-box Attack: Attackers know the architecture of the deep neural networks classifier and its defense strategy, but have no knowledge of their parameters",
        "We show that modeling the intrinsic properties of a deep neural networks classifier can be a reliable strategy to detect adversarial attacks",
        "It does not suffer from attack methods unseen during training and is able to robustly defense against various of black-box and gray-box attacks, which is sufficient for most application scenarios",
        "Experiment results validate that our implementation achieves state-of-the-art performance among unsupervised methods and generalizes better than supervised ones",
        "Since our method models the hidden states of a deep neural networks instead of the inputs, it can be directly applied to other modalities"
    ],
    "key_statements": [
        "Since the successful application of deep convolutional neural network to large-scale image recognition [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] by Krizhevsky et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], neural network based Deep Learning has gained significant attentions",
        "Even though the perturbations are too small to affect human recognition, the deep neural networks classifier can misclassify the perturbed input with high confidence",
        "We propose a strategy for detecting adversarial inputs by modeling the intrinsic properties of a deep neural networks classifier",
        "We reason that: When the deep neural networks classifier mis-assigns a specific class label to an adversarial input, its hidden states are quite different from those given natural data of the same class",
        "Gray-box Attack: Attackers know the architecture of the deep neural networks classifier and its defense strategy, but have no knowledge of their parameters",
        "We show that modeling the intrinsic properties of a deep neural networks classifier can be a reliable strategy to detect adversarial attacks",
        "It does not suffer from attack methods unseen during training and is able to robustly defense against various of black-box and gray-box attacks, which is sufficient for most application scenarios",
        "Experiment results validate that our implementation achieves state-of-the-art performance among unsupervised methods and generalizes better than supervised ones",
        "Since our method models the hidden states of a deep neural networks instead of the inputs, it can be directly applied to other modalities"
    ],
    "summary": [
        "Since the successful application of deep convolutional neural network to large-scale image recognition [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] by Krizhevsky et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], neural network based Deep Learning has gained significant attentions.",
        "We propose a strategy for detecting adversarial inputs by modeling the intrinsic properties of a DNN classifier.",
        "This strategy does not need to know the attack methods or require to train the classifier with adversarial samples.",
        "Termed I-defender (\"I\" stands for intrinsic), which explores one of the intrinsic properties of a DNN classifier, i.e., the distributions of its hidden states given natural training data.",
        "We reason that: When the DNN classifier mis-assigns a specific class label to an adversarial input, its hidden states are quite different from those given natural data of the same class.",
        "I-defender models the hidden state distributions of a DNN classifier given natural data and uses them to detect adversarial inputs.",
        "We do not attempt to model the hidden state distributions of a classifier presented with adversarial inputs because such distributions will depend specifically on the attack methods that generate the adversarial training inputs.",
        "The subnetwork connects each layer of the DNN and is trained separately using a dataset containing both natural samples and adversarial samples generated by known attack methods.",
        "Black-box Attack: Attackers know nothing about the defense strategy and use a substitute network to generate adversarial samples.",
        "3. Gray-box Attack: Attackers know the architecture of the DNN classifier and its defense strategy, but have no knowledge of their parameters.",
        "Attackers use a network of the same architecture and the same defense strategy to generate adversarial samples.",
        "It was shown that the Defense-GAN detection method outperformed previous approaches on these two datasets under the FGSM attack.",
        "Since Defense-GAN did not report their results on CIFAR-10, we compared I-defender with two supervised detection methods, SafetyNet[<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and Subnetwork[<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "In a gray-box attack, an attacker knows the structure and defense strategy of a DNN classifier, but has no knowledge of its parameters.",
        "Adversarial training is able to increase the robustness of a DNN classifier when attacked by single-step gray-box methods, but not by iterative gray-box methods.",
        "A perturbed input generated to a \"Source\" DNN can lead to a remarkably different hidden state configuration in a \"Target\" DNN, which means it is challenging for attackers to rely on a \"Source\" DNN to successfully attack a \"Target\" DNN.",
        "We show that modeling the intrinsic properties of a DNN classifier can be a reliable strategy to detect adversarial attacks.",
        "Since our method models the hidden states of a DNN instead of the inputs, it can be directly applied to other modalities"
    ],
    "headline": "We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] E. Ackerman. How drive. ai is mastering autonomous driving with deep learning. IEEE Spectrum, March, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ackerman%2C%20E.%20How%20drive.%20ai%20is%20mastering%20autonomous%20driving%20with%20deep%20learning%202017-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ackerman%2C%20E.%20How%20drive.%20ai%20is%20mastering%20autonomous%20driving%20with%20deep%20learning%202017-03"
        },
        {
            "id": "2",
            "entry": "[2] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.07316"
        },
        {
            "id": "3",
            "entry": "[3] N. Carlini and D. Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04311"
        },
        {
            "id": "4",
            "entry": "[4] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273\u2013297, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20C.%20Vapnik%2C%20V.%20Support-vector%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20C.%20Vapnik%2C%20V.%20Support-vector%20networks%201995"
        },
        {
            "id": "6",
            "entry": "[6] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S. Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esteva%2C%20A.%20Kuprel%2C%20B.%20Novoa%2C%20R.A.%20Ko%2C%20J.%20Dermatologist-level%20classification%20of%20skin%20cancer%20with%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esteva%2C%20A.%20Kuprel%2C%20B.%20Novoa%2C%20R.A.%20Ko%2C%20J.%20Dermatologist-level%20classification%20of%20skin%20cancer%20with%20deep%20neural%20networks%202017"
        },
        {
            "id": "7",
            "entry": "[7] A. Giusti, J. Guzzi, D. C. Ciresan, F.-L. He, J. P. Rodr\u00edguez, F. Fontana, M. Faessler, C. Forster, J. Schmidhuber, G. Di Caro, et al. A machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and Automation Letters, 1(2):661\u2013667, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giusti%2C%20A.%20Guzzi%2C%20J.%20Ciresan%2C%20D.C.%20He%2C%20F.-L.%20A%20machine%20learning%20approach%20to%20visual%20perception%20of%20forest%20trails%20for%20mobile%20robots%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giusti%2C%20A.%20Guzzi%2C%20J.%20Ciresan%2C%20D.C.%20He%2C%20F.-L.%20A%20machine%20learning%20approach%20to%20visual%20perception%20of%20forest%20trails%20for%20mobile%20robots%202016"
        },
        {
            "id": "8",
            "entry": "[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "9",
            "entry": "[9] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "10",
            "entry": "[10] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "11",
            "entry": "[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "12",
            "entry": "[12] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "13",
            "entry": "[13] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01236"
        },
        {
            "id": "14",
            "entry": "[14] Y. LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "15",
            "entry": "[15] J. Lu, T. Issaranon, and D. Forsyth. Safetynet: Detecting and rejecting adversarial examples robustly. CoRR, abs/1704.00103, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00103"
        },
        {
            "id": "16",
            "entry": "[16] G. McLachlan and T. Krishnan. The EM algorithm and extensions, volume 382. John Wiley & Sons, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McLachlan%2C%20G.%20Krishnan%2C%20T.%20The%20EM%20algorithm%20and%20extensions%2C%20volume%20382%202007"
        },
        {
            "id": "17",
            "entry": "[17] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff. On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04267"
        },
        {
            "id": "18",
            "entry": "[18] T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00677"
        },
        {
            "id": "19",
            "entry": "[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "20",
            "entry": "[20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "21",
            "entry": "[21] S. M. Moosavi Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20S.M.Moosavi%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20S.M.Moosavi%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "22",
            "entry": "[22] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.-M.%20Fawzi%2C%20A.%20Fawzi%2C%20O.%20Frossard%2C%20P.%20Universal%20adversarial%20perturbations.%20arXiv%20p%202017"
        },
        {
            "id": "23",
            "entry": "[23] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Goodfellow%2C%20I.%20Jha%2C%20S.%20Practical%20black-box%20attacks%20against%20deep%20learning%20systems%20using%20adversarial%20examples.%20arXiv%20p%202016"
        },
        {
            "id": "24",
            "entry": "[24] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pages 372\u2013387. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Jha%2C%20S.%20Fredrikson%2C%20M.%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Jha%2C%20S.%20Fredrikson%2C%20M.%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "25",
            "entry": "[25] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pages 582\u2013597. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Wu%2C%20X.%20Jha%2C%20S.%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Wu%2C%20X.%20Jha%2C%20S.%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "26",
            "entry": "[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "27",
            "entry": "[27] P. Samangouei, M. Kabkab, and R. Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. In International Conference on Learning Representations, volume 9, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samangouei%2C%20P.%20Kabkab%2C%20M.%20Chellappa%2C%20R.%20Defense-gan%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samangouei%2C%20P.%20Kabkab%2C%20M.%20Chellappa%2C%20R.%20Defense-gan%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%202018"
        },
        {
            "id": "28",
            "entry": "[28] D. Shen, G. Wu, and H.-I. Suk. Deep learning in medical image analysis. Annual review of biomedical engineering, 19:221\u2013248, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20D.%20Wu%2C%20G.%20Suk%2C%20H.-I.%20Deep%20learning%20in%20medical%20image%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20D.%20Wu%2C%20G.%20Suk%2C%20H.-I.%20Deep%20learning%20in%20medical%20image%20analysis%202017"
        },
        {
            "id": "29",
            "entry": "[29] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "30",
            "entry": "[30] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "31",
            "entry": "[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "32",
            "entry": "[32] J. Su, D. V. Vargas, and S. Kouichi. One pixel attack for fooling deep neural networks. arXiv preprint arXiv:1710.08864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.08864"
        },
        {
            "id": "33",
            "entry": "[33] Y. Sun, D. Liang, X. Wang, and X. Tang. Deepid3: Face recognition with very deep neural networks. arXiv preprint arXiv:1502.00873, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.00873"
        },
        {
            "id": "34",
            "entry": "[34] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "35",
            "entry": "[35] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20H.%20Rasul%2C%20K.%20Vollgraf%2C%20R.%20Fashion-mnist%3A%20a%20novel%20image%20dataset%20for%20benchmarking%20machine%20learning%20algorithms%202017"
        },
        {
            "id": "36",
            "entry": "[36] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        }
    ]
}
