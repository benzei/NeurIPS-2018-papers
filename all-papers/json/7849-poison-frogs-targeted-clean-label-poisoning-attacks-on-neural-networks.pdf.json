{
    "filename": "7849-poison-frogs-targeted-clean-label-poisoning-attacks-on-neural-networks.pdf",
    "metadata": {
        "title": "Recurrent Processing during Object Recognition",
        "author": "Randall C. O\u2019Reilly, Dean Wyatte, Seth Herd, Brian Mingus, David J. Jilk",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7849-poison-frogs-targeted-clean-label-poisoning-attacks-on-neural-networks.pdf",
            "doi": "10.3389/fpsyg.2013.00124"
        },
        "journal": "Frontiers in Psychology",
        "volume": "4",
        "abstract": "Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use \u201cclean-labels\u201d; they don\u2019t require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a specific test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a \u201cwatermarking\u201d strategy that makes poisoning reliable using multiple (\u2248 50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.",
        "date": 2013
    },
    "keywords": [
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "clean label",
            "url": "https://en.wikipedia.org/wiki/clean_label"
        },
        {
            "term": "spam filter",
            "url": "https://en.wikipedia.org/wiki/spam_filter"
        }
    ],
    "highlights": [
        "Before deep learning algorithms can be deployed in high stakes, security-critical applications, their robustness against adversarial attacks must be put to the test",
        "The proposed attack crafts poison images that collide with a target image in feature space, making it difficult for a network to discern between the two",
        "These attacks are extremely powerful in the transfer learning scenario, and can be made powerful in more general contexts by using multiple poison images and a watermarking trick",
        "Training with poison instances is akin to the adversarial training technique for defending against evasion attacks (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al [2015</a>])",
        "The poison instance can here be seen as an adversarial example to the base class",
        "While our poisoned dataset training does make the network more robust to base-class adversarial examples designed to be misclassified as the target, it has the effect of causing the unaltered target instance to be misclassified as a base"
    ],
    "key_statements": [
        "Before deep learning algorithms can be deployed in high stakes, security-critical applications, their robustness against adversarial attacks must be put to the test",
        "Adversarial examples fall within a category of attacks called evasion attacks",
        "Evasion attacks happen at test time \u2013 a clean target instance is modified to avoid detection by a classifier, or spur misclassification",
        "We propose clean label attacks that do not require control over the labeling function; the poisoned training data appear to be labeled correctly according to an expert observer",
        "A similar type of attack was demonstrated using influence functions (<a class=\"ref-link\" id=\"cKoh_2017_a\" href=\"#rKoh_2017_a\"><a class=\"ref-link\" id=\"cKoh_2017_a\" href=\"#rKoh_2017_a\">Koh and Liang [2017</a></a>]) for the scenario where only the final fully connected layer of the network was retrained on the poisoned dataset, with a success rate of 57%.We demonstrate an optimization-based clean-label attack under the transfer learning scenario studied by <a class=\"ref-link\" id=\"cKoh_2017_a\" href=\"#rKoh_2017_a\"><a class=\"ref-link\" id=\"cKoh_2017_a\" href=\"#rKoh_2017_a\">Koh and Liang [2017</a></a>], but we achieve 100% attack success rate on the same dog-vs-fish classification task",
        "We present a simple poisoning attack on transfer learned networks",
        "The proposed attack crafts poison images that collide with a target image in feature space, making it difficult for a network to discern between the two",
        "These attacks are extremely powerful in the transfer learning scenario, and can be made powerful in more general contexts by using multiple poison images and a watermarking trick",
        "Training with poison instances is akin to the adversarial training technique for defending against evasion attacks (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2015_a\" href=\"#rGoodfellow_et+al_2015_a\">Goodfellow et al [2015</a>])",
        "The poison instance can here be seen as an adversarial example to the base class",
        "While our poisoned dataset training does make the network more robust to base-class adversarial examples designed to be misclassified as the target, it has the effect of causing the unaltered target instance to be misclassified as a base"
    ],
    "summary": [
        "Before deep learning algorithms can be deployed in high stakes, security-critical applications, their robustness against adversarial attacks must be put to the test.",
        "We craft a 50 poison instance attack on a deep network which achieves success rates of up to 60% in the end-to-end training scenario.",
        "We propose an optimization-based procedure for crafting poison instances that, when added to the training data, manipulate the test-time behavior of a classifier.",
        "At test time, the model mistakes the target instance as being in the base class, the poisoning attack is considered successful.",
        "If the model is retrained on the clean data + poison instances, the linear decision boundary in feature space is expected to rotate to label the poison instance as if it were in the base class.",
        "A \u201cone-shot kill\u201d attack is possible; by adding just one poison instance to the training set, we cause misclassification of the target with 100% success rate.",
        "The experiment is performed 1099 times \u2013 each with a different test-set image as the target instance \u2013 yielding an attack success rate of 100%.",
        "Fig. 3a shows the feature space representations of the target, base, and poison instances along with the training data under a clean and poisoned model.",
        "In their clean model feature space representations, the target and poison instances are overlapped, indicating that our poison-crafting optimization procedure (Algorithm 1) works.",
        "Unlike the transfer learning scenario where the final layer decision boundary rotates to accommodate the poison instance within the base region, the decision boundary in the end-to-end training scenario is unchanged after retraining on the poisoned dataset, as seen through the red bars and lines in Fig. 2.",
        "We make the following important observation: During retraining with the poison data, the network modifies its lower-level feature extraction kernels in the shallow layers so the poison instance is returned to the base class distribution in the deep layers.",
        "Poisoning in the end-to-end training scenario is difficult because the network learns feature embeddings that optimally distinguish the target from the poison.",
        "Clean-label attacks under the end-to-end scenario require multiple techniques to work: (1) optimization via Algorithm 1, (2) diversity of poison instances, and (3) watermarking.",
        "While our poisoned dataset training does make the network more robust to base-class adversarial examples designed to be misclassified as the target, it has the effect of causing the unaltered target instance to be misclassified as a base.",
        "Many neural networks are trained using data sources that are manipulated by adversaries.We hope that this work will raise attention for the important issue of data reliability and provenance"
    ],
    "headline": "This paper explores poisoning attacks on neural nets",
    "reference_links": [
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Goodfellow_et+al_2015_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. International Conference on Learning Representation, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "Biggio_et+al_2012_a",
            "entry": "Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXiv preprint arXiv:1206.6389, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6389"
        },
        {
            "id": "Nelson_et+al_2008_a",
            "entry": "Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D. Joseph, Benjamin I. P. Rubinstein, Udam Saini, Charles Sutton, J. D. Tygar, and Kai Xia. Exploiting machine learning to subvert your spam filter. In Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats, pages 7:1\u20137:9, Berkeley, CA, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelson%2C%20Blaine%20Barreno%2C%20Marco%20Chi%2C%20Fuching%20Jack%20Joseph%2C%20Anthony%20D.%20Exploiting%20machine%20learning%20to%20subvert%20your%20spam%20filter%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelson%2C%20Blaine%20Barreno%2C%20Marco%20Chi%2C%20Fuching%20Jack%20Joseph%2C%20Anthony%20D.%20Exploiting%20machine%20learning%20to%20subvert%20your%20spam%20filter%202008"
        },
        {
            "id": "Steinhardt_et+al_2017_a",
            "entry": "Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified Defenses for Data Poisoning Attacks. arXiv preprint arXiv:1706.03691, (i), 2017. URL http://arxiv.org/abs/1706.03691.",
            "url": "http://arxiv.org/abs/1706.03691",
            "arxiv_url": "https://arxiv.org/pdf/1706.03691"
        },
        {
            "id": "Mu_et+al_2017_a",
            "entry": "Luis Mu\u00f1oz-Gonz\u00e1lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 27\u201338. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mu%C3%B1oz-Gonz%C3%A1lez%2C%20Luis%20Biggio%2C%20Battista%20Demontis%2C%20Ambra%20Paudice%2C%20Andrea%20Towards%20poisoning%20of%20deep%20learning%20algorithms%20with%20back-gradient%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mu%C3%B1oz-Gonz%C3%A1lez%2C%20Luis%20Biggio%2C%20Battista%20Demontis%2C%20Ambra%20Paudice%2C%20Andrea%20Towards%20poisoning%20of%20deep%20learning%20algorithms%20with%20back-gradient%20optimization%202017"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative poisoning attack method against neural networks. arXiv preprint arXiv:1703.01340, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01340"
        },
        {
            "id": "Chen_et+al_2017_a",
            "entry": "Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. arXiv preprint arXiv:1712.05526, 2017. URL http://arxiv.org/abs/1712.05526.",
            "url": "http://arxiv.org/abs/1712.05526",
            "arxiv_url": "https://arxiv.org/pdf/1712.05526"
        },
        {
            "id": "Gu_et+al_2017_a",
            "entry": "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.06733"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yingqi%20Ma%2C%20Shiqing%20Aafer%2C%20Yousra%20Lee%2C%20Wen-Chuan%20Trojaning%20attack%20on%20neural%20networks%202017"
        },
        {
            "id": "Suciu_et+al_2018_a",
            "entry": "Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daum\u00e9 III, and Tudor Dumitras. When does machine learning fail? generalized transferability for evasion and poisoning attacks. arXiv preprint arXiv:1803.06975, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06975"
        },
        {
            "id": "Mahloujifar_2017_a",
            "entry": "Saeed Mahloujifar and Mohammad Mahmoody. Blockwise p-tampering attacks on cryptographic primitives, extractors, and learners. Cryptology ePrint Archive, Report 2017/950, 2017. https://eprint.iacr.org/2017/950.",
            "url": "https://eprint.iacr.org/2017/950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahloujifar%2C%20Saeed%20Mahmoody%2C%20Mohammad%20Blockwise%20p-tampering%20attacks%20on%20cryptographic%20primitives%2C%20extractors%2C%20and%20learners%202017"
        },
        {
            "id": "Mahloujifar_et+al_2017_b",
            "entry": "Saeed Mahloujifar, Dimitrios I. Diochnos, and Mohammad Mahmoody. Learning under p-tampering attacks. CoRR, abs/1711.03707, 2017. URL http://arxiv.org/abs/1711.03707.",
            "url": "http://arxiv.org/abs/1711.03707",
            "arxiv_url": "https://arxiv.org/pdf/1711.03707"
        },
        {
            "id": "Diakonikolas_et+al_2016_a",
            "entry": "Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Efficient robust proper learning of logconcave distributions. arXiv preprint arXiv:1606.03077, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03077"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385, 7(3):171\u2013180, 2015. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00124. URL http://arxiv.org/pdf/1512.03385v1.pdf.",
            "crossref": "https://dx.doi.org/10.3389/fpsyg.2013.00124",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "Szegedy_et+al_2014_a",
            "entry": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going Deeper with Convolutions. arXiv:1409.4842, 2014. ISSN 10636919. doi: 10.1109/CVPR.2015.7298594. URL https://arxiv.org/abs/1409.4842.",
            "crossref": "https://dx.doi.org/10.1109/CVPR.2015.7298594",
            "arxiv_url": "https://arxiv.org/pdf/1409.4842"
        },
        {
            "id": "Barreno_et+al_2010_a",
            "entry": "Marco Barreno, Blaine Nelson, Anthony D Joseph, and JD Tygar. The security of machine learning. Machine Learning, 81(2):121\u2013148, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marco%20Barreno%20Blaine%20Nelson%20Anthony%20D%20Joseph%20and%20JD%20Tygar%20The%20security%20of%20machine%20learning%20Machine%20Learning%20812121148%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marco%20Barreno%20Blaine%20Nelson%20Anthony%20D%20Joseph%20and%20JD%20Tygar%20The%20security%20of%20machine%20learning%20Machine%20Learning%20812121148%202010"
        },
        {
            "id": "Koh_2017_a",
            "entry": "Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04730"
        },
        {
            "id": "Goldstein_et+al_2014_a",
            "entry": "Tom Goldstein, Christoph Studer, and Richard Baraniuk. A field guide to forward-backward splitting with a fasta implementation. arXiv preprint arXiv:1411.3406, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.3406"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        }
    ]
}
