{
    "filename": "7577-learning-latent-variable-structured-prediction-models-with-gaussian-perturbations.pdf",
    "metadata": {
        "title": "Learning latent variable structured prediction models with Gaussian perturbations",
        "author": "Kevin Bello, Jean Honorio",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7577-learning-latent-variable-structured-prediction-models-with-gaussian-perturbations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The standard margin-based structured prediction commonly uses a maximum loss over all possible structured outputs [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]. The large-margin formulation including latent variables [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] not only results in a non-convex formulation but also increases the search space by a factor of the size of the latent space. Recent work [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution, with theoretical guarantees. We extend this work by including latent variables. We study a new family of loss functions under Gaussian perturbations and analyze the effect of the latent space on the generalization bounds. We show that the non-convexity of learning with latent variables originates naturally, as it relates to a tight upper bound of the Gibbs decoder distortion with respect to the latent space. Finally, we provide a formulation using random samples and relaxations that produces a tighter upper bound of the Gibbs decoder distortion up to a statistical accuracy, which enables a polynomial time evaluation of the objective function. We illustrate the method with synthetic experiments and a computer vision application."
    },
    "keywords": [
        {
            "term": "conditional random fields",
            "url": "https://en.wikipedia.org/wiki/conditional_random_fields"
        },
        {
            "term": "countable set",
            "url": "https://en.wikipedia.org/wiki/countable_set"
        },
        {
            "term": "structured prediction",
            "url": "https://en.wikipedia.org/wiki/structured_prediction"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "maximum loss",
            "url": "https://en.wikipedia.org/wiki/maximum_loss"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        }
    ],
    "highlights": [
        "We denote the input space as X , the output space as Y, and the latent space as H",
        "We focus on the learning aspects of structured prediction problems using latent variables",
        "We provide a tighter upper bound of the Gibbs decoder distortion by randomizing the search space of the optimization problem",
        "We provide a way to obtain an upper bound that is logarithmic in the size of the latent space",
        "The slack re-scaling has been less popular than the margin re-scaling approach due to computational requirements. Both formulations require optimizing over the output space, but while margin re-scaling preserves the structure of the score and error functions, the slack re-scaling does not. This results in harder inference problems during training. [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] analyze the slack re-scaling approach and theoretically show that using random structures one can obtain a tighter upper bound of the Gibbs decoder distortion",
        "We evaluated the maximum loss over random structured outputs and latent variables, using the original latent space, as well as, the superset relaxation as in eq(6)"
    ],
    "key_statements": [
        "We denote the input space as X , the output space as Y, and the latent space as H",
        "We focus on the learning aspects of structured prediction problems using latent variables",
        "We provide a tighter upper bound of the Gibbs decoder distortion by randomizing the search space of the optimization problem",
        "We provide a way to obtain an upper bound that is logarithmic in the size of the latent space",
        "The slack re-scaling has been less popular than the margin re-scaling approach due to computational requirements. Both formulations require optimizing over the output space, but while margin re-scaling preserves the structure of the score and error functions, the slack re-scaling does not. This results in harder inference problems during training. [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] analyze the slack re-scaling approach and theoretically show that using random structures one can obtain a tighter upper bound of the Gibbs decoder distortion",
        "We show that the slack re-scaling objective function) is an upper bound of the Gibbs decoder distortion) up to an statistical accuracy of O( log n/n) for n training samples",
        "We show that our new formulation in eq(6) is related to an upper bound of the Gibbs decoder distortion up to statistical accuracy of O for n training samples",
        "In order to account for the statistical aspects, we use Assumption C and Rademacher complexity arguments for bounding a stochastic quantity for all sets T (w, x) of random structured outputs and latent variables, and all possible proposal distributions R(w, x)",
        "We evaluated the maximum loss over random structured outputs and latent variables, using the original latent space, as well as, the superset relaxation as in eq(6)",
        "Despite leading to a looser upper bound of the Gibbs decoder distortion, if one could control the statistical accuracy under this approach one could obtain a fully polynomial-time evaluation of the objective function, even if |H| is exponential"
    ],
    "summary": [
        "We denote the input space as X , the output space as Y, and the latent space as H.",
        "We show that the slack re-scaling objective function) is an upper bound of the Gibbs decoder distortion) up to an statistical accuracy of O( log n/n) for n training samples.",
        "We show the relation between PAC-Bayes bounds and the maximum loss over random structured outputs and latent variables sampled i.i.d. from some proposal distribution.",
        "Instead of using a maximization over Yx \u00d7 Hx, we will perform a maximization over a set T (w, x) of random elements sampled i.i.d. from some proposal distribution R(w, x) with support on Yx \u00d7 Hx. More explicitly, our new formulation is: 1 min max d(y, y, h) 1 m(x, y, y, h, w) \u2264 1 w n (x,y)\u2208S (y,h)\u2208T (w,x)",
        "We show that our new formulation in eq(6) is related to an upper bound of the Gibbs decoder distortion up to statistical accuracy of O for n training samples.",
        "With probability at least 1 \u2212 \u03b4 over the choice of both n training samples and n sets of random structured outputs and latent variables, simultaneously for all parameters w \u2208 W with w 0 \u2264 s, unit-variance Gaussian perturbation distributions Q(w) centered at w\u03b3",
        "In order to account for the statistical aspects, we use Assumption C and Rademacher complexity arguments for bounding a stochastic quantity for all sets T (w, x) of random structured outputs and latent variables, and all possible proposal distributions R(w, x).",
        "Let R(w, x) and R (w, x) two proposal distributions, both with support on Yx \u00d7 Hx. Assume that R(w, x) fulfills Assumption A with value \u03b21.",
        "We compared three training methods: the maximum loss over all possible structured outputs and latent variables with slack re-scaling as in eq(5).",
        "For inference on an independent test set, we used eq(1) for the maximum loss over all possible structured outputs and latent variables.",
        "For the maximum loss over random structured outputs and latent variables, we use the following approximate inference approach: fw(x) \u2261 argmax \u03a6(x, y, h) \u00b7 w.",
        "We obtained an average error of 0.3878 (6.98 incorrectly matched keypoints) in the test set, which is an improvement to the values of 8.47 for maximum-a-posteriori perturbations and 8.69 for max-margin, as reported in [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>].",
        "Despite leading to a looser upper bound of the Gibbs decoder distortion, if one could control the statistical accuracy under this approach one could obtain a fully polynomial-time evaluation of the objective function, even if |H| is exponential."
    ],
    "headline": "We extend this work by including latent variables",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Altun, Y. and Hofmann, T. [2003], \u2018Large margin methods for label sequence learning\u2019, European Conference on Speech Communication and Technology pp. 145\u2013152.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altun%2C%20Y.%20Hofmann%2C%20T.%20Large%20margin%20methods%20for%20label%20sequence%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altun%2C%20Y.%20Hofmann%2C%20T.%20Large%20margin%20methods%20for%20label%20sequence%20learning%202003"
        },
        {
            "id": "2",
            "entry": "[2] Bennett, J. [1956], \u2018Determination of the number of independent parameters of a score matrix from the examination of rank orders\u2019, Psychometrika 21(4), 383\u2013393.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bennett%2C%20J.%20Determination%20of%20the%20number%20of%20independent%20parameters%20of%20a%20score%20matrix%20from%20the%20examination%20of%20rank%20orders",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bennett%2C%20J.%20Determination%20of%20the%20number%20of%20independent%20parameters%20of%20a%20score%20matrix%20from%20the%20examination%20of%20rank%20orders"
        },
        {
            "id": "3",
            "entry": "[3] Bennett, J. and Hays, W. [1960], \u2018Multidimensional unfolding: Determining the dimensionality of ranked preference data\u2019, Psychometrika 25(1), 27\u201343.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bennett%2C%20J.%20Hays%2C%20W.%20Multidimensional%20unfolding%3A%20Determining%20the%20dimensionality%20of%20ranked%20preference%20data",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bennett%2C%20J.%20Hays%2C%20W.%20Multidimensional%20unfolding%3A%20Determining%20the%20dimensionality%20of%20ranked%20preference%20data"
        },
        {
            "id": "4",
            "entry": "[4] Choi, H., Meshi, O. and Srebro, N. [2016], Fast and scalable structural svm with slack rescaling, in \u2018Artificial Intelligence and Statistics\u2019, pp. 667\u2013675.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20H.%20Meshi%2C%20O.%20Srebro%2C%20N.%20%5B2016%5D%2C%20Fast%20and%20scalable%20structural%20svm%20with%20slack%20rescaling%2C%20in%20%E2%80%98Artificial%20Intelligence%20and%20Statistics%E2%80%99"
        },
        {
            "id": "5",
            "entry": "[5] Collins, M. [2004], Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods, in \u2018New Developments in Parsing Technology\u2019, Vol. 23, Kluwer Academic, pp. 19\u201355.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collins%2C%20M.%20%5B2004%5D%2C%20Parameter%20estimation%20for%20statistical%20parsing%20models%3A%20Theory%20and%20practice%20of%20distribution-free%20methods%2C%20in%20%E2%80%98New%20Developments%20in%20Parsing%20Technology%E2%80%99",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collins%2C%20M.%20%5B2004%5D%2C%20Parameter%20estimation%20for%20statistical%20parsing%20models%3A%20Theory%20and%20practice%20of%20distribution-free%20methods%2C%20in%20%E2%80%98New%20Developments%20in%20Parsing%20Technology%E2%80%99"
        },
        {
            "id": "6",
            "entry": "[6] Collins, M. and Roark, B. [2004], \u2018Incremental parsing with the perceptron algorithm\u2019, Annual Meeting of the Association for Computational Linguistics pp. 111\u2013118.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collins%2C%20M.%20Roark%2C%20B.%20Incremental%20parsing%20with%20the%20perceptron%20algorithm%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collins%2C%20M.%20Roark%2C%20B.%20Incremental%20parsing%20with%20the%20perceptron%20algorithm%202004"
        },
        {
            "id": "7",
            "entry": "[7] Cortes, C., Kuznetsov, V., Mohri, M. and Yang, S. [2016], Structured prediction theory based on factor graph complexity, in \u2018Advances in Neural Information Processing Systems\u2019, pp. 2514\u2013 2522.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20C.%20Kuznetsov%2C%20V.%20Mohri%2C%20M.%20Yang%2C%20S.%20%5B2016%5D%2C%20Structured%20prediction%20theory%20based%20on%20factor%20graph%20complexity%2C%20in%20%E2%80%98Advances",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20C.%20Kuznetsov%2C%20V.%20Mohri%2C%20M.%20Yang%2C%20S.%20%5B2016%5D%2C%20Structured%20prediction%20theory%20based%20on%20factor%20graph%20complexity%2C%20in%20%E2%80%98Advances"
        },
        {
            "id": "8",
            "entry": "[8] Cover, T. [1967], \u2018The number of linearly inducible orderings of points in d-space\u2019, SIAM Journal on Applied Mathematics 15(2), 434\u2013439.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20T.%20The%20number%20of%20linearly%20inducible%20orderings%20of%20points%20in%20d-space%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cover%2C%20T.%20The%20number%20of%20linearly%20inducible%20orderings%20of%20points%20in%20d-space%201967"
        },
        {
            "id": "9",
            "entry": "[9] Gane, A., Hazan, T. and Jaakkola, T. [2014], Learning with maximum a-posteriori perturbation models, in \u2018Artificial Intelligence and Statistics\u2019, pp. 247\u2013256.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gane%2C%20A.%20Hazan%2C%20T.%20Jaakkola%2C%20T.%20Learning%20with%20maximum%20a-posteriori%20perturbation%20models%2C%20in%20%E2%80%98Artificial%20Intelligence%20and%20Statistics%E2%80%99%202014"
        },
        {
            "id": "10",
            "entry": "[10] Hinton, G. E. [2012], A practical guide to training restricted boltzmann machines, in \u2018Neural networks: Tricks of the trade\u2019, Springer, pp. 599\u2013619.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20%5B2012%5D%2C%20A%20practical%20guide%20to%20training%20restricted%20boltzmann%20machines%2C%20in%20%E2%80%98Neural%20networks%3A%20Tricks%20of%20the%20trade%E2%80%99"
        },
        {
            "id": "11",
            "entry": "[11] Honorio, J. and Jaakkola, T. [2016], \u2018Structured prediction: from gaussian perturbations to linear-time principled algorithms\u2019, UAI .",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Honorio%2C%20J.%20Jaakkola%2C%20T.%20Structured%20prediction%3A%20from%20gaussian%20perturbations%20to%20linear-time%20principled%20algorithms"
        },
        {
            "id": "12",
            "entry": "[12] Kulesza, A. and Pereira, F. [2007], \u2018Structured learning with approximate inference\u2019, Neural Information Processing Systems 20, 785\u2013792.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulesza%2C%20A.%20Pereira%2C%20F.%20Structured%20learning%20with%20approximate%20inference",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulesza%2C%20A.%20Pereira%2C%20F.%20Structured%20learning%20with%20approximate%20inference"
        },
        {
            "id": "13",
            "entry": "[13] Lafferty, J., McCallum, A. and Pereira, F. C. [2001], \u2018Conditional random fields: Probabilistic models for segmenting and labeling sequence data\u2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lafferty%2C%20J.%20McCallum%2C%20A.%20Pereira%2C%20F.C.%20Conditional%20random%20fields%3A%20Probabilistic%20models%20for%20segmenting%20and%20labeling%20sequence%20data"
        },
        {
            "id": "14",
            "entry": "[14] Li, M.-H., Lin, L., Wang, X.-L. and Liu, T. [2007], \u2018Protein\u2013protein interaction site prediction based on conditional random fields\u2019, Bioinformatics 23(5), 597\u2013604.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20M.-H.%20Lin%2C%20L.%20Wang%2C%20X.-L.%20Liu%2C%20T.%20Protein%E2%80%93protein%20interaction%20site%20prediction%20based%20on%20conditional%20random%20fields",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20M.-H.%20Lin%2C%20L.%20Wang%2C%20X.-L.%20Liu%2C%20T.%20Protein%E2%80%93protein%20interaction%20site%20prediction%20based%20on%20conditional%20random%20fields"
        },
        {
            "id": "15",
            "entry": "[15] London, B., Meshi, O. and Weller, A. [2016], \u2018Bounding the integrality distance of lp relaxations for structured prediction\u2019, NIPS workshop on Optimization for Machine Learning .",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=London%2C%20B.%20Meshi%2C%20O.%20Weller%2C%20A.%20Bounding%20the%20integrality%20distance%20of%20lp%20relaxations%20for%20structured%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=London%2C%20B.%20Meshi%2C%20O.%20Weller%2C%20A.%20Bounding%20the%20integrality%20distance%20of%20lp%20relaxations%20for%20structured%20prediction"
        },
        {
            "id": "16",
            "entry": "[16] McAllester, D. [2007], Generalization bounds and consistency, in \u2018Predicting Structured Data\u2019, MIT Press, pp. 247\u2013261.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20D.%20Generalization%20bounds%20and%20consistency%2C%20in%20%E2%80%98Predicting%20Structured%20Data%E2%80%99"
        },
        {
            "id": "17",
            "entry": "[17] Meshi, O., Mahdavi, M., Weller, A. and Sontag, D. [2016], \u2018Train and test tightness of lp relaxations in structured prediction\u2019, International Conference on Machine Learning .",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meshi%2C%20O.%20Mahdavi%2C%20M.%20Weller%2C%20A.%20Sontag%2C%20D.%20Train%20and%20test%20tightness%20of%20lp%20relaxations%20in%20structured%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meshi%2C%20O.%20Mahdavi%2C%20M.%20Weller%2C%20A.%20Sontag%2C%20D.%20Train%20and%20test%20tightness%20of%20lp%20relaxations%20in%20structured%20prediction"
        },
        {
            "id": "18",
            "entry": "[18] Neylon, T. [2006], Sparse Solutions for Linear Prediction Problems, PhD thesis, New York University.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neylon%2C%20T.%20Sparse%20Solutions%20for%20Linear%20Prediction%20Problems%202006"
        },
        {
            "id": "19",
            "entry": "[19] Nowozin, S., Lampert, C. H. et al. [2011], \u2018Structured learning and prediction in computer vision\u2019, Foundations and Trends\u00ae in Computer Graphics and Vision 6(3\u20134), 185\u2013365.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20S.%20Lampert%2C%20C.H.%20Structured%20learning%20and%20prediction%20in%20computer%20vision",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20S.%20Lampert%2C%20C.H.%20Structured%20learning%20and%20prediction%20in%20computer%20vision"
        },
        {
            "id": "20",
            "entry": "[20] Petrov, S. and Klein, D. [2008], Discriminative log-linear grammars with latent variables, in \u2018Advances in neural information processing systems\u2019, pp. 1153\u20131160.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petrov%2C%20S.%20Klein%2C%20D.%20Discriminative%20log-linear%20grammars%20with%20latent%20variables%2C%20in%20%E2%80%98Advances%20in%20neural%20information%20processing%20systems%E2%80%99"
        },
        {
            "id": "21",
            "entry": "[21] Ping, W., Liu, Q. and Ihler, A. [2014], \u2018Marginal structured SVM with hidden variables\u2019, International Conference on Machine Learning pp. 190\u2013198.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ping%2C%20W.%20Liu%2C%20Q.%20Ihler%2C%20A.%20Marginal%20structured%20SVM%20with%20hidden%20variables%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ping%2C%20W.%20Liu%2C%20Q.%20Ihler%2C%20A.%20Marginal%20structured%20SVM%20with%20hidden%20variables%202014"
        },
        {
            "id": "22",
            "entry": "[22] Quattoni, A., Collins, M. and Darrell, T. [2005], Conditional random fields for object recognition, in \u2018Advances in neural information processing systems\u2019, pp. 1097\u20131104.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quattoni%2C%20A.%20Collins%2C%20M.%20Darrell%2C%20T.%20Conditional%20random%20fields%20for%20object%20recognition%2C%20in%20%E2%80%98Advances%20in%20neural%20information%20processing%20systems%E2%80%99"
        },
        {
            "id": "23",
            "entry": "[23] Quattoni, A., Wang, S., Morency, L.-P., Collins, M. and Darrell, T. [2007], \u2018Hidden conditional random fields\u2019, IEEE transactions on pattern analysis and machine intelligence 29(10).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quattoni%2C%20A.%20Wang%2C%20S.%20Morency%2C%20L.-P.%20Collins%2C%20M.%20Hidden%20conditional%20random%20fields",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quattoni%2C%20A.%20Wang%2C%20S.%20Morency%2C%20L.-P.%20Collins%2C%20M.%20Hidden%20conditional%20random%20fields"
        },
        {
            "id": "24",
            "entry": "[24] Sarawagi, S. and Gupta, R. [2008], Accurate max-margin training for structured output spaces, in \u2018Proceedings of the 25th international conference on Machine learning\u2019, ACM, pp. 888\u2013895.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sarawagi%2C%20S.%20Gupta%2C%20R.%20%5B2008%20Accurate%20max-margin%20training%20for%20structured%20output%20spaces%2C%20in%20%E2%80%98Proceedings",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sarawagi%2C%20S.%20Gupta%2C%20R.%20%5B2008%20Accurate%20max-margin%20training%20for%20structured%20output%20spaces%2C%20in%20%E2%80%98Proceedings"
        },
        {
            "id": "25",
            "entry": "[25] Taskar, B., Guestrin, C. and Koller, D. [2003], \u2018Max-margin Markov networks\u2019, Neural Information Processing Systems 16, 25\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taskar%2C%20B.%20Guestrin%2C%20C.%20Koller%2C%20D.%20Max-margin%20Markov%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taskar%2C%20B.%20Guestrin%2C%20C.%20Koller%2C%20D.%20Max-margin%20Markov%20networks"
        },
        {
            "id": "26",
            "entry": "[26] Tsochantaridis, I., Joachims, T., Hofmann, T. and Altun, Y. [2005], \u2018Large margin methods for structured and interdependent output variables\u2019, Journal of machine learning research 6(Sep), 1453\u20131484.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsochantaridis%2C%20I.%20Joachims%2C%20T.%20Hofmann%2C%20T.%20Altun%2C%20Y.%20Large%20margin%20methods%20for%20structured%20and%20interdependent%20output%20variables",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsochantaridis%2C%20I.%20Joachims%2C%20T.%20Hofmann%2C%20T.%20Altun%2C%20Y.%20Large%20margin%20methods%20for%20structured%20and%20interdependent%20output%20variables"
        },
        {
            "id": "27",
            "entry": "[27] Volkovs, M. and Zemel, R. S. [2012], Efficient sampling for bipartite matching problems, in \u2018Advances in Neural Information Processing Systems\u2019, pp. 1313\u20131321.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Volkovs%2C%20M.%20Zemel%2C%20R.S.%20%5B2012%5D%2C%20Efficient%20sampling%20for%20bipartite%20matching%20problems%2C%20in%20%E2%80%98Advances",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Volkovs%2C%20M.%20Zemel%2C%20R.S.%20%5B2012%5D%2C%20Efficient%20sampling%20for%20bipartite%20matching%20problems%2C%20in%20%E2%80%98Advances"
        },
        {
            "id": "28",
            "entry": "[28] Wang, H., Gould, S. and Roller, D. [2013], \u2018Discriminative learning with latent variables for cluttered indoor scene understanding\u2019, Communications of the ACM 56(4), 92\u201399.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20H.%20Gould%2C%20S.%20Roller%2C%20D.%20Discriminative%20learning%20with%20latent%20variables%20for%20cluttered%20indoor%20scene%20understanding",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20H.%20Gould%2C%20S.%20Roller%2C%20D.%20Discriminative%20learning%20with%20latent%20variables%20for%20cluttered%20indoor%20scene%20understanding"
        },
        {
            "id": "29",
            "entry": "[29] Wang, S. B., Quattoni, A., Morency, L.-P., Demirdjian, D. and Darrell, T. [2006], Hidden conditional random fields for gesture recognition, in \u2018Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on\u2019, Vol. 2, IEEE, pp. 1521\u20131527.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20S.B.%20Quattoni%2C%20A.%20Morency%2C%20L.-P.%20Demirdjian%2C%20D.%20Hidden%20conditional%20random%20fields%20for%20gesture%20recognition%2C%20in%20%E2%80%98Computer%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20S.B.%20Quattoni%2C%20A.%20Morency%2C%20L.-P.%20Demirdjian%2C%20D.%20Hidden%20conditional%20random%20fields%20for%20gesture%20recognition%2C%20in%20%E2%80%98Computer%202006"
        },
        {
            "id": "30",
            "entry": "[30] Yu, C. and Joachims, T. [2009], \u2018Learning structural SVMs with latent variables\u2019, International Conference on Machine Learning pp. 1169\u20131176.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20C.%20Joachims%2C%20T.%20Learning%20structural%20SVMs%20with%20latent%20variables",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20C.%20Joachims%2C%20T.%20Learning%20structural%20SVMs%20with%20latent%20variables"
        },
        {
            "id": "31",
            "entry": "[31] Yuille, A. L. and Rangarajan, A. [2002], The concave-convex procedure (cccp), in \u2018Advances in neural information processing systems\u2019, pp. 1033\u20131040.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuille%2C%20A.L.%20Rangarajan%2C%20A.%20%5B2002%5D%2C%20The%20concave-convex%20procedure%20%28cccp%29%2C%20in%20%E2%80%98Advances%20in%20neural%20information%20processing%20systems%E2%80%99"
        },
        {
            "id": "32",
            "entry": "[32] Zhang, Y., Lei, T., Barzilay, R. and Jaakkola, T. [2014], \u2018Greed is good if randomized: New inference for dependency parsing\u2019, Empirical Methods in Natural Language Processing pp. 1013\u2013 1024.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Lei%2C%20T.%20Barzilay%2C%20R.%20Jaakkola%2C%20T.%20Greed%20is%20good%20if%20randomized%3A%20New%20inference%20for%20dependency%20parsing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Lei%2C%20T.%20Barzilay%2C%20R.%20Jaakkola%2C%20T.%20Greed%20is%20good%20if%20randomized%3A%20New%20inference%20for%20dependency%20parsing%202014"
        },
        {
            "id": "33",
            "entry": "[33] Zhang, Y., Li, C., Barzilay, R. and Darwish, K. [2015], \u2018Randomized greedy inference for joint segmentation, POS tagging and dependency parsing\u2019, North American Chapter of the Association for Computational Linguistics pp. 42\u201352. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Li%2C%20C.%20Barzilay%2C%20R.%20Darwish%2C%20K.%20Randomized%20greedy%20inference%20for%20joint%20segmentation%2C%20POS%20tagging%20and%20dependency%20parsing%202015"
        }
    ]
}
