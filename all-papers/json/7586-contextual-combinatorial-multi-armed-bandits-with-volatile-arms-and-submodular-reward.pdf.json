{
    "filename": "7586-contextual-combinatorial-multi-armed-bandits-with-volatile-arms-and-submodular-reward.pdf",
    "metadata": {
        "title": "Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward",
        "author": "Lixing Chen, Jie Xu, Zhuo Lu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7586-contextual-combinatorial-multi-armed-bandits-with-volatile-arms-and-submodular-reward.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we study the stochastic contextual combinatorial multi-armed bandit (CC-MAB) framework that is tailored for volatile arms and submodular reward functions. CC-MAB inherits properties from both contextual bandit and combinatorial bandit: it aims to select a set of arms in each round based on the side information (a.k.a. context) associated with the arms. By \u201cvolatile arms\u201d, we mean that the available arms to select from in each round may change; and by \u201csubmodular rewards\u201d, we mean that the total reward achieved by selected arms is not a simple sum of individual rewards but demonstrates a feature of diminishing returns determined by the relations between selected arms (e.g. relevance and redundancy). Volatile arms and submodular rewards are often seen in many real-world applications, e.g. recommender systems and crowdsourcing, in which multi-armed bandit (MAB) based strategies are extensively applied. Although there exist works that investigate these issues separately based on standard MAB, jointly considering all these issues in a single MAB problem requires very different algorithm design and regret analysis. Our algorithm CC-MAB provides an online decision-making policy in a contextual and combinatorial bandit setting and effectively addresses the issues raised by volatile arms and submodular reward functions. The proposed"
    },
    "keywords": [
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "Multi-armed bandit",
            "url": "https://en.wikipedia.org/wiki/Multi-armed_bandit"
        }
    ],
    "highlights": [
        "Multi-armed bandit (MAB) problems are among the most fundamental sequential decision problems with an exploration vs. exploitation trade-off",
        "We develop a novel online decision-making approach based on contextual and combinatorial bandit to address various challenges caused by volatile arms and submodular reward functions",
        "We presented a framework called contextual combinatorial multi-armed bandit that accommodates combinatorial nature of contextual arms",
        "An efficient algorithm contextual combinatorial Multi-armed bandit problem was proposed, which is tailored to volatile arms and submodular reward functions",
        "Experiments on real-world crowdsourcing data demonstrated that our algorithm helps to explore and exploit arms\u2019 reward by considering the context information and the submodularity of reward function and improves the cumulative reward compared to many existing Multi-armed bandit algorithms",
        "contextual combinatorial Multi-armed bandit problem currently creates static context partitions during initialization which may be inappropriate in certain cases, a meaningful extension is to generate appropriate partitions dynamically over time based on the distribution of arrived arms on the context space"
    ],
    "key_statements": [
        "Multi-armed bandit (MAB) problems are among the most fundamental sequential decision problems with an exploration vs. exploitation trade-off",
        "We develop a novel online decision-making approach based on contextual and combinatorial bandit to address various challenges caused by volatile arms and submodular reward functions",
        "The main contribution of this paper is summarized as follows: (i) We propose a contextual combinatorial multi-armed bandit algorithm (CC-Multi-armed bandit) framework that is compatible with submodular reward functions and volatile arms. We rigorously prove the performance guarantee of the proposed contextual combinatorial Multi-armed bandit problem, which shows a O) regret upper bound after playing T rounds. We evaluate the proposed algorithm on a real-world dataset as a crowdsourcing problem",
        "Figure 3 shows the cumulative system rewards achieved by contextual combinatorial Multi-armed bandit problem and 5 benchmark algorithms",
        "We presented a framework called contextual combinatorial multi-armed bandit that accommodates combinatorial nature of contextual arms",
        "An efficient algorithm contextual combinatorial Multi-armed bandit problem was proposed, which is tailored to volatile arms and submodular reward functions",
        "Experiments on real-world crowdsourcing data demonstrated that our algorithm helps to explore and exploit arms\u2019 reward by considering the context information and the submodularity of reward function and improves the cumulative reward compared to many existing Multi-armed bandit algorithms",
        "contextual combinatorial Multi-armed bandit problem currently creates static context partitions during initialization which may be inappropriate in certain cases, a meaningful extension is to generate appropriate partitions dynamically over time based on the distribution of arrived arms on the context space"
    ],
    "summary": [
        "Multi-armed bandit (MAB) problems are among the most fundamental sequential decision problems with an exploration vs. exploitation trade-off.",
        "The main contribution of this paper is summarized as follows: (i) We propose a contextual combinatorial multi-armed bandit algorithm (CC-MAB) framework that is compatible with submodular reward functions and volatile arms.",
        "Combinatorial bandit and submodular reward function: Efforts have been made to generalize the classical MAB problems to combinatorial bandit [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] which allows the decision makers to pull a set of arms in each round.",
        "The considered submodular reward function is general, and is featured by the diminishing returns property: given the available arm set M and the corresponding quality r in an arbitrary time slot, for all possible arm subsets S \u2286 B \u2286 M and any arm m \u2208/ B, we have u(r, {m} \u222a S) \u2212 u(r, S) \u2265 u(r, {m} \u222a B) \u2212 u(r, B).",
        "Let us consider an extreme case that the arms arrived in every slot are completely new, the decision maker is unable to play an arm several times to learn the expected quality as in standard MAB algorithms and it is meaningless to do that since the arm will not appear again.",
        "In each time slot t, CC-MAB performs the following steps: the context of arrived arms xt =",
        "The form of upper regret bound for CC-MAB is very different from that for many existing MAB algorithms, e.g. contextual/combinatorial/volatile MAB, which are developed on a finite arm set.",
        "Consider the special case of B = M max, CC-MAB is identical to the oracle algorithm, i.e., choosing all arms arriving in each time slot and the regret is 0.",
        "1) Arms and Context: We divide the time span of the dataset into daily instances, and every day a set of users are selected to review businesses.",
        "This is due to the fact that the most beneficial arms can be efficiently identified by Oracle and CC-MAB algorithms by considering the submodularity of the reward function, and the arms that are left out only offer little marginal reward.",
        "The regret upper bound is proved for an arbitrary submodular function and an arbitrarily large arm set in each round.",
        "An efficient algorithm CC-MAB was proposed, which is tailored to volatile arms and submodular reward functions.",
        "Experiments on real-world crowdsourcing data demonstrated that our algorithm helps to explore and exploit arms\u2019 reward by considering the context information and the submodularity of reward function and improves the cumulative reward compared to many existing MAB algorithms.",
        "CC-MAB currently creates static context partitions during initialization which may be inappropriate in certain cases, a meaningful extension is to generate appropriate partitions dynamically over time based on the distribution of arrived arms on the context space"
    ],
    "headline": "We study the stochastic contextual combinatorial multi-armed bandit  framework that is tailored for volatile arms and submodular reward functions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Using%20confidence%20bounds%20for%20exploitation-exploration%20trade-offs%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Using%20confidence%20bounds%20for%20exploitation-exploration%20trade-offs%202002"
        },
        {
            "id": "2",
            "entry": "[2] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "3",
            "entry": "[3] Z. Bnaya, R. Puzis, R. Stern, and A. Felner. Volatile multi-armed bandits for guaranteed targeted social crawling. In AAAI (Late-Breaking Developments), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bnaya%2C%20Z.%20Puzis%2C%20R.%20Stern%2C%20R.%20Felner%2C%20A.%20Volatile%20multi-armed%20bandits%20for%20guaranteed%20targeted%20social%20crawling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bnaya%2C%20Z.%20Puzis%2C%20R.%20Stern%2C%20R.%20Felner%2C%20A.%20Volatile%20multi-armed%20bandits%20for%20guaranteed%20targeted%20social%20crawling%202013"
        },
        {
            "id": "4",
            "entry": "[4] S. Bubeck, N. Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "5",
            "entry": "[5] L. Chen, A. Krause, and A. Karbasi. Interactive submodular bandit. In Advances in Neural Information Processing Systems, pages 141\u2013152, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20L.%20Krause%2C%20A.%20Karbasi%2C%20A.%20Interactive%20submodular%20bandit%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20L.%20Krause%2C%20A.%20Karbasi%2C%20A.%20Interactive%20submodular%20bandit%202017"
        },
        {
            "id": "6",
            "entry": "[6] W. Chen, Y. Wang, and Y. Yuan. Combinatorial multi-armed bandit: General framework and applications. In International Conference on Machine Learning, pages 151\u2013159, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20W.%20Wang%2C%20Y.%20Yuan%2C%20Y.%20Combinatorial%20multi-armed%20bandit%3A%20General%20framework%20and%20applications%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20W.%20Wang%2C%20Y.%20Yuan%2C%20Y.%20Combinatorial%20multi-armed%20bandit%3A%20General%20framework%20and%20applications%202013"
        },
        {
            "id": "7",
            "entry": "[7] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM (JACM), 45(4):634\u2013652, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feige%2C%20U.%20A%20threshold%20of%20ln%20n%20for%20approximating%20set%20cover%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feige%2C%20U.%20A%20threshold%20of%20ln%20n%20for%20approximating%20set%20cover%201998"
        },
        {
            "id": "8",
            "entry": "[8] Y. Gai, B. Krishnamachari, and R. Jain. Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactions on Networking (TON), 20(5):1466\u20131478, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gai%2C%20Y.%20Krishnamachari%2C%20B.%20Jain%2C%20R.%20Combinatorial%20network%20optimization%20with%20unknown%20variables%3A%20Multi-armed%20bandits%20with%20linear%20rewards%20and%20individual%20observations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gai%2C%20Y.%20Krishnamachari%2C%20B.%20Jain%2C%20R.%20Combinatorial%20network%20optimization%20with%20unknown%20variables%3A%20Multi-armed%20bandits%20with%20linear%20rewards%20and%20individual%20observations%202012"
        },
        {
            "id": "9",
            "entry": "[9] E. Hazan and S. Kale. Online submodular minimization. Journal of Machine Learning Research, 13(Oct):2903\u20132922, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Kale%2C%20S.%20Online%20submodular%20minimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Kale%2C%20S.%20Online%20submodular%20minimization%202012"
        },
        {
            "id": "10",
            "entry": "[10] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 58(301):13\u201330, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoeffding%2C%20W.%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoeffding%2C%20W.%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963"
        },
        {
            "id": "11",
            "entry": "[11] R. Kleinberg, A. Niculescu-Mizil, and Y. Sharma. Regret bounds for sleeping experts and bandits. Machine learning, 80(2-3):245\u2013272, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20R.%20Niculescu-Mizil%2C%20A.%20Sharma%2C%20Y.%20Regret%20bounds%20for%20sleeping%20experts%20and%20bandits%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20R.%20Niculescu-Mizil%2C%20A.%20Sharma%2C%20Y.%20Regret%20bounds%20for%20sleeping%20experts%20and%20bandits%202010"
        },
        {
            "id": "12",
            "entry": "[12] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 681\u2013690. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20R.%20Slivkins%2C%20A.%20Upfal%2C%20E.%20Multi-armed%20bandits%20in%20metric%20spaces%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20R.%20Slivkins%2C%20A.%20Upfal%2C%20E.%20Multi-armed%20bandits%20in%20metric%20spaces%202008"
        },
        {
            "id": "13",
            "entry": "[13] R. D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural Information Processing Systems, pages 697\u2013704, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20R.D.%20Nearly%20tight%20bounds%20for%20the%20continuum-armed%20bandit%20problem%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20R.D.%20Nearly%20tight%20bounds%20for%20the%20continuum-armed%20bandit%20problem%202005"
        },
        {
            "id": "14",
            "entry": "[14] A. Krause and C. Guestrin. Beyond convexity: Submodularity in machine learning. ICML Tutorials, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20A.%20Guestrin%2C%20C.%20Beyond%20convexity%3A%20Submodularity%20in%20machine%20learning%202008"
        },
        {
            "id": "15",
            "entry": "[15] J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in neural information processing systems, pages 817\u2013824, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20Zhang%2C%20T.%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20J.%20Zhang%2C%20T.%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008"
        },
        {
            "id": "16",
            "entry": "[16] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Schapire%2C%20R.E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Schapire%2C%20R.E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010"
        },
        {
            "id": "17",
            "entry": "[17] G. L. Nemhauser and L. A. Wolsey. Best algorithms for approximating the maximum of a submodular set function. Mathematics of operations research, 3(3):177\u2013188, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Best%20algorithms%20for%20approximating%20the%20maximum%20of%20a%20submodular%20set%20function%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Best%20algorithms%20for%20approximating%20the%20maximum%20of%20a%20submodular%20set%20function%201978"
        },
        {
            "id": "18",
            "entry": "[18] L. Qin, S. Chen, and X. Zhu. Contextual combinatorial bandit and its application on diversified online recommendation. In Proceedings of the 2014 SIAM International Conference on Data Mining, pages 461\u2013469. SIAM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qin%2C%20L.%20Chen%2C%20S.%20Zhu%2C%20X.%20Contextual%20combinatorial%20bandit%20and%20its%20application%20on%20diversified%20online%20recommendation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qin%2C%20L.%20Chen%2C%20S.%20Zhu%2C%20X.%20Contextual%20combinatorial%20bandit%20and%20its%20application%20on%20diversified%20online%20recommendation%202014"
        },
        {
            "id": "19",
            "entry": "[19] G. Radanovic, A. Singla, A. Krause, and B. Faltings. Information gathering with peers: Submodular optimization with peer-prediction constraints. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI\u201918), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radanovic%2C%20G.%20Singla%2C%20A.%20Krause%2C%20A.%20Faltings%2C%20B.%20Information%20gathering%20with%20peers%3A%20Submodular%20optimization%20with%20peer-prediction%20constraints%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radanovic%2C%20G.%20Singla%2C%20A.%20Krause%2C%20A.%20Faltings%2C%20B.%20Information%20gathering%20with%20peers%3A%20Submodular%20optimization%20with%20peer-prediction%20constraints%202018"
        },
        {
            "id": "20",
            "entry": "[20] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th international conference on Machine learning, pages 784\u2013 791. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radlinski%2C%20F.%20Kleinberg%2C%20R.%20Joachims%2C%20T.%20Learning%20diverse%20rankings%20with%20multi-armed%20bandits%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radlinski%2C%20F.%20Kleinberg%2C%20R.%20Joachims%2C%20T.%20Learning%20diverse%20rankings%20with%20multi-armed%20bandits%202008"
        },
        {
            "id": "21",
            "entry": "[21] P. Yang, N. Zhang, S. Zhang, K. Yang, L. Yu, and X. Shen. Identifying the most valuable workers in fog-assisted spatial crowdsourcing. IEEE Internet of Things Journal, 4(5):1193\u2013 1203, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20P.%20Zhang%2C%20N.%20Zhang%2C%20S.%20Yang%2C%20K.%20Identifying%20the%20most%20valuable%20workers%20in%20fog-assisted%20spatial%20crowdsourcing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20P.%20Zhang%2C%20N.%20Zhang%2C%20S.%20Yang%2C%20K.%20Identifying%20the%20most%20valuable%20workers%20in%20fog-assisted%20spatial%20crowdsourcing%202017"
        },
        {
            "id": "22",
            "entry": "[22] Y. Yue and C. Guestrin. Linear submodular bandits and their application to diversified retrieval. In Advances in Neural Information Processing Systems, pages 2483\u20132491, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yue%2C%20Y.%20Guestrin%2C%20C.%20Linear%20submodular%20bandits%20and%20their%20application%20to%20diversified%20retrieval%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yue%2C%20Y.%20Guestrin%2C%20C.%20Linear%20submodular%20bandits%20and%20their%20application%20to%20diversified%20retrieval%202011"
        },
        {
            "id": "23",
            "entry": "[23] Y. Zhang and M. van der Schaar. Information production and link formation in social computing systems. IEEE Journal on selected Areas in communications, 30(11):2136\u20132145, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20van%20der%20Schaar%2C%20M.%20Information%20production%20and%20link%20formation%20in%20social%20computing%20systems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20van%20der%20Schaar%2C%20M.%20Information%20production%20and%20link%20formation%20in%20social%20computing%20systems%202012"
        }
    ]
}
