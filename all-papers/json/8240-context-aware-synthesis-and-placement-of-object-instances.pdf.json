{
    "filename": "8240-context-aware-synthesis-and-placement-of-object-instances.pdf",
    "metadata": {
        "title": "Context-aware Synthesis and Placement of Object Instances",
        "author": "Donghoon Lee, Ming-Yu Liu, Ming-Hsuan Yang, Sifei Liu, Jinwei Gu, Jan Kautz",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8240-context-aware-synthesis-and-placement-of-object-instances.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning to insert an object instance into an image in a semantically coherent manner is a challenging and interesting problem. Solving it requires (a) determining a location to place an object in the scene and (b) determining its appearance at the location. Such an object insertion model can potentially facilitate numerous image editing and scene parsing applications. In this paper, we propose an end-to-end trainable neural network for the task of inserting an object instance mask of a specified class into the semantic label map of an image. Our network consists of two generative modules where one determines where the inserted object mask should be (i.e., location and scale) and the other determines what the object mask shape (and pose) should look like. The two modules are connected together via a spatial transformation network and jointly trained. We devise a learning procedure that leverage both supervised and unsupervised data and show our model can insert an object at diverse locations with various appearances. We conduct extensive experimental validations with comparisons to strong baselines to verify the effectiveness of the proposed network."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "image synthesis",
            "url": "https://en.wikipedia.org/wiki/image_synthesis"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        }
    ],
    "highlights": [
        "Inserting objects into an image that conforms to scene semantics is a challenging and interesting task",
        "The task is closely related to many real-world applications, including image synthesis, AR and VR content editing and domain randomization [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "Numerous methods [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] have recently been proposed to generate realistic images based on generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "The main technical novelty of this work is to construct an end-to-end trainable neural network that can sample plausible locations and shapes for the new object from its joint distribution conditioned on the input semantic label map",
        "With the affine transformation learned from the spatial transformation network, the synthesized object instance is placed into the input semantic map as the final output. As both the where module and the what module aim to learn the distributions of the location and the shape of object instances conditioned on the input semantic map, they are both generative models implemented with generative adversarial networks",
        "We construct an end-to-end trainable neural network that can sample the plausible locations and shapes for inserting an object mask into a segmentation label map, from their joint distribution conditioned on the semantic label map"
    ],
    "key_statements": [
        "Inserting objects into an image that conforms to scene semantics is a challenging and interesting task",
        "The task is closely related to many real-world applications, including image synthesis, AR and VR content editing and domain randomization [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "Numerous methods [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] have recently been proposed to generate realistic images based on generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "Is it possible to learn this conditional probabilistic distribution of object instances in order to generate novel semantic maps? In this paper, we propose a conditional generative adversarial networks framework for the task",
        "Our work differs from the ST-generative adversarial networks work in two ways",
        "Our algorithm operates in the semantic label map space",
        "The main technical novelty of this work is to construct an end-to-end trainable neural network that can sample plausible locations and shapes for the new object from its joint distribution conditioned on the input semantic label map",
        "We present a novel and flexible solution to synthesize and place object instances in images, with a focus on semantic label maps",
        "With the affine transformation learned from the spatial transformation network, the synthesized object instance is placed into the input semantic map as the final output. As both the where module and the what module aim to learn the distributions of the location and the shape of object instances conditioned on the input semantic map, they are both generative models implemented with generative adversarial networks",
        "As shown in Figure 2, given the input semantic map x, the where module aims to learn the conditional distribution of the location and size of object instances valid for the given scene context",
        "We find that for 43% of the time that a worker chooses the object instance synthesized and inserted by our approach as the real one instead of a real object instance in the original image",
        "We construct an end-to-end trainable neural network that can sample the plausible locations and shapes for inserting an object mask into a segmentation label map, from their joint distribution conditioned on the semantic label map"
    ],
    "summary": [
        "Inserting objects into an image that conforms to scene semantics is a challenging and interesting task.",
        "We aim to learn the distribution of not only locations but shapes of objects conditioned on the input semantic label map.",
        "The main technical novelty of this work is to construct an end-to-end trainable neural network that can sample plausible locations and shapes for the new object from its joint distribution conditioned on the input semantic label map.",
        "Different from the proposed approach, none of the existing methods are constructed based on end-to-end trainable networks, in which location prediction, as well as object generation, can be regularized and optimized jointly.",
        "Given the semantic map as an input, our model first predicts possible locations where an object instance is likely to appear.",
        "As both the where module and the what module aim to learn the distributions of the location and the shape of object instances conditioned on the input semantic map, they are both generative models implemented with GANs. To reduce mode collapse, during training, both the where module and the what module consist of two parallel paths \u2014 a supervised path and an unsupervised path, as shown in Figure 2 and 3.",
        "As shown in Figure 2, given the input semantic map x, the where module aims to learn the conditional distribution of the location and size of object instances valid for the given scene context.",
        "As shown in Figure 3, given the input semantic map x, the what module aims to learn the shape distribution of object instances, with which the inserted object instance fits naturally within the surrounding context.",
        "The input to the generator network Gs is a semantic map x with a bounding box A(b), and a random vector zs, while the output is a binary mask of the instance shape s, i.e., Gs(x \u2295 A(b), zs) = s.",
        "Similar to the location prediction network, as shown in Figure 3(a), we set a minmax game between Gs and discriminators Ds as minGs maxDs Ls(Gs, Ds) where Ds consists of Dlianysotaunt ce, which checks whether the new instance fits into the entire scene, and Dshape, which examines whether the generated shape of the object s is real or fake.",
        "We construct an end-to-end trainable neural network that can sample the plausible locations and shapes for inserting an object mask into a segmentation label map, from their joint distribution conditioned on the semantic label map.",
        "One of the interesting future works would be handling occlusions between objects"
    ],
    "headline": "We propose an end-to-end trainable neural network for the task of inserting an object instance mask of a specified class into the semantic label map of an image",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Bar and S. Ullman. Spatial context in recognition. Perception, 25(3):343\u2013352, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bar%2C%20M.%20Ullman%2C%20S.%20Spatial%20context%20in%20recognition%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bar%2C%20M.%20Ullman%2C%20S.%20Spatial%20context%20in%20recognition%201996"
        },
        {
            "id": "2",
            "entry": "[2] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousmalis%2C%20K.%20Silberman%2C%20N.%20Dohan%2C%20D.%20Erhan%2C%20D.%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousmalis%2C%20K.%20Silberman%2C%20N.%20Dohan%2C%20D.%20Erhan%2C%20D.%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016"
        },
        {
            "id": "4",
            "entry": "[4] S. K. Divvala, D. Hoiem, J. H. Hays, A. A. Efros, and M. Hebert. An empirical study of context in object detection. In IEEE Conference on Computer Vision and Pattern Recognition, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Divvala%2C%20S.K.%20Hoiem%2C%20D.%20Hays%2C%20J.H.%20Efros%2C%20A.A.%20An%20empirical%20study%20of%20context%20in%20object%20detection%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Divvala%2C%20S.K.%20Hoiem%2C%20D.%20Hays%2C%20J.H.%20Efros%2C%20A.A.%20An%20empirical%20study%20of%20context%20in%20object%20detection%202009"
        },
        {
            "id": "5",
            "entry": "[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20Neural%20Information%20Processing%20Systems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20Neural%20Information%20Processing%20Systems%202014"
        },
        {
            "id": "6",
            "entry": "[6] S. Hong, D. Yang, J. Choi, and H. Lee. Inferring semantic layout for hierarchical text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20S.%20Yang%2C%20D.%20Choi%2C%20J.%20Lee%2C%20H.%20Inferring%20semantic%20layout%20for%20hierarchical%20text-to-image%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20S.%20Yang%2C%20D.%20Choi%2C%20J.%20Lee%2C%20H.%20Inferring%20semantic%20layout%20for%20hierarchical%20text-to-image%20synthesis%202018"
        },
        {
            "id": "7",
            "entry": "[7] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In European Conference on Computer Vision, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018"
        },
        {
            "id": "8",
            "entry": "[8] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "9",
            "entry": "[9] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks. In Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Jaderberg%20K%20Simonyan%20A%20Zisserman%20and%20K%20Kavukcuoglu%20Spatial%20transformer%20networks%20In%20Neural%20Information%20Processing%20Systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Jaderberg%20K%20Simonyan%20A%20Zisserman%20and%20K%20Kavukcuoglu%20Spatial%20transformer%20networks%20In%20Neural%20Information%20Processing%20Systems%202015"
        },
        {
            "id": "10",
            "entry": "[10] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "11",
            "entry": "[11] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "12",
            "entry": "[12] A. B. L. Larsen, S. K. S\u00f8nderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20B.%20L.%20Larsen%2C%20S.%20K.%20S%C3%B8nderby%2C%20H.%20Larochelle%20Winther%2C%20O.%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20B.%20L.%20Larsen%2C%20S.%20K.%20S%C3%B8nderby%2C%20H.%20Larochelle%20Winther%2C%20O.%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016"
        },
        {
            "id": "13",
            "entry": "[13] Y. Li, S. Liu, J. Yang, and M.-H. Yang. Generative face completion. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Liu%2C%20S.%20Yang%2C%20J.%20Yang%2C%20M.-H.%20Generative%20face%20completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Liu%2C%20S.%20Yang%2C%20J.%20Yang%2C%20M.-H.%20Generative%20face%20completion%202017"
        },
        {
            "id": "14",
            "entry": "[14] C.-H. Lin, E. Yumer, O. Wang, E. Shechtman, and S. Lucey. ST-GAN: Spatial transformer generative adversarial networks for image compositing. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20C.-H.%20Yumer%2C%20E.%20Wang%2C%20O.%20Shechtman%2C%20E.%20ST-GAN%3A%20Spatial%20transformer%20generative%20adversarial%20networks%20for%20image%20compositing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20C.-H.%20Yumer%2C%20E.%20Wang%2C%20O.%20Shechtman%2C%20E.%20ST-GAN%3A%20Spatial%20transformer%20generative%20adversarial%20networks%20for%20image%20compositing%202018"
        },
        {
            "id": "15",
            "entry": "[15] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. In Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.-Y.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.-Y.%20Tuzel%2C%20O.%20Coupled%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.-Y.%20Tuzel%2C%20O.%20Coupled%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] X. Ouyang, Y. Cheng, Y. Jiang, C.-L. Li, and P. Zhou. Pedestrian-Synthesis-GAN: Generating pedestrian data in real scene and beyond. arXiv preprint arXiv:1804.02047, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02047"
        },
        {
            "id": "18",
            "entry": "[18] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Krahenbuhl%2C%20P.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Krahenbuhl%2C%20P.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "19",
            "entry": "[19] X. Qi, Q. Chen, J. Jia, and V. Koltun. Semi-parametric image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20X.%20Chen%2C%20Q.%20Jia%2C%20J.%20Koltun%2C%20V.%20Semi-parametric%20image%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20X.%20Chen%2C%20Q.%20Jia%2C%20J.%20Koltun%2C%20V.%20Semi-parametric%20image%20synthesis%202018"
        },
        {
            "id": "20",
            "entry": "[20] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "21",
            "entry": "[21] J. Redmon and A. Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02767"
        },
        {
            "id": "22",
            "entry": "[22] S. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee. Learning what and where to draw. In Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Akata%2C%20Z.%20Mohan%2C%20S.%20Tenka%2C%20S.%20Learning%20what%20and%20where%20to%20draw%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Akata%2C%20Z.%20Mohan%2C%20S.%20Tenka%2C%20S.%20Learning%20what%20and%20where%20to%20draw%202016"
        },
        {
            "id": "23",
            "entry": "[23] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "24",
            "entry": "[24] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrivastava%2C%20A.%20Pfister%2C%20T.%20Tuzel%2C%20O.%20Susskind%2C%20J.%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrivastava%2C%20A.%20Pfister%2C%20T.%20Tuzel%2C%20O.%20Susskind%2C%20J.%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%202017"
        },
        {
            "id": "25",
            "entry": "[25] J. Sun and D. W. Jacobs. Seeing what is not there: Learning context to determine where objects are missing. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20J.%20Jacobs%2C%20D.W.%20Seeing%20what%20is%20not%20there%3A%20Learning%20context%20to%20determine%20where%20objects%20are%20missing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20J.%20Jacobs%2C%20D.W.%20Seeing%20what%20is%20not%20there%3A%20Learning%20context%20to%20determine%20where%20objects%20are%20missing%202017"
        },
        {
            "id": "26",
            "entry": "[26] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-domain image generation. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taigman%2C%20Y.%20Polyak%2C%20A.%20Wolf%2C%20L.%20Unsupervised%20cross-domain%20image%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taigman%2C%20Y.%20Polyak%2C%20A.%20Wolf%2C%20L.%20Unsupervised%20cross-domain%20image%20generation%202017"
        },
        {
            "id": "27",
            "entry": "[27] F. Tan, C. Bernier, B. Cohen, V. Ordonez, and C. Barnes. Where and who? Automatic semanticaware person composition. In IEEE Winter Conference on Applications of Computer Vision, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tan%2C%20F.%20Bernier%2C%20C.%20Cohen%2C%20B.%20Ordonez%2C%20V.%20Where%20and%20who%3F%20Automatic%20semanticaware%20person%20composition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tan%2C%20F.%20Bernier%2C%20C.%20Cohen%2C%20B.%20Ordonez%2C%20V.%20Where%20and%20who%3F%20Automatic%20semanticaware%20person%20composition%202018"
        },
        {
            "id": "28",
            "entry": "[28] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tobin%2C%20J.%20Fong%2C%20R.%20Ray%2C%20A.%20Schneider%2C%20J.%20Domain%20randomization%20for%20transferring%20deep%20neural%20networks%20from%20simulation%20to%20the%20real%20world%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tobin%2C%20J.%20Fong%2C%20R.%20Ray%2C%20A.%20Schneider%2C%20J.%20Domain%20randomization%20for%20transferring%20deep%20neural%20networks%20from%20simulation%20to%20the%20real%20world%202017"
        },
        {
            "id": "29",
            "entry": "[29] A. Torralba. Contextual priming for object detection. International Journal of Computer Vision, 53(2):169\u2013191, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Torralba%2C%20A.%20Contextual%20priming%20for%20object%20detection%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Torralba%2C%20A.%20Contextual%20priming%20for%20object%20detection%202003"
        },
        {
            "id": "30",
            "entry": "[30] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz, and B. Catanzaro. Video-to-video synthesis. In Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Liu%2C%20G.%20Video-to-video%20synthesis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Liu%2C%20G.%20Video-to-video%20synthesis%202018"
        },
        {
            "id": "31",
            "entry": "[31] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Tao%2C%20A.%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20T.-C.%20Liu%2C%20M.-Y.%20Zhu%2C%20J.-Y.%20Tao%2C%20A.%20High-resolution%20image%20synthesis%20and%20semantic%20manipulation%20with%20conditional%20GANs%202018"
        },
        {
            "id": "32",
            "entry": "[32] X. Wang, R. Girdhar, and A. Gupta. Binge watching: Scaling affordance learning from sitcoms. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Girdhar%2C%20R.%20Gupta%2C%20A.%20Binge%20watching%3A%20Scaling%20affordance%20learning%20from%20sitcoms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Girdhar%2C%20R.%20Gupta%2C%20A.%20Binge%20watching%3A%20Scaling%20affordance%20learning%20from%20sitcoms%202017"
        },
        {
            "id": "33",
            "entry": "[33] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE International Conference on Computer Vision, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Xu%2C%20T.%20Li%2C%20H.%20Zhang%2C%20S.%20StackGAN%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Xu%2C%20T.%20Li%2C%20H.%20Zhang%2C%20S.%20StackGAN%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "34",
            "entry": "[34] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networkss. In IEEE International Conference on Computer Vision, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networkss%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networkss%202017"
        },
        {
            "id": "35",
            "entry": "[35] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. In Neural Information Processing Systems, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017"
        }
    ]
}
