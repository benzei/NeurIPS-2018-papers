{
    "filename": "7404-image-to-image-translation-for-cross-domain-disentanglement.pdf",
    "metadata": {
        "title": "Image-to-image translation for cross-domain disentanglement",
        "author": "Abel Gonzalez-Garcia, Joost van de Weijer, Yoshua Bengio",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7404-image-to-image-translation-for-cross-domain-disentanglement.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of crossdomain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domainspecific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-ofthe-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets."
    },
    "keywords": [
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "internal representation",
            "url": "https://en.wikipedia.org/wiki/internal_representation"
        }
    ],
    "highlights": [
        "Deep learning has greatly improved the quality of image-to-image translation methods",
        "Our approach is similar to the recent work of Zhu et al [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>], we explicitly model variations in both domains, whereas they only consider variations in the output domain; Cross domain retrieval: we can retrieve similar images in both domains based on the part of the representation that is shared between the domains, and, unlike [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], we do not require labeled data to learn the shared representation; Domain-specific image transfer: domain-specific features can be transferred between images; and Domain-specific interpolation: we can interpolate between two images with respect to domain-specific features",
        "We demonstrate the disentanglement properties of our method on variations on the MNIST dataset [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], and apply it to bidirectional multi-modal image translation in more complex datasets [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], achieving better results than state-of-the-art methods [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] due to the finer control and generality granted by our disentangled representation",
        "We explore here the applicability of our method to other datasets by presenting cross-domain retrieval experiments on two realistic datasets: maps \u2194 satellite images [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] and labels \u2194 facades [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>], and comparing with BicycleGAN [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>]",
        "Our model effectively disentangles the representation into a part shared across domains and two parts exclusive to each domain",
        "We introduced the many-to-many image translation setting and paved the way to overcome some limitations of current approaches through the use of a disentangled representation"
    ],
    "key_statements": [
        "Deep learning has greatly improved the quality of image-to-image translation methods",
        "Where the aim is to map a grayscale image to a plausible colored image of the same scene [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>], and semantic segmentation, where an RGB image is translated to a map indicating the semantic class of each pixel in the RGB image [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "Our approach is similar to the recent work of Zhu et al [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>], we explicitly model variations in both domains, whereas they only consider variations in the output domain; Cross domain retrieval: we can retrieve similar images in both domains based on the part of the representation that is shared between the domains, and, unlike [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], we do not require labeled data to learn the shared representation; Domain-specific image transfer: domain-specific features can be transferred between images; and Domain-specific interpolation: we can interpolate between two images with respect to domain-specific features",
        "We demonstrate the disentanglement properties of our method on variations on the MNIST dataset [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], and apply it to bidirectional multi-modal image translation in more complex datasets [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], achieving better results than state-of-the-art methods [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] due to the finer control and generality granted by our disentangled representation",
        "We found out that in the considered domains, the exclusive part can be successfully modeled by a 1\u00d71\u00d78 vector, which is later tiled and concatenated with the shared part before decoding",
        "The image translation modules impose three main constraints: (1) the shared part of the representation must be identical for both domains, (2) the exclusive part only has information about its own domain, and (3) the generated output must belong to the other domain",
        "We extend MNIST [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], the handwritten-digit dataset used in [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], with variations that correspond to two different domains",
        "We explore here the applicability of our method to other datasets by presenting cross-domain retrieval experiments on two realistic datasets: maps \u2194 satellite images [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] and labels \u2194 facades [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>], and comparing with BicycleGAN [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>]",
        "We have presented the concept of cross-domain disentanglement and proposed a model to solve it",
        "Our model effectively disentangles the representation into a part shared across domains and two parts exclusive to each domain",
        "We introduced the many-to-many image translation setting and paved the way to overcome some limitations of current approaches through the use of a disentangled representation"
    ],
    "summary": [
        "Deep learning has greatly improved the quality of image-to-image translation methods.",
        "The shared parts of the representations SX and SY of a pair of corresponding images (x, y) \u2208 (X , Y) should contain similar information and be invariant to the domain.",
        "Most image translation approaches [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] are devised for pairs of domains that retain the spatial structure, and a great amount of information is shared between input and output.",
        "The image translation modules impose three main constraints: (1) the shared part of the representation must be identical for both domains, (2) the exclusive part only has information about its own domain, and (3) the generated output must belong to the other domain.",
        "The generated images need not correspond to the input if the encoders learn to map different concepts to the same shared latent representation.",
        "Our representation grants a finer control on the stochastic factors of the generated images as we model variations on the inputs, allowing us to keep selected properties fixed.",
        "The obtained disentangled image features are useful for additional tasks beyond image translation, such as cross-domain retrieval or visual analogies.",
        "Several approaches [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] have attempted to improve on image-toimage translation by disentangling the internal representation into content and style, which enable the effective generation of multi-modal outputs.",
        "Image translation is not the only final task: we demonstrate the generality of our distentangled features by applying them to other tasks such as cross-domain retrieval or domain-specific image transfer.",
        "That the model has no knowledge of the digit in the image as labels are not provided, it effectively learns what information is shared across both domains.",
        "When interpolating on the exclusive part, we generate samples along domain-specific factors of variation, i.e. color, and maintain the digit, which is the shared information.",
        "The shared information between domains is semantic whereas the exclusive is stylistic, and so our disentangled representation enables both semantic and visual retrieval using either of its parts.",
        "The high values obtained by shared features show how using our representation provides an effective approach for cross-domain retrieval, clearly superior to directly using image pixels.",
        "Our method is able to abstract the patterns exhibited for both domains in the shared representation, which results in retrieved images that follow those patterns.",
        "We applied this to multiple tasks such as diverse sample generation, crossdomain retrieval, domain-specific image transfer and interpolation.",
        "We introduced the many-to-many image translation setting and paved the way to overcome some limitations of current approaches through the use of a disentangled representation"
    ],
    "headline": "We investigate and impose structure to the specific representation learned in image-to-image translation models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Almahairi, S. Rajeswar, A. Sordoni, P. Bachman, and A. Courville. Augmented cyclegan: Learning many-to-many mappings from unpaired data. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Almahairi%2C%20A.%20Rajeswar%2C%20S.%20Sordoni%2C%20A.%20Bachman%2C%20P.%20Augmented%20cyclegan%3A%20Learning%20many-to-many%20mappings%20from%20unpaired%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Almahairi%2C%20A.%20Rajeswar%2C%20S.%20Sordoni%2C%20A.%20Bachman%2C%20P.%20Augmented%20cyclegan%3A%20Learning%20many-to-many%20mappings%20from%20unpaired%20data%202018"
        },
        {
            "id": "2",
            "entry": "[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In CVPR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aubry%2C%20M.%20Maturana%2C%20D.%20Efros%2C%20A.A.%20Russell%2C%20B.C.%20Seeing%203d%20chairs%3A%20exemplar%20part-based%202d-3d%20alignment%20using%20a%20large%20dataset%20of%20cad%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aubry%2C%20M.%20Maturana%2C%20D.%20Efros%2C%20A.A.%20Russell%2C%20B.C.%20Seeing%203d%20chairs%3A%20exemplar%20part-based%202d-3d%20alignment%20using%20a%20large%20dataset%20of%20cad%20models%202014"
        },
        {
            "id": "4",
            "entry": "[4] Y. Aytar, L. Castrejon, C. Vondrick, H. Pirsiavash, and A. Torralba. Cross-modal scene networks. IEEE Trans. on PAMI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aytar%2C%20Y.%20Castrejon%2C%20L.%20Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Cross-modal%20scene%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aytar%2C%20Y.%20Castrejon%2C%20L.%20Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Cross-modal%20scene%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE Trans. on PAMI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Badrinarayanan%2C%20V.%20Kendall%2C%20A.%20Cipolla%2C%20R.%20Segnet%3A%20A%20deep%20convolutional%20encoder-decoder%20architecture%20for%20image%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Badrinarayanan%2C%20V.%20Kendall%2C%20A.%20Cipolla%2C%20R.%20Segnet%3A%20A%20deep%20convolutional%20encoder-decoder%20architecture%20for%20image%20segmentation%202017"
        },
        {
            "id": "6",
            "entry": "[6] H. Barrow and J. Tenenbaum. Recovering intrinsic scene characteristics. Comput. Vis. Syst, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barrow%2C%20H.%20Tenenbaum%2C%20J.%20Recovering%20intrinsic%20scene%20characteristics%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barrow%2C%20H.%20Tenenbaum%2C%20J.%20Recovering%20intrinsic%20scene%20characteristics%201978"
        },
        {
            "id": "7",
            "entry": "[7] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Trans. on PAMI, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Courville%2C%20A.%20Vincent%2C%20P.%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Courville%2C%20A.%20Vincent%2C%20P.%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "8",
            "entry": "[8] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20Bousmalis%20G%20Trigeorgis%20N%20Silberman%20D%20Krishnan%20and%20D%20Erhan%20Domain%20separation%20networks%20In%20NIPS%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20Bousmalis%20G%20Trigeorgis%20N%20Silberman%20D%20Krishnan%20and%20D%20Erhan%20Domain%20separation%20networks%20In%20NIPS%202016"
        },
        {
            "id": "9",
            "entry": "[9] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousmalis%2C%20K.%20Silberman%2C%20N.%20Dohan%2C%20D.%20Erhan%2C%20D.%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousmalis%2C%20K.%20Silberman%2C%20N.%20Dohan%2C%20D.%20Erhan%2C%20D.%20Unsupervised%20pixel-level%20domain%20adaptation%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "10",
            "entry": "[10] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "11",
            "entry": "[11] J. Donahue, P. Kr\u00e4henb\u00fchl, and T. Darrell. Adversarial feature learning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20J.%20Kr%C3%A4henb%C3%BChl%2C%20P.%20Darrell%2C%20T.%20Adversarial%20feature%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20J.%20Kr%C3%A4henb%C3%BChl%2C%20P.%20Darrell%2C%20T.%20Adversarial%20feature%20learning%202017"
        },
        {
            "id": "12",
            "entry": "[12] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville. Adversarially learned inference. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20V.%20Belghazi%2C%20I.%20Poole%2C%20B.%20Mastropietro%2C%20O.%20Adversarially%20learned%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20V.%20Belghazi%2C%20I.%20Poole%2C%20B.%20Mastropietro%2C%20O.%20Adversarially%20learned%20inference%202017"
        },
        {
            "id": "13",
            "entry": "[13] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eigen%2C%20D.%20Fergus%2C%20R.%20Predicting%20depth%2C%20surface%20normals%20and%20semantic%20labels%20with%20a%20common%20multi-scale%20convolutional%20architecture%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eigen%2C%20D.%20Fergus%2C%20R.%20Predicting%20depth%2C%20surface%20normals%20and%20semantic%20labels%20with%20a%20common%20multi-scale%20convolutional%20architecture%202015"
        },
        {
            "id": "14",
            "entry": "[14] C. Feutry, P. Piantanida, Y. Bengio, and P. Duhamel. Learning anonymized representations with adversarial neural networks. arXiv preprint arXiv:1802.09386, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09386"
        },
        {
            "id": "15",
            "entry": "[15] S. Fidler, S. Dickinson, and R. Urtasun. 3d object detection and viewpoint estimation with a deformable 3d cuboid model. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fidler%2C%20S.%20Dickinson%2C%20S.%20Urtasun%2C%20R.%203d%20object%20detection%20and%20viewpoint%20estimation%20with%20a%20deformable%203d%20cuboid%20model%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fidler%2C%20S.%20Dickinson%2C%20S.%20Urtasun%2C%20R.%203d%20object%20detection%20and%20viewpoint%20estimation%20with%20a%20deformable%203d%20cuboid%20model%202012"
        },
        {
            "id": "16",
            "entry": "[16] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Y.%20Lempitsky%2C%20V.%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Y.%20Lempitsky%2C%20V.%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015"
        },
        {
            "id": "17",
            "entry": "[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "18",
            "entry": "[18] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "19",
            "entry": "[19] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In ICANN, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Wang%2C%20S.D.%20Transforming%20auto-encoders%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Wang%2C%20S.D.%20Transforming%20auto-encoders%202011"
        },
        {
            "id": "20",
            "entry": "[20] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018"
        },
        {
            "id": "21",
            "entry": "[21] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification. ACM TOG, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Let%20there%20be%20color%21%3A%20joint%20end-to-end%20learning%20of%20global%20and%20local%20image%20priors%20for%20automatic%20image%20colorization%20with%20simultaneous%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Let%20there%20be%20color%21%3A%20joint%20end-to-end%20learning%20of%20global%20and%20local%20image%20priors%20for%20automatic%20image%20colorization%20with%20simultaneous%20classification%202016"
        },
        {
            "id": "22",
            "entry": "[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "23",
            "entry": "[23] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "24",
            "entry": "[24] X. Ji, W. Wang, M. Zhang, and Y. Yang. Cross-domain image retrieval with attention modeling. In ACM Multimedia, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ji%2C%20X.%20Wang%2C%20W.%20Zhang%2C%20M.%20Yang%2C%20Y.%20Cross-domain%20image%20retrieval%20with%20attention%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ji%2C%20X.%20Wang%2C%20W.%20Zhang%2C%20M.%20Yang%2C%20Y.%20Cross-domain%20image%20retrieval%20with%20attention%20modeling%202017"
        },
        {
            "id": "25",
            "entry": "[25] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20Welling%2C%20M.%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20Welling%2C%20M.%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "26",
            "entry": "[26] Y. LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "27",
            "entry": "[27] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. Singh, and M.-H. Yang. Diverse image-to-image translation via disentangled representations. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20H.-Y.%20Tseng%2C%20H.-Y.%20Huang%2C%20J.-B.%20Singh%2C%20M.%20Diverse%20image-to-image%20translation%20via%20disentangled%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20H.-Y.%20Tseng%2C%20H.-Y.%20Huang%2C%20J.-B.%20Singh%2C%20M.%20Diverse%20image-to-image%20translation%20via%20disentangled%20representations%202018"
        },
        {
            "id": "28",
            "entry": "[28] C. Li and M. Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Wand%2C%20M.%20Precomputed%20real-time%20texture%20synthesis%20with%20markovian%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Wand%2C%20M.%20Precomputed%20real-time%20texture%20synthesis%20with%20markovian%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "29",
            "entry": "[29] Y.-C. Liu, Y.-Y. Yeh, T.-C. Fu, S.-D. Wang, W.-C. Chiu, and Y.-C. F. Wang. Detach and adapt: Learning cross-domain disentangled deep representation. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Y.-C.%20Yeh%2C%20Y.-Y.%20Fu%2C%20T.-C.%20Wang%2C%20S.-D.%20Detach%20and%20adapt%3A%20Learning%20cross-domain%20disentangled%20deep%20representation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Y.-C.%20Yeh%2C%20Y.-Y.%20Fu%2C%20T.-C.%20Wang%2C%20S.-D.%20Detach%20and%20adapt%3A%20Learning%20cross-domain%20disentangled%20deep%20representation%202018"
        },
        {
            "id": "30",
            "entry": "[30] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20J.%20Shelhamer%2C%20E.%20Darrell%2C%20T.%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20J.%20Shelhamer%2C%20E.%20Darrell%2C%20T.%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015"
        },
        {
            "id": "31",
            "entry": "[31] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and M. Fritz. Disentangled person image generation. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20L.%20Sun%2C%20Q.%20Georgoulis%2C%20S.%20Gool%2C%20L.Van%20Disentangled%20person%20image%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20L.%20Sun%2C%20Q.%20Georgoulis%2C%20S.%20Gool%2C%20L.Van%20Disentangled%20person%20image%20generation%202018"
        },
        {
            "id": "32",
            "entry": "[32] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016"
        },
        {
            "id": "33",
            "entry": "[33] M. F. Mathieu, J. J. Zhao, J. Zhao, A. Ramesh, P. Sprechmann, and Y. LeCun. Disentangling factors of variation in deep representation using adversarial training. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20M.F.%20Zhao%2C%20J.J.%20Zhao%2C%20J.%20Ramesh%2C%20A.%20Disentangling%20factors%20of%20variation%20in%20deep%20representation%20using%20adversarial%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20M.F.%20Zhao%2C%20J.J.%20Zhao%2C%20J.%20Ramesh%2C%20A.%20Disentangling%20factors%20of%20variation%20in%20deep%20representation%20using%20adversarial%20training%202016"
        },
        {
            "id": "34",
            "entry": "[34] K. Pang, Y.-Z. Song, T. Xiang, and T. Hospedales. Cross-domain generative learning for fine-grained sketch-based image retrieval. In BMVC, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pang%2C%20K.%20Song%2C%20Y.-Z.%20Xiang%2C%20T.%20Hospedales%2C%20T.%20Cross-domain%20generative%20learning%20for%20fine-grained%20sketch-based%20image%20retrieval%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20K.%20Song%2C%20Y.-Z.%20Xiang%2C%20T.%20Hospedales%2C%20T.%20Cross-domain%20generative%20learning%20for%20fine-grained%20sketch-based%20image%20retrieval%202017"
        },
        {
            "id": "35",
            "entry": "[35] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20A.%20Metz%2C%20L.%20Chintala%2C%20S.%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202015"
        },
        {
            "id": "36",
            "entry": "[36] S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Sohn%2C%20K.%20Zhang%2C%20Y.%20Lee%2C%20H.%20Learning%20to%20disentangle%20factors%20of%20variation%20with%20manifold%20interaction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Sohn%2C%20K.%20Zhang%2C%20Y.%20Lee%2C%20H.%20Learning%20to%20disentangle%20factors%20of%20variation%20with%20manifold%20interaction%202014"
        },
        {
            "id": "37",
            "entry": "[37] S. E. Reed, Y. Zhang, Y. Zhang, and H. Lee. Deep visual analogy-making. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.E.%20Zhang%2C%20Y.%20Zhang%2C%20Y.%20Lee%2C%20H.%20Deep%20visual%20analogy-making%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.E.%20Zhang%2C%20Y.%20Zhang%2C%20Y.%20Lee%2C%20H.%20Deep%20visual%20analogy-making%202015"
        },
        {
            "id": "38",
            "entry": "[38] S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza. Disentangling factors of variation for facial expression recognition. In ECCV, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rifai%2C%20S.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Vincent%2C%20P.%20Disentangling%20factors%20of%20variation%20for%20facial%20expression%20recognition%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rifai%2C%20S.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Vincent%2C%20P.%20Disentangling%20factors%20of%20variation%20for%20facial%20expression%20recognition%202012"
        },
        {
            "id": "39",
            "entry": "[39] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In ICMICCAI, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ronneberger%2C%20O.%20Fischer%2C%20P.%20Brox%2C%20T.%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ronneberger%2C%20O.%20Fischer%2C%20P.%20Brox%2C%20T.%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%202015"
        },
        {
            "id": "40",
            "entry": "[40] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "41",
            "entry": "[41] M. F. Tappen, W. T. Freeman, and E. H. Adelson. Recovering intrinsic images from a single image. In NIPS, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tappen%2C%20M.F.%20Freeman%2C%20W.T.%20Adelson%2C%20E.H.%20Recovering%20intrinsic%20images%20from%20a%20single%20image%202003"
        },
        {
            "id": "42",
            "entry": "[42] J. B. Tenenbaum and W. T. Freeman. Separating style and content. In NIPS, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tenenbaum%2C%20J.B.%20Freeman%2C%20W.T.%20Separating%20style%20and%20content%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tenenbaum%2C%20J.B.%20Freeman%2C%20W.T.%20Separating%20style%20and%20content%201997"
        },
        {
            "id": "43",
            "entry": "[43] L. Tran, X. Yin, and X. Liu. Disentangled representation learning gan for pose-invariant face recognition. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20L.%20Yin%2C%20X.%20Liu%2C%20X.%20Disentangled%20representation%20learning%20gan%20for%20pose-invariant%20face%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20L.%20Yin%2C%20X.%20Liu%2C%20X.%20Disentangled%20representation%20learning%20gan%20for%20pose-invariant%20face%20recognition%202017"
        },
        {
            "id": "44",
            "entry": "[44] R. Tylecek and R. \u0160\u00e1ra. Spatial pattern templates for recognition of objects with regular structure. In GCPR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tylecek%2C%20R.%20%C5%A0%C3%A1ra%2C%20R.%20Spatial%20pattern%20templates%20for%20recognition%20of%20objects%20with%20regular%20structure%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tylecek%2C%20R.%20%C5%A0%C3%A1ra%2C%20R.%20Spatial%20pattern%20templates%20for%20recognition%20of%20objects%20with%20regular%20structure%202013"
        },
        {
            "id": "45",
            "entry": "[45] Y. Wang, J. van de Weijer, and L. Herranz. Mix and match networks: encoder-decoder alignment for zero-pair image translation. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20van%20de%20Weijer%2C%20J.%20Herranz%2C%20L.%20Mix%20and%20match%20networks%3A%20encoder-decoder%20alignment%20for%20zero-pair%20image%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20van%20de%20Weijer%2C%20J.%20Herranz%2C%20L.%20Mix%20and%20match%20networks%3A%20encoder-decoder%20alignment%20for%20zero-pair%20image%20translation%202018"
        },
        {
            "id": "46",
            "entry": "[46] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Colorful%20image%20colorization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Colorful%20image%20colorization%202016"
        },
        {
            "id": "47",
            "entry": "[47] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep networks as a perceptual metric. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Shechtman%2C%20E.%20The%20unreasonable%20effectiveness%20of%20deep%20networks%20as%20a%20perceptual%20metric%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Shechtman%2C%20E.%20The%20unreasonable%20effectiveness%20of%20deep%20networks%20as%20a%20perceptual%20metric%202018"
        },
        {
            "id": "48",
            "entry": "[48] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        },
        {
            "id": "49",
            "entry": "[49] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. In NIPS, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Zhang%2C%20R.%20Pathak%2C%20D.%20Darrell%2C%20T.%20Toward%20multimodal%20image-to-image%20translation%202017"
        }
    ]
}
