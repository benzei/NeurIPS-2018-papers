{
    "filename": "8222-learning-with-sgd-and-random-features.pdf",
    "metadata": {
        "title": "Learning with SGD and Random Features",
        "author": "Luigi Carratino, Alessandro Rudi, Lorenzo Rosasco",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8222-learning-with-sgd-and-random-features.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Sketching and stochastic gradient methods are arguably the most common techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and random features. The latter can be seen as form of nonlinear sketching and used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlights how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard assumptions. The obtained results are corroborated and illustrated by numerical experiments."
    },
    "keywords": [
        {
            "term": "kernel method",
            "url": "https://en.wikipedia.org/wiki/kernel_method"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "The interplay between statistical and computational performances is key for modern machine learning algorithms [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We show some examples of kernels determined by specific choices of random features",
        "Our results apply to a general class of random features described by the following assumption",
        "When the number of random features goes to infinity we recover the results for the infinite dimensional case of the single-pass and multiple pass stochastic gradient method [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] when fH 2 H",
        "In particular we studied the statistical and computational properties of the estimator defined by stochastic gradient descent with multiple passes, mini-batches and random features",
        "We proved that the estimator achieves optimal statistical properties with a number of random features in the order of p n"
    ],
    "key_statements": [
        "The interplay between statistical and computational performances is key for modern machine learning algorithms [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We show some examples of kernels determined by specific choices of random features",
        "Our results apply to a general class of random features described by the following assumption",
        "When the number of random features goes to infinity we recover the results for the infinite dimensional case of the single-pass and multiple pass stochastic gradient method [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] when fH 2 H",
        "In particular we studied the statistical and computational properties of the estimator defined by stochastic gradient descent with multiple passes, mini-batches and random features",
        "We proved that the estimator achieves optimal statistical properties with a number of random features in the order of p n"
    ],
    "summary": [
        "The interplay between statistical and computational performances is key for modern machine learning algorithms [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "If for example we assume X \u2713 RD and consider random features defined as in the previous section, computing M (x) requires M random projections of D dimensional vectors [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], for a total time complexity of O(M D) for evaluating the feature map at one point.",
        "A ridge regression framework is considered in [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], where it is shown that it is possible to achieve optimal statistical guarantees with a number of random features in the order of pn.",
        "The kernel introduced above allows to compare random feature maps of different size and to express the regularity of the largest function class they induce.",
        "For the limit case where the number of random features grows to infinity for Corollary 1 under conditions (c1.2) and (c1.3) we recover the same results for one pass SGD of [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>], [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>].",
        "The combination of the stochastic gradient iteration, random features and mini-batches allows our algorithm to achieve a complexity of O in time and O(n) in space for certain choices of the free parameters and",
        "The corollary above shows that multi-pass SGD achieves a learning rate that is the same as kernel ridge regression under the regularity assumption 5 and is again minimax optimal.",
        "When the number of random features goes to infinity we recover the results for the infinite dimensional case of the single-pass and multiple pass stochastic gradient method [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] when fH 2 H.",
        "It is worth noting that, under the additional regularity assumption 5, the number of both random features and passes over the data sufficient for optimal learning rates increase with respect to the one required in the worst case.",
        "We show in Figure 2 the classification error of SGD with RF after 1 pass over the data, with a fixed number of random features pn, as mini-batch size and step-size vary, on test sets of 105 points.",
        "In particular we studied the statistical and computational properties of the estimator defined by stochastic gradient descent with multiple passes, mini-batches and random features.",
        "We analyzed possible trade-offs between the number of passes, the step and the dimension of the mini-batches showing that there exist different configurations which achieve the same optimal statistical guarantees, with different computational impacts.",
        "(b) we can extend our analysis to consider more refined assumptions, generalizing [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] to SGD with random features.",
        "We proved that the estimator achieves optimal statistical properties with a number of random features in the order of p n.<br/><br/> we analyzed possible trade-offs between the number of passes, the step and the dimension of the mini-batches showing that there exist different configurations which achieve the same optimal statistical guarantees, with different computational impacts"
    ],
    "headline": "We investigate their application in the context of nonparametric statistical learning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions. In Advances in Neural Information Processing Systems, pages 1538\u20131546, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Stochastic%20optimization%20and%20sparse%20statistical%20recovery%3A%20Optimal%20algorithms%20for%20high%20dimensions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Stochastic%20optimization%20and%20sparse%20statistical%20recovery%3A%20Optimal%20algorithms%20for%20high%20dimensions%202012"
        },
        {
            "id": "2",
            "entry": "[2] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method.%20The%20annals%20of%20mathematical%20statistics%201951"
        },
        {
            "id": "3",
            "entry": "[3] Haim Avron, Vikas Sindhwani, and David Woodruff. Sketching structured matrices for faster nonlinear regression. In Advances in neural information processing systems, pages 2994\u20133002, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avron%2C%20Haim%20Sindhwani%2C%20Vikas%20Woodruff%2C%20David%20Sketching%20structured%20matrices%20for%20faster%20nonlinear%20regression%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avron%2C%20Haim%20Sindhwani%2C%20Vikas%20Woodruff%2C%20David%20Sketching%20structured%20matrices%20for%20faster%20nonlinear%20regression%202013"
        },
        {
            "id": "4",
            "entry": "[4] L\u00e9on Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural information processing systems, pages 161\u2013168, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Bousquet%2C%20Olivier%20The%20tradeoffs%20of%20large%20scale%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Bousquet%2C%20Olivier%20The%20tradeoffs%20of%20large%20scale%20learning%202008"
        },
        {
            "id": "5",
            "entry": "[5] Francesco Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. In Advances in Neural Information Processing Systems, pages 1116\u20131124, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orabona%2C%20Francesco%20Simultaneous%20model%20selection%20and%20optimization%20through%20parameter-free%20stochastic%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orabona%2C%20Francesco%20Simultaneous%20model%20selection%20and%20optimization%20through%20parameter-free%20stochastic%20learning%202014"
        },
        {
            "id": "6",
            "entry": "[6] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177\u20131184, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "7",
            "entry": "[7] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pages 342\u2013350, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009"
        },
        {
            "id": "8",
            "entry": "[8] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems 30, pages 3215\u20133225. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "9",
            "entry": "[9] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337\u2013404, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950"
        },
        {
            "id": "10",
            "entry": "[10] Bernhard Sch\u00f6lkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20Bernhard%20Smola%2C%20Alexander%20J.%20Learning%20with%20kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202002"
        },
        {
            "id": "11",
            "entry": "[11] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007"
        },
        {
            "id": "12",
            "entry": "[12] Luc Devroye, L\u00e1szl\u00f3 Gy\u00f6rfi, and G\u00e1bor Lugosi. A probabilistic theory of pattern recognition, volume 31. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luc%20Devroye%2C%20L%C3%A1szl%C3%B3%20Gy%C3%B6rfi%20Lugosi%2C%20G%C3%A1bor%20A%20probabilistic%20theory%20of%20pattern%20recognition%202013"
        },
        {
            "id": "13",
            "entry": "[13] David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends in Theoretical Computer Science, 10(1\u20132):1\u2013157, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodruff%2C%20David%20P.%20Sketching%20as%20a%20tool%20for%20numerical%20linear%20algebra%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodruff%2C%20David%20P.%20Sketching%20as%20a%20tool%20for%20numerical%20linear%20algebra%202014"
        },
        {
            "id": "14",
            "entry": "[14] Bharath Sriperumbudur and Zolt\u00e1n Szab\u00f3. Optimal rates for random fourier features. In Advances in Neural Information Processing Systems, pages 1144\u20131152, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sriperumbudur%2C%20Bharath%20Szab%C3%B3%2C%20Zolt%C3%A1n%20Optimal%20rates%20for%20random%20fourier%20features%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sriperumbudur%2C%20Bharath%20Szab%C3%B3%2C%20Zolt%C3%A1n%20Optimal%20rates%20for%20random%20fourier%20features%202015"
        },
        {
            "id": "15",
            "entry": "[15] Raffay Hamid, Ying Xiao, Alex Gittens, and Dennis DeCoste. Compact random feature maps. In International Conference on Machine Learning, pages 19\u201327, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamid%2C%20Raffay%20Xiao%2C%20Ying%20Gittens%2C%20Alex%20DeCoste%2C%20Dennis%20Compact%20random%20feature%20maps%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamid%2C%20Raffay%20Xiao%2C%20Ying%20Gittens%2C%20Alex%20DeCoste%2C%20Dennis%20Compact%20random%20feature%20maps%202014"
        },
        {
            "id": "16",
            "entry": "[16] X Yu Felix, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and Sanjiv Kumar. Orthogonal random features. In Advances in Neural Information Processing Systems, pages 1975\u20131983, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Felix%2C%20X.Yu%20Suresh%2C%20Ananda%20Theertha%20Choromanski%2C%20Krzysztof%20M.%20Holtmann-Rice%2C%20Daniel%20N.%20Orthogonal%20random%20features%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Felix%2C%20X.Yu%20Suresh%2C%20Ananda%20Theertha%20Choromanski%2C%20Krzysztof%20M.%20Holtmann-Rice%2C%20Daniel%20N.%20Orthogonal%20random%20features%201975"
        },
        {
            "id": "17",
            "entry": "[17] Quoc Le, Tam\u00e1s Sarl\u00f3s, and Alex Smola. Fastfood-approximating kernel expansions in loglinear time. In Proceedings of the international conference on machine learning, volume 85, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20Sarl%C3%B3s%2C%20Tam%C3%A1s%20Smola%2C%20Alex%20Fastfood-approximating%20kernel%20expansions%20in%20loglinear%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20Sarl%C3%B3s%2C%20Tam%C3%A1s%20Smola%2C%20Alex%20Fastfood-approximating%20kernel%20expansions%20in%20loglinear%20time%202013"
        },
        {
            "id": "18",
            "entry": "[18] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20vector%20machines%202008"
        },
        {
            "id": "19",
            "entry": "[19] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in neural information processing systems, pages 1313\u20131320, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Weighted%20sums%20of%20random%20kitchen%20sinks%3A%20Replacing%20minimization%20with%20randomization%20in%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Weighted%20sums%20of%20random%20kitchen%20sinks%3A%20Replacing%20minimization%20with%20randomization%20in%20learning%202009"
        },
        {
            "id": "20",
            "entry": "[20] Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions. Journal of Machine Learning Research, 18(21):1\u201338, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017"
        },
        {
            "id": "21",
            "entry": "[21] Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information Processing Systems, pages 3041\u20133049, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Bo%20Xie%2C%20Bo%20He%2C%20Niao%20Liang%2C%20Yingyu%20Scalable%20kernel%20methods%20via%20doubly%20stochastic%20gradients%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Bo%20Xie%2C%20Bo%20He%2C%20Niao%20Liang%2C%20Yingyu%20Scalable%20kernel%20methods%20via%20doubly%20stochastic%20gradients%202014"
        },
        {
            "id": "22",
            "entry": "[22] Junhong Lin and Lorenzo Rosasco. Generalization properties of doubly online learning algorithms. arXiv preprint arXiv:1707.00577, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00577"
        },
        {
            "id": "23",
            "entry": "[23] Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht. Large scale kernel learning using block coordinate descent. arXiv preprint arXiv:1602.05310, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.05310"
        },
        {
            "id": "24",
            "entry": "[24] Alex J Smola and Bernhard Sch\u00f6lkopf. Sparse greedy matrix approximation for machine learning. 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20Alex%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20Sparse%20greedy%20matrix%20approximation%20for%20machine%20learning%202000"
        },
        {
            "id": "25",
            "entry": "[25] Christopher KI Williams and Matthias Seeger. Using the nystr\u00f6m method to speed up kernel machines. In Advances in neural information processing systems, pages 682\u2013688, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20K.I.%20Seeger%2C%20Matthias%20Using%20the%20nystr%C3%B6m%20method%20to%20speed%20up%20kernel%20machines%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Christopher%20K.I.%20Seeger%2C%20Matthias%20Using%20the%20nystr%C3%B6m%20method%20to%20speed%20up%20kernel%20machines%202001"
        },
        {
            "id": "26",
            "entry": "[26] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystr\u00f6m computational regularization. In Advances in Neural Information Processing Systems, pages 1657\u20131665, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015"
        },
        {
            "id": "27",
            "entry": "[27] Yun Yang, Mert Pilanci, and Martin J Wainwright. Randomized sketches for kernels: Fast and optimal non-parametric regression. arXiv preprint arXiv:1501.06195, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1501.06195"
        },
        {
            "id": "28",
            "entry": "[28] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical guarantees. In Advances in Neural Information Processing Systems, pages 775\u2013783, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alaoui%2C%20Ahmed%20Mahoney%2C%20Michael%20W.%20Fast%20randomized%20kernel%20ridge%20regression%20with%20statistical%20guarantees%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alaoui%2C%20Ahmed%20Mahoney%2C%20Michael%20W.%20Fast%20randomized%20kernel%20ridge%20regression%20with%20statistical%20guarantees%202015"
        },
        {
            "id": "29",
            "entry": "[29] Raffaello Camoriano, Tom\u00e1s Angles, Alessandro Rudi, and Lorenzo Rosasco. Nytro: When subsampling meets early stopping. In Artificial Intelligence and Statistics, pages 1403\u20131411, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camoriano%2C%20Raffaello%20Angles%2C%20Tom%C3%A1s%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Nytro%3A%20When%20subsampling%20meets%20early%20stopping%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Camoriano%2C%20Raffaello%20Angles%2C%20Tom%C3%A1s%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Nytro%3A%20When%20subsampling%20meets%20early%20stopping%202016"
        },
        {
            "id": "30",
            "entry": "[30] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. FALKON: An optimal large scale kernel method. In Advances in Neural Information Processing Systems, pages 3891\u20133901, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20FALKON%3A%20An%20optimal%20large%20scale%20kernel%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20FALKON%3A%20An%20optimal%20large%20scale%20kernel%20method%202017"
        },
        {
            "id": "31",
            "entry": "[31] Junhong Lin and Lorenzo Rosasco. Optimal rates for learning with nystr\u00f6m stochastic gradient methods. arXiv preprint arXiv:1710.07797, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07797"
        },
        {
            "id": "32",
            "entry": "[32] Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence rates for least-squares regression. The Journal of Machine Learning Research, 18(1):3520\u20133570, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieuleveut%2C%20Aymeric%20Flammarion%2C%20Nicolas%20Harder%2C%20Francis%20Bach%20better%2C%20faster%20stronger%20convergence%20rates%20for%20least-squares%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieuleveut%2C%20Aymeric%20Flammarion%2C%20Nicolas%20Harder%2C%20Francis%20Bach%20better%2C%20faster%20stronger%20convergence%20rates%20for%20least-squares%20regression%202017"
        },
        {
            "id": "33",
            "entry": "[33] Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. Journal of Machine Learning Research, 18(97):1\u201347, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Junhong%20Rosasco%2C%20Lorenzo%20Optimal%20rates%20for%20multi-pass%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Junhong%20Rosasco%2C%20Lorenzo%20Optimal%20rates%20for%20multi-pass%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "34",
            "entry": "[34] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing error for stochastic gradient methods. In Proceedings of the 31st Conference On Learning Theory, volume 75, pages 250\u2013296, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pillaud-Vivien%2C%20Loucas%20Rudi%2C%20Alessandro%20Bach%2C%20Francis%20Exponential%20convergence%20of%20testing%20error%20for%20stochastic%20gradient%20methods%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pillaud-Vivien%2C%20Loucas%20Rudi%2C%20Alessandro%20Bach%2C%20Francis%20Exponential%20convergence%20of%20testing%20error%20for%20stochastic%20gradient%20methods%202018"
        },
        {
            "id": "35",
            "entry": "[35] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. arXiv preprint arXiv:1805.10074, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10074"
        },
        {
            "id": "36",
            "entry": "[36] Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization. In Advances in Neural Information Processing Systems, pages 1630\u20131638, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosasco%2C%20Lorenzo%20Villa%2C%20Silvia%20Learning%20with%20incremental%20iterative%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosasco%2C%20Lorenzo%20Villa%2C%20Silvia%20Learning%20with%20incremental%20iterative%20regularization%202015"
        },
        {
            "id": "37",
            "entry": "[37] Ingo Steinwart, Don R Hush, Clint Scovel, et al. Optimal rates for regularized least squares regression. In COLT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression%202009"
        },
        {
            "id": "38",
            "entry": "[38] Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral algorithms with least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018"
        },
        {
            "id": "39",
            "entry": "[39] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International Conference on Machine Learning, pages 71\u201379, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Zhang%2C%20Tong%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%3A%20Convergence%20results%20and%20optimal%20averaging%20schemes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Zhang%2C%20Tong%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%3A%20Convergence%20results%20and%20optimal%20averaging%20schemes%202013"
        },
        {
            "id": "40",
            "entry": "[40] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13(Jan):165\u2013202, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012"
        },
        {
            "id": "41",
            "entry": "[41] Aymeric Dieuleveut, Francis Bach, et al. Nonparametric stochastic approximation with large step-sizes. The Annals of Statistics, 44(4):1363\u20131399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieuleveut%2C%20Aymeric%20Bach%2C%20Francis%20Nonparametric%20stochastic%20approximation%20with%20large%20step-sizes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieuleveut%2C%20Aymeric%20Bach%2C%20Francis%20Nonparametric%20stochastic%20approximation%20with%20large%20step-sizes%202016"
        },
        {
            "id": "42",
            "entry": "[42] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature communications, 5:4308, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20Pierre%20Sadowski%2C%20Peter%20Whiteson%2C%20Daniel%20Searching%20for%20exotic%20particles%20in%20high-energy%20physics%20with%20deep%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20Pierre%20Sadowski%2C%20Peter%20Whiteson%2C%20Daniel%20Searching%20for%20exotic%20particles%20in%20high-energy%20physics%20with%20deep%20learning%202014"
        },
        {
            "id": "43",
            "entry": "[43] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243\u2013272, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argyriou%2C%20Andreas%20Evgeniou%2C%20Theodoros%20Pontil%2C%20Massimiliano%20Convex%20multi-task%20feature%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argyriou%2C%20Andreas%20Evgeniou%2C%20Theodoros%20Pontil%2C%20Massimiliano%20Convex%20multi-task%20feature%20learning%202008"
        },
        {
            "id": "44",
            "entry": "[44] Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, and Massimiliano Pontil. Consistent multitask learning with nonlinear output relations. In Advances in Neural Information Processing Systems, pages 1986\u20131996, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986"
        },
        {
            "id": "45",
            "entry": "[45] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A consistent regularization approach for structured prediction. Advances in Neural Information Processing Systems 29 (NIPS), pages 4412\u20134420, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016"
        },
        {
            "id": "46",
            "entry": "[46] Anton Osokin, Francis Bach, and Simon Lacoste-Julien. On structured prediction theory with calibrated convex surrogate losses. In Advances in Neural Information Processing Systems, pages 302\u2013313, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017"
        },
        {
            "id": "47",
            "entry": "[47] Carlo Ciliberto, Francis Bach, and Alessandro Rudi. Localized structured prediction. arXiv preprint arXiv:1806.02402, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02402"
        },
        {
            "id": "48",
            "entry": "[48] Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13(Dec):3475\u20133506, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Magdon-Ismail%2C%20Malik%20Mahoney%2C%20Michael%20W.%20Woodruff%2C%20David%20P.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Magdon-Ismail%2C%20Malik%20Mahoney%2C%20Michael%20W.%20Woodruff%2C%20David%20P.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012"
        },
        {
            "id": "49",
            "entry": "[49] Alessandro Rudi, Daniele Calandriello, Luigi Carratino, and Lorenzo Rosasco. On fast leverage score sampling and optimal learning. In arXiv preprint, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Calandriello%2C%20Daniele%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20On%20fast%20leverage%20score%20sampling%20and%20optimal%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Calandriello%2C%20Daniele%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20On%20fast%20leverage%20score%20sampling%20and%20optimal%20learning%202018"
        },
        {
            "id": "50",
            "entry": "[50] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the American mathematical society, 39(1):1\u201349, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cucker%2C%20Felipe%20Smale%2C%20Steve%20On%20the%20mathematical%20foundations%20of%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cucker%2C%20Felipe%20Smale%2C%20Steve%20On%20the%20mathematical%20foundations%20of%20learning%202002"
        },
        {
            "id": "51",
            "entry": "[51] Ernesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini, and Francesca Odone. Learning from examples as an inverse problem. Journal of Machine Learning Research, 6(May):883\u2013904, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vito%2C%20Ernesto%20De%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20Giovannini%2C%20Umberto%20De%20Learning%20from%20examples%20as%20an%20inverse%20problem%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vito%2C%20Ernesto%20De%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20Giovannini%2C%20Umberto%20De%20Learning%20from%20examples%20as%20an%20inverse%20problem%202005"
        },
        {
            "id": "52",
            "entry": "[52] Alessandro Rudi, Guillermo D Canas, and Lorenzo Rosasco. On the sample complexity of subspace learning. In Advances in Neural Information Processing Systems, pages 2067\u20132075, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Canas%2C%20Guillermo%20D.%20Rosasco%2C%20Lorenzo%20On%20the%20sample%20complexity%20of%20subspace%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Canas%2C%20Guillermo%20D.%20Rosasco%2C%20Lorenzo%20On%20the%20sample%20complexity%20of%20subspace%20learning%202013"
        }
    ]
}
