{
    "filename": "7980-knowledge-distillation-by-on-the-fly-native-ensemble.pdf",
    "metadata": {
        "title": "Knowledge Distillation by On-the-Fly Native Ensemble",
        "author": "xu lan, Xiatian Zhu, Shaogang Gong",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7980-knowledge-distillation-by-on-the-fly-native-ensemble.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Knowledge distillation is effective to train the small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a high-capacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) learning strategy for one-stage online distillation. Specifically, ONE only trains a single multi-branch network while simultaneously establishing a strong teacher on-the-fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance of a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "computational cost",
            "url": "https://en.wikipedia.org/wiki/computational_cost"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "low memory",
            "url": "https://en.wikipedia.org/wiki/low_memory"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Deep neural networks have gained impressive success in many computer vision tasks [1; 2; 3; 4; 5; 6; 7; 8]",
        "We have these observations: (1) All different networks benefit from the On-the-fly Native Ensemble training algorithm, particularly with small models achieving larger performance gains",
        "It is shown that the proposed On-the-fly Native Ensemble learning algorithm again yields more effective training and more generalisable models in comparison to the vanilla SGD. This indicates that our method is generically applicable in large scale image classification settings",
        "For the online methods Deep Mutual Learning and On-the-fly Native Ensemble, we evaluated their performances using either ResNet-32 or ResNet-110 as the target student model",
        "We presented a novel On-the-fly Native Ensemble (ONE) strategy for improving deep network learning through online knowledge distillation in a one-stage training procedure",
        "Extensive experiments on four image classification benchmarks show that a variety of deep networks can all benefit from the On-the-fly Native Ensemble approach"
    ],
    "key_statements": [
        "Deep neural networks have gained impressive success in many computer vision tasks [1; 2; 3; 4; 5; 6; 7; 8]",
        "We have these observations: (1) All different networks benefit from the On-the-fly Native Ensemble training algorithm, particularly with small models achieving larger performance gains",
        "It is shown that the proposed On-the-fly Native Ensemble learning algorithm again yields more effective training and more generalisable models in comparison to the vanilla SGD. This indicates that our method is generically applicable in large scale image classification settings",
        "For the online methods Deep Mutual Learning and On-the-fly Native Ensemble, we evaluated their performances using either ResNet-32 or ResNet-110 as the target student model",
        "We presented a novel On-the-fly Native Ensemble (ONE) strategy for improving deep network learning through online knowledge distillation in a one-stage training procedure",
        "Extensive experiments on four image classification benchmarks show that a variety of deep networks can all benefit from the On-the-fly Native Ensemble approach"
    ],
    "summary": [
        "Deep neural networks have gained impressive success in many computer vision tasks [1; 2; 3; 4; 5; 6; 7; 8].",
        "Extensive experiments on four benchmarks (CIFAR10/100, SVHN, and ImageNet) show that the proposed ONE distillation method enables to train more generalisable target models in an one-phase process than the alternative strategies of offline learning a larger teacher network or simultaneously distilling peer students, the previous state-of-the-art techniques for training small target models.",
        "We overcome both limitations by designing a new online distillation training algorithm characterised by simultaneously learning a teacher on-the-fly and the target net as well as performing batch-wise knowledge transfer in an one-phase training procedure.",
        "To further enhance the model generalisation, we concurrently distil extra knowledge from an on-the-fly native ensemble (ONE) teacher to each branch in training.",
        "Under the reconfiguration of network, we add a separate CE loss Lcie to each branch which simultaneously learns to predict the same ground-truth class label of a training sample.",
        "Table 1 compares top-1 error rate performances of four varyingcapacity state-of-the-art network models trained by the conventional and our ONE learning algorithms.",
        "This suggests a generic superiority of our method for online knowledge distillation from the on-the-fly teacher to the target student model.",
        "(2) All individual branches have similar performances, indicating that they have made sufficient agreement and exchanged respective knowledge to each other well through the proposed ONE teacher model during training.",
        "Results on ImageNet. Table 2 shows the comparative performances on the 1000-classes ImageNet. It is shown that the proposed ONE learning algorithm again yields more effective training and more generalisable models in comparison to the vanilla SGD.",
        "The target model effectively approaches the ONE teacher (Fig 3(a) vs 3(b)) on both training and test error performance, indicating the success of teacher knowledge transfer.",
        "Table 6 shows that ONE scales well with more branches and the ResNet-32 model generalisation improves on CIFAR100 with the number of branches added during training its performance advantage over the independently trained network (31.18% error rate).",
        "100 Epochs 200 branches of ONE have higher correlations, due to the proposed learning constraint from the distillation loss that enforces them align to the same teacher prediction, which probably hurts the ensemble performance.",
        "We presented a novel On-the-fly Native Ensemble (ONE) strategy for improving deep network learning through online knowledge distillation in a one-stage training procedure.",
        "Our method is superior over existing online counterparts due to the unique capability of constructing a high-capacity online teacher to more effectively mine knowledge from the training data and supervise the target network concurrently.",
        "Smaller networks obtain more performance gains, making our method specially good for low-memory and fast execution scenarios"
    ],
    "headline": "We present an On-the-fly Native Ensemble  learning strategy for one-stage online distillation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "2",
            "entry": "[2] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "3",
            "entry": "[3] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "4",
            "entry": "[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "5",
            "entry": "[5] Ross Girshick. Fast r-cnn. In IEEE International Conference on Computer Vision, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%20Girshick%20Fast%20rcnn%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%20Girshick%20Fast%20rcnn%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%202015"
        },
        {
            "id": "6",
            "entry": "[6] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015"
        },
        {
            "id": "7",
            "entry": "[7] Wei Li, Xiatian Zhu, and Shaogang Gong. Person re-identification by deep joint learning of multi-loss classification. In International Joint Conference of Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Wei%20Zhu%2C%20Xiatian%20Gong%2C%20Shaogang%20Person%20re-identification%20by%20deep%20joint%20learning%20of%20multi-loss%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Wei%20Zhu%2C%20Xiatian%20Gong%2C%20Shaogang%20Person%20re-identification%20by%20deep%20joint%20learning%20of%20multi-loss%20classification%202017"
        },
        {
            "id": "8",
            "entry": "[8] Xu Lan, Xiatian Zhu, and Shaogang Gong. Person search by multi-scale matching. In European Conference on Computer Vision, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Xu%20Zhu%2C%20Xiatian%20Gong%2C%20Shaogang%20Person%20search%20by%20multi-scale%20matching%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Xu%20Zhu%2C%20Xiatian%20Gong%2C%20Shaogang%20Person%20search%20by%20multi-scale%20matching%202018"
        },
        {
            "id": "9",
            "entry": "[9] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv e-print, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks.%20arXiv%20e-print%202016"
        },
        {
            "id": "10",
            "entry": "[10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv e-print, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeff%20Distilling%20the%20knowledge%20in%20a%20neural%20network.%20arXiv%20e-print%202015"
        },
        {
            "id": "11",
            "entry": "[11] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "12",
            "entry": "[12] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016"
        },
        {
            "id": "13",
            "entry": "[13] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Hao%20Kadav%2C%20Asim%20Durdanovic%2C%20Igor%20Samet%2C%20Hanan%20Pruning%20filters%20for%20efficient%20convnets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Hao%20Kadav%2C%20Asim%20Durdanovic%2C%20Igor%20Samet%2C%20Hanan%20Pruning%20filters%20for%20efficient%20convnets%202017"
        },
        {
            "id": "14",
            "entry": "[14] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014"
        },
        {
            "id": "15",
            "entry": "[15] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv e-print, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romero%2C%20Adriana%20Ballas%2C%20Nicolas%20Kahou%2C%20Samira%20Ebrahimi%20Chassang%2C%20Antoine%20Fitnets%3A%20Hints%20for%20thin%20deep%20nets%202014"
        },
        {
            "id": "16",
            "entry": "[16] Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E Hinton. Large scale distributed neural network training through online distillation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anil%2C%20Rohan%20Pereyra%2C%20Gabriel%20Passos%2C%20Alexandre%20Ormandi%2C%20Robert%20Large%20scale%20distributed%20neural%20network%20training%20through%20online%20distillation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anil%2C%20Rohan%20Pereyra%2C%20Gabriel%20Passos%2C%20Alexandre%20Ormandi%2C%20Robert%20Large%20scale%20distributed%20neural%20network%20training%20through%20online%20distillation%202018"
        },
        {
            "id": "17",
            "entry": "[17] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. arXiv e-print, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Ying%20Xiang%2C%20Tao%20Hospedales%2C%20Timothy%20M.%20Lu%2C%20Huchuan%20Deep%20mutual%20learning.%20arXiv%20e-print%202017"
        },
        {
            "id": "18",
            "entry": "[18] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bucilua%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bucilua%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006"
        },
        {
            "id": "19",
            "entry": "[19] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yim%2C%20Junho%20Joo%2C%20Donggyu%20Bae%2C%20Jihoon%20Kim%2C%20Junmo%20A%20gift%20from%20knowledge%20distillation%3A%20Fast%20optimization%2C%20network%20minimization%20and%20transfer%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yim%2C%20Junho%20Joo%2C%20Donggyu%20Bae%2C%20Jihoon%20Kim%2C%20Junmo%20A%20gift%20from%20knowledge%20distillation%3A%20Fast%20optimization%2C%20network%20minimization%20and%20transfer%20learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] Xu Lan, Xiatian Zhu, and Shaogang Gong. Self-referenced deep learning. In Asian Conference on Computer Vision, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Xu%20Zhu%2C%20Xiatian%20Gong%2C%20Shaogang%20Self-referenced%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Xu%20Zhu%2C%20Xiatian%20Gong%2C%20Shaogang%20Self-referenced%20deep%20learning%202018"
        },
        {
            "id": "21",
            "entry": "[21] Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. arXiv e-print, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Furlanello%2C%20Tommaso%20Lipton%2C%20Zachary%20C.%20Tschannen%2C%20Michael%20Itti%2C%20Laurent%20Born%20again%20neural%20networks.%20arXiv%20e-print%202018"
        },
        {
            "id": "22",
            "entry": "[22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "23",
            "entry": "[23] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "24",
            "entry": "[24] Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient densenet using learned group convolutions. arXiv e-print, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Shichen%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Condensenet%3A%20An%20efficient%20densenet%20using%20learned%20group%20convolutions.%20arXiv%20e-print%202017"
        },
        {
            "id": "25",
            "entry": "[25] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "26",
            "entry": "[26] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv e-print, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks.%20arXiv%20e-print%202016"
        },
        {
            "id": "27",
            "entry": "[27] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeplysupervised nets. In Artificial Intelligence and Statistics, pages 562\u2013570, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Chen-Yu%20Xie%2C%20Saining%20Gallagher%2C%20Patrick%20Zhang%2C%20Zhengyou%20Deeplysupervised%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Chen-Yu%20Xie%2C%20Saining%20Gallagher%2C%20Patrick%20Zhang%2C%20Zhengyou%20Deeplysupervised%20nets%202015"
        },
        {
            "id": "28",
            "entry": "[28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "29",
            "entry": "[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Sun%2C%20Yu%20Liu%2C%20Zhuang%20Sedra%2C%20Daniel%20Deep%20networks%20with%20stochastic%20depth%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Sun%2C%20Yu%20Liu%2C%20Zhuang%20Sedra%2C%20Daniel%20Deep%20networks%20with%20stochastic%20depth%202016"
        },
        {
            "id": "30",
            "entry": "[30] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "31",
            "entry": "[31] Shen Li Sun Gang Hu, Jie. Squeeze-and-excitation networks. arXiv e-print, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Shen%20Li%20Sun%20Gang%20Jie%20Squeeze-and-excitation%20networks.%20arXiv%20e-print%202017"
        },
        {
            "id": "32",
            "entry": "[32] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles%202017"
        },
        {
            "id": "33",
            "entry": "[33] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv e-print, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima.%20arXiv%20e-print%202016"
        },
        {
            "id": "34",
            "entry": "[34] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv e-print, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhari%2C%20Pratik%20Choromanska%2C%20Anna%20Soatto%2C%20Stefano%20LeCun%2C%20Yann%20Entropy-sgd%3A%20Biasing%20gradient%20descent%20into%20wide%20valleys.%20arXiv%20e-print%202016"
        }
    ]
}
