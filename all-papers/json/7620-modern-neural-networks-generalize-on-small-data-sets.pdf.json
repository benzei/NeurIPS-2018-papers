{
    "filename": "7620-modern-neural-networks-generalize-on-small-data-sets.pdf",
    "metadata": {
        "title": "Modern Neural Networks Generalize on Small Data Sets",
        "author": "Matthew Olson, Abraham Wyner, Richard Berk",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7620-modern-neural-networks-generalize-on-small-data-sets.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we use a linear program to empirically decompose fitted neural networks into ensembles of low-bias sub-networks. We show that these sub-networks are relatively uncorrelated which leads to an internal regularization process, very much like a random forest, which can explain why a neural network is surprisingly resistant to overfitting. We then demonstrate this in practice by applying large neural networks, with hundreds of parameters per training observation, to a collection of 116 real-world data sets from the UCI Machine Learning Repository. This collection of data sets contains a much smaller number of training examples than the types of image classification tasks generally studied in the deep learning literature, as well as non-trivial label noise. We show that even in this setting deep neural nets are capable of achieving superior classification accuracy without overfitting."
    },
    "keywords": [
        {
            "term": "generalization error",
            "url": "https://en.wikipedia.org/wiki/generalization_error"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "sub network",
            "url": "https://en.wikipedia.org/wiki/sub_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "random forest",
            "url": "https://en.wikipedia.org/wiki/random_forest"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "data set",
            "url": "https://en.wikipedia.org/wiki/data_set"
        }
    ],
    "highlights": [
        "A recent focus in the deep learning community has been resolving the \u201cparadox\" that extremely large, high capacity neural networks are able to simultaneously memorize training data and achieve good generalization error",
        "In a number of experiments, Zhang et al [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] demonstrated that large neural networks were capable of both achieving state of the art performance on image classification tasks, as well as perfectly fitting training data with permuted labels",
        "We will discuss the results from a large scale classification study comparing the performance of a deep neural network and a random forest on a collection of 116 data sets from the UCI Machine Learning Repository",
        "We have shown that the same mantra can be applied to a deep neural network",
        "We remark that the small size of data sets we consider and relatively small network sizes have obvious computational advantages, which allow for rapid experimentation",
        "Some of the recent norms proposed for explaining neural network generalization are intractable on networks with millions of parameters: Schatten norms, for example, require computing a full SVD [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]"
    ],
    "key_statements": [
        "A recent focus in the deep learning community has been resolving the \u201cparadox\" that extremely large, high capacity neural networks are able to simultaneously memorize training data and achieve good generalization error",
        "In a number of experiments, Zhang et al [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] demonstrated that large neural networks were capable of both achieving state of the art performance on image classification tasks, as well as perfectly fitting training data with permuted labels",
        "We will discuss the results from a large scale classification study comparing the performance of a deep neural network and a random forest on a collection of 116 data sets from the UCI Machine Learning Repository",
        "We turn first to Figure 3, which plots the cross-validated accuracy of the neural network classifiers and the random forest for each data set",
        "We have shown that the same mantra can be applied to a deep neural network",
        "We remark that the small size of data sets we consider and relatively small network sizes have obvious computational advantages, which allow for rapid experimentation",
        "Some of the recent norms proposed for explaining neural network generalization are intractable on networks with millions of parameters: Schatten norms, for example, require computing a full SVD [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]"
    ],
    "summary": [
        "A recent focus in the deep learning community has been resolving the \u201cparadox\" that extremely large, high capacity neural networks are able to simultaneously memorize training data and achieve good generalization error.",
        "We establish that with minimal tuning, deep neural networks are able to achieve performance on par with a random forest classifier, which is considered to have state-of-the-art performance on data sets from the UCI Repository [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>].",
        "The later half of this paper focuses on empirically decomposing a neural network into an ensemble of sub-networks, each of which achieves low training error and less than perfect pairwise correlation.",
        "Large neural networks with hundreds of parameters per training observation are able to generalize well on small, noisy data sets.",
        "This perspective allows us to borrow insights from random forests and model stacking to gain better insight as to how a neural network with many parameters per observation is still able to generalize well on small data sets.",
        "The final set of constraints ensures that each sub-network achieves 100% accuracy on the training data.",
        "We train two classifiers: a neural network with L = 10 hidden layers and M = 100 hidden nodes, and a random forest.",
        "For which it is relatively easy to find sub-ensembles with low training error and low correlation, the corresponding search in the case of neural networks requires more work.",
        "The test accuracies of these sub-networks range from 79% to 82%, and every classifier fits the training data perfectly by construction.",
        "We will discuss the results from a large scale classification study comparing the performance of a deep neural network and a random forest on a collection of 116 data sets from the UCI Machine Learning Repository.",
        "3.2 Experimental Setting For each data set in our collection, we fit three classifiers: a random forest, and neural networks with and without dropout.",
        "We turn first to Figure 3, which plots the cross-validated accuracy of the neural network classifiers and the random forest for each data set.",
        "We see that a random forest outperforms an unregularized neural network on 72 out of 116 data sets, by a small margin.",
        "A neural network achieves an accuracy of 90.3% on the monks-2 data set, compared to 62.9% for a random forest.",
        "We restrict our analysis to data sets with at least 500 observations, and for which the fitted neural network achieved 100% training accuracy.",
        "Contrast this with the mantra of a random forest: fit the training data perfectly with very deep decision trees, and rely on randomization and averaging for variance reduction 1."
    ],
    "headline": "We show that these sub-networks are relatively uncorrelated which leads to an internal regularization process, very much like a random forest, which can explain why a neural network is surprisingly resistant to overfitting",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. (2017). Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240\u20136249.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "2",
            "entry": "[2] Bengio, Y. (2012). Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade, pages 437\u2013478. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Practical%20recommendations%20for%20gradient-based%20training%20of%20deep%20architectures%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Practical%20recommendations%20for%20gradient-based%20training%20of%20deep%20architectures%202012"
        },
        {
            "id": "3",
            "entry": "[3] Bengio, Y. et al. (2009). Learning deep architectures for ai. Foundations and trends R in Machine Learning, 2(1):1\u2013127.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Learning%20deep%20architectures%20for%20ai%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Learning%20deep%20architectures%20for%20ai%202009"
        },
        {
            "id": "4",
            "entry": "[4] Breiman, L. (2000a). Some infinity theory for predictor ensembles. Technical report, Technical Report 579, Statistics Dept. UCB.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20L.%20Some%20infinity%20theory%20for%20predictor%20ensembles%202000"
        },
        {
            "id": "5",
            "entry": "[5] Breiman, L. (2000b). Special invited paper. additive logistic regression: A statistical view of boosting: Discussion. The annals of statistics, 28(2):374\u2013377.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20L.%20Special%20invited%20paper.%20additive%20logistic%20regression%3A%20A%20statistical%20view%20of%20boosting%3A%20Discussion%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20L.%20Special%20invited%20paper.%20additive%20logistic%20regression%3A%20A%20statistical%20view%20of%20boosting%3A%20Discussion%202000"
        },
        {
            "id": "6",
            "entry": "[6] Breiman, L. (2001). Random forests. Machine Learning, 45:5\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20L.%20Random%20forests%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20L.%20Random%20forests%202001"
        },
        {
            "id": "7",
            "entry": "[7] Chollet, F. (2017). Deep learning with python. Manning Publications Co.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chollet%2C%20F.%20Deep%20learning%20with%20python%202017"
        },
        {
            "id": "8",
            "entry": "[8] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "9",
            "entry": "[9] Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pages 1019\u20131028.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20L.%20Pascanu%2C%20R.%20Bengio%2C%20S.%20Bengio%2C%20Y.%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20L.%20Pascanu%2C%20R.%20Bengio%2C%20S.%20Bengio%2C%20Y.%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017"
        },
        {
            "id": "10",
            "entry": "[10] Fern\u00e1ndez-Delgado, M., Cernadas, E., Barro, S., and Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems. J. Mach. Learn. Res, 15(1):3133\u20133181.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fern%C3%A1ndez-Delgado%2C%20M.%20Cernadas%2C%20E.%20Barro%2C%20S.%20Amorim%2C%20D.%20Do%20we%20need%20hundreds%20of%20classifiers%20to%20solve%20real%20world%20classification%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fern%C3%A1ndez-Delgado%2C%20M.%20Cernadas%2C%20E.%20Barro%2C%20S.%20Amorim%2C%20D.%20Do%20we%20need%20hundreds%20of%20classifiers%20to%20solve%20real%20world%20classification%20problems%202014"
        },
        {
            "id": "11",
            "entry": "[11] Golowich, N., Rakhlin, A., and Shamir, O. (2018). Size-independent sample complexity of neural networks. In Conference On Learning Theory, pages 297\u2013299.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golowich%2C%20N.%20Rakhlin%2C%20A.%20Shamir%2C%20O.%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20N.%20Rakhlin%2C%20A.%20Shamir%2C%20O.%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018"
        },
        {
            "id": "12",
            "entry": "[12] He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "13",
            "entry": "[13] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "14",
            "entry": "[14] Kawaguchi, K., Kaelbling, L. P., and Bengio, Y. (2017). Generalization in deep learning. arXiv preprint arXiv:1710.05468.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05468"
        },
        {
            "id": "15",
            "entry": "[15] Kingma, D. P. and Ba, J. L. (2015). Adam: Amethod for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20Amethod%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20Amethod%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "16",
            "entry": "[16] Liang, T., Poggio, T., Rakhlin, A., and Stokes, J. (2017). Fisher-rao metric, geometry, and complexity of neural networks. arXiv preprint arXiv:1711.01530.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01530"
        },
        {
            "id": "17",
            "entry": "[17] Lin, H. W., Tegmark, M., and Rolnick, D. (2017). Why does deep and cheap learning work so well? Journal of Statistical Physics, 168(6):1223\u20131247.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20H.W.%20Tegmark%2C%20M.%20Rolnick%2C%20D.%20Why%20does%20deep%20and%20cheap%20learning%20work%20so%20well%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20H.W.%20Tegmark%2C%20M.%20Rolnick%2C%20D.%20Why%20does%20deep%20and%20cheap%20learning%20work%20so%20well%3F%202017"
        },
        {
            "id": "18",
            "entry": "[18] Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pages 5947\u20135956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "19",
            "entry": "[19] Rolnick, D., Veit, A., Belongie, S., and Shavit, N. (2017). Deep learning is robust to massive label noise. arXiv preprint arXiv:1705.10694.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10694"
        },
        {
            "id": "20",
            "entry": "[20] Segal, M. R. (2004). Machine learning benchmarks and random forest regression.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Segal%2C%20M.R.%20Machine%20learning%20benchmarks%20and%20random%20forest%20regression%202004"
        },
        {
            "id": "21",
            "entry": "[21] Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929\u20131958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "22",
            "entry": "[22] Veit, A., Wilber, M. J., and Belongie, S. (2016). Residual networks behave like ensembles of relatively shallow networks. In Advances in Neural Information Processing Systems, pages 550\u2013558.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veit%2C%20A.%20Wilber%2C%20M.J.%20Belongie%2C%20S.%20Residual%20networks%20behave%20like%20ensembles%20of%20relatively%20shallow%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veit%2C%20A.%20Wilber%2C%20M.J.%20Belongie%2C%20S.%20Residual%20networks%20behave%20like%20ensembles%20of%20relatively%20shallow%20networks%202016"
        },
        {
            "id": "23",
            "entry": "[23] Wyner, A. J., Olson, M., Bleich, J., and Mease, D. (2017). Explaining the success of adaboost and random forests as interpolating classifiers. Journal of Machine Learning Research, 18(48):1\u2013 33.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wyner%2C%20A.J.%20Olson%2C%20M.%20Bleich%2C%20J.%20Mease%2C%20D.%20Explaining%20the%20success%20of%20adaboost%20and%20random%20forests%20as%20interpolating%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wyner%2C%20A.J.%20Olson%2C%20M.%20Bleich%2C%20J.%20Mease%2C%20D.%20Explaining%20the%20success%20of%20adaboost%20and%20random%20forests%20as%20interpolating%20classifiers%202017"
        },
        {
            "id": "24",
            "entry": "[24] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. International Conference on Learning Representations. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization.%20International%20Conference%20on%20Learning%20Representations%202017"
        }
    ]
}
