{
    "filename": "7925-rho-pomdps-have-lipschitz-continuous-epsilon-optimal-value-functions.pdf",
    "metadata": {
        "title": "rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions",
        "author": "Mathieu Fehr, Olivier Buffet, Vincent Thomas, Jilles Dibangoye",
        "date": 1971,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7925-rho-pomdps-have-lipschitz-continuous-epsilon-optimal-value-functions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Many state-of-the-art algorithms for solving Partially Observable Markov Decision Processes (POMDPs) rely on turning the problem into a \u201cfully observable\u201d problem\u2014a belief MDP\u2014and exploiting the piece-wise linearity and convexity (PWLC) of the optimal value function in this new state space (the belief simplex \u2206). This approach has been extended to solving \u03c1-POMDPs\u2014i.e., for informationoriented criteria\u2014when the reward \u03c1 is convex in \u2206. General \u03c1-POMDPs can also be turned into \u201cfully observable\u201d problems, but with no means to exploit the PWLC property. In this paper, we focus on POMDPs and \u03c1-POMDPs with \u03bb\u03c1-Lipschitz reward function, and demonstrate that, for finite horizons, the optimal value function is Lipschitz-continuous. Then, value function approximators are proposed for both upperand lower-bounding the optimal value function, which are shown to provide uniformly improvable bounds. This allows proposing two algorithms derived from HSVI which are empirically evaluated on various benchmark problems."
    },
    "keywords": [
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        },
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "lipschitz continuous",
            "url": "https://en.wikipedia.org/wiki/lipschitz_continuous"
        },
        {
            "term": "partially observable",
            "url": "https://en.wikipedia.org/wiki/partially_observable"
        },
        {
            "term": "POMDP",
            "url": "https://en.wikipedia.org/wiki/POMDP"
        }
    ],
    "highlights": [
        "This paper shows that, for \u03c1-Partially Observable Markov Decision Processes with \u03bb\u03c1-Lipschitz reward function and for finite horizons, the optimal value function is still LC, a property that shall replace the piece-wise linear and convex property",
        "State of the art off-line algorithms [<a class=\"ref-link\" id=\"cPineau_et+al_2006_a\" href=\"#rPineau_et+al_2006_a\">Pineau et al, 2006</a>, <a class=\"ref-link\" id=\"cSmith_2004_a\" href=\"#rSmith_2004_a\">Smith and Simmons, 2004</a>] maintain approximators that (i) are upper or lower bounds, and have generalization capabilities: a local update at b improves the bound in a surrounding region of b. This approach has been extended to solving \u03c1-Partially Observable Markov Decision Processes as belief MDPs\u2014i.e., problems whose performance criterion depends on the belief\u2014when the reward \u03c1 is convex in \u2206 [Araya-L\u00f3pez et al, 2010].1",
        "This paper shows that, for \u03c1-Partially Observable Markov Decision Processes with \u03bb\u03c1-Lipschitz reward function and for finite horizons, the optimal value function is still LC, a property that shall replace the piece-wise linear and convex property",
        "A Partially Observable Markov Decision Processes [<a class=\"ref-link\" id=\"cAstrom_1965_a\" href=\"#rAstrom_1965_a\">Astrom, 1965</a>] is defined by a tuple S, A, Z, P, r, \u03b3, b0 , where S, A and Z are finite sets of states, actions and observations; Pa,z(s, s ) gives the probability of transiting to state s and observing observation z when applying action a in state s (Pa,z is an S \u00d7 S matrix); r(s, a) \u2208 R is the reward associated to performing action a in state s; \u03b3 \u2208 [0; 1) is a discount factor; and b0 is the initial belief state\u2014i.e., the initial probability distribution over possible states",
        "A Partially Observable Markov Decision Processes is often turned into a belief MDP \u2206, A, T, r, \u03b3, b0 where \u2206 is the simplex of possible belief states, A is the same action set, and T (b, a, b ) = P (b |b, a) and r(b, a) = pb]o.lsiOcbim\u03c0r(a:sl\u2206,pao)\u2192liacrAiees,themaecaihxnibdmeuiicnzegedaVtsrs\u03c0aoncinsiaitatieloldnbtoealniitdesfvrseatwlautaeersfdurnfeucantcichotaniboVlnes\u03c0.fmh=.isbE0s.e[tTtihnt\u221ee=gir0av\u03b3llatorlwuo|enbr0iVn=g\u2217 is the fixed point of Bellman\u2019s optimality operator (H) [<a class=\"ref-link\" id=\"cBellman_1957_a\" href=\"#rBellman_1957_a\">Bellman, 1957</a>] HV : b \u2192 maxa[r(b, a) + \u03b3 z Pa,zb 1V], and acting greedily with respect to V \u2217 provides such a policy"
    ],
    "key_statements": [
        "This paper shows that, for \u03c1-Partially Observable Markov Decision Processes with \u03bb\u03c1-Lipschitz reward function and for finite horizons, the optimal value function is still LC, a property that shall replace the piece-wise linear and convex property",
        "State of the art off-line algorithms [<a class=\"ref-link\" id=\"cPineau_et+al_2006_a\" href=\"#rPineau_et+al_2006_a\">Pineau et al, 2006</a>, <a class=\"ref-link\" id=\"cSmith_2004_a\" href=\"#rSmith_2004_a\">Smith and Simmons, 2004</a>] maintain approximators that (i) are upper or lower bounds, and have generalization capabilities: a local update at b improves the bound in a surrounding region of b. This approach has been extended to solving \u03c1-Partially Observable Markov Decision Processes as belief MDPs\u2014i.e., problems whose performance criterion depends on the belief\u2014when the reward \u03c1 is convex in \u2206 [Araya-L\u00f3pez et al, 2010].1",
        "This paper shows that, for \u03c1-Partially Observable Markov Decision Processes with \u03bb\u03c1-Lipschitz reward function and for finite horizons, the optimal value function is still LC, a property that shall replace the piece-wise linear and convex property",
        "A Partially Observable Markov Decision Processes [<a class=\"ref-link\" id=\"cAstrom_1965_a\" href=\"#rAstrom_1965_a\">Astrom, 1965</a>] is defined by a tuple S, A, Z, P, r, \u03b3, b0 , where S, A and Z are finite sets of states, actions and observations; Pa,z(s, s ) gives the probability of transiting to state s and observing observation z when applying action a in state s (Pa,z is an S \u00d7 S matrix); r(s, a) \u2208 R is the reward associated to performing action a in state s; \u03b3 \u2208 [0; 1) is a discount factor; and b0 is the initial belief state\u2014i.e., the initial probability distribution over possible states",
        "A Partially Observable Markov Decision Processes is often turned into a belief MDP \u2206, A, T, r, \u03b3, b0 where \u2206 is the simplex of possible belief states, A is the same action set, and T (b, a, b ) = P (b |b, a) and r(b, a) = pb]o.lsiOcbim\u03c0r(a:sl\u2206,pao)\u2192liacrAiees,themaecaihxnibdmeuiicnzegedaVtsrs\u03c0aoncinsiaitatieloldnbtoealniitdesfvrseatwlautaeersfdurnfeucantcichotaniboVlnes\u03c0.fmh=.isbE0s.e[tTtihnt\u221ee=gir0av\u03b3llatorlwuo|enbr0iVn=g\u2217 is the fixed point of Bellman\u2019s optimality operator (H) [<a class=\"ref-link\" id=\"cBellman_1957_a\" href=\"#rBellman_1957_a\">Bellman, 1957</a>] HV : b \u2192 maxa[r(b, a) + \u03b3 z Pa,zb 1V], and acting greedily with respect to V \u2217 provides such a policy"
    ],
    "summary": [
        "This paper shows that, for \u03c1-POMDPs with \u03bb\u03c1-Lipschitz reward function and for finite horizons, the optimal value function is still LC, a property that shall replace the PWLC property.",
        "Following Smith [2007], these approximators are shown to provide uniformly improvable bounds [<a class=\"ref-link\" id=\"cZhang_2001_a\" href=\"#rZhang_2001_a\">Zhang and Zhang, 2001</a>] for use with point-based algorithms like HSVI, which is guaranteed to converge to an -optimal solution.",
        "Sec. 4 demonstrates that, for finite horizons, the optimal value function is Lipschitz-continuous, Sec. 5 proposes value function approximators and two point-based algorithms.",
        "We will exploit Lipschitz-continuous reward functions \u03c1 to solve more general \u03c1-POMDPs with similar algorithmic schemes.",
        "Assuming a \u03c1-POMDP with local and vector Lipschitz-continuous reward function with \u201cconstant\u201d \u03bb\u03c1(b, a) in (b, a), this section first states that Bellman\u2019s optimality operator preserves the LC property, which allows proving that V \u2217 is LC for finite horizons, but not necessarily for infinite ones.",
        "We show how to define, initialize, update and prune LC upperand lower-bounding approximators of V \u2217, and derive an -optimal variant of the HSVI algorithm.",
        "The upper-bounding LC approximator is defined as a finite set of downward-pointing L1-cones), where an upper-bounding cone c\u03b2U = \u03b2, u, \u03bb \u2014located at belief \u03b2, with \u201csummit\u201d value u and \u201cslope\u201d vector \u03bb\u2014induces a function U\u03b2(b) = u + \u03bb \u00b7 |\u03b2 \u2212 b|.",
        "We (i) show how the update of the upperor lower-bound preserves this representation; verify that the properties required for HSVI to converge to an -optimal solution still hold; and discuss their initialization.",
        "Preservation of HSVI\u2019s Convergence Properties Ensuring finite time convergence of HSVI to an -optimal solution requires using (a) a uniformly improvable (UI) lower bound L, i.e., HL \u2265 L, where H is Bellman optimality operator; (b) respectively a uniformly improvable (UI) upper bound U ; (c) a strong pointwise update operator for the lower bound L, K\u00b7L, i.e., for each b where it is applied and any L, (i) (HL)(b) = (KbLL)(b) and (HL)(b ) \u2265 (KbLL)(b ) in any other point b ; and (d) resp.",
        "This work shows that, for finite horizons, the optimal value function of a \u03c1-POMDP with Lipschitz \u03c1 is Lipschitz-continuous (LC).",
        "Two algorithms are proposed: HSVI used with these \u201cLC bounds\u201d\u2014which preserves HSVI\u2019s convergence properties\u2014, and an incremental algorithm that searches for a scalar Lipschitz constant allowing for fast computations\u2014with no guarantees that the bounds are valid."
    ],
    "headline": "We focus on Partially Observable Markov Decision Processes and \u03c1-Partially Observable Markov Decision Processes with \u03bb\u03c1-Lipschitz reward function, and demonstrate that, for finite horizons, the optimal value function is Lipschitz-continuous",
    "reference_links": [
        {
            "id": "Araya-L_et+al_2010_a",
            "entry": "M. Araya-L\u00f3pez, O. Buffet, V. Thomas, and F. Charpillet. A POMDP extension with belief-dependent rewards. In Advances in Neural Information Processing Systems 23 (NIPS-10), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Araya-L%C3%B3pez%2C%20M.%20Buffet%2C%20O.%20Thomas%2C%20V.%20Charpillet%2C%20F.%20A%20POMDP%20extension%20with%20belief-dependent%20rewards%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Araya-L%C3%B3pez%2C%20M.%20Buffet%2C%20O.%20Thomas%2C%20V.%20Charpillet%2C%20F.%20A%20POMDP%20extension%20with%20belief-dependent%20rewards%202010"
        },
        {
            "id": "Astrom_1965_a",
            "entry": "K. Astrom. Optimal control of Markov processes with incomplete state information. Journal of Mathematical Analysis and Applications, 10(1), 1965. ISSN 0022-247X.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Astrom%2C%20K.%20Optimal%20control%20of%20Markov%20processes%20with%20incomplete%20state%20information%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Astrom%2C%20K.%20Optimal%20control%20of%20Markov%20processes%20with%20incomplete%20state%20information%201965"
        },
        {
            "id": "Bellman_1957_a",
            "entry": "R. Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, 6(5), 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellman%2C%20R.%20A%20Markovian%20decision%20process%201957",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellman%2C%20R.%20A%20Markovian%20decision%20process%201957"
        },
        {
            "id": "Dibangoye_et+al_2013_a",
            "entry": "J. Dibangoye, C. Amato, O. Buffet, and F. Charpillet. Optimally solving Dec-POMDPs as continuousstate MDPs. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence (IJCAI-13), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dibangoye%2C%20J.%20Amato%2C%20C.%20Buffet%2C%20O.%20Charpillet%2C%20F.%20Optimally%20solving%20Dec-POMDPs%20as%20continuousstate%20MDPs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dibangoye%2C%20J.%20Amato%2C%20C.%20Buffet%2C%20O.%20Charpillet%2C%20F.%20Optimally%20solving%20Dec-POMDPs%20as%20continuousstate%20MDPs%202013"
        },
        {
            "id": "Dibangoye_et+al_2016_a",
            "entry": "J. Dibangoye, C. Amato, O. Buffet, and F. Charpillet. Optimally solving Dec-POMDPs as continuousstate MDPs. Journal of Artificial Intelligence Research, 55, 2016. URL http://www.jair.org/papers/paper4623.html.",
            "url": "http://www.jair.org/papers/paper4623.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dibangoye%2C%20J.%20Amato%2C%20C.%20Buffet%2C%20O.%20Charpillet%2C%20F.%20Optimally%20solving%20Dec-POMDPs%20as%20continuousstate%20MDPs%202016"
        },
        {
            "id": "Dufour_2012_a",
            "entry": "F. Dufour and T. Prieto-Rumeau. Approximation of Markov decision processes with general state space. Journal of Mathematical Analysis and Applications, 388(2), 2012. ISSN 0022-247X. doi: https://doi.org/10.1016/j.jmaa.2011.11.015. URL http://www.sciencedirect.com/science/article/pii/S0022247X11010353.",
            "crossref": "https://dx.doi.org/10.1016/j.jmaa.2011.11.015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.jmaa.2011.11.015"
        },
        {
            "id": "Egorov_et+al_2016_a",
            "entry": "M. Egorov, M. J. Kochenderfer, and J. J. Uudmae. Target surveillance in adversarial environments using POMDPs. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Egorov%2C%20M.%20Kochenderfer%2C%20M.J.%20Uudmae%2C%20J.J.%20Target%20surveillance%20in%20adversarial%20environments%20using%20POMDPs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Egorov%2C%20M.%20Kochenderfer%2C%20M.J.%20Uudmae%2C%20J.J.%20Target%20surveillance%20in%20adversarial%20environments%20using%20POMDPs%202016"
        },
        {
            "id": "Fonteneau_et+al_2009_a",
            "entry": "R. Fonteneau, S. Murphy, L. Wehenkel, and D. Ernst. Inferring bounds on the performance of a control policy from a sample of trajectories. In Proceedings of the IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning (ADPRL-09), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fonteneau%2C%20R.%20Murphy%2C%20S.%20Wehenkel%2C%20L.%20Ernst%2C%20D.%20Inferring%20bounds%20on%20the%20performance%20of%20a%20control%20policy%20from%20a%20sample%20of%20trajectories%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fonteneau%2C%20R.%20Murphy%2C%20S.%20Wehenkel%2C%20L.%20Ernst%2C%20D.%20Inferring%20bounds%20on%20the%20performance%20of%20a%20control%20policy%20from%20a%20sample%20of%20trajectories%202009"
        },
        {
            "id": "Fox_et+al_1998_a",
            "entry": "D. Fox, W. Burgard, and S. Thrun. Active Markov localization for mobile robots. Robotics and Autonomous Systems, 25(3\u20134), 1998. doi: http://dx.doi.org/10.1016/S0921-8890(98)00049-9.",
            "crossref": "https://dx.doi.org/10.1016/S0921-8890(98)00049-9",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/S0921-8890%2898%2900049-9"
        },
        {
            "id": "Hansen_et+al_2004_a",
            "entry": "E. Hansen, D. Bernstein, and S. Zilberstein. Dynamic programming for partially observable stochastic games. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20E.%20Bernstein%2C%20D.%20Zilberstein%2C%20S.%20Dynamic%20programming%20for%20partially%20observable%20stochastic%20games%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20E.%20Bernstein%2C%20D.%20Zilberstein%2C%20S.%20Dynamic%20programming%20for%20partially%20observable%20stochastic%20games%202004"
        },
        {
            "id": "Hinderer_2005_a",
            "entry": "K. Hinderer. Lipschitz continuity of value functions in Markovian decision processes. Mathematical Methods of Operations Research, 62(1), Sep 2005. doi: 10.1007/s00186-005-0438-1. URL https://doi.org/10.1007/s00186-005-0438-1.",
            "crossref": "https://dx.doi.org/10.1007/s00186-005-0438-1",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s00186-005-0438-1"
        },
        {
            "id": "Ieong_et+al_2007_a",
            "entry": "S. Ieong, N. Lambert, Y. Shoham, and R. Brafman. Near-optimal search in continuous domains. In Proceedings of the National Conference on Artificial Intelligence (AAAI-07), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ieong%2C%20S.%20Lambert%2C%20N.%20Shoham%2C%20Y.%20Brafman%2C%20R.%20Near-optimal%20search%20in%20continuous%20domains%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ieong%2C%20S.%20Lambert%2C%20N.%20Shoham%2C%20Y.%20Brafman%2C%20R.%20Near-optimal%20search%20in%20continuous%20domains%202007"
        },
        {
            "id": "Kurniawati_et+al_2008_a",
            "entry": "H. Kurniawati, D. Hsu, and W. Lee. SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Robotics: Science and Systems IV, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurniawati%2C%20H.%20Hsu%2C%20D.%20Lee%2C%20W.%20SARSOP%3A%20Efficient%20point-based%20POMDP%20planning%20by%20approximating%20optimally%20reachable%20belief%20spaces%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurniawati%2C%20H.%20Hsu%2C%20D.%20Lee%2C%20W.%20SARSOP%3A%20Efficient%20point-based%20POMDP%20planning%20by%20approximating%20optimally%20reachable%20belief%20spaces%202008"
        },
        {
            "id": "Laraki_2004_a",
            "entry": "R. Laraki and W. D. Sudderth. The preservation of continuity and Lipschitz continuity by optimal reward operators. Mathematics of Operations Research, 29(3), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laraki%2C%20R.%20Sudderth%2C%20W.D.%20The%20preservation%20of%20continuity%20and%20Lipschitz%20continuity%20by%20optimal%20reward%20operators%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laraki%2C%20R.%20Sudderth%2C%20W.D.%20The%20preservation%20of%20continuity%20and%20Lipschitz%20continuity%20by%20optimal%20reward%20operators%202004"
        },
        {
            "id": "Mihaylova_et+al_2006_a",
            "entry": "L. Mihaylova, T. Lefebvre, H. Bruyninckx, and J. D. Schutter. volume 198, chapter Active Robotic Sensing as Decision Making with Statistical Methods. 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mihaylova%2C%20L.%20Lefebvre%2C%20T.%20Bruyninckx%2C%20H.%20Schutter%2C%20J.D.%20chapter%20Active%20Robotic%20Sensing%20as%20Decision%20Making%20with%20Statistical%20Methods%202006"
        },
        {
            "id": "Mnih_et+al_2013_a",
            "entry": "V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Graves%2C%20A.%20Playing%20atari%20with%20deep%20reinforcement%20learning.%20In%20NIPS%20Deep%20Learning%20Workshop%202013"
        },
        {
            "id": "Pineau_et+al_2003_a",
            "entry": "J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for POMDPs. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI03), 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pineau%2C%20J.%20Gordon%2C%20G.%20Thrun%2C%20S.%20Point-based%20value%20iteration%3A%20An%20anytime%20algorithm%20for%20POMDPs%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pineau%2C%20J.%20Gordon%2C%20G.%20Thrun%2C%20S.%20Point-based%20value%20iteration%3A%20An%20anytime%20algorithm%20for%20POMDPs%202003"
        },
        {
            "id": "Pineau_et+al_2006_a",
            "entry": "J. Pineau, G. Gordon, and S. Thrun. Anytime point-based approximations for large POMDPs. Journal of Artificial Intelligence Research (JAIR), 27, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pineau%2C%20J.%20Gordon%2C%20G.%20Thrun%2C%20S.%20Anytime%20point-based%20approximations%20for%20large%20POMDPs%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pineau%2C%20J.%20Gordon%2C%20G.%20Thrun%2C%20S.%20Anytime%20point-based%20approximations%20for%20large%20POMDPs%202006"
        },
        {
            "id": "Platzman_1977_a",
            "entry": "L. K. Platzman. Finite Memory Estimation and Control of Finite Probabilistic Systems. PhD thesis, Massachusetts Institute of Technology (MIT), 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Platzman%2C%20L.K.%20Finite%20Memory%20Estimation%20and%20Control%20of%20Finite%20Probabilistic%20Systems%201977"
        },
        {
            "id": "Poupart_et+al_2011_a",
            "entry": "P. Poupart, K.-E. Kim, and D. Kim. Closing the gap: Improved bounds on optimal POMDP solutions. In Proceedings of the Twenty-First International Conference on Automated Planning and Scheduling (ICAPS-11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poupart%2C%20P.%20Kim%2C%20K.-E.%20Kim%2C%20D.%20Closing%20the%20gap%3A%20Improved%20bounds%20on%20optimal%20POMDP%20solutions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poupart%2C%20P.%20Kim%2C%20K.-E.%20Kim%2C%20D.%20Closing%20the%20gap%3A%20Improved%20bounds%20on%20optimal%20POMDP%20solutions%202011"
        },
        {
            "id": "Rachelson_2010_a",
            "entry": "E. Rachelson and M. Lagoudakis. On the locality of action domination in sequential decision making. In Proc. of the International Symposium on Artificial Intelligence and Mathematics (ISAIM-10), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rachelson%2C%20E.%20Lagoudakis%2C%20M.%20On%20the%20locality%20of%20action%20domination%20in%20sequential%20decision%20making%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rachelson%2C%20E.%20Lagoudakis%2C%20M.%20On%20the%20locality%20of%20action%20domination%20in%20sequential%20decision%20making%202010"
        },
        {
            "id": "Satsangi_et+al_2015_a",
            "entry": "Y. Satsangi, S. Whiteson, and M. T. J. Spaan. An analysis of piecewise-linear and convex value functions for active perception POMDPs. Technical Report IAS-UVA-15-01, IAS, Universiteit van Amsterdam, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Satsangi%2C%20Y.%20Whiteson%2C%20S.%20Spaan%2C%20M.T.J.%20An%20analysis%20of%20piecewise-linear%20and%20convex%20value%20functions%20for%20active%20perception%20POMDPs%202015"
        },
        {
            "id": "Smallwood_1973_a",
            "entry": "R. Smallwood and E. Sondik. The optimal control of partially observable Markov decision processes over a finite horizon. Operation Research, 21, 1973.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smallwood%2C%20R.%20Sondik%2C%20E.%20The%20optimal%20control%20of%20partially%20observable%20Markov%20decision%20processes%20over%20a%20finite%20horizon%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smallwood%2C%20R.%20Sondik%2C%20E.%20The%20optimal%20control%20of%20partially%20observable%20Markov%20decision%20processes%20over%20a%20finite%20horizon%201973"
        },
        {
            "id": "Smith_2007_a",
            "entry": "T. Smith. Probabilistic Planning for Robotic Exploration. PhD thesis, The Robotics Institute, Carnegie Mellon University, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20T.%20Probabilistic%20Planning%20for%20Robotic%20Exploration%202007"
        },
        {
            "id": "Smith_2004_a",
            "entry": "T. Smith and R. Simmons. Heuristic search value iteration for POMDPs. In Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence (UAI-04), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20T.%20Simmons%2C%20R.%20Heuristic%20search%20value%20iteration%20for%20POMDPs%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20T.%20Simmons%2C%20R.%20Heuristic%20search%20value%20iteration%20for%20POMDPs%202004"
        },
        {
            "id": "Smith_2005_a",
            "entry": "T. Smith and R. Simmons. Point-based POMDP algorithms: Improved analysis and implementation. In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence (UAI-05), 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20T.%20Simmons%2C%20R.%20Point-based%20POMDP%20algorithms%3A%20Improved%20analysis%20and%20implementation%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20T.%20Simmons%2C%20R.%20Point-based%20POMDP%20algorithms%3A%20Improved%20analysis%20and%20implementation%202005"
        },
        {
            "id": "Sondik_1971_a",
            "entry": "E. Sondik. The Optimal Control of Partially Observable Markov Decision Processes. PhD thesis, Stanford University, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sondik%2C%20E.%20The%20Optimal%20Control%20of%20Partially%20Observable%20Markov%20Decision%20Processes%201971"
        },
        {
            "id": "Spaan_et+al_2015_a",
            "entry": "M. T. Spaan, T. S. Veiga, and P. U. Lima. Decision-theoretic planning under uncertainty with information rewards for active cooperative perception. Autonomous Agents and Multi-Agent Systems, 29(6), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spaan%2C%20M.T.%20Veiga%2C%20T.S.%20Lima%2C%20P.U.%20Decision-theoretic%20planning%20under%20uncertainty%20with%20information%20rewards%20for%20active%20cooperative%20perception%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spaan%2C%20M.T.%20Veiga%2C%20T.S.%20Lima%2C%20P.U.%20Decision-theoretic%20planning%20under%20uncertainty%20with%20information%20rewards%20for%20active%20cooperative%20perception%202015"
        },
        {
            "id": "Zhang_2001_a",
            "entry": "N. L. Zhang and W. Zhang. Speeding up the convergence of value iteration in partially observable Markov decision processes. Journal of Artificial Intelligence Research, 14, 2001. URL http://dx.doi.org/10.1613/jair.761.",
            "crossref": "https://dx.doi.org/10.1613/jair.761",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1613/jair.761"
        },
        {
            "id": "Zhang_et+al_2014_a",
            "entry": "Z. Zhang, D. Hsu, and W. S. Lee. Covering number for efficient heuristic-based pomdp planning. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014. URL http://jmlr.org/proceedings/papers/v32/zhanga14.pdf.",
            "url": "http://jmlr.org/proceedings/papers/v32/zhanga14.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Z.%20Hsu%2C%20D.%20Lee%2C%20W.S.%20Covering%20number%20for%20efficient%20heuristic-based%20pomdp%20planning%202014"
        }
    ]
}
