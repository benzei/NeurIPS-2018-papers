{
    "filename": "7490-the-description-length-of-deep-learning-models.pdf",
    "metadata": {
        "title": "The Description Length of Deep Learning models",
        "author": "L\u00e9onard Blier, Yann Ollivier",
        "date": 1964,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7490-the-description-length-of-deep-learning-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Solomonoff\u2019s general theory of inference (Solomonoff, 1964) and the Minimum Description Length principle (Gr\u00fcnwald, 2007; <a class=\"ref-link\" id=\"cRissanen_2007_a\" href=\"#rRissanen_2007_a\">Rissanen, 2007</a>) formalize Occam\u2019s razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks (<a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\">Hinton and Van Camp, 1993</a>; <a class=\"ref-link\" id=\"cSchmidhuber_1997_a\" href=\"#rSchmidhuber_1997_a\">Schmidhuber, 1997</a>). Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff\u2019s approach."
    },
    "keywords": [
        {
            "term": "minimum description length principle",
            "url": "https://en.wikipedia.org/wiki/Minimum_Description_Length_Principle"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "information theory",
            "url": "https://en.wikipedia.org/wiki/information_theory"
        },
        {
            "term": "variational method",
            "url": "https://en.wikipedia.org/wiki/variational_method"
        },
        {
            "term": "large number",
            "url": "https://en.wikipedia.org/wiki/large_number"
        },
        {
            "term": "intrinsic dimension",
            "url": "https://en.wikipedia.org/wiki/intrinsic_dimension"
        }
    ],
    "highlights": [
        "Deep learning has achieved remarkable results in many different areas (LeCun et al, 2015)",
        "We show that the traditional method to estimate Minimum Description Length codelengths in deep learning, variational inference (<a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\">Hinton and Van Camp, 1993</a>), yields surprisingly inefficient codelengths for deep models, despite explicitly minimizing this criterion",
        "We introduce new practical ways to compute tight compression bounds in deep learning models, based on the Minimum Description Length toolbox (Gr\u00fcnwald, 2007; <a class=\"ref-link\" id=\"cRissanen_2007_a\" href=\"#rRissanen_2007_a\">Rissanen, 2007</a>)",
        "Too Many Parameters in Deep Learning Models? >From an information theory perspective, the goal of a model is to extract as much mutual information between the labels and inputs as possible\u2014equivalently (Section 2.4), to compress the labels",
        "We were surprised to observe that variational inference, though explicitly designed to minimize such codelengths, provides very poor such values compared to a simple incremental coding scheme",
        "Understanding this limitation of variational inference is a topic for future research"
    ],
    "key_statements": [
        "Deep learning has achieved remarkable results in many different areas (LeCun et al, 2015)",
        "We show that the traditional method to estimate Minimum Description Length codelengths in deep learning, variational inference (<a class=\"ref-link\" id=\"cHinton_1993_a\" href=\"#rHinton_1993_a\">Hinton and Van Camp, 1993</a>), yields surprisingly inefficient codelengths for deep models, despite explicitly minimizing this criterion",
        "We introduce new practical ways to compute tight compression bounds in deep learning models, based on the Minimum Description Length toolbox (Gr\u00fcnwald, 2007; <a class=\"ref-link\" id=\"cRissanen_2007_a\" href=\"#rRissanen_2007_a\">Rissanen, 2007</a>)",
        "Too Many Parameters in Deep Learning Models? >From an information theory perspective, the goal of a model is to extract as much mutual information between the labels and inputs as possible\u2014equivalently (Section 2.4), to compress the labels",
        "We were surprised to observe that variational inference, though explicitly designed to minimize such codelengths, provides very poor such values compared to a simple incremental coding scheme",
        "Understanding this limitation of variational inference is a topic for future research"
    ],
    "summary": [
        "Deep learning has achieved remarkable results in many different areas (LeCun et al, 2015).",
        "We introduce new practical ways to compute tight compression bounds in deep learning models, based on the MDL toolbox (Gr\u00fcnwald, 2007; <a class=\"ref-link\" id=\"cRissanen_2007_a\" href=\"#rRissanen_2007_a\">Rissanen, 2007</a>).",
        "We show that prequential coding on top of standard learning, yields much better codelengths than variational inference, correlating better with test set performance.",
        "In turn, various methods to encode the labels y, with or without a deep learning model.",
        "The gain of any codelength compared to the uniform code is limited by the amount of mutual information between input and output.",
        "(This bound is reached with the true model q(y|x).) Any successful compression of the labels is, at the same time, a direct estimation of the mutual information between input and output.",
        "The model with best variational codelength has low classification accuracy on the test set on MNIST and CIFAR, compared to models trained in a non-variational way.",
        "(Even though the encoding procedure is called \u201conline\u201d, it does not mean that only the most recent sample is used to update the parameter \u03b8: the optimization procedure \u03b8can be any predefined technique using all the previous samples (x1:k, y1:k), only requiring that the algorithm has an explicit stopping criterion.) This yields the following description length: Definition 3 (Prequential code).",
        "Prequential coding with deep models provides excellent compression bounds.",
        "Large architectures might overfit during the first steps of the prequential encoding, when the model is trained with few data samples.",
        "This can be observed in practice with neural networks: in Fig. 2, the VGGb model needs 5,000 samples on CIFAR to reach a cumulative compression ratio < 1, even though the encoding cost per label becomes drops below uniform after just 1,000 samples.",
        "The models do compress the data, but with a worse prediction performance: one could conclude that deep learning models that achieve the best prediction performance cannot compress the data.",
        "Thanks to the prequential code, we have seen that deep learning models, even with a large number of parameters, compress the data well: from an information theory point of view, the number of parameters is not an obstacle to compression.",
        "Deep learning models can represent the data together with the model in fewer bits than a naive encoding, despite their many parameters.",
        "We were surprised to observe that variational inference, though explicitly designed to minimize such codelengths, provides very poor such values compared to a simple incremental coding scheme.",
        "Understanding this limitation of variational inference is a topic for future research"
    ],
    "headline": "We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding",
    "reference_links": [
        {
            "id": "Achille_2017_a",
            "entry": "A. Achille and S. Soatto. On the Emergence of Invariance and Disentangling in Deep Representations. arXiv preprint arXiv:1706.01350, jun 2017. URL http://arxiv.org/abs/1706.01350.",
            "url": "http://arxiv.org/abs/1706.01350",
            "arxiv_url": "https://arxiv.org/pdf/1706.01350"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "S. Arora, R. Ge, B. Neyshabur, and Y. Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05296"
        },
        {
            "id": "Ba_2014_a",
            "entry": "L. J. Ba and R. Caruana. Do Deep Nets Really Need to be Deep? In Advances in Neural Information Processing Systems, pages 2654\u20132662, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20L.J.%20Caruana%2C%20R.%20Do%20Deep%20Nets%20Really%20Need%20to%20be%20Deep%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20L.J.%20Caruana%2C%20R.%20Do%20Deep%20Nets%20Really%20Need%20to%20be%20Deep%3F%202014"
        },
        {
            "id": "Barron_1999_a",
            "entry": "A. Barron and Y. Yang. Information-theoretic determination of minimax rates of convergence. The Annals of Statistics, 27(5):1564\u20131599, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20A.%20Yang%2C%20Y.%20Information-theoretic%20determination%20of%20minimax%20rates%20of%20convergence%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20A.%20Yang%2C%20Y.%20Information-theoretic%20determination%20of%20minimax%20rates%20of%20convergence%201999"
        },
        {
            "id": "Blum_2003_a",
            "entry": "A. Blum and J. Langford. PAC-MDL Bounds. In B. Sch\u00f6lkopf and M. K. Warmuth, editors, Learning Theory and Kernel Machines, pages 344\u2013357, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg. ISBN 978-3-540-45167-9.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Langford%2C%20J.%20PAC-MDL%20Bounds%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20A.%20Langford%2C%20J.%20PAC-MDL%20Bounds%202003"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight Uncertainty in Neural Networks. In International Conference on Machine Learning, pages 1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20Blundell%20J%20Cornebise%20K%20Kavukcuoglu%20and%20D%20Wierstra%20Weight%20Uncertainty%20in%20Neural%20Networks%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2016131622%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20Blundell%20J%20Cornebise%20K%20Kavukcuoglu%20and%20D%20Wierstra%20Weight%20Uncertainty%20in%20Neural%20Networks%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2016131622%202015"
        },
        {
            "id": "Chaitin_2007_a",
            "entry": "G. J. Chaitin. On the intelligibility of the universe and the notions of simplicity, complexity and irreducibility. In Thinking about Godel and Turing: Essays on Complexity, 1970-2007. World scientific, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%20J%20Chaitin%20On%20the%20intelligibility%20of%20the%20universe%20and%20the%20notions%20of%20simplicity%20complexity%20and%20irreducibility%20In%20Thinking%20about%20Godel%20and%20Turing%20Essays%20on%20Complexity%2019702007%20World%20scientific%202007"
        },
        {
            "id": "Dawid_1984_a",
            "entry": "A. P. Dawid. Present Position and Potential Developments: Some Personal Views: Statistical Theory: The Prequential Approach. Journal of the Royal Statistical Society. Series A (General), 147 (2):278, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dawid%2C%20A.P.%20Present%20Position%20and%20Potential%20Developments%3A%20Some%20Personal%20Views%3A%20Statistical%20Theory%3A%20The%20Prequential%20Approach%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dawid%2C%20A.P.%20Present%20Position%20and%20Potential%20Developments%3A%20Some%20Personal%20Views%3A%20Statistical%20Theory%3A%20The%20Prequential%20Approach%201984"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "G. K. Dziugaite and D. M. Roy. Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, Sydney, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20G.K.%20Roy%2C%20D.M.%20Computing%20Nonvacuous%20Generalization%20Bounds%20for%20Deep%20%28Stochastic%29%20Neural%20Networks%20with%20Many%20More%20Parameters%20than%20Training%20Data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20G.K.%20Roy%2C%20D.M.%20Computing%20Nonvacuous%20Generalization%20Bounds%20for%20Deep%20%28Stochastic%29%20Neural%20Networks%20with%20Many%20More%20Parameters%20than%20Training%20Data%202017"
        },
        {
            "id": "Foster_1994_a",
            "entry": "D. P. Foster and E. I. George. The Risk Inflation Criterion for Multiple Regression. The Annals of Statistics, 22(4):1947\u20131975, dec 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foster%2C%20D.P.%20George%2C%20E.I.%20The%20Risk%20Inflation%20Criterion%20for%20Multiple%20Regression%201994-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foster%2C%20D.P.%20George%2C%20E.I.%20The%20Risk%20Inflation%20Criterion%20for%20Multiple%20Regression%201994-12"
        },
        {
            "id": "Gao_2016_a",
            "entry": "T. Gao and V. Jojic. Degrees of Freedom in Deep Neural Networks. arXiv preprint arXiv:1603.09260, mar 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.09260"
        },
        {
            "id": "Graves_2011_a",
            "entry": "A. Graves. Practical Variational Inference for Neural Networks. In Neural Information Processing Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20A.%20Practical%20Variational%20Inference%20for%20Neural%20Networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20A.%20Practical%20Variational%20Inference%20for%20Neural%20Networks%202011"
        },
        {
            "id": "Gruenwald_2007_a",
            "entry": "P. D. Gr\u00fcnwald. The Minimum Description Length principle. MIT press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%BCnwald%2C%20P.D.%20The%20Minimum%20Description%20Length%20principle%202007"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "S. Han, H. Mao, and W. J. Dally. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. arXiv preprint arXiv:1510.00149, 2015a.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "Han_et+al_2015_b",
            "entry": "S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both Weights and Connections for Efficient Neural Networks. In Advances in Neural Information Processing Systems, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20S.%20Pool%2C%20J.%20Tran%2C%20J.%20Dally%2C%20W.J.%20Learning%20both%20Weights%20and%20Connections%20for%20Efficient%20Neural%20Networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20S.%20Pool%2C%20J.%20Tran%2C%20J.%20Dally%2C%20W.J.%20Learning%20both%20Weights%20and%20Connections%20for%20Efficient%20Neural%20Networks%202015"
        },
        {
            "id": "Hinton_1993_a",
            "entry": "G. E. Hinton and D. Van Camp. Keeping Neural Networks Simple by Minimizing the Description Length of the Weights. In Proceedings of the sixth annual conference on Computational learning theory. ACM, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Camp%2C%20D.Van%20Keeping%20Neural%20Networks%20Simple%20by%20Minimizing%20the%20Description%20Length%20of%20the%20Weights%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Camp%2C%20D.Van%20Keeping%20Neural%20Networks%20Simple%20by%20Minimizing%20the%20Description%20Length%20of%20the%20Weights%201993"
        },
        {
            "id": "Honkela_2004_a",
            "entry": "A. Honkela and H. Valpola. Variational Learning and Bits-Back Coding: An Information-Theoretic View to Bayesian Learning. IEEE transactions on Neural Networks, 15(4), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Honkela%2C%20A.%20Valpola%2C%20H.%20Variational%20Learning%20and%20Bits-Back%20Coding%3A%20An%20Information-Theoretic%20View%20to%20Bayesian%20Learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Honkela%2C%20A.%20Valpola%2C%20H.%20Variational%20Learning%20and%20Bits-Back%20Coding%3A%20An%20Information-Theoretic%20View%20to%20Bayesian%20Learning%202004"
        },
        {
            "id": "Hutter_2007_a",
            "entry": "M. Hutter. On Universal Prediction and Bayesian Confirmation. Theoretical Computer Science, 384 (1), sep 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hutter%2C%20M.%20On%20Universal%20Prediction%20and%20Bayesian%20Confirmation%202007-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hutter%2C%20M.%20On%20Universal%20Prediction%20and%20Bayesian%20Confirmation%202007-09"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "S. Ioffe and C. Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20Normalization%3A%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20Normalization%3A%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%202015"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images%202009"
        },
        {
            "id": "Kucukelbir_et+al_2017_a",
            "entry": "A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei. Automatic Differentiation Variational Inference. Journal of Machine Learning Research, 18:1\u201345, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kucukelbir%2C%20A.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Gelman%2C%20A.%20Automatic%20Differentiation%20Variational%20Inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kucukelbir%2C%20A.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Gelman%2C%20A.%20Automatic%20Differentiation%20Variational%20Inference%202017"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11), 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-Based%20Learning%20Applied%20to%20Document%20Recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-Based%20Learning%20Applied%20to%20Document%20Recognition%201998"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "C. Li, H. Farkhoor, R. Liu, and J. Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv preprint arXiv:1804.08838, apr 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08838"
        },
        {
            "id": "Li_2008_a",
            "entry": "M. Li and P. Vit\u00e1nyi. An introduction to Kolmogorov complexity. Springer, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20M.%20Vit%C3%A1nyi%2C%20P.%20An%20introduction%20to%20Kolmogorov%20complexity%202008"
        },
        {
            "id": "Louizos_et+al_2017_a",
            "entry": "C. Louizos, K. Ullrich, and M. Welling. Bayesian compression for deep learning. In Advances in Neural Information Processing Systems, pages 3290\u20133300, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20C.%20Ullrich%2C%20K.%20Welling%2C%20M.%20Bayesian%20compression%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20C.%20Ullrich%2C%20K.%20Welling%2C%20M.%20Bayesian%20compression%20for%20deep%20learning%202017"
        },
        {
            "id": "Mackay_2003_a",
            "entry": "D. J. C. Mackay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, cambridge edition, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mackay%2C%20D.J.C.%20Information%20Theory%2C%20Inference%2C%20and%20Learning%20Algorithms%202003"
        },
        {
            "id": "Ollivier_2014_a",
            "entry": "Y. Ollivier. Auto-encoders: reconstruction versus compression. arXiv preprint arXiv:1403.7752, mar 2014. URL http://arxiv.org/abs/1403.7752.",
            "url": "http://arxiv.org/abs/1403.7752",
            "arxiv_url": "https://arxiv.org/pdf/1403.7752"
        },
        {
            "id": "Rissanen_2007_a",
            "entry": "J. Rissanen. Information and complexity in statistical modeling. Springer Science & Business Media, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rissanen%2C%20J.%20Information%20and%20complexity%20in%20statistical%20modeling%202007"
        },
        {
            "id": "Rissanen_et+al_1992_a",
            "entry": "J. Rissanen, T. Speed, and B. Yu. Density estimation by stochastic complexity. IEEE Transactions on Information Theory, 38(2):315\u2013323, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rissanen%2C%20J.%20Speed%2C%20T.%20Yu%2C%20B.%20Density%20estimation%20by%20stochastic%20complexity%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rissanen%2C%20J.%20Speed%2C%20T.%20Yu%2C%20B.%20Density%20estimation%20by%20stochastic%20complexity%201992"
        },
        {
            "id": "Romero_et+al_2015_a",
            "entry": "A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In Proceedings of the International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romero%2C%20A.%20Ballas%2C%20N.%20Kahou%2C%20S.E.%20Chassang%2C%20A.%20Fitnets%3A%20Hints%20for%20thin%20deep%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romero%2C%20A.%20Ballas%2C%20N.%20Kahou%2C%20S.E.%20Chassang%2C%20A.%20Fitnets%3A%20Hints%20for%20thin%20deep%20nets%202015"
        },
        {
            "id": "Schmidhuber_1997_a",
            "entry": "J. Schmidhuber. Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability. Neural Networks, 10(5):857\u2013873, jul 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Discovering%20Neural%20Nets%20with%20Low%20Kolmogorov%20Complexity%20and%20High%20Generalization%20Capability%201997-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Discovering%20Neural%20Nets%20with%20Low%20Kolmogorov%20Complexity%20and%20High%20Generalization%20Capability%201997-07"
        },
        {
            "id": "See_et+al_2016_a",
            "entry": "A. See, M.-T. Luong, and C. D. Manning. Compression of Neural Machine Translation Models via Pruning. arXiv preprint arXiv:1606.09274, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.09274"
        },
        {
            "id": "Shannon_1948_a",
            "entry": "C. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27, 1948.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shannon%2C%20C.%20A%20mathematical%20theory%20of%20communication%201948",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shannon%2C%20C.%20A%20mathematical%20theory%20of%20communication%201948"
        },
        {
            "id": "Shwartz-Ziv_2017_a",
            "entry": "R. Shwartz-Ziv and N. Tishby. Opening the Black Box of Deep Neural Networks via Information. arXiv preprint arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1409.1556, sep 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Solomonoff_2018_a",
            "entry": "R. Solomonoff. A formal theory of inductive inference. Information and control, 1964. C. Tallec and L. Blier. Pyvarinf : Variational Inference for PyTorch, 2018. URL https://github.",
            "url": "https://github"
        },
        {
            "id": "Com_2015_a",
            "entry": "com/ctallec/pyvarinf. N. Tishby and N. Zaslavsky. Deep Learning and the Information Bottleneck Principle. In Information Theory Workshop, pages 1\u20135. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=com/ctallec/pyvarinf.%20N.%20Tishby%20Zaslavsky%2C%20N.%20Deep%20Learning%20and%20the%20Information%20Bottleneck%20Principle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=com/ctallec/pyvarinf.%20N.%20Tishby%20Zaslavsky%2C%20N.%20Deep%20Learning%20and%20the%20Information%20Bottleneck%20Principle%202015"
        },
        {
            "id": "Ullrich_et+al_2017_a",
            "entry": "K. Ullrich, E. Meeds, and M. Welling. Soft Weight-Sharing for Neural Network Compression. arXiv preprint arXiv:1702.04008, 2017. T. Van Erven, P. Gr\u00fcnwald, and S. De Rooij. Catching Up Faster by Switching Sooner: A predictive approach to adaptive estimation with an application to the AIC-BIC Dilemma. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):361\u2013417, 2012. T.-B. Xu, P. Yang, X.-Y. Zhang, and C.-L. Liu. Margin-Aware Binarized Weight Networks for Image Classification. In International Conference on Image and Graphics, pages 590\u2013601.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04008"
        },
        {
            "id": "Springer_2017_a",
            "entry": "Springer, Cham, sep 2017. S. Zagoruyko. 92.45% on CIFAR-10 in Torch, 2015. URL http://torch.ch/blog/2015/07/30/cifar.html. C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In Proceedings of the International Conference on Learning Representations, 2017.",
            "url": "http://torch.ch/blog/2015/07/30/cifar.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springer%2C%20Cham%2092.45%25%20on%20CIFAR-10%20in%202017-09"
        }
    ]
}
