{
    "filename": "7790-estimating-learnability-in-the-sublinear-data-regime.pdf",
    "metadata": {
        "title": "Estimating Learnability in the Sublinear Data Regime",
        "author": "Weihao Kong, Gregory Valiant",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7790-estimating-learnability-in-the-sublinear-data-regime.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider the problem of estimating how well a model class is capable of fitting a distribution of labeled data. We show that it is possible to accurately estimate this \u201clearnability\u201d even when given an amount of data that is too small to reliably learn any accurate model. Our first result applies to the setting where the data is drawn from a d-dimensional distribution with isotropic covariance, and the label of each datapoint is an\u221aarbitrary noisy function of the datapoint. In this setting, we show that with O( d) samples, one can accurately estimate the fraction of the variance of the label that can be explained via the best linear function of the data. For comparison, even if the labels are noiseless linear functions of the data, a sample size linear in the dimension, d, is required to learn any function correlated with the underlying model. Our estimation approach also applies to the setting where the data distribution has an (unknown) arbitrary covariance matrix, allowing these techniques to be applied to settings where the model class consists of a linear function applied to a nonlinear embedding of the data. In this setting we give a consistent estimator of the fraction of explainable variance that uses o(d) samples. Finally, our techniques also extend to the setting of binary classification, where we obtain analogous results under the logistic model, for estimating the classification accuracy of the best linear classifier. We demonstrate the practical viability of our approaches on synthetic and real data. This ability to estimate the explanatory value of a set of features (or dataset), even in the regime in which there is too little data to realize that explanatory value, may be relevant to the scientific and industrial settings for which data collection is expensive and there are many potentially relevant feature sets that could be collected."
    },
    "keywords": [
        {
            "term": "linear function",
            "url": "https://en.wikipedia.org/wiki/linear_function"
        },
        {
            "term": "binary classification",
            "url": "https://en.wikipedia.org/wiki/binary_classification"
        },
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        }
    ],
    "highlights": [
        "Given too little labeled data to learn a model or classifier, is it possible to determine whether an accurate classifier or predictor exists? For example, consider a setting where you are given n datapoints with real-valued labels drawn from some distribution of interest, D",
        "Suppose we are given access to independent samples from a d-dimensional isotropic Gaussian, and each sample, x \u2208 Rd is labeled according to a noisy linear function y = x, \u03b2 + \u03b7, where \u03b2 is the true model and the noise \u03b7 is drawn from a distribution of variance \u03b42",
        "Our first result applies to the setting where the data is drawn according to a d dimensional distribution with identity covariance, and the labels are noisy linear functions",
        "Our estimation machinery extends beyond the isotropic setting, and we prove an analog of Proposition 1 in the more general setting where the datapoints, xi are drawn from a d dimensional distribution with non-isotropic covariance",
        "Our algorithms and techniques do not rely on the assumption that the labels consist of a linear function plus independent noise, and our results partially extend to the agnostic setting",
        "Our result is more general in the following senses: 1) Our estimator and analysis do not rely on Gaussianity assumptions; 2) Our results apply beyond the setting where the label y is a linear function of x plus independent noise, and estimates the fraction of the variance that can be explained via a linear function (the \u201cagnostic\u201d"
    ],
    "key_statements": [
        "Given too little labeled data to learn a model or classifier, is it possible to determine whether an accurate classifier or predictor exists? For example, consider a setting where you are given n datapoints with real-valued labels drawn from some distribution of interest, D",
        "Suppose we are given access to independent samples from a d-dimensional isotropic Gaussian, and each sample, x \u2208 Rd is labeled according to a noisy linear function y = x, \u03b2 + \u03b7, where \u03b2 is the true model and the noise \u03b7 is drawn from a distribution of variance \u03b42",
        "Our first result applies to the setting where the data is drawn according to a d dimensional distribution with identity covariance, and the labels are noisy linear functions",
        "Our estimation machinery extends beyond the isotropic setting, and we prove an analog of Proposition 1 in the more general setting where the datapoints, xi are drawn from a d dimensional distribution with non-isotropic covariance",
        "Our algorithms and techniques do not rely on the assumption that the labels consist of a linear function plus independent noise, and our results partially extend to the agnostic setting",
        "Our result is more general in the following senses: 1) Our estimator and analysis do not rely on Gaussianity assumptions; 2) Our results apply beyond the setting where the label y is a linear function of x plus independent noise, and estimates the fraction of the variance that can be explained via a linear function (the \u201cagnostic\u201d"
    ],
    "summary": [
        "Given too little labeled data to learn a model or classifier, is it possible to determine whether an accurate classifier or predictor exists? For example, consider a setting where you are given n datapoints with real-valued labels drawn from some distribution of interest, D.",
        "Our first result applies to the setting where the data is drawn according to a d dimensional distribution with identity covariance, and the labels are noisy linear functions.",
        "In the setting where the distribution of x is an isotropic Gaussian, we obtain the simpl\u221aer result that the classification error of the best linear classifier can be accurately estimated with O( d) samples.",
        "In the setting of Theorem 3 but where each datapoint, xi is drawn according to a d-dimension isotropic Gaussian distribution, there is an algorithm that takes n labeled samp\u221ales, and with probability",
        "Despite the strong assumptions on the data-generating distribution in the above theorem and corollary, the algorithm to which they apply seems to perform quite well on real-world data, and is capable of accurately estimating the classification error of the best linear predictor, even in the data regime where it is impossible to learn any good predictor.",
        "These estimators apply to the most restrictive setting we consider, where each label y = \u03b2T x + \u03b7 is given as a linear function of x plus independent noise \u03b7 of variance \u03b42.",
        "Their theoretical results rely on the assumptions that the data x is drawn from an isotropic Gaussian distribution, and that the label is a linear function plus independent noise, and the performance bounds become trivial if n/d \u2192 0.",
        "Our result is more general in the following senses: 1) Our estimator and analysis do not rely on Gaussianity assumptions; 2) Our results apply beyond the setting where the label y is a linear function of x plus independent noise, and estimates the fraction of the variance that can be explained via a linear function; and 3) our approach extends to the unknown non-isotropic covariance setting.",
        "To see the intuition for the unbiased estimators of \u03b2T \u03a3k\u03b2 computed in Algorithm 1, we examine the k = 2 case: consider drawing two independent samples (x1, y1), (x2, y2) from our linear model plus noise.",
        "This dataset is well-suited for our setting because the NLP setting presents a variety of natural high-dimensional featurizations, and the 150k datapoints were sufficient to accurately estimate a \u201cground truth\u201d prediction error, allowing us to approximate the residual variance in the point value that cannot be captured via a linear model over the specified features."
    ],
    "headline": "We show that it is possible to accurately estimate this \u201clearnability\u201d even when given an amount of data that is too small to reliably learn any accurate model",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Zhidong D Bai, Yong Q Yin, and Paruchuri R Krishnaiah. On the limiting empirical distribution function of the eigenvalues of a multivariate f matrix. Theory of Probability & Its Applications, 32(3):490\u2013500, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bai%2C%20Zhidong%20D.%20Yin%2C%20Yong%20Q.%20Krishnaiah%2C%20Paruchuri%20R.%20On%20the%20limiting%20empirical%20distribution%20function%20of%20the%20eigenvalues%20of%20a%20multivariate%20f%20matrix%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bai%2C%20Zhidong%20D.%20Yin%2C%20Yong%20Q.%20Krishnaiah%2C%20Paruchuri%20R.%20On%20the%20limiting%20empirical%20distribution%20function%20of%20the%20eigenvalues%20of%20a%20multivariate%20f%20matrix%201988"
        },
        {
            "id": "2",
            "entry": "[2] Mohsen Bayati, Murat A Erdogdu, and Andrea Montanari. Estimating lasso risk and noise level. In Advances in Neural Information Processing Systems, pages 944\u2013952, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bayati%2C%20Mohsen%20Erdogdu%2C%20Murat%20A.%20Montanari%2C%20Andrea%20Estimating%20lasso%20risk%20and%20noise%20level%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bayati%2C%20Mohsen%20Erdogdu%2C%20Murat%20A.%20Montanari%2C%20Andrea%20Estimating%20lasso%20risk%20and%20noise%20level%202013"
        },
        {
            "id": "3",
            "entry": "[3] Mihir Bellare, Don Coppersmith, JOHAN Hastad, Marcos Kiwi, and Madhu Sudan. Linearity testing in characteristic two. IEEE Transactions on Information Theory, 42(6):1781\u20131795, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellare%2C%20Mihir%20Don%20Coppersmith%2C%20J.O.H.A.N.Hastad%20Kiwi%2C%20Marcos%20Sudan%2C%20Madhu%20Linearity%20testing%20in%20characteristic%20two%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellare%2C%20Mihir%20Don%20Coppersmith%2C%20J.O.H.A.N.Hastad%20Kiwi%2C%20Marcos%20Sudan%2C%20Madhu%20Linearity%20testing%20in%20characteristic%20two%201996"
        },
        {
            "id": "4",
            "entry": "[4] Eli Ben-Sasson, Madhu Sudan, Salil Vadhan, and Avi Wigderson. Randomness-efficient low degree tests and short pcps via epsilon-biased sets. In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing, pages 612\u2013621. ACM, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-Sasson%2C%20Eli%20Sudan%2C%20Madhu%20Vadhan%2C%20Salil%20Wigderson%2C%20Avi%20Randomness-efficient%20low%20degree%20tests%20and%20short%20pcps%20via%20epsilon-biased%20sets%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-Sasson%2C%20Eli%20Sudan%2C%20Madhu%20Vadhan%2C%20Salil%20Wigderson%2C%20Avi%20Randomness-efficient%20low%20degree%20tests%20and%20short%20pcps%20via%20epsilon-biased%20sets%202003"
        },
        {
            "id": "5",
            "entry": "[5] Xi Chen, Rocco A Servedio, and Li-Yang Tan. New algorithms and lower bounds for monotonicity testing. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 286\u2013295. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Servedio%2C%20Rocco%20A.%20Tan%2C%20Li-Yang%20New%20algorithms%20and%20lower%20bounds%20for%20monotonicity%20testing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Servedio%2C%20Rocco%20A.%20Tan%2C%20Li-Yang%20New%20algorithms%20and%20lower%20bounds%20for%20monotonicity%20testing%202014"
        },
        {
            "id": "6",
            "entry": "[6] Lee H Dicker. Variance estimation in high-dimensional linear models. Biometrika, 101(2):269\u2013 284, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dicker%2C%20Lee%20H.%20Variance%20estimation%20in%20high-dimensional%20linear%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dicker%2C%20Lee%20H.%20Variance%20estimation%20in%20high-dimensional%20linear%20models%202014"
        },
        {
            "id": "7",
            "entry": "[7] Jianqing Fan, Shaojun Guo, and Ning Hao. Variance estimation using refitted cross-validation in ultrahigh dimensional regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(1):37\u201365, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Jianqing%20Guo%2C%20Shaojun%20Hao%2C%20Ning%20Variance%20estimation%20using%20refitted%20cross-validation%20in%20ultrahigh%20dimensional%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Jianqing%20Guo%2C%20Shaojun%20Hao%2C%20Ning%20Variance%20estimation%20using%20refitted%20cross-validation%20in%20ultrahigh%20dimensional%20regression%202012"
        },
        {
            "id": "8",
            "entry": "[8] Oded Goldreich, S Goldwassert, Eric Lehman, and Dana Ron. Testing monotonicity. In Foundations of Computer Science, 1998. Proceedings. 39th Annual Symposium on, pages 426\u2013435. IEEE, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oded%20Goldreich%2C%20S.Goldwassert%20Lehman%2C%20Eric%20Ron%2C%20Dana%20Testing%20monotonicity%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oded%20Goldreich%2C%20S.Goldwassert%20Lehman%2C%20Eric%20Ron%2C%20Dana%20Testing%20monotonicity%201998"
        },
        {
            "id": "9",
            "entry": "[9] Zijian Guo, Wanjie Wang, T Tony Cai, and Hongzhe Li. Optimal estimation of genetic relatedness in high-dimensional linear models. Journal of the American Statistical Association, (just-accepted), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Zijian%20Wanjie%20Wang%2C%20T.Tony%20Cai%20Li%2C%20Hongzhe%20Optimal%20estimation%20of%20genetic%20relatedness%20in%20high-dimensional%20linear%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Zijian%20Wanjie%20Wang%2C%20T.Tony%20Cai%20Li%2C%20Hongzhe%20Optimal%20estimation%20of%20genetic%20relatedness%20in%20high-dimensional%20linear%20models%202017"
        },
        {
            "id": "10",
            "entry": "[10] Lucas Janson, Rina Foygel Barber, and Emmanuel Candes. Eigenprism: inference for high dimensional signal-to-noise ratios. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(4):1037\u20131065, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Janson%2C%20Lucas%20Barber%2C%20Rina%20Foygel%20Candes%2C%20Emmanuel%20Eigenprism%3A%20inference%20for%20high%20dimensional%20signal-to-noise%20ratios%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Janson%2C%20Lucas%20Barber%2C%20Rina%20Foygel%20Candes%2C%20Emmanuel%20Eigenprism%3A%20inference%20for%20high%20dimensional%20signal-to-noise%20ratios%202017"
        },
        {
            "id": "11",
            "entry": "[11] Weihao Kong and Gregory Valiant. Spectrum estimation from samples. The Annals of Statistics, 45(5):2218\u20132247, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kong%2C%20Weihao%20Valiant%2C%20Gregory%20Spectrum%20estimation%20from%20samples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kong%2C%20Weihao%20Valiant%2C%20Gregory%20Spectrum%20estimation%20from%20samples%202017"
        },
        {
            "id": "12",
            "entry": "[12] Nicolas St\u00e4dler, Peter B\u00fchlmann, and Sara Van De Geer. l1-penalization for mixture regression models. Test, 19(2):209\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nicolas%20St%C3%A4dler%2C%20Peter%20B%C3%BChlmann%20Geer%2C%20Sara%20Van%20De%20l1-penalization%20for%20mixture%20regression%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nicolas%20St%C3%A4dler%2C%20Peter%20B%C3%BChlmann%20Geer%2C%20Sara%20Van%20De%20l1-penalization%20for%20mixture%20regression%20models%202010"
        },
        {
            "id": "13",
            "entry": "[13] Tingni Sun and Cun-Hui Zhang. Scaled sparse linear regression. Biometrika, 99(4):879\u2013898, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Tingni%20Zhang%2C%20Cun-Hui%20Scaled%20sparse%20linear%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Tingni%20Zhang%2C%20Cun-Hui%20Scaled%20sparse%20linear%20regression%202012"
        },
        {
            "id": "14",
            "entry": "[14] Nicolas Verzelen, Elisabeth Gassiat, et al. Adaptive estimation of high-dimensional signal-tonoise ratios. Bernoulli, 24(4B):3683\u20133710, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verzelen%2C%20Nicolas%20Gassiat%2C%20Elisabeth%20Adaptive%20estimation%20of%20high-dimensional%20signal-tonoise%20ratios%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verzelen%2C%20Nicolas%20Gassiat%2C%20Elisabeth%20Adaptive%20estimation%20of%20high-dimensional%20signal-tonoise%20ratios%202018"
        },
        {
            "id": "15",
            "entry": "[15] Nicolas Verzelen, Fanny Villers, et al. Goodness-of-fit tests for high-dimensional gaussian linear models. The Annals of Statistics, 38(2):704\u2013752, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verzelen%2C%20Nicolas%20Villers%2C%20Fanny%20Goodness-of-fit%20tests%20for%20high-dimensional%20gaussian%20linear%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verzelen%2C%20Nicolas%20Villers%2C%20Fanny%20Goodness-of-fit%20tests%20for%20high-dimensional%20gaussian%20linear%20models%202010"
        },
        {
            "id": "16",
            "entry": "[16] Eshetu Wencheko. Estimation of the signal-to-noise in the linear regression model. Statistical Papers, 41(3):327, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wencheko%2C%20Eshetu%20Estimation%20of%20the%20signal-to-noise%20in%20the%20linear%20regression%20model%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wencheko%2C%20Eshetu%20Estimation%20of%20the%20signal-to-noise%20in%20the%20linear%20regression%20model%202000"
        },
        {
            "id": "17",
            "entry": "[17] Yong Q Yin and Paruchuri R Krishnaiah. A limit theorem for the eigenvalues of product of two random matrices. Journal of Multivariate Analysis, 13(4):489\u2013507, 1983. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20Yong%20Q.%20Krishnaiah%2C%20Paruchuri%20R.%20A%20limit%20theorem%20for%20the%20eigenvalues%20of%20product%20of%20two%20random%20matrices%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20Yong%20Q.%20Krishnaiah%2C%20Paruchuri%20R.%20A%20limit%20theorem%20for%20the%20eigenvalues%20of%20product%20of%20two%20random%20matrices%201983"
        }
    ]
}
