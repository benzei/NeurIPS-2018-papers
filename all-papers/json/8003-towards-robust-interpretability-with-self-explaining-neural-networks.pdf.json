{
    "filename": "8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf",
    "metadata": {
        "title": "Towards Robust Interpretability with Self-Explaining Neural Networks",
        "author": "David Alvarez Melis, Tommi Jaakkola",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general \u2013 explicitness, faithfulness, and stability \u2013 and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        }
    ],
    "highlights": [
        "Interpretability or lack thereof can limit the adoption of machine learning methods in decision-critical domains such as medical or legal",
        "We can directly estimate this quantity for self-explaining neural network since the explanation generation is end-to-end differentiable with respect to concepts, and we can rely on direct automatic differentiation and back-propagation to optimize for the maximizing argument xj, as often done for computing adversarial examples for neural networks [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "Li et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] propose an interpretable neural network architecture whose predictions are based on the similarity of the input to a small set of prototypes, which are learnt during training",
        "Interpretability and performance currently stand in apparent conflict in machine learning",
        "We demonstrate how the fusion of these ideas leads to a class of rich, complex models that are able to produce robust explanations, a key property that we show is missing from various popular interpretability frameworks"
    ],
    "key_statements": [
        "Interpretability or lack thereof can limit the adoption of machine learning methods in decision-critical domains such as medical or legal",
        "Most post-hoc interpretability frameworks are not stable in this sense as shown in detail in Section 5.4",
        "We can directly estimate this quantity for self-explaining neural network since the explanation generation is end-to-end differentiable with respect to concepts, and we can rely on direct automatic differentiation and back-propagation to optimize for the maximizing argument xj, as often done for computing adversarial examples for neural networks [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "Li et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] propose an interpretable neural network architecture whose predictions are based on the similarity of the input to a small set of prototypes, which are learnt during training",
        "Interpretability and performance currently stand in apparent conflict in machine learning",
        "We demonstrate how the fusion of these ideas leads to a class of rich, complex models that are able to produce robust explanations, a key property that we show is missing from various popular interpretability frameworks"
    ],
    "summary": [
        "Interpretability or lack thereof can limit the adoption of machine learning methods in decision-critical domains such as medical or legal.",
        "We refer to these more general features as interpretable basis concepts and use them in place of raw inputs in our models.",
        "Note that the notion of locality must take into account how the concepts rather than inputs vary since the model is interpreted as being linear in the concepts rather than x.",
        "For a given input x, we define the explanation of f (x) to be the set Ef (x) \u2318 {, \u2713i(x))}ki=1 of basis concepts and their influence scores.",
        "Since this is rarely available, a common approach to measuring the faithfulness of relevance scores with respect to the model they are explaining relies on a proxy notion of importance: observing the effect of removing features on the model\u2019s prediction.",
        "As argued throughout this work, a crucial property that interpretability methods should satisfy to generate meaningful explanations is that of robustness with respect to local perturbations of the input.",
        "We can directly estimate this quantity for SENN since the explanation generation is end-to-end differentiable with respect to concepts, and we can rely on direct automatic differentiation and back-propagation to optimize for the maximizing argument xj, as often done for computing adversarial examples for neural networks [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].",
        "It is interesting to visualize the inputs and corresponding explanations that maximize criterion (5) \u2013or its discrete counterpart, when appropriate\u2013 for different methods and datasets, since these succinctly exhibit the issue of lack of robustness that our work seeks to address.",
        "Our approach differs both in what it considers the units of explanation\u2014general concepts, not necessarily raw inputs\u2014and how it uses them, intrinsically relying on the relevance scores it produces to make predictions, obviating the need for additional computation.",
        "Contextual Explanation Networks [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] are inspired by the goal of designing a class of models that learns to predict and explain jointly, but differ from our approach in their formulation and realization of the model.",
        "Our approach departs from that work in that we explicitly enforce robustness with respect to the units of explanation and we formulate concepts as part of the explanation, requiring them to be grounded and interpretable.",
        "Li et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] propose an interpretable neural network architecture whose predictions are based on the similarity of the input to a small set of prototypes, which are learnt during training."
    ],
    "headline": "We propose three desiderata for explanations in general \u2013 explicitness, faithfulness, and stability \u2013 and show that existing methods do not satisfy them",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Alvarez-Melis and T. S. Jaakkola. \u201cOn the Robustness of Interpretability Methods\u201d. In: Proceedings of the 2018 ICML Workshop in Human Interpretability in Machine Learning. 2018. arXiv: 1806.08049.",
            "arxiv_url": "https://arxiv.org/pdf/1806.08049"
        },
        {
            "id": "2",
            "entry": "[2] D. Alvarez-Melis and T. S. Jaakkola. \u201cA causal framework for explaining the predictions of black-box sequence-to-sequence models\u201d. In: Conference on Empirical Methods in Natural Language Processing (EMNLP). 2017, pp. 412\u2013421.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alvarez-Melis%2C%20D.%20Jaakkola%2C%20T.S.%20A%20causal%20framework%20for%20explaining%20the%20predictions%20of%20black-box%20sequence-to-sequence%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alvarez-Melis%2C%20D.%20Jaakkola%2C%20T.S.%20A%20causal%20framework%20for%20explaining%20the%20predictions%20of%20black-box%20sequence-to-sequence%20models%202017"
        },
        {
            "id": "3",
            "entry": "[3] L. Arras, F. Horn, G. Montavon, K.-R. M\u00fcller, and W. Samek. \u201c\"What is relevant in a text document?\": An interpretable machine learning approach\u201d. In: PLos ONE 12.8 (2017), pp. 1\u2013 23.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%22What%20is%20relevant%20in%20a%20text%20document%3F%22%3A%20An%20interpretable%20machine%20learning%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=%22What%20is%20relevant%20in%20a%20text%20document%3F%22%3A%20An%20interpretable%20machine%20learning%20approach%202017"
        },
        {
            "id": "4",
            "entry": "[4] S. Bach, A. Binder, G. Montavon, F. Klauschen, K. R. M\u00fcller, and W. Samek. \u201cOn pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\u201d. In: PLos ONE 10.7 (2015).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "5",
            "entry": "[5] F. Doshi-Velez and B. Kim. \u201cTowards a Rigorous Science of Interpretable Machine Learning\u201d. In: ArXiv e-prints Ml (2017), pp. 1\u201312. arXiv: 1702.08608.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08608"
        },
        {
            "id": "6",
            "entry": "[6] I. J. Goodfellow, J. Shlens, and C. Szegedy. \u201cExplaining and Harnessing Adversarial Examples\u201d. In: International Conference on Learning Representations. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20Harnessing%20Adversarial%20Examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20Harnessing%20Adversarial%20Examples%202015"
        },
        {
            "id": "7",
            "entry": "[7] N. Grgic-Hlaca, M. B. Zafar, K. P. Gummadi, and A. Weller. \u201cBeyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning\u201d. In: AAAI Conference on Artificial Intelligence. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grgic-Hlaca%2C%20N.%20Zafar%2C%20M.B.%20Gummadi%2C%20K.P.%20Weller%2C%20A.%20Beyond%20Distributive%20Fairness%20in%20Algorithmic%20Decision%20Making%3A%20Feature%20Selection%20for%20Procedurally%20Fair%20Learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grgic-Hlaca%2C%20N.%20Zafar%2C%20M.B.%20Gummadi%2C%20K.P.%20Weller%2C%20A.%20Beyond%20Distributive%20Fairness%20in%20Algorithmic%20Decision%20Making%3A%20Feature%20Selection%20for%20Procedurally%20Fair%20Learning%202018"
        },
        {
            "id": "8",
            "entry": "[8] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, and R. Sayres. \u201c Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) \u201d. In: International Conference on Machine Learning (ICML). 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20B.%20Wattenberg%2C%20M.%20Gilmer%2C%20J.%20Cai%2C%20C.%20Interpretability%20Beyond%20Feature%20Attribution%3A%20Quantitative%20Testing%20with%20Concept%20Activation%20Vectors%20%28TCAV%29%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20B.%20Wattenberg%2C%20M.%20Gilmer%2C%20J.%20Cai%2C%20C.%20Interpretability%20Beyond%20Feature%20Attribution%3A%20Quantitative%20Testing%20with%20Concept%20Activation%20Vectors%20%28TCAV%29%202018"
        },
        {
            "id": "9",
            "entry": "[9] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. Sch\u00fctt, S. D\u00e4hne, D. Erhan, and B. Kim. \u201cThe (Un)reliability of saliency methods\u201d. In: NIPS workshop on Explaining and Visualizing Deep Learning (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kindermans%2C%20P.-J.%20Hooker%2C%20S.%20Adebayo%2C%20J.%20Alber%2C%20M.%20The%20%28Un%29reliability%20of%20saliency%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kindermans%2C%20P.-J.%20Hooker%2C%20S.%20Adebayo%2C%20J.%20Alber%2C%20M.%20The%20%28Un%29reliability%20of%20saliency%20methods%202017"
        },
        {
            "id": "10",
            "entry": "[10] A. Krizhevsky. Learning multiple layers of features from tiny images. Tech. rep. Citeseer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "11",
            "entry": "[11] T. Lei, R. Barzilay, and T. Jaakkola. \u201cRationalizing Neural Predictions\u201d. In: Conference on Empirical Methods in Natural Language Processing (EMNLP). 2016, pp. 107\u2013117. arXiv: 1606.04155.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04155"
        },
        {
            "id": "12",
            "entry": "[12] O. Li, H. Liu, C. Chen, and C. Rudin. \u201cDeep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions\u201d. In: AAAI Conference on Artificial Intelligence. 2018. arXiv: 1710.04806.",
            "arxiv_url": "https://arxiv.org/pdf/1710.04806"
        },
        {
            "id": "13",
            "entry": "[13] M. Lichman and K. Bache. UCI Machine Learning Repository. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lichman%2C%20M.%20Bache%2C%20K.%20UCI%20Machine%20Learning%20Repository%202013"
        },
        {
            "id": "14",
            "entry": "[14] S. Lundberg and S.-I. Lee. \u201cA unified approach to interpreting model predictions\u201d. In: Advances in Neural Information Processing Systems 30. 2017, pp. 4768\u20134777. arXiv: 1705.07874.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07874"
        },
        {
            "id": "15",
            "entry": "[15] G. Montavon, W. Samek, and K.-R. M\u00fcller. \u201cMethods for interpreting and understanding deep neural networks\u201d. In: Digital Signal Processing (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montavon%2C%20G.%20Samek%2C%20W.%20M%C3%BCller%2C%20K.-R.%20Methods%20for%20interpreting%20and%20understanding%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montavon%2C%20G.%20Samek%2C%20W.%20M%C3%BCller%2C%20K.-R.%20Methods%20for%20interpreting%20and%20understanding%20deep%20neural%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] M. T. Ribeiro, S. Singh, and C. Guestrin. \u201c\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\u201d. In: ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). New York, NY, USA: ACM, 2016, pp. 1135\u20131144. arXiv: 1602.04938.",
            "arxiv_url": "https://arxiv.org/pdf/1602.04938"
        },
        {
            "id": "17",
            "entry": "[17] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K. R. M\u00fcller. \u201cEvaluating the visualization of what a deep neural network has learned\u201d. In: IEEE Transactions on Neural Networks and Learning Systems 28.11 (2017), pp. 2660\u20132673. arXiv: 1509.06321.",
            "arxiv_url": "https://arxiv.org/pdf/1509.06321"
        },
        {
            "id": "18",
            "entry": "[18] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra. \u201cGrad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization\u201d. In: ICCV. 2017. arXiv: 1610.02391.",
            "arxiv_url": "https://arxiv.org/pdf/1610.02391"
        },
        {
            "id": "19",
            "entry": "[19] M. Al-Shedivat, A. Dubey, and E. P. Xing. \u201cContextual Explanation Networks\u201d. In: arXiv preprint arXiv:1705.10301 (2017).",
            "arxiv_url": "https://arxiv.org/pdf/1705.10301"
        },
        {
            "id": "20",
            "entry": "[20] A. Shrikumar, P. Greenside, and A. Kundaje. \u201cLearning Important Features Through Propagating Activation Differences\u201d. In: International Conference on Machine Learning (ICML). Ed. by D. Precup and Y. W. Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, June 2017, pp. 3145\u20133153. arXiv: 1704.02685.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02685"
        },
        {
            "id": "21",
            "entry": "[21] K. Simonyan, A. Vedaldi, and A. Zisserman. \u201cDeep inside convolutional networks: Visualising image classification models and saliency maps\u201d. In: International Conference on Learning Representations (Workshop Track). 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Vedaldi%2C%20A.%20Zisserman%2C%20A.%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Vedaldi%2C%20A.%20Zisserman%2C%20A.%20Deep%20inside%20convolutional%20networks%3A%20Visualising%20image%20classification%20models%20and%20saliency%20maps%202014"
        },
        {
            "id": "22",
            "entry": "[22] J. Snoek, H. Larochelle, and R. P. Adams. \u201cPractical Bayesian Optimization of Machine Learning Algorithms\u201d. In: Advances in Neural Information Processing Systems (NIPS). 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20Optimization%20of%20Machine%20Learning%20Algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20Optimization%20of%20Machine%20Learning%20Algorithms%202012"
        },
        {
            "id": "23",
            "entry": "[23] M. Sundararajan, A. Taly, and Q. Yan. \u201cAxiomatic attribution for deep networks\u201d. In: arXiv preprint arXiv:1703.01365 (2017).",
            "arxiv_url": "https://arxiv.org/pdf/1703.01365"
        },
        {
            "id": "24",
            "entry": "[24] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. \u201cUnderstanding neural networks through deep visualization\u201d. In: arXiv preprint arXiv:1506.06579 (2015).",
            "arxiv_url": "https://arxiv.org/pdf/1506.06579"
        },
        {
            "id": "25",
            "entry": "[25] M. B. Zafar, I. Valera, M. Rodriguez, K. Gummadi, and A. Weller. \u201cFrom parity to preferencebased notions of fairness in classification\u201d. In: Advances in Neural Information Processing Systems (NIPS). 2017, pp. 228\u2013238.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.%20Gummadi%2C%20K.%20From%20parity%20to%20preferencebased%20notions%20of%20fairness%20in%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.%20Gummadi%2C%20K.%20From%20parity%20to%20preferencebased%20notions%20of%20fairness%20in%20classification%202017"
        },
        {
            "id": "26",
            "entry": "[26] M. D. Zeiler and R. Fergus. \u201cVisualizing and understanding convolutional networks\u201d. In: European conference on computer vision. Springer. 2014, pp. 818\u2013833. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20M.D.%20Fergus%2C%20R.%20%E2%80%9CVisualizing%20and%20understanding%20convolutional%20networks%E2%80%9D.%20In%3A%20European%20conference%20on%20computer%20vision%202014"
        }
    ]
}
