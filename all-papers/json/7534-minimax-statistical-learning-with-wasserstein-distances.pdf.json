{
    "filename": "7534-minimax-statistical-learning-with-wasserstein-distances.pdf",
    "metadata": {
        "date": 2018,
        "title": "Minimax statistical learning with Wasserstein distances",
        "author": "Jaeho Lee Maxim Raginsky {jlee620, maxim}@illinois.edu\u21e4",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7534-minimax-statistical-learning-with-wasserstein-distances.pdf"
        },
        "abstract": "As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove generalization bounds that involve the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for transport-based domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples."
    },
    "keywords": [
        {
            "term": "wasserstein distance",
            "url": "https://en.wikipedia.org/wiki/wasserstein_distance"
        },
        {
            "term": "hypothesis",
            "url": "https://en.wikipedia.org/wiki/hypothesis"
        },
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        },
        {
            "term": "statistical learning",
            "url": "https://en.wikipedia.org/wiki/statistical_learning"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "robust optimization",
            "url": "https://en.wikipedia.org/wiki/robust_optimization"
        }
    ],
    "highlights": [
        "In the traditional paradigm of statistical learning [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z ! R+",
        "Each f 2 F quantifies the loss of some decision rule or a hypothesis applied to instances z 2 Z, so, with a slight abuse of terminology, we will refer to F as the hypothesis space",
        "In Section 3, we show that the hypothesis learned with the Empirical Risk Minimization (ERM) procedure based on the local minimax risk closely achieves the optimal local minimax risk",
        "We provide a data-dependent bound on the generalization error, which behaves like the bound for ordinary Empirical Risk Minimization in the no-ambiguity regime (Theorem 1), and excess risk bounds under uniform smoothness assumptions on F (Theorem 2) and a less restrictive assumption that F contains at least one smooth hypothesis (Theorem 3)",
        "In Section 4, we provide an alternative perspective on domain adaptation based on the minimax statistical learning under the framework of Courty et al [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], where the domain drift is due to an unknown transformation of the feature space that preserves the conditional distribution of the labels given the features",
        "On the other hand, aims to provide an excess risk bound for a specific target hypothesis fb given by the solution of a minimax Empirical Risk Minimization"
    ],
    "key_statements": [
        "In the traditional paradigm of statistical learning [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z ! R+",
        "Each f 2 F quantifies the loss of some decision rule or a hypothesis applied to instances z 2 Z, so, with a slight abuse of terminology, we will refer to F as the hypothesis space",
        "Given an n-tuple Z1, . . . , Zn of i.i.d. training examples drawn from an unknown P 2 P, the objective is to find a hypothesis fb 2 F whose risk R(P, fb) is close to the minimum risk",
        "The rest of this paper is organized as follows: In Section 2, we formally present the notion of local minimax risk and discuss its relationship to the statistical risk, which allows us to assess the performance of minimax-optimal hypothesis in specific domains",
        "In Section 3, we show that the hypothesis learned with the Empirical Risk Minimization (ERM) procedure based on the local minimax risk closely achieves the optimal local minimax risk",
        "We provide a data-dependent bound on the generalization error, which behaves like the bound for ordinary Empirical Risk Minimization in the no-ambiguity regime (Theorem 1), and excess risk bounds under uniform smoothness assumptions on F (Theorem 2) and a less restrictive assumption that F contains at least one smooth hypothesis (Theorem 3)",
        "In Section 4, we provide an alternative perspective on domain adaptation based on the minimax statistical learning under the framework of Courty et al [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], where the domain drift is due to an unknown transformation of the feature space that preserves the conditional distribution of the labels given the features",
        "Bypassing the estimation of the transport map, we provide a proper excess risk bound that compares the risk of the learned hypothesis to the minimal risk achievable within the given hypothesis class on the target domain (Theorem 4)",
        "The illustrative example presented in Section 2.2 implies that the minimax learning might be possible even when the functions in F are not uniformly Lipschitz, but there exists at least one smooth hypothesis",
        "On the other hand, aims to provide an excess risk bound for a specific target hypothesis fb given by the solution of a minimax Empirical Risk Minimization",
        "We assume that domain drift is due to an unknown transformation T : X ! X of the feature space that preserves the conditional distribution of the labels given the features, e.g. acquisition condition, sensor drift, thermal noise, etc",
        "We propose the following domain adaptation scheme: 1"
    ],
    "summary": [
        "In the traditional paradigm of statistical learning [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z ! R+.",
        "In Section 4, we provide an alternative perspective on domain adaptation based on the minimax statistical learning under the framework of Courty et al [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], where the domain drift is due to an unknown transformation of the feature space that preserves the conditional distribution of the labels given the features.",
        "Bypassing the estimation of the transport map, we provide a proper excess risk bound that compares the risk of the learned hypothesis to the minimal risk achievable within the given hypothesis class on the target domain (Theorem 4).",
        "The benefits of using the entropy integral C(F) instead of usual complexity measures such as Rademacher or Gaussian complexity [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] are twofold: (1) C(F) takes into account the behavior of hypotheses outside the support of the data-generating distribution P , and can be applied for the assessment of local worst-case risk; (2) Rademacher complexity of ' ,f can be upper-bounded naturally via C(F) and the covering number of a suitable bounded subset of [0, 1).",
        "The illustrative example presented in Section 2.2 implies that the minimax learning might be possible even when the functions in F are not uniformly Lipschitz, but there exists at least one smooth hypothesis.",
        "Consider again the setting of regression with quadratic loss as in Proposition 3; the functions f 2 F are of the form f (z) = f (x, y) = (y h(x))2, where h runs over some given class of candidate predictors that contains constants.",
        "Note that the excess risk bound of [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] shows the same behavior as Theorem 3, where in that case % is the slack in the moment constraints defining the ambiguity set.",
        "While these generalization bounds provide a uniform guarantee for all predictors in a class, they can be considered \u2018pessimistic\u2019 in the sense that we compare the excess risk to R(P, h), which is the performance of some selected predictor at the source domain.",
        "On the other hand, aims to provide an excess risk bound for a specific target hypothesis fb given by the solution of a minimax ERM.",
        "As we show below, we can provide a generalization bound for the target domain by combining estimation guarantees for Wp(P, Q) with risk inequalities of Section 2.",
        "Suppose that the feature space X is a bounded subset of Rd with d > 2p, take dX(x, x0) = kx x0kp, and let F be a family of hypotheses with Lipschitz constant at most L."
    ],
    "headline": "We describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] L. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient Flows in Metric Spaces and in the Space of Probability Measures. Birkh\u00e4user, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ambrosio%2C%20L.%20Gigli%2C%20N.%20Savar%C3%A9%2C%20G.%20Gradient%20Flows%20in%20Metric%20Spaces%20and%20in%20the%20Space%20of%20Probability%20Measures%202008"
        },
        {
            "id": "2",
            "entry": "[2] Peter Bartlett and John Shawe-Taylor. Generalization performance of support vector machines and other pattern classifiers. Advances in Kernel methods\u2014support vector learning, pages 43\u201354, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20Shawe-Taylor%2C%20John%20Generalization%20performance%20of%20support%20vector%20machines%20and%20other%20pattern%20classifiers%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20Shawe-Taylor%2C%20John%20Generalization%20performance%20of%20support%20vector%20machines%20and%20other%20pattern%20classifiers%201999"
        },
        {
            "id": "3",
            "entry": "[3] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "4",
            "entry": "[4] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. Machine Learning, 79:151\u2013175, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20S.%20Blitzer%2C%20J.%20Crammer%2C%20K.%20Kulesza%2C%20A.%20A%20theory%20of%20learning%20from%20different%20domains%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-David%2C%20S.%20Blitzer%2C%20J.%20Crammer%2C%20K.%20Kulesza%2C%20A.%20A%20theory%20of%20learning%20from%20different%20domains%202010"
        },
        {
            "id": "5",
            "entry": "[5] J. Blanchet, Y. Kang, and K. Murthy. Robust wasserstein profile inference and applications to machine learning. arXiv preprint 1610.05627v2, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05627v2"
        },
        {
            "id": "6",
            "entry": "[6] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9):1853\u20131865, September 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courty%2C%20N.%20Flamary%2C%20R.%20Tuia%2C%20D.%20Rakotomamonjy%2C%20A.%20Optimal%20transport%20for%20domain%20adaptation%202017-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courty%2C%20N.%20Flamary%2C%20R.%20Tuia%2C%20D.%20Rakotomamonjy%2C%20A.%20Optimal%20transport%20for%20domain%20adaptation%202017-09"
        },
        {
            "id": "7",
            "entry": "[7] F. Cucker and D. Zhou. Learning theory: an approximation theory viewpoint. Cambridge University Press, Cambridge, MA, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cucker%2C%20F.%20Zhou%2C%20D.%20Learning%20theory%3A%20an%20approximation%20theory%20viewpoint%202007"
        },
        {
            "id": "8",
            "entry": "[8] J. C. Duchi, P. W. Glynn, and H. Namkoong. Statistics of robust optimization: a generalized empirical likelihood approach. arXiv preprint 1610.03425, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.03425"
        },
        {
            "id": "9",
            "entry": "[9] F. Farnia and D. Tse. A minimax approach to supervised learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4240\u20134248. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farnia%2C%20F.%20Tse%2C%20D.%20A%20minimax%20approach%20to%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farnia%2C%20F.%20Tse%2C%20D.%20A%20minimax%20approach%20to%20supervised%20learning%202016"
        },
        {
            "id": "10",
            "entry": "[10] N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162(3\u20134):707\u2013738, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fournier%2C%20N.%20Guillin%2C%20A.%20On%20the%20rate%20of%20convergence%20in%20Wasserstein%20distance%20of%20the%20empirical%20measure%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fournier%2C%20N.%20Guillin%2C%20A.%20On%20the%20rate%20of%20convergence%20in%20Wasserstein%20distance%20of%20the%20empirical%20measure%202015"
        },
        {
            "id": "11",
            "entry": "[11] R. Gao and A. J. Kleywegt. Distributionally robust stochastic optimization with Wasserstein distance. arXiv preprint 1604.02199, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.02199"
        },
        {
            "id": "12",
            "entry": "[12] I. J. Goodfellow, J. Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint 1412.6572v3, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572v3"
        },
        {
            "id": "13",
            "entry": "[13] V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems, volume 2033 of Lecture Notes in Mathematics. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20V.%20Oracle%20Inequalities%20in%20Empirical%20Risk%20Minimization%20and%20Sparse%20Recovery%20Problems%2C%20volume%202033%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koltchinskii%2C%20V.%20Oracle%20Inequalities%20in%20Empirical%20Risk%20Minimization%20and%20Sparse%20Recovery%20Problems%2C%20volume%202033%202011"
        },
        {
            "id": "14",
            "entry": "[14] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30(1):1\u201350, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20Vladimir%20Panchenko%2C%20Dmitry%20Empirical%20margin%20distributions%20and%20bounding%20the%20generalization%20error%20of%20combined%20classifiers%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koltchinskii%2C%20Vladimir%20Panchenko%2C%20Dmitry%20Empirical%20margin%20distributions%20and%20bounding%20the%20generalization%20error%20of%20combined%20classifiers%202002"
        },
        {
            "id": "15",
            "entry": "[15] Y. Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In Sanjoy Dasgupta and Adam Klivans, editors, Proceedings of 22nd Annual Conference on Learning Theory. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Y.%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%3A%20Learning%20bounds%20and%20algorithms%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Y.%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%3A%20Learning%20bounds%20and%20algorithms%202009"
        },
        {
            "id": "16",
            "entry": "[16] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations. Mathematical Programming, 171(1):115\u2013166, Sep 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esfahani%2C%20Peyman%20Mohajerin%20Kuhn%2C%20Daniel%20Data-driven%20distributionally%20robust%20optimization%20using%20the%20Wasserstein%20metric%3A%20performance%20guarantees%20and%20tractable%20reformulations%202018-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esfahani%2C%20Peyman%20Mohajerin%20Kuhn%2C%20Daniel%20Data-driven%20distributionally%20robust%20optimization%20using%20the%20Wasserstein%20metric%3A%20performance%20guarantees%20and%20tractable%20reformulations%202018-09"
        },
        {
            "id": "17",
            "entry": "[17] S. Shafieezadeh-Abadeh, P. Mohajerin Esfahani, and D. Kuhn. Distributionally robust logistic regression. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1576\u20131584. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shafieezadeh-Abadeh%2C%20S.%20Esfahani%2C%20P.Mohajerin%20Kuhn%2C%20D.%20Distributionally%20robust%20logistic%20regression%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shafieezadeh-Abadeh%2C%20S.%20Esfahani%2C%20P.Mohajerin%20Kuhn%2C%20D.%20Distributionally%20robust%20logistic%20regression%202015"
        },
        {
            "id": "18",
            "entry": "[18] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifying%20some%20distributional%20robustness%20with%20principled%20adversarial%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifying%20some%20distributional%20robustness%20with%20principled%20adversarial%20training%202018"
        },
        {
            "id": "19",
            "entry": "[19] M. Talagrand. Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems. Springer, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20M.%20Upper%20and%20Lower%20Bounds%20for%20Stochastic%20Processes%3A%20Modern%20Methods%20and%20Classical%20Problems%202014"
        },
        {
            "id": "20",
            "entry": "[20] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20V.N.%20Statistical%20Learning%20Theory%201998"
        },
        {
            "id": "21",
            "entry": "[21] C. Villani. Topics in optimal transportation. American Mathematics Society, Providence, RI, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C.%20Topics%20in%20optimal%20transportation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villani%2C%20C.%20Topics%20in%20optimal%20transportation%202003"
        }
    ]
}
