{
    "filename": "7976-adaptive-learning-with-unknown-information-flows.pdf",
    "metadata": {
        "title": "Adaptive Learning with Unknown Information Flows",
        "author": "Yonatan Gur, Ahmadreza Momeni",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7976-adaptive-learning-with-unknown-information-flows.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "An agent facing sequential decisions that are characterized by partial feedback needs to strike a balance between maximizing immediate payoffs based on available information, and acquiring new information that may be essential for maximizing future payoffs. This trade-off is captured by the multi-armed bandit (MAB) framework that has been studied and applied when at each time epoch payoff observations are collected on the actions that are selected at that epoch. In this paper we introduce a new, generalized MAB formulation in which additional information on each arm may appear arbitrarily throughout the decision horizon, and study the impact of such information flows on the achievable performance and the design of efficient decision-making policies. By obtaining matching lower and upper bounds, we characterize the (regret) complexity of this family of MAB problems as a function of the information flows. We introduce an adaptive exploration policy that, without any prior knowledge of the information arrival process, attains the best performance (in terms of regret rate) that is achievable when the information arrival process is a priori known. Our policy uses dynamically customized virtual time indexes to endogenously control the exploration rate based on the realized information arrival process."
    },
    "keywords": [
        {
            "term": "multi-armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi-armed_bandit"
        }
    ],
    "highlights": [
        "We introduce an adaptive exploration policy that, without any prior knowledge of the auxiliary information flows, approximates the best performance that is achievable when the information arrival process is known in advance",
        "A well-studied framework that captures this trade-off between new information acquisition, and optimizing payoffs based on available information is the one of multi-armed bandits (MAB) that first emerged in [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] in the context of drug testing, and was later extended by [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] to a more general setting",
        "We identify conditions on the information arrival process that guarantee the optimality of myopic policies, and further identify adaptive multi-armed bandit policies that guarantee the \u201cbest of all worlds\" in the sense of establishing optimal performance without prior knowledge of the information collection process",
        "We introduce a new, generalized multi-armed bandit formulation that relaxes strong assumptions classical multi-armed bandit settings typically make on the information collection process",
        "We identify conditions on the information arrival process that guarantee the optimality of myopic policies, and further identify adaptive multi-armed bandit policies that guarantee optimal performance (\u201cbest of all worlds\") without prior knowledge on the information arrival process",
        "In this study we considered a generalization of the stationary multi-armed bandits problem in the presence of unknown and arbitrary information flows on each arm"
    ],
    "key_statements": [
        "We introduce an adaptive exploration policy that, without any prior knowledge of the auxiliary information flows, approximates the best performance that is achievable when the information arrival process is known in advance",
        "A well-studied framework that captures this trade-off between new information acquisition, and optimizing payoffs based on available information is the one of multi-armed bandits (MAB) that first emerged in [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] in the context of drug testing, and was later extended by [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] to a more general setting",
        "We identify conditions on the information arrival process that guarantee the optimality of myopic policies, and further identify adaptive multi-armed bandit policies that guarantee the \u201cbest of all worlds\" in the sense of establishing optimal performance without prior knowledge of the information collection process",
        "We introduce a new, generalized multi-armed bandit formulation that relaxes strong assumptions classical multi-armed bandit settings typically make on the information collection process",
        "We introduce an adaptive exploration policy that, without any prior knowledge of the auxiliary information flows, approximates the best performance that is achievable when the information arrival process is known in advance",
        "We identify conditions on the information arrival process that guarantee the optimality of myopic policies, and further identify adaptive multi-armed bandit policies that guarantee optimal performance (\u201cbest of all worlds\") without prior knowledge on the information arrival process",
        "One of the challenges we address is to design a policy that adapts to unknown problem characteristics, in the sense of achieving ex-post performance that is as good as the one achievable under ex-ante knowledge of the information arrival process",
        "For the sake of simplicity, our model is based on a simple and well studied multi-armed bandit framework [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]; we note that our methods and analysis can be directly applied to more general multi-armed bandit frameworks such as the contextual multi-armed bandit framework where mean rewards are linearly dependent on context vectors; see, e.g., [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and references therein",
        "In this study we considered a generalization of the stationary multi-armed bandits problem in the presence of unknown and arbitrary information flows on each arm"
    ],
    "summary": [
        "We introduce an adaptive exploration policy that, without any prior knowledge of the auxiliary information flows, approximates the best performance that is achievable when the information arrival process is known in advance.",
        "Theorem 1 further establishes that in the presence of auxiliary information flows regret rates may be lower relative to classical regret rates, and that the impact of information arrivals on the achievable performance depends on the frequency of these arrivals and on the time at which these arrivals occur; We discuss these observations in \u00a73.1.",
        "As long as there are no more than order \u2206\u22122 information arrivals over T time periods, this information does not impact achievable regret rates.3 When \u2206 is fixed and independent of the horizon l\u221aength T , the lower bound scales logarithmically with T .",
        "The -greedy policy, employs an exploration rate that is independent of the number of observations obtained for each arms and effectively incurs regret of order log T due to performing unnecessary exploration.",
        "We provide a simple and deterministic adaptive exploration policy that includes the key elements that are essential for appropriately adjusting the exploration rate and achieving good performance in the presence of auxiliary information flows.",
        "In the absence of information flows, an exploration rate of order 1/t guarantees that the arm with the highest estimated mean reward can be suboptimal only with a probability of order 1/t; see, e.g., the analysis of the -greedy policy in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], where at each time period t exploration occurs uniformly at random with probability 1/t.",
        "The adaptive exploration policy explores arm k at a rate that would have been appropriate without auxiliary information flows at a future time step \u03c4k,t.",
        "The general idea of adapting the exploration rate of a policy by advancing a virtual time index as a function of the information arrival process is illustrated in Figure 1.",
        "To bound the regret at exploitation time periods we use Chernoff-Hoeffding inequality to bound the probability that a sub-optimal arm has the highest estimated reward, given the minimal number of observations that must be collected on each arm.",
        "The upper bound in Theorem 2 holds for any arbitrary sample path of information arrivals that is captured by the matrix H, and matches the lower bound in Theorem 1 with respect to dependence on the time horizon T , as well as the sample path of information arrivals \u03b7k,t\u2019s, the number of arms K, and the minimum expected reward difference \u2206.",
        "We established that using this policy, one may guarantee the best performance that is achievable under prior knowledge on the information arrival process"
    ],
    "headline": "In this paper we introduce a new, generalized multi-armed bandit formulation in which additional information on each arm may appear arbitrarily throughout the decision horizon, and study the impact of such information flows on the achievable performance and the design of efficient decision-making policies",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Agrawal, S. and N. Goyal (2013). Further optimal regret bounds for thompson sampling. In Artificial Intelligence and Statistics, pp. 99\u2013107.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20S.%20N.%20Further%20optimal%20regret%20bounds%20for%20thompson%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20S.%20N.%20Further%20optimal%20regret%20bounds%20for%20thompson%20sampling%202013"
        },
        {
            "id": "2",
            "entry": "[2] Audibert, J.-Y. and S. Bubeck (2010). Best arm identification in multi-armed bandits. In COLT-23th Conference on Learning Theory-2010, pp. 13\u2013p.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.-Y.%20S.%20Best%20arm%20identification%20in%20multi-armed%20bandits%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.-Y.%20S.%20Best%20arm%20identification%20in%20multi-armed%20bandits%202010"
        },
        {
            "id": "3",
            "entry": "[3] Auer, P., N. Cesa-Bianchi, and P. Fischer (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning 47(2-3), 235\u2013256.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20P.%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20P.%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "4",
            "entry": "[4] Auer, P., N. Cesa-Bianchi, Y. Freund, and R. E. Schapire (1995). Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pp. 322\u2013331. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20Gambling%20in%20a%20rigged%20casino%3A%20The%20adversarial%20multi-armed%20bandit%20problem%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20Gambling%20in%20a%20rigged%20casino%3A%20The%20adversarial%20multi-armed%20bandit%20problem%201995"
        },
        {
            "id": "5",
            "entry": "[5] Bastani, H. and M. Bayati (2015). Online decision-making with high-dimensional covariates. Preprint, available at SSRN: http://ssrn.com/abstract=2661896.",
            "url": "http://ssrn.com/abstract=2661896",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bastani%2C%20H.%20M.%20Online%20decision-making%20with%20high-dimensional%20covariates%202015"
        },
        {
            "id": "6",
            "entry": "[6] Bastani, H., M. Bayati, and K. Khosravi (2017). Exploiting the natural exploration in contextual bandits. arXiv preprint arXiv:1704.09011.",
            "arxiv_url": "https://arxiv.org/pdf/1704.09011"
        },
        {
            "id": "7",
            "entry": "[7] Besbes, O., Y. Gur, and A. Zeevi (2014). Stochastic multi-armed-bandit problem with non-stationary rewards. Advances in Neural Information Processing Systems 27, 199\u2013207.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Besbes%2C%20O.%20Gur%2C%20Y.%20A.%20Stochastic%20multi-armed-bandit%20problem%20with%20non-stationary%20rewards%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Besbes%2C%20O.%20Gur%2C%20Y.%20A.%20Stochastic%20multi-armed-bandit%20problem%20with%20non-stationary%20rewards%202014"
        },
        {
            "id": "8",
            "entry": "[8] Bubeck, S., V. Perchet, and P. Rigollet (2013). Bounded regret in stochastic multi-armed bandits. In Conference on Learning Theory, pp. 122\u2013134.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Perchet%2C%20V.%20P.%20Bounded%20regret%20in%20stochastic%20multi-armed%20bandits%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Perchet%2C%20V.%20P.%20Bounded%20regret%20in%20stochastic%20multi-armed%20bandits%202013"
        },
        {
            "id": "9",
            "entry": "[9] Freund, Y. and R. E. Schapire (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences 55(1), 119\u2013139.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Y.%20E%2C%20R.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Y.%20E%2C%20R.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997"
        },
        {
            "id": "10",
            "entry": "[10] Goldenshluger, A. and A. Zeevi (2013). A linear response bandit problem. Stochastic Systems 3(1), 230\u2013261.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldenshluger%2C%20A.%20A.%20A%20linear%20response%20bandit%20problem%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldenshluger%2C%20A.%20A.%20A%20linear%20response%20bandit%20problem%202013"
        },
        {
            "id": "11",
            "entry": "[11] Jadbabaie, A., A. Rakhlin, S. Shahrampour, and K. Sridharan (2015). Online optimization: Competing with dynamic comparators. In Artificial Intelligence and Statistics, pp. 398\u2013406.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jadbabaie%2C%20A.%20Rakhlin%2C%20A.%20Shahrampour%2C%20S.%20K.%20Online%20optimization%3A%20Competing%20with%20dynamic%20comparators%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jadbabaie%2C%20A.%20Rakhlin%2C%20A.%20Shahrampour%2C%20S.%20K.%20Online%20optimization%3A%20Competing%20with%20dynamic%20comparators%202015"
        },
        {
            "id": "12",
            "entry": "[12] Komiyama, J., I. Sato, and H. Nakagawa (2013). Multi-armed bandit problem with lock-up periods. In Asian Conference on Machine Learning, pp. 100\u2013115.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Komiyama%2C%20J.%20Sato%2C%20I.%20H.%20Multi-armed%20bandit%20problem%20with%20lock-up%20periods%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Komiyama%2C%20J.%20Sato%2C%20I.%20H.%20Multi-armed%20bandit%20problem%20with%20lock-up%20periods%202013"
        },
        {
            "id": "13",
            "entry": "[13] Lai, T. L. and H. Robbins (1985). Asymptotically efficient adaptive allocation rules. Advances in applied mathematics 6(1), 4\u201322.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20T.L.%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20T.L.%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985"
        },
        {
            "id": "14",
            "entry": "[14] Langford, J. and T. Zhang (2008). The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in neural information processing systems, pp. 817\u2013824.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20T.%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20J.%20T.%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008"
        },
        {
            "id": "15",
            "entry": "[15] Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society 58(5), 527\u2013535.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952"
        },
        {
            "id": "16",
            "entry": "[16] Sani, A., G. Neu, and A. Lazaric (2014). Exploiting easy data in online optimization. In Advances in Neural Information Processing Systems, pp. 810\u2013818.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sani%2C%20A.%20Neu%2C%20G.%20A.%20Exploiting%20easy%20data%20in%20online%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sani%2C%20A.%20Neu%2C%20G.%20A.%20Exploiting%20easy%20data%20in%20online%20optimization%202014"
        },
        {
            "id": "17",
            "entry": "[17] Seldin, Y. and A. Slivkins (2014). One practical algorithm for both stochastic and adversarial bandits. In International Conference on Machine Learning, pp. 1287\u20131295.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seldin%2C%20Y.%20A.%20One%20practical%20algorithm%20for%20both%20stochastic%20and%20adversarial%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seldin%2C%20Y.%20A.%20One%20practical%20algorithm%20for%20both%20stochastic%20and%20adversarial%20bandits%202014"
        },
        {
            "id": "18",
            "entry": "[18] Shah, V., J. Blanchet, and R. Johari (2018). Bandit learning with positive externalities. arXiv preprint arXiv:1802.05693.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05693"
        },
        {
            "id": "19",
            "entry": "[19] Strehl, A. L., C. Mesterharm, M. L. Littman, and H. Hirsh (2006). Experience-efficient learning in associative bandit problems. In Proceedings of the 23rd international conference on Machine learning, pp. 889\u2013896. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20A.L.%20Mesterharm%2C%20C.%20Littman%2C%20M.L.%20Hirsh%2C%20H.%20Experience-efficient%20learning%20in%20associative%20bandit%20problems%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20A.L.%20Mesterharm%2C%20C.%20Littman%2C%20M.L.%20Hirsh%2C%20H.%20Experience-efficient%20learning%20in%20associative%20bandit%20problems%202006"
        },
        {
            "id": "20",
            "entry": "[20] Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika 25(3/4), 285\u2013294.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "21",
            "entry": "[21] Trac\u00e0, S. and C. Rudin (2015). Regulating greed over time. arXiv preprint arXiv:1505.05629.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05629"
        },
        {
            "id": "22",
            "entry": "[22] Tsybakov, A. B. (2008). Introduction to Nonparametric Estimation (1st ed.). Springer Publishing Company, Incorporated.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20A.B.%20Introduction%20to%20Nonparametric%20Estimation%202008"
        },
        {
            "id": "23",
            "entry": "[23] Wang, C.-C., S. R. Kulkarni, and H. V. Poor (2005). Bandit problems with side observations. IEEE ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20C.-C.%20Kulkarni%2C%20S.R.%20V%2C%20H.%20Bandit%20problems%20with%20side%20observations%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20C.-C.%20Kulkarni%2C%20S.R.%20V%2C%20H.%20Bandit%20problems%20with%20side%20observations%202005"
        }
    ]
}
