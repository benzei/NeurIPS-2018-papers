{
    "filename": "7662-neural-networks-trained-to-solve-differential-equations-learn-general-representations.pdf",
    "metadata": {
        "title": "Neural Networks Trained to Solve Differential Equations Learn General Representations",
        "author": "Martin Magill, Faisal Qureshi, Hendrick de Haan",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7662-neural-networks-trained-to-solve-differential-equations-learn-general-representations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce a technique based on the singular vector canonical correlation analysis (SVCCA) for measuring the generality of neural network layers across a continuously-parametrized set of tasks. We illustrate this method by studying generality in neural networks trained to solve parametrized boundary value problems based on the Poisson partial differential equation. We find that the first hidden layers are general, and that they learn generalized coordinates over the input domain. Deeper layers are successively more specific. Next, we validate our method against an existing technique that measures layer generality using transfer learning experiments. We find excellent agreement between the two methods, and note that our method is much faster, particularly for continuously-parametrized problems. Finally, we also apply our method to networks trained on MNIST, and show it is consistent with, and complimentary to, another study of intrinsic dimensionality."
    },
    "keywords": [
        {
            "term": "differential equation",
            "url": "https://en.wikipedia.org/wiki/differential_equation"
        },
        {
            "term": "partial differential equation",
            "url": "https://en.wikipedia.org/wiki/partial_differential_equation"
        },
        {
            "term": "artificial neural network",
            "url": "https://en.wikipedia.org/wiki/artificial_neural_network"
        },
        {
            "term": "Ontario Graduate Scholarship",
            "url": "https://en.wikipedia.org/wiki/Ontario_Graduate_Scholarship"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        }
    ],
    "highlights": [
        "Generality of a neural network layer indicates that it can be used successfully in neural networks trained on a variety of tasks [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "Since we typically found that the majority of the correlations were nearly 1.0 or nearly 0.0, this singular vector canonical correlation analysis similarity roughly measures the number of significant dimensions shared by two layers",
        "We use the singular vector canonical correlation analysis to study the generality of layers in DENNs trained to solve our family of boundary value problems",
        "We train DENNs to solve the boundary value problems for a range of x values, each from four different random initializations per x value",
        "We presented a method for measuring layer generality over a continuously-parametrized set of problems using the singular vector canonical correlation analysis",
        "We studied the generality of layers in DENNs over a parametrized family of boundary value problems"
    ],
    "key_statements": [
        "Generality of a neural network layer indicates that it can be used successfully in neural networks trained on a variety of tasks [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "By relating generality to similarity, we develop a computationally efficient measure of generality that uses the singular vector canonical correlation analysis (SVCCA)",
        "We demonstrate this method by measuring layer generality in neural networks trained to solve differential equations",
        "Since we typically found that the majority of the correlations were nearly 1.0 or nearly 0.0, this singular vector canonical correlation analysis similarity roughly measures the number of significant dimensions shared by two layers",
        "We use the singular vector canonical correlation analysis to study the generality of layers in DENNs trained to solve our family of boundary value problems",
        "We train DENNs to solve the boundary value problems for a range of x values, each from four different random initializations per x value",
        "We found the first layers could be interpreted this way at any x and whether we found components by self-singular vector canonical correlation analysis or cross-singular vector canonical correlation analysis, and have included examples of this in the supplemental material",
        "We presented a method for measuring layer generality over a continuously-parametrized set of problems using the singular vector canonical correlation analysis",
        "We studied the generality of layers in DENNs over a parametrized family of boundary value problems",
        "We visualized the general components identified in the first layers and interpreted them as generalized coordinates capturing features of interest in the input domain"
    ],
    "summary": [
        "Generality of a neural network layer indicates that it can be used successfully in neural networks trained on a variety of tasks [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>].",
        "We demonstrate this method by measuring layer generality in neural networks trained to solve differential equations.",
        "Yosinski et al [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] developed an experimental protocol for quantifying the generality of neural network layers using transfer learning experiments.",
        "We are interested in studying layer generality across a continuously parametrized set of tasks, and the transfer learning methodology is prohibitively computationally expensive.",
        "We define a layer to be general across some group of tasks if similar layers are consistently learned by networks trained on any of those tasks.",
        "Figure 1 shows the SVCCA similarities computed between the first, third, and fourth hidden layers of networks of width 20.",
        "Correspond to all similarities computed between lth layers from networks trained on x values that differ by \u2206x , which we will denote \u03c1\u2206l x .",
        "The third and fourth layers in Figure 2 have a gap of roughly 3 out of 20 between \u03c1\u2206l x =0 and \u03c1lself : networks from different random seeds at the same x value are consistently dissimilar in about 15% of their canonical components.",
        "We can see in Figure 2 that, for networks of width 20, the first and second layers appear to be general, whereas the third and fourth are progressively more specific.",
        "As discussed in the supplemental material, very wide networks seemed to experience very broad minima in the loss landscape, so our training protocol may have terminated before the layers converged to optimal and general representations.",
        "Figure 4 shows a visualization of the first 9 principal components of the first layer of a network of width 192 trained at x = 0.6, shown as contour maps.",
        "The first and second layers have transfer specificities of roughly 5, and are general at all widths.",
        "Future work should try measuring co-adaptation using the SVCCA, perhaps by measuring the similarities of different layers within the same network, as done by Raghu et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "We presented a method for measuring layer generality over a continuously-parametrized set of problems using the SVCCA.",
        "We studied the generality of layers in DENNs over a parametrized family of BVPs. We found that the first layer is general; the second is somewhat less so; the third is general in wide networks but specific in narrow ones; and the fourth is specific for widths up to 192.",
        "The two are distinct but complimentary, and produce consistent results for networks trained on the MNIST dataset [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]"
    ],
    "headline": "We introduce a technique based on the singular vector canonical correlation analysis  for measuring the generality of neural network layers across a continuously-parametrized set of tasks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "2",
            "entry": "[2] Jens Berg and Kaj Nystr\u00f6m. A unified deep artificial neural network approach to partial differential equations in complex geometries. arXiv preprint arXiv:1711.06464, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06464"
        },
        {
            "id": "3",
            "entry": "[3] M. W. M. G. Dissanayake and N. Phan-Thien. Neural-network-based approximations for solving partial differential equations. Communications in Numerical Methods in Engineering, 10(3):195\u2013201, 1994. doi: 10.1002/cnm.1640100303. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.1640100303.",
            "crossref": "https://dx.doi.org/10.1002/cnm.1640100303",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1002/cnm.1640100303"
        },
        {
            "id": "4",
            "entry": "[4] Philipp Grohs, Fabian Hornung, Arnulf Jentzen, and Philippe von Wurstemberger. A proof that artificial neural networks overcome the curse of dimensionality in the numerical approximation of Black-Scholes partial differential equations. arXiv preprint arXiv:1809.02362, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.02362"
        },
        {
            "id": "5",
            "entry": "[5] Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505\u20138510, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Jiequn%20Jentzen%2C%20Arnulf%20Weinan%2C%20E.%20Solving%20high-dimensional%20partial%20differential%20equations%20using%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Jiequn%20Jentzen%2C%20Arnulf%20Weinan%2C%20E.%20Solving%20high-dimensional%20partial%20differential%20equations%20using%20deep%20learning%202018"
        },
        {
            "id": "6",
            "entry": "[6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%202012"
        },
        {
            "id": "7",
            "entry": "[7] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE Transactions on Neural Networks, 9(5):987\u20131000, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lagaris%2C%20Isaac%20E.%20Aristidis%20Likas%2C%20and%20Dimitrios%20I%20Fotiadis.%20Artificial%20neural%20networks%20for%20solving%20ordinary%20and%20partial%20differential%20equations%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lagaris%2C%20Isaac%20E.%20Aristidis%20Likas%2C%20and%20Dimitrios%20I%20Fotiadis.%20Artificial%20neural%20networks%20for%20solving%20ordinary%20and%20partial%20differential%20equations%201998"
        },
        {
            "id": "8",
            "entry": "[8] Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Y Ng. ICA with reconstruction cost for efficient overcomplete feature learning. In Advances in neural information processing systems, pages 1017\u20131025, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20V.%20Karpenko%2C%20Alexandre%20Ngiam%2C%20Jiquan%20and%20Andrew%20Y%20Ng.%20ICA%20with%20reconstruction%20cost%20for%20efficient%20overcomplete%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20V.%20Karpenko%2C%20Alexandre%20Ngiam%2C%20Jiquan%20and%20Andrew%20Y%20Ng.%20ICA%20with%20reconstruction%20cost%20for%20efficient%20overcomplete%20feature%20learning%202011"
        },
        {
            "id": "9",
            "entry": "[9] Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "10",
            "entry": "[10] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th annual international conference on machine learning, pages 609\u2013616. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrew%20Y%20Ng.%20Convolutional%20deep%20belief%20networks%20for%20scalable%20unsupervised%20learning%20of%20hierarchical%20representations%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrew%20Y%20Ng.%20Convolutional%20deep%20belief%20networks%20for%20scalable%20unsupervised%20learning%20of%20hierarchical%20representations%202009"
        },
        {
            "id": "11",
            "entry": "[11] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv preprint arXiv:1804.08838, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08838"
        },
        {
            "id": "12",
            "entry": "[12] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do different neural networks learn the same representations? In Feature Extraction: Modern Questions and Challenges, pages 196\u2013212, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202015"
        },
        {
            "id": "13",
            "entry": "[13] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Sinno%20Jialin%20Pan%20and%20Qiang%20Yang.%20A%20survey%20on%20transfer%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Sinno%20Jialin%20Pan%20and%20Qiang%20Yang.%20A%20survey%20on%20transfer%20learning%202010"
        },
        {
            "id": "14",
            "entry": "[14] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. In Advances in Neural Information Processing Systems, pages 6078\u20136087, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20Maithra%20Gilmer%2C%20Justin%20Yosinski%2C%20Jason%20Sohl-Dickstein%2C%20Jascha%20SVCCA%3A%20Singular%20Vector%20Canonical%20Correlation%20Analysis%20for%20Deep%20Learning%20Dynamics%20and%20Interpretability%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Gilmer%2C%20Justin%20Yosinski%2C%20Jason%20Sohl-Dickstein%2C%20Jascha%20SVCCA%3A%20Singular%20Vector%20Canonical%20Correlation%20Analysis%20for%20Deep%20Learning%20Dynamics%20and%20Interpretability%202017"
        },
        {
            "id": "15",
            "entry": "[15] J\u00fcrgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85\u2013117, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J%C3%BCrgen%20Deep%20learning%20in%20neural%20networks%3A%20An%20overview%202015"
        },
        {
            "id": "16",
            "entry": "[16] Justin Sirignano and Konstantinos Spiliopoulos. DGM: A deep learning algorithm for solving partial differential equations. arXiv preprint arXiv:1708.07469, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07469"
        },
        {
            "id": "17",
            "entry": "[17] B Ph van Milligen, V Tribaldos, and J A Jim\u00e9nez. Neural Network Differential Equation and Plasma Equilibrium Solver. Physical Review Letters, 75(20):3594, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Milligen%2C%20B.Ph%20Tribaldos%2C%20V.%20Jim%C3%A9nez%2C%20J.A.%20Neural%20Network%20Differential%20Equation%20and%20Plasma%20Equilibrium%20Solver%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Milligen%2C%20B.Ph%20Tribaldos%2C%20V.%20Jim%C3%A9nez%2C%20J.A.%20Neural%20Network%20Differential%20Equation%20and%20Plasma%20Equilibrium%20Solver%201995"
        },
        {
            "id": "18",
            "entry": "[18] Neha Yadav, Anupam Yadav, and Manoj Kumar. An introduction to neural network methods for differential equations. Springer, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yadav%2C%20Neha%20Yadav%2C%20Anupam%20Kumar%2C%20Manoj%20An%20introduction%20to%20neural%20network%20methods%20for%20differential%20equations%202015"
        },
        {
            "id": "19",
            "entry": "[19] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        }
    ]
}
