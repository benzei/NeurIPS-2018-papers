{
    "filename": "7574-q-learning-with-nearest-neighbors.pdf",
    "metadata": {
        "date": 2018,
        "title": "Q-learning with Nearest Neighbors",
        "author": "Devavrat Shah Massachusetts Institute of Technology devavrat@mit.edu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7574-q-learning-with-nearest-neighbors.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available. We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a d-dimensional state space and the discounted factor 2 (0, 1), given an arbitrary sample path with \u201ccovering time\u201d L, we establish that the algorithm is guaranteed to output an \"-accurate estimate of the optimal Q-function using Oe L/(\"3(1 )7) samples. For instance, for a wellbehaved MDP, the covering time of the sample path under the purely random policy scales as Oe 1/\"d , so the sample complexity scales as Oe 1/\"d+3 . Indeed, we establish a lower bound that argues that the dependence of \u2326e 1/\"d+2 is necessary."
    },
    "keywords": [
        {
            "term": "Markov Decision Processes",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Processes"
        },
        {
            "term": "q learning",
            "url": "https://en.wikipedia.org/wiki/q_learning"
        },
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "dynamic programming",
            "url": "https://en.wikipedia.org/wiki/dynamic_programming"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "Markov Decision Processes (MDPs) are natural models for a wide variety of sequential decisionmaking problems",
        "We provide finite sample analysis of nearest-neighbor Q-learning for a single, arbitrary sequence of data for any infinite-horizon discounted-reward Markov Decision Processes with continuous state space",
        "The algorithm is based on constructing a finite-state discretization of the original Markov Decision Processes, and combining Q-learning with nearest neighbor regression to estimate the Q-values over the discretized state space, which is interpolated and extended to the original continuous state space",
        "We considered the reinforcement learning problem for infinite-horizon discounted Markov Decision Processes with a continuous state space",
        "We focused on a reinforcement learning algorithm nearest-neighbor Q-learning that is based on kernelized nearest neighbor regression",
        "We established nearly tight finite-sample convergence guarantees showing that nearest-neighbor Q-learning can accurately estimate optimal Q function using nearly optimal number of samples"
    ],
    "key_statements": [
        "Markov Decision Processes (MDPs) are natural models for a wide variety of sequential decisionmaking problems",
        "We provide finite sample analysis of nearest-neighbor Q-learning for a single, arbitrary sequence of data for any infinite-horizon discounted-reward Markov Decision Processes with continuous state space",
        "We consider a samplebased approach which learns the optimal value functions/policies by directly observing data generated by the Markov Decision Processes",
        "We introduce necessary notations, definitions for the framework of Markov Decision Processes that will be used throughout the paper",
        "The algorithm is based on constructing a finite-state discretization of the original Markov Decision Processes, and combining Q-learning with nearest neighbor regression to estimate the Q-values over the discretized state space, which is interpolated and extended to the original continuous state space",
        "We focus on nonparametric regression operators that can be written as nearest neighbors averaging in terms of the data q of the form",
        "In Section C we describe three representative choices that correspond to k-Nearest Neighbor Regression, Fixed-Radius Near Neighbor Regression and Kernel Regression.\n3.3",
        "Following the approach taken by [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] and [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], we introduce the notion of the covering time of Markov Decision Processes under a policy \u21e1",
        "Define \u2327\u21e1,h(x, t), the covering time of the Markov Decision Processes under the policy \u21e1, as the minimum number of steps required to visit all ball-action pairs starting from state x 2 X at time-step t 0",
        "We focus on a class of Markov Decision Processes satisfying a form of the uniform ergodic assumptions, and show that the standard \"-greedy policy has a small covering time",
        "We describe the nearest-neighbor Q-learning (NNQL) policy",
        "We considered the reinforcement learning problem for infinite-horizon discounted Markov Decision Processes with a continuous state space",
        "We focused on a reinforcement learning algorithm nearest-neighbor Q-learning that is based on kernelized nearest neighbor regression",
        "We established nearly tight finite-sample convergence guarantees showing that nearest-neighbor Q-learning can accurately estimate optimal Q function using nearly optimal number of samples"
    ],
    "summary": [
        "Markov Decision Processes (MDPs) are natural models for a wide variety of sequential decisionmaking problems.",
        "We provide finite sample analysis of NNQL for a single, arbitrary sequence of data for any infinite-horizon discounted-reward MDPs with continuous state space.",
        "We establish effectively matching lower bound stating that for any policy to learn optimal Q function within \" approximation, the number of samples required must scale as \u2326e(1/\"d+2).",
        "We consider a samplebased approach which learns the optimal value functions/policies by directly observing data generated by the MDP.",
        "The y number Q\u21e1(x, a) is called the Q-value of the pair (x, a), which is the return of initially performing action a at state s and following policy \u21e1.",
        "The algorithm is based on constructing a finite-state discretization of the original MDP, and combining Q-learning with nearest neighbor regression to estimate the Q-values over the discretized state space, which is interpolated and extended to the original continuous state space.",
        "Define \u2327\u21e1,h(x, t), the covering time of the MDP under the policy \u21e1, as the minimum number of steps required to visit all ball-action pairs starting from state x 2 X at time-step t 0.",
        "We utilize the nearest neighbor regressed Q function using the learned Q values restricted to Zh. The policy assumes access to an existing policy \u21e1 that is used to sample data points for learning.",
        "The policy keeps track of the Q-function over the finite set Zh. Specifically, let qk denote the approximate Q-values on Zh within iteration k.",
        "This update is similar to standard Q-learning updates \u2014 the Q-values are updated by taking a weighted average of qk, the previous estimate, and Gkqk, an one-step application of the Bellman operator estimated using newly observed data.",
        "Given the output qof Policy 1, we obtain an approximate Q-value for each state-action pair (x, a) 2 Z via the nearest-neighbor average operation, i.e., QT (x, a) = (",
        "Theorem 1, combined with Proposition 1, immediately yield the following bound that quantify the number of samples required to obtain an \"-optimal action-value function with high probability, if the sample path is generated per the uniformly random policy.",
        "We considered the reinforcement learning problem for infinite-horizon discounted MDPs with a continuous state space.",
        "Our results state that the sample, space and computational complexities of NNQL scale polynomially with the covering number of the state space, which is continuous and has uncountably infinite cardinality."
    ],
    "headline": "We provide tight finite sample analysis of the convergence rate",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Antos, C. Szepesv\u00e1ri, and R. Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89\u2013 129, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antos%2C%20A.%20Szepesv%C3%A1ri%2C%20C.%20Munos%2C%20R.%20Learning%20near-optimal%20policies%20with%20Bellman-residual%20minimization%20based%20fitted%20policy%20iteration%20and%20a%20single%20sample%20path%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antos%2C%20A.%20Szepesv%C3%A1ri%2C%20C.%20Munos%2C%20R.%20Learning%20near-optimal%20policies%20with%20Bellman-residual%20minimization%20based%20fitted%20policy%20iteration%20and%20a%20single%20sample%20path%202008"
        },
        {
            "id": "2",
            "entry": "[2] M. G. Azar, R. Munos, M. Ghavamzadeh, and H. J. Kappen. Reinforcement learning with a near optimal rate of convergence. Technical Report, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20M.G.%20Munos%2C%20R.%20Ghavamzadeh%2C%20M.%20Kappen%2C%20H.J.%20Reinforcement%20learning%20with%20a%20near%20optimal%20rate%20of%20convergence%202011"
        },
        {
            "id": "3",
            "entry": "[3] M. G. Azar, R. Munos, M. Ghavamzadeh, and H. J. Kappen. Speedy Q-learning. In NIPS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20M.G.%20Munos%2C%20R.%20Ghavamzadeh%2C%20M.%20Kappen%2C%20H.J.%20Speedy%20Q-learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azar%2C%20M.G.%20Munos%2C%20R.%20Ghavamzadeh%2C%20M.%20Kappen%2C%20H.J.%20Speedy%20Q-learning%202011"
        },
        {
            "id": "4",
            "entry": "[4] A. Barreto, D. Precup, and J. Pineau. Practical kernel-based reinforcement learning. The Journal of Machine Learning Research, 17(1):2372\u20132441, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barreto%2C%20A.%20Precup%2C%20D.%20Pineau%2C%20J.%20Practical%20kernel-based%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barreto%2C%20A.%20Precup%2C%20D.%20Pineau%2C%20J.%20Practical%20kernel-based%20reinforcement%20learning%202016"
        },
        {
            "id": "5",
            "entry": "[5] J. L. Bentley. Multidimensional binary search trees in database applications. IEEE Transactions on Software Engineering, (4):333\u2013340, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bentley%2C%20J.L.%20Multidimensional%20binary%20search%20trees%20in%20database%20applications%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bentley%2C%20J.L.%20Multidimensional%20binary%20search%20trees%20in%20database%20applications%201979"
        },
        {
            "id": "6",
            "entry": "[6] D. Bertsekas. Convergence of discretization procedures in dynamic programming. IEEE Transactions on Automatic Control, 20(3):415\u2013419, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.%20Convergence%20of%20discretization%20procedures%20in%20dynamic%20programming%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20D.%20Convergence%20of%20discretization%20procedures%20in%20dynamic%20programming%201975"
        },
        {
            "id": "7",
            "entry": "[7] D. P. Bertsekas. Dynamic programming and optimal control, volume II. Athena Scientific, Belmont, MA, 3rd edition, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Dynamic%20programming%20and%20optimal%20control%2C%20volume%20II.%20Athena%20Scientific%202007"
        },
        {
            "id": "8",
            "entry": "[8] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In S\u00e9bastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 1691\u20131692. PMLR, 06\u201309 Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhandari%2C%20Jalaj%20Russo%2C%20Daniel%20Singal%2C%20Raghav%20A%20finite%20time%20analysis%20of%20temporal%20difference%20learning%20with%20linear%20function%20approximation%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhandari%2C%20Jalaj%20Russo%2C%20Daniel%20Singal%2C%20Raghav%20A%20finite%20time%20analysis%20of%20temporal%20difference%20learning%20with%20linear%20function%20approximation%202018-07"
        },
        {
            "id": "9",
            "entry": "[9] N. Bhat, V. F. Farias, and C. C. Moallemi. Non-parametric approximate dynamic programming via the kernel method. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhat%2C%20N.%20Farias%2C%20V.F.%20Moallemi%2C%20C.C.%20Non-parametric%20approximate%20dynamic%20programming%20via%20the%20kernel%20method%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhat%2C%20N.%20Farias%2C%20V.F.%20Moallemi%2C%20C.C.%20Non-parametric%20approximate%20dynamic%20programming%20via%20the%20kernel%20method%202012"
        },
        {
            "id": "10",
            "entry": "[10] C.-S. Chow and J. N. Tsitsiklis. The complexity of dynamic programming. Journal of Complexity, 5(4):466\u2013488, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20C.-S.%20Tsitsiklis%2C%20J.N.%20The%20complexity%20of%20dynamic%20programming%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20C.-S.%20Tsitsiklis%2C%20J.N.%20The%20complexity%20of%20dynamic%20programming%201989"
        },
        {
            "id": "11",
            "entry": "[11] C.-S. Chow and J. N. Tsitsiklis. An optimal one-way multigrid algorithm for discrete-time stochastic control. IEEE Transactions on Automatic Control, 36(8):898\u2013914, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20C.-S.%20Tsitsiklis%2C%20J.N.%20An%20optimal%20one-way%20multigrid%20algorithm%20for%20discrete-time%20stochastic%20control%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20C.-S.%20Tsitsiklis%2C%20J.N.%20An%20optimal%20one-way%20multigrid%20algorithm%20for%20discrete-time%20stochastic%20control%201991"
        },
        {
            "id": "12",
            "entry": "[12] Gal Dalal, Bal\u00e1zs Sz\u00f6r\u00e9nyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis for TD(0) with linear function approximation. arXiv preprint arXiv:1704.01161, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01161"
        },
        {
            "id": "13",
            "entry": "[13] S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, pages 537\u2013546. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20S.%20Freund%2C%20Y.%20Random%20projection%20trees%20and%20low%20dimensional%20manifolds%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20S.%20Freund%2C%20Y.%20Random%20projection%20trees%20and%20low%20dimensional%20manifolds%202008"
        },
        {
            "id": "14",
            "entry": "[14] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u2013 1338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "15",
            "entry": "[15] F. Dufour and T. Prieto-Rumeau. Approximation of Markov decision processes with general state space. Journal of Mathematical Analysis and applications, 388(2):1254\u20131267, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dufour%2C%20F.%20Prieto-Rumeau%2C%20T.%20Approximation%20of%20Markov%20decision%20processes%20with%20general%20state%20space%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dufour%2C%20F.%20Prieto-Rumeau%2C%20T.%20Approximation%20of%20Markov%20decision%20processes%20with%20general%20state%20space%202012"
        },
        {
            "id": "16",
            "entry": "[16] F. Dufour and T. Prieto-Rumeau. Finite linear programming approximations of constrained discounted Markov decision processes. SIAM Journal on Control and Optimization, 51(2):1298\u2013 1324, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dufour%2C%20F.%20Prieto-Rumeau%2C%20T.%20Finite%20linear%20programming%20approximations%20of%20constrained%20discounted%20Markov%20decision%20processes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dufour%2C%20F.%20Prieto-Rumeau%2C%20T.%20Finite%20linear%20programming%20approximations%20of%20constrained%20discounted%20Markov%20decision%20processes%202013"
        },
        {
            "id": "17",
            "entry": "[17] F. Dufour and T. Prieto-Rumeau. Approximation of average cost Markov decision processes using empirical distributions and concentration inequalities. Stochastics: An International Journal of Probability and Stochastic Processes, 87(2):273\u2013307, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dufour%2C%20F.%20Prieto-Rumeau%2C%20T.%20Approximation%20of%20average%20cost%20Markov%20decision%20processes%20using%20empirical%20distributions%20and%20concentration%20inequalities%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dufour%2C%20F.%20Prieto-Rumeau%2C%20T.%20Approximation%20of%20average%20cost%20Markov%20decision%20processes%20using%20empirical%20distributions%20and%20concentration%20inequalities%202015"
        },
        {
            "id": "18",
            "entry": "[18] E. Even-Dar and Y. Mansour. Learning rates for Q-learning. JMLR, 5, December 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20E.%20Mansour%2C%20Y.%20Learning%20rates%20for%20Q-learning%202004-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20E.%20Mansour%2C%20Y.%20Learning%20rates%20for%20Q-learning%202004-12-05"
        },
        {
            "id": "19",
            "entry": "[19] W. B. Haskell, R. Jain, and D. Kalathil. Empirical dynamic programming. Mathematics of Operations Research, 41(2), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haskell%2C%20W.B.%20Jain%2C%20R.%20Kalathil%2C%20D.%20Empirical%20dynamic%20programming%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haskell%2C%20W.B.%20Jain%2C%20R.%20Kalathil%2C%20D.%20Empirical%20dynamic%20programming%202016"
        },
        {
            "id": "20",
            "entry": "[20] H. V. Hasselt. Double Q-learning. In NIPS. 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H%20V%20Hasselt%20Double%20Qlearning%20In%20NIPS%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H%20V%20Hasselt%20Double%20Qlearning%20In%20NIPS%202010"
        },
        {
            "id": "21",
            "entry": "[21] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, pages 604\u2013613. ACM, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Indyk%2C%20P.%20Motwani%2C%20R.%20Approximate%20nearest%20neighbors%3A%20towards%20removing%20the%20curse%20of%20dimensionality%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Indyk%2C%20P.%20Motwani%2C%20R.%20Approximate%20nearest%20neighbors%3A%20towards%20removing%20the%20curse%20of%20dimensionality%201998"
        },
        {
            "id": "22",
            "entry": "[22] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the convergence of stochastic iterative dynamic programming algorithms. Neural Comput., 6(6), 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaakkola%2C%20T.%20Jordan%2C%20M.I.%20Singh%2C%20S.P.%20On%20the%20convergence%20of%20stochastic%20iterative%20dynamic%20programming%20algorithms%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaakkola%2C%20T.%20Jordan%2C%20M.I.%20Singh%2C%20S.P.%20On%20the%20convergence%20of%20stochastic%20iterative%20dynamic%20programming%20algorithms%201994"
        },
        {
            "id": "23",
            "entry": "[23] M. Kearns and S. Singh. Finite-sample convergence rates for Q-learning and indirect algorithms. In NIPS, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20M.%20Singh%2C%20S.%20Finite-sample%20convergence%20rates%20for%20Q-learning%20and%20indirect%20algorithms%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20M.%20Singh%2C%20S.%20Finite-sample%20convergence%20rates%20for%20Q-learning%20and%20indirect%20algorithms%201999"
        },
        {
            "id": "24",
            "entry": "[24] S. H. Lim and G. DeJong. Towards finite-sample convergence of direct reinforcement learning. In Proceedings of the 16th European Conference on Machine Learning, pages 230\u2013241. Springer-Verlag, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lim%2C%20S.H.%20DeJong%2C%20G.%20Towards%20finite-sample%20convergence%20of%20direct%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lim%2C%20S.H.%20DeJong%2C%20G.%20Towards%20finite-sample%20convergence%20of%20direct%20reinforcement%20learning%202005"
        },
        {
            "id": "25",
            "entry": "[25] Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample analysis of proximal gradient TD algorithms. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pages 504\u2013513. AUAI Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Bo%20Liu%2C%20Ji%20Ghavamzadeh%2C%20Mohammad%20Mahadevan%2C%20Sridhar%20Finite-sample%20analysis%20of%20proximal%20gradient%20TD%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Bo%20Liu%2C%20Ji%20Ghavamzadeh%2C%20Mohammad%20Mahadevan%2C%20Sridhar%20Finite-sample%20analysis%20of%20proximal%20gradient%20TD%20algorithms%202015"
        },
        {
            "id": "26",
            "entry": "[26] C. Mathy, N. Derbinsky, J. Bento, J. Rosenthal, and J. S. Yedidia. The boundary forest algorithm for online supervised and unsupervised learning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 2864\u20132870, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathy%2C%20C.%20Derbinsky%2C%20N.%20Bento%2C%20J.%20Rosenthal%2C%20J.%20The%20boundary%20forest%20algorithm%20for%20online%20supervised%20and%20unsupervised%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathy%2C%20C.%20Derbinsky%2C%20N.%20Bento%2C%20J.%20Rosenthal%2C%20J.%20The%20boundary%20forest%20algorithm%20for%20online%20supervised%20and%20unsupervised%20learning%202015"
        },
        {
            "id": "27",
            "entry": "[27] F. S. Melo, S. P. Meyn, and M. I. Ribeiro. An analysis of reinforcement learning with function approximation. In Proceedings of the 25th international conference on Machine learning, pages 664\u2013671. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melo%2C%20F.S.%20Meyn%2C%20S.P.%20Ribeiro%2C%20M.I.%20An%20analysis%20of%20reinforcement%20learning%20with%20function%20approximation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melo%2C%20F.S.%20Meyn%2C%20S.P.%20Ribeiro%2C%20M.I.%20An%20analysis%20of%20reinforcement%20learning%20with%20function%20approximation%202008"
        },
        {
            "id": "28",
            "entry": "[28] Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In International Conference on Computational Learning Theory, pages 308\u2013322.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melo%2C%20Francisco%20S.%20Ribeiro%2C%20M.Isabel%20Q-learning%20with%20linear%20function%20approximation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melo%2C%20Francisco%20S.%20Ribeiro%2C%20M.Isabel%20Q-learning%20with%20linear%20function%20approximation"
        },
        {
            "id": "29",
            "entry": "[29] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K Fidjeland, and G. Ostrovski. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "30",
            "entry": "[30] R. Munos and C. Szepesv\u00e1ri. Finite-time bounds for fitted value iteration. Journal of Machine Learning Research, 9(May):815\u2013857, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R.%20Szepesv%C3%A1ri%2C%20C.%20Finite-time%20bounds%20for%20fitted%20value%20iteration%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R.%20Szepesv%C3%A1ri%2C%20C.%20Finite-time%20bounds%20for%20fitted%20value%20iteration%202008"
        },
        {
            "id": "31",
            "entry": "[31] E. A. Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):141\u2013 142, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nadaraya%2C%20E.A.%20On%20estimating%20regression.%20Theory%20of%20Probability%20%26%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nadaraya%2C%20E.A.%20On%20estimating%20regression.%20Theory%20of%20Probability%20%26%201964"
        },
        {
            "id": "32",
            "entry": "[32] D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems. IEEE Trans. Automatic Control, 47(10), 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ormoneit%2C%20D.%20Glynn%2C%20P.%20Kernel-based%20reinforcement%20learning%20in%20average-cost%20problems%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ormoneit%2C%20D.%20Glynn%2C%20P.%20Kernel-based%20reinforcement%20learning%20in%20average-cost%20problems%202002"
        },
        {
            "id": "33",
            "entry": "[33] D. Ormoneit and S . Sen. Kernel-based reinforcement learning. Mach. Learning, 49(2-3), 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ormoneit%2C%20D.%20Sen%2C%20S%20.%20Kernel-based%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ormoneit%2C%20D.%20Sen%2C%20S%20.%20Kernel-based%20reinforcement%20learning%202002"
        },
        {
            "id": "34",
            "entry": "[34] Jason Pazis and Ronald Parr. PAC optimal exploration in continuous space Markov decision processes. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, pages 774\u2013781. AAAI Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pazis%2C%20Jason%20Parr%2C%20Ronald%20PAC%20optimal%20exploration%20in%20continuous%20space%20Markov%20decision%20processes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pazis%2C%20Jason%20Parr%2C%20Ronald%20PAC%20optimal%20exploration%20in%20continuous%20space%20Markov%20decision%20processes%202013"
        },
        {
            "id": "35",
            "entry": "[35] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "36",
            "entry": "[36] J. Rust. Using randomization to break the curse of dimensionality. Econometrica, 65(3), 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rust%2C%20J.%20Using%20randomization%20to%20break%20the%20curse%20of%20dimensionality%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rust%2C%20J.%20Using%20randomization%20to%20break%20the%20curse%20of%20dimensionality%201997"
        },
        {
            "id": "37",
            "entry": "[37] N. Saldi, S. Yuksel, and T. Linder. On the asymptotic optimality of finite approximations to markov decision processes with borel spaces. Math. of Operations Research, 42(4), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saldi%2C%20N.%20Yuksel%2C%20S.%20Linder%2C%20T.%20On%20the%20asymptotic%20optimality%20of%20finite%20approximations%20to%20markov%20decision%20processes%20with%20borel%20spaces%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saldi%2C%20N.%20Yuksel%2C%20S.%20Linder%2C%20T.%20On%20the%20asymptotic%20optimality%20of%20finite%20approximations%20to%20markov%20decision%20processes%20with%20borel%20spaces%202017"
        },
        {
            "id": "38",
            "entry": "[38] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, and M. Lanctot. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "39",
            "entry": "[39] Charles J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals of Statistics, pages 1040\u20131053, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stone%2C%20Charles%20J.%20Optimal%20global%20rates%20of%20convergence%20for%20nonparametric%20regression%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stone%2C%20Charles%20J.%20Optimal%20global%20rates%20of%20convergence%20for%20nonparametric%20regression%201982"
        },
        {
            "id": "40",
            "entry": "[40] A. L Strehl, L. Li, E. Wiewiora, J. Langford, and M. L. Littman. PAC model-free reinforcement learning. In ICML, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20A.L.%20Li%2C%20L.%20Wiewiora%2C%20E.%20Langford%2C%20J.%20PAC%20model-free%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20A.L.%20Li%2C%20L.%20Wiewiora%2C%20E.%20Langford%2C%20J.%20PAC%20model-free%20reinforcement%20learning%202006"
        },
        {
            "id": "41",
            "entry": "[41] C. Szepesv\u00e1ri. The asymptotic convergence-rate of Q-learning. In NIPS, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesv%C3%A1ri%2C%20C.%20The%20asymptotic%20convergence-rate%20of%20Q-learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szepesv%C3%A1ri%2C%20C.%20The%20asymptotic%20convergence-rate%20of%20Q-learning%201997"
        },
        {
            "id": "42",
            "entry": "[42] C. Szepesv\u00e1ri and W. D. Smart. Interpolation-based Q-learning. In Proceedings of the TwentyFirst International Conference on Machine learning, page 100. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesv%C3%A1ri%2C%20C.%20Smart%2C%20W.D.%20Interpolation-based%20Q-learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szepesv%C3%A1ri%2C%20C.%20Smart%2C%20W.D.%20Interpolation-based%20Q-learning%202004"
        },
        {
            "id": "43",
            "entry": "[43] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Mach. Learning, 16(3), 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20J.N.%20Asynchronous%20stochastic%20approximation%20and%20Q-learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20J.N.%20Asynchronous%20stochastic%20approximation%20and%20Q-learning%201994"
        },
        {
            "id": "44",
            "entry": "[44] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Trans. Automatic Control, 42(5), 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20J.N.%20Roy%2C%20B.Van%20An%20analysis%20of%20temporal-difference%20learning%20with%20function%20approximation%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20J.N.%20Roy%2C%20B.Van%20An%20analysis%20of%20temporal-difference%20learning%20with%20function%20approximation%201997"
        },
        {
            "id": "45",
            "entry": "[45] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. Springer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexandre%20B.%20Introduction%20to%20Nonparametric%20Estimation.%20Springer%20Series%20in%20Statistics%202009"
        },
        {
            "id": "46",
            "entry": "[46] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20Roman%20High-Dimensional%20Probability%3A%20An%20Introduction%20with%20Applications%20in%20Data%20Science%202017"
        },
        {
            "id": "47",
            "entry": "[47] C. J. C. H. Watkins and P. Dayan. Q-learning. Mach. learning, 8(3-4), 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20J%20C%20H%20Watkins%20and%20P%20Dayan%20Qlearning%20Mach%20learning%20834%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20J%20C%20H%20Watkins%20and%20P%20Dayan%20Qlearning%20Mach%20learning%20834%201992"
        },
        {
            "id": "48",
            "entry": "[48] G. S. Watson. Smooth regression analysis. Sankhya: The Indian Journal of Statistics, Series A, pages 359\u2013372, 1964. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watson%2C%20G.S.%20Smooth%20regression%20analysis.%20Sankhya%3A%20The%20Indian",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watson%2C%20G.S.%20Smooth%20regression%20analysis.%20Sankhya%3A%20The%20Indian"
        }
    ]
}
