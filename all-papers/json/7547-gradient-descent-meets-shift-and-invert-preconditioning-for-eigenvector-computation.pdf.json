{
    "filename": "7547-gradient-descent-meets-shift-and-invert-preconditioning-for-eigenvector-computation.pdf",
    "metadata": {
        "title": "Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation",
        "author": "Zhiqiang Xu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7547-gradient-descent-meets-shift-and-invert-preconditioning-for-eigenvector-computation.pdf"
        },
        "abstract": "Shift-and-invert preconditioning, as a classic acceleration technique for the leading eigenvector computation, has received much attention again recently, owing to fast least-squares solvers for efficiently approximating matrix inversions in power iterations. In this work, we adopt an inexact Riemannian gradient descent perspective to investigate this technique on the effect of the step-size scheme."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "principal component analysis",
            "url": "https://en.wikipedia.org/wiki/principal_component_analysis"
        },
        {
            "term": "power method",
            "url": "https://en.wikipedia.org/wiki/power_method"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "spectral clustering",
            "url": "https://en.wikipedia.org/wiki/spectral_clustering"
        },
        {
            "term": "singular value decomposition",
            "url": "https://en.wikipedia.org/wiki/singular_value_decomposition"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Eigenvector computation is a fundamental problem in numerical algebra and often of central importance to a variety of scientific and engineering computing tasks such as principal component analysis [<a class=\"ref-link\" id=\"cFan_et+al_2018_a\" href=\"#rFan_et+al_2018_a\"><a class=\"ref-link\" id=\"cFan_et+al_2018_a\" href=\"#rFan_et+al_2018_a\">Fan et al, 2018</a></a>], spectral clustering [<a class=\"ref-link\" id=\"cNg_et+al_2001_a\" href=\"#rNg_et+al_2001_a\"><a class=\"ref-link\" id=\"cNg_et+al_2001_a\" href=\"#rNg_et+al_2001_a\">Ng et al, 2001</a></a>], low-rank matrix approximation [<a class=\"ref-link\" id=\"cHastie_et+al_2015_a\" href=\"#rHastie_et+al_2015_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2015_a\" href=\"#rHastie_et+al_2015_a\">Hastie et al., 2015</a></a>, <a class=\"ref-link\" id=\"cLiu_2014_a\" href=\"#rLiu_2014_a\"><a class=\"ref-link\" id=\"cLiu_2014_a\" href=\"#rLiu_2014_a\">Liu and Li, 2014</a></a>], among others",
        "Eigenvector computation is a fundamental problem in numerical algebra and often of central importance to a variety of scientific and engineering computing tasks such as principal component analysis",
        "We take a Riemannian gradient descent view to investigate the shift-and-invert preconditioning for the leading eigenvector computation on the effect of the step-size scheme",
        "We demonstrate the performance of Algorithm 1 on real data from the sparse matrix collection, and compare with the accelerated power method with optimal momentum \u03b2 = \u03bb22/4 [<a class=\"ref-link\" id=\"cSa_et+al_2017_a\" href=\"#rSa_et+al_2017_a\">Sa et al, 2017</a>]",
        "We investigated Riemannian gradient descent with shift-and-invert preconditioning for the leading eigenvector computation on the effect of step-size schemes, in comparison to the recently popular shift-and-inverted power method"
    ],
    "key_statements": [
        "Eigenvector computation is a fundamental problem in numerical algebra and often of central importance to a variety of scientific and engineering computing tasks such as principal component analysis [<a class=\"ref-link\" id=\"cFan_et+al_2018_a\" href=\"#rFan_et+al_2018_a\"><a class=\"ref-link\" id=\"cFan_et+al_2018_a\" href=\"#rFan_et+al_2018_a\">Fan et al, 2018</a></a>], spectral clustering [<a class=\"ref-link\" id=\"cNg_et+al_2001_a\" href=\"#rNg_et+al_2001_a\"><a class=\"ref-link\" id=\"cNg_et+al_2001_a\" href=\"#rNg_et+al_2001_a\">Ng et al, 2001</a></a>], low-rank matrix approximation [<a class=\"ref-link\" id=\"cHastie_et+al_2015_a\" href=\"#rHastie_et+al_2015_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2015_a\" href=\"#rHastie_et+al_2015_a\">Hastie et al., 2015</a></a>, <a class=\"ref-link\" id=\"cLiu_2014_a\" href=\"#rLiu_2014_a\"><a class=\"ref-link\" id=\"cLiu_2014_a\" href=\"#rLiu_2014_a\">Liu and Li, 2014</a></a>], among others",
        "Eigenvector computation is a fundamental problem in numerical algebra and often of central importance to a variety of scientific and engineering computing tasks such as principal component analysis",
        "We take a Riemannian gradient descent view to investigate the shift-and-invert preconditioning for the leading eigenvector computation on the effect of the step-size scheme",
        "We demonstrate the performance of Algorithm 1 on real data from the sparse matrix collection, and compare with the accelerated power method with optimal momentum \u03b2 = \u03bb22/4 [<a class=\"ref-link\" id=\"cSa_et+al_2017_a\" href=\"#rSa_et+al_2017_a\">Sa et al, 2017</a>]",
        "We investigated Riemannian gradient descent with shift-and-invert preconditioning for the leading eigenvector computation on the effect of step-size schemes, in comparison to the recently popular shift-and-inverted power method"
    ],
    "summary": [
        "Eigenvector computation is a fundamental problem in numerical algebra and often of central importance to a variety of scientific and engineering computing tasks such as principal component analysis [<a class=\"ref-link\" id=\"cFan_et+al_2018_a\" href=\"#rFan_et+al_2018_a\"><a class=\"ref-link\" id=\"cFan_et+al_2018_a\" href=\"#rFan_et+al_2018_a\">Fan et al, 2018</a></a>], spectral clustering [<a class=\"ref-link\" id=\"cNg_et+al_2001_a\" href=\"#rNg_et+al_2001_a\"><a class=\"ref-link\" id=\"cNg_et+al_2001_a\" href=\"#rNg_et+al_2001_a\">Ng et al, 2001</a></a>], low-rank matrix approximation [<a class=\"ref-link\" id=\"cHastie_et+al_2015_a\" href=\"#rHastie_et+al_2015_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2015_a\" href=\"#rHastie_et+al_2015_a\">Hastie et al., 2015</a></a>, <a class=\"ref-link\" id=\"cLiu_2014_a\" href=\"#rLiu_2014_a\"><a class=\"ref-link\" id=\"cLiu_2014_a\" href=\"#rLiu_2014_a\">Liu and Li, 2014</a></a>], among others.",
        "Each power iteration step can be reduced to approximately solving a linear system subproblem that can leverage fast least-squares solvers, e.g., accelerated gradient descent (AGD)",
        "We take a Riemannian gradient descent view to investigate the shift-and-invert preconditioning for the leading eigenvector computation on the effect of the step-size scheme.",
        "It includes the shift-and-invert preconditioned power method as a special case with adaptive step-sizes.",
        "Shamir [2016a] adopted the plain power method to warm-start the stochastic variance reduced projected gradient descent without preconditioning for principal component analysis (VR-PCA).",
        "The algorithm steps into an accurate phase by calling the Riemannian gradient descent solver on the shift-and-inverted matrix\u22121, i.e., solving the following problem: min",
        "As we will see for Problem (1), the inexact Riemannian gradient method includes the shift-and-inverted power method as a special case with adaptive step-sizes.",
        "We briefly discuss recent literature in Section 2 and present our shift-and-inverted Riemannian gradient descent solver with theoretical analysis in Section 3.",
        "<a class=\"ref-link\" id=\"cGarber_et+al_2016_a\" href=\"#rGarber_et+al_2016_a\">Garber et al [2016</a>] presented a robust analysis of the shift-and-invert preconditioned power method and achieved optimal convergence rates.",
        "We present our shift-and-inverted Riemannian gradient descent solver.",
        "], Algorithm with fixed step-sizes and using accelerated gradient descent as a least-squares solver is able to converge to one of the leading eigenvectors of A, i.e., \u03c8 < , after T = O ) gradient steps, and the overall complexity is O(",
        "We set n = 1000 and \u03c3 = 1.005, and three solvers are compared: Rimennian gradient descent solver with/without shift-and-invert preconditioning under the constant step-size setting, and the shift-and-inverted power method [<a class=\"ref-link\" id=\"cGarber_et+al_2016_a\" href=\"#rGarber_et+al_2016_a\">Garber et al, 2016</a>].",
        "We demonstrate the performance of Algorithm 1 on real data from the sparse matrix collection, and compare with the accelerated power method with optimal momentum \u03b2 = \u03bb22/4 [<a class=\"ref-link\" id=\"cSa_et+al_2017_a\" href=\"#rSa_et+al_2017_a\">Sa et al, 2017</a>].",
        "We investigated Riemannian gradient descent with shift-and-invert preconditioning for the leading eigenvector computation on the effect of step-size schemes, in comparison to the recently popular shift-and-inverted power method.",
        "The algorithm was theoretically analyzed under the constant step-size setting and shown for the first time to able to achieve a rate of the type O(",
        "Experimental results demonstrated that the shift-and-invert preconditioning can accelerate gradient descent solver.",
        "The adaptive step-size setting with the shift-and-inverted power method is outperformed by the considered step-size settings, especially the BB step-size scheme on real data, albeit with a provable optimal rate."
    ],
    "headline": "We present a novel convergence analysis for the constant step-size setting that achieves a rate at O(",
    "reference_links": [
        {
            "id": "Absil_et+al_2008_a",
            "entry": "P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Absil%2C%20P.-A.%20Mahony%2C%20Robert%20Sepulchre%2C%20Rodolphe%20Optimization%20algorithms%20on%20matrix%20manifolds%202008"
        },
        {
            "id": "Allen-Zhu_2016_a",
            "entry": "Zeyuan Allen-Zhu and Yuanzhi Li. Even faster svd decomposition yet without agonizing pain. In Advances in Neural Information Processing Systems, pages 974\u2013982, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Even%20faster%20svd%20decomposition%20yet%20without%20agonizing%20pain%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Even%20faster%20svd%20decomposition%20yet%20without%20agonizing%20pain%202016"
        },
        {
            "id": "Arora_et+al_2012_a",
            "entry": "Raman Arora, Andrew Cotter, Karen Livescu, and Nathan Srebro. Stochastic optimization for PCA and PLS. In 50th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2012, Allerton Park & Retreat Center, Monticello, IL, USA, October 1-5, 2012, pages 861\u2013 868, 2012. doi: 10.1109/Allerton.2012.6483308. URL https://doi.org/10.1109/Allerton.2012.6483308.",
            "crossref": "https://dx.doi.org/10.1109/Allerton.2012.6483308",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/Allerton.2012.6483308"
        },
        {
            "id": "Arora_et+al_2013_a",
            "entry": "Raman Arora, Andrew Cotter, and Nati Srebro. Stochastic optimization of PCA with capped MSG. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 1815\u20131823, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Raman%20Cotter%2C%20Andrew%20Srebro%2C%20Nati%20Stochastic%20optimization%20of%20PCA%20with%20capped%20MSG%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Raman%20Cotter%2C%20Andrew%20Srebro%2C%20Nati%20Stochastic%20optimization%20of%20PCA%20with%20capped%20MSG%202013"
        },
        {
            "id": "Balcan_et+al_2016_a",
            "entry": "Maria-Florina Balcan, Simon Shaolei Du, Yining Wang, and Adams Wei Yu. An improved gapdependency analysis of the noisy power method. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, pages 284\u2013309, 2016. URL http://jmlr.org/proceedings/papers/v49/balcan16a.html.",
            "url": "http://jmlr.org/proceedings/papers/v49/balcan16a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20Maria-Florina%20Du%2C%20Simon%20Shaolei%20Wang%2C%20Yining%20Yu%2C%20Adams%20Wei%20An%20improved%20gapdependency%20analysis%20of%20the%20noisy%20power%20method%202016-06-23"
        },
        {
            "id": "Balsubramani_et+al_2013_a",
            "entry": "Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental pca. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3174\u20133182. Curran Associates, Inc., 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balsubramani%2C%20Akshay%20Dasgupta%2C%20Sanjoy%20Freund%2C%20Yoav%20The%20fast%20convergence%20of%20incremental%20pca%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balsubramani%2C%20Akshay%20Dasgupta%2C%20Sanjoy%20Freund%2C%20Yoav%20The%20fast%20convergence%20of%20incremental%20pca%202013"
        },
        {
            "id": "Fan_et+al_2018_a",
            "entry": "Jianqing Fan, Qiang Sun, Wen-Xin Zhou, and Ziwei Zhu. Principal component analysis for big data. arXiv preprint arXiv:1801.01602, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01602"
        },
        {
            "id": "Gao_et+al_2017_a",
            "entry": "Chao Gao, Dan Garber, Nathan Srebro, Jialei Wang, and Weiran Wang. Stochastic canonical correlation analysis. CoRR, abs/1702.06533, 2017. URL http://arxiv.org/abs/1702.06533.",
            "url": "http://arxiv.org/abs/1702.06533",
            "arxiv_url": "https://arxiv.org/pdf/1702.06533"
        },
        {
            "id": "Garber_2015_a",
            "entry": "Dan Garber and Elad Hazan. Fast and simple pca via convex optimization. arXiv preprint arXiv:1509.05647, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.05647"
        },
        {
            "id": "Garber_et+al_2016_a",
            "entry": "Dan Garber, Elad Hazan, Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Faster eigenvector computation via shift-and-invert preconditioning. In International Conference on Machine Learning, pages 2626\u20132634, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016"
        },
        {
            "id": "Golub_1996_a",
            "entry": "Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins University Press, Baltimore, MD, USA, 1996. ISBN 0-8018-5414-8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20Gene%20H.%20Loan%2C%20Charles%20F.Van%20Matrix%20Computations%201996"
        },
        {
            "id": "Halko_et+al_2011_a",
            "entry": "Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53 (2):217\u2013288, 2011. doi: 10.1137/090771806.",
            "crossref": "https://dx.doi.org/10.1137/090771806",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/090771806"
        },
        {
            "id": "Moritz_2014_a",
            "entry": "Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In Advances in Neural Information Processing Systems, pages 2861\u20132869, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Moritz%20Hardt%20and%20Eric%20Price.%20The%20noisy%20power%20method%3A%20A%20meta%20algorithm%20with%20applications%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Moritz%20Hardt%20and%20Eric%20Price.%20The%20noisy%20power%20method%3A%20A%20meta%20algorithm%20with%20applications%202014"
        },
        {
            "id": "Hastie_et+al_2015_a",
            "entry": "Trevor Hastie, Rahul Mazumder, Jason D. Lee, and Reza Zadeh. Matrix completion and low-rank SVD via fast alternating least squares. Journal of Machine Learning Research, 16:3367\u20133402, 2015. URL http://dl.acm.org/citation.cfm?id=2912106.",
            "url": "http://dl.acm.org/citation.cfm?id=2912106",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastie%2C%20Trevor%20Mazumder%2C%20Rahul%20Lee%2C%20Jason%20D.%20Zadeh%2C%20Reza%20Matrix%20completion%20and%20low-rank%20SVD%20via%20fast%20alternating%20least%20squares%202015"
        },
        {
            "id": "Johnson_2013_a",
            "entry": "Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013-12-05"
        },
        {
            "id": "Lei_et+al_2016_a",
            "entry": "Qi Lei, Kai Zhong, and Inderjit S. Dhillon. Coordinate-wise power method. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2056\u20132064, 2016. URL http://papers.nips.cc/paper/6103-coordinate-wise-power-method.",
            "url": "http://papers.nips.cc/paper/6103-coordinate-wise-power-method",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Qi%20Zhong%2C%20Kai%20Dhillon%2C%20Inderjit%20S.%20Coordinate-wise%20power%20method%202016-12-05"
        },
        {
            "id": "Liu_2014_a",
            "entry": "Guangcan Liu and Ping Li. Recovery of coherent data via low-rank dictionary pursuit. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 1206\u20131214, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Guangcan%20Li%2C%20Ping%20Recovery%20of%20coherent%20data%20via%20low-rank%20dictionary%20pursuit%202014-12-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Guangcan%20Li%2C%20Ping%20Recovery%20of%20coherent%20data%20via%20low-rank%20dictionary%20pursuit%202014-12-08"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Huikang Liu, Weijie Wu, and Anthony Man-Cho So. Quadratic optimization with orthogonality constraints: Explicit lojasiewicz exponent and linear convergence of line-search methods. In ICML, pages 1158\u20131167, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Huikang%20Wu%2C%20Weijie%20So%2C%20Anthony%20Man-Cho%20Quadratic%20optimization%20with%20orthogonality%20constraints%3A%20Explicit%20lojasiewicz%20exponent%20and%20linear%20convergence%20of%20line-search%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Huikang%20Wu%2C%20Weijie%20So%2C%20Anthony%20Man-Cho%20Quadratic%20optimization%20with%20orthogonality%20constraints%3A%20Explicit%20lojasiewicz%20exponent%20and%20linear%20convergence%20of%20line-search%20methods%202016"
        },
        {
            "id": "Musco_2015_a",
            "entry": "Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster approximate singular value decomposition. In Advances in Neural Information Processing Systems, pages 1396\u20131404, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Randomized%20block%20krylov%20methods%20for%20stronger%20and%20faster%20approximate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Randomized%20block%20krylov%20methods%20for%20stronger%20and%20faster%20approximate%202015"
        },
        {
            "id": "Nesterov_2014_a",
            "entry": "Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing Company, Incorporated, 1 edition, 2014. ISBN 1461346916, 9781461346913.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20on%20Convex%20Optimization%3A%20A%20Basic%20Course%202014"
        },
        {
            "id": "Ng_et+al_2001_a",
            "entry": "Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada], pages 849\u2013856, 2001. URL http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA35.ps.gz.",
            "url": "http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA35.ps.gz",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Weiss%2C%20Yair%20On%20spectral%20clustering%3A%20Analysis%20and%20an%20algorithm%202001-12-03"
        },
        {
            "id": "Sa_et+al_2017_a",
            "entry": "Christopher De Sa, Bryan D. He, Ioannis Mitliagkas, Christopher R\u00e9, and Peng Xu. Accelerated stochastic power iteration. CoRR, abs/1707.02670, 2017. URL http://arxiv.org/abs/1707.02670.",
            "url": "http://arxiv.org/abs/1707.02670",
            "arxiv_url": "https://arxiv.org/pdf/1707.02670"
        },
        {
            "id": "Shamir_2015_a",
            "entry": "Ohad Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 144\u2013152, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20A%20stochastic%20PCA%20and%20SVD%20algorithm%20with%20an%20exponential%20convergence%20rate%202015-07-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20A%20stochastic%20PCA%20and%20SVD%20algorithm%20with%20an%20exponential%20convergence%20rate%202015-07-06"
        },
        {
            "id": "Shamir_0000_a",
            "entry": "Ohad Shamir. Fast stochastic algorithms for SVD and PCA: convergence properties and convexity. In International Conference on Machine Learning, pages 248\u2013256, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Fast%20stochastic%20algorithms%20for%20SVD%20and%20PCA%3A%20convergence%20properties%20and%20convexity",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Fast%20stochastic%20algorithms%20for%20SVD%20and%20PCA%3A%20convergence%20properties%20and%20convexity"
        },
        {
            "id": "Shamir_0000_b",
            "entry": "Ohad Shamir. Convergence of stochastic gradient descent for PCA. In ICML, pages 257\u2013265, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Convergence%20of%20stochastic%20gradient%20descent%20for%20PCA",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Convergence%20of%20stochastic%20gradient%20descent%20for%20PCA"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Jialei Wang, Weiran Wang, Dan Garber, and Nathan Srebro. Efficient coordinate-wise leading eigenvector computation. CoRR, abs/1702.07834, 2017. URL http://arxiv.org/abs/1702.07834.",
            "url": "http://arxiv.org/abs/1702.07834",
            "arxiv_url": "https://arxiv.org/pdf/1702.07834"
        },
        {
            "id": "Wen_2013_a",
            "entry": "Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. Mathematical Programming, 142(1-2):397\u2013434, 2013. doi: 10.1007/s10107-012-0584-1. URL http://dx.doi.org/10.1007/s10107-012-0584-1.",
            "crossref": "https://dx.doi.org/10.1007/s10107-012-0584-1",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10107-012-0584-1"
        },
        {
            "id": "Xu_2018_a",
            "entry": "Zhiqiang Xu and Xin Gao. On truly block eigensolvers via riemannian optimization. In International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain, pages 168\u2013177, 2018. URL http://proceedings.mlr.press/v84/xu18b.html.",
            "url": "http://proceedings.mlr.press/v84/xu18b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Zhiqiang%20Gao%2C%20Xin%20On%20truly%20block%20eigensolvers%20via%20riemannian%20optimization%202018-04-09"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Zhiqiang Xu, Yiping Ke, and Xin Gao. A fast stochastic riemannian eigensolver. In UAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Zhiqiang%20Ke%2C%20Yiping%20Gao%2C%20Xin%20A%20fast%20stochastic%20riemannian%20eigensolver%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Zhiqiang%20Ke%2C%20Yiping%20Gao%2C%20Xin%20A%20fast%20stochastic%20riemannian%20eigensolver%202017"
        },
        {
            "id": "Xu_et+al_2018_b",
            "entry": "Zhiqiang Xu, Xin Cao, and Xin Gao. Convergence analysis of gradient descent for eigenvector computation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 2933\u20132939. International Joint Conferences on Artificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/407. URL https://doi.org/10.24963/ijcai.2018/407.",
            "crossref": "https://dx.doi.org/10.24963/ijcai.2018/407",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.24963/ijcai.2018/407"
        }
    ]
}
