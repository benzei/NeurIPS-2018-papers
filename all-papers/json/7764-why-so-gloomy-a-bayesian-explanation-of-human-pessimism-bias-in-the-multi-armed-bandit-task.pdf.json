{
    "filename": "7764-why-so-gloomy-a-bayesian-explanation-of-human-pessimism-bias-in-the-multi-armed-bandit-task.pdf",
    "metadata": {
        "title": "Why so gloomy? A Bayesian explanation of human pessimism bias in the multi-armed bandit task",
        "author": "Dalin Guo, Angela J. Yu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7764-why-so-gloomy-a-bayesian-explanation-of-human-pessimism-bias-in-the-multi-armed-bandit-task.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "How humans make repeated choices among options with imperfectly known reward outcomes is an important problem in psychology and neuroscience. This is often studied using multi-armed bandits, which is also frequently studied in machine learning. We present data from a human stationary bandit experiment, in which we vary the average abundance and variability of reward availability (mean and variance of reward rate distributions). Surprisingly, we find subjects significantly underestimate prior mean of reward rates \u2013 based on their self-report, at the end of a game, on their reward expectation of non-chosen arms. Previously, human learning in the bandit task was found to be well captured by a Bayesian ideal learning model, the Dynamic Belief Model (DBM), albeit under an incorrect generative assumption of the temporal structure \u2013 humans assume reward rates can change over time even though they are truly fixed. We find that the \u201cpessimism bias\u201d in the bandit task is well captured by the prior mean of DBM when fitted to human choices; but it is poorly captured by the prior mean of the Fixed Belief Model (FBM), an alternative Bayesian model that (correctly) assumes reward rates to be constants. This pessimism bias is also incompletely captured by a simple reinforcement learning model (RL) commonly used in neuroscience and psychology, in terms of fitted initial Q-values. While it seems sub-optimal, and thus mysterious, that humans have an underestimated prior reward expectation, our simulations show that an underestimated prior mean helps to maximize long-term gain, if the observer assumes volatility when reward rates are stable and utilizes a softmax decision policy instead of the optimal one (obtainable by dynamic programming). This raises the intriguing possibility that the brain underestimates reward rates to compensate for the incorrect non-stationarity assumption in the generative model and a simplified decision policy."
    },
    "keywords": [
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "human learning",
            "url": "https://en.wikipedia.org/wiki/human_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "pessimism bias",
            "url": "https://en.wikipedia.org/wiki/pessimism_bias"
        }
    ],
    "highlights": [
        "Humans and animals frequently have to make choices among options with imperfectly known outcomes",
        "It has been shown that human learning in the bandit task is well captured by a Bayesian ideal learning model [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], the Dynamic Belief Model (DBM) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], which assumes the reward distribution to undergo occasional and abrupt changes",
        "The reported estimation is on the arm(s) that they never chose at the end of each game, which is their belief of the mean reward rate before any observation, i.e., mathematically equivalent to the prior mean (DBM & Fixed belief model) or the initial value (RL)",
        "The fitted Dynamic belief model \u03b3 values imply that human participants behave as if they believe the reward rates change on average approximately once every three trials in high-abundance environments, and once every four trials in low-abundance environments).\n3.3",
        "Our results show that humans underestimate the expected reward rates (a pessimism bias), and this underestimation is only recoverable by the prior mean of Dynamic belief model",
        "Participants\u2019 actual assumed prior mean is not at the mode of the simulated performance curve, which underestimates prior mean even more than subjects do"
    ],
    "key_statements": [
        "Humans and animals frequently have to make choices among options with imperfectly known outcomes",
        "It has been shown that human learning in the bandit task is well captured by a Bayesian ideal learning model [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], the Dynamic Belief Model (DBM) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], which assumes the reward distribution to undergo occasional and abrupt changes",
        "The reported reward rates of the never-chosen arms are shown in Fig. 2A",
        "The reported estimation is on the arm(s) that they never chose at the end of each game, which is their belief of the mean reward rate before any observation, i.e., mathematically equivalent to the prior mean (DBM & Fixed belief model) or the initial value (RL)",
        "The fitted Dynamic belief model \u03b3 values imply that human participants behave as if they believe the reward rates change on average approximately once every three trials in high-abundance environments, and once every four trials in low-abundance environments).\n3.3",
        "Reinforcement learning can adjust its reward estimate according to unexpected observations, but is much slower than Dynamic belief model in doing so since it has a constant learning rate that is not increased when there is high posterior probability of a recent change point; Reinforcement learning cannot persistently encode prior information about overall reward abundance in the environment when a change occurs",
        "Our results show that humans underestimate the expected reward rates (a pessimism bias), and this underestimation is only recoverable by the prior mean of Dynamic belief model",
        "Participants\u2019 actual assumed prior mean is not at the mode of the simulated performance curve, which underestimates prior mean even more than subjects do"
    ],
    "summary": [
        "Humans and animals frequently have to make choices among options with imperfectly known outcomes.",
        "Human subjects reported estimates of reward rate significantly lower than the true generative prior mean (p < .001), except in low abundance and low variance environment (p = 0.2973).",
        "We use a version of the softmax decision policy that assumes the choice probabilities among the options to be normalized polynomial functions of the estimated expected reward rates [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], with a parameter b: p(Dt = k) =",
        "The reported estimation is on the arm(s) that they never chose at the end of each game, which is their belief of the mean reward rate before any observation, i.e., mathematically equivalent to the prior mean (DBM & FBM) or the initial value (RL).",
        "The fitted DBM \u03b3 values imply that human participants behave as if they believe the reward rates change on average approximately once every three trials in high-abundance environments, and once every four trials in low-abundance environments).",
        "To gain some insight into how DBM behaves differently than FBM and RL, and what it implies about human psychological processes, we consider the empirical/simulated probability of the participants/model switching away from a \u201cwinning\u201d arm after it suddenly produces a loss (Fig. 3A).",
        "RL can adjust its reward estimate according to unexpected observations, but is much slower than DBM in doing so since it has a constant learning rate that is not increased when there is high posterior probability of a recent change point; RL cannot persistently encode prior information about overall reward abundance in the environment when a change occurs.",
        "Consider the diamond symbols in Fig. 3B;C: the combination of human subjects\u2019 actual average per-trial earned reward (y-axis) and the fitted prior mean for each of the three models (x-axis, color-coded) is very close to DBM\u2019s joint predictions of the two quantities, but very far away from FBM and RL\u2019s joint predictions of the two quantities.",
        "Our study suggests that human learning and decision making are sub-optimal in various ways: assuming an incorrect prior mean, assuming environmental statistics to be non-stationary when they are stable; our analyses show that these sub-optimalities may potentially be combined to achieve better performance than one might expect, as they somehow compensate for each other.",
        "Given that these sub-optimal assumptions have certain computational advantages, e.g., softmax is computationally much simpler than optimal policy, DBM can handle a much broader range of temporal statistics than FBM, understanding how these algorithms fit together in humans may, in the future, yield better algorithms for machine learning applications as well."
    ],
    "headline": "We present data from a human stationary bandit experiment, in which we vary the average abundance and variability of reward availability ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] N D Daw, J P O\u2019Doherty, P Dayan, B Seymour, and R J Dolan. Cortical substrates for exploratory decisions in humans. Nature, 441(7095):876\u20139, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daw%2C%20N.D.%20O%E2%80%99Doherty%2C%20J.P.%20Dayan%2C%20P.%20Seymour%2C%20B.%20Cortical%20substrates%20for%20exploratory%20decisions%20in%20humans%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daw%2C%20N.D.%20O%E2%80%99Doherty%2C%20J.P.%20Dayan%2C%20P.%20Seymour%2C%20B.%20Cortical%20substrates%20for%20exploratory%20decisions%20in%20humans%202006"
        },
        {
            "id": "2",
            "entry": "[2] J D Cohen, S M McClure, and A J Yu. Should I stay or should I go? how the human brain manages the tradeoff between exploitation and exploration. Philos Trans R Soc Lond B Biol Sci, 362(1481):933\u201342, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20J.D.%20McClure%2C%20S.M.%20Yu%2C%20A.J.%20Should%20I%20stay%20or%20should%20I%20go%3F%20how%20the%20human%20brain%20manages%20the%20tradeoff%20between%20exploitation%20and%20exploration%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20J.D.%20McClure%2C%20S.M.%20Yu%2C%20A.J.%20Should%20I%20stay%20or%20should%20I%20go%3F%20how%20the%20human%20brain%20manages%20the%20tradeoff%20between%20exploitation%20and%20exploration%202007"
        },
        {
            "id": "3",
            "entry": "[3] T Sch\u00f6nberg, N D Daw, D Joel, and J P O\u2019Doherty. Reinforcement learning signals in the human striatum distinguish learners from nonlearners during reward-based decision making. Journal of Neuroscience, 27(47):12860\u201312867, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6nberg%2C%20T.%20Daw%2C%20N.D.%20Joel%2C%20D.%20O%E2%80%99Doherty%2C%20J.P.%20Reinforcement%20learning%20signals%20in%20the%20human%20striatum%20distinguish%20learners%20from%20nonlearners%20during%20reward-based%20decision%20making%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6nberg%2C%20T.%20Daw%2C%20N.D.%20Joel%2C%20D.%20O%E2%80%99Doherty%2C%20J.P.%20Reinforcement%20learning%20signals%20in%20the%20human%20striatum%20distinguish%20learners%20from%20nonlearners%20during%20reward-based%20decision%20making%202007"
        },
        {
            "id": "4",
            "entry": "[4] S Zhang and A J Yu. Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting. Advances in Neural Information Processing Systems, 26, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20S.%20Yu%2C%20A.J.%20Forgetful%20Bayes%20and%20myopic%20planning%3A%20Human%20learning%20and%20decision-making%20in%20a%20bandit%20setting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20S.%20Yu%2C%20A.J.%20Forgetful%20Bayes%20and%20myopic%20planning%3A%20Human%20learning%20and%20decision-making%20in%20a%20bandit%20setting%202013"
        },
        {
            "id": "5",
            "entry": "[5] A J Yu and J D Cohen. Sequential effects: Superstition or rational behavior? Advances in Neural Information Processing Systems, 21:1873\u201380, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20A.J.%20Cohen%2C%20J.D.%20Sequential%20effects%3A%20Superstition%20or%20rational%20behavior%3F%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20A.J.%20Cohen%2C%20J.D.%20Sequential%20effects%3A%20Superstition%20or%20rational%20behavior%3F%202009"
        },
        {
            "id": "6",
            "entry": "[6] P Shenoy, R Rao, and A J Yu. A rational decision making framework for inhibitory control. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, volume 23, pages 2146\u201354. MIT Press, Cambridge, MA, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shenoy%2C%20P.%20Rao%2C%20R.%20Yu%2C%20A.J.%20A%20rational%20decision%20making%20framework%20for%20inhibitory%20control%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shenoy%2C%20P.%20Rao%2C%20R.%20Yu%2C%20A.J.%20A%20rational%20decision%20making%20framework%20for%20inhibitory%20control%202010"
        },
        {
            "id": "7",
            "entry": "[7] J S Ide, P Shenoy, A J Yu*, and C-S R Li*. Bayesian prediction and evaluation in the anterior cingulate cortex. Journal of Neuroscience, 33:2039\u20132047, 2013. *Co-senior authors.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ide%2C%20J.S.%20Shenoy%2C%20P.%20Yu%2A%2C%20A.J.%20Li%2C%20C.-S.R.%20Bayesian%20prediction%20and%20evaluation%20in%20the%20anterior%20cingulate%20cortex%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ide%2C%20J.S.%20Shenoy%2C%20P.%20Yu%2A%2C%20A.J.%20Li%2C%20C.-S.R.%20Bayesian%20prediction%20and%20evaluation%20in%20the%20anterior%20cingulate%20cortex%202013"
        },
        {
            "id": "8",
            "entry": "[8] A J Yu and H Huang. Maximizing masquerading as matching: Statistical learning and decision-making in choice behavior. Decision, 1(4):275\u2013287, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20A.J.%20Huang%2C%20H.%20Maximizing%20masquerading%20as%20matching%3A%20Statistical%20learning%20and%20decision-making%20in%20choice%20behavior%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20A.J.%20Huang%2C%20H.%20Maximizing%20masquerading%20as%20matching%3A%20Statistical%20learning%20and%20decision-making%20in%20choice%20behavior%202014"
        },
        {
            "id": "9",
            "entry": "[9] C Ryali, G Reddy, and A J Yu. Demystifying excessively volatile human learning: A bayesian persistent prior and a neural approximation. Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ryali%2C%20C.%20Reddy%2C%20G.%20Yu%2C%20A.J.%20Demystifying%20excessively%20volatile%20human%20learning%3A%20A%20bayesian%20persistent%20prior%20and%20a%20neural%20approximation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ryali%2C%20C.%20Reddy%2C%20G.%20Yu%2C%20A.J.%20Demystifying%20excessively%20volatile%20human%20learning%3A%20A%20bayesian%20persistent%20prior%20and%20a%20neural%20approximation%202018"
        },
        {
            "id": "10",
            "entry": "[10] M Steyvers, M D Lee, and E J Wagenmakers. A bayesian analysis of human decision-making on bandit problems. J Math Psychol, 53:168\u201379, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steyvers%2C%20M.%20Lee%2C%20M.D.%20Wagenmakers%2C%20E.J.%20A%20bayesian%20analysis%20of%20human%20decision-making%20on%20bandit%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steyvers%2C%20M.%20Lee%2C%20M.D.%20Wagenmakers%2C%20E.J.%20A%20bayesian%20analysis%20of%20human%20decision-making%20on%20bandit%20problems%202009"
        },
        {
            "id": "11",
            "entry": "[11] K M Harl\u00e9, S Zhang, M Schiff, S Mackey, M P Paulus*, and A J Yu*. Altered statistical learning and decision-making in methamphetamine dependence: evidence from a two-armed bandit task. Frontiers in Psychology, 6(1910), 2015. *Co-senior authors.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harl%C3%A9%2C%20K.M.%20Zhang%2C%20S.%20Schiff%2C%20M.%20Mackey%2C%20S.%20Altered%20statistical%20learning%20and%20decision-making%20in%20methamphetamine%20dependence%3A%20evidence%20from%20a%20two-armed%20bandit%20task%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harl%C3%A9%2C%20K.M.%20Zhang%2C%20S.%20Schiff%2C%20M.%20Mackey%2C%20S.%20Altered%20statistical%20learning%20and%20decision-making%20in%20methamphetamine%20dependence%3A%20evidence%20from%20a%20two-armed%20bandit%20task%202015"
        },
        {
            "id": "12",
            "entry": "[12] K M Harl\u00e9, D Guo, S Zhang, M P Paulus, and A J Yu. Anhedonia and anxiety underlying depressive symptomatology have distinct effects on reward-based decision-making. PloS One, 12(10):e0186473, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harl%C3%A9%2C%20K.M.%20Guo%2C%20D.%20Zhang%2C%20S.%20Paulus%2C%20M.P.%20Anhedonia%20and%20anxiety%20underlying%20depressive%20symptomatology%20have%20distinct%20effects%20on%20reward-based%20decision-making%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harl%C3%A9%2C%20K.M.%20Guo%2C%20D.%20Zhang%2C%20S.%20Paulus%2C%20M.P.%20Anhedonia%20and%20anxiety%20underlying%20depressive%20symptomatology%20have%20distinct%20effects%20on%20reward-based%20decision-making%202017"
        },
        {
            "id": "13",
            "entry": "[13] Samuel J Gershman and Yael Niv. Novelty and inductive generalization in human reinforcement learning. Topics in Cognitive Science, 7(3):391\u2013415, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gershman%2C%20Samuel%20J.%20Niv%2C%20Yael%20Novelty%20and%20inductive%20generalization%20in%20human%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gershman%2C%20Samuel%20J.%20Niv%2C%20Yael%20Novelty%20and%20inductive%20generalization%20in%20human%20reinforcement%20learning%202015"
        },
        {
            "id": "14",
            "entry": "[14] R A Rescorla and A R Wagner. A theory of pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In A H Black and W F Prokasy, editors, Classical Conditioning II: Current Research and Theory, pages 64\u201399. Appleton-Century-Crofts, Mew York, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R.%20A%20Rescorla%20and%20A%20R%20Wagner.%20A%20theory%20of%20pavlovian%20conditioning%3A%20Variations%20in%20the%20effectiveness%20of%20reinforcement%20and%20nonreinforcement%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R.%20A%20Rescorla%20and%20A%20R%20Wagner.%20A%20theory%20of%20pavlovian%20conditioning%3A%20Variations%20in%20the%20effectiveness%20of%20reinforcement%20and%20nonreinforcement%201972"
        },
        {
            "id": "15",
            "entry": "[15] W Schultz. Predictive reward signal of dopamine neurons. J. Neurophysiol., 80:1\u201327, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schultz%2C%20W.%20Predictive%20reward%20signal%20of%20dopamine%20neurons%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schultz%2C%20W.%20Predictive%20reward%20signal%20of%20dopamine%20neurons%201998"
        },
        {
            "id": "16",
            "entry": "[16] T E J Behrens, M W Woolrich, M E Walton, and M F S Rushworth. Learning the value of information in an uncertain world. Nature Neurosci, 10(9):1214\u201321, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Behrens%2C%20T.E.J.%20Woolrich%2C%20M.W.%20Walton%2C%20M.E.%20Rushworth%2C%20M.F.S.%20Learning%20the%20value%20of%20information%20in%20an%20uncertain%20world%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Behrens%2C%20T.E.J.%20Woolrich%2C%20M.W.%20Walton%2C%20M.E.%20Rushworth%2C%20M.F.S.%20Learning%20the%20value%20of%20information%20in%20an%20uncertain%20world%202007"
        },
        {
            "id": "17",
            "entry": "[17] P R Montague, P Dayan, and T J Sejnowski. A framework for mesencephalic dopamine systems based on predictive hebbian learning. Journal of Neuroscience, 16:1936\u20131947, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montague%2C%20P.R.%20Dayan%2C%20P.%20Sejnowski%2C%20T.J.%20A%20framework%20for%20mesencephalic%20dopamine%20systems%20based%20on%20predictive%20hebbian%20learning%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montague%2C%20P.R.%20Dayan%2C%20P.%20Sejnowski%2C%20T.J.%20A%20framework%20for%20mesencephalic%20dopamine%20systems%20based%20on%20predictive%20hebbian%20learning%201996"
        },
        {
            "id": "18",
            "entry": "[18] I C Dezza, A J Yu, A Cleeremans, and A William. Learning the value of information and reward over time when solving exploration-exploitation problems. Scientific Reports, 7(1):16919, 2017. Biology, 11(3):e1004164, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezza%2C%20I.C.%20Yu%2C%20A.J.%20Cleeremans%2C%20A.%20William%2C%20A.%20Learning%20the%20value%20of%20information%20and%20reward%20over%20time%20when%20solving%20exploration-exploitation%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezza%2C%20I.C.%20Yu%2C%20A.J.%20Cleeremans%2C%20A.%20William%2C%20A.%20Learning%20the%20value%20of%20information%20and%20reward%20over%20time%20when%20solving%20exploration-exploitation%20problems%202017"
        },
        {
            "id": "20",
            "entry": "[20] T Sharot. The optimism bias. Current Biology, 21(23):R941\u2013R945, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharot%2C%20T.%20The%20optimism%20bias%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharot%2C%20T.%20The%20optimism%20bias%202011"
        },
        {
            "id": "21",
            "entry": "[21] Aistis Stankevicius, Quentin JM Huys, Aditi Kalra, and Peggy Seri\u00e8s. Optimism as a prior belief about the probability of future reward. PLoS Computational Biology, 10(5):e1003605, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stankevicius%2C%20Aistis%20Huys%2C%20Quentin%20J.M.%20Kalra%2C%20Aditi%20Seri%C3%A8s%2C%20Peggy%20Optimism%20as%20a%20prior%20belief%20about%20the%20probability%20of%20future%20reward%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stankevicius%2C%20Aistis%20Huys%2C%20Quentin%20J.M.%20Kalra%2C%20Aditi%20Seri%C3%A8s%2C%20Peggy%20Optimism%20as%20a%20prior%20belief%20about%20the%20probability%20of%20future%20reward%202014"
        },
        {
            "id": "22",
            "entry": "[22] Melissa Bateson. Optimistic and pessimistic biases: a primer for behavioural ecologists. Current Opinion in Behavioral Sciences, 12:115\u2013121, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bateson%2C%20Melissa%20Optimistic%20and%20pessimistic%20biases%3A%20a%20primer%20for%20behavioural%20ecologists%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bateson%2C%20Melissa%20Optimistic%20and%20pessimistic%20biases%3A%20a%20primer%20for%20behavioural%20ecologists%202016"
        },
        {
            "id": "23",
            "entry": "[23] R S Nickerson. The production and perception of randomness. Psychol. Rev., 109:330\u201357, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickerson%2C%20R.S.%20The%20production%20and%20perception%20of%20randomness%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickerson%2C%20R.S.%20The%20production%20and%20perception%20of%20randomness%202002"
        },
        {
            "id": "24",
            "entry": "[24] Richard A Berk. An introduction to sample selection bias in sociological data. American Sociological Review, pages 386\u2013398, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berk%2C%20Richard%20A.%20An%20introduction%20to%20sample%20selection%20bias%20in%20sociological%20data%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berk%2C%20Richard%20A.%20An%20introduction%20to%20sample%20selection%20bias%20in%20sociological%20data%201983"
        },
        {
            "id": "25",
            "entry": "[25] P Frazier and A J Yu. Sequential hypothesis testing under stochastic deadlines. Advances in Neural Information Processing Systems, 20, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frazier%2C%20P.%20Yu%2C%20A.J.%20Sequential%20hypothesis%20testing%20under%20stochastic%20deadlines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frazier%2C%20P.%20Yu%2C%20A.J.%20Sequential%20hypothesis%20testing%20under%20stochastic%20deadlines%202008"
        },
        {
            "id": "26",
            "entry": "[26] S J Gershman. Deconstructing the human algorithms for exploration. Cognition, 173:34\u201342, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gershman%2C%20S.J.%20Deconstructing%20the%20human%20algorithms%20for%20exploration%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gershman%2C%20S.J.%20Deconstructing%20the%20human%20algorithms%20for%20exploration%202018"
        },
        {
            "id": "27",
            "entry": "[27] M Speekenbrink and E Konstantinidis. Uncertainty and exploration in a restless bandit problem. Topics in Cognitive Science, 7(2):351\u2013367, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Speekenbrink%2C%20M.%20Konstantinidis%2C%20E.%20Uncertainty%20and%20exploration%20in%20a%20restless%20bandit%20problem%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Speekenbrink%2C%20M.%20Konstantinidis%2C%20E.%20Uncertainty%20and%20exploration%20in%20a%20restless%20bandit%20problem%202015"
        }
    ]
}
