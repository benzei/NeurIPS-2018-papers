{
    "filename": "7369-reinforced-continual-learning.pdf",
    "metadata": {
        "title": "Reinforced Continual Learning",
        "author": "Ju Xu, Zhanxing Zhu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7369-reinforced-continual-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks."
    },
    "keywords": [
        {
            "term": "episodic memory",
            "url": "https://en.wikipedia.org/wiki/episodic_memory"
        },
        {
            "term": "lifelong learning",
            "url": "https://en.wikipedia.org/wiki/lifelong_learning"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        },
        {
            "term": "artificial intelligence",
            "url": "https://en.wikipedia.org/wiki/artificial_intelligence"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Lifelong learning [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], the ability to learn consecutive tasks without forgetting how to perform previously trained tasks, is an important topic for developing artificial intelligence",
        "We propose the new method Reinforced Continual Learning in Section 3, a model to learn a sequence of tasks dynamically based on reinforcement learning",
        "We summarize our Reinforced Continual Learning approach for continual learning in Algorithm 1, in which the subroutine for network expansion is described in Algorithm 2.\n3.5",
        "From Figure 2, we can observe that the approaches with fixed-size network architectures, such as IN, EWC and GEM, own low model complexity, but their prediction accuracy is much worse than those methods with expandable networks, including PGN, DEN and Reinforced Continual Learning",
        "We propose a novel framework for continual learning, Reinforced Continual Learning",
        "Our method searches for the best neural architecture for coming task by reinforcement learning, which increases its capacity when necessary and effectively prevents semantic drift"
    ],
    "key_statements": [
        "Lifelong learning [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], the ability to learn consecutive tasks without forgetting how to perform previously trained tasks, is an important topic for developing artificial intelligence",
        "The primary goal of continual learning is to overcome the forgetting of learned tasks and to leverage the earlier knowledge for obtaining better performance or faster convergence/training speed on the newly coming tasks",
        "When training the network for consecutive tasks, some regularization term is enforced to prevent the model parameters from deviating too much from the previous learned parameters according to their significance to old tasks [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "FearNet [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] mitigates catastrophic forgetting by consolidating recent memories into long-term storage using pseudorehearsal [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] which employs a generative autoencoder to generate previously learned examples that are replayed alongside novel information during consolidation",
        "Fernando et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] proposed PathNet, in which a neural network has ten or twenty modules in each layer, and three or four modules are picked for one task in each layer by an evolutionary approach",
        "These methods typically require unnecessarily large-capacity networks, particularly when the number of tasks is large, since the network architecture is never dynamically adjusted during training.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "In order to better facilitate knowledge transfer and avoid catastrophic forgetting, we propose a novel framework to adaptively expand the network",
        "We provide a sophisticatedly designed reinforcement learning method to solve this problem",
        "In Section 2, we introduce the preliminary knowledge on reinforcement learning",
        "We propose the new method Reinforced Continual Learning in Section 3, a model to learn a sequence of tasks dynamically based on reinforcement learning",
        "The controller\u2019s prediction can be viewed as a list of actions a1:m, which means the number of filters added in m layers , to design an new architecture for a child network and be trained in a new task",
        "We summarize our Reinforced Continual Learning approach for continual learning in Algorithm 1, in which the subroutine for network expansion is described in Algorithm 2.\n3.5",
        "Reinforced Continual Learning largely reduces the number of hyperparameters and boils down to only balancing the average validation accuracy and model complexity when the designed reward function",
        "Through different experiments in Section 4, we demonstrate that Reinforced Continual Learning could achieve more stable results, and better model performance could be achieved simultaneously with even much less neurons than DEN",
        "From Figure 2, we can observe that the approaches with fixed-size network architectures, such as IN, EWC and GEM, own low model complexity, but their prediction accuracy is much worse than those methods with expandable networks, including PGN, DEN and Reinforced Continual Learning",
        "Since Reinforced Continual Learning is based on reinforcement learning, a large number of trials are typically required that leads to more training time than other methods",
        "We propose a novel framework for continual learning, Reinforced Continual Learning",
        "Our method searches for the best neural architecture for coming task by reinforcement learning, which increases its capacity when necessary and effectively prevents semantic drift"
    ],
    "summary": [
        "Lifelong learning [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], the ability to learn consecutive tasks without forgetting how to perform previously trained tasks, is an important topic for developing artificial intelligence.",
        "We propose the new method RCL in Section 3, a model to learn a sequence of tasks dynamically based on reinforcement learning.",
        "We use the controller to decide how many filters should be added to each layer, and we obtain an expanded child network, whose parameters to be learned are denoted as Wt. The training procedure for the new task is as follows, keeping Wta\u22121 fixed and only back-propagating the newly added parameters of Wt\\Wta\u22121.",
        "M where At represents the validation accuracy on Vt, the network complexity as Ct = \u2212 ki, ki i=1 is the number of filters added in layer i, and \u03b1 is a parameter to balance between the prediction performance and model complexity.",
        "The controller\u2019s prediction can be viewed as a list of actions a1:m, which means the number of filters added in m layers , to design an new architecture for a child network and be trained in a new task.",
        "This child network will achieve an accuracy At on a validation dataset and the model complexity Ct, we can obtain the reward Rt as defined in Eq (6).",
        "Compared with DEN, instead of performing selective retraining and network split, RCL keeps the learned parameters for previous tasks fixed and only updates the added parameters.",
        "RCL largely reduces the number of hyperparameters and boils down to only balancing the average validation accuracy and model complexity when the designed reward function.",
        "We evaluate each compared approach by considering average test accuracy on all the tasks, model complexity and training time.",
        "From Figure 2, we can observe that the approaches with fixed-size network architectures, such as IN, EWC and GEM, own low model complexity, but their prediction accuracy is much worse than those methods with expandable networks, including PGN, DEN and RCL.",
        "Regarding to the expandable networks, RCL outperforms PGN and DEN on both test accuracy and model complexity.",
        "To further see the difference of the three methods, we vary the hyperparameters settings and train the networks and obtain how test accuracy changes with respect to the number of parameters, as shown in Figure 3.",
        "Since RCL is based on reinforcement learning, a large number of trials are typically required that leads to more training time than other methods.",
        "Our method searches for the best neural architecture for coming task by reinforcement learning, which increases its capacity when necessary and effectively prevents semantic drift.",
        "The experiments demonstrate that our proposal outperforms the exiting baselines significantly both on prediction accuracy and model complexity"
    ],
    "headline": "In order to better facilitate knowledge transfer and avoid catastrophic forgetting, we propose a novel framework to adaptively expand the network",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Neural optimizer search with reinforcement learning. In International Conference on Machine Learning(ICML), pages 459\u2013468, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bello%2C%20Irwan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Le%2C%20Quoc%20V.%20Neural%20optimizer%20search%20with%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bello%2C%20Irwan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Le%2C%20Quoc%20V.%20Neural%20optimizer%20search%20with%20reinforcement%20learning%202017"
        },
        {
            "id": "2",
            "entry": "[2] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "3",
            "entry": "[3] Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. arXiv preprint arXiv:1711.10563, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10563"
        },
        {
            "id": "4",
            "entry": "[4] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20C.%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20C.%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "6",
            "entry": "[6] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, pages 4655\u20134665, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Sang-Woo%20Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Ha%2C%20Jung-Woo%20Overcoming%20catastrophic%20forgetting%20by%20incremental%20moment%20matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Sang-Woo%20Kim%2C%20Jin-Hwa%20Jun%2C%20Jaehyun%20Ha%2C%20Jung-Woo%20Overcoming%20catastrophic%20forgetting%20by%20incremental%20moment%20matching%202017"
        },
        {
            "id": "7",
            "entry": "[7] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20David%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Gradient%20episodic%20memory%20for%20continual%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20David%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Gradient%20episodic%20memory%20for%20continual%20learning%202017"
        },
        {
            "id": "8",
            "entry": "[8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "9",
            "entry": "[9] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In CVPR, pages 5533\u20135542. IEEE Computer Society, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20H%2C%20Christoph%20Lampert.%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20H%2C%20Christoph%20Lampert.%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017"
        },
        {
            "id": "10",
            "entry": "[10] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123\u2013146, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robins%2C%20Anthony%20Catastrophic%20forgetting%2C%20rehearsal%20and%20pseudorehearsal%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robins%2C%20Anthony%20Catastrophic%20forgetting%2C%20rehearsal%20and%20pseudorehearsal%201995"
        },
        {
            "id": "11",
            "entry": "[11] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04671"
        },
        {
            "id": "12",
            "entry": "[12] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "13",
            "entry": "[13] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Cambridge: MIT press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20Learning%3A%20An%20Introduction%201998"
        },
        {
            "id": "14",
            "entry": "[14] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, pages 1057\u20131063, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%201999"
        },
        {
            "id": "15",
            "entry": "[15] Sebastian Thrun. A lifelong learning perspective for mobile robot control. In International Conference on Intelligent Robots and Systems, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20Sebastian%20A%20lifelong%20learning%20perspective%20for%20mobile%20robot%20control%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thrun%2C%20Sebastian%20A%20lifelong%20learning%20perspective%20for%20mobile%20robot%20control%201995"
        },
        {
            "id": "16",
            "entry": "[16] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "17",
            "entry": "[17] J. Yoon and E. Yang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.01547"
        },
        {
            "id": "18",
            "entry": "[18] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852\u20132858, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Lantao%20Zhang%2C%20Weinan%20Wang%2C%20Jun%20Yu%2C%20Yong%20Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Lantao%20Zhang%2C%20Weinan%20Wang%2C%20Jun%20Yu%2C%20Yong%20Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient%202017"
        },
        {
            "id": "19",
            "entry": "[19] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zenke%2C%20Friedemann%20Poole%2C%20Ben%20Ganguli%2C%20Surya%20Continual%20learning%20through%20synaptic%20intelligence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zenke%2C%20Friedemann%20Poole%2C%20Ben%20Ganguli%2C%20Surya%20Continual%20learning%20through%20synaptic%20intelligence%202017"
        },
        {
            "id": "20",
            "entry": "[20] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        }
    ]
}
