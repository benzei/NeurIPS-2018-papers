{
    "filename": "7349-spider-near-optimal-non-convex-optimization-via-stochastic-path-integrated-differential-estimator.pdf",
    "metadata": {
        "title": "SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator",
        "author": "Cong Fang, Chris Junchi Li, Zhouchen Lin, Tong Zhang",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7349-spider-near-optimal-non-convex-optimization-via-stochastic-path-integrated-differential-estimator.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we propose a new technique named Stochastic Path-Integrated Differential EstimatoR (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced computational cost. Combining SPIDER with the method of normalized gradient descent, we propose SPIDER-SFO that solve non-convex stochastic optimization problems using stochastic gradients only. We provide a few error-bound results on its convergence rates. Specially, we prove that the SPIDER-SFO algorithm achieves a gradient computation cost of O min(n1/2 \u22122, \u22123) to find an -approximate first-order stationary point. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting. Our SPIDER technique can be further applied to find an ( , O( 0.5))-approximate second-order stationary point at a gradient computation cost of Omin(n1/2 \u22122 + \u22122.5, \u22123) ."
    },
    "keywords": [
        {
            "term": "Natural Science Foundation",
            "url": "https://en.wikipedia.org/wiki/Natural_Science_Foundation"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "SPIDER",
            "url": "https://en.wikipedia.org/wiki/SPIDER"
        },
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        }
    ],
    "highlights": [
        "We study the optimization problem minimize f (x) \u2261 E [F (x; \u03b6)] x\u2208Rd (1.1)<br/><br/>where the stochastic component F (x; \u03b6), indexed by some random vector \u03b6, is smooth and possibly non-convex",
        "Measured by gradient cost which is the total number of computation of stochastic gradients, our proposed Stochastic Path-Integrated Differential Estimator-Stochastic First-Order algorithm achieves a faster rate of convergence in O) which outperforms the previous best-known results in both finite-sum [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>][<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and online cases [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] by a factor of O)",
        "We propose the Stochastic Path-Integrated Differential Estimator (SPIDER) technique, which significantly avoids excessive access of stochastic oracles and reduces the time complexity",
        "(i) We propose the Stochastic Path-Integrated Differential Estimator-Stochastic First-Order algorithm (Algorithm 1) for finding approximate first-order stationary points for non-convex stochastic optimization problem (1.2), and prove the optimality of such rate in at least one case",
        "We propose Stochastic Path-Integrated Differential Estimator-Stochastic First-Order in Algorithm 1, which resembles a stochastic variant of NGD with the Stochastic Path-Integrated Differential Estimator technique applied, so that one can maintain an estimate of \u2207f at a higher accuracy under limited gradient budgets",
        "We propose in this work the Stochastic Path-Integrated Differential Estimator method for non-convex optimization"
    ],
    "key_statements": [
        "We study the optimization problem minimize f (x) \u2261 E [F (x; \u03b6)] x\u2208Rd (1.1)<br/><br/>where the stochastic component F (x; \u03b6), indexed by some random vector \u03b6, is smooth and possibly non-convex",
        "Measured by gradient cost which is the total number of computation of stochastic gradients, our proposed Stochastic Path-Integrated Differential Estimator-Stochastic First-Order algorithm achieves a faster rate of convergence in O) which outperforms the previous best-known results in both finite-sum [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>][<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and online cases [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] by a factor of O)",
        "We propose the Stochastic Path-Integrated Differential Estimator (SPIDER) technique, which significantly avoids excessive access of stochastic oracles and reduces the time complexity",
        "(i) We propose the Stochastic Path-Integrated Differential Estimator-Stochastic First-Order algorithm (Algorithm 1) for finding approximate first-order stationary points for non-convex stochastic optimization problem (1.2), and prove the optimality of such rate in at least one case",
        " Following Allen-Zhu & Li [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], Carmon et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], Xu et al [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>], we propose Stochastic Path-Integrated Differential Estimator-Stochastic First-Order+ algorithm for finding an approximate second-order stationary point for non-convex stochastic optimization problem",
        "We propose Stochastic Path-Integrated Differential Estimator-Stochastic First-Order in Algorithm 1, which resembles a stochastic variant of NGD with the Stochastic Path-Integrated Differential Estimator technique applied, so that one can maintain an estimate of \u2207f at a higher accuracy under limited gradient budgets",
        "We propose in this work the Stochastic Path-Integrated Differential Estimator method for non-convex optimization"
    ],
    "summary": [
        "We study the optimization problem minimize f (x) \u2261 E [F (x; \u03b6)] x\u2208Rd (1.1)<br/><br/>where the stochastic component F (x; \u03b6), indexed by some random vector \u03b6, is smooth and possibly non-convex.",
        "Measured by gradient cost which is the total number of computation of stochastic gradients, our proposed SPIDER-SFO algorithm achieves a faster rate of convergence in O) which outperforms the previous best-known results in both finite-sum [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>][<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and online cases [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] by a factor of O).",
        "(i) We propose the SPIDER-SFO algorithm (Algorithm 1) for finding approximate first-order stationary points for non-convex stochastic optimization problem (1.2), and prove the optimality of such rate in at least one case.",
        "Inspired by recent works Carmon et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], Johnson & Zhang [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] and independent of Zhou et al [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>], this is the first time that the gradient cost of O) in both upper and lower bound for finding first-order stationary points for problem (1.2) were obtained.",
        "Following Allen-Zhu & Li [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], Carmon et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], Xu et al [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>], we propose SPIDER-SFO+ algorithm for finding an approximate second-order stationary point for non-convex stochastic optimization problem.",
        "We conclude our second upper-bound result: Theorem 2 (First-order stationary point, finite-sum setting, in expectation).",
        "For any L > 0, \u2206 > 0, and 2 \u2264 n \u2264 O \u22062L2 \u00b7 \u22124 , for any algorithm A satisfying (3.7), there exists a dimension d = O \u22062L2\u00b7n2 \u22124 , and a function f satisfies Assumption 1 in the finite-sum case, such that in order to find a point xfor which \u2207f (x) \u2264 , A must cost at least \u03a9 L\u2206 \u00b7 n1/2 \u22122 stochastic gradient accesses.",
        "To obtain the lower bound result for the online case with the additional Assumption 1, with more efforts one might be able to construct a second counterexample that requires \u03a9( \u22123) stochastic gradient accesses with the knowledge of \u03c3 instead of n.",
        "Upper Bound for Finding First-Order Stationary Points, in High-Probability Under more stringent assumptions on the moments of stochastic gradients, our Algorithm 1 with OPTION I achieves a gradient cost of O) with high probability.",
        "Zeroth-Order Stationary Point After the NIPS submission of this work, we propose a second application of our SPIDER technique to the stochastic zeroth-order method for problem (1.2) and achieves individual function accesses of O).",
        "It is not yet clear if our O( \u22123) for the online case and O(n1/2 \u22122) for the finite-sum case gradient cost upper bound for finding a second-order stationary point) is optimal or the gradient cost can be further improved, assuming both Lipschitz gradient and Lipschitz Hessian."
    ],
    "headline": "We propose a new technique named Stochastic Path-Integrated Differential EstimatoR , which can be used to track many deterministic quantities of interests with significantly reduced computational cost",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., & Ma, T. (2017). Finding approximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing (pp. 1195\u20131199).: ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017"
        },
        {
            "id": "2",
            "entry": "[2] Allen-Zhu, Z. (2018). Natasha 2: Faster non-convex optimization than sgd. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Natasha%202%3A%20Faster%20non-convex%20optimization%20than%20sgd%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Natasha%202%3A%20Faster%20non-convex%20optimization%20than%20sgd%202018"
        },
        {
            "id": "3",
            "entry": "[3] Allen-Zhu, Z. & Hazan, E. (2016). Variance reduction for faster non-convex optimization. In International Conference on Machine Learning (pp. 699\u2013707).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Hazan%2C%20E.%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Hazan%2C%20E.%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] Allen-Zhu, Z. & Li, Y. (2018). Neon2: Finding local minima via first-order oracles. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Li%2C%20Y.%20Neon2%3A%20Finding%20local%20minima%20via%20first-order%20oracles%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Li%2C%20Y.%20Neon2%3A%20Finding%20local%20minima%20via%20first-order%20oracles%202018"
        },
        {
            "id": "5",
            "entry": "[5] Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010 (pp. 177\u2013186). Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010"
        },
        {
            "id": "6",
            "entry": "[6] Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2), 223\u2013311.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018"
        },
        {
            "id": "7",
            "entry": "[7] Bubeck, S. et al. (2015). Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4), 231\u2013357.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015"
        },
        {
            "id": "8",
            "entry": "[8] Carmon, Y., Duchi, J. C., Hinder, O., & Sidford, A. (2016). Accelerated methods for non-convex optimization. To appear in SIAM Journal on Optimization, accepted.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20Accelerated%20methods%20for%20non-convex%20optimization%202016"
        },
        {
            "id": "9",
            "entry": "[9] Carmon, Y., Duchi, J. C., Hinder, O., & Sidford, A. (2017a). \u201cConvex Until Proven Guilty\u201d: Dimension-free acceleration of gradient descent on non-convex functions. In International Conference on Machine Learning (pp. 654\u2013663).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20%E2%80%9CConvex%20Until%20Proven%20Guilty%E2%80%9D%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20%E2%80%9CConvex%20Until%20Proven%20Guilty%E2%80%9D%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017"
        },
        {
            "id": "10",
            "entry": "[10] Carmon, Y., Duchi, J. C., Hinder, O., & Sidford, A. (2017b). Lower bounds for finding stationary points i. arXiv preprint arXiv:1710.11606.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11606"
        },
        {
            "id": "11",
            "entry": "[11] Cauchy, A. (1847). M\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution des systemes d\u00e9quations simultan\u00e9es. Comptes Rendus de l\u2019Academie des Science, 25, 536\u2013538.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cauchy%2C%20A.%20M%C3%A9thode%20g%C3%A9n%C3%A9rale%20pour%20la%20r%C3%A9solution%20des%20systemes%20d%C3%A9quations%20simultan%C3%A9es%201847",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cauchy%2C%20A.%20M%C3%A9thode%20g%C3%A9n%C3%A9rale%20pour%20la%20r%C3%A9solution%20des%20systemes%20d%C3%A9quations%20simultan%C3%A9es%201847"
        },
        {
            "id": "12",
            "entry": "[12] Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems (pp. 2933\u20132941).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "13",
            "entry": "[13] Defazio, A., Bach, F., & Lacoste-Julien, S. (2014). SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems (pp. 1646\u20131654).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "14",
            "entry": "[14] Durrett, R. (2010). Probability: Theory and Examples (4th edition). Cambridge University Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durrett%2C%20R.%20Probability%3A%20Theory%20and%20Examples%202010"
        },
        {
            "id": "15",
            "entry": "[15] Ge, R., Huang, F., Jin, C., & Yuan, Y. (2015). Escaping from saddle points \u2013 online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory (pp. 797\u2013842).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%93%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%93%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "16",
            "entry": "[16] Ghadimi, S. & Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4), 2341\u20132368.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "17",
            "entry": "[17] Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press. http://www.deeplearningbook.org.",
            "url": "http://www.deeplearningbook.org"
        },
        {
            "id": "18",
            "entry": "[18] Hazan, E., Levy, K., & Shalev-Shwartz, S. (2015). Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems (pp. 1594\u20131602).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Levy%2C%20K.%20Shalev-Shwartz%2C%20S.%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Levy%2C%20K.%20Shalev-Shwartz%2C%20S.%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015"
        },
        {
            "id": "19",
            "entry": "[19] Jain, P., Kar, P., et al. (2017). Non-convex optimization for machine learning. Foundations and Trends R in Machine Learning, 10(3-4), 142\u2013336.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20P.%20Kar%2C%20P.%20Non-convex%20optimization%20for%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20P.%20Kar%2C%20P.%20Non-convex%20optimization%20for%20machine%20learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., & Jordan, M. I. (2017a). How to escape saddle points efficiently. In International Conference on Machine Learning (pp. 1724\u20131732).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017"
        },
        {
            "id": "21",
            "entry": "[21] Jin, C., Netrapalli, P., & Jordan, M. I. (2017b). Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10456"
        },
        {
            "id": "22",
            "entry": "[22] Johnson, R. & Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems (pp. 315\u2013323).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "23",
            "entry": "[23] Lee, J. D., Simchowitz, M., Jordan, M. I., & Recht, B. (2016). Gradient descent only converges to minimizers. In Proceedings of The 29th Conference on Learning Theory (pp. 1246\u20131257).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20J.D.%20Simchowitz%2C%20M.%20Jordan%2C%20M.I.%20Recht%2C%20B.%20Gradient%20descent%20only%20converges%20to%20minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20J.D.%20Simchowitz%2C%20M.%20Jordan%2C%20M.I.%20Recht%2C%20B.%20Gradient%20descent%20only%20converges%20to%20minimizers%202016"
        },
        {
            "id": "24",
            "entry": "[24] Lei, L., Ju, C., Chen, J., & Jordan, M. I. (2017). Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems (pp. 2345\u20132355).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017"
        },
        {
            "id": "25",
            "entry": "[25] Levy, K. Y. (2016). The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04831"
        },
        {
            "id": "26",
            "entry": "[26] Nesterov, Y. (2004). Introductory lectures on convex optimization: A basic course, volume 87. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202004"
        },
        {
            "id": "27",
            "entry": "[27] Nesterov, Y. & Polyak, B. T. (2006). Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1), 177\u2013205.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Polyak%2C%20B.T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Polyak%2C%20B.T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "28",
            "entry": "[28] Nguyen, L. M., Liu, J., Scheinberg, K., & Tak\u00e1c, M. (2017a). SARAH: A novel method for machine learning problems using stochastic recursive gradient. In D. Precup & Y. W. Teh (Eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research (pp. 2613\u20132621). International Convention Centre, Sydney, Australia: PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20L.M.%20Liu%2C%20J.%20Scheinberg%2C%20K.%20Tak%C3%A1c%2C%20M.%20SARAH%3A%20A%20novel%20method%20for%20machine%20learning%20problems%20using%20stochastic%20recursive%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20L.M.%20Liu%2C%20J.%20Scheinberg%2C%20K.%20Tak%C3%A1c%2C%20M.%20SARAH%3A%20A%20novel%20method%20for%20machine%20learning%20problems%20using%20stochastic%20recursive%20gradient%202017"
        },
        {
            "id": "29",
            "entry": "[29] Nguyen, L. M., Liu, J., Scheinberg, K., & Tak\u00e1c, M. (2017b). Stochastic recursive gradient algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07261"
        },
        {
            "id": "30",
            "entry": "[30] Paquette, C., Lin, H., Drusvyatskiy, D., Mairal, J., & Harchaoui, Z. (2018). Catalyst for gradientbased nonconvex optimization. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics (pp. 613\u2013622).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paquette%2C%20C.%20Lin%2C%20H.%20Drusvyatskiy%2C%20D.%20Mairal%2C%20J.%20Catalyst%20for%20gradientbased%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paquette%2C%20C.%20Lin%2C%20H.%20Drusvyatskiy%2C%20D.%20Mairal%2C%20J.%20Catalyst%20for%20gradientbased%20nonconvex%20optimization%202018"
        },
        {
            "id": "31",
            "entry": "[31] Reddi, S., Zaheer, M., Sra, S., Poczos, B., Bach, F., Salakhutdinov, R., & Smola, A. (2018). A generic approach for escaping saddle points. In A. Storkey & F. Perez-Cruz (Eds.), Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research (pp. 1233\u20131242). Playa Blanca, Lanzarote, Canary Islands: PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018"
        },
        {
            "id": "32",
            "entry": "[32] Reddi, S. J., Hefny, A., Sra, S., Poczos, B., & Smola, A. (2016). Stochastic variance reduction for nonconvex optimization. In International conference on machine learning (pp. 314\u2013323).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "33",
            "entry": "[33] Robbins, H. & Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, (pp. 400\u2013407).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method.%20The%20annals%20of%20mathematical%20statistics%201951"
        },
        {
            "id": "34",
            "entry": "[34] Schmidt, M., Le Roux, N., & Bach, F. (2017). Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1-2), 83\u2013112.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20M.%20Le%20Roux%2C%20N.%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20M.%20Le%20Roux%2C%20N.%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017"
        },
        {
            "id": "35",
            "entry": "[35] Tripuraneni, N., Stern, M., Jin, C., Regier, J., & Jordan, M. I. (2018). Stochastic cubic regularization for fast nonconvex optimization. In Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tripuraneni%2C%20N.%20Stern%2C%20M.%20Jin%2C%20C.%20Regier%2C%20J.%20Stochastic%20cubic%20regularization%20for%20fast%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tripuraneni%2C%20N.%20Stern%2C%20M.%20Jin%2C%20C.%20Regier%2C%20J.%20Stochastic%20cubic%20regularization%20for%20fast%20nonconvex%20optimization%202018"
        },
        {
            "id": "36",
            "entry": "[36] Woodworth, B. & Srebro, N. (2017). Lower bound for randomized first order convex optimization. arXiv preprint arXiv:1709.03594.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03594"
        },
        {
            "id": "37",
            "entry": "[37] Woodworth, B. E. & Srebro, N. (2016). Tight complexity bounds for optimizing composite objectives. In Advances in Neural Information Processing Systems (pp. 3639\u20133647).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodworth%2C%20B.E.%20Srebro%2C%20N.%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodworth%2C%20B.E.%20Srebro%2C%20N.%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016"
        },
        {
            "id": "38",
            "entry": "[38] Xu, Y., Jin, R., & Yang, T. (2017). First-order stochastic algorithms for escaping from saddle points in almost linear time. arXiv preprint arXiv:1711.01944.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01944"
        },
        {
            "id": "39",
            "entry": "[39] Zhou, D., Xu, P., & Gu, Q. (2018a). Finding local minima via stochastic nested variance reduction. arXiv preprint arXiv:1806.08782.",
            "arxiv_url": "https://arxiv.org/pdf/1806.08782"
        },
        {
            "id": "40",
            "entry": "[40] Zhou, D., Xu, P., & Gu, Q. (2018b). Stochastic nested variance reduction for nonconvex optimization. arXiv preprint arXiv:1806.07811. ",
            "arxiv_url": "https://arxiv.org/pdf/1806.07811"
        }
    ]
}
