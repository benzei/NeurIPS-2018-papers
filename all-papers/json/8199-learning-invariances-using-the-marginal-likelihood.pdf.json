{
    "filename": "8199-learning-invariances-using-the-marginal-likelihood.pdf",
    "metadata": {
        "title": "Learning Invariances using the Marginal Likelihood",
        "author": "Mark van der Wilk, Matthias Bauer, ST John, James Hensman",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8199-learning-invariances-using-the-marginal-likelihood.pdf"
        },
        "abstract": "Generalising well in supervised learning tasks relies on correctly extrapolating the training data to a large region of the input space. One way to achieve this is to constrain the predictions to be invariant to transformations on the input that are known to be irrelevant (e.g. translation). Commonly, this is done through data augmentation, where the training set is enlarged by applying hand-crafted transformations to the inputs. We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by back-propagating through it to maximise the marginal likelihood."
    },
    "keywords": [
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "Convolutional Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Convolutional_Neural_Network"
        },
        {
            "term": "process model",
            "url": "https://en.wikipedia.org/wiki/process_model"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "prior distribution",
            "url": "https://en.wikipedia.org/wiki/prior_distribution"
        },
        {
            "term": "marginal likelihood",
            "url": "https://en.wikipedia.org/wiki/marginal_likelihood"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        }
    ],
    "highlights": [
        "We want to predict some quantity y \u2208 Y given an input x \u2208 X from a limited mnuumchbeorfotfhNe intrpauint isnpgacexeaXmpalsespo{sxsnib, lyen. }BNny=1c.oWnsetrwaiannint gouorumr pordeedlitcotomr atokemcaokrerescitmpirleadricptrieodnisctiinonass for inputs which are modified in ways that are irrelevant to the prediction, we can generalise what we learn from a single training example to a wide range of new inputs",
        "It is unsatisfactory from a Bayesian perspective, according to which assumptions and expert knowledge should be explicitly encoded in the prior distribution only",
        "Given that the marginal likelihood correctly identifies which invariances in the prior benefit generalisation, we focus our efforts in the rest of this paper on finding a good approximation that can be practically used to learn invariances.\n4 Inference for Gaussian processes with invariances",
        "We show how invariances described by general data transformations can be incorporated into Gaussian process models",
        "This variational lower bound allows us to learn a good invariance by maximising the marginal likelihood bound through back-propagation of the sampling procedure"
    ],
    "key_statements": [
        "We want to predict some quantity y \u2208 Y given an input x \u2208 X from a limited mnuumchbeorfotfhNe intrpauint isnpgacexeaXmpalsespo{sxsnib, lyen. }BNny=1c.oWnsetrwaiannint gouorumr pordeedlitcotomr atokemcaokrerescitmpirleadricptrieodnisctiinonass for inputs which are modified in ways that are irrelevant to the prediction, we can generalise what we learn from a single training example to a wide range of new inputs",
        "Developing an augmentation for a particular dataset relies on expert knowledge, trial and error, and cross-validation",
        "It is unsatisfactory from a Bayesian perspective, according to which assumptions and expert knowledge should be explicitly encoded in the prior distribution only",
        "By adding data that are not true observations, the posterior may become overconfident, and the marginal likelihood can no longer be used to compare to other models",
        "We investigate prior distributions which incorporate invariances",
        "The main benefit of treating invariances in this way is that models with different invariances can be compared using the marginal likelihood",
        "Parameterised invariances can even be learned by backpropagating through the marginal likelihood",
        "Given that the marginal likelihood correctly identifies which invariances in the prior benefit generalisation, we focus our efforts in the rest of this paper on finding a good approximation that can be practically used to learn invariances.\n4 Inference for Gaussian processes with invariances",
        "In the previous section we argued that the marginal likelihood was a more appropriate objective function for learning invariances than the regular likelihood",
        "We focus our efforts on Gaussian processes based on kernels with complex, parameterised invariances built in, for which we will derive a practical marginal likelihood approximation",
        "Over the few sections, we will develop an approximation that will allow us to evaluate a lower bound to the marginal likelihood which only requires samples of the orbit A(x) or augmentation distribution p",
        "We emphasise that the marginal likelihood bound does correctly identify the best-performing invariance in this case as well",
        "We show how invariances described by general data transformations can be incorporated into Gaussian process models",
        "This variational lower bound allows us to learn a good invariance by maximising the marginal likelihood bound through back-propagation of the sampling procedure"
    ],
    "summary": [
        "We want to predict some quantity y \u2208 Y given an input x \u2208 X from a limited mnuumchbeorfotfhNe intrpauint isnpgacexeaXmpalsespo{sxsnib, lyen. }BNny=1c.oWnsetrwaiannint gouorumr pordeedlitcotomr atokemcaokrerescitmpirleadricptrieodnisctiinonass for inputs which are modified in ways that are irrelevant to the prediction, we can generalise what we learn from a single training example to a wide range of new inputs.",
        "While the prediction of a non-invariant learning method is only influenced in a small region around a training point, invariant models are constrained to make similar predictions in a much larger area, with the area being determined by the set or distribution of transformations.",
        "Given that the marginal likelihood correctly identifies which invariances in the prior benefit generalisation, we focus our efforts in the rest of this paper on finding a good approximation that can be practically used to learn invariances.",
        "We focus our efforts on Gaussian processes based on kernels with complex, parameterised invariances built in, for which we will derive a practical marginal likelihood approximation.",
        "Over the few sections, we will develop an approximation that will allow us to evaluate a lower bound to the marginal likelihood which only requires samples of the orbit A(x) or augmentation distribution p.",
        "While this technique allows variational inference to be applied to invariant kernels with moderately sized orbits, similar to the convolutional kernels in van der Wilk et al [2017], it does not help when even a single sum is too large, or when intractable integrals are needed, like in eq (8), since evaluations of kf are still needed, and the covariance function kfu requires an intractable integral: kfu(x, z) = Ep(g) pgg(z) dxa = pkdxa .",
        "While MNIST has been accurately solved by other methods, we intend to show that a model like an RBF GP (Radial Basis Function or squared exponential kernel), for which MNIST is challenging, can be significantly improved by learning the correct invariances.",
        "We do not require a validation set, but can use the log marginal likelihood of the training data to monitor performance.",
        "We compare general affine transformations, rotations with learned angle bounds, and fixed rotational invariance.",
        "We show how invariances described by general data transformations can be incorporated into Gaussian process models.",
        "This variational lower bound allows us to learn a good invariance by maximising the marginal likelihood bound through back-propagation of the sampling procedure.",
        "While we focus on kernels with invariances in this work, we hope that our demonstration of learning with kernels that do not admit a closed-form evaluation will prove to be more generally useful by increasing the set of kernels with interesting generalisation properties that can be used."
    ],
    "headline": "We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models",
    "reference_links": [
        {
            "id": "Antoniou_et+al_2017_a",
            "entry": "Antoniou, A., Storkey, A., and Edwards, H. (2017). Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04340"
        },
        {
            "id": "Bauer_et+al_2016_a",
            "entry": "Bauer, M., van der Wilk, M., and Rasmussen, C. E. (2016). Understanding probabilistic sparse Gaussian process approximations. In Advances in Neural Information Processing Systems 29.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauer%2C%20M.%20van%20der%20Wilk%2C%20M.%20Rasmussen%2C%20C.E.%20Understanding%20probabilistic%20sparse%20Gaussian%20process%20approximations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bauer%2C%20M.%20van%20der%20Wilk%2C%20M.%20Rasmussen%2C%20C.E.%20Understanding%20probabilistic%20sparse%20Gaussian%20process%20approximations%202016"
        },
        {
            "id": "Beymer_1995_a",
            "entry": "Beymer, D. and Poggio, T. (1995). Face recognition from one example view. In Proceedings of IEEE International Conference on Computer Vision.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beymer%2C%20D.%20Poggio%2C%20T.%20Face%20recognition%20from%20one%20example%20view%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beymer%2C%20D.%20Poggio%2C%20T.%20Face%20recognition%20from%20one%20example%20view%201995"
        },
        {
            "id": "Chapelle_2002_a",
            "entry": "Chapelle, O. and Sch\u00f6lkopf, B. (2002). Incorporating invariances in non-linear support vector machines. In Advances in Neural Information Processing Systems 14.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20O.%20Sch%C3%B6lkopf%2C%20B.%20Incorporating%20invariances%20in%20non-linear%20support%20vector%20machines%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20O.%20Sch%C3%B6lkopf%2C%20B.%20Incorporating%20invariances%20in%20non-linear%20support%20vector%20machines%202002"
        },
        {
            "id": "Cohen_2016_a",
            "entry": "Cohen, T. and Welling, M. (2016). Group equivariant convolutional networks. In Proceedings of The 33rd International Conference on Machine Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20T.%20Welling%2C%20M.%20Group%20equivariant%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20T.%20Welling%2C%20M.%20Group%20equivariant%20convolutional%20networks%202016"
        },
        {
            "id": "Dao_et+al_2018_a",
            "entry": "Dao, T., Gu, A., Ratner, A. J., Smith, V., Sa, C. D., and R\u00e9, C. (2018). A kernel theory of modern data augmentation. arXiv preprint arXiv:1803.06084.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06084"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "Figueiras-Vidal_2009_a",
            "entry": "Figueiras-Vidal, A. and L\u00e1zaro-Gredilla, M. (2009). Inter-domain Gaussian processes for sparse inference using inducing features. In Advances in Neural Information Processing Systems 22.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Figueiras-Vidal%2C%20A.%20L%C3%A1zaro-Gredilla%2C%20M.%20Inter-domain%20Gaussian%20processes%20for%20sparse%20inference%20using%20inducing%20features%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Figueiras-Vidal%2C%20A.%20L%C3%A1zaro-Gredilla%2C%20M.%20Inter-domain%20Gaussian%20processes%20for%20sparse%20inference%20using%20inducing%20features%202009"
        },
        {
            "id": "Germain_et+al_2016_a",
            "entry": "Germain, P., Bach, F., Lacoste, A., and Lacoste-Julien, S. (2016). PAC-Bayesian theory meets Bayesian inference. In Advances in Neural Information Processing Systems 29.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20P.%20Bach%2C%20F.%20Lacoste%2C%20A.%20Lacoste-Julien%2C%20S.%20PAC-Bayesian%20theory%20meets%20Bayesian%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20P.%20Bach%2C%20F.%20Lacoste%2C%20A.%20Lacoste-Julien%2C%20S.%20PAC-Bayesian%20theory%20meets%20Bayesian%20inference%202016"
        },
        {
            "id": "Gibbs_2000_a",
            "entry": "Gibbs, M. N. and Mackay, D. J. C. (2000). Variational Gaussian process classifiers. IEEE Transactions on Neural Networks, 11(6):1458\u20131464.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gibbs%2C%20M.N.%20Mackay%2C%20D.J.C.%20Variational%20Gaussian%20process%20classifiers%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gibbs%2C%20M.N.%20Mackay%2C%20D.J.C.%20Variational%20Gaussian%20process%20classifiers%202000"
        },
        {
            "id": "Ginsbourger_et+al_2012_a",
            "entry": "Ginsbourger, D., Bay, X., Roustant, O., and Carraro, L. (2012). Argumentwise invariant kernels for the approximation of invariant functions. Annales de la Facult\u00e9 de Sciences de Toulouse.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ginsbourger%2C%20D.%20Bay%2C%20X.%20Roustant%2C%20O.%20Carraro%2C%20L.%20Argumentwise%20invariant%20kernels%20for%20the%20approximation%20of%20invariant%20functions%202012"
        },
        {
            "id": "Ginsbourger_et+al_2013_a",
            "entry": "Ginsbourger, D., Roustant, O., and Durrande, N. (2013). Invariances of random fields paths, with applications in Gaussian process regression. arXiv preprint arXiv:1308.1359.",
            "arxiv_url": "https://arxiv.org/pdf/1308.1359"
        },
        {
            "id": "Ginsbourger_et+al_2016_a",
            "entry": "Ginsbourger, D., Roustant, O., and Durrande, N. (2016). On degeneracy and invariances of random fields paths with applications in Gaussian process modelling. Journal of Statistical Planning and Inference, 170:117\u2013128.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ginsbourger%2C%20D.%20Roustant%2C%20O.%20Durrande%2C%20N.%20On%20degeneracy%20and%20invariances%20of%20random%20fields%20paths%20with%20applications%20in%20Gaussian%20process%20modelling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ginsbourger%2C%20D.%20Roustant%2C%20O.%20Durrande%2C%20N.%20On%20degeneracy%20and%20invariances%20of%20random%20fields%20paths%20with%20applications%20in%20Gaussian%20process%20modelling%202016"
        },
        {
            "id": "Graepel_2004_a",
            "entry": "Graepel, T. and Herbrich, R. (2004). Invariant pattern recognition by semi-definite programming machines. In Advances in Neural Information Processing Systems 16.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graepel%2C%20T.%20Herbrich%2C%20R.%20Invariant%20pattern%20recognition%20by%20semi-definite%20programming%20machines%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graepel%2C%20T.%20Herbrich%2C%20R.%20Invariant%20pattern%20recognition%20by%20semi-definite%20programming%20machines%202004"
        },
        {
            "id": "Hauberg_et+al_2016_a",
            "entry": "Hauberg, S., Freifeld, O., Larsen, A. B. L., Fisher, J., and Hansen, L. (2016). Dreaming more data: Classdependent distributions over diffeomorphisms for learned data augmentation. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hauberg%2C%20S.%20Freifeld%2C%20O.%20Larsen%2C%20A.B.L.%20Fisher%2C%20J.%20Dreaming%20more%20data%3A%20Classdependent%20distributions%20over%20diffeomorphisms%20for%20learned%20data%20augmentation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hauberg%2C%20S.%20Freifeld%2C%20O.%20Larsen%2C%20A.B.L.%20Fisher%2C%20J.%20Dreaming%20more%20data%3A%20Classdependent%20distributions%20over%20diffeomorphisms%20for%20learned%20data%20augmentation%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hensman_et+al_2013_a",
            "entry": "Hensman, J., Fusi, N., and Lawrence, N. D. (2013). Gaussian processes for big data. In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Fusi%2C%20N.%20Lawrence%2C%20N.D.%20Gaussian%20processes%20for%20big%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Fusi%2C%20N.%20Lawrence%2C%20N.D.%20Gaussian%20processes%20for%20big%20data%202013"
        },
        {
            "id": "Hensman_et+al_2015_a",
            "entry": "Hensman, J., Matthews, A. G., Filippone, M., and Ghahramani, Z. (2015a). MCMC for variationally sparse Gaussian processes. In Advances in Neural Information Processing Systems 28.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Filippone%2C%20M.%20Ghahramani%2C%20Z.%20MCMC%20for%20variationally%20sparse%20Gaussian%20processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Filippone%2C%20M.%20Ghahramani%2C%20Z.%20MCMC%20for%20variationally%20sparse%20Gaussian%20processes%202015"
        },
        {
            "id": "Hensman_et+al_2015_b",
            "entry": "Hensman, J., Matthews, A. G., and Ghahramani, Z. (2015b). Scalable variational Gaussian process classification. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Ghahramani%2C%20Z.%20Scalable%20variational%20Gaussian%20process%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Ghahramani%2C%20Z.%20Scalable%20variational%20Gaussian%20process%20classification%202015"
        },
        {
            "id": "Jaderberg_et+al_2015_a",
            "entry": "Jaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K. (2015). Spatial transformer networks. In Advances in Neural Information Processing Systems 28.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Kavukcuoglu%2C%20K.%20Spatial%20transformer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Kavukcuoglu%2C%20K.%20Spatial%20transformer%20networks%202015"
        },
        {
            "id": "Kim_2018_a",
            "entry": "Kim, H. and Teh, Y. W. (2018). Scaling up the Automatic Statistician: Scalable structure discovery using Gaussian processes. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20H.%20Teh%2C%20Y.W.%20Scaling%20up%20the%20Automatic%20Statistician%3A%20Scalable%20structure%20discovery%20using%20Gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20H.%20Teh%2C%20Y.W.%20Scaling%20up%20the%20Automatic%20Statistician%3A%20Scalable%20structure%20discovery%20using%20Gaussian%20processes%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. In Proceedings of the 2nd International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Kondor_2008_a",
            "entry": "Kondor, R. (2008). Group theoretical methods in machine learning. PhD thesis, Columbia University.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kondor%2C%20R.%20Group%20theoretical%20methods%20in%20machine%20learning%202008"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems 25, pages 1097\u20131105. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541\u2013551. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201998"
        },
        {
            "id": "Linderman_et+al_2015_a",
            "entry": "Linderman, S., Johnson, M., and Adams, R. P. (2015). Dependent multinomial models made easy: Stick-breaking with the P\u00f2lya-Gamma augmentation. In Advances in Neural Information Processing Systems 28.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linderman%2C%20S.%20Johnson%2C%20M.%20Adams%2C%20R.P.%20Dependent%20multinomial%20models%20made%20easy%3A%20Stick-breaking%20with%20the%20P%C3%B2lya-Gamma%20augmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linderman%2C%20S.%20Johnson%2C%20M.%20Adams%2C%20R.P.%20Dependent%20multinomial%20models%20made%20easy%3A%20Stick-breaking%20with%20the%20P%C3%B2lya-Gamma%20augmentation%202015"
        },
        {
            "id": "Loosli_et+al_2007_a",
            "entry": "Loosli, G., Canu, S., and Bottou, L. (2007). Training invariant support vector machines using selective sampling.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loosli%2C%20G.%20Canu%2C%20S.%20Bottou%2C%20L.%20Training%20invariant%20support%20vector%20machines%20using%20selective%20sampling%202007"
        },
        {
            "id": "Mackay_1998_a",
            "entry": "Large scale kernel machines, pages 301\u2013320. MacKay, D. J. C. (1998). Introduction to Gaussian processes. In Bishop, C. M., editor, Neural Networks and",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20D.J.C.%20Large%20scale%20kernel%20machines%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacKay%2C%20D.J.C.%20Large%20scale%20kernel%20machines%201998"
        },
        {
            "id": "Press_2002_a",
            "entry": "Machine Learning, NATO ASI Series, pages 133\u2013166. Kluwer Academic Press. MacKay, D. J. C. (2002). Model Comparison and Occam\u2019s Razor. In Information Theory, Inference & Learning",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Press.%20MacKay%2C%20D.J.C.%20Model%20Comparison%20and%20Occam%E2%80%99s%20Razor%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Press.%20MacKay%2C%20D.J.C.%20Model%20Comparison%20and%20Occam%E2%80%99s%20Razor%202002"
        },
        {
            "id": "Matthews_et+al_2016_a",
            "entry": "Matthews, A. G., Hensman, J., Turner, R. E., and Ghahramani, Z. (2016). On sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthews%2C%20A.G.%20Hensman%2C%20J.%20Turner%2C%20R.E.%20Ghahramani%2C%20Z.%20On%20sparse%20variational%20methods%20and%20the%20Kullback-Leibler%20divergence%20between%20stochastic%20processes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matthews%2C%20A.G.%20Hensman%2C%20J.%20Turner%2C%20R.E.%20Ghahramani%2C%20Z.%20On%20sparse%20variational%20methods%20and%20the%20Kullback-Leibler%20divergence%20between%20stochastic%20processes%202016"
        },
        {
            "id": "Niyogi_et+al_1998_a",
            "entry": "Niyogi, P., Girosi, F., and Poggio, T. (1998). Incorporating prior information in machine learning by creating virtual examples. Proceedings of the IEEE, 86(11):2196\u20132209.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niyogi%2C%20P.%20Girosi%2C%20F.%20Poggio%2C%20T.%20Incorporating%20prior%20information%20in%20machine%20learning%20by%20creating%20virtual%20examples%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niyogi%2C%20P.%20Girosi%2C%20F.%20Poggio%2C%20T.%20Incorporating%20prior%20information%20in%20machine%20learning%20by%20creating%20virtual%20examples%201998"
        },
        {
            "id": "Polson_et+al_2013_a",
            "entry": "Polson, N. G., Scott, J. G., and Windle, J. (2013). Bayesian inference for logistic models using P\u00f2lya\u2013Gamma latent variables. Journal of the American Statistical Association, 108(504):1339\u20131349.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polson%2C%20N.G.%20Scott%2C%20J.G.%20Windle%2C%20J.%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B2lya%E2%80%93Gamma%20latent%20variables%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polson%2C%20N.G.%20Scott%2C%20J.G.%20Windle%2C%20J.%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B2lya%E2%80%93Gamma%20latent%20variables%202013"
        },
        {
            "id": "Qui_2005_a",
            "entry": "Qui\u00f1onero-Candela, J. and Rasmussen, C. E. (2005). A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6:1939\u20131959.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005"
        },
        {
            "id": "Rasmussen_2001_a",
            "entry": "Rasmussen, C. E. and Ghahramani, Z. (2001). Occam\u2019s razor. In Advances in Neural Information Processing Systems 13.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%20C%20E%20and%20Ghahramani%20Z%202001%20Occams%20razor%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%20C%20E%20and%20Ghahramani%20Z%202001%20Occams%20razor%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2013"
        },
        {
            "id": "Rasmussen_2005_a",
            "entry": "Rasmussen, C. E. and Williams, C. K. I. (2005). Model selection and adaptation of hyperparameters. In Gaussian Processes for Machine Learning, chapter 5. The MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Model%20selection%20and%20adaptation%20of%20hyperparameters%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Model%20selection%20and%20adaptation%20of%20hyperparameters%202005"
        },
        {
            "id": "Schoelkopf_et+al_1998_a",
            "entry": "Sch\u00f6lkopf, B., Simard, P., Smola, A., and Vapnik, V. (1998). Prior knowledge in support vector kernels. In Advances in Neural Information Processing Systems 10.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20B.%20Simard%2C%20P.%20Smola%2C%20A.%20Vapnik%2C%20V.%20Prior%20knowledge%20in%20support%20vector%20kernels%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6lkopf%2C%20B.%20Simard%2C%20P.%20Smola%2C%20A.%20Vapnik%2C%20V.%20Prior%20knowledge%20in%20support%20vector%20kernels%201998"
        },
        {
            "id": "Seeger_2003_a",
            "entry": "Seeger, M. (2003). Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations. PhD thesis, University of Edinburgh.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20M.%20Bayesian%20Gaussian%20Process%20Models%3A%20PAC-Bayesian%20Generalisation%20Error%20Bounds%20and%20Sparse%20Approximations%202003"
        },
        {
            "id": "Simard_et+al_1992_a",
            "entry": "Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent Prop - a formalism for specifying selected invariances in an adaptive network. In Advances in Neural Information Processing Systems 4.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simard%2C%20P.%20Victorri%2C%20B.%20LeCun%2C%20Y.%20Denker%2C%20J.%20Tangent%20Prop%20-%20a%20formalism%20for%20specifying%20selected%20invariances%20in%20an%20adaptive%20network%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simard%2C%20P.%20Victorri%2C%20B.%20LeCun%2C%20Y.%20Denker%2C%20J.%20Tangent%20Prop%20-%20a%20formalism%20for%20specifying%20selected%20invariances%20in%20an%20adaptive%20network%201992"
        },
        {
            "id": "Simard_et+al_2000_a",
            "entry": "Simard, P. Y., Le Cun, Y. A., Denker, J. S., and Victorri, B. (2000). Transformation invariance in pattern recognition: Tangent distance and propagation. International Journal of Imaging Systems and Technology, 11(3):181\u2013197.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simard%2C%20P.Y.%20Le%20Cun%2C%20Y.A.%20Denker%2C%20J.S.%20Victorri%2C%20B.%20Transformation%20invariance%20in%20pattern%20recognition%3A%20Tangent%20distance%20and%20propagation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simard%2C%20P.Y.%20Le%20Cun%2C%20Y.A.%20Denker%2C%20J.S.%20Victorri%2C%20B.%20Transformation%20invariance%20in%20pattern%20recognition%3A%20Tangent%20distance%20and%20propagation%202000"
        },
        {
            "id": "Titsias_2009_a",
            "entry": "Titsias, M. K. (2009). Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20M.K.%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20M.K.%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009"
        },
        {
            "id": "Turner_2011_a",
            "entry": "Turner, R. E. and Sahani, M. (2011). Two problems with variational expectation maximisation for time-series models. In Barber, D., Cemgil, T., and Chiappa, S., editors, Bayesian Time Series Models, chapter 5, pages 109\u2013130. Cambridge University Press. van der Wilk, M., Rasmussen, C. E., and Hensman, J. (2017). Convolutional Gaussian Processes. Advances in Neural Information Processing Systems 30.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Turner%2C%20R.E.%20Sahani%2C%20M.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Turner%2C%20R.E.%20Sahani%2C%20M.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011"
        },
        {
            "id": "Wenzel_et+al_2018_a",
            "entry": "Wenzel, F., Galy-Fajou, T., Donner, C., Kloft, M., and Opper, M. (2018). Efficient Gaussian process classification using P\u00f2lya-Gamma data augmentation. arXiv preprint arXiv:1802.06383.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06383"
        }
    ]
}
