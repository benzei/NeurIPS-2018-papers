{
    "filename": "7839-stacked-semantics-guided-attention-model-for-fine-grained-zero-shot-learning.pdf",
    "metadata": {
        "title": "Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning",
        "author": "yunlong yu, Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei (Mark) Zhang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7839-stacked-semantics-guided-attention-model-for-fine-grained-zero-shot-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Zero-Shot Learning (ZSL) is generally achieved via aligning the semantic relationships between the visual features and the corresponding class semantic descriptions. However, using the global features to represent fine-grained images may lead to sub-optimal results since they neglect the discriminative differences of local regions. Besides, different regions contain distinct discriminative information. The important regions should contribute more to the prediction. To this end, we propose a novel stacked semantics-guided attention (S2GA) model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions. Feeding both the integrated visual features and the class semantic features into a multi-class classification architecture, the proposed framework can be trained end-to-end. Extensive experimental results on CUB and NABird datasets show that the proposed approach has a consistent improvement on both fine-grained zero-shot classification and retrieval tasks."
    },
    "keywords": [
        {
            "term": "national natural science foundation",
            "url": "https://en.wikipedia.org/wiki/National_Natural_Science_Foundation"
        },
        {
            "term": "Principal Component Analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_Component_Analysis"
        }
    ],
    "highlights": [
        "Traditional object classification tasks require the test classes to be identical or a subset of the training classes",
        "To effectively obtain the attention map to distribute weights for local region features, we propose a stacked attention network guided by the class semantic features for Zero-Shot Learning",
        "Different from [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], we argue that important regions should contribute more to the prediction and design an attention method to distribute different weights for different regions according to their relevance with class semantic features, and integrate both the global visual features and the weighted region features into more semantics-relevant features to represent images.\n2.2",
        "We have proposed a stacked semantics-guided attention approach for fine-grained zero-shot learning",
        "It progressively assigns weights for different region features guided by class semantic descriptions and integrates both the local and global features to obtain semantic-relevant representations for images",
        "Experimental results on zero-shot classification and retrieval tasks show the impressive effectiveness of the proposed approach"
    ],
    "key_statements": [
        "Traditional object classification tasks require the test classes to be identical or a subset of the training classes",
        "To effectively obtain the attention map to distribute weights for local region features, we propose a stacked attention network guided by the class semantic features for Zero-Shot Learning",
        "We evaluate the proposed S2GA framework for fine-grained Zero-Shot Learning on two bird datasets: Caltech UCSD Birds-2011 (CUB) [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], and North America Birds (NABird) [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "Different from [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], we argue that important regions should contribute more to the prediction and design an attention method to distribute different weights for different regions according to their relevance with class semantic features, and integrate both the global visual features and the weighted region features into more semantics-relevant features to represent images.\n2.2",
        "We propose to indirectly learn the attention maps to weight different regions guided by the class semantic descriptions during training",
        "To narrow the semantic gap between the visual and the class semantic modalities, we propose a semantic-guided attention approach to use the class semantic descriptions to guide the local region features to obtain more semantic-relevant visual features for the subsequent visual-semantic matching",
        "We find that the performances of the methods with attention mechanism are much better than those without attention mechanism on both Caltech UCSD Birds-2011 and North America Birds datasets, which verifies the effectiveness of the attention mechanism",
        "We have proposed a stacked semantics-guided attention approach for fine-grained zero-shot learning",
        "It progressively assigns weights for different region features guided by class semantic descriptions and integrates both the local and global features to obtain semantic-relevant representations for images",
        "Experimental results on zero-shot classification and retrieval tasks show the impressive effectiveness of the proposed approach",
        "This work was supported by the National Natural Science Foundation of China under Grant 61771329, the National Basic Research Program of China under Grant 2014CB340403, the National Natural Science Foundation of China under Grant 61632018"
    ],
    "summary": [
        "Traditional object classification tasks require the test classes to be identical or a subset of the training classes.",
        "2. To effectively obtain the attention map to distribute weights for local region features, we propose a stacked attention network guided by the class semantic features for ZSL.",
        "We design a stacked attention network to assign different importance weights to features of different local regions to obtain a more semantics-relevant feature representation.",
        "Given the local features of an image and its corresponding class semantic vector, the attention networks distribute different weights for each visual region vector via multi-step attention layers, and integrate both the global and weighted local features to obtain more semantics-relevant representations for images.",
        "We propose an attention approach using the local region features to gradually filter out noises and weight the regions that are highly relevant to the class semantic descriptions via multiple attention layers.",
        "Given the image lo-Region features cal feature representations VI and its corresponding class semantic vector s, the attention map is obtained with two separated net-VI works.",
        "Compared with the approaches that use the global image vector, the attention method constructs a more informative u since higher weights are put on the visual regions that are more relevant to the class semantic descriptions.",
        "To compare with the existing methods more fairly, we select five state-of-the-art approaches that use both the same visual representations and class semantic representations on both CUB and NABird datasets.",
        "To evaluate the effectiveness of the proposed attention mechanism, we conduct experiments on both CUB and NABird datasets using local features based on detected regions as visual representations under the SCS split setting.",
        "We use the above well trained method to embed both all the images of unseen classes and the class semantic descriptions into the integrated feature space spanned both the global features and the weighted local features where the semantic similarities of the visual and class semantic representations are obtained.",
        "For comparing with the competitors fairly, we use both the same feature representations as well as the same settings as that in [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] where 50% and 100% of the number of images for each class from the whole dataset are ranked based on their final semantic similarity scores.",
        "It progressively assigns weights for different region features guided by class semantic descriptions and integrates both the local and global features to obtain semantic-relevant representations for images.",
        "The authors are very grateful for NVIDIA\u2019s support in providing GPUs that made this work possible"
    ],
    "headline": "We propose a novel stacked semantics-guided attention  model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Zeynep Akata, Mateusz Malinowski, Mario Fritz, and Bernt Schiele. Multi-cue zero-shot learning with strong supervision. In CVPR, pages 59\u201368, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akata%2C%20Zeynep%20Malinowski%2C%20Mateusz%20Fritz%2C%20Mario%20Schiele%2C%20Bernt%20Multi-cue%20zero-shot%20learning%20with%20strong%20supervision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akata%2C%20Zeynep%20Malinowski%2C%20Mateusz%20Fritz%2C%20Mario%20Schiele%2C%20Bernt%20Multi-cue%20zero-shot%20learning%20with%20strong%20supervision%202016"
        },
        {
            "id": "2",
            "entry": "[2] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for image classification. TPAMI, 38(7):1425\u20131438, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akata%2C%20Zeynep%20Perronnin%2C%20Florent%20Harchaoui%2C%20Zaid%20Schmid%2C%20Cordelia%20Label-embedding%20for%20image%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akata%2C%20Zeynep%20Perronnin%2C%20Florent%20Harchaoui%2C%20Zaid%20Schmid%2C%20Cordelia%20Label-embedding%20for%20image%20classification%202016"
        },
        {
            "id": "3",
            "entry": "[3] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for fine-grained image classification. In CVPR, pages 2927\u20132936, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akata%2C%20Zeynep%20Reed%2C%20Scott%20Walter%2C%20Daniel%20Lee%2C%20Honglak%20Evaluation%20of%20output%20embeddings%20for%20fine-grained%20image%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akata%2C%20Zeynep%20Reed%2C%20Scott%20Walter%2C%20Daniel%20Lee%2C%20Honglak%20Evaluation%20of%20output%20embeddings%20for%20fine-grained%20image%20classification%202015"
        },
        {
            "id": "4",
            "entry": "[4] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual attention. Computer Science, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Multiple%20object%20recognition%20with%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Multiple%20object%20recognition%20with%20visual%20attention%202014"
        },
        {
            "id": "5",
            "entry": "[5] Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei Sha. Synthesized classifiers for zero-shot learning. In CVPR, pages 5327\u20135336, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Changpinyo%2C%20Soravit%20Chao%2C%20Wei-Lun%20Gong%2C%20Boqing%20Sha%2C%20Fei%20Synthesized%20classifiers%20for%20zero-shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Changpinyo%2C%20Soravit%20Chao%2C%20Wei-Lun%20Gong%2C%20Boqing%20Sha%2C%20Fei%20Synthesized%20classifiers%20for%20zero-shot%20learning%202016"
        },
        {
            "id": "6",
            "entry": "[6] Mohamed Elhoseiny, Yizhe Zhu, Han Zhang, and Ahmed Elgammal. Link the head to the \u201cbeak\u201d: Zero shot learning from noisy text description at part precision. In CVPR, pages 6288\u20136297, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elhoseiny%2C%20Mohamed%20Zhu%2C%20Yizhe%20Zhang%2C%20Han%20Elgammal%2C%20Ahmed%20Link%20the%20head%20to%20the%20%E2%80%9Cbeak%E2%80%9D%3A%20Zero%20shot%20learning%20from%20noisy%20text%20description%20at%20part%20precision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elhoseiny%2C%20Mohamed%20Zhu%2C%20Yizhe%20Zhang%2C%20Han%20Elgammal%2C%20Ahmed%20Link%20the%20head%20to%20the%20%E2%80%9Cbeak%E2%80%9D%3A%20Zero%20shot%20learning%20from%20noisy%20text%20description%20at%20part%20precision%202017"
        },
        {
            "id": "7",
            "entry": "[7] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In NIPS, pages 2121\u20132129, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013"
        },
        {
            "id": "8",
            "entry": "[8] Yanwei Fu, Timothy M Hospedales, Tao Xiang, and Shaogang Gong. Transductive multi-view zero-shot learning. TPAMI, 37(11):2332\u20132345, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Yanwei%20Hospedales%2C%20Timothy%20M.%20Xiang%2C%20Tao%20Gong%2C%20Shaogang%20Transductive%20multi-view%20zero-shot%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Yanwei%20Hospedales%2C%20Timothy%20M.%20Xiang%2C%20Tao%20Gong%2C%20Shaogang%20Transductive%20multi-view%20zero-shot%20learning%202015"
        },
        {
            "id": "9",
            "entry": "[9] Dinesh Jayaraman and Kristen Grauman. Zero-shot recognition with unreliable attributes. In NIPS, pages 3464\u20133472, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jayaraman%2C%20Dinesh%20Grauman%2C%20Kristen%20Zero-shot%20recognition%20with%20unreliable%20attributes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jayaraman%2C%20Dinesh%20Grauman%2C%20Kristen%20Zero-shot%20recognition%20with%20unreliable%20attributes%202014"
        },
        {
            "id": "10",
            "entry": "[10] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation. ACL, 5(1):339\u2013351, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Melvin%20Schuster%2C%20Mike%20Le%2C%20Quoc%20V.%20Krikun%2C%20Maxim%20Google%E2%80%99s%20multilingual%20neural%20machine%20translation%20system%3A%20Enabling%20zero-shot%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Melvin%20Schuster%2C%20Mike%20Le%2C%20Quoc%20V.%20Krikun%2C%20Maxim%20Google%E2%80%99s%20multilingual%20neural%20machine%20translation%20system%3A%20Enabling%20zero-shot%20translation%202017"
        },
        {
            "id": "11",
            "entry": "[11] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zero-shot visual object categorization. TPAMI, 36(3):453\u2013465, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampert%2C%20Christoph%20H.%20Nickisch%2C%20Hannes%20Harmeling%2C%20Stefan%20Attribute-based%20classification%20for%20zero-shot%20visual%20object%20categorization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampert%2C%20Christoph%20H.%20Nickisch%2C%20Hannes%20Harmeling%2C%20Stefan%20Attribute-based%20classification%20for%20zero-shot%20visual%20object%20categorization%202014"
        },
        {
            "id": "12",
            "entry": "[12] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "13",
            "entry": "[13] Pedro Morgado and Nuno Vasconcelos. Semantically consistent regularization for zero-shot recognition. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morgado%2C%20Pedro%20Vasconcelos%2C%20Nuno%20Semantically%20consistent%20regularization%20for%20zero-shot%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morgado%2C%20Pedro%20Vasconcelos%2C%20Nuno%20Semantically%20consistent%20regularization%20for%20zero-shot%20recognition%202017"
        },
        {
            "id": "14",
            "entry": "[14] Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and Jakob Verbeek. Areas of attention for image captioning. In ICCV, pages 1251\u20131259, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedersoli%2C%20Marco%20Lucas%2C%20Thomas%20Schmid%2C%20Cordelia%20Verbeek%2C%20Jakob%20Areas%20of%20attention%20for%20image%20captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedersoli%2C%20Marco%20Lucas%2C%20Thomas%20Schmid%2C%20Cordelia%20Verbeek%2C%20Jakob%20Areas%20of%20attention%20for%20image%20captioning%202017"
        },
        {
            "id": "15",
            "entry": "[15] Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, and Anton van den Hengel. Less is more: zero-shot learning from online textual documents with noise suppression. In CVPR, pages 2249\u20132257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qiao%2C%20Ruizhi%20Liu%2C%20Lingqiao%20Shen%2C%20Chunhua%20van%20den%20Hengel%2C%20Anton%20Less%20is%20more%3A%20zero-shot%20learning%20from%20online%20textual%20documents%20with%20noise%20suppression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qiao%2C%20Ruizhi%20Liu%2C%20Lingqiao%20Shen%2C%20Chunhua%20van%20den%20Hengel%2C%20Anton%20Less%20is%20more%3A%20zero-shot%20learning%20from%20online%20textual%20documents%20with%20noise%20suppression%202016"
        },
        {
            "id": "16",
            "entry": "[16] Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In ICML, pages 2152\u20132161, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romera-Paredes%2C%20Bernardino%20Torr%2C%20Philip%20An%20embarrassingly%20simple%20approach%20to%20zero-shot%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romera-Paredes%2C%20Bernardino%20Torr%2C%20Philip%20An%20embarrassingly%20simple%20approach%20to%20zero-shot%20learning%202015"
        },
        {
            "id": "17",
            "entry": "[17] Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513\u2013523, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salton%2C%20Gerard%20Buckley%2C%20Christopher%20Term-weighting%20approaches%20in%20automatic%20text%20retrieval%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salton%2C%20Gerard%20Buckley%2C%20Christopher%20Term-weighting%20approaches%20in%20automatic%20text%20retrieval%201987"
        },
        {
            "id": "18",
            "entry": "[18] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202014"
        },
        {
            "id": "19",
            "entry": "[19] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning through cross-modal transfer. In NIPS, pages 935\u2013943, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Ganjoo%2C%20Milind%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Zero-shot%20learning%20through%20cross-modal%20transfer%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Ganjoo%2C%20Milind%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Zero-shot%20learning%20through%20cross-modal%20transfer%202013"
        },
        {
            "id": "20",
            "entry": "[20] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018"
        },
        {
            "id": "21",
            "entry": "[21] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. In CVPR, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "22",
            "entry": "[22] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, pages 595\u2013604, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horn%2C%20Grant%20Van%20Branson%2C%20Steve%20Farrell%2C%20Ryan%20Haber%2C%20Scott%20Building%20a%20bird%20recognition%20app%20and%20large%20scale%20dataset%20with%20citizen%20scientists%3A%20The%20fine%20print%20in%20fine-grained%20dataset%20collection%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Horn%2C%20Grant%20Van%20Branson%2C%20Steve%20Farrell%2C%20Ryan%20Haber%2C%20Scott%20Building%20a%20bird%20recognition%20app%20and%20large%20scale%20dataset%20with%20citizen%20scientists%3A%20The%20fine%20print%20in%20fine-grained%20dataset%20collection%202015"
        },
        {
            "id": "23",
            "entry": "[23] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wah%2C%20Catherine%20Branson%2C%20Steve%20Welinder%2C%20Peter%20Perona%2C%20Pietro%20The%20caltech-ucsd%20birds-200-2011%202011"
        },
        {
            "id": "24",
            "entry": "[24] Peng Wang, Lingqiao Liu, Chunhua Shen, Zi Huang, Anton van den Hengel, and Heng Tao Shen. Multiattention network for one shot learning. In CVPR, pages 22\u201325, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Peng%20Liu%2C%20Lingqiao%20Shen%2C%20Chunhua%20Huang%2C%20Zi%20Multiattention%20network%20for%20one%20shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Peng%20Liu%2C%20Lingqiao%20Shen%2C%20Chunhua%20Huang%2C%20Zi%20Multiattention%20network%20for%20one%20shot%20learning%202017"
        },
        {
            "id": "25",
            "entry": "[25] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In ECCV, pages 451\u2013466, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huijuan%20Ask%2C%20Kate%20Saenko%20attend%20and%20answer%3A%20Exploring%20question-guided%20spatial%20attention%20for%20visual%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huijuan%20Ask%2C%20Kate%20Saenko%20attend%20and%20answer%3A%20Exploring%20question-guided%20spatial%20attention%20for%20visual%20question%20answering%202016"
        },
        {
            "id": "26",
            "entry": "[26] Xing Xu, Fumin Shen, Yang Yang, Dongxiang Zhang, Heng Tao Shen, and Jingkuan Song. Matrix tri-factorization with manifold regularizations for zero-shot learning. In CVPR, pages 2007\u20132016, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Xing%20Shen%2C%20Fumin%20Yang%2C%20Yang%20Zhang%2C%20Dongxiang%20Matrix%20tri-factorization%20with%20manifold%20regularizations%20for%20zero-shot%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Xing%20Shen%2C%20Fumin%20Yang%2C%20Yang%20Zhang%2C%20Dongxiang%20Matrix%20tri-factorization%20with%20manifold%20regularizations%20for%20zero-shot%20learning%202007"
        },
        {
            "id": "27",
            "entry": "[27] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, pages 21\u201329, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zichao%20He%2C%20Xiaodong%20Gao%2C%20Jianfeng%20Deng%2C%20Li%20Stacked%20attention%20networks%20for%20image%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zichao%20He%2C%20Xiaodong%20Gao%2C%20Jianfeng%20Deng%2C%20Li%20Stacked%20attention%20networks%20for%20image%20question%20answering%202016"
        },
        {
            "id": "28",
            "entry": "[28] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In CVPR, pages 4651\u20134659, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=You%2C%20Quanzeng%20Jin%2C%20Hailin%20Wang%2C%20Zhaowen%20Fang%2C%20Chen%20Image%20captioning%20with%20semantic%20attention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=You%2C%20Quanzeng%20Jin%2C%20Hailin%20Wang%2C%20Zhaowen%20Fang%2C%20Chen%20Image%20captioning%20with%20semantic%20attention%202016"
        },
        {
            "id": "29",
            "entry": "[29] Dongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui. Multi-level attention networks for visual question answering. In CVPR, pages 4187\u20134195, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Dongfei%20Fu%2C%20Jianlong%20Mei%2C%20Tao%20Rui%2C%20Yong%20Multi-level%20attention%20networks%20for%20visual%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Dongfei%20Fu%2C%20Jianlong%20Mei%2C%20Tao%20Rui%2C%20Yong%20Multi-level%20attention%20networks%20for%20visual%20question%20answering%202017"
        },
        {
            "id": "30",
            "entry": "[30] Y. Yu, Z. Ji, X. Li, J. Guo, Z. Zhang, H. Ling, and F. Wu. Transductive zero-shot learning with a self-training dictionary approach. IEEE Transactions on Cybernetics, (99):1\u201312, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Y.%20Ji%2C%20Z.%20Li%2C%20X.%20Guo%2C%20J.%20Transductive%20zero-shot%20learning%20with%20a%20self-training%20dictionary%20approach%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Y.%20Ji%2C%20Z.%20Li%2C%20X.%20Guo%2C%20J.%20Transductive%20zero-shot%20learning%20with%20a%20self-training%20dictionary%20approach%202018"
        },
        {
            "id": "31",
            "entry": "[31] Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang, Shaoting Zhang, Ahmed Elgammal, and Dimitris Metaxas. Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition. In CVPR, pages 1143\u20131152, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Han%20Xu%2C%20Tao%20Elhoseiny%2C%20Mohamed%20Huang%2C%20Xiaolei%20Spda-cnn%3A%20Unifying%20semantic%20part%20detection%20and%20abstraction%20for%20fine-grained%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Han%20Xu%2C%20Tao%20Elhoseiny%2C%20Mohamed%20Huang%2C%20Xiaolei%20Spda-cnn%3A%20Unifying%20semantic%20part%20detection%20and%20abstraction%20for%20fine-grained%20recognition%202016"
        },
        {
            "id": "32",
            "entry": "[32] Li Zhang, Tao Xiang, Shaogang Gong, et al. Learning a deep embedding model for zero-shot learning. In CVPR, pages 3010\u20133019, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Li%20Xiang%2C%20Tao%20Gong%2C%20Shaogang%20Learning%20a%20deep%20embedding%20model%20for%20zero-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Li%20Xiang%2C%20Tao%20Gong%2C%20Shaogang%20Learning%20a%20deep%20embedding%20model%20for%20zero-shot%20learning%202017"
        },
        {
            "id": "33",
            "entry": "[33] Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, and Ahmed Elgammal. Imagine it for me: Generative adversarial approach for zero-shot learning from noisy texts. In CVPR, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Yizhe%20Elhoseiny%2C%20Mohamed%20Liu%2C%20Bingchen%20Elgammal%2C%20Ahmed%20Imagine%20it%20for%20me%3A%20Generative%20adversarial%20approach%20for%20zero-shot%20learning%20from%20noisy%20texts%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Yizhe%20Elhoseiny%2C%20Mohamed%20Liu%2C%20Bingchen%20Elgammal%2C%20Ahmed%20Imagine%20it%20for%20me%3A%20Generative%20adversarial%20approach%20for%20zero-shot%20learning%20from%20noisy%20texts%202018"
        }
    ]
}
