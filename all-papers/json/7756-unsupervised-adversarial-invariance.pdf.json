{
    "filename": "7756-unsupervised-adversarial-invariance.pdf",
    "metadata": {
        "title": "Unsupervised Adversarial Invariance",
        "author": "Ayush Jaiswal, Rex Yue Wu, Wael Abd-Almageed, Prem Natarajan",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7756-unsupervised-adversarial-invariance.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality."
    },
    "keywords": [
        {
            "term": "Deep neural networks",
            "url": "https://en.wikipedia.org/wiki/Deep_neural_networks"
        },
        {
            "term": "multi-task learning",
            "url": "https://en.wikipedia.org/wiki/multi-task_learning"
        },
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        }
    ],
    "highlights": [
        "Arguably the most popular branch of machine learning, involves estimating a mapping from data samples (x) to target variables (y)",
        "We provide results on three tasks involving a diverse collection of datasets \u2014 (1) invariance to inherent nuisance factors, (2) effective use of synthetic data augmentation for learning invariance and (3) domain adaptation",
        "We have presented a novel unsupervised framework for invariance induction in neural networks",
        "Our method models invariance as an information separation task achieved by competitive training between a predictor and a decoder coupled with disentanglement",
        "Experimental evaluation shows that our unsupervised adversarial invariance induction model outperforms state-of-the-art methods, which are supervised, on learning invariance to inherent nuisance factors, effectively using synthetic data augmentation for learning invariance, and domain adaptation",
        "The fact that our framework requires no annotations for variations of nuisance factors, or even knowledge of such factors, shows the conceptual superiority of our approach compared to previous methods"
    ],
    "key_statements": [
        "Arguably the most popular branch of machine learning, involves estimating a mapping from data samples (x) to target variables (y)",
        "We present a novel unsupervised framework for invariance induction that overcomes the drawbacks of previous methods",
        "We provide results on three tasks involving a diverse collection of datasets \u2014 (1) invariance to inherent nuisance factors, (2) effective use of synthetic data augmentation for learning invariance and (3) domain adaptation",
        "We provide experimental results on three tasks relevant to invariant feature learning for improved prediction of target variables: (1) invariance to inherent nuisance factors, (2) effective use of synthetic data augmentation for learning invariance, and (3) domain adaptation through learning invariance to \u201cdomain\u201d information",
        "Table 5 shows the results of the proposed unsupervised adversarial model and supervised state-of-the-art methods Variational Fair Autoencoder and Domain Adversarial Neural Network (DANN) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "We have presented a novel unsupervised framework for invariance induction in neural networks",
        "Our method models invariance as an information separation task achieved by competitive training between a predictor and a decoder coupled with disentanglement",
        "We described an adversarial instantiation of this framework and provided analysis of its working",
        "Experimental evaluation shows that our unsupervised adversarial invariance induction model outperforms state-of-the-art methods, which are supervised, on learning invariance to inherent nuisance factors, effectively using synthetic data augmentation for learning invariance, and domain adaptation",
        "The fact that our framework requires no annotations for variations of nuisance factors, or even knowledge of such factors, shows the conceptual superiority of our approach compared to previous methods",
        "Since our model does not make any assumptions about the data, it can be applied to any supervised learning task, eg., binary/multi-class classification or regression, without loss of generality"
    ],
    "summary": [
        "Arguably the most popular branch of machine learning, involves estimating a mapping from data samples (x) to target variables (y).",
        "Models trained na\u00efvely on the augmented dataset become robust to limited forms of nuisance by learning to associate every seen variation of such factors to the target variables.",
        "We describe a generalized framework for unsupervised induction of invariance to nuisance factors by disentangling information required for predicting y from other unrelated information contained in x through the incorporation of data reconstruction as a competing task for the primary prediction task and a disentanglement term in the training objective.",
        "We provide experimental results on three tasks relevant to invariant feature learning for improved prediction of target variables: (1) invariance to inherent nuisance factors, (2) effective use of synthetic data augmentation for learning invariance, and (3) domain adaptation through learning invariance to \u201cdomain\u201d information.",
        "We provide results of our framework at the task of learning invariance to inherent nuisance factors on two datasets \u2013 Extended Yale-B [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] and Chairs [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>].",
        "We use two variants of the MNIST [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] dataset of handwritten digits to (1) show the advantage of unsupervised invariance induction at this task over its supervised variant through comparison with CAI, and (2) perform ablation experiments for our model to justify our framework design.",
        "Table 3 summarizes the results, showing that our unsupervised adversarial model not only performs better than the baseline ablation versions but outperforms CAI, which uses supervised information about the rotation angle.",
        "We use models trained on MNIST-ROT to report evaluation results on this dataset, to show the advantage of unsupervised invariance induction in cases where certain z are not annotated",
        "CAI performs significantly worse than our baseline models, indicating that the supervised approach of invariance induction can worsen performance with respect to nuisance factors not accounted for during training.",
        "Table 5 shows the results of the proposed unsupervised adversarial model and supervised state-of-the-art methods VFAE and Domain Adversarial Neural Network (DANN) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].",
        "Our method models invariance as an information separation task achieved by competitive training between a predictor and a decoder coupled with disentanglement.",
        "Experimental evaluation shows that our unsupervised adversarial invariance induction model outperforms state-of-the-art methods, which are supervised, on learning invariance to inherent nuisance factors, effectively using synthetic data augmentation for learning invariance, and domain adaptation.",
        "We will augment our model with the capability to use supervised information about known nuisance factors for learning invariance to them, which will, help our model learn fair representations.",
        "Since our model does not make any assumptions about the data, it can be applied to any supervised learning task, eg., binary/multi-class classification or regression, without loss of generality"
    ],
    "headline": "We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04340"
        },
        {
            "id": "2",
            "entry": "[2] Mathieu Aubry, Daniel Maturana, Alexei Efros, Bryan C. Russell, and Josef Sivic. Seeing 3d chairs: Exemplar part-based 2d-3d alignment using a large dataset of cad models. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 06 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aubry%2C%20Mathieu%20Maturana%2C%20Daniel%20Efros%2C%20Alexei%20Russell%2C%20Bryan%20C.%20Seeing%203d%20chairs%3A%20Exemplar%20part-based%202d-3d%20alignment%20using%20a%20large%20dataset%20of%20cad%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aubry%2C%20Mathieu%20Maturana%2C%20Daniel%20Efros%2C%20Alexei%20Russell%2C%20Bryan%20C.%20Seeing%203d%20chairs%3A%20Exemplar%20part-based%202d-3d%20alignment%20using%20a%20large%20dataset%20of%20cad%20models%202014"
        },
        {
            "id": "3",
            "entry": "[3] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "4",
            "entry": "[4] Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. In Proceedings of the 29th International Conference on Machine Learning, ICML\u201912, pages 1627\u20131634, USA, 2012. Omnipress.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Minmin%20Xu%2C%20Zhixiang%20Weinberger%2C%20Kilian%20Q.%20Sha%2C%20Fei%20Marginalized%20denoising%20autoencoders%20for%20domain%20adaptation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Minmin%20Xu%2C%20Zhixiang%20Weinberger%2C%20Kilian%20Q.%20Sha%2C%20Fei%20Marginalized%20denoising%20autoencoders%20for%20domain%20adaptation%202012"
        },
        {
            "id": "5",
            "entry": "[5] Iacopo Masi et al. Learning pose-aware models for pose-invariant face recognition in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Masi%2C%20Iacopo%20Learning%20pose-aware%20models%20for%20pose-invariant%20face%20recognition%20in%20the%20wild%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Masi%2C%20Iacopo%20Learning%20pose-aware%20models%20for%20pose-invariant%20face%20recognition%20in%20the%20wild%202018"
        },
        {
            "id": "6",
            "entry": "[6] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096\u20132030, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016"
        },
        {
            "id": "7",
            "entry": "[7] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. From few to many: illumination cone models for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6):643\u2013660, Jun 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Georghiades%2C%20A.S.%20Belhumeur%2C%20P.N.%20Kriegman%2C%20D.J.%20From%20few%20to%20many%3A%20illumination%20cone%20models%20for%20face%20recognition%20under%20variable%20lighting%20and%20pose%202001-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Georghiades%2C%20A.S.%20Belhumeur%2C%20P.N.%20Kriegman%2C%20D.J.%20From%20few%20to%20many%3A%20illumination%20cone%20models%20for%20face%20recognition%20under%20variable%20lighting%20and%20pose%202001-06"
        },
        {
            "id": "8",
            "entry": "[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20Adversarial%20Nets%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%2026722680%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20Adversarial%20Nets%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%2026722680%202014"
        },
        {
            "id": "9",
            "entry": "[9] Ayush Jaiswal, Dong Guo, Cauligi S Raghavendra, and Paul Thompson. Large-scale unsupervised deep representation learning for brain structure. arXiv preprint arXiv:1805.01049, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01049"
        },
        {
            "id": "10",
            "entry": "[10] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. Audio augmentation for speech recognition. In Sixteenth Annual Conference of the International Speech Communication Association, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ko%2C%20Tom%20Peddinti%2C%20Vijayaditya%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20Audio%20augmentation%20for%20speech%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ko%2C%20Tom%20Peddinti%2C%20Vijayaditya%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20Audio%20augmentation%20for%20speech%20recognition%202015"
        },
        {
            "id": "11",
            "entry": "[11] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "12",
            "entry": "[12] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20Learning%20Applied%20to%20Document%20Recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20Learning%20Applied%20to%20Document%20Recognition%201998"
        },
        {
            "id": "13",
            "entry": "[13] Yujia Li, Kevin Swersky, and Richard Zemel. Learning unbiased features. arXiv preprint arXiv:1412.5244, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.5244"
        },
        {
            "id": "14",
            "entry": "[14] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zeme. The variational fair autoencoder. In Proceedings of International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Swersky%2C%20Kevin%20Li%2C%20Yujia%20Welling%2C%20Max%20The%20variational%20fair%20autoencoder%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Swersky%2C%20Kevin%20Li%2C%20Yujia%20Welling%2C%20Max%20The%20variational%20fair%20autoencoder%202016"
        },
        {
            "id": "15",
            "entry": "[15] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "16",
            "entry": "[16] Jianyu Miao and Lingfeng Niu. A survey on feature selection. Procedia Computer Science, 91:919 \u2013 926, 2016. Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miao%2C%20Jianyu%20Niu%2C%20Lingfeng%20A%20survey%20on%20feature%20selection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miao%2C%20Jianyu%20Niu%2C%20Lingfeng%20A%20survey%20on%20feature%20selection%202016"
        },
        {
            "id": "17",
            "entry": "[17] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05098"
        },
        {
            "id": "18",
            "entry": "[18] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "19",
            "entry": "[19] Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance through adversarial feature learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 585\u2013596. Curran Associates, Inc., 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Qizhe%20Dai%2C%20Zihang%20Du%2C%20Yulun%20Hovy%2C%20Eduard%20Controllable%20invariance%20through%20adversarial%20feature%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Qizhe%20Dai%2C%20Zihang%20Du%2C%20Yulun%20Hovy%2C%20Eduard%20Controllable%20invariance%20through%20adversarial%20feature%20learning%202017"
        }
    ]
}
