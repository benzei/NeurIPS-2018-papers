{
    "filename": "8126-learning-small-predictors.pdf",
    "metadata": {
        "title": "Learning SMaLL Predictors",
        "author": "Vikas Garg, Ofer Dekel, Lin Xiao",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8126-learning-small-predictors.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce a new framework for learning in severely resource-constrained settings. Our technique delicately amalgamates the representational richness of multiple linear predictors with the sparsity of Boolean relaxations, and thereby yields classifiers that are compact, interpretable, and accurate. We provide a rigorous formalism of the learning problem, and establish fast convergence of the ensuing algorithm via relaxation to a minimax saddle point objective. We supplement the theoretical foundations of our work with an extensive empirical evaluation."
    },
    "keywords": [
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "Gradient Boosting",
            "url": "https://en.wikipedia.org/wiki/Gradient_Boosting"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "disjunctive normal forms",
            "url": "https://en.wikipedia.org/wiki/Disjunctive_Normal_Form"
        },
        {
            "term": "Gaussian Process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_Process"
        },
        {
            "term": "radial basis function",
            "url": "https://en.wikipedia.org/wiki/radial_basis_function"
        },
        {
            "term": "Decision Trees",
            "url": "https://en.wikipedia.org/wiki/Decision_Trees"
        },
        {
            "term": "Logistic Regression",
            "url": "https://en.wikipedia.org/wiki/Logistic_Regression"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        },
        {
            "term": "uniform distribution",
            "url": "https://en.wikipedia.org/wiki/uniform_distribution"
        },
        {
            "term": "Random Forest",
            "url": "https://en.wikipedia.org/wiki/Random_Forest"
        }
    ],
    "highlights": [
        "Modern advances in machine learning have produced models that achieve unprecedented accuracy on standard prediction tasks",
        "We found best validation parameters for Decision Trees, k-Nearest Neighbor (1, 3, 5 or 7 neighbors), and Gaussian Process (RBF kernel scaled with scaled by a coefficient in the set f0:1; 1:0; 5g and dot product kernel with inhomogeneity parameter set to 1)"
    ],
    "key_statements": [
        "Modern advances in machine learning have produced models that achieve unprecedented accuracy on standard prediction tasks",
        "We found best validation parameters for Decision Trees, k-Nearest Neighbor (1, 3, 5 or 7 neighbors), and Gaussian Process (RBF kernel scaled with scaled by a coefficient in the set f0:1; 1:0; 5g and dot product kernel with inhomogeneity parameter set to 1)"
    ],
    "summary": [
        "Modern advances in machine learning have produced models that achieve unprecedented accuracy on standard prediction tasks.",
        "We first training derive set of a convex loss function for multi-prototype instance-label pairs, where each xi 2 Rn binary classification.",
        "For any x 2 Rn, let the predictors f D .w1; w2; : : : ; wp/ 2 Ck take the following form: \u00c1 f .x/ D sign max wj x k : j 2\u0152p\u008d",
        "There will be no loss of using.wj.i/ xi / compared with the upper bound in (1) when we set j.i / D arg maxj 2\u0152p\u008d wj xi .",
        "This clustering step helps us provide a fast parametric alternative to the essentially non-parametric setting that assumes one prototype per positive example.",
        "In order to train a multi-prototype classifier, we minimize the regularized surrogate loss: min",
        "Thisis a smooth function, the overall loss h defined in (2) is non-smooth, due to the max operator in the sum over the set I .",
        "In order to take advantage of fast algorithms for smooth convex optimization, we smooth the loss function using soft-max.",
        "With some abuse of notation, we let kwj k0 denote the number of non-zero entries of the vector wj , and define kW k0;1 max kwj k0: j 2\u0152p\u008d",
        "As we will show the introduction of the binary matrix allows us to derive a saddle-point formulation of problem (7), which in turn admits a convex-concave relaxation that can be solved efficiently by the Mirror-Prox algorithm [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>].",
        "This is a convex-concave saddle-point problem, which can be solved efficiently, for example, by the Mirror-Prox algorithm [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>].",
        "Algorithm 1 lists the Mirror-Prox algorithm customized for solving the convex-concave saddle-point problem (13), which enjoys a O.1=t / convergence rate [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>].",
        "We compare the accuracy of SMaLL with k D n to the accuracy of other standard classification algorithms, on several low-dimensional (n \u00c4 20) binary classification datasets from the OpenML repository.",
        "Note that while we can explicitly control the amount of sparsity in SMaLL, ProtoNN, and Bonsai, the methods that use1 or elastic net regularization do not have this flexibility.",
        "We trained each of the linear baselines by setting a high value of the1 coefficient and selected the features with the largest absolute values.",
        "Figure 3 provides strong empirical evidence that SMaLL compares favorably to the baselines on several high dimensional OpenML datasets belonging to the Fri series.",
        "Note that in case of SMaLL, some features might be selected in more than one prototype."
    ],
    "headline": "We introduce a new framework for learning in severely resource-constrained settings",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. Gupta, A. S. Suggala, A. Goyal, H. V. Simhadri, B. Paranjape, A. Kumar, S. Goyal, R. Udupa, M. Varma, and P. Jain. ProtoNN: compressed and accurate kNN for resource-scarce devices. In International Conference on Machine Learning (ICML), pages 1331\u20131340, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20C.%20Suggala%2C%20A.S.%20Goyal%2C%20A.%20Simhadri%2C%20H.V.%20ProtoNN%3A%20compressed%20and%20accurate%20kNN%20for%20resource-scarce%20devices%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20C.%20Suggala%2C%20A.S.%20Goyal%2C%20A.%20Simhadri%2C%20H.V.%20ProtoNN%3A%20compressed%20and%20accurate%20kNN%20for%20resource-scarce%20devices%202017"
        },
        {
            "id": "2",
            "entry": "[2] A. Kumar, S. Goyal, and M. Varma. Resource-efficient machine learning in 2 kb ram for the internet of things. In International Conference on Machine Learning (ICML), pages 1935\u20131944, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20A.%20Goyal%2C%20S.%20Varma%2C%20M.%20Resource-efficient%20machine%20learning%20in%202%20kb%20ram%20for%20the%20internet%20of%20things%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20A.%20Goyal%2C%20S.%20Varma%2C%20M.%20Resource-efficient%20machine%20learning%20in%202%20kb%20ram%20for%20the%20internet%20of%20things%202017"
        },
        {
            "id": "3",
            "entry": "[3] L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134\u20131142, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valiant%2C%20L.G.%20A%20theory%20of%20the%20learnable%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valiant%2C%20L.G.%20A%20theory%20of%20the%20learnable%201984"
        },
        {
            "id": "4",
            "entry": "[4] J. R. Hauser, O. Toubia, T. Evgeniou, R. Befurt, and D. Dzyabura. Disjunctions of conjunctions, cognitive simplicity, and consideration sets. Journal of Marketing Research, 47(3):485\u2013496, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hauser%2C%20J.R.%20Toubia%2C%20O.%20Evgeniou%2C%20T.%20Befurt%2C%20R.%20Disjunctions%20of%20conjunctions%2C%20cognitive%20simplicity%2C%20and%20consideration%20sets%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hauser%2C%20J.R.%20Toubia%2C%20O.%20Evgeniou%2C%20T.%20Befurt%2C%20R.%20Disjunctions%20of%20conjunctions%2C%20cognitive%20simplicity%2C%20and%20consideration%20sets%202010"
        },
        {
            "id": "5",
            "entry": "[5] T. Wang, C. Rudin, F. Doshi, Y. Liu, E. Klampfl, and P. MacNeille. Bayesian rule sets for interpretable classification, with application to context-aware recommender systems. Journal of Machine Learning Research (JMLR), 18(70):1\u201337, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20T.%20Rudin%2C%20C.%20Doshi%2C%20F.%20Liu%2C%20Y.%20Bayesian%20rule%20sets%20for%20interpretable%20classification%2C%20with%20application%20to%20context-aware%20recommender%20systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20T.%20Rudin%2C%20C.%20Doshi%2C%20F.%20Liu%2C%20Y.%20Bayesian%20rule%20sets%20for%20interpretable%20classification%2C%20with%20application%20to%20context-aware%20recommender%20systems%202017"
        },
        {
            "id": "6",
            "entry": "[6] O. Cord. Genetic fuzzy systems: evolutionary tuning and learning of fuzzy knowledge bases, volume 19. World Scientific, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cord%2C%20O.%20Genetic%20fuzzy%20systems%3A%20evolutionary%20tuning%20and%20learning%20of%20fuzzy%20knowledge%20bases%2C%20volume%2019%202001"
        },
        {
            "id": "7",
            "entry": "[7] A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning dnf and characterizing statistical query learning using fourier analysis. In Proceedings of the Twenty-sixth Annual ACM Symposium on Theory of Computing (STOC), pages 253\u2013262, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Furst%2C%20M.%20Jackson%2C%20J.%20Kearns%2C%20M.%20Weakly%20learning%20dnf%20and%20characterizing%20statistical%20query%20learning%20using%20fourier%20analysis%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20A.%20Furst%2C%20M.%20Jackson%2C%20J.%20Kearns%2C%20M.%20Weakly%20learning%20dnf%20and%20characterizing%20statistical%20query%20learning%20using%20fourier%20analysis%201994"
        },
        {
            "id": "8",
            "entry": "[8] Y. Mansour. An o (nlog log n) learning algorithm for dnf under the uniform distribution. Journal of Computer and System Sciences, 50(3):543\u2013550, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Y.%20An%20o%20%28nlog%20log%20n%29%20learning%20algorithm%20for%20dnf%20under%20the%20uniform%20distribution%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Y.%20An%20o%20%28nlog%20log%20n%29%20learning%20algorithm%20for%20dnf%20under%20the%20uniform%20distribution%201995"
        },
        {
            "id": "9",
            "entry": "[9] J. C. Jackson. An efficient membership-query algorithm for learning dnf with respect to the uniform distribution. Journal of Computer and System Sciences, 55(3):414\u2013440, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jackson%2C%20J.C.%20An%20efficient%20membership-query%20algorithm%20for%20learning%20dnf%20with%20respect%20to%20the%20uniform%20distribution%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jackson%2C%20J.C.%20An%20efficient%20membership-query%20algorithm%20for%20learning%20dnf%20with%20respect%20to%20the%20uniform%20distribution%201997"
        },
        {
            "id": "10",
            "entry": "[10] K. Verbeurgt. Learning suc-classes of monotone dnf on the uniform distribution. In Proceedings of the Ninth Conference on Algorithmic Learning Theory, pages 385\u2013399, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verbeurgt%2C%20K.%20Learning%20suc-classes%20of%20monotone%20dnf%20on%20the%20uniform%20distribution%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verbeurgt%2C%20K.%20Learning%20suc-classes%20of%20monotone%20dnf%20on%20the%20uniform%20distribution%201998"
        },
        {
            "id": "11",
            "entry": "[11] N. H. Bshouty, J. C. Jackson, and C. Tamon. More efficient pac-learning of dnf with membership queries under the uniform distribution. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory (COLT), pages 286\u2013295, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bshouty%2C%20N.H.%20Jackson%2C%20J.C.%20Tamon%2C%20C.%20More%20efficient%20pac-learning%20of%20dnf%20with%20membership%20queries%20under%20the%20uniform%20distribution%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bshouty%2C%20N.H.%20Jackson%2C%20J.C.%20Tamon%2C%20C.%20More%20efficient%20pac-learning%20of%20dnf%20with%20membership%20queries%20under%20the%20uniform%20distribution%201999"
        },
        {
            "id": "12",
            "entry": "[12] Y. Sakai and A. Maruoka. Learning monotone log-term dnf formulas under the uniform distribution. Theory of Computing Systems, 33(1):17\u201333, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sakai%2C%20Y.%20Maruoka%2C%20A.%20Learning%20monotone%20log-term%20dnf%20formulas%20under%20the%20uniform%20distribution%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sakai%2C%20Y.%20Maruoka%2C%20A.%20Learning%20monotone%20log-term%20dnf%20formulas%20under%20the%20uniform%20distribution%202000"
        },
        {
            "id": "13",
            "entry": "[13] R. A. Servedio. On learning monotone dnf under product distributions. Information and Computation, 193(1):57\u201374, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Servedio%2C%20R.A.%20On%20learning%20monotone%20dnf%20under%20product%20distributions%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Servedio%2C%20R.A.%20On%20learning%20monotone%20dnf%20under%20product%20distributions%202004"
        },
        {
            "id": "14",
            "entry": "[14] N. H. Bshouty, E. Mossel, R. O\u2019Donnell, and R. A. Servedio. Learning dnf from random walks. Journal of Computer and System Sciences, 71(3):250\u2013265, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bshouty%2C%20N.H.%20Mossel%2C%20E.%20O%E2%80%99Donnell%2C%20R.%20Servedio%2C%20R.A.%20Learning%20dnf%20from%20random%20walks%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bshouty%2C%20N.H.%20Mossel%2C%20E.%20O%E2%80%99Donnell%2C%20R.%20Servedio%2C%20R.A.%20Learning%20dnf%20from%20random%20walks%202005"
        },
        {
            "id": "15",
            "entry": "[15] V. Feldman. Learning DNF expressions from fourier spectrum. In Conference on Learning Theory (COLT), pages 17.1\u201317.19, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20V.%20Learning%20DNF%20expressions%20from%20fourier%20spectrum%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20V.%20Learning%20DNF%20expressions%20from%20fourier%20spectrum%202012"
        },
        {
            "id": "16",
            "entry": "[16] A. R. Klivans and R. A. Servedio. Learning dnf in time 2o (n1/3). Journal of Computer and System Sciences, 68(2):303\u2013318, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klivans%2C%20A.R.%20Servedio%2C%20R.A.%20Learning%20dnf%20in%20time%202o%20%28n1/3%29%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klivans%2C%20A.R.%20Servedio%2C%20R.A.%20Learning%20dnf%20in%20time%202o%20%28n1/3%29%202004"
        },
        {
            "id": "17",
            "entry": "[17] S. Khot and R. Saket. Hardness of minimizing and learning dnf expressions. In Foundations of Computer Science (FOCS), pages 231\u2013240, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khot%2C%20S.%20Saket%2C%20R.%20Hardness%20of%20minimizing%20and%20learning%20dnf%20expressions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khot%2C%20S.%20Saket%2C%20R.%20Hardness%20of%20minimizing%20and%20learning%20dnf%20expressions%202008"
        },
        {
            "id": "18",
            "entry": "[18] F. Aiolli and A. Sperduti. Multiclass classification with multi-prototype support vector machines. Journal of Machine Learning Research (JMLR), 6:817\u2013850, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aiolli%2C%20F.%20Sperduti%2C%20A.%20Multiclass%20classification%20with%20multi-prototype%20support%20vector%20machines%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aiolli%2C%20F.%20Sperduti%2C%20A.%20Multiclass%20classification%20with%20multi-prototype%20support%20vector%20machines%202005"
        },
        {
            "id": "19",
            "entry": "[19] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetron: A kernel-based perceptron on a budget. SIAM Journal on Computing, 37(5):1342\u20131372, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20O.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20The%20Forgetron%3A%20A%20kernel-based%20perceptron%20on%20a%20budget%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20O.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20The%20Forgetron%3A%20A%20kernel-based%20perceptron%20on%20a%20budget%202008"
        },
        {
            "id": "20",
            "entry": "[20] O. Dekel and Y. Singer. Support vector machines on a budget. In NIPS, pages 345\u2013352, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20O.%20Singer%2C%20Y.%20Support%20vector%20machines%20on%20a%20budget%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20O.%20Singer%2C%20Y.%20Support%20vector%20machines%20on%20a%20budget%202007"
        },
        {
            "id": "21",
            "entry": "[21] M. Kusner, S. Tyree, K. Q. Weinberger, and K. Agrawal. Stochastic neighbor compression. In ICML, pages 622\u2013630, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kusner%2C%20M.%20Tyree%2C%20S.%20Weinberger%2C%20K.Q.%20Agrawal%2C%20K.%20Stochastic%20neighbor%20compression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kusner%2C%20M.%20Tyree%2C%20S.%20Weinberger%2C%20K.Q.%20Agrawal%2C%20K.%20Stochastic%20neighbor%20compression%202014"
        },
        {
            "id": "22",
            "entry": "[22] K. Zhong, R. Guo, S. Kumar, B. Yan, D. Simcha, and I. Dhillon. Fast Classification with Binary Prototypes. In AISTATS, pages 1255\u20131263, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhong%2C%20K.%20Guo%2C%20R.%20Kumar%2C%20S.%20Yan%2C%20B.%20Fast%20Classification%20with%20Binary%20Prototypes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhong%2C%20K.%20Guo%2C%20R.%20Kumar%2C%20S.%20Yan%2C%20B.%20Fast%20Classification%20with%20Binary%20Prototypes%202017"
        },
        {
            "id": "23",
            "entry": "[23] M. Pilanci and M. J. Wainwright. Sparse learning via Boolean relaxations. Mathematical Programming, 151:63\u201387, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilanci%2C%20M.%20Wainwright%2C%20M.J.%20Sparse%20learning%20via%20Boolean%20relaxations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilanci%2C%20M.%20Wainwright%2C%20M.J.%20Sparse%20learning%20via%20Boolean%20relaxations%202015"
        },
        {
            "id": "24",
            "entry": "[24] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (methodological), 58(1):267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "25",
            "entry": "[25] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society. Series B (methodological), 67(2):301\u2013320, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20H.%20Hastie%2C%20T.%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20H.%20Hastie%2C%20T.%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005"
        },
        {
            "id": "26",
            "entry": "[26] V. K. Garg, L. Xiao, and O. Dekel. Sparse Multiprototype Classification. In UAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garg%2C%20V.K.%20Xiao%2C%20L.%20Dekel%2C%20O.%20Sparse%20Multiprototype%20Classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garg%2C%20V.K.%20Xiao%2C%20L.%20Dekel%2C%20O.%20Sparse%20Multiprototype%20Classification%202018"
        },
        {
            "id": "27",
            "entry": "[27] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20S.%20Mao%2C%20H.%20Dally%2C%20W.J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20S.%20Mao%2C%20H.%20Dally%2C%20W.J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016"
        },
        {
            "id": "28",
            "entry": "[28] F. Nan, J. Wang, and V. Saligrama. Pruning random forests for prediction on a budget. In NIPS, pages 2334\u20132342, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nan%2C%20F.%20Wang%2C%20J.%20Saligrama%2C%20V.%20Pruning%20random%20forests%20for%20prediction%20on%20a%20budget%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nan%2C%20F.%20Wang%2C%20J.%20Saligrama%2C%20V.%20Pruning%20random%20forests%20for%20prediction%20on%20a%20budget%202016"
        },
        {
            "id": "29",
            "entry": "[29] J.-H. Luo, J. Wu, and W. Lin. Thinet: A filter level pruning method for deep neural network compression. In ICCV, pages 5068\u20135076, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20J.-H.%20Wu%2C%20J.%20Lin%2C%20W.%20Thinet%3A%20A%20filter%20level%20pruning%20method%20for%20deep%20neural%20network%20compression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20J.-H.%20Wu%2C%20J.%20Lin%2C%20W.%20Thinet%3A%20A%20filter%20level%20pruning%20method%20for%20deep%20neural%20network%20compression%202017"
        },
        {
            "id": "30",
            "entry": "[30] T. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In ICASSP, pages 6655\u20136659, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20T.%20Kingsbury%2C%20B.%20Sindhwani%2C%20V.%20Arisoy%2C%20E.%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20T.%20Kingsbury%2C%20B.%20Sindhwani%2C%20V.%20Arisoy%2C%20E.%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013"
        },
        {
            "id": "31",
            "entry": "[31] P. Nakkiran, R. Alvarez, R. Prabhavalkar, and C. Parada. Compressing deep neural networks using a rank-constrained topology. In Sixteenth Annual Conference of the International Speech Communication Association, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nakkiran%2C%20P.%20Alvarez%2C%20R.%20Prabhavalkar%2C%20R.%20Parada%2C%20C.%20Compressing%20deep%20neural%20networks%20using%20a%20rank-constrained%20topology%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nakkiran%2C%20P.%20Alvarez%2C%20R.%20Prabhavalkar%2C%20R.%20Parada%2C%20C.%20Compressing%20deep%20neural%20networks%20using%20a%20rank-constrained%20topology%202015"
        },
        {
            "id": "32",
            "entry": "[32] W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen. Compressing neural networks with the hashing trick. In ICML, pages 2285\u20132294, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20W.%20Wilson%2C%20J.%20Tyree%2C%20S.%20Weinberger%2C%20K.%20Compressing%20neural%20networks%20with%20the%20hashing%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20W.%20Wilson%2C%20J.%20Tyree%2C%20S.%20Weinberger%2C%20K.%20Compressing%20neural%20networks%20with%20the%20hashing%20trick%202015"
        },
        {
            "id": "33",
            "entry": "[33] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. In NIPS, pages 4107\u20134115, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Hubara%20M%20Courbariaux%20D%20Soudry%20R%20ElYaniv%20and%20Y%20Bengio%20Binarized%20neural%20networks%20In%20NIPS%20pages%2041074115%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Hubara%20M%20Courbariaux%20D%20Soudry%20R%20ElYaniv%20and%20Y%20Bengio%20Binarized%20neural%20networks%20In%20NIPS%20pages%2041074115%202016"
        },
        {
            "id": "34",
            "entry": "[34] Dimitris Bertsimas and Romy Shioda. Classification and regression via integer optimization. Operations Research, 55(2):252\u2013271, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Shioda%2C%20Romy%20Classification%20and%20regression%20via%20integer%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsimas%2C%20Dimitris%20Shioda%2C%20Romy%20Classification%20and%20regression%20via%20integer%20optimization%202007"
        },
        {
            "id": "35",
            "entry": "[35] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20Optimization%202004"
        },
        {
            "id": "36",
            "entry": "[36] A. Nemirovski. Prox-method with rate of convergence O.1=t / for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Prox-method%20with%20rate%20of%20convergence%20O.1%3Dt%20/%20for%20variational%20inequalities%20with%20Lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Prox-method%20with%20rate%20of%20convergence%20O.1%3Dt%20/%20for%20variational%20inequalities%20with%20Lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004"
        },
        {
            "id": "37",
            "entry": "[37] A. Juditsky and A. Nemirovski. First-order methods for nonsmooth convex large-scale optimization, II: Utilizing problems\u2019s structure. In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning, chapter 6, pages 149\u2013184. The MIT Press, Cambridge, MA., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Juditsky%2C%20A.%20Nemirovski%2C%20A.%20First-order%20methods%20for%20nonsmooth%20convex%20large-scale%20optimization%2C%20II%3A%20Utilizing%20problems%E2%80%99s%20structure%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Juditsky%2C%20A.%20Nemirovski%2C%20A.%20First-order%20methods%20for%20nonsmooth%20convex%20large-scale%20optimization%2C%20II%3A%20Utilizing%20problems%E2%80%99s%20structure%202011"
        },
        {
            "id": "38",
            "entry": "[38] P. Brucker. An O.n/ algorithm for quadratic Knapsack problems. Operations Research Letters, 3(3):163\u2013166, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brucker%2C%20P.%20An%20O.n/%20algorithm%20for%20quadratic%20Knapsack%20problems%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brucker%2C%20P.%20An%20O.n/%20algorithm%20for%20quadratic%20Knapsack%20problems%201984"
        },
        {
            "id": "39",
            "entry": "[39] P. M. Pardalos and N. Kovoor. An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds. Mathematical Programming, 46:321\u2013328, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pardalos%2C%20P.M.%20Kovoor%2C%20N.%20An%20algorithm%20for%20a%20singly%20constrained%20class%20of%20quadratic%20programs%20subject%20to%20upper%20and%20lower%20bounds%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pardalos%2C%20P.M.%20Kovoor%2C%20N.%20An%20algorithm%20for%20a%20singly%20constrained%20class%20of%20quadratic%20programs%20subject%20to%20upper%20and%20lower%20bounds%201990"
        },
        {
            "id": "40",
            "entry": "[40] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efficient projection onto the1-ball for learning in high dimensions. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 272\u2013279, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Chandra%2C%20T.%20Efficient%20projection%20onto%20the1-ball%20for%20learning%20in%20high%20dimensions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Chandra%2C%20T.%20Efficient%20projection%20onto%20the1-ball%20for%20learning%20in%20high%20dimensions%202008"
        },
        {
            "id": "41",
            "entry": "[41] A. Jalali, M. Fazel, and L. Xiao. Variational Gram functions: Convex analysis and optimization. SIAM Journal on Optimization, 27(4):2634\u20132661, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jalali%2C%20A.%20Fazel%2C%20M.%20Xiao%2C%20L.%20Variational%20Gram%20functions%3A%20Convex%20analysis%20and%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jalali%2C%20A.%20Fazel%2C%20M.%20Xiao%2C%20L.%20Variational%20Gram%20functions%3A%20Convex%20analysis%20and%20optimization%202017"
        }
    ]
}
