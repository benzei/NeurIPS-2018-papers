{
    "filename": "7811-unsupervised-video-object-segmentation-for-deep-reinforcement-learning.pdf",
    "metadata": {
        "title": "Unsupervised Video Object Segmentation for Deep Reinforcement Learning",
        "author": "Vikash Goel, Jameson Weng, Pascal Poupart",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7811-unsupervised-video-object-segmentation-for-deep-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information. Our code is available at https://github.com/vik-goel/MOREL."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "motion estimation",
            "url": "https://en.wikipedia.org/wiki/motion_estimation"
        },
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "raw image",
            "url": "https://en.wikipedia.org/wiki/raw_image"
        }
    ],
    "highlights": [
        "Reinforcement Learning<br/><br/>Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] provides a principled framework to optimize sequences of interdependent decisions in stochastic environments with unknown dynamics",
        "The beauty of deep Reinforcement learning is that there is no need for practitioners to handcraft features such as object detectors, but this obviously requires more interactions with the environment since the convolutional neural network needs to learn what features to extract for a good policy and/or value function",
        "Precisely, we describe how to obtain an object mask with flow information for each moving object in an unsupervised fashion based on a modified version of Structure from Motion Network [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] that is trained on the first 1% of the frames seen by the agent while playing",
        "Since our approach is orthogonal to the choice of Reinforcement learning algorithm, it can in principle be combined with any Reinforcement learning algorithm",
        "This paper describes an unsupervised technique for object segmentation and motion estimation in reinforcement learning from raw images",
        "This approach reduces the amount of interaction with the environment when moving objects are important features"
    ],
    "key_statements": [
        "Reinforcement Learning<br/><br/>Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] provides a principled framework to optimize sequences of interdependent decisions in stochastic environments with unknown dynamics",
        "The beauty of deep Reinforcement learning is that there is no need for practitioners to handcraft features such as object detectors, but this obviously requires more interactions with the environment since the convolutional neural network needs to learn what features to extract for a good policy and/or value function",
        "Precisely, we describe how to obtain an object mask with flow information for each moving object in an unsupervised fashion based on a modified version of Structure from Motion Network [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] that is trained on the first 1% of the frames seen by the agent while playing",
        "We show how the object masks produced by our technique in an unsupervised fashion can help to interpret and visualize the information used by Reinforcement learning policies",
        "We perform a qualitative evaluation of our unsupervised video object segmentation network to analyze the quality of the produced object masks",
        "We show the results of Motion-Oriented REinforcement Learning on all 59 Atari games which were available at the time of this publication in our supplementary materials",
        "Since our approach is orthogonal to the choice of Reinforcement learning algorithm, it can in principle be combined with any Reinforcement learning algorithm",
        "This paper describes an unsupervised technique for object segmentation and motion estimation in reinforcement learning from raw images",
        "This approach reduces the amount of interaction with the environment when moving objects are important features",
        "It would be interesting to combine this work with object-oriented frameworks [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], physics-based dynamics [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and model-based reinforcement learning [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] to reason about and plan object interactions"
    ],
    "summary": [
        "Reinforcement Learning<br/><br/>Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] provides a principled framework to optimize sequences of interdependent decisions in stochastic environments with unknown dynamics.",
        "The beauty of deep RL is that there is no need for practitioners to handcraft features such as object detectors, but this obviously requires more interactions with the environment since the convolutional neural network needs to learn what features to extract for a good policy and/or value function.",
        "Sec. 4 describes our new approach, Motion-Oriented REinforcement Learning (MOREL), that learns to segment moving objects and infer their motion in an unsupervised way as part of a model free RL technique.",
        "In the arcade learning environment [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], observations consist of images and convolutional neural networks [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] are often used to automatically extract relevant features for the selection of actions and the computation of values [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>].",
        "We describe an object-sensitive representation learning technique that can be combined with most value-based, policy optimization, and actor-critic algorithms.",
        "All of these frameworks can improve generalization and reduce sample complexity in reinforcement learning, but they assume that object features and relations are directly available from the environment.",
        "We propose an unsupervised technique that does not require any domain information, labeled data, or manual input from practitioners to automatically detect moving objects with their motion in visual reinforcement learning tasks.",
        "An agent capable of predicting future rewards and observations is extracting useful information that can speed up the learning of a good policy or value function [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "We learn a structured representation of the input that captures information about all moving objects in the scene through training on the task of unsupervised video object segmentation.",
        "After learning a disentangled representation of the scene, we transfer the weights to our reinforcement learning agent and continue to optimize our segmentation network jointly along with the policy and/or value function.",
        "Figure 3 shows object masks and optical flow predicted by our unsupervised video object segmentation network.",
        "The autoencoder architecture is the same as our network, only modified to output one frame instead of K object masks, and without the object translation and camera motion prediction.",
        "We do a simple transfer of the object segmentation network, and use only the motion path without joint training.",
        "This paper describes an unsupervised technique for object segmentation and motion estimation in reinforcement learning from raw images.",
        "It would be interesting to combine this work with object-oriented frameworks [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], physics-based dynamics [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and model-based reinforcement learning [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] to reason about and plan object interactions.",
        "We plan to further extend this work to 3D environments such as Doom and real-world environments"
    ],
    "headline": "We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481\u20132495, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Badrinarayanan%2C%20Vijay%20Kendall%2C%20Alex%20Cipolla%2C%20Roberto%20Segnet%3A%20A%20deep%20convolutional%20encoder-decoder%20architecture%20for%20image%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Badrinarayanan%2C%20Vijay%20Kendall%2C%20Alex%20Cipolla%2C%20Roberto%20Segnet%3A%20A%20deep%20convolutional%20encoder-decoder%20architecture%20for%20image%20segmentation%202017"
        },
        {
            "id": "2",
            "entry": "[2] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pages 4502\u20134510, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016"
        },
        {
            "id": "3",
            "entry": "[3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "4",
            "entry": "[4] James R Bergen, Patrick Anandan, Keith J Hanna, and Rajesh Hingorani. Hierarchical modelbased motion estimation. In European conference on computer vision, pages 237\u2013252.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergen%2C%20James%20R.%20Anandan%2C%20Patrick%20Hanna%2C%20Keith%20J.%20Hingorani%2C%20Rajesh%20Hierarchical%20modelbased%20motion%20estimation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergen%2C%20James%20R.%20Anandan%2C%20Patrick%20Hanna%2C%20Keith%20J.%20Hingorani%2C%20Rajesh%20Hierarchical%20modelbased%20motion%20estimation"
        },
        {
            "id": "5",
            "entry": "[5] Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pages 240\u2013247. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diuk%2C%20Carlos%20Cohen%2C%20Andre%20Littman%2C%20Michael%20L.%20An%20object-oriented%20representation%20for%20efficient%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diuk%2C%20Carlos%20Cohen%2C%20Andre%20Littman%2C%20Michael%20L.%20An%20object-oriented%20representation%20for%20efficient%20reinforcement%20learning%202008"
        },
        {
            "id": "6",
            "entry": "[6] Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L Griffiths, and Alexei A Efros. Investigating human priors for playing video games. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dubey%2C%20Rachit%20Agrawal%2C%20Pulkit%20Pathak%2C%20Deepak%20Griffiths%2C%20Thomas%20L.%20and%20Alexei%20A%20Efros.%20Investigating%20human%20priors%20for%20playing%20video%20games%202018"
        },
        {
            "id": "7",
            "entry": "[7] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial autoencoders for visuomotor learning. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512\u2013519. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Tan%2C%20Xin%20Yu%20Duan%2C%20Yan%20Darrell%2C%20Trevor%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Tan%2C%20Xin%20Yu%20Duan%2C%20Yan%20Darrell%2C%20Trevor%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016"
        },
        {
            "id": "8",
            "entry": "[8] Nir Friedman and Stuart Russell. Image segmentation in video sequences: A probabilistic approach. In Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence, pages 175\u2013181. Morgan Kaufmann Publishers Inc., 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20Nir%20Russell%2C%20Stuart%20Image%20segmentation%20in%20video%20sequences%3A%20A%20probabilistic%20approach%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20Nir%20Russell%2C%20Stuart%20Image%20segmentation%20in%20video%20sequences%3A%20A%20probabilistic%20approach%201997"
        },
        {
            "id": "9",
            "entry": "[9] Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actorcritic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291\u20131307, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grondman%2C%20Ivo%20Busoniu%2C%20Lucian%20Lopes%2C%20Gabriel%20A.D.%20Babuska%2C%20Robert%20A%20survey%20of%20actorcritic%20reinforcement%20learning%3A%20Standard%20and%20natural%20policy%20gradients%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grondman%2C%20Ivo%20Busoniu%2C%20Lucian%20Lopes%2C%20Gabriel%20A.D.%20Babuska%2C%20Robert%20A%20survey%20of%20actorcritic%20reinforcement%20learning%3A%20Standard%20and%20natural%20policy%20gradients%202012"
        },
        {
            "id": "10",
            "entry": "[10] David Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10122"
        },
        {
            "id": "11",
            "entry": "[11] Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. arXiv preprint arXiv:1707.08475, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08475"
        },
        {
            "id": "12",
            "entry": "[12] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural networks. 313:504\u20137, 08 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "13",
            "entry": "[13] Rahul Iyer, Yuezhang Li, Huao Li, Michael Lewis, Ramitha Sundar, and Katia Sycara. Transparency and explanation in deep reinforcement learning neural networks. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyer%2C%20Rahul%20Li%2C%20Yuezhang%20Li%2C%20Huao%20Lewis%2C%20Michael%20Transparency%20and%20explanation%20in%20deep%20reinforcement%20learning%20neural%20networks%202018"
        },
        {
            "id": "14",
            "entry": "[14] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Mnih%2C%20Volodymyr%20Czarnecki%2C%20Wojciech%20Marian%20Schaul%2C%20Tom%20Reinforcement%20learning%20with%20unsupervised%20auxiliary%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Mnih%2C%20Volodymyr%20Czarnecki%2C%20Wojciech%20Marian%20Schaul%2C%20Tom%20Reinforcement%20learning%20with%20unsupervised%20auxiliary%20tasks%202017"
        },
        {
            "id": "15",
            "entry": "[15] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pages 2017\u20132025, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] Ken Kansky, Tom Silver, David A M\u00e9ly, Mohamed Eldawy, Miguel L\u00e1zaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot transfer with a generative causal model of intuitive physics. In International Conference on Machine Learning, pages 1809\u20131818, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kansky%2C%20Ken%20Silver%2C%20Tom%20M%C3%A9ly%2C%20David%20A.%20Eldawy%2C%20Mohamed%20Schema%20networks%3A%20Zero-shot%20transfer%20with%20a%20generative%20causal%20model%20of%20intuitive%20physics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kansky%2C%20Ken%20Silver%2C%20Tom%20M%C3%A9ly%2C%20David%20A.%20Eldawy%2C%20Mohamed%20Schema%20networks%3A%20Zero-shot%20transfer%20with%20a%20generative%20causal%20model%20of%20intuitive%20physics%202017"
        },
        {
            "id": "17",
            "entry": "[17] Omar Zia Khan, Pascal Poupart, and James P Black. Minimal sufficient explanations for factored markov decision processes. In ICAPS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20Omar%20Zia%20Poupart%2C%20Pascal%20Black%2C%20James%20P.%20Minimal%20sufficient%20explanations%20for%20factored%20markov%20decision%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20Omar%20Zia%20Poupart%2C%20Pascal%20Black%2C%20James%20P.%20Minimal%20sufficient%20explanations%20for%20factored%20markov%20decision%20processes%202009"
        },
        {
            "id": "18",
            "entry": "[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "19",
            "entry": "[19] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In The 2010 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20Deep%20auto-encoder%20neural%20networks%20in%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20Deep%20auto-encoder%20neural%20networks%20in%20reinforcement%20learning%202010"
        },
        {
            "id": "20",
            "entry": "[20] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "21",
            "entry": "[21] Yuezhang Li, Katia Sycara, and Rahul Iyer. Object-sensitive deep reinforcement learning. In Global Conference on Artificial Intelligence, volume 50, pages 20\u201335, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuezhang%20Sycara%2C%20Katia%20Iyer%2C%20Rahul%20Object-sensitive%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuezhang%20Sycara%2C%20Katia%20Iyer%2C%20Rahul%20Object-sensitive%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "22",
            "entry": "[22] Parsa Mahmoudieh. Self-supervision for reinforcement learning. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahmoudieh%2C%20Parsa%20Self-supervision%20for%20reinforcement%20learning%202017"
        },
        {
            "id": "23",
            "entry": "[23] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirowski%2C%20Piotr%20Pascanu%2C%20Razvan%20Viola%2C%20Fabio%20Soyer%2C%20Hubert%20Learning%20to%20navigate%20in%20complex%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirowski%2C%20Piotr%20Pascanu%2C%20Razvan%20Viola%2C%20Fabio%20Soyer%2C%20Hubert%20Learning%20to%20navigate%20in%20complex%20environments%202017"
        },
        {
            "id": "24",
            "entry": "[24] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "25",
            "entry": "[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "26",
            "entry": "[26] Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Learning multimodal transition dynamics for model-based reinforcement learning. arXiv preprint arXiv:1705.00470, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.00470"
        },
        {
            "id": "27",
            "entry": "[27] S\u00e9bastien Racani\u00e8re, Th\u00e9ophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri\u00e0 Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 5694\u20135705, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Racani%C3%A8re%2C%20S%C3%A9bastien%20Weber%2C%20Th%C3%A9ophane%20Reichert%2C%20David%20Buesing%2C%20Lars%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Racani%C3%A8re%2C%20S%C3%A9bastien%20Weber%2C%20Th%C3%A9ophane%20Reichert%2C%20David%20Buesing%2C%20Lars%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "28",
            "entry": "[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ronneberger%2C%20Olaf%20Fischer%2C%20Philipp%20Brox%2C%20Thomas%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ronneberger%2C%20Olaf%20Fischer%2C%20Philipp%20Brox%2C%20Thomas%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation"
        },
        {
            "id": "29",
            "entry": "[29] Jonathan Scholz, Martin Levihn, Charles Isbell, and David Wingate. A physics-based model prior for object-oriented mdps. In International Conference on Machine Learning, pages 1089\u20131097, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholz%2C%20Jonathan%20Levihn%2C%20Martin%20Isbell%2C%20Charles%20Wingate%2C%20David%20A%20physics-based%20model%20prior%20for%20object-oriented%20mdps%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scholz%2C%20Jonathan%20Levihn%2C%20Martin%20Isbell%2C%20Charles%20Wingate%2C%20David%20A%20physics-based%20model%20prior%20for%20object-oriented%20mdps%202014"
        },
        {
            "id": "30",
            "entry": "[30] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "31",
            "entry": "[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "32",
            "entry": "[32] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "33",
            "entry": "[33] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "34",
            "entry": "[34] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In AAAI, volume 16, pages 2094\u20132100, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hasselt%2C%20Hado%20Van%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20q-learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hasselt%2C%20Hado%20Van%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20q-learning%202016"
        },
        {
            "id": "35",
            "entry": "[35] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, and Katerina Fragkiadaki. Sfm-net: Learning of structure and motion from video. arXiv preprint arXiv:1704.07804, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.07804"
        },
        {
            "id": "36",
            "entry": "[36] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013 612, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zhou%20Bovik%2C%20Alan%20C.%20Sheikh%2C%20Hamid%20R.%20Simoncelli%2C%20Eero%20P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zhou%20Bovik%2C%20Alan%20C.%20Sheikh%2C%20Hamid%20R.%20Simoncelli%2C%20Eero%20P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004"
        },
        {
            "id": "37",
            "entry": "[37] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pages 5\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning"
        },
        {
            "id": "38",
            "entry": "[38] Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pages 5285\u20135294, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Mansimov%2C%20Elman%20Grosse%2C%20Roger%20B.%20Liao%2C%20Shun%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Mansimov%2C%20Elman%20Grosse%2C%20Roger%20B.%20Liao%2C%20Shun%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017"
        },
        {
            "id": "39",
            "entry": "[39] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, volume 2, page 7, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Tinghui%20Brown%2C%20Matthew%20Snavely%2C%20Noah%20Lowe%2C%20David%20G.%20Unsupervised%20learning%20of%20depth%20and%20ego-motion%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Tinghui%20Brown%2C%20Matthew%20Snavely%2C%20Noah%20Lowe%2C%20David%20G.%20Unsupervised%20learning%20of%20depth%20and%20ego-motion%20from%20video%202017"
        }
    ]
}
