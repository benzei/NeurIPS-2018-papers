{
    "filename": "7997-adversarial-scene-editing-automatic-object-removal-from-weak-supervision.pdf",
    "metadata": {
        "title": "Adversarial Scene Editing: Automatic Object Removal from Weak Supervision",
        "author": "Rakshith R. Shetty, Mario Fritz, Bernt Schiele",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7997-adversarial-scene-editing-automatic-object-removal-from-weak-supervision.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets. In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only."
    },
    "keywords": [
        {
            "term": "image manipulation",
            "url": "https://en.wikipedia.org/wiki/image_manipulation"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "peak signal-to-noise ratio",
            "url": "https://en.wikipedia.org/wiki/peak_signal-to-noise_ratio"
        }
    ],
    "highlights": [
        "Automatic editing of scene-level images to add/remove objects and manipulate attributes of objects like color/shape etc. is a challenging problem with a wide variety of applications",
        "We investigate such an automatic interaction free image manipulation approach that involves editing an input image to remove target objects, while leaving the rest of the image intact",
        "We propose an automatic object removal model that takes an input image and a target class and edits the image to remove the target object class",
        "We presented an automatic object removal model which learns to find and remove objects from general scene images",
        "Our model learns to perform this task with only image level labels and unpaired data",
        "We developed a Generative adversarial networks based framework to impose different priors to the mask generator, which encourages it to generate clean compact masks to remove objects"
    ],
    "key_statements": [
        "Automatic editing of scene-level images to add/remove objects and manipulate attributes of objects like color/shape etc. is a challenging problem with a wide variety of applications",
        "We investigate such an automatic interaction free image manipulation approach that involves editing an input image to remove target objects, while leaving the rest of the image intact",
        "We propose an automatic object removal model that takes an input image and a target class and edits the image to remove the target object class",
        "To avoid these degenerate solutions, we propose a novel mechanism to regularize Figure 2: Imposing mask priors with a Generative adversarial networks framework the mask generator to produce masks close to a prior distribution",
        "We use the least-square Generative adversarial networks loss [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] to train the Drf, since we found the Wasserstein GAN loss to be unstable with local real/fake prediction",
        "We presented an automatic object removal model which learns to find and remove objects from general scene images",
        "Our model learns to perform this task with only image level labels and unpaired data",
        "We developed a Generative adversarial networks based framework to impose different priors to the mask generator, which encourages it to generate clean compact masks to remove objects"
    ],
    "summary": [
        "Automatic editing of scene-level images to add/remove objects and manipulate attributes of objects like color/shape etc. is a challenging problem with a wide variety of applications.",
        "We propose a two-staged editor with a mask-generator and image in-painter which jointly learn to remove the target object class.",
        "Our model learns to perform removal by jointly optimizing the mask generator and the in-painter for the removal task with only weak supervision from image-level labels.",
        "It learns to perform this removal with only access to image-level labels without needing expensive ground-truth location information like bounding boxes or masks.",
        "Similar to prior works on in-painting [22\u2013 24], we train GI with self-supervision by trying to reconstruct random image patches and weak supervision from fooling an adversarial real/fake classifier.",
        "In recent works on in-painting using adversarial loss [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>\u2013<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], in-painter is trained adversarially against a classifier Drf which learns to predict global \u201creal\u201d and \u201cfake\u201d labels for input x and the generated images y respectively.",
        "Keeping with the goal of performing removal on general scene images, we train and test our model mainly on the COCO dataset [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] since it contains significant diversity within object classes and in the contexts in which they appear.",
        "Since there is no prior work proposing a fully automatic object removal solution, we compare our model against removal using a stand-alone fully supervised segmentation model, Mask-RCNN [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>].",
        "Quantitative analysis shows that our weakly supervised model performs on par with the fully supervised Mask-RCNN in the removal task, in both automatic and human evaluation.",
        "Figure 3 shows the results of object removal performed by our model on the COCO dataset compared to the Mask-RCNN baseline.",
        "The masks generated by the model using no prior are very noisy since the model has no information about object shapes and is trying to infer everything from the image level classifier.",
        "Using unpaired segmentation masks from the pascal dataset as shape priors makes the generated masks more accurate and the model is able to recover the object shapes better.",
        "Comparing removal performance in Table 1 we see that while the model with no prior achieves very high removal rate (94%), but it does so with large masks (37 %) which causes low output image quality.",
        "This is still a significant result, given that our model is trained without expensive ground truth segmentation annotation for each image, but instead uses only unpaired masks from a smaller dataset.",
        "Results show that our model achieves similar performance as a fully-supervised segmenter based removal, demonstrating the feasibility of weakly supervised solutions for the general scene-level editing task."
    ],
    "headline": "We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb, \u201cLearning from simulated and unsupervised images through adversarial training,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), no. 4, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrivastava%2C%20A.%20Pfister%2C%20T.%20Tuzel%2C%20O.%20Susskind%2C%20J.%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrivastava%2C%20A.%20Pfister%2C%20T.%20Tuzel%2C%20O.%20Susskind%2C%20J.%20Learning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%2C%202017"
        },
        {
            "id": "2",
            "entry": "[2] T. Orekondy, M. Fritz, and B. Schiele, \u201cConnecting pixels to privacy and utility: Automatic redaction of private information in images,\u201d arXiv preprint arXiv:1712.01066, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01066"
        },
        {
            "id": "3",
            "entry": "[3] R. Huang, S. Zhang, T. Li, and R. He, \u201cBeyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20R.%20Zhang%2C%20S.%20Li%2C%20T.%20He%2C%20R.%20Beyond%20face%20rotation%3A%20Global%20and%20local%20perception%20gan%20for%20photorealistic%20and%20identity%20preserving%20frontal%20view%20synthesis%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20R.%20Zhang%2C%20S.%20Li%2C%20T.%20He%2C%20R.%20Beyond%20face%20rotation%3A%20Global%20and%20local%20perception%20gan%20for%20photorealistic%20and%20identity%20preserving%20frontal%20view%20synthesis%2C%202017"
        },
        {
            "id": "4",
            "entry": "[4] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer et al., \u201cFader networks: Manipulating images by sliding attributes,\u201d in Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20G.%20Zeghidour%2C%20N.%20Usunier%2C%20N.%20Bordes%2C%20A.%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lample%2C%20G.%20Zeghidour%2C%20N.%20Usunier%2C%20N.%20Bordes%2C%20A.%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%2C%202017"
        },
        {
            "id": "5",
            "entry": "[5] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, \u201cStargan: Unified generative adversarial networks for multi-domain image-to-image translation,\u201d arXiv preprint arXiv:1711.09020, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09020"
        },
        {
            "id": "6",
            "entry": "[6] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \u201cUnpaired image-to-image translation using cycle-consistent adversarial networks,\u201d Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%2C%202017"
        },
        {
            "id": "7",
            "entry": "[7] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro, \u201cHigh-resolution image synthesis and semantic manipulation with conditional gans,\u201d arXiv preprint arXiv:1711.11585, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11585"
        },
        {
            "id": "8",
            "entry": "[8] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \u201cMask R-CNN,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K.%20He%2C%20G.%20Gkioxari%2C%20P.%20Doll%C3%A1r%20Girshick%2C%20R.%20Mask%20R-CNN%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K.%20He%2C%20G.%20Gkioxari%2C%20P.%20Doll%C3%A1r%20Girshick%2C%20R.%20Mask%20R-CNN%2C%202017"
        },
        {
            "id": "9",
            "entry": "[9] X. Chen, T.-Y. L. Hao Fang, R. Vedantam, S. Gupta, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft COCO captions: Data collection and evaluation server,\u201d arXiv preprint arxiv:1504.00325, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00325"
        },
        {
            "id": "10",
            "entry": "[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d in Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014"
        },
        {
            "id": "11",
            "entry": "[11] M. Mirza and S. Osindero, \u201cConditional generative adversarial nets,\u201d arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "12",
            "entry": "[12] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \u201cImage-to-image translation with conditional adversarial networks,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%2C%202016"
        },
        {
            "id": "13",
            "entry": "[13] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, \u201cGenerative adversarial text to image synthesis,\u201d in 33rd International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Akata%2C%20Z.%20Yan%2C%20X.%20Logeswaran%2C%20L.%20Generative%20adversarial%20text%20to%20image%20synthesis%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Akata%2C%20Z.%20Yan%2C%20X.%20Logeswaran%2C%20L.%20Generative%20adversarial%20text%20to%20image%20synthesis%2C%202016"
        },
        {
            "id": "14",
            "entry": "[14] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie, \u201cStacked generative adversarial networks,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Li%2C%20Y.%20Poursaeed%2C%20O.%20Hopcroft%2C%20J.%20Stacked%20generative%20adversarial%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Li%2C%20Y.%20Poursaeed%2C%20O.%20Hopcroft%2C%20J.%20Stacked%20generative%20adversarial%20networks%2C%202017"
        },
        {
            "id": "15",
            "entry": "[15] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He, \u201cAttngan: Fine-grained text to image generation with attentional generative adversarial networks,\u201d arXiv preprint arXiv:1711.10485, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10485"
        },
        {
            "id": "16",
            "entry": "[16] J. Johnson, A. Gupta, and L. Fei-Fei, \u201cImage generation from scene graphs,\u201d arXiv preprint arXiv:1804.01622, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01622"
        },
        {
            "id": "17",
            "entry": "[17] T. Karras, T. Aila, S. Laine, and J. Lehtinen, \u201cProgressive growing of gans for improved quality, stability, and variation,\u201d Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20T.%20Aila%2C%20T.%20Laine%2C%20S.%20Lehtinen%2C%20J.%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%2C%202018"
        },
        {
            "id": "18",
            "entry": "[18] A. Criminisi, P. P\u00e9rez, and K. Toyama, \u201cRegion filling and object removal by exemplar-based image inpainting,\u201d IEEE Transactions on image processing, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Criminisi%2C%20A.%20P%C3%A9rez%2C%20P.%20Toyama%2C%20K.%20Region%20filling%20and%20object%20removal%20by%20exemplar-based%20image%20inpainting%2C%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Criminisi%2C%20A.%20P%C3%A9rez%2C%20P.%20Toyama%2C%20K.%20Region%20filling%20and%20object%20removal%20by%20exemplar-based%20image%20inpainting%2C%202004"
        },
        {
            "id": "19",
            "entry": "[19] J. Hays and A. A. Efros, \u201cScene completion using millions of photographs,\u201d in ACM Transactions on Graphics (TOG), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hays%2C%20J.%20Efros%2C%20A.A.%20Scene%20completion%20using%20millions%20of%20photographs%2C%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hays%2C%20J.%20Efros%2C%20A.A.%20Scene%20completion%20using%20millions%20of%20photographs%2C%202007"
        },
        {
            "id": "20",
            "entry": "[20] S. S. Mirkamali and P. Nagabhushan, \u201cObject removal by depth-wise image inpainting,\u201d Signal, Image and Video Processing, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirkamali%2C%20S.S.%20Nagabhushan%2C%20P.%20Object%20removal%20by%20depth-wise%20image%20inpainting%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirkamali%2C%20S.S.%20Nagabhushan%2C%20P.%20Object%20removal%20by%20depth-wise%20image%20inpainting%2C%202015"
        },
        {
            "id": "21",
            "entry": "[21] M. Arjovsky, S. Chintala, and L. Bottou, \u201cWasserstein generative adversarial networks,\u201d in Proceedings of the International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%2C%202017"
        },
        {
            "id": "22",
            "entry": "[22] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, \u201cGenerative image inpainting with contextual attention,\u201d arXiv preprint arXiv:1801.07892, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07892"
        },
        {
            "id": "23",
            "entry": "[23] S. Iizuka, E. Simo-Serra, and H. Ishikawa, \u201cGlobally and locally consistent image completion,\u201d ACM Transactions on Graphics (TOG), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Globally%20and%20locally%20consistent%20image%20completion%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Globally%20and%20locally%20consistent%20image%20completion%2C%202017"
        },
        {
            "id": "24",
            "entry": "[24] G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catanzaro, \u201cImage inpainting for irregular holes using partial convolutions,\u201d arXiv preprint arXiv:1804.07723, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07723"
        },
        {
            "id": "25",
            "entry": "[25] L. Gatys, A. Ecker, and M. Bethge, \u201cA neural algorithm of artistic style,\u201d Nature Communications, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20L.%20Ecker%2C%20A.%20Bethge%2C%20M.%20A%20neural%20algorithm%20of%20artistic%20style%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20L.%20Ecker%2C%20A.%20Bethge%2C%20M.%20A%20neural%20algorithm%20of%20artistic%20style%2C%202015"
        },
        {
            "id": "26",
            "entry": "[26] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley, \u201cLeast squares generative adversarial networks,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.Y.%20Least%20squares%20generative%20adversarial%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.Y.%20Least%20squares%20generative%20adversarial%20networks%2C%202017"
        },
        {
            "id": "27",
            "entry": "[27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, \u201cThe PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results,\u201d http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.",
            "url": "http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Everingham%2C%20M.%20Gool%2C%20L.Van%20Williams%2C%20C.K.I.%20Winn%2C%20J.%20%E2%80%9CThe%202012"
        },
        {
            "id": "28",
            "entry": "[28] Y. Kalantidis, L. Pueyo, M. Trevisiol, R. van Zwol, and Y. Avrithis, \u201cScalable triangulation-based logo recognition,\u201d in Proceedings of ACM International Conference on Multimedia Retrieval (ICMR), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalantidis%2C%20Y.%20Pueyo%2C%20L.%20Trevisiol%2C%20M.%20van%20Zwol%2C%20R.%20Scalable%20triangulation-based%20logo%20recognition%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalantidis%2C%20Y.%20Pueyo%2C%20L.%20Trevisiol%2C%20M.%20van%20Zwol%2C%20R.%20Scalable%20triangulation-based%20logo%20recognition%2C%202011"
        },
        {
            "id": "29",
            "entry": "[29] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \u201cImage quality assessment: from error visibility to structural similarity,\u201d IEEE transactions on image processing, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%2C%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%2C%202004"
        },
        {
            "id": "30",
            "entry": "[30] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe unreasonable effectiveness of deep features as a perceptual metric,\u201d arXiv preprint arXiv:1801.03924, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03924"
        },
        {
            "id": "31",
            "entry": "[31] A. Khoreva, R. Benenson, J. H. Hosang, M. Hein, and B. Schiele, \u201cSimple does it: Weakly supervised instance and semantic segmentation.\u201d in CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khoreva%2C%20A.%20Benenson%2C%20R.%20Hosang%2C%20J.H.%20Hein%2C%20M.%20%E2%80%9CSimple%20does%20it%3A%20Weakly%20supervised%20instance%20and%20semantic%20segmentation.%E2%80%9D%20in%20CVPR%202017"
        },
        {
            "id": "32",
            "entry": "[32] J. Wang, Y. Yang, J. Mao, Z. Huang, C. Huang, and W. Xu, \u201cCnn-rnn: A unified framework for multi-label image classification,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20J.%20Yang%2C%20Y.%20Mao%2C%20J.%20Huang%2C%20Z.%20Cnn-rnn%3A%20A%20unified%20framework%20for%20multi-label%20image%20classification%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20J.%20Yang%2C%20Y.%20Mao%2C%20J.%20Huang%2C%20Z.%20Cnn-rnn%3A%20A%20unified%20framework%20for%20multi-label%20image%20classification%2C%202016"
        }
    ]
}
