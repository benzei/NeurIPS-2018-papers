{
    "filename": "7661-gumbolt-extending-gumbel-trick-to-boltzmann-priors.pdf",
    "metadata": {
        "title": "GumBolt: Extending Gumbel trick to Boltzmann priors",
        "author": "Amir H. Khoshaman, Mohammad Amin",
        "date": 2016,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7661-gumbolt-extending-gumbel-trick-to-boltzmann-priors.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Boltzmann machines (BMs) are appealing candidates for powerful priors in variational autoencoders (VAEs), as they are capable of capturing nontrivial and multimodal distributions over discrete variables. However, non-differentiability of the discrete units prohibits using the reparameterization trick, essential for low-noise back propagation. The Gumbel trick resolves this problem in a consistent way by relaxing the variables and distributions, but it is incompatible with BM priors. Here, we propose the GumBolt, a model that extends the Gumbel trick to BM priors in VAEs. GumBolt is significantly simpler than the recently proposed methods with BM prior and outperforms them by a considerable margin. It achieves state-of-theart performance on permutation invariant MNIST and OMNIGLOT datasets in the scope of models with only discrete latent variables. Moreover, the performance can be further improved by allowing multi-sampled (importance-weighted) estimation of log-likelihood in training, which was not possible with previous models."
    },
    "keywords": [
        {
            "term": "deep belief network",
            "url": "https://en.wikipedia.org/wiki/deep_belief_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "markov chain",
            "url": "https://en.wikipedia.org/wiki/markov_chain"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "discrete variable",
            "url": "https://en.wikipedia.org/wiki/discrete_variable"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "restricted boltzmann machine",
            "url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_Machine"
        },
        {
            "term": "boltzmann machine",
            "url": "https://en.wikipedia.org/wiki/boltzmann_machine"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Variational autoencoders<br/><br/>Consider a generative model involving observable variables x and latent variables z",
        "The contributions of this work are as follows: we propose the GumBolt, which extends the Gumbel trick to Boltzmann machines and Markov fields priors and is significantly simpler than previous models that marginalize discrete variables",
        "We show that Boltzmann machines are compatible with relaxation of discrete variables in Gumbel trick",
        "We propose an objective using such relaxation and show that the main limitations of previous models with Boltzmann machines priors can be circumvented; we do not need marginalization of the discrete variables, and can have an importance-weighted objective",
        "We have proposed the GumBolt that extends the Gumbel trick to Markov random fields and Boltzmann machines",
        "GumBolt is much simpler than previous models that require marginalization of the discrete variables and achieves state-of-the-art performance on MNIST and OMNIGLOT datasets in the context of models with only discrete variables"
    ],
    "key_statements": [
        "Variational autoencoders<br/><br/>Consider a generative model involving observable variables x and latent variables z",
        "The contributions of this work are as follows: we propose the GumBolt, which extends the Gumbel trick to Boltzmann machines and Markov fields priors and is significantly simpler than previous models that marginalize discrete variables",
        "We show that Boltzmann machines are compatible with relaxation of discrete variables in Gumbel trick",
        "We propose an objective using such relaxation and show that the main limitations of previous models with Boltzmann machines priors can be circumvented; we do not need marginalization of the discrete variables, and can have an importance-weighted objective",
        "If the Boltzmann machines prior is replaced with a factorial Bernoulli distribution, GumBolt transforms into CONCRETE (<a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\">Maddison et al, 2016</a>) and Gumbel-Softmax (<a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\">Jang et al, 2016</a>)",
        "We have proposed the GumBolt that extends the Gumbel trick to Markov random fields and Boltzmann machines",
        "We have shown that this approach is effective and on the entirety of a wide host of structures outperforms the other models that use Boltzmann machines in their priors",
        "GumBolt is much simpler than previous models that require marginalization of the discrete variables and achieves state-of-the-art performance on MNIST and OMNIGLOT datasets in the context of models with only discrete variables"
    ],
    "summary": [
        "Variational autoencoders<br/><br/>Consider a generative model involving observable variables x and latent variables z.",
        "The contributions of this work are as follows: we propose the GumBolt, which extends the Gumbel trick to BM and MRF priors and is significantly simpler than previous models that marginalize discrete variables.",
        "We propose an objective using such relaxation and show that the main limitations of previous models with BM priors can be circumvented; we do not need marginalization of the discrete variables, and can have an importance-weighted objective.",
        "The term in the discrete objective that involves the prior distribution is Eq (z|x) [log p\u2713(z)].",
        "In the case of a BM prior, according to theorem (1), the extrema of log p\u2713(\u21e3) / E\u2713(z) occur on the boundaries; this shows that having a BM rather than a factorial Bernoulli distribution does not exacerbate the training of GumBolt.",
        "In order to explore the effectiveness of the GumBolt, we present the results of a wide set of experiments conducted on standard feed-forward structures that have been used to study models with discrete latent variables (<a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\"><a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\"><a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\">Maddison et al, 2016</a></a></a>; Tucker et al, 2017; <a class=\"ref-link\" id=\"cVahdat_et+al_2018_a\" href=\"#rVahdat_et+al_2018_a\"><a class=\"ref-link\" id=\"cVahdat_et+al_2018_a\" href=\"#rVahdat_et+al_2018_a\">Vahdat et al, 2018</a></a>).",
        "In order to estimate the log-partition function, log Z\u2713, a GPU implementation of parallel tempering algorithm with bridge sampling was used (<a class=\"ref-link\" id=\"cDesjardins_et+al_2010_a\" href=\"#rDesjardins_et+al_2010_a\">Desjardins et al, 2010</a>; <a class=\"ref-link\" id=\"cBennett_1976_a\" href=\"#rBennett_1976_a\">Bennett, 1976</a>; <a class=\"ref-link\" id=\"cShirts_2008_a\" href=\"#rShirts_2008_a\">Shirts and Chodera, 2008</a>), with a set of parameters to ensure the variance in log Z\u2713 is less than 0.01: 20K burn-in steps were followed by 100K sweeps, 20 times, with a pilot run to determine the inverse temperatures.",
        "If the BM prior is replaced with a factorial Bernoulli distribution, GumBolt transforms into CONCRETE (<a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\"><a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\"><a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\">Maddison et al, 2016</a></a></a>) and Gumbel-Softmax (<a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\"><a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\">Jang et al, 2016</a></a>).",
        "Since the performance of CONCRETE and Gumbel-Softmax has been extensively compared against other models (<a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\"><a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\"><a class=\"ref-link\" id=\"cMaddison_et+al_2016_a\" href=\"#rMaddison_et+al_2016_a\">Maddison et al, 2016</a></a></a>; <a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\"><a class=\"ref-link\" id=\"cJang_et+al_2016_a\" href=\"#rJang_et+al_2016_a\">Jang et al, 2016</a></a>; Tucker et al, 2017; <a class=\"ref-link\" id=\"cGrathwohl_et+al_2017_a\" href=\"#rGrathwohl_et+al_2017_a\">Grathwohl et al, 2017</a>), we do not repeat these experiments here; we note that CONCRETE performs favorably to other discrete latent variable models in most cases.",
        "It was shown in (<a class=\"ref-link\" id=\"cVahdat_et+al_2018_a\" href=\"#rVahdat_et+al_2018_a\"><a class=\"ref-link\" id=\"cVahdat_et+al_2018_a\" href=\"#rVahdat_et+al_2018_a\">Vahdat et al, 2018</a></a>) that dVAE and dVAE++ outperform other models with discrete latent variables (REBAR, RELAX, VIMCO, CONCRETE and Gumbel-Softmax) on the same structure.",
        "We have proposed the GumBolt that extends the Gumbel trick to Markov random fields and BMs. We have shown that this approach is effective and on the entirety of a wide host of structures outperforms the other models that use BMs in their priors.",
        "GumBolt is much simpler than previous models that require marginalization of the discrete variables and achieves state-of-the-art performance on MNIST and OMNIGLOT datasets in the context of models with only discrete variables"
    ],
    "headline": "We propose the GumBolt, a model that extends the Gumbel trick to Boltzmann machines priors in Variational autoencoders",
    "reference_links": [
        {
            "id": "Amin_et+al_2016_a",
            "entry": "Amin, M. H., Andriyash, E., Rolfe, J., Kulchytskyy, B., and Melko, R. (2016). Quantum boltzmann machine.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amin%2C%20M.H.%20Andriyash%2C%20E.%20Rolfe%2C%20J.%20Kulchytskyy%2C%20B.%20Quantum%20boltzmann%20machine%202016"
        },
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Bengio, Y., L\u00e9onard, N., and Courville, A. (2013). Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "Bennett_1976_a",
            "entry": "Bennett, C. H. (1976). Efficient estimation of free energy differences from monte carlo data. Journal of Computational Physics, 22(2):245\u2013268.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bennett%2C%20C.H.%20Efficient%20estimation%20of%20free%20energy%20differences%20from%20monte%20carlo%20data%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bennett%2C%20C.H.%20Efficient%20estimation%20of%20free%20energy%20differences%20from%20monte%20carlo%20data%201976"
        },
        {
            "id": "Bishop_2011_a",
            "entry": "Bishop, C. M. (2011). Pattern Recognition and Machine Learning. Springer, New York, 1st ed. 2006. corr. 2nd printing 2011 edition edition.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.M.%20Pattern%20Recognition%20and%20Machine%20Learning%202011"
        },
        {
            "id": "Burda_et+al_2015_a",
            "entry": "Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. arXiv preprint arXiv:1509.00519.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00519"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I., and Abbeel, P. (2016). Variational lossy autoencoder. arXiv preprint arXiv:1611.02731.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02731"
        },
        {
            "id": "Desjardins_et+al_2010_a",
            "entry": "Desjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tempered markov chain monte carlo for training of restricted boltzmann machines. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 145\u2013152.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Desjardins%2C%20G.%20Courville%2C%20A.%20Bengio%2C%20Y.%20Vincent%2C%20P.%20Tempered%20markov%20chain%20monte%20carlo%20for%20training%20of%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Desjardins%2C%20G.%20Courville%2C%20A.%20Bengio%2C%20Y.%20Vincent%2C%20P.%20Tempered%20markov%20chain%20monte%20carlo%20for%20training%20of%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Germain, M., Gregor, K., Murray, I., and Larochelle, H. (2015). Made: masked autoencoder for distribution estimation. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 881\u2013889.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20M.%20Gregor%2C%20K.%20Murray%2C%20I.%20Larochelle%2C%20H.%20Made%3A%20masked%20autoencoder%20for%20distribution%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20M.%20Gregor%2C%20K.%20Murray%2C%20I.%20Larochelle%2C%20H.%20Made%3A%20masked%20autoencoder%20for%20distribution%20estimation%202015"
        },
        {
            "id": "Goyal_et+al_2017_a",
            "entry": "Goyal, A. G. A. P., Sordoni, A., C\u00f4t\u00e9, M.-A., Ke, N., and Bengio, Y. (2017). Z-forcing: Training stochastic recurrent networks. In Advances in Neural Information Processing Systems, pages 6716\u20136726.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20A.G.A.P.%20Sordoni%2C%20A.%20C%C3%B4t%C3%A9%2C%20M.-A.%20Ke%2C%20N.%20Z-forcing%3A%20Training%20stochastic%20recurrent%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20A.G.A.P.%20Sordoni%2C%20A.%20C%C3%B4t%C3%A9%2C%20M.-A.%20Ke%2C%20N.%20Z-forcing%3A%20Training%20stochastic%20recurrent%20networks%202017"
        },
        {
            "id": "Grathwohl_et+al_2017_a",
            "entry": "Grathwohl, W., Choi, D., Wu, Y., Roeder, G., and Duvenaud, D. (2017). Backpropagation through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint arXiv:1711.00123.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00123"
        },
        {
            "id": "Gregor_et+al_2015_a",
            "entry": "Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. (2015). Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623.",
            "arxiv_url": "https://arxiv.org/pdf/1502.04623"
        },
        {
            "id": "Gregor_et+al_2013_a",
            "entry": "Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2013). Deep autoregressive networks. arXiv preprint arXiv:1310.8499.",
            "arxiv_url": "https://arxiv.org/pdf/1310.8499"
        },
        {
            "id": "Gu_et+al_2015_a",
            "entry": "Gu, S., Levine, S., Sutskever, I., and Mnih, A. (2015). Muprop: Unbiased backpropagation for stochastic neural networks. arXiv preprint arXiv:1511.05176.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05176"
        },
        {
            "id": "Gulrajani_et+al_2016_a",
            "entry": "Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D., and Courville, A. (2016). Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05013"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013 456.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jang_et+al_2016_a",
            "entry": "Jang, E., Gu, S., and Poole, B. (2016). Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "Khoshaman_et+al_2018_a",
            "entry": "Khoshaman, A., Vinci, W., Denis, B., Andriyash, E., and Amin, M. H. (2018). Quantum variational autoencoder. Quantum Science and Technology, 4(1):014001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khoshaman%2C%20A.%20Vinci%2C%20W.%20Denis%2C%20B.%20Andriyash%2C%20E.%20Quantum%20variational%20autoencoder%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khoshaman%2C%20A.%20Vinci%2C%20W.%20Denis%2C%20B.%20Andriyash%2C%20E.%20Quantum%20variational%20autoencoder%202018"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_et+al_2014_b",
            "entry": "Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. (2014). Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pages 3581\u20133589.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20Welling%2C%20M.%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20Welling%2C%20M.%20Semi-supervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016). Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743\u20134751.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Jozefowicz%2C%20R.%20Chen%2C%20X.%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Jozefowicz%2C%20R.%20Chen%2C%20X.%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Lake_et+al_2015_a",
            "entry": "Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "Le_2008_a",
            "entry": "Le Roux, N. and Bengio, Y. (2008). Representational power of restricted boltzmann machines and deep belief networks. Neural computation, 20(6):1631\u20131649.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%20Roux%2C%20N.%20Bengio%2C%20Y.%20Representational%20power%20of%20restricted%20boltzmann%20machines%20and%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%20Roux%2C%20N.%20Bengio%2C%20Y.%20Representational%20power%20of%20restricted%20boltzmann%20machines%20and%20deep%20belief%20networks%202008"
        },
        {
            "id": "Maal_et+al_2017_a",
            "entry": "Maal\u00f8e, L., Fraccaro, M., and Winther, O. (2017). Semi-supervised generation with cluster-aware generative models. arXiv preprint arXiv:1704.00637.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00637"
        },
        {
            "id": "Maddison_et+al_2016_a",
            "entry": "Maddison, C. J., Mnih, A., and Teh, Y. W. (2016). The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "Mnih_2014_a",
            "entry": "Mnih, A. and Gregor, K. (2014). Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0030"
        },
        {
            "id": "Mnih_2016_a",
            "entry": "Mnih, A. and Rezende, D. (2016). Variational inference for monte carlo objectives. In International Conference on Machine Learning, pages 2188\u20132196.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20A.%20Rezende%2C%20D.%20Variational%20inference%20for%20monte%20carlo%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20A.%20Rezende%2C%20D.%20Variational%20inference%20for%20monte%20carlo%20objectives%202016"
        },
        {
            "id": "Oord_et+al_2016_a",
            "entry": "Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu, K. (2016). Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "Raiko_et+al_2014_a",
            "entry": "Raiko, T., Berglund, M., Alain, G., and Dinh, L. (2014). Techniques for learning binary stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989.",
            "arxiv_url": "https://arxiv.org/pdf/1406.2989"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05770"
        },
        {
            "id": "Rolfe_2016_a",
            "entry": "Rolfe, J. T. (2016). Discrete variational autoencoders. arXiv preprint arXiv:1609.02200.",
            "arxiv_url": "https://arxiv.org/pdf/1609.02200"
        },
        {
            "id": "Ross_2013_a",
            "entry": "Ross, S. M. (2013). Applied probability models with optimization applications. Courier Corporation.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20S.M.%20Applied%20probability%20models%20with%20optimization%20applications%202013"
        },
        {
            "id": "Salakhutdinov_2008_a",
            "entry": "Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Murray%2C%20I.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Murray%2C%20I.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008"
        },
        {
            "id": "Salimans_et+al_2015_a",
            "entry": "Proceedings of the 25th international conference on Machine learning, pages 872\u2013879. ACM. Salimans, T., Kingma, D., and Welling, M. (2015). Markov chain monte carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pages 1218\u20131226.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20A.C.M.%20T.%2C%20Kingma%20D.%20Welling%2C%20M.%20Markov%20chain%20monte%20carlo%20and%20variational%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20A.C.M.%20T.%2C%20Kingma%20D.%20Welling%2C%20M.%20Markov%20chain%20monte%20carlo%20and%20variational%20inference%202015"
        },
        {
            "id": "Serr_et+al_2018_a",
            "entry": "Serr\u00e0, J., Sur\u00eds, D., Miron, M., and Karatzoglou, A. (2018). Overcoming catastrophic forgetting with hard attention to the task. arXiv preprint arXiv:1801.01423.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01423"
        },
        {
            "id": "Shirts_2008_a",
            "entry": "Shirts, M. R. and Chodera, J. D. (2008). Statistically optimal analysis of samples from multiple equilibrium states. The Journal of chemical physics, 129(12):124105.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shirts%2C%20M.R.%20Chodera%2C%20J.D.%20Statistically%20optimal%20analysis%20of%20samples%20from%20multiple%20equilibrium%20states%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shirts%2C%20M.R.%20Chodera%2C%20J.D.%20Statistically%20optimal%20analysis%20of%20samples%20from%20multiple%20equilibrium%20states%202008"
        },
        {
            "id": "S_et+al_2016_a",
            "entry": "S\u00f8nderby, C. K., Raiko, T., Maal\u00f8e, L., S\u00f8nderby, S. K., and Winther, O. (2016). Ladder variational autoencoders. In Advances in neural information processing systems, pages 3738\u20133746.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%B8nderby%2C%20C.K.%20Raiko%2C%20T.%20Maal%C3%B8e%2C%20L.%20S%C3%B8nderby%2C%20S.K.%20Ladder%20variational%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%B8nderby%2C%20C.K.%20Raiko%2C%20T.%20Maal%C3%B8e%2C%20L.%20S%C3%B8nderby%2C%20S.K.%20Ladder%20variational%20autoencoders%202016"
        },
        {
            "id": "Tieleman_2008_a",
            "entry": "Tieleman, T. (2008). Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings of the 25th international conference on Machine learning, pages 1064\u2013 1071. ACM. Tucker, G., Mnih, A., Maddison, C. J., Lawson, J., and Sohl-Dickstein, J. (2017). Rebar: Lowvariance, unbiased gradient estimates for discrete latent variable models. In Advances in Neural Information Processing Systems, pages 2624\u20132633.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20Training%20restricted%20boltzmann%20machines%20using%20approximations%20to%20the%20likelihood%20gradient%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20Training%20restricted%20boltzmann%20machines%20using%20approximations%20to%20the%20likelihood%20gradient%202008"
        },
        {
            "id": "Vahdat_et+al_2018_a",
            "entry": "Vahdat, A., Macready, W. G., Bian, Z., and Khoshaman, A. (2018). Dvae++: Discrete variational autoencoders with overlapping transformations. arXiv preprint arXiv:1802.04920.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04920"
        },
        {
            "id": "Williams_1992_a",
            "entry": "Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pages 5\u201332. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "Yeung_et+al_2017_a",
            "entry": "Yeung, S., Kannan, A., Dauphin, Y., and Fei-Fei, L. (2017). Tackling over-pruning in variational autoencoders. arXiv preprint arXiv:1706.03643.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03643"
        }
    ]
}
