{
    "filename": "8124-learning-safe-policies-with-expert-guidance.pdf",
    "metadata": {
        "title": "Learning Safe Policies with Expert Guidance",
        "author": "Jessie Huang, Fa Wu, Doina Precup, Yang Cai",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8124-learning-safe-policies-with-expert-guidance.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the \"follow-the-perturbed-leader\" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states."
    },
    "keywords": [
        {
            "term": "fully polynomial time approximation scheme",
            "url": "https://en.wikipedia.org/wiki/fully_polynomial_time_approximation_scheme"
        },
        {
            "term": "markov decision process",
            "url": "https://en.wikipedia.org/wiki/markov_decision_process"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "ellipsoid method",
            "url": "https://en.wikipedia.org/wiki/ellipsoid_method"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        }
    ],
    "highlights": [
        "In Reinforcement Learning (RL), agent behavior is driven by an objective function defined through the specification of rewards",
        "Misspecified rewards may lead to negative side effects [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], when the agent acts unpredictably responding to the aspects of the environment that the designer overlooked, and potentially causes harms to the environment or itself",
        "We provided a theoretical treatment of the problem of reinforcement learning in the presence of mis-specifications of the agent\u2019s reward function, by leveraging data provided by experts",
        "The posed optimization can be solved exactly in polynomial-time by using the ellipsoid methods, but a more practical solution is provided by an algorithm which takes a follow-the-perturbed-leader approach",
        "It will be interesting to see whether our maxmin formulation can be combined with other methods in Reinforcement Learning such as hierarchical learning to produce robust solutions in larger problems"
    ],
    "key_statements": [
        "In Reinforcement Learning (RL), agent behavior is driven by an objective function defined through the specification of rewards",
        "Misspecified rewards may lead to negative side effects [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], when the agent acts unpredictably responding to the aspects of the environment that the designer overlooked, and potentially causes harms to the environment or itself",
        "If we want to have some type of safety guarantees in terms of the behavior of an agent learned by Reinforcement Learning once it is deployed in the real world, it is crucial to have a learning algorithm that is robust to mis-specifications",
        "We show that Algorithm 3 still has similar performance when we only have a fully polynomial time approximation scheme (FPTAS) for solving the MDP",
        "We provide three different ways to define the set of consistent reward functions in Section 3, and present the ellipsoid-based exact algorithm and its analysis in Section 4.1",
        "We provided a theoretical treatment of the problem of reinforcement learning in the presence of mis-specifications of the agent\u2019s reward function, by leveraging data provided by experts",
        "The posed optimization can be solved exactly in polynomial-time by using the ellipsoid methods, but a more practical solution is provided by an algorithm which takes a follow-the-perturbed-leader approach",
        "It will be interesting to see whether our maxmin formulation can be combined with other methods in Reinforcement Learning such as hierarchical learning to produce robust solutions in larger problems"
    ],
    "summary": [
        "In Reinforcement Learning (RL), agent behavior is driven by an objective function defined through the specification of rewards.",
        "Our first result (Theorem 1) shows that given any algorithm that can find the optimal policy for an MDP in polynomial time, we can solve the maxmin learning problem exactly in polynomial time.",
        "We can solve the maxmin learning problem in polynomial time using the ellipsoid method.",
        "Separation Oracles To perform maxmin learning, we often need to optimize linear functions over convex sets that are intersections of exponentially many halfspaces.",
        "We will not distinguish between different ways to define and access the consistent reward polytope PR, but assume that we have a polynomial time separation oracle for it.",
        "Given a polynomial time separation oracle SOR for the consistent reward polytope PR and an exact polynomial time MDP solver ALG, we have a polynomial time algorithm such that for any MDP without the reward function M \\R, the algorithm computes the maxmin policy \u21e1\u21e4 with respect to M \\R and PR.",
        "We show that given an exact MDP solver ALG, we can design a polynomial time separation oracle for the set of feasible variables (\u03bc, z) of LP 1.",
        "The key observation is that the linear optimization problem over polytope PF : max\u03bc2PF w \u00b7 \u03bc is exactly the same as solving the MDP with reward function R(\u00b7) = w \u00b7 (\u00b7).",
        "We compute the maxmin policy in the \"real-world\" MDP of a much larger size (50\u21e550) with all 5 terrain types using Algorithm 3 with the reward polytope PR implicitly specified by the expert policy.",
        "The baseline was operates to be different from the training MDP computed with a random reward for the fifth terin which the expert demonstrates, as long as the rain and the other four terrain rewards set the same two MDPs share the same feature space.",
        "In the gridworld experiments, goal of avoiding potential negative side effects, one has to explicitly assign a reward to the \"unas the fraction of trajectory of each terrain type known\" feature in order to apply apprenticeship closely resemble the expert.",
        "While we can only solve the problem approximately using model-free learning methods, our experiments show that our FPL-based algorithm can learn a safe policy efficiently for a continuous task.",
        "The experiment demonstrates that our maxmin method works well with complex reinforcement learning tasks where only approximate MDP solvers are available.",
        "The posed optimization can be solved exactly in polynomial-time by using the ellipsoid methods, but a more practical solution is provided by an algorithm which takes a follow-the-perturbed-leader approach."
    ],
    "headline": "We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "2",
            "entry": "[2] Kareem Amin, Nan Jiang, and Satinder Singh. Repeated inverse reinforcement learning. In Advances in Neural Information Processing Systems, pages 1813\u20131822, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amin%2C%20Kareem%20Jiang%2C%20Nan%20Singh%2C%20Satinder%20Repeated%20inverse%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amin%2C%20Kareem%20Jiang%2C%20Nan%20Singh%2C%20Satinder%20Repeated%20inverse%20reinforcement%20learning%202017"
        },
        {
            "id": "3",
            "entry": "[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "4",
            "entry": "[4] Aharon Ben-Tal, Elad Hazan, Tomer Koren, and Shie Mannor. Oracle-based robust optimization via online learning. Operations Research, 63(3):628\u2013638, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-Tal%2C%20Aharon%20Hazan%2C%20Elad%20Koren%2C%20Tomer%20Mannor%2C%20Shie%20Oracle-based%20robust%20optimization%20via%20online%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-Tal%2C%20Aharon%20Hazan%2C%20Elad%20Koren%2C%20Tomer%20Mannor%2C%20Shie%20Oracle-based%20robust%20optimization%20via%20online%20learning%202015"
        },
        {
            "id": "5",
            "entry": "[5] Stephen Boyd and Lieven Vandenberghe. Localization and cutting-plane methods. From Stanford EE 364b lecture notes, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Localization%20and%20cutting-plane%20methods.%20From%202007"
        },
        {
            "id": "6",
            "entry": "[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "7",
            "entry": "[7] Yang Cai, Constantinos Daskalakis, and S. Matthew Weinberg. Reducing Revenue to Welfare Maximization : Approximation Algorithms and other Generalizations. In the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Yang%20Daskalakis%2C%20Constantinos%20Weinberg%2C%20S.Matthew%20Reducing%20Revenue%20to%20Welfare%20Maximization%20%3A%20Approximation%20Algorithms%20and%20other%20Generalizations.%20In%20the%2024th%20Annual%20ACM-SIAM%20Symposium%20on%20Discrete%20Algorithms%20%28SODA%29%202013"
        },
        {
            "id": "8",
            "entry": "[8] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicolo%20Prediction%2C%20Gabor%20Lugosi%20Learning%2C%20and%20Games%202006"
        },
        {
            "id": "9",
            "entry": "[9] Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. The Ellipsoid Method and its Consequences in Combinatorial Optimization. Combinatorica, 1(2):169\u2013197, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%B6tschel%2C%20Martin%20Lov%C3%A1sz%2C%20L%C3%A1szl%C3%B3%20Schrijver%2C%20Alexander%20The%20Ellipsoid%20Method%20and%20its%20Consequences%20in%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gr%C3%B6tschel%2C%20Martin%20Lov%C3%A1sz%2C%20L%C3%A1szl%C3%B3%20Schrijver%2C%20Alexander%20The%20Ellipsoid%20Method%20and%20its%20Consequences%20in%201981"
        },
        {
            "id": "10",
            "entry": "[10] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design. In Advances in Neural Information Processing Systems, pages 6768\u20136777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dylan%20HadfieldMenell%20Smitha%20Milli%20Pieter%20Abbeel%20Stuart%20J%20Russell%20and%20Anca%20Dragan%20Inverse%20reward%20design%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2067686777%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dylan%20HadfieldMenell%20Smitha%20Milli%20Pieter%20Abbeel%20Stuart%20J%20Russell%20and%20Anca%20Dragan%20Inverse%20reward%20design%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2067686777%202017"
        },
        {
            "id": "11",
            "entry": "[11] Marcus Hutter and Jan Poland. Adaptive online prediction by following the perturbed leader. Journal of Machine Learning Research, 6(Apr):639\u2013660, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hutter%2C%20Marcus%20Poland%2C%20Jan%20Adaptive%20online%20prediction%20by%20following%20the%20perturbed%20leader%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hutter%2C%20Marcus%20Poland%2C%20Jan%20Adaptive%20online%20prediction%20by%20following%20the%20perturbed%20leader%202005"
        },
        {
            "id": "12",
            "entry": "[12] Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):257\u2013280, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyengar%2C%20Garud%20N.%20Robust%20dynamic%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iyengar%2C%20Garud%20N.%20Robust%20dynamic%20programming%202005"
        },
        {
            "id": "13",
            "entry": "[13] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291\u2013307, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalai%2C%20Adam%20Vempala%2C%20Santosh%20Efficient%20algorithms%20for%20online%20decision%20problems%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalai%2C%20Adam%20Vempala%2C%20Santosh%20Efficient%20algorithms%20for%20online%20decision%20problems%202005"
        },
        {
            "id": "14",
            "entry": "[14] Richard M. Karp and Christos H. Papadimitriou. On linear characterizations of combinatorial optimization problems. SIAM J. Comput., 11(4):620\u2013632, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karp%2C%20Richard%20M.%20Papadimitriou%2C%20Christos%20H.%20On%20linear%20characterizations%20of%20combinatorial%20optimization%20problems%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karp%2C%20Richard%20M.%20Papadimitriou%2C%20Christos%20H.%20On%20linear%20characterizations%20of%20combinatorial%20optimization%20problems%201982"
        },
        {
            "id": "15",
            "entry": "[15] Leonid G. Khachiyan. A Polynomial Algorithm in Linear Programming. Soviet Mathematics Doklady, 20(1):191\u2013194, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khachiyan%2C%20Leonid%20G.%20A%20Polynomial%20Algorithm%20in%20Linear%20Programming%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khachiyan%2C%20Leonid%20G.%20A%20Polynomial%20Algorithm%20in%20Linear%20Programming%201979"
        },
        {
            "id": "16",
            "entry": "[16] Shiau Hong Lim, Huan Xu, and Shie Mannor. Reinforcement learning in robust markov decision processes. In Advances in Neural Information Processing Systems, pages 701\u2013709, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lim%2C%20Shiau%20Hong%20Xu%2C%20Huan%20Mannor%2C%20Shie%20Reinforcement%20learning%20in%20robust%20markov%20decision%20processes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lim%2C%20Shiau%20Hong%20Xu%2C%20Huan%20Mannor%2C%20Shie%20Reinforcement%20learning%20in%20robust%20markov%20decision%20processes%202013"
        },
        {
            "id": "17",
            "entry": "[17] Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation, 17(2):335\u2013 359, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morimoto%2C%20Jun%20Doya%2C%20Kenji%20Robust%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morimoto%2C%20Jun%20Doya%2C%20Kenji%20Robust%20reinforcement%20learning%202005"
        },
        {
            "id": "18",
            "entry": "[18] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, pages 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "19",
            "entry": "[19] Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition matrices. Operations Research, 53(5):780\u2013798, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nilim%2C%20Arnab%20Ghaoui%2C%20Laurent%20El%20Robust%20control%20of%20markov%20decision%20processes%20with%20uncertain%20transition%20matrices%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nilim%2C%20Arnab%20Ghaoui%2C%20Laurent%20El%20Robust%20control%20of%20markov%20decision%20processes%20with%20uncertain%20transition%20matrices%202005"
        },
        {
            "id": "20",
            "entry": "[20] Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th international conference on Machine learning, pages 1032\u20131039. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Schapire.%20Apprenticeship%20learning%20using%20linear%20programming%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Schapire.%20Apprenticeship%20learning%20using%20linear%20programming%202008"
        },
        {
            "id": "21",
            "entry": "[21] Umar Syed and Robert E Schapire. A game-theoretic approach to apprenticeship learning. In Advances in neural information processing systems, pages 1449\u20131456, 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syed%2C%20Umar%20Schapire%2C%20Robert%20E.%20A%20game-theoretic%20approach%20to%20apprenticeship%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Syed%2C%20Umar%20Schapire%2C%20Robert%20E.%20A%20game-theoretic%20approach%20to%20apprenticeship%20learning%202008"
        }
    ]
}
