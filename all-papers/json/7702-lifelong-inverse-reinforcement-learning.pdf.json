{
    "filename": "7702-lifelong-inverse-reinforcement-learning.pdf",
    "metadata": {
        "title": "Lifelong Inverse Reinforcement Learning",
        "author": "Jorge Armando Mendez Mendez, Shashank Shivkumar, Eric Eaton",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7702-lifelong-inverse-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required. As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance."
    },
    "keywords": [
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "Maximum Entropy",
            "url": "https://en.wikipedia.org/wiki/Maximum_Entropy"
        },
        {
            "term": "lifelong learning",
            "url": "https://en.wikipedia.org/wiki/lifelong_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "learning from demonstration",
            "url": "https://en.wikipedia.org/wiki/learning_from_demonstration"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "Such as personal robotics or intelligent virtual assistants, a user may want to teach an agent to perform some sequential decision-making task",
        "As an instantiation of our framework, we propose the Efficient Lifelong inverse reinforcement learning (ELIRL) algorithm, which adapts Maximum Entropy (MaxEnt) inverse reinforcement learning [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] into a lifelong learning setting",
        "In contrast to most previous work on inverse reinforcement learning, which focuses on single-task learning, this paper focuses on online multi-task inverse reinforcement learning",
        "We evaluated Efficient Lifelong IRL on two environments, chosen to allow us to create arbitrarily many tasks with distinct reward functions",
        "Objectworld: Similar to the environment presented by Levine et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], Objectworld is a 32 \u21e5 32 grid populated by colored objects in random cells",
        "We introduced the novel problem of lifelong inverse reinforcement learning, and presented a general framework that is capable of sharing learned knowledge about the reward functions between inverse reinforcement learning tasks"
    ],
    "key_statements": [
        "Such as personal robotics or intelligent virtual assistants, a user may want to teach an agent to perform some sequential decision-making task",
        "We introduce the novel problem of lifelong learning from demonstration, in which an agent will face multiple consecutive learning from demonstration tasks and must optimize its overall performance",
        "As an instantiation of our framework, we propose the Efficient Lifelong inverse reinforcement learning (ELIRL) algorithm, which adapts Maximum Entropy (MaxEnt) inverse reinforcement learning [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] into a lifelong learning setting",
        "In contrast to most previous work on inverse reinforcement learning, which focuses on single-task learning, this paper focuses on online multi-task inverse reinforcement learning",
        "We focus on Maximum Entropy inverse reinforcement learning, Efficient Lifelong IRL can be adapted to other inverse reinforcement learning approaches, as shown in Appendix D",
        "We evaluated Efficient Lifelong IRL on two environments, chosen to allow us to create arbitrarily many tasks with distinct reward functions",
        "Objectworld: Similar to the environment presented by Levine et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], Objectworld is a 32 \u21e5 32 grid populated by colored objects in random cells",
        "Figure 1 shows the advantage of sharing knowledge among inverse reinforcement learning tasks",
        "We examined how performance on the earliest tasks changed during the lifelong learning process",
        "We introduced the novel problem of lifelong inverse reinforcement learning, and presented a general framework that is capable of sharing learned knowledge about the reward functions between inverse reinforcement learning tasks"
    ],
    "summary": [
        "Such as personal robotics or intelligent virtual assistants, a user may want to teach an agent to perform some sequential decision-making task.",
        "An early approach to multi-task IRL [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] learned different tasks by sampling from a joint prior on the rewards and policies, assuming that the state-action spaces are shared.",
        "These methods assume the state-action space is shared across tasks, and, as an inner loop in the optimization, learn a single policy for all tasks.",
        "It is fully online and computationally efficient, enabling it to rapidly learn the reward function for each new task via transfer and update a shared knowledge repository.",
        "New knowledge is transferred in reverse to improve the reward functions of previous tasks, thereby optimizing all tasks.",
        "When the IRL tasks are related, knowledge transfer between their reward functions has the potential to improve the learned reward function for each task and reduce the number of expert demonstrations needed.",
        "From the parameterized reward learned by single-task MaxEnt. We compute the feature counts for a fixed number of finite horizon paths by following the stochastic policy",
        "This online approximation removes the dependence of Equation 3 on the numbers of training samples and tasks, making it scalable for lifelong learning, and provides guarantees on its convergence with equivalent performance to the full multi-task objective [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>].",
        "Since IRL in general requires finding the optimal policy for different choices of the reward function as an inner loop in the optimization, the additional dependence on N would make any IRL method intractable in a lifelong setting.",
        "ELIRL learned the reward functions more accurately than its base learner, MaxEnt IRL, after sufficient tasks were used to train the knowledge base L.",
        "In order to analyze how ELIRL captures the latent structure underlying the tasks, we created new instances of Objectworld and used a single learned latent component as the reward of each new MDP (i.e., a column of L, which can be treated as a latent reward function factor).",
        "We performed additional experiments to show how simple extensions to ELIRL can transfer knowledge across tasks with different feature spaces and with continuous state-action spaces.",
        "ELIRL can support transfer across task domains with different feature spaces by adapting prior work in cross-domain transfer [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]; details of this extension are given in Appendix C.",
        "We introduced the novel problem of lifelong IRL, and presented a general framework that is capable of sharing learned knowledge about the reward functions between IRL tasks.",
        "We intend to study how more powerful base learners can be used for the learning of more complex tasks, potentially from human demonstrations"
    ],
    "headline": "We introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning (ICML-04), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20Pieter%20Ng%2C%20Andrew%20Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20Pieter%20Ng%2C%20Andrew%20Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "2",
            "entry": "[2] Monica Babes, Vukosi N. Marivate, Kaushik Subramanian, and Michael L. Littman. Apprenticeship learning about multiple intentions. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Babes%2C%20Monica%20Marivate%2C%20Vukosi%20N.%20Subramanian%2C%20Kaushik%20Littman%2C%20Michael%20L.%20Apprenticeship%20learning%20about%20multiple%20intentions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Babes%2C%20Monica%20Marivate%2C%20Vukosi%20N.%20Subramanian%2C%20Kaushik%20Littman%2C%20Michael%20L.%20Apprenticeship%20learning%20about%20multiple%20intentions%202011"
        },
        {
            "id": "3",
            "entry": "[3] Haitham Bou Ammar, Eric Eaton, Jose Marcio Luna, and Paul Ruvolo. Autonomous crossdomain knowledge transfer in lifelong policy gradient reinforcement learning. In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI-15), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ammar%2C%20Haitham%20Bou%20Eaton%2C%20Eric%20Luna%2C%20Jose%20Marcio%20Ruvolo%2C%20Paul%20Autonomous%20crossdomain%20knowledge%20transfer%20in%20lifelong%20policy%20gradient%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ammar%2C%20Haitham%20Bou%20Eaton%2C%20Eric%20Luna%2C%20Jose%20Marcio%20Ruvolo%2C%20Paul%20Autonomous%20crossdomain%20knowledge%20transfer%20in%20lifelong%20policy%20gradient%20reinforcement%20learning%202015"
        },
        {
            "id": "4",
            "entry": "[4] Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E. Taylor. Online multi-task learning for policy gradient methods. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), June 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ammar%2C%20Haitham%20Bou%20Eaton%2C%20Eric%20Ruvolo%2C%20Paul%20Taylor%2C%20Matthew%20E.%20Online%20multi-task%20learning%20for%20policy%20gradient%20methods%202014-06-14",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ammar%2C%20Haitham%20Bou%20Eaton%2C%20Eric%20Ruvolo%2C%20Paul%20Taylor%2C%20Matthew%20E.%20Online%20multi-task%20learning%20for%20policy%20gradient%20methods%202014-06-14"
        },
        {
            "id": "5",
            "entry": "[5] Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E. Taylor. Unsupervised crossdomain transfer in policy gradient reinforcement learning via manifold alignment. In Proceedings of the 29th Conference on Artificial Intelligence (AAAI-15), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ammar%2C%20Haitham%20Bou%20Eaton%2C%20Eric%20Ruvolo%2C%20Paul%20Taylor%2C%20Matthew%20E.%20Unsupervised%20crossdomain%20transfer%20in%20policy%20gradient%20reinforcement%20learning%20via%20manifold%20alignment%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ammar%2C%20Haitham%20Bou%20Eaton%2C%20Eric%20Ruvolo%2C%20Paul%20Taylor%2C%20Matthew%20E.%20Unsupervised%20crossdomain%20transfer%20in%20policy%20gradient%20reinforcement%20learning%20via%20manifold%20alignment%202015"
        },
        {
            "id": "6",
            "entry": "[6] Haitham Bou Ammar, Decebal Constantin Mocanu, Matthew E. Taylor, Kurt Driessens, Karl Tuyls, and Gerhard Weiss. Automatically mapped transfer between reinforcement learning tasks via three-way restricted Boltzmann machines. In Proceedings of the 2013 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD-13), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ammar%2C%20Haitham%20Bou%20Mocanu%2C%20Decebal%20Constantin%20Taylor%2C%20Matthew%20E.%20Driessens%2C%20Kurt%20Automatically%20mapped%20transfer%20between%20reinforcement%20learning%20tasks%20via%20three-way%20restricted%20Boltzmann%20machines%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ammar%2C%20Haitham%20Bou%20Mocanu%2C%20Decebal%20Constantin%20Taylor%2C%20Matthew%20E.%20Driessens%2C%20Kurt%20Automatically%20mapped%20transfer%20between%20reinforcement%20learning%20tasks%20via%20three-way%20restricted%20Boltzmann%20machines%202013"
        },
        {
            "id": "7",
            "entry": "[7] Haitham Bou Ammar and Matthew E. Taylor. Common subspace transfer for reinforcement learning tasks. In Proceedings of the Adaptive and Learning Agents Workshop at the 10th Autonomous Agents and Multi-Agent Systems Conference (AAMAS-11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ammar%2C%20Haitham%20Bou%20Taylor%2C%20Matthew%20E.%20Common%20subspace%20transfer%20for%20reinforcement%20learning%20tasks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ammar%2C%20Haitham%20Bou%20Taylor%2C%20Matthew%20E.%20Common%20subspace%20transfer%20for%20reinforcement%20learning%20tasks%202011"
        },
        {
            "id": "8",
            "entry": "[8] Haitham Bou Ammar, Matthew E. Taylor, Karl Tuyls, and Gerhard Weiss. Reinforcement learning transfer using a sparse coded inter-task mapping. In Proceedings of the 11th European Workshop on Multi-Agent Systems (EUMAS-13), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ammar%2C%20Haitham%20Bou%20Taylor%2C%20Matthew%20E.%20Tuyls%2C%20Karl%20Weiss%2C%20Gerhard%20Reinforcement%20learning%20transfer%20using%20a%20sparse%20coded%20inter-task%20mapping%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ammar%2C%20Haitham%20Bou%20Taylor%2C%20Matthew%20E.%20Tuyls%2C%20Karl%20Weiss%2C%20Gerhard%20Reinforcement%20learning%20transfer%20using%20a%20sparse%20coded%20inter-task%20mapping%202013"
        },
        {
            "id": "9",
            "entry": "[9] Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS-11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boularias%2C%20Abdeslam%20Kober%2C%20Jens%20Peters%2C%20Jan%20Relative%20entropy%20inverse%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boularias%2C%20Abdeslam%20Kober%2C%20Jens%20Peters%2C%20Jan%20Relative%20entropy%20inverse%20reinforcement%20learning%202011"
        },
        {
            "id": "10",
            "entry": "[10] Zhiyuan Chen and Bing Liu. Lifelong Machine Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Zhiyuan%20Liu%2C%20Bing%20Lifelong%20Machine%20Learning.%20Synthesis%20Lectures%20on%20Artificial%20Intelligence%20and%20Machine%20Learning%202016"
        },
        {
            "id": "11",
            "entry": "[11] Jaedeug Choi and Kee-eung Kim. Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. In Advances in Neural Information Processing Systems 25 (NIPS-12). 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Jaedeug%20Kim%2C%20Kee-eung%20Nonparametric%20Bayesian%20inverse%20reinforcement%20learning%20for%20multiple%20reward%20functions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20Jaedeug%20Kim%2C%20Kee-eung%20Nonparametric%20Bayesian%20inverse%20reinforcement%20learning%20for%20multiple%20reward%20functions%202012"
        },
        {
            "id": "12",
            "entry": "[12] Christos Dimitrakakis and Constantin A. Rothkopf. Bayesian multitask inverse reinforcement learning. In Proceedings of the 9th European Workshop on Reinforcement Learning (EWRL-11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dimitrakakis%2C%20Christos%20Rothkopf%2C%20Constantin%20A.%20Bayesian%20multitask%20inverse%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dimitrakakis%2C%20Christos%20Rothkopf%2C%20Constantin%20A.%20Bayesian%20multitask%20inverse%20reinforcement%20learning%202011"
        },
        {
            "id": "13",
            "entry": "[13] Yan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in Neural Information Processing Systems 30 (NIPS-17). 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20Jonathan%20One-shot%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20Jonathan%20One-shot%20imitation%20learning%202017"
        },
        {
            "id": "14",
            "entry": "[14] Anestis Fachantidis, Ioannis Partalas, Matthew E. Taylor, and Ioannis Vlahavas. Transfer learning via multiple inter-task mappings. In Proceedings of the 9th European Workshop on Reinforcement Learning (EWRL-11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fachantidis%2C%20Anestis%20Partalas%2C%20Ioannis%20Taylor%2C%20Matthew%20E.%20Vlahavas%2C%20Ioannis%20Transfer%20learning%20via%20multiple%20inter-task%20mappings%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fachantidis%2C%20Anestis%20Partalas%2C%20Ioannis%20Taylor%2C%20Matthew%20E.%20Vlahavas%2C%20Ioannis%20Transfer%20learning%20via%20multiple%20inter-task%20mappings%202011"
        },
        {
            "id": "15",
            "entry": "[15] Anestis Fachantidis, Ioannis Partalas, Matthew E. Taylor, and Ioannis Vlahavas. Transfer learning with probabilistic mapping selection. Adaptive Behavior, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fachantidis%2C%20Anestis%20Partalas%2C%20Ioannis%20Taylor%2C%20Matthew%20E.%20Vlahavas%2C%20Ioannis%20Transfer%20learning%20with%20probabilistic%20mapping%20selection%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fachantidis%2C%20Anestis%20Partalas%2C%20Ioannis%20Taylor%2C%20Matthew%20E.%20Vlahavas%2C%20Ioannis%20Transfer%20learning%20with%20probabilistic%20mapping%20selection%202015"
        },
        {
            "id": "16",
            "entry": "[16] Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, and Sergey Levine. Generalizing skills with semi-supervised reinforcement learning. In Proceedings of the 5th International Conference on Learning Representations (ICLR-17), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Fu%2C%20Justin%20Abbeel%2C%20Pieter%20Generalizing%20skills%20with%20semi-supervised%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Fu%2C%20Justin%20Abbeel%2C%20Pieter%20Generalizing%20skills%20with%20semi-supervised%20reinforcement%20learning%202017"
        },
        {
            "id": "17",
            "entry": "[17] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. In Proceedings of the 1st Annual Conference on Robot Learning (CoRL-17), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Zhang%2C%20Tianhao%20Abbeel%2C%20Pieter%20One-shot%20visual%20imitation%20learning%20via%20meta-learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Zhang%2C%20Tianhao%20Abbeel%2C%20Pieter%20One-shot%20visual%20imitation%20learning%20via%20meta-learning%202017"
        },
        {
            "id": "18",
            "entry": "[18] George Konidaris, Ilya Scheidwasser, and Andrew Barto. Transfer in reinforcement learning via shared features. Journal of Machine Learning Research (JMLR), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20George%20Scheidwasser%2C%20Ilya%20Barto%2C%20Andrew%20Transfer%20in%20reinforcement%20learning%20via%20shared%20features%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20George%20Scheidwasser%2C%20Ilya%20Barto%2C%20Andrew%20Transfer%20in%20reinforcement%20learning%20via%20shared%20features%202012"
        },
        {
            "id": "19",
            "entry": "[19] A. Kumar and H. Daum\u00e9 III. Learning task grouping and overlap in multi-task learning. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20A.%20Daum%C3%A9%2C%20III%2C%20H.%20Learning%20task%20grouping%20and%20overlap%20in%20multi-task%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20A.%20Daum%C3%A9%2C%20III%2C%20H.%20Learning%20task%20grouping%20and%20overlap%20in%20multi-task%20learning%202012"
        },
        {
            "id": "20",
            "entry": "[20] Sergey Levine and Vladlen Koltun. Continuous inverse optimal control with locally optimal examples. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Continuous%20inverse%20optimal%20control%20with%20locally%20optimal%20examples%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Continuous%20inverse%20optimal%20control%20with%20locally%20optimal%20examples%202012"
        },
        {
            "id": "21",
            "entry": "[21] Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse reinforcement learning with Gaussian processes. In Advances in Neural Information Processing Systems 24 (NIPS-11). 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Popovic%2C%20Zoran%20Koltun%2C%20Vladlen%20Nonlinear%20inverse%20reinforcement%20learning%20with%20Gaussian%20processes%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Popovic%2C%20Zoran%20Koltun%2C%20Vladlen%20Nonlinear%20inverse%20reinforcement%20learning%20with%20Gaussian%20processes%202011"
        },
        {
            "id": "22",
            "entry": "[22] Yong Luo, Dacheng Tao, and Yonggang Wen. Exploiting high-order information in heterogeneous multi-task feature learning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI-17), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Yong%20Tao%2C%20Dacheng%20Wen%2C%20Yonggang%20Exploiting%20high-order%20information%20in%20heterogeneous%20multi-task%20feature%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Yong%20Tao%2C%20Dacheng%20Wen%2C%20Yonggang%20Exploiting%20high-order%20information%20in%20heterogeneous%20multi-task%20feature%20learning%202017"
        },
        {
            "id": "23",
            "entry": "[23] Yong Luo, Yonggang Wen, and Dacheng Tao. On combining side information and unlabeled data for heterogeneous multi-task metric learning. In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Yong%20Wen%2C%20Yonggang%20Tao%2C%20Dacheng%20On%20combining%20side%20information%20and%20unlabeled%20data%20for%20heterogeneous%20multi-task%20metric%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Yong%20Wen%2C%20Yonggang%20Tao%2C%20Dacheng%20On%20combining%20side%20information%20and%20unlabeled%20data%20for%20heterogeneous%20multi-task%20metric%20learning%202016"
        },
        {
            "id": "24",
            "entry": "[24] James MacGlashan. Brown-UMBC reinforcement learning and planning (BURLAP) Java library, version 3.0. Available online at http://burlap.cs.brown.edu, 2016.",
            "url": "http://burlap.cs.brown.edu"
        },
        {
            "id": "25",
            "entry": "[25] Olivier Mangin and Pierre-Yves Oudeyer. Feature learning for multi-task inverse reinforcement learning. Available online at https://olivier.mangin.com/media/pdf/mangin.2014.firl.pdf, 2013.",
            "url": "https://olivier.mangin.com/media/pdf/mangin.2014.firl.pdf"
        },
        {
            "id": "26",
            "entry": "[26] Andreas Maurer, Massi Pontil, and Bernardino Romera-Paredes. Sparse coding for multitask and transfer learning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20Andreas%20Pontil%2C%20Massi%20Romera-Paredes%2C%20Bernardino%20Sparse%20coding%20for%20multitask%20and%20transfer%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20Pontil%2C%20Massi%20Romera-Paredes%2C%20Bernardino%20Sparse%20coding%20for%20multitask%20and%20transfer%20learning%202013"
        },
        {
            "id": "27",
            "entry": "[27] Francisco S. Melo and Manuel Lopes. Learning from demonstration using MDP induced metrics. In Proceedings of the 2010 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD-10), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melo%2C%20Francisco%20S.%20Lopes%2C%20Manuel%20Learning%20from%20demonstration%20using%20MDP%20induced%20metrics%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melo%2C%20Francisco%20S.%20Lopes%2C%20Manuel%20Learning%20from%20demonstration%20using%20MDP%20induced%20metrics%202010"
        },
        {
            "id": "28",
            "entry": "[28] Gergely Neu and Csaba Szepesv\u00e1ri. Apprenticeship learning using inverse reinforcement learning and gradient methods. In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI-07), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neu%2C%20Gergely%20Szepesv%C3%A1ri%2C%20Csaba%20Apprenticeship%20learning%20using%20inverse%20reinforcement%20learning%20and%20gradient%20methods%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neu%2C%20Gergely%20Szepesv%C3%A1ri%2C%20Csaba%20Apprenticeship%20learning%20using%20inverse%20reinforcement%20learning%20and%20gradient%20methods%202007"
        },
        {
            "id": "29",
            "entry": "[29] Andrew Y. Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In Proceedings of the 17th International Conference on Machine Learning (ICML-00), 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "30",
            "entry": "[30] Qifeng Qiao and Peter A. Beling. Inverse reinforcement learning with Gaussian process. In Proceedings of the 2011 American Control Conference (ACC-11). IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qiao%2C%20Qifeng%20Beling%2C%20Peter%20A.%20Inverse%20reinforcement%20learning%20with%20Gaussian%20process%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qiao%2C%20Qifeng%20Beling%2C%20Peter%20A.%20Inverse%20reinforcement%20learning%20with%20Gaussian%20process%202011"
        },
        {
            "id": "31",
            "entry": "[31] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramachandran%2C%20Deepak%20Amir%2C%20Eyal%20Bayesian%20inverse%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramachandran%2C%20Deepak%20Amir%2C%20Eyal%20Bayesian%20inverse%20reinforcement%20learning%202007"
        },
        {
            "id": "32",
            "entry": "[32] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In Proceedings of the 23rd International Conference on Machine Learning (ICML-06), 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ratliff%2C%20Nathan%20D.%20Bagnell%2C%20J.Andrew%20Zinkevich%2C%20Martin%20A.%20Maximum%20margin%20planning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ratliff%2C%20Nathan%20D.%20Bagnell%2C%20J.Andrew%20Zinkevich%2C%20Martin%20A.%20Maximum%20margin%20planning%202006"
        },
        {
            "id": "33",
            "entry": "[33] Constantin A. Rothkopf and Christos Dimitrakakis. Preference elicitation and inverse reinforcement learning. In Proceedings of the 2011 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD-11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rothkopf%2C%20Constantin%20A.%20Dimitrakakis%2C%20Christos%20Preference%20elicitation%20and%20inverse%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rothkopf%2C%20Constantin%20A.%20Dimitrakakis%2C%20Christos%20Preference%20elicitation%20and%20inverse%20reinforcement%20learning%202011"
        },
        {
            "id": "34",
            "entry": "[34] Paul Ruvolo and Eric Eaton. ELLA: An efficient lifelong learning algorithm. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), June 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Paul%20Ruvolo%20and%20Eric%20Eaton.%20ELLA%3A%20An%20efficient%20lifelong%20learning%20algorithm%202013-06-13",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Paul%20Ruvolo%20and%20Eric%20Eaton.%20ELLA%3A%20An%20efficient%20lifelong%20learning%20algorithm%202013-06-13"
        },
        {
            "id": "35",
            "entry": "[35] David Silver, J. Andrew Bagnell, and Anthony Stentz. Perceptual interpretation for autonomous navigation through dynamic imitation learning. In Proceedings of the 14th International Symposium on Robotics Research (ISRR-09), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20Silver%2C%20J.Andrew%20Bagnell%20Stentz%2C%20Anthony%20Perceptual%20interpretation%20for%20autonomous%20navigation%20through%20dynamic%20imitation%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=David%20Silver%2C%20J.Andrew%20Bagnell%20Stentz%2C%20Anthony%20Perceptual%20interpretation%20for%20autonomous%20navigation%20through%20dynamic%20imitation%20learning%202009"
        },
        {
            "id": "36",
            "entry": "[36] Jonathan Sorg and Satinder Singh. Transfer via soft homomorphisms. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-09), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sorg%2C%20Jonathan%20Singh%2C%20Satinder%20Transfer%20via%20soft%20homomorphisms%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sorg%2C%20Jonathan%20Singh%2C%20Satinder%20Transfer%20via%20soft%20homomorphisms%202009"
        },
        {
            "id": "37",
            "entry": "[37] Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural Information Processing Systems 20 (NIPS-07). 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syed%2C%20Umar%20Schapire%2C%20Robert%20E.%20A%20game-theoretic%20approach%20to%20apprenticeship%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Syed%2C%20Umar%20Schapire%2C%20Robert%20E.%20A%20game-theoretic%20approach%20to%20apprenticeship%20learning%202007"
        },
        {
            "id": "38",
            "entry": "[38] Ajay Kumar Tanwani and Aude Billard. Transfer in inverse reinforcement learning for multiple strategies. In Proceedings of the 2013 International Conference on Intelligent Robots and Systems (IROS-13). IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tanwani%2C%20Ajay%20Kumar%20Billard%2C%20Aude%20Transfer%20in%20inverse%20reinforcement%20learning%20for%20multiple%20strategies%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tanwani%2C%20Ajay%20Kumar%20Billard%2C%20Aude%20Transfer%20in%20inverse%20reinforcement%20learning%20for%20multiple%20strategies%202013"
        },
        {
            "id": "39",
            "entry": "[39] Matthew E. Taylor, Gregory Kuhlmann, and Peter Stone. Autonomous transfer for reinforcement learning. In Proceedings of the 7th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-08), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Matthew%20E.%20Kuhlmann%2C%20Gregory%20Stone%2C%20Peter%20Autonomous%20transfer%20for%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Matthew%20E.%20Kuhlmann%2C%20Gregory%20Stone%2C%20Peter%20Autonomous%20transfer%20for%20reinforcement%20learning%202008"
        },
        {
            "id": "40",
            "entry": "[40] Matthew E. Taylor and Peter Stone. Cross-domain transfer for reinforcement learning. In Proceedings of the 24th International Conference on Machine Learning (ICML-07), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Matthew%20E.%20Stone%2C%20Peter%20Cross-domain%20transfer%20for%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Matthew%20E.%20Stone%2C%20Peter%20Cross-domain%20transfer%20for%20reinforcement%20learning%202007"
        },
        {
            "id": "41",
            "entry": "[41] Matthew E. Taylor, Shimon Whiteson, and Peter Stone. Transfer via inter-task mappings in policy search reinforcement learning. In Proceedings of the 6th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-07), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Matthew%20E.%20Whiteson%2C%20Shimon%20Stone%2C%20Peter%20Transfer%20via%20inter-task%20mappings%20in%20policy%20search%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Matthew%20E.%20Whiteson%2C%20Shimon%20Stone%2C%20Peter%20Transfer%20via%20inter-task%20mappings%20in%20policy%20search%20reinforcement%20learning%202007"
        },
        {
            "id": "42",
            "entry": "[42] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforcement learning. arXiv preprint arXiv:1507.04888, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.04888"
        },
        {
            "id": "43",
            "entry": "[43] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd Conference on Artificial Intelligence (AAAI-08), 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        }
    ]
}
