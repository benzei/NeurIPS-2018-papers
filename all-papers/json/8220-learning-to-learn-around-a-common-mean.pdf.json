{
    "filename": "8220-learning-to-learn-around-a-common-mean.pdf",
    "metadata": {
        "title": "Learning To Learn Around A Common Mean",
        "author": "Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, Massimiliano Pontil",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8220-learning-to-learn-around-a-common-mean.pdf"
        },
        "abstract": "The problem of learning-to-learn (LTL) or meta-learning is gaining increasing attention due to recent empirical evidence of its effectiveness in applications. The goal addressed in LTL is to select an algorithm that works well on tasks sampled from a meta-distribution. In this work, we consider the family of algorithms given by a variant of Ridge Regression, in which the regularizer is the square distance to an unknown mean vector. We show that, in this setting, the LTL problem can be reformulated as a Least Squares (LS) problem and we exploit a novel metaalgorithm to efficiently solve it. At each iteration the meta-algorithm processes only one dataset. Specifically, it firstly estimates the stochastic LS objective function, by splitting this dataset into two subsets used to train and test the inner algorithm, respectively. Secondly, it performs a stochastic gradient step with the estimated value. Under specific assumptions, we present a bound for the generalization error of our meta-algorithm, which suggests the right splitting parameter to choose. When the hyper-parameters of the problem are fixed, this bound is consistent as the number of tasks grows, even if the sample size is kept constant. Preliminary experiments confirm our theoretical findings, highlighting the advantage of our approach, with respect to independent task learning."
    },
    "keywords": [
        {
            "term": "positive semidefinite",
            "url": "https://en.wikipedia.org/wiki/positive_semidefinite"
        },
        {
            "term": "sample size",
            "url": "https://en.wikipedia.org/wiki/sample_size"
        },
        {
            "term": "meta learning",
            "url": "https://en.wikipedia.org/wiki/meta_learning"
        },
        {
            "term": "ridge regression",
            "url": "https://en.wikipedia.org/wiki/ridge_regression"
        }
    ],
    "highlights": [
        "Learning-to-learn (LTL) or meta-learning addresses the problem of learning an algorithm that \u201cworks well\u201d on a class of learning tasks, which are randomly observed via a corresponding finite set of training examples, see [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and references therein",
        "Our second contribution is to show that the specific LTL problem studied in this paper can be naturally tackled incrementally by Stochastic Gradient Descent (SGD) procedures and, when the environment satisfies certain assumptions given in the paper, we provide a complete statistical analysis of this approach, which highlights the role of the splitting parameter, namely, the number of points we use to train the inner algorithm",
        "In Sec. 2 we introduce the LTL problem for the class of algorithms based on Ridge Regression around a common mean and we show that this problem is equivalent to a Least Squares problem",
        "In this paper we considered the Learning-To-Learn setting in which the underlying algorithm is Ridge Regression around a common mean",
        "We showed that the associated LTL problem coincides with a Least Squares problem with data distribution induced by the tasks\u2019 meta-distribution and, in order to solve it, we presented a novel stochastic meta-algorithm based on direct optimization of the transfer risk",
        "Under specific assumptions of the environment, we derived an analysis of the generalization performance of the method, which highlights the role of the splitting parameter in the learning process"
    ],
    "key_statements": [
        "Learning-to-learn (LTL) or meta-learning addresses the problem of learning an algorithm that \u201cworks well\u201d on a class of learning tasks, which are randomly observed via a corresponding finite set of training examples, see [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and references therein",
        "This approach brings a substantial improvement over learning the tasks in isolation \u2013 known as independent task learning (ITL) \u2013 especially when the sample size per task is small, a common setting in many applications [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "These ideas are strongly related to multitask learning (MTL) [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We study a particular kind of environment, in which the randomly observed tasks are linear regression problems and the underlying family of learning algorithms is Ridge Regression around a common mean, that is, Ridge Regression in which we introduce in the regularizer a bias term, playing the role of a common mean among the tasks",
        "Starting from a stream of datasets sampled from this environment, our goal is to learn the common mean by minimizing the transfer risk [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], which measures the average error of the underlying learning algorithm, trained on a random dataset from the environment",
        "Previous theoretical investigations of LTL minimize a proxy for the transfer risk, given by the average multitask empirical error [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] or the so-called future empirical risk [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] of the algorithm, as a first contribution of this work, we show that the specific family of algorithms considered here, naturally lends itself to directly minimize the transfer risk of 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "Our second contribution is to show that the specific LTL problem studied in this paper can be naturally tackled incrementally by Stochastic Gradient Descent (SGD) procedures and, when the environment satisfies certain assumptions given in the paper, we provide a complete statistical analysis of this approach, which highlights the role of the splitting parameter, namely, the number of points we use to train the inner algorithm",
        "In Sec. 2 we introduce the LTL problem for the class of algorithms based on Ridge Regression around a common mean and we show that this problem is equivalent to a Least Squares problem",
        "The subject of our study in this paper is the family of learning algorithms based on Ridge Regression, where we introduce a further bias term in the regularizer",
        "We have described the basic requirements of the environment considered in this work, making further assumptions about the data generation process will allow us to further analyze the LTL problem",
        "In this paper we considered the Learning-To-Learn setting in which the underlying algorithm is Ridge Regression around a common mean",
        "We showed that the associated LTL problem coincides with a Least Squares problem with data distribution induced by the tasks\u2019 meta-distribution and, in order to solve it, we presented a novel stochastic meta-algorithm based on direct optimization of the transfer risk",
        "Under specific assumptions of the environment, we derived an analysis of the generalization performance of the method, which highlights the role of the splitting parameter in the learning process",
        "Future work will be devoted to extend the statistical analysis of our meta-algorithm to more general environments and to compare our approach with the more standard one working with the future empirical risk"
    ],
    "summary": [
        "Learning-to-learn (LTL) or meta-learning addresses the problem of learning an algorithm that \u201cworks well\u201d on a class of learning tasks, which are randomly observed via a corresponding finite set of training examples, see [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and references therein.",
        "To compute the transfer risk, we first draw a task \u03bc \u223c \u03c1 and a corresponding n-sample Zn \u2208 Zn from \u03bcn, we apply the learning algorithm to obtain the estimator A(Zn) and we measure the risk of this estimator on the distribution \u03bc by taking the expectation over a test point z.",
        "We address the problem of choosing an algorithm in the above family that performs well on tasks sampled from the environment according to the above LTL setting.",
        "To get the above statement, according to Asm. 1, we have exploited the independence of the sampling of x, w, , conditioned with respect to p, the fact that the linear model is well-specified for the original tasks and the relation E[v|p] = E[w|p] \u2212 w = 0, which holds in the setting of Ex. 1.",
        "We conclude this section by describing how the previous setting can be naturally extended to the case in which we use all the points in each dataset to test the inner algorithm.",
        "Once we have received a dataset and we have splitted it in order to compute the transfer risk Er in Eq (10), we apply Stochastic Gradient Descent (SGD) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] to minimize the function Er, see Alg. 1.",
        "In our case, the application of the algorithm is slightly different to the classical setting of LS regression, since, at each iteration, we process a data point of the form (X, y) with X matrix and y vector, while, in the standard setting, usually we sample a vector x and a real number y.",
        "3 is increasing in r, it suggests that, in the specific case of Ex. 1, the best choice of the optimal splitting parameter is r = 0, corresponding to using all the points in each dataset to test the inner algorithm that outputs h, for any training set.",
        "We showed that the associated LTL problem coincides with a Least Squares problem with data distribution induced by the tasks\u2019 meta-distribution and, in order to solve it, we presented a novel stochastic meta-algorithm based on direct optimization of the transfer risk.",
        "Under specific assumptions of the environment, we derived an analysis of the generalization performance of the method, which highlights the role of the splitting parameter in the learning process.",
        "Future work will be devoted to extend the statistical analysis of our meta-algorithm to more general environments and to compare our approach with the more standard one working with the future empirical risk."
    ],
    "headline": "In this setting, the LTL problem can be reformulated as a Least Squares  problem and we exploit a novel metaalgorithm to efficiently solve it",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. Alquier, T. T. Mai, and M. Pontil. Regret bounds for lifelong learning. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine 9 Learning Research, pages 261\u2013269, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alquier%2C%20P.%20Mai%2C%20T.T.%20Pontil%2C%20M.%20Regret%20bounds%20for%20lifelong%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alquier%2C%20P.%20Mai%2C%20T.T.%20Pontil%2C%20M.%20Regret%20bounds%20for%20lifelong%20learning%202017"
        },
        {
            "id": "2",
            "entry": "[2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243\u2013272, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argyriou%2C%20A.%20Evgeniou%2C%20T.%20Pontil%2C%20M.%20Convex%20multi-task%20feature%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argyriou%2C%20A.%20Evgeniou%2C%20T.%20Pontil%2C%20M.%20Convex%20multi-task%20feature%20learning%202008"
        },
        {
            "id": "3",
            "entry": "[3] M.-F. Balcan, A. Blum, and S. Vempala. Efficient representations for lifelong learning and autoencoding. In Conference on Learning Theory, pages 191\u2013210, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.-F.%20Blum%2C%20A.%20Vempala%2C%20S.%20Efficient%20representations%20for%20lifelong%20learning%20and%20autoencoding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.-F.%20Blum%2C%20A.%20Vempala%2C%20S.%20Efficient%20representations%20for%20lifelong%20learning%20and%20autoencoding%202015"
        },
        {
            "id": "4",
            "entry": "[4] J. Baxter. A model of inductive bias learning. J. Artif. Intell. Res., 12(149\u2013198):3, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baxter%2C%20J.%20A%20model%20of%20inductive%20bias%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baxter%2C%20J.%20A%20model%20of%20inductive%20bias%20learning%202000"
        },
        {
            "id": "5",
            "entry": "[5] R. Bhatia. Matrix Analysis. Springer, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhatia%2C%20R.%20Matrix%20Analysis%201997"
        },
        {
            "id": "6",
            "entry": "[6] R. Camoriano, G. Pasquale, C. Ciliberto, L. Natale, L. Rosasco, and G. Metta. Incremental robot learning of new objects with fixed update time. In International Conference on Robotics and Automation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camoriano%2C%20R.%20Pasquale%2C%20G.%20Ciliberto%2C%20C.%20Natale%2C%20L.%20Incremental%20robot%20learning%20of%20new%20objects%20with%20fixed%20update%20time%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Camoriano%2C%20R.%20Pasquale%2C%20G.%20Ciliberto%2C%20C.%20Natale%2C%20L.%20Incremental%20robot%20learning%20of%20new%20objects%20with%20fixed%20update%20time%202017"
        },
        {
            "id": "7",
            "entry": "[7] R. Caruana. Multitask learning. Machine Learning, 28(1):41\u201375, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20R.%20Multitask%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20R.%20Multitask%20learning%201997"
        },
        {
            "id": "8",
            "entry": "[8] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear algorithms for online multitask classification. Journal of Machine Learning Research, 11:2901\u20132934, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cavallanti%2C%20G.%20Cesa-Bianchi%2C%20N.%20Gentile%2C%20C.%20Linear%20algorithms%20for%20online%20multitask%20classification%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cavallanti%2C%20G.%20Cesa-Bianchi%2C%20N.%20Gentile%2C%20C.%20Linear%20algorithms%20for%20online%20multitask%20classification%202010"
        },
        {
            "id": "9",
            "entry": "[9] C. Ciliberto, L. Rosasco, and S. Villa. Learning multiple visual tasks while discovering their structure. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 131\u2013139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20C.%20Rosasco%2C%20L.%20Villa%2C%20S.%20Learning%20multiple%20visual%20tasks%20while%20discovering%20their%20structure%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20C.%20Rosasco%2C%20L.%20Villa%2C%20S.%20Learning%20multiple%20visual%20tasks%20while%20discovering%20their%20structure%202015"
        },
        {
            "id": "10",
            "entry": "[10] C. Ciliberto, A. Rudi, L. Rosasco, and M. Pontil. Consistent multitask learning with nonlinear output relations. In Advances in Neural Information Processing Systems, pages 1986\u20131996, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20C.%20Rudi%2C%20A.%20Rosasco%2C%20L.%20Pontil%2C%20M.%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20C.%20Rudi%2C%20A.%20Rosasco%2C%20L.%20Pontil%2C%20M.%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986"
        },
        {
            "id": "11",
            "entry": "[11] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar):551\u2013585, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crammer%2C%20K.%20Dekel%2C%20O.%20Keshet%2C%20J.%20Shalev-Shwartz%2C%20S.%20Online%20passive-aggressive%20algorithms%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crammer%2C%20K.%20Dekel%2C%20O.%20Keshet%2C%20J.%20Shalev-Shwartz%2C%20S.%20Online%20passive-aggressive%20algorithms%202006"
        },
        {
            "id": "12",
            "entry": "[12] G. Denevi, C. Ciliberto, D. Stamos, and M. Pontil. Incremental learning-to-learn with statistical guarantees. In Proc. 34th Conference on Uncertainty in Artificial Intelligence (UAI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denevi%2C%20G.%20Ciliberto%2C%20C.%20Stamos%2C%20D.%20Pontil%2C%20M.%20Incremental%20learning-to-learn%20with%20statistical%20guarantees%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denevi%2C%20G.%20Ciliberto%2C%20C.%20Stamos%2C%20D.%20Pontil%2C%20M.%20Incremental%20learning-to-learn%20with%20statistical%20guarantees%202018"
        },
        {
            "id": "13",
            "entry": "[13] A. Dieuleveut, N. Flammarion, and F. Bach. Harder, better, faster, stronger convergence rates for leastsquares regression. The Journal of Machine Learning Research, 18(1):3520\u20133570, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieuleveut%2C%20A.%20Flammarion%2C%20N.%20Harder%2C%20F.Bach%20better%2C%20faster%20stronger%20convergence%20rates%20for%20leastsquares%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieuleveut%2C%20A.%20Flammarion%2C%20N.%20Harder%2C%20F.Bach%20better%2C%20faster%20stronger%20convergence%20rates%20for%20leastsquares%20regression%202017"
        },
        {
            "id": "14",
            "entry": "[14] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6:615\u2013637, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evgeniou%2C%20T.%20Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Learning%20multiple%20tasks%20with%20kernel%20methods%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evgeniou%2C%20T.%20Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Learning%20multiple%20tasks%20with%20kernel%20methods%202005"
        },
        {
            "id": "15",
            "entry": "[15] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126\u20131135. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, PMLR 80, pages 568\u20131577, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franceschi%2C%20L.%20Frasconi%2C%20P.%20Salzo%2C%20S.%20Grazzi%2C%20R.%20Bilevel%20programming%20for%20hyperparameter%20optimization%20and%20meta-learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franceschi%2C%20L.%20Frasconi%2C%20P.%20Salzo%2C%20S.%20Grazzi%2C%20R.%20Bilevel%20programming%20for%20hyperparameter%20optimization%20and%20meta-learning%202018"
        },
        {
            "id": "17",
            "entry": "[17] M. Herbster, S. Pasteris, and M. Pontil. Mistake bounds for binary matrix completion. In Advances in Neural Information Processing Systems, pages 3954\u20133962, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herbster%2C%20M.%20Pasteris%2C%20S.%20Pontil%2C%20M.%20Mistake%20bounds%20for%20binary%20matrix%20completion%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herbster%2C%20M.%20Pasteris%2C%20S.%20Pontil%2C%20M.%20Mistake%20bounds%20for%20binary%20matrix%20completion%202016"
        },
        {
            "id": "18",
            "entry": "[18] L. Jacob, J.-P. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In Advances in neural information processing systems, pages 745\u2013752, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%2C%20L.%20Vert%2C%20J.-P.%20Bach%2C%20F.R.%20Clustered%20multi-task%20learning%3A%20A%20convex%20formulation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%2C%20L.%20Vert%2C%20J.-P.%20Bach%2C%20F.R.%20Clustered%20multi-task%20learning%3A%20A%20convex%20formulation%202009"
        },
        {
            "id": "19",
            "entry": "[19] A. Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research, 6:967\u2013994, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20Algorithmic%20stability%20and%20meta-learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20A.%20Algorithmic%20stability%20and%20meta-learning%202005"
        },
        {
            "id": "20",
            "entry": "[20] A. Maurer. Transfer bounds for linear feature learning. Machine Learning, 75(3):327\u2013350, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20Transfer%20bounds%20for%20linear%20feature%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20A.%20Transfer%20bounds%20for%20linear%20feature%20learning%202009"
        },
        {
            "id": "21",
            "entry": "[21] A. Maurer, M. Pontil, and B. Romera-Paredes. Sparse coding for multitask and transfer learning. In International Conference on Machine Learning, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20Pontil%2C%20M.%20Romera-Paredes%2C%20B.%20Sparse%20coding%20for%20multitask%20and%20transfer%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20A.%20Pontil%2C%20M.%20Romera-Paredes%2C%20B.%20Sparse%20coding%20for%20multitask%20and%20transfer%20learning%202013"
        },
        {
            "id": "22",
            "entry": "[22] A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning. The Journal of Machine Learning Research, 17(1):2853\u20132884, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20Pontil%2C%20M.%20Romera-Paredes%2C%20B.%20The%20benefit%20of%20multitask%20representation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20A.%20Pontil%2C%20M.%20Romera-Paredes%2C%20B.%20The%20benefit%20of%20multitask%20representation%20learning%202016"
        },
        {
            "id": "23",
            "entry": "[23] A. M. McDonald, M. Pontil, and D. Stamos. New perspectives on k-support and cluster norms. Journal of Machine Learning Research, 17(155):1\u201338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDonald%2C%20A.M.%20Pontil%2C%20M.%20Stamos%2C%20D.%20New%20perspectives%20on%20k-support%20and%20cluster%20norms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDonald%2C%20A.M.%20Pontil%2C%20M.%20Stamos%2C%20D.%20New%20perspectives%20on%20k-support%20and%20cluster%20norms%202016"
        },
        {
            "id": "24",
            "entry": "[24] A. Pentina and C. Lampert. A PAC-Bayesian bound for lifelong learning. In International Conference on Machine Learning, pages 991\u2013999, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pentina%2C%20A.%20Lampert%2C%20C.%20A%20PAC-Bayesian%20bound%20for%20lifelong%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pentina%2C%20A.%20Lampert%2C%20C.%20A%20PAC-Bayesian%20bound%20for%20lifelong%20learning%202014"
        },
        {
            "id": "25",
            "entry": "[25] A. Pentina and R. Urner. Lifelong learning with weighted majority votes. In Advances in Neural Information Processing Systems, pages 3612\u20133620, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pentina%2C%20A.%20Urner%2C%20R.%20Lifelong%20learning%20with%20weighted%20majority%20votes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pentina%2C%20A.%20Urner%2C%20R.%20Lifelong%20learning%20with%20weighted%20majority%20votes%202016"
        },
        {
            "id": "26",
            "entry": "[26] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In I5th International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "27",
            "entry": "[27] S.-A. Rebuffi, A. Kolesnikov, and C. H. Lampert. iCaRL: Incremental classifier and representation learning. In Proc. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20S.-A.%20Kolesnikov%2C%20A.%20H%2C%20C.%20Lampert.%20iCaRL%3A%20Incremental%20classifier%20and%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20S.-A.%20Kolesnikov%2C%20A.%20H%2C%20C.%20Lampert.%20iCaRL%3A%20Incremental%20classifier%20and%20representation%20learning%202017"
        },
        {
            "id": "28",
            "entry": "[28] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "29",
            "entry": "[29] S. Thrun and L. Pratt. Learning to Learn. Springer, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20S.%20Pratt%2C%20L.%20Learning%20to%20Learn%201998"
        },
        {
            "id": "30",
            "entry": "[30] Y. Wu, Y. Su, and Y. Demiris. A morphable template framework for robot learning by demonstration: Integrating one-shot and incremental learning approaches. Robotics and Autonomous Systems, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Su%2C%20Y.%20Demiris%2C%20Y.%20A%20morphable%20template%20framework%20for%20robot%20learning%20by%20demonstration%3A%20Integrating%20one-shot%20and%20incremental%20learning%20approaches%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Y.%20Su%2C%20Y.%20Demiris%2C%20Y.%20A%20morphable%20template%20framework%20for%20robot%20learning%20by%20demonstration%3A%20Integrating%20one-shot%20and%20incremental%20learning%20approaches%202014"
        }
    ]
}
