{
    "filename": "8186-adaptive-methods-for-nonconvex-optimization.pdf",
    "metadata": {
        "title": "Adaptive Methods for Nonconvex Optimization",
        "author": "Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Adaptive gradient methods that rely on scaling gradients down by the square root of exponential moving averages of past squared gradients, such RMSPROP, ADAM, ADADELTA have found wide application in optimizing the nonconvex problems that arise in deep learning. However, it has been recently demonstrated that such methods can fail to converge even in simple convex optimization settings. In this work, we provide a new analysis of such methods applied to nonconvex stochastic optimization problems, characterizing the effect of increasing minibatch size. Our analysis shows that under this scenario such methods do converge to stationarity up to the statistical limit of variance in the stochastic gradients (scaled by a constant factor). In particular, our result implies that increasing minibatch sizes enables convergence, thus providing a way to circumvent the nonconvergence issues. Furthermore, we provide a new adaptive optimization algorithm, YOGI, which controls the increase in effective learning rate, leading to even better performance with similar theoretical guarantees on convergence. Extensive experiments show that YOGI with very little hyperparameter tuning outperforms methods such as ADAM in several challenging machine learning tasks."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "YOGI",
            "url": "https://en.wikipedia.org/wiki/Yogi"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "Named Entity Recognition",
            "url": "https://en.wikipedia.org/wiki/Named_Entity_Recognition"
        },
        {
            "term": "Exponential moving average",
            "url": "https://en.wikipedia.org/wiki/Exponential_moving_average"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "Stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "We study nonconvex stochastic optimization problems of the form min f (x) := Es\u223cP[ (x, s)], (1)<br/><br/>x\u2208Rd where is a smooth function and P is a probability distribution on the domain S \u2282 Rk",
        "Inspired by our analysis of ADAM and the intuition that controlled increase of effective learning rate is essential for good convergence, we propose a new algorithm (YOGI) for achieving adaptivity in Stochastic gradient descent",
        "While we stated our theoretical results with batch size b = \u0398(T ) for the sake of simplicity, similar results can be obtained for increasing minibatches bt = \u0398(t)",
        "Based on the insights gained from our theoretical analysis, we present empirical results showcasing three aspects of our framework: (i) the value gained by controlled variation in learning rate using YOGI, fast optimization, and wide applicability on several large-scale problems ranging from natural language processing to computer vision",
        "YOGI tends to perform superior to ADAM and highly hand-tuned Stochastic gradient descent across various architectures and datasets",
        "Very little hyperparameter tuning was used for YOGI to remain faithful to our goal"
    ],
    "key_statements": [
        "We study nonconvex stochastic optimization problems of the form min f (x) := Es\u223cP[ (x, s)], (1)<br/><br/>x\u2208Rd where is a smooth function and P is a probability distribution on the domain S \u2282 Rk",
        "We study nonconvex stochastic optimization problems of the form min f (x) := Es\u223cP[ (x, s)], (1)",
        "[<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] showed that Exponential moving average based adaptive methods may not converge to the optimal solution even in simple convex settings when a constant minibatch size is used",
        "Inspired by our analysis of ADAM and the intuition that controlled increase of effective learning rate is essential for good convergence, we propose a new algorithm (YOGI) for achieving adaptivity in Stochastic gradient descent",
        "We demonstrate that YOGI achieves similar, or better, results to best performance reported on these models without much hyperparameter tuning",
        "We focus on the stochastic nonconvex optimization setting since that is precisely the right model for risk minimization in machine learning problems",
        "We propose a simple additive adaptive method, YOGI, for optimizing the stochastic nonconvex optimization problem of our interest. (Name derived from the Sanskrit word yuj meaning to add.)",
        "While we stated our theoretical results with batch size b = \u0398(T ) for the sake of simplicity, similar results can be obtained for increasing minibatches bt = \u0398(t)",
        "Based on the insights gained from our theoretical analysis, we present empirical results showcasing three aspects of our framework: (i) the value gained by controlled variation in learning rate using YOGI, fast optimization, and wide applicability on several large-scale problems ranging from natural language processing to computer vision",
        "YOGI tends to perform superior to ADAM and highly hand-tuned Stochastic gradient descent across various architectures and datasets",
        "It is interesting to note that typically YOGI has a smaller variance across different runs when compared to ADAM",
        "Very little hyperparameter tuning was used for YOGI to remain faithful to our goal",
        "The patience of ReduceLRonPlateau scheduler will depend on data size, but in most experiments we saw a patience of 5,000-10,000 gradient steps works well with a decay factor of 0.5"
    ],
    "summary": [
        "We study nonconvex stochastic optimization problems of the form min f (x) := Es\u223cP[ (x, s)], (1)<br/><br/>x\u2208Rd where is a smooth function and P is a probability distribution on the domain S \u2282 Rk.",
        "One has to decay this learning rate as the algorithm proceeds in order to control the variance in the stochastic gradients computed over a minibatch and thereby, ensure convergence.",
        "[<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] showed that EMA based adaptive methods may not converge to the optimal solution even in simple convex settings when a constant minibatch size is used.",
        "We develop convergence analysis for ADAM under certain useful parameter settings, showing convergence to a stationary point up to the statistical limit of variance in the stochastic gradients even for nonconvex problems.",
        "Inspired by our analysis of ADAM and the intuition that controlled increase of effective learning rate is essential for good convergence, we propose a new algorithm (YOGI) for achieving adaptivity in SGD.",
        "We provide extensive empirical experiments for YOGI and show that it performs better than ADAM in many state-of-the-art machine learning models.",
        "Similar to ADAM, we provide the following convergence result for YOGI in the nonconvex setting.",
        "The following results on bounded gradient norm with increasing batch size can be obtained as a simple corollary of Theorem 2.",
        "We would like to emphasize that the SFO complexity obtained here for ADAM or YOGI with large batch size is similar to that of SGD.",
        "Based on the insights gained from our theoretical analysis, we present empirical results showcasing three aspects of our framework: (i) the value gained by controlled variation in learning rate using YOGI, fast optimization, and wide applicability on several large-scale problems ranging from natural language processing to computer vision.",
        "Such an initialization can result in very large learning rates at start of the training procedure; thereby, leading to convergence and performance issues.",
        "In \u201cMNIST\u201d autoencoder, we perform significantly better than all prior results including ADAM with specially tuned learning rate.",
        "The original paper proposed a method based on linearly increasing the learning rate for a specified number of optimization steps followed by inverse square root decay.",
        "We use YOGI to train ResNets [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and DenseNets [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], which are very popular architectures, producing state-of-the-art results across many computer vision tasks.",
        "Consistent with our previous experiments, we observe that YOGI outperforms ADAM and obtains better results than those reported in the original paper (Table 6).",
        "Using YOGI with essentially the default parameters and an alternate decay schedule of reducing learning rate by 0.7 every 3 epochs achieves better performance."
    ],
    "headline": "We provide a new analysis of such methods applied to nonconvex stochastic optimization problems, characterizing the effect of increasing minibatch size",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima for nonconvex optimization in linear time. CoRR, abs/1611.01146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01146"
        },
        {
            "id": "2",
            "entry": "[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. stat, 1050:21, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Lei%20Kiros%2C%20Jamie%20Ryan%20Hinton%2C%20Geoffrey%20E.%20Layer%20normalization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Lei%20Kiros%2C%20Jamie%20Ryan%20Hinton%2C%20Geoffrey%20E.%20Layer%20normalization%202016"
        },
        {
            "id": "3",
            "entry": "[3] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD: Compressed optimisation for non-convex problems. 2018. arXiv:1802.04434.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04434"
        },
        {
            "id": "4",
            "entry": "[4] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization. CoRR, abs/1611.00756, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "5",
            "entry": "[5] Jason PC Chiu and Eric Nichols. Named entity recognition with bidirectional lstm-cnns. arXiv preprint arXiv:1511.08308, 2015. 5https://keras.io/callbacks 6The code for Inception-Resnet-v2 is available at https://github.com/tensorflow/models/blob/master/research/slim/train_image_classifier.py.",
            "url": "https://github.com/tensorflow/models/blob/master/research/slim/train_image_classifier.py",
            "arxiv_url": "https://arxiv.org/pdf/1511.08308"
        },
        {
            "id": "6",
            "entry": "[6] Kevin Clark. Semi-supervised learning for nlp. http://web.stanford.edu/class/cs224n/lectures/lecture17.pdf , 2018.",
            "url": "http://web.stanford.edu/class/cs224n/lectures/lecture17.pdf"
        },
        {
            "id": "7",
            "entry": "[7] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "8",
            "entry": "[8] Saeed Ghadimi and Guanghui Lan. Stochastic firstand zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "9",
            "entry": "[9] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(12):267\u2013305, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Zhang%2C%20Hongchao%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Zhang%2C%20Hongchao%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202014"
        },
        {
            "id": "10",
            "entry": "[10] Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems, pages 1585\u20131593, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Shalev-Shwartz%2C%20Shai%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Shalev-Shwartz%2C%20Shai%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015"
        },
        {
            "id": "11",
            "entry": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "12",
            "entry": "[12] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Salakhutdinov%2C%20Ruslan%20R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "13",
            "entry": "[13] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, volume 1, page 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "14",
            "entry": "[14] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. CoRR, abs/1703.00887, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00887"
        },
        {
            "id": "15",
            "entry": "[15] Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to sgd. arXiv preprint arXiv:1712.07628, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07628"
        },
        {
            "id": "16",
            "entry": "[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "17",
            "entry": "[17] Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, and Jarvis Haupt. Stochastic variance reduced optimization for nonconvex sparse learning. In ICML, 2016. arXiv:1605.02711.",
            "arxiv_url": "https://arxiv.org/pdf/1605.02711"
        },
        {
            "id": "18",
            "entry": "[18] Minh-Thang Luong and Christopher D. Manning. Stanford neural machine translation systems for spoken language domain. In International Workshop on Spoken Language Translation, Da Nang, Vietnam, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Minh-Thang%20Manning%2C%20Christopher%20D.%20Stanford%20neural%20machine%20translation%20systems%20for%20spoken%20language%20domain%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Minh-Thang%20Manning%2C%20Christopher%20D.%20Stanford%20neural%20machine%20translation%20systems%20for%20spoken%20language%20domain%202015"
        },
        {
            "id": "19",
            "entry": "[19] Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.01354"
        },
        {
            "id": "20",
            "entry": "[20] James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 735\u2013742, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20James%20Deep%20learning%20via%20hessian-free%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20James%20Deep%20learning%20via%20hessian-free%20optimization%202010"
        },
        {
            "id": "21",
            "entry": "[21] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International Conference on Machine Learning, pages 2408\u20132417, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20James%20Grosse%2C%20Roger%20Optimizing%20neural%20networks%20with%20kronecker-factored%20approximate%20curvature%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20James%20Grosse%2C%20Roger%20Optimizing%20neural%20networks%20with%20kronecker-factored%20approximate%20curvature%202015"
        },
        {
            "id": "22",
            "entry": "[22] H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization. In Proceedings of the 23rd Annual Conference On Learning Theory, pages 244\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20H.Brendan%20Streeter%2C%20Matthew%20J.%20Adaptive%20bound%20optimization%20for%20online%20convex%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20H.Brendan%20Streeter%2C%20Matthew%20J.%20Adaptive%20bound%20optimization%20for%20online%20convex%20optimization%202010"
        },
        {
            "id": "23",
            "entry": "[23] Yurii Nesterov. Introductory Lectures On Convex Optimization: A Basic Course. Springer, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20On%20Convex%20Optimization%3A%20A%20Basic%20Course%202003"
        },
        {
            "id": "24",
            "entry": "[24] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander J. Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016-06-19",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016-06-19"
        },
        {
            "id": "25",
            "entry": "[25] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam & Beyond. In Proceedings of the 6th International Conference on Learning Representations., 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Kale%2C%20Satyen%20Kumar%2C%20Sanjiv%20On%20the%20Convergence%20of%20Adam%20%26%20Beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Kale%2C%20Satyen%20Kumar%2C%20Sanjiv%20On%20the%20Convergence%20of%20Adam%20%26%20Beyond%202018"
        },
        {
            "id": "26",
            "entry": "[26] Sashank J. Reddi, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander J. Smola. Fast incremental method for nonconvex optimization. CoRR, abs/1603.06159, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.06159"
        },
        {
            "id": "27",
            "entry": "[27] Sashank J. Reddi, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander J. Smola. Fast stochastic methods for nonsmooth nonconvex optimization. CoRR, abs/1605.06900, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.06900"
        },
        {
            "id": "28",
            "entry": "[28] Sashank J. Reddi, Manzil Zaheer, Suvrit Sra, Barnab\u00e1s P\u00f3czos, Francis Bach, Ruslan Salakhutdinov, and Alexander J. Smola. A generic approach for escaping saddle points. In International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain, pages 1233\u20131242, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Zaheer%2C%20Manzil%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018-04-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Zaheer%2C%20Manzil%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018-04-09"
        },
        {
            "id": "29",
            "entry": "[29] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "30",
            "entry": "[30] Stefan Schweter. Neural machine translation system for english to vietnamese. https://github.com/stefan-it/nmt-en-vi, 2018.",
            "url": "https://github.com/stefan-it/nmt-en-vi"
        },
        {
            "id": "31",
            "entry": "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.07909"
        },
        {
            "id": "32",
            "entry": "[32] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u20131147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "33",
            "entry": "[33] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In AAAI, volume 4, page 12, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inception-resnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inception-resnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017"
        },
        {
            "id": "34",
            "entry": "[34] T. Tieleman and G. Hinton. RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20Hinton%2C%20G.%20RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20Hinton%2C%20G.%20RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "35",
            "entry": "[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "36",
            "entry": "[36] Oriol Vinyals and Daniel Povey. Krylov subspace descent for deep learning. In AISTATS, pages 1261\u20131268, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Povey%2C%20Daniel%20Krylov%20subspace%20descent%20for%20deep%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Povey%2C%20Daniel%20Krylov%20subspace%20descent%20for%20deep%20learning%202012"
        },
        {
            "id": "37",
            "entry": "[37] Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis P. Langlotz, and Jiawei Han. Cross-type biomedical named entity recognition with deep multi-task learning. CoRR, abs/1801.09851, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09851"
        },
        {
            "id": "38",
            "entry": "[38] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912\u20131920, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Zhirong%20Song%2C%20Shuran%20Khosla%2C%20Aditya%20Yu%2C%20Fisher%20and%20Jianxiong%20Xiao.%203d%20shapenets%3A%20A%20deep%20representation%20for%20volumetric%20shapes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Zhirong%20Song%2C%20Shuran%20Khosla%2C%20Aditya%20Yu%2C%20Fisher%20and%20Jianxiong%20Xiao.%203d%20shapenets%3A%20A%20deep%20representation%20for%20volumetric%20shapes%202015"
        },
        {
            "id": "39",
            "entry": "[39] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pages 3394\u20133404, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2033943404%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manzil%20Zaheer%20Satwik%20Kottur%20Siamak%20Ravanbakhsh%20Barnabas%20Poczos%20Ruslan%20R%20Salakhutdinov%20and%20Alexander%20J%20Smola%20Deep%20sets%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2033943404%202017"
        },
        {
            "id": "40",
            "entry": "[40] Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        },
        {
            "id": "41",
            "entry": "[41] Zeyuan Allen Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. CoRR, abs/1603.05643, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1603.05643"
        }
    ]
}
