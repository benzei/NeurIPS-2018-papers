{
    "filename": "7741-learning-confidence-sets-using-support-vector-machines.pdf",
    "metadata": {
        "title": "Learning Confidence Sets using Support Vector Machines",
        "author": "Wenbo Wang, Xingye Qiao",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7741-learning-confidence-sets-using-support-vector-machines.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The goal of confidence-set learning in the binary classification setting [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] is to construct two sets, each with a specific probability guarantee to cover a class. An observation outside the overlap of the two sets is deemed to be from one of the two classes, while the overlap is an ambiguity region which could belong to either class. Instead of plug-in approaches, we propose a support vector classifier to construct confidence sets in a flexible manner. Theoretically, we show that the proposed learner can control the non-coverage rates and minimize the ambiguity with high probability. Efficient algorithms are developed and numerical studies illustrate the effectiveness of the proposed method."
    },
    "keywords": [
        {
            "term": "confidence set",
            "url": "https://en.wikipedia.org/wiki/confidence_set"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        },
        {
            "term": "t-distributed stochastic neighbor embedding",
            "url": "https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding"
        }
    ],
    "highlights": [
        "We first formally define the problem and give some useful notations.<br/><br/>It is desirable to keep the ambiguity as small as possible",
        "To avoid estimating the conditional class probability \u03b7(x), we propose a support vector classifier to construct confidence sets by empirical risk minimization",
        "We would like as many class j observations as possible to be covered by constraint that Pj",
        "We show the performance of confidence-support vector machine with weighting but without robust implementation",
        "Our weighted confidence-support vector machine performs the best when sample size is small in the linear case and it outperforms kNN, Random Forest and naive SVM in nonlinear cases",
        "We propose to learn confidence sets using support vector machine"
    ],
    "key_statements": [
        "We first formally define the problem and give some useful notations.<br/><br/>It is desirable to keep the ambiguity as small as possible",
        "To avoid estimating the conditional class probability \u03b7(x), we propose a support vector classifier to construct confidence sets by empirical risk minimization",
        "We would like as many class j observations as possible to be covered by constraint that Pj",
        "We show the performance of confidence-support vector machine with weighting but without robust implementation",
        "Our weighted confidence-support vector machine performs the best when sample size is small in the linear case and it outperforms kNN, Random Forest and naive SVM in nonlinear cases",
        "We propose to learn confidence sets using support vector machine"
    ],
    "summary": [
        "We first formally define the problem and give some useful notations.<br/><br/>It is desirable to keep the ambiguity as small as possible.",
        "Lei [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] showed that the best such classifier, the Bayes optimal rule, depends on the conditional class probability function \u03b7(x) = P (Y = 1|X = x).",
        "To avoid estimating the conditional class probability \u03b7(x), we propose a support vector classifier to construct confidence sets by empirical risk minimization.",
        "In the finite-sample case, our classifier can control both non-coverage rates while minimizing the ambiguity.",
        "We compare our confidence-support vector machine (CSVM) method and methods based on the plug-in principal, including L2 penalized logistic regression [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], kernel logistic regression [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>], kNN [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], random forest [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and SVM [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] using both simulated and real data.",
        "We choose the optimal pair which gives the smallest tuning ambiguity and has the two non-coverage rates for the tuning set controlled.",
        "To adapt traditional classification methods to the confidence set learning problem, we use the plug-in principal [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "One by-product of the robust implementation above is that the non-coverage rate of most of the methods will become very similar and we only need to compare the size of the ambiguity.",
        "We include a naive SVM approach (\u2019SVM_r\u2019 in plots below) whose discriminant function is obtained in the traditional way, but which induces confidence sets by thresholding in the same way described above.",
        "In the first scenario we compare the linear approaches (SVM and penalized logistic regression), while in the two cases we consider nonlinear methods.",
        "Our weighted CSVM performs the best when sample size is small in the linear case and it outperforms kNN, Random Forest and naive SVM in nonlinear cases.",
        "It is not surprising that the naive SVM method performs significantly worse than all other methods in the nonlinear settings, as the hinge loss is well known to not lead to consistent estimates for class probabilities.",
        "The non-coverage rates of CSVM, random forest, kernel logistic regression and naive SVM methods are close to each other while CSVM without robust implementation and kNN have similar non-coverage rates.",
        "Lei [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] set both nominal non-coverage rates to be 0.05 in their study which focused on linear methods, it needs to be pointed out that many nonlinear classifiers, such as SVM with Gaussian kernel, can achieve this non-coverage rate without introducing any ambiguity.",
        "Theoretical studies have shown the effectiveness of our approach in controlling the non-coverage rate and minimizing the ambiguity.",
        "This has a natural connection to the literature of multi-class classification with confidence [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], classification with reject and refine options [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and conformal learning [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]"
    ],
    "headline": "Instead of plug-in approaches, we propose a support vector classifier to construct confidence sets in a flexible manner",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Naomi S Altman. An introduction to kernel and nearest-neighbor nonparametric regression. The American Statistician, 46(3):175\u2013185, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altman%2C%20Naomi%20S.%20An%20introduction%20to%20kernel%20and%20nearest-neighbor%20nonparametric%20regression%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altman%2C%20Naomi%20S.%20An%20introduction%20to%20kernel%20and%20nearest-neighbor%20nonparametric%20regression%201992"
        },
        {
            "id": "2",
            "entry": "[2] Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 9(Aug):1823\u20131840, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20a%20reject%20option%20using%20a%20hinge%20loss%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20a%20reject%20option%20using%20a%20hinge%20loss%202008"
        },
        {
            "id": "3",
            "entry": "[3] Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Jordan%2C%20Michael%20I.%20McAuliffe%2C%20Jon%20D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Jordan%2C%20Michael%20I.%20McAuliffe%2C%20Jon%20D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006"
        },
        {
            "id": "4",
            "entry": "[4] Adam Cannon, James Howse, Don Hush, and Clint Scovel. Learning with the neyman-pearson and min-max criteria. Los Alamos National Laboratory, Tech. Rep. LA-UR, pages 02\u20132951, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cannon%2C%20Adam%20Howse%2C%20James%20Hush%2C%20Don%20Scovel%2C%20Clint%20Learning%20with%20the%20neyman-pearson%20and%20min-max%20criteria%202002"
        },
        {
            "id": "5",
            "entry": "[5] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3): 273\u2013297, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Vapnik%2C%20Vladimir%20Support-vector%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Vapnik%2C%20Vladimir%20Support-vector%20networks%201995"
        },
        {
            "id": "6",
            "entry": "[6] Christophe Denis and Mohamed Hebiri. Confidence sets with expected sizes for multiclass classification. arXiv preprint arXiv:1608.08783, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.08783"
        },
        {
            "id": "7",
            "entry": "[7] Manuel Fern\u00e1ndez-Delgado, Eva Cernadas, Sen\u00e9n Barro, and Dinani Amorim. Do we need hundreds of classifiers to solve real world classification problems. J. Mach. Learn. Res, 15(1): 3133\u20133181, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fern%C3%A1ndez-Delgado%2C%20Manuel%20Cernadas%2C%20Eva%20Barro%2C%20Sen%C3%A9n%20Amorim%2C%20Dinani%20Do%20we%20need%20hundreds%20of%20classifiers%20to%20solve%20real%20world%20classification%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fern%C3%A1ndez-Delgado%2C%20Manuel%20Cernadas%2C%20Eva%20Barro%2C%20Sen%C3%A9n%20Amorim%2C%20Dinani%20Do%20we%20need%20hundreds%20of%20classifiers%20to%20solve%20real%20world%20classification%20problems%202014"
        },
        {
            "id": "8",
            "entry": "[8] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1):1, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20Jerome%20Hastie%2C%20Trevor%20Tibshirani%2C%20Rob%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20Jerome%20Hastie%2C%20Trevor%20Tibshirani%2C%20Rob%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010"
        },
        {
            "id": "9",
            "entry": "[9] Johannes F\u00fcrnkranz and Eyke H\u00fcllermeier. Preference learning: An introduction. In Preference learning, pages 1\u201317.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=F%C3%BCrnkranz%2C%20Johannes%20H%C3%BCllermeier%2C%20Eyke%20Preference%20learning%3A%20An%20introduction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=F%C3%BCrnkranz%2C%20Johannes%20H%C3%BCllermeier%2C%20Eyke%20Preference%20learning%3A%20An%20introduction"
        },
        {
            "id": "10",
            "entry": "[10] Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, and St\u00e9phane Canu. Support vector machines with a reject option. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 537\u2013544. Curran Associates, Inc., 2009. URL http://papers.nips.cc/paper/3594-support-vector-machines-with-a-reject-option.pdf.",
            "url": "http://papers.nips.cc/paper/3594-support-vector-machines-with-a-reject-option.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grandvalet%2C%20Yves%20Rakotomamonjy%2C%20Alain%20Keshet%2C%20Joseph%20Canu%2C%20St%C3%A9phane%20Support%20vector%20machines%20with%20a%20reject%20option%202009"
        },
        {
            "id": "11",
            "entry": "[11] Radu Herbei and Marten H Wegkamp. Classification with reject option. Canadian Journal of Statistics, 34(4):709\u2013721, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herbei%2C%20Radu%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20reject%20option%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herbei%2C%20Radu%20Wegkamp%2C%20Marten%20H.%20Classification%20with%20reject%20option%202006"
        },
        {
            "id": "12",
            "entry": "[12] Saskia Le Cessie and Johannes C Van Houwelingen. Ridge estimators in logistic regression. Applied statistics, pages 191\u2013201, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cessie%2C%20Saskia%20Le%20Houwelingen%2C%20Johannes%20C.Van%20Ridge%20estimators%20in%20logistic%20regression%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cessie%2C%20Saskia%20Le%20Houwelingen%2C%20Johannes%20C.Van%20Ridge%20estimators%20in%20logistic%20regression%201992"
        },
        {
            "id": "13",
            "entry": "[13] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541\u2013551, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Boser%2C%20Bernhard%20Denker%2C%20John%20S.%20Henderson%2C%20Donnie%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Boser%2C%20Bernhard%20Denker%2C%20John%20S.%20Henderson%2C%20Donnie%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201989"
        },
        {
            "id": "14",
            "entry": "[14] Jing Lei. Classification with confidence. Biometrika, page asu038, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Jing%20Classification%20with%20confidence%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Jing%20Classification%20with%20confidence%202014"
        },
        {
            "id": "15",
            "entry": "[15] Andy Liaw, Matthew Wiener, et al. Classification and regression by randomforest. R news, 2 (3):18\u201322, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liaw%2C%20Andy%20Wiener%2C%20Matthew%20Classification%20and%20regression%20by%20randomforest%202002"
        },
        {
            "id": "16",
            "entry": "[16] Yufeng Liu, Hao Helen Zhang, and Yichao Wu. Hard or soft classification? large-margin unified machines. Journal of the American Statistical Association, 106(493):166\u2013177, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yufeng%20Zhang%2C%20Hao%20Helen%20Wu%2C%20Yichao%20Hard%20or%20soft%20classification%3F%20large-margin%20unified%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Yufeng%20Zhang%2C%20Hao%20Helen%20Wu%2C%20Yichao%20Hard%20or%20soft%20classification%3F%20large-margin%20unified%20machines%202011"
        },
        {
            "id": "17",
            "entry": "[17] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "18",
            "entry": "[18] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Platt%2C%20John%20Probabilistic%20outputs%20for%20support%20vector%20machines%20and%20comparisons%20to%20regularized%20likelihood%20methods%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Platt%2C%20John%20Probabilistic%20outputs%20for%20support%20vector%20machines%20and%20comparisons%20to%20regularized%20likelihood%20methods%201999"
        },
        {
            "id": "19",
            "entry": "[19] Philippe Rigollet and Xin Tong. Neyman-pearson classification, convexity and stochastic constraints. Journal of Machine Learning Research, 12(Oct):2831\u20132855, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20Philippe%20Tong%2C%20Xin%20Neyman-pearson%20classification%2C%20convexity%20and%20stochastic%20constraints%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rigollet%2C%20Philippe%20Tong%2C%20Xin%20Neyman-pearson%20classification%2C%20convexity%20and%20stochastic%20constraints%202011"
        },
        {
            "id": "20",
            "entry": "[20] Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. Journal of the American Statistical Association, (just-accepted), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sadinle%2C%20Mauricio%20Lei%2C%20Jing%20Wasserman%2C%20Larry%20Least%20ambiguous%20set-valued%20classifiers%20with%20bounded%20error%20levels%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sadinle%2C%20Mauricio%20Lei%2C%20Jing%20Wasserman%2C%20Larry%20Least%20ambiguous%20set-valued%20classifiers%20with%20bounded%20error%20levels%202017"
        },
        {
            "id": "21",
            "entry": "[21] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(Mar):371\u2013421, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shafer%2C%20Glenn%20Vovk%2C%20Vladimir%20A%20tutorial%20on%20conformal%20prediction%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shafer%2C%20Glenn%20Vovk%2C%20Vladimir%20A%20tutorial%20on%20conformal%20prediction%202008"
        },
        {
            "id": "22",
            "entry": "[22] Xin Tong, Yang Feng, and Anqi Zhao. A survey on neyman-pearson classification and suggestions for future research. Wiley Interdisciplinary Reviews: Computational Statistics, 8(2):64\u201381, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tong%2C%20Xin%20Feng%2C%20Yang%20Zhao%2C%20Anqi%20A%20survey%20on%20neyman-pearson%20classification%20and%20suggestions%20for%20future%20research%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tong%2C%20Xin%20Feng%2C%20Yang%20Zhao%2C%20Anqi%20A%20survey%20on%20neyman-pearson%20classification%20and%20suggestions%20for%20future%20research%202016"
        },
        {
            "id": "23",
            "entry": "[23] Vladimir Vovk, Ilia Nouretdinov, Valentina Fedorova, Ivan Petej, and Alex Gammerman. Criteria of efficiency for set-valued classification. Annals of Mathematics and Artificial Intelligence, pages 1\u201326, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vovk%2C%20Vladimir%20Nouretdinov%2C%20Ilia%20Fedorova%2C%20Valentina%20Petej%2C%20Ivan%20Criteria%20of%20efficiency%20for%20set-valued%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vovk%2C%20Vladimir%20Nouretdinov%2C%20Ilia%20Fedorova%2C%20Valentina%20Petej%2C%20Ivan%20Criteria%20of%20efficiency%20for%20set-valued%20classification%202017"
        },
        {
            "id": "24",
            "entry": "[24] Junhui Wang, Xiaotong Shen, and Yufeng Liu. Probability estimation for large-margin classifiers. Biometrika, 95(1):149\u2013167, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Junhui%20Shen%2C%20Xiaotong%20Liu%2C%20Yufeng%20Probability%20estimation%20for%20large-margin%20classifiers%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Junhui%20Shen%2C%20Xiaotong%20Liu%2C%20Yufeng%20Probability%20estimation%20for%20large-margin%20classifiers%202007"
        },
        {
            "id": "25",
            "entry": "[25] Yichao Wu and Yufeng Liu. Adaptively weighted large margin classifiers. Journal of Computational and Graphical Statistics, 22(2):416\u2013432, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yichao%20Liu%2C%20Yufeng%20Adaptively%20weighted%20large%20margin%20classifiers%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yichao%20Liu%2C%20Yufeng%20Adaptively%20weighted%20large%20margin%20classifiers%202013"
        },
        {
            "id": "26",
            "entry": "[26] Yichao Wu, Hao Helen Zhang, and Yufeng Liu. Robust model-free multiclass probability estimation. Journal of the American Statistical Association, 105(489):424\u2013436, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yichao%20Zhang%2C%20Hao%20Helen%20Liu%2C%20Yufeng%20Robust%20model-free%20multiclass%20probability%20estimation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yichao%20Zhang%2C%20Hao%20Helen%20Liu%2C%20Yufeng%20Robust%20model-free%20multiclass%20probability%20estimation%202010"
        },
        {
            "id": "27",
            "entry": "[27] Chong Zhang and Yufeng Liu. Multicategory large-margin unified machines. The Journal of Machine Learning Research, 14(1):1349\u20131386, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chong%20Liu%2C%20Yufeng%20Multicategory%20large-margin%20unified%20machines%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chong%20Liu%2C%20Yufeng%20Multicategory%20large-margin%20unified%20machines%202013"
        },
        {
            "id": "28",
            "entry": "[28] Chong Zhang, Wenbo Wang, and Xingye Qiao. On reject and refine options in multicategory classification. Journal of the American Statistical Association, 2017. accepted.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chong%20Wang%2C%20Wenbo%20Qiao%2C%20Xingye%20On%20reject%20and%20refine%20options%20in%20multicategory%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chong%20Wang%2C%20Wenbo%20Qiao%2C%20Xingye%20On%20reject%20and%20refine%20options%20in%20multicategory%20classification%202017"
        },
        {
            "id": "29",
            "entry": "[29] Hao Helen Zhang, Yufeng Liu, Yichao Wu, Ji Zhu, et al. Variable selection for the multicategory svm via adaptive sup-norm regularization. Electronic Journal of Statistics, 2:149\u2013167, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Hao%20Helen%20Liu%2C%20Yufeng%20Wu%2C%20Yichao%20Zhu%2C%20Ji%20Variable%20selection%20for%20the%20multicategory%20svm%20via%20adaptive%20sup-norm%20regularization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Hao%20Helen%20Liu%2C%20Yufeng%20Wu%2C%20Yichao%20Zhu%2C%20Ji%20Variable%20selection%20for%20the%20multicategory%20svm%20via%20adaptive%20sup-norm%20regularization%202008"
        },
        {
            "id": "30",
            "entry": "[30] Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, pages 56\u201385, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Statistical%20behavior%20and%20consistency%20of%20classification%20methods%20based%20on%20convex%20risk%20minimization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Statistical%20behavior%20and%20consistency%20of%20classification%20methods%20based%20on%20convex%20risk%20minimization%202004"
        },
        {
            "id": "31",
            "entry": "[31] Ji Zhu and Trevor Hastie. Kernel logistic regression and the import vector machine. Journal of Computational and Graphical Statistics, 14(1):185\u2013205, 2005. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Ji%20Hastie%2C%20Trevor%20Kernel%20logistic%20regression%20and%20the%20import%20vector%20machine%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Ji%20Hastie%2C%20Trevor%20Kernel%20logistic%20regression%20and%20the%20import%20vector%20machine%202005"
        }
    ]
}
