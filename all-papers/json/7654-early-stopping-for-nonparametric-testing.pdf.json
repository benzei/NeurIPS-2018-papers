{
    "filename": "7654-early-stopping-for-nonparametric-testing.pdf",
    "metadata": {
        "title": "Early Stopping for Nonparametric Testing",
        "author": "Meimei Liu, Guang Cheng",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7654-early-stopping-for-nonparametric-testing.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Early stopping of iterative algorithms is an algorithmic regularization method to avoid over-fitting in estimation and classification. In this paper, we show that early stopping can also be applied to obtain the minimax optimal testing in a general non-parametric setup. Specifically, a Wald-type test statistic is obtained based on an iterated estimate produced by functional gradient descent algorithms in a reproducing kernel Hilbert space. A notable contribution is to establish a \u201csharp\u201d stopping rule: when the number of iterations achieves an optimal order, testing optimality is achievable; otherwise, testing optimality becomes impossible. As a by-product, a similar sharpness result is also derived for minimax optimal estimation under early stopping. All obtained results hold for various kernel classes, including Sobolev smoothness classes and Gaussian kernel classes."
    },
    "keywords": [
        {
            "term": "cross validation",
            "url": "https://en.wikipedia.org/wiki/cross_validation"
        },
        {
            "term": "stopping rule",
            "url": "https://en.wikipedia.org/wiki/stopping_rule"
        },
        {
            "term": "early stopping",
            "url": "https://en.wikipedia.org/wiki/early_stopping"
        },
        {
            "term": "kernel method",
            "url": "https://en.wikipedia.org/wiki/kernel_method"
        }
    ],
    "highlights": [
        "We begin by introducing some background on reproducing kernel Hilbert space (RKHS), and functional gradient descent algorithms in the reproducing kernel Hilbert space, together with our nonparametric testing formulation.<br/><br/>2.1",
        "We address this issue by proposing a data-dependent early stopping rule that enjoys both theoretical support and computational efficiency",
        "In the specific examples of polynomial decaying kernel and exponential decaying kernel, we further show that the developed stopping rule is \u201csharp\u201d: testing optimality is obtained if and only if the number of iterations obtains an optimal order defined by the stopping rule",
        "We discover an interesting connection that the inverse of these \u21e4 share the same order as the stopping rules in",
        "As shown in Figure 2 (c), the ES enjoys great computation efficiency compared with the Wald-test with 10 fold cross validation, and it is reasonable that our proposed ES takes more time than the oracle ES, due to the extra step for bootstrapping"
    ],
    "key_statements": [
        "We begin by introducing some background on reproducing kernel Hilbert space (RKHS), and functional gradient descent algorithms in the reproducing kernel Hilbert space, together with our nonparametric testing formulation.<br/><br/>2.1",
        "We address this issue by proposing a data-dependent early stopping rule that enjoys both theoretical support and computational efficiency",
        "In the specific examples of polynomial decaying kernel and exponential decaying kernel, we further show that the developed stopping rule is \u201csharp\u201d: testing optimality is obtained if and only if the number of iterations obtains an optimal order defined by the stopping rule",
        "We discover an interesting connection that the inverse of these \u21e4 share the same order as the stopping rules in",
        "As shown in Figure 2 (c), the ES enjoys great computation efficiency compared with the Wald-test with 10 fold cross validation, and it is reasonable that our proposed ES takes more time than the oracle ES, due to the extra step for bootstrapping"
    ],
    "summary": [
        "We begin by introducing some background on reproducing kernel Hilbert space (RKHS), and functional gradient descent algorithms in the RKHS, together with our nonparametric testing formulation.<br/><br/>2.1.",
        "Given a sequence of step size {\u21b5t}t1=0 satisfying Assumption A2, we first introduce the stopping rule as follows: 8 < T \u21e4 := argmin :t 2 N",
        "For testing is different from another type of bias-variance tradeoff in estimation, leading to different optimal stopping time.",
        "The following Theorem 3.3 shows that, if the alternative signal f is separated from zero by an order dn,t, the proposed test statistic Dn,t asymptotically achieves high power at the total step size \u2318t.",
        "The general Theorem 3.3 implies the following concrete stopping rules under various kernel classes.",
        "We claim that T \u21e4 is sharp in the sense that testing optimality is obtained if and only if the total step size achieves the order of \u2318T \u21e4 .",
        "Given a sequence of step size {\u21b5t}t1=0 satisfying Assumption A2, we have the following results.",
        "Given the stopping time Te defined by (4.1), there are universal positive constants (c1, c2) such that the following events hold with probability at least 1 c1 exp( c2n/\u2318Te): (a) For all iterations t = 1, 2, \u00b7 \u00b7 \u00b7 , Te: kft f \u21e4k2 n e\u2318t .",
        "An \u201coracle\u201d method is to base its stopping time on the exact in-sample bias of ft and the standard derivation of Dn,t, which is defined as follows: (",
        "For the oracle ES, we follow the stopping rule in (5.1) with constant step size \u21b5 = 1.",
        "As shown in Figure 2 (c), the ES enjoys great computation efficiency compared with the Wald-test with 10 fold CV, and it is reasonable that our proposed ES takes more time than the oracle ES, due to the extra step for bootstrapping.",
        "In Figure 3 (a) and (b), we use the second-order Sobolev kernel (i.e., m = 2 in PDK) to fit the model, and set the constant step size \u21b5 = 1.",
        "To display the impact of the stopping time on power performance, we set the total iteration steps T as (n8/9) with = 2/3, 1, 4/3 and n ranges from 100 to 1000.",
        "In Figure 3 (c) and (d), we use Gaussian kernel (i.e., p = 2 in EDK) to fit the model, and set the constant step size \u21b5 = 1.",
        "The main contribution of this paper is that we apply the early stopping strategy to nonparametric testing, and propose the first \u201csharp\u201d stopping rule to guarantee minimax optimal testing."
    ],
    "headline": "We show that early stopping can be applied to obtain the minimax optimal testing in a general non-parametric setup",
    "reference_links": [
        {
            "id": "Buehlmann_2003_a",
            "entry": "Peter B\u00fchlmann and Bin Yu. Boosting with the l 2 loss: regression and classification. Journal of the American Statistical Association, 98(462):324\u2013339, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%BChlmann%2C%20Peter%20Yu%2C%20Bin%20Boosting%20with%20the%20l%202%20loss%3A%20regression%20and%20classification%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=B%C3%BChlmann%2C%20Peter%20Yu%2C%20Bin%20Boosting%20with%20the%20l%202%20loss%3A%20regression%20and%20classification%202003"
        },
        {
            "id": "Drineas_2005_a",
            "entry": "Petros Drineas and Michael W Mahoney. On the nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning. journal of machine learning research, 6(Dec):2153\u20132175, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20On%20the%20nystr%C3%B6m%20method%20for%20approximating%20a%20gram%20matrix%20for%20improved%20kernel-based%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20On%20the%20nystr%C3%B6m%20method%20for%20approximating%20a%20gram%20matrix%20for%20improved%20kernel-based%20learning%202005"
        },
        {
            "id": "Fan_2007_a",
            "entry": "Jianqing Fan and Jiancheng Jiang. Nonparametric inference with generalized likelihood ratio tests. TEST, 16(3):409\u2013444, Dec 2007. ISSN 1863-8260.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Jianqing%20Jiang%2C%20Jiancheng%20Nonparametric%20inference%20with%20generalized%20likelihood%20ratio%20tests%202007-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Jianqing%20Jiang%2C%20Jiancheng%20Nonparametric%20inference%20with%20generalized%20likelihood%20ratio%20tests%202007-12"
        },
        {
            "id": "Fan_et+al_2001_a",
            "entry": "Jianqing Fan, Chunming Zhang, and Jian Zhang. Generalized likelihood ratio statistics and wilks phenomenon. The Annals of statistics, 29(1):153\u2013193, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Jianqing%20Zhang%2C%20Chunming%20Zhang%2C%20Jian%20Generalized%20likelihood%20ratio%20statistics%20and%20wilks%20phenomenon%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Jianqing%20Zhang%2C%20Chunming%20Zhang%2C%20Jian%20Generalized%20likelihood%20ratio%20statistics%20and%20wilks%20phenomenon%202001"
        },
        {
            "id": "Golub_et+al_1979_a",
            "entry": "Gene H Golub, Michael Heath, and Grace Wahba. Generalized cross-validation as a method for choosing a good ridge parameter. Technometrics, 21(2):215\u2013223, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20Gene%20H.%20Heath%2C%20Michael%20Wahba%2C%20Grace%20Generalized%20cross-validation%20as%20a%20method%20for%20choosing%20a%20good%20ridge%20parameter%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golub%2C%20Gene%20H.%20Heath%2C%20Michael%20Wahba%2C%20Grace%20Generalized%20cross-validation%20as%20a%20method%20for%20choosing%20a%20good%20ridge%20parameter%201979"
        },
        {
            "id": "Guo_2002_a",
            "entry": "Wensheng Guo. Inference in smoothing spline analysis of variance. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4):887\u2013898, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Wensheng%20Inference%20in%20smoothing%20spline%20analysis%20of%20variance%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Wensheng%20Inference%20in%20smoothing%20spline%20analysis%20of%20variance%202002"
        },
        {
            "id": "Ingster_1993_a",
            "entry": "Yuri I Ingster. Asymptotically minimax hypothesis testing for nonparametric alternatives. i, ii, iii. Mathematical Methods of Statistics, 2(2):85\u2013114, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ingster%2C%20Yuri%20I.%20Asymptotically%20minimax%20hypothesis%20testing%20for%20nonparametric%20alternatives%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ingster%2C%20Yuri%20I.%20Asymptotically%20minimax%20hypothesis%20testing%20for%20nonparametric%20alternatives%201993"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Meimei Liu, Zuofeng Shang, and Guang Cheng. Nonparametric testing under random projection. arXiv preprint arXiv:1802.06308, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06308"
        },
        {
            "id": "Lu_et+al_2016_a",
            "entry": "Junwei Lu, Guang Cheng, and Han Liu. Nonparametric heterogeneity testing for massive data. arXiv preprint arXiv:1601.06212, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06212"
        },
        {
            "id": "Ma_2017_a",
            "entry": "Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective on large-scale shallow learning. In Advances in Neural Information Processing Systems, pages 3781\u20133790, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Siyuan%20Belkin%2C%20Mikhail%20Diving%20into%20the%20shallows%3A%20a%20computational%20perspective%20on%20large-scale%20shallow%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Siyuan%20Belkin%2C%20Mikhail%20Diving%20into%20the%20shallows%3A%20a%20computational%20perspective%20on%20large-scale%20shallow%20learning%202017"
        },
        {
            "id": "Raskutti_et+al_2014_a",
            "entry": "Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. Journal of Machine Learning Research, 15(1):335\u2013366, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Early%20stopping%20and%20non-parametric%20regression%3A%20an%20optimal%20data-dependent%20stopping%20rule%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Early%20stopping%20and%20non-parametric%20regression%3A%20an%20optimal%20data-dependent%20stopping%20rule%202014"
        },
        {
            "id": "Rudelson_2013_a",
            "entry": "Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration. Electronic Communications in Probability, 18(82):1\u20139, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudelson%2C%20Mark%20Vershynin%2C%20Roman%20Hanson-wright%20inequality%20and%20sub-gaussian%20concentration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudelson%2C%20Mark%20Vershynin%2C%20Roman%20Hanson-wright%20inequality%20and%20sub-gaussian%20concentration%202013"
        },
        {
            "id": "Schoelkopf_et+al_1999_a",
            "entry": "Bernhard Sch\u00f6lkopf, Christopher JC Burges, and Alexander J Smola. Advances in kernel methods: support vector learning. MIT press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20Bernhard%20Burges%2C%20Christopher%20J.C.%20Smola%2C%20Alexander%20J.%20Advances%20in%20kernel%20methods%3A%20support%20vector%20learning%201999"
        },
        {
            "id": "Shang_2013_a",
            "entry": "Zuofeng Shang and Guang Cheng. Local and global asymptotic inference in smoothing spline models. The Annals of Statistics, 41(5):2608\u20132638, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shang%2C%20Zuofeng%20Cheng%2C%20Guang%20Local%20and%20global%20asymptotic%20inference%20in%20smoothing%20spline%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shang%2C%20Zuofeng%20Cheng%2C%20Guang%20Local%20and%20global%20asymptotic%20inference%20in%20smoothing%20spline%20models%202013"
        },
        {
            "id": "Shawe-Taylor_2004_a",
            "entry": "John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shawe-Taylor%2C%20John%20Cristianini%2C%20Nello%20Kernel%20methods%20for%20pattern%20analysis%202004"
        },
        {
            "id": "Stewart_2002_a",
            "entry": "Gilbert W Stewart. A krylov\u2013schur algorithm for large eigenproblems. SIAM Journal on Matrix Analysis and Applications, 23(3):601\u2013614, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stewart%2C%20Gilbert%20W.%20A%20krylov%E2%80%93schur%20algorithm%20for%20large%20eigenproblems%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stewart%2C%20Gilbert%20W.%20A%20krylov%E2%80%93schur%20algorithm%20for%20large%20eigenproblems%202002"
        },
        {
            "id": "Wahba_1990_a",
            "entry": "Grace Wahba. Spline models for observational data. SIAM, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wahba%2C%20Grace%20Spline%20models%20for%20observational%20data%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wahba%2C%20Grace%20Spline%20models%20for%20observational%20data%201990"
        },
        {
            "id": "Wei_2017_a",
            "entry": "Yuting Wei and Martin J Wainwright. The local geometry of testing in ellipses: Tight control via localized kolomogorov widths. arXiv preprint arXiv:1712.00711, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00711"
        },
        {
            "id": "Wei_et+al_2017_b",
            "entry": "Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. In Advances in Neural Information Processing Systems, pages 6067\u20136077, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Yuting%20Yang%2C%20Fanny%20Wainwright%2C%20Martin%20J.%20Early%20stopping%20for%20kernel%20boosting%20algorithms%3A%20A%20general%20analysis%20with%20localized%20complexities%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Yuting%20Yang%2C%20Fanny%20Wainwright%2C%20Martin%20J.%20Early%20stopping%20for%20kernel%20boosting%20algorithms%3A%20A%20general%20analysis%20with%20localized%20complexities%202017"
        },
        {
            "id": "Yao_et+al_2007_a",
            "entry": "Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007"
        },
        {
            "id": "Zhang_2005_a",
            "entry": "Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. The Annals of Statistics, 33(4):1538\u20131579, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Yu%2C%20Bin%20Boosting%20with%20early%20stopping%3A%20Convergence%20and%20consistency%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Yu%2C%20Bin%20Boosting%20with%20early%20stopping%3A%20Convergence%20and%20consistency%202005"
        }
    ]
}
