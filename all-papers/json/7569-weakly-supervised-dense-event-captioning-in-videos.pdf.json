{
    "filename": "7569-weakly-supervised-dense-event-captioning-in-videos.pdf",
    "metadata": {
        "title": "Weakly Supervised Dense Event Captioning in Videos",
        "author": "Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu, Junzhou Huang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7569-weakly-supervised-dense-event-captioning-in-videos.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Dense event captioning aims to detect and describe all events of interest contained in a video. Despite the advanced development in this area, existing methods tackle this task by making use of dense temporal annotations, which is dramatically source-consuming. This paper formulates a new problem: weakly supervised dense event captioning, which does not require temporal segment annotations for model training. Our solution is based on the one-to-one correspondence assumption, each caption describes one temporal segment, and each temporal segment has one caption, which holds in current benchmark datasets and most real-world cases. We decompose the problem into a pair of dual problems: event captioning and sentence localization and present a cycle system to train our model. Extensive experimental results are provided to demonstrate the ability of our model on both dense event captioning and sentence localization in videos."
    },
    "keywords": [
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "video content",
            "url": "https://en.wikipedia.org/wiki/video_content"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Dramatic improvements have been made on video understanding due to the development of deep neural networks and large-scale video datasets [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "The machine is required to describe the video content in the natural language form, which makes it more meticulous and challenging compared to other tasks describing the video content using a few tags or labels, such as video classification and action detection [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "We introduce a new problem, Weakly Supervised Dense Event Captioning (WS-DEC)2, which aims at dense event captioning only using the caption annotations for training",
        "We propose to solve the DEC task without the need of temporal segments annotation, introduce a new problem Weakly Supervised Dense Event Caption, aiming at making use of the huge amount of data in the web and reducing the cost of annotation",
        "As for the unsupervised scenario, we can see that our unsupervised model outperforms Cross-modal Temporal Regression Localizer by a considerable margin, which shows that our model can really learn to locate meaningful temporal segment from the indirect losses"
    ],
    "key_statements": [
        "Dramatic improvements have been made on video understanding due to the development of deep neural networks and large-scale video datasets [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "The machine is required to describe the video content in the natural language form, which makes it more meticulous and challenging compared to other tasks describing the video content using a few tags or labels, such as video classification and action detection [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "We introduce a new problem, Weakly Supervised Dense Event Captioning (WS-DEC)2, which aims at dense event captioning only using the caption annotations for training",
        "We propose to solve the DEC task without the need of temporal segments annotation, introduce a new problem Weakly Supervised Dense Event Caption, aiming at making use of the huge amount of data in the web and reducing the cost of annotation",
        "A remaining issue that it is still infeasible to perform dense event captioning in the testing phase by applying l\u03b81 or g\u03b82 since both the temporal segment and caption sentence are unknown",
        "The event captioning results are summarized in Table 1",
        "As for the unsupervised scenario, we can see that our unsupervised model outperforms Cross-modal Temporal Regression Localizer by a considerable margin, which shows that our model can really learn to locate meaningful temporal segment from the indirect losses"
    ],
    "summary": [
        "Dramatic improvements have been made on video understanding due to the development of deep neural networks and large-scale video datasets [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "Dataset, only a paragraph or a set of sentences is available to describe each video, but the temporal segment coordinate of each event and its correspondence to the captioning sentence is not given.",
        "We perform sentence localization from the given caption annotation, to obtain the associated segment that is fed to the caption generator to reconstruct the caption back.",
        "Despite the similar weakly-supervised setting to this work, our paper differently is to localize different events temporally and perform captioning for each detected event, which generates descriptions based on meaningful events instead of bewildering visual features.",
        "The sentence localization is originally formulated as an intermediate task to enable weakly supervised training for dense event captioning.",
        "A remaining issue that it is still infeasible to perform dense event captioning in the testing phase by applying l\u03b81 or g\u03b82 since both the temporal segment and caption sentence are unknown.",
        "Any differential model can be applied to formulate the sentence localizer and caption generator.",
        "We evenly divide the input video into multiple anchor segments under multiple scales, and train a FC layer on thef cv to predict the best anchor that produces the highest Meteor score [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] of the generated caption sentence.",
        "The temporal segment and caption sentence of each human event is annotated.",
        "As detailed in the supplementary material, both the video and sentence encoders apply the GRU models[<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] for feature extraction, where the dimensions of hidden and output layers are 512.",
        "The first variant is the pretrained model where we randomly sample an event segment from each video and feed it into the pretrained caption generator for captioning in the testing phase.",
        "As we use a bunch of randomly selected temporal segments to generate the caption results, the robustness of the model towards such random strategy should be evaluated.",
        "We use a different number of temporal segments and different random seeds to generate event caption sentences, and the evaluation results are summarized in Table 3.",
        "One is that our model sometimes cannot capture the beginning of an event, which, in our opinion, is due to the fact that we use the final hidden state of a temporal segment to generate description which does not rely much on the starting coordinate.",
        "We raise a new task termed Weakly Supervised Dense Event Caption(WS-DEC) and propose an efficient method to tackle it.",
        "Since weakly supervised learning is becoming an important research vein in the domain, our proposed method by using the cycle process and fixed-point iteration could be applied to more other tasks, e.g., weakly-supervised detection"
    ],
    "headline": "We introduce a new problem, Weakly Supervised Dense Event Captioning 2, which aims at dense event captioning only using the caption annotations for training",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. Activitynet: A largescale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961\u2013970, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heilbron%2C%20Bernard%20Ghanem%20Fabian%20Caba%20Escorcia%2C%20Victor%20Niebles%2C%20Juan%20Carlos%20Activitynet%3A%20A%20largescale%20video%20benchmark%20for%20human%20activity%20understanding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heilbron%2C%20Bernard%20Ghanem%20Fabian%20Caba%20Escorcia%2C%20Victor%20Niebles%2C%20Juan%20Carlos%20Activitynet%3A%20A%20largescale%20video%20benchmark%20for%20human%20activity%20understanding%202015"
        },
        {
            "id": "2",
            "entry": "[2] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014"
        },
        {
            "id": "3",
            "entry": "[3] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06950"
        },
        {
            "id": "4",
            "entry": "[4] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.4729"
        },
        {
            "id": "5",
            "entry": "[5] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang, Bret Harsham, John R Hershey, Tim K Marks, and Kazuhiko Sumi. Attention-based multimodal fusion for video description. In Computer Vision (ICCV), 2017 IEEE International Conference on, pages 4203\u20134212. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hori%2C%20Chiori%20Hori%2C%20Takaaki%20Lee%2C%20Teng-Yok%20Zhang%2C%20Ziming%20Attention-based%20multimodal%20fusion%20for%20video%20description%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hori%2C%20Chiori%20Hori%2C%20Takaaki%20Lee%2C%20Teng-Yok%20Zhang%2C%20Ziming%20Attention-based%20multimodal%20fusion%20for%20video%20description%202017"
        },
        {
            "id": "6",
            "entry": "[6] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2625\u20132634, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20Anne%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20Anne%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015"
        },
        {
            "id": "7",
            "entry": "[7] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision, pages 4534\u20134542, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Venugopalan%2C%20Subhashini%20Rohrbach%2C%20Marcus%20Donahue%2C%20Jeffrey%20Mooney%2C%20Raymond%20Sequence%20to%20sequence-video%20to%20text%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Venugopalan%2C%20Subhashini%20Rohrbach%2C%20Marcus%20Donahue%2C%20Jeffrey%20Mooney%2C%20Raymond%20Sequence%20to%20sequence-video%20to%20text%202015"
        },
        {
            "id": "8",
            "entry": "[8] Huijuan Xu, Subhashini Venugopalan, Vasili Ramanishka, Marcus Rohrbach, and Kate Saenko. A multi-scale multiple instance video description network. arXiv preprint arXiv:1505.05914, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05914"
        },
        {
            "id": "9",
            "entry": "[9] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video paragraph captioning using hierarchical recurrent neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4584\u20134593, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Haonan%20Wang%2C%20Jiang%20Huang%2C%20Zhiheng%20Yang%2C%20Yi%20Video%20paragraph%20captioning%20using%20hierarchical%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Haonan%20Wang%2C%20Jiang%20Huang%2C%20Zhiheng%20Yang%2C%20Yi%20Video%20paragraph%20captioning%20using%20hierarchical%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "10",
            "entry": "[10] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 706\u2013715, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishna%2C%20Ranjay%20Hata%2C%20Kenji%20Ren%2C%20Frederic%20Fei-Fei%2C%20Li%20Dense-captioning%20events%20in%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishna%2C%20Ranjay%20Hata%2C%20Kenji%20Ren%2C%20Frederic%20Fei-Fei%2C%20Li%20Dense-captioning%20events%20in%20videos%202017"
        },
        {
            "id": "11",
            "entry": "[11] Ting Yao, Yehao Li, Zhaofan Qiu, Fuchen Long, Yingwei Pan, Dong Li, and Tao Mei. Msr asia msm at activitynet challenge 2017: Trimmed action recognition, temporal action proposals and dense-captioning events in videos.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ting%20Yao%20Yehao%20Li%20Zhaofan%20Qiu%20Fuchen%20Long%20Yingwei%20Pan%20Dong%20Li%20and%20Tao%20Mei%20Msr%20asia%20msm%20at%20activitynet%20challenge%202017%20Trimmed%20action%20recognition%20temporal%20action%20proposals%20and%20densecaptioning%20events%20in%20videos"
        },
        {
            "id": "12",
            "entry": "[12] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 568\u2013576. Curran Associates, Inc., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014"
        },
        {
            "id": "13",
            "entry": "[13] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European Conference on Computer Vision, pages 20\u201336.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Wang%2C%20Zhe%20Qiao%2C%20Yu%20Temporal%20segment%20networks%3A%20Towards%20good%20practices%20for%20deep%20action%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Wang%2C%20Zhe%20Qiao%2C%20Yu%20Temporal%20segment%20networks%3A%20Towards%20good%20practices%20for%20deep%20action%20recognition"
        },
        {
            "id": "14",
            "entry": "[14] Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recurrent%20neural%20network%20based%20language%20model%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "15",
            "entry": "[15] Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, and Xiangyang Xue. Weakly supervised dense video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 2, page 10, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Zhiqiang%20Li%2C%20Jianguo%20Su%2C%20Zhou%20Li%2C%20Minjun%20Weakly%20supervised%20dense%20video%20captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Zhiqiang%20Li%2C%20Jianguo%20Su%2C%20Zhou%20Li%2C%20Minjun%20Weakly%20supervised%20dense%20video%20captioning%202017"
        },
        {
            "id": "16",
            "entry": "[16] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Daps: Deep action proposals for action understanding. In European Conference on Computer Vision, pages 768\u2013784.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Escorcia%2C%20Victor%20Heilbron%2C%20Fabian%20Caba%20Niebles%2C%20Juan%20Carlos%20Ghanem%2C%20Bernard%20Daps%3A%20Deep%20action%20proposals%20for%20action%20understanding",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Escorcia%2C%20Victor%20Heilbron%2C%20Fabian%20Caba%20Niebles%2C%20Juan%20Carlos%20Ghanem%2C%20Bernard%20Daps%3A%20Deep%20action%20proposals%20for%20action%20understanding"
        },
        {
            "id": "17",
            "entry": "[17] Ting Yao, Tao Mei, and Yong Rui. Highlight detection with pairwise deep ranking for first-person video summarization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 982\u2013990, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Ting%20Mei%2C%20Tao%20Rui%2C%20Yong%20Highlight%20detection%20with%20pairwise%20deep%20ranking%20for%20first-person%20video%20summarization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Ting%20Mei%2C%20Tao%20Rui%2C%20Yong%20Highlight%20detection%20with%20pairwise%20deep%20ranking%20for%20first-person%20video%20summarization%202016"
        },
        {
            "id": "18",
            "entry": "[18] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei. Boosting image captioning with attributes. OpenReview, 2(5):8, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Ting%20Pan%2C%20Yingwei%20Li%2C%20Yehao%20Qiu%2C%20Zhaofan%20Boosting%20image%20captioning%20with%20attributes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Ting%20Pan%2C%20Yingwei%20Li%2C%20Yehao%20Qiu%2C%20Zhaofan%20Boosting%20image%20captioning%20with%20attributes%202016"
        },
        {
            "id": "19",
            "entry": "[19] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. Jointly localizing and describing events for dense video captioning. arXiv preprint arXiv:1804.08274, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08274"
        },
        {
            "id": "20",
            "entry": "[20] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense video captioning with masked transformer. arXiv preprint arXiv:1804.00819, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00819"
        },
        {
            "id": "21",
            "entry": "[21] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu. Bidirectional attentive fusion with context gating for dense video captioning. arXiv preprint arXiv:1804.00100, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00100"
        },
        {
            "id": "22",
            "entry": "[22] Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard Ghanem, and Juan Carlos Niebles. Sst: Singlestream temporal action proposals. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 6373\u20136382. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buch%2C%20Shyamal%20Escorcia%2C%20Victor%20Shen%2C%20Chuanqi%20Ghanem%2C%20Bernard%20Sst%3A%20Singlestream%20temporal%20action%20proposals%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buch%2C%20Shyamal%20Escorcia%2C%20Victor%20Shen%2C%20Chuanqi%20Ghanem%2C%20Bernard%20Sst%3A%20Singlestream%20temporal%20action%20proposals%202017"
        },
        {
            "id": "23",
            "entry": "[23] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramakrishna%20Vedantam%2C%20C.Lawrence%20Zitnick%20Parikh%2C%20Devi%20Cider%3A%20Consensus-based%20image%20description%20evaluation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramakrishna%20Vedantam%2C%20C.Lawrence%20Zitnick%20Parikh%2C%20Devi%20Cider%3A%20Consensus-based%20image%20description%20evaluation%202015"
        },
        {
            "id": "24",
            "entry": "[24] Iftekhar Naim, Young Chol Song, Qiguang Liu, Henry A Kautz, Jiebo Luo, and Daniel Gildea. Unsupervised alignment of natural language instructions with video segments. In AAAI, pages 1558\u20131564, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naim%2C%20Iftekhar%20Song%2C%20Young%20Chol%20Liu%2C%20Qiguang%20Kautz%2C%20Henry%20A.%20Unsupervised%20alignment%20of%20natural%20language%20instructions%20with%20video%20segments%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Naim%2C%20Iftekhar%20Song%2C%20Young%20Chol%20Liu%2C%20Qiguang%20Kautz%2C%20Henry%20A.%20Unsupervised%20alignment%20of%20natural%20language%20instructions%20with%20video%20segments%202014"
        },
        {
            "id": "25",
            "entry": "[25] Young Chol Song, Iftekhar Naim, Abdullah Al Mamun, Kaustubh Kulkarni, Parag Singla, Jiebo Luo, Daniel Gildea, and Henry A Kautz. Unsupervised alignment of actions in video with text descriptions. In IJCAI, pages 2025\u20132031, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Young%20Chol%20Naim%2C%20Iftekhar%20Mamun%2C%20Abdullah%20Al%20Kulkarni%2C%20Kaustubh%20and%20Henry%20A%20Kautz.%20Unsupervised%20alignment%20of%20actions%20in%20video%20with%20text%20descriptions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Young%20Chol%20Naim%2C%20Iftekhar%20Mamun%2C%20Abdullah%20Al%20Kulkarni%2C%20Kaustubh%20and%20Henry%20A%20Kautz.%20Unsupervised%20alignment%20of%20actions%20in%20video%20with%20text%20descriptions%202016"
        },
        {
            "id": "26",
            "entry": "[26] Piotr Bojanowski, R\u00e9mi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, and Cordelia Schmid. Weakly-supervised alignment of video with text. In Computer Vision (ICCV), 2015 IEEE International Conference on, pages 4462\u20134470. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20Piotr%20Lajugie%2C%20R%C3%A9mi%20Grave%2C%20Edouard%20Bach%2C%20Francis%20Weakly-supervised%20alignment%20of%20video%20with%20text%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojanowski%2C%20Piotr%20Lajugie%2C%20R%C3%A9mi%20Grave%2C%20Edouard%20Bach%2C%20Francis%20Weakly-supervised%20alignment%20of%20video%20with%20text%202015"
        },
        {
            "id": "27",
            "entry": "[27] Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, and Ram Nevatia. Turn tap: Temporal unit regression network for temporal action proposals. arXiv preprint arXiv:1703.06189, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06189"
        },
        {
            "id": "28",
            "entry": "[28] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5267\u20135275, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Jiyang%20Sun%2C%20Chen%20Yang%2C%20Zhenheng%20Nevatia%2C%20Ram%20Tall%3A%20Temporal%20activity%20localization%20via%20language%20query%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Jiyang%20Sun%2C%20Chen%20Yang%2C%20Zhenheng%20Nevatia%2C%20Ram%20Tall%3A%20Temporal%20activity%20localization%20via%20language%20query%202017"
        },
        {
            "id": "29",
            "entry": "[29] Yitian Yuan, Tao Mei, and Wenwu Zhu. To find where you talk: Temporal sentence localization in video with attention based location regression. arXiv preprint arXiv:1804.07014, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07014"
        },
        {
            "id": "30",
            "entry": "[30] CE Chidume. Iterative approximation of fixed points of lipschitzian strictly pseudocontractive mappings. Proceedings of the American Mathematical Society, 99(2):283\u2013288, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20CE%20Chidume.%20Iterative%20approximation%20of%20fixed%20points%20of%20lipschitzian%20strictly%20pseudocontractive%20mappings%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20CE%20Chidume.%20Iterative%20approximation%20of%20fixed%20points%20of%20lipschitzian%20strictly%20pseudocontractive%20mappings%201987"
        },
        {
            "id": "31",
            "entry": "[31] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "32",
            "entry": "[32] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20Satanjeev%20Lavie%2C%20Alon%20Meteor%3A%20An%20automatic%20metric%20for%20mt%20evaluation%20with%20improved%20correlation%20with%20human%20judgments%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20Satanjeev%20Lavie%2C%20Alon%20Meteor%3A%20An%20automatic%20metric%20for%20mt%20evaluation%20with%20improved%20correlation%20with%20human%20judgments%202005"
        },
        {
            "id": "33",
            "entry": "[33] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "34",
            "entry": "[34] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 605. Association for Computational Linguistics, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Chin-Yew%20Och%2C%20Franz%20Josef%20Automatic%20evaluation%20of%20machine%20translation%20quality%20using%20longest%20common%20subsequence%20and%20skip-bigram%20statistics%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Chin-Yew%20Och%2C%20Franz%20Josef%20Automatic%20evaluation%20of%20machine%20translation%20quality%20using%20longest%20common%20subsequence%20and%20skip-bigram%20statistics%202004"
        },
        {
            "id": "35",
            "entry": "[35] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics, 2002. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        }
    ]
}
