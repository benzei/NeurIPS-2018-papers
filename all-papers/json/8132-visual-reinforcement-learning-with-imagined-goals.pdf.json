{
    "filename": "8132-visual-reinforcement-learning-with-imagined-goals.pdf",
    "metadata": {
        "title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
        "author": "Adrien Baranes, Pierre-Yves Oudeyer",
        "date": 2012,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8132-visual-reinforcement-learning-with-imagined-goals.pdf",
            "doi": "10.1016/j.robot.2012.05.008"
        },
        "journal": "Robotics and Autonomous Systems",
        "volume": "61",
        "abstract": "For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised \u201cpractice\u201d phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",
        "pages": "49-73"
    },
    "keywords": [
        {
            "term": "general purpose",
            "url": "https://en.wikipedia.org/wiki/general_purpose"
        },
        {
            "term": "mean-squared error",
            "url": "https://en.wikipedia.org/wiki/mean-squared_error"
        },
        {
            "term": "q function",
            "url": "https://en.wikipedia.org/wiki/q_function"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "active learning",
            "url": "https://en.wikipedia.org/wiki/active_learning"
        }
    ],
    "highlights": [
        "Our method combines reinforcement learning with goal-conditioned value functions and unsupervised representation learning",
        "If our aim is instead to obtain a policy that can accomplish a variety of tasks, we can construct a goal-conditioned policy and reward, and optimize the expected return with respect to a goal distribution: Eg\u21e0G[Eri,si\u21e0E,ai\u21e0\u21e1[R0]], where G is the set of goals and the reward is a function of g",
        "Reward Specification Comparison We evaluate how effective distance in the variational autoencoder latent space is for the Visual Pusher task",
        "We include the following methods for comparison: Latent Distance, which uses the reward used in reinforcement learning with imagined goals, i.e. A = I in Equation (3); Log Probability, which uses the Mahalanobis distance in Equation (3), where A is the precision matrix of the encoder; and Pixel mean-squared error, which uses mean-squared error (MSE) between state and goal in pixel space.\n3 In Figure 4, we see that latent distance significantly outperforms log probability",
        "We present a new Reinforcement learning algorithm that can efficiently solve goal-conditioned, vision-based tasks without access to any ground truth state or reward functions",
        "Algorithms that can learn in the real world and directly use raw images can allow a single policy to solve a large and diverse set of tasks, even when these tasks require distinct internal representations"
    ],
    "key_statements": [
        "Our method combines reinforcement learning with goal-conditioned value functions and unsupervised representation learning",
        "If our aim is instead to obtain a policy that can accomplish a variety of tasks, we can construct a goal-conditioned policy and reward, and optimize the expected return with respect to a goal distribution: Eg\u21e0G[Eri,si\u21e0E,ai\u21e0\u21e1[R0]], where G is the set of goals and the reward is a function of g",
        "We address these problems by learning a latent embedding using a -variational autoencoder",
        "We show in Section 5 that relabeling the goal with samples from the variational autoencoder prior results in significantly better sample-efficiency.\n4.4",
        "Reward Specification Comparison We evaluate how effective distance in the variational autoencoder latent space is for the Visual Pusher task",
        "We include the following methods for comparison: Latent Distance, which uses the reward used in reinforcement learning with imagined goals, i.e. A = I in Equation (3); Log Probability, which uses the Mahalanobis distance in Equation (3), where A is the precision matrix of the encoder; and Pixel mean-squared error, which uses mean-squared error (MSE) between state and goal in pixel space.\n3 In Figure 4, we see that latent distance significantly outperforms log probability",
        "We demonstrate that our method can handle this difficult scenario by evaluating on a task where the environment, based on the Visual Multi-Object Pusher, randomly contains zero, one, or two objects in each episode during testing",
        "reinforcement learning with imagined goals is a practical and straightforward algorithm to apply to real physical systems: the efficiency of off-policy learning with goal relabeling makes training times manageable, while the use of imagebased rewards through the learned representation frees us from the burden of manually design reward functions, which itself can require hand-engineered perception systems [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]",
        "Note that unlike previous results, we do not have access to the true puck position during training so for the learning curve we report test episode returns on the variational autoencoder latent distance reward",
        "We present a new Reinforcement learning algorithm that can efficiently solve goal-conditioned, vision-based tasks without access to any ground truth state or reward functions",
        "Algorithms that can learn in the real world and directly use raw images can allow a single policy to solve a large and diverse set of tasks, even when these tasks require distinct internal representations"
    ],
    "summary": [
        "Our method combines reinforcement learning with goal-conditioned value functions and unsupervised representation learning.",
        "No prior method uses model-free RL to learn policies conditioned on a single goal image with sufficient efficiency to train directly on real-world robotic systems, without access to ground-truth state or reward information during training.",
        "How does our method compare to prior model-free RL algorithms in terms of sample efficiency and performance, when learning continuous control tasks from images?",
        "No prior work demonstrates policies that can reach a variety of goal images without access to a true-state reward function, and so we needed to make modifications to make the comparisons feasible.",
        "We see in Figure 3 that our method can efficiently learn policies from visual inputs to perform simulated reaching and pushing, without access to the object state.",
        "We include the following methods for comparison: Latent Distance, which uses the reward used in RIG, i.e. A = I in Equation (3); Log Probability, which uses the Mahalanobis distance in Equation (3), where A is the precision matrix of the encoder; and Pixel MSE, which uses mean-squared error (MSE) between state and goal in pixel space.",
        "Learning with Variable Numbers of Objects A major advantage of working directly from pixels is that the policy input can represent combinatorial structure in the environment, which would be difficult to encode into a fixed-length state vector even if a perfect perception system were available.",
        "RIG is a practical and straightforward algorithm to apply to real physical systems: the efficiency of off-policy learning with goal relabeling makes training times manageable, while the use of imagebased rewards through the learned representation frees us from the burden of manually design reward functions, which itself can require hand-engineered perception systems [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>].",
        "We present a new RL algorithm that can efficiently solve goal-conditioned, vision-based tasks without access to any ground truth state or reward functions.",
        "Our method trains a generative model that is used for multiple purposes: we embed the state and goals using the encoder; we sample from the prior to generate goals for exploration; we sample latents to retroactively relabel goals and rewards; and we use distances in the latent space for rewards to train a goal-conditioned value function.",
        "Algorithms that can learn in the real world and directly use raw images can allow a single policy to solve a large and diverse set of tasks, even when these tasks require distinct internal representations."
    ],
    "headline": "We propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to Poke by Poking: Experiential Learning of Intuitive Physics. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Pulkit%20Nair%2C%20Ashvin%20Abbeel%2C%20Pieter%20Malik%2C%20Jitendra%20Learning%20to%20Poke%20by%20Poking%3A%20Experiential%20Learning%20of%20Intuitive%20Physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Pulkit%20Nair%2C%20Ashvin%20Abbeel%2C%20Pieter%20Malik%2C%20Jitendra%20Learning%20to%20Poke%20by%20Poking%3A%20Experiential%20Learning%20of%20Intuitive%20Physics%202016"
        },
        {
            "id": "2",
            "entry": "[2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay. In Advances in Neural Information Processing Systems (NIPS), jul 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcin%20Andrychowicz%20Filip%20Wolski%20Alex%20Ray%20Jonas%20Schneider%20Rachel%20Fong%20Peter%20Welinder%20Bob%20Mcgrew%20Josh%20Tobin%20Pieter%20Abbeel%20and%20Wojciech%20Zaremba%20Hindsight%20Experience%20Replay%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%20jul%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcin%20Andrychowicz%20Filip%20Wolski%20Alex%20Ray%20Jonas%20Schneider%20Rachel%20Fong%20Peter%20Welinder%20Bob%20Mcgrew%20Josh%20Tobin%20Pieter%20Abbeel%20and%20Wojciech%20Zaremba%20Hindsight%20Experience%20Replay%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%20jul%202017"
        },
        {
            "id": "3",
            "entry": "[3] A Baranes and P-Y Oudeyer. Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots. Robotics and Autonomous Systems, 61(1):49\u201373, 2012. doi: 10.1016/j.robot.2012. 05.008.",
            "crossref": "https://dx.doi.org/10.1016/j.robot.2012.05.008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.robot.2012.05.008"
        },
        {
            "id": "4",
            "entry": "[4] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems (NIPS), pages 1471\u20131479, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "5",
            "entry": "[5] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "6",
            "entry": "[6] Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. Discovering hidden factors of variation in deep networks. arXiv preprint arXiv:1412.6583, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6583"
        },
        {
            "id": "7",
            "entry": "[7] Guillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors of variation via generative entangling. CoRR, abs/1210.5, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Desjardins%2C%20Guillaume%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Disentangling%20factors%20of%20variation%20via%20generative%20entangling%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Desjardins%2C%20Guillaume%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Disentangling%20factors%20of%20variation%20via%20generative%20entangling%202012"
        },
        {
            "id": "8",
            "entry": "[8] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-Supervised Visual Planning with Temporal Skip Connections. In Conference on Robot Learning (CoRL), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ebert%2C%20Frederik%20Finn%2C%20Chelsea%20Lee%2C%20Alex%20X.%20Levine%2C%20Sergey%20Self-Supervised%20Visual%20Planning%20with%20Temporal%20Skip%20Connections%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ebert%2C%20Frederik%20Finn%2C%20Chelsea%20Lee%2C%20Alex%20X.%20Levine%2C%20Sergey%20Self-Supervised%20Visual%20Planning%20with%20Temporal%20Skip%20Connections%202017"
        },
        {
            "id": "9",
            "entry": "[9] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is All You Need: Learning Skills without a Reward Function. arXiv preprint arXiv:1802.06070, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06070"
        },
        {
            "id": "10",
            "entry": "[10] Chelsea Finn and Sergey Levine. Deep Visual Foresight for Planning Robot Motion. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Deep%20Visual%20Foresight%20for%20Planning%20Robot%20Motion%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Deep%20Visual%20Foresight%20for%20Planning%20Robot%20Motion%202016"
        },
        {
            "id": "11",
            "entry": "[11] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial autoencoders for visuomotor learning. In IEEE International Conference on Robotics and Automation (ICRA), volume 2016-June, pages 512\u2013519. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Tan%2C%20Xin%20Yu%20Duan%2C%20Yan%20Darrell%2C%20Trevor%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Tan%2C%20Xin%20Yu%20Duan%2C%20Yan%20Darrell%2C%20Trevor%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016"
        },
        {
            "id": "12",
            "entry": "[12] Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement learning. arXiv preprint arXiv:1704.03012, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03012"
        },
        {
            "id": "13",
            "entry": "[13] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error in ActorCritic Methods. arXiv preprint arXiv:1802.09477, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09477"
        },
        {
            "id": "14",
            "entry": "[14] David Ha and J\u00fcrgen Schmidhuber. World Models. arXiv preprint arXiv:1803.10122, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10122"
        },
        {
            "id": "15",
            "entry": "[15] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. -VAE: Learning basic visual concepts with a constrained variational framework. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Matthey%2C%20Loic%20Pal%2C%20Arka%20Burgess%2C%20Christopher%20-VAE%3A%20Learning%20basic%20visual%20concepts%20with%20a%20constrained%20variational%20framework%202017"
        },
        {
            "id": "16",
            "entry": "[16] Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higgins%2C%20Irina%20Pal%2C%20Arka%20Rusu%2C%20Andrei%20A.%20Matthey%2C%20Loic%20Darla%3A%20Improving%20zero-shot%20transfer%20in%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Higgins%2C%20Irina%20Pal%2C%20Arka%20Rusu%2C%20Andrei%20A.%20Matthey%2C%20Loic%20Darla%3A%20Improving%20zero-shot%20transfer%20in%20reinforcement%20learning%202017"
        },
        {
            "id": "17",
            "entry": "[17] Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin Riedmiller. Pves: Position-velocity encoders for unsupervised learning of structured state representations. arXiv preprint arXiv:1705.09805, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09805"
        },
        {
            "id": "18",
            "entry": "[18] L P Kaelbling. Learning to achieve goals. In IJCAI-93. Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, volume vol.2, pages 1094 \u2013 8, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaelbling%2C%20L.P.%20Learning%20to%20achieve%20goals%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaelbling%2C%20L.P.%20Learning%20to%20achieve%20goals%201993"
        },
        {
            "id": "19",
            "entry": "[19] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-Encoding%20Variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-Encoding%20Variational%20Bayes%202014"
        },
        {
            "id": "20",
            "entry": "[20] Sascha Lange and Martin A Riedmiller. Deep learning of visual control policies. In European Symposium on Artificial Neural Networks (ESANN). Citeseer, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20A.%20Deep%20learning%20of%20visual%20control%20policies%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20A.%20Deep%20learning%20of%20visual%20control%20policies%202010"
        },
        {
            "id": "21",
            "entry": "[21] Sascha Lange, Martin Riedmiller, Arne Voigtlander, and Arne Voigtl\u00e4nder. Autonomous reinforcement learning on raw visual input data in a real world application. In International Joint Conference on Neural Networks (IJCNN), number June, pages 1\u20138. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20Voigtlander%2C%20Arne%20Voigtl%C3%A4nder%2C%20Arne%20Autonomous%20reinforcement%20learning%20on%20raw%20visual%20input%20data%20in%20a%20real%20world%20application%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20Voigtlander%2C%20Arne%20Voigtl%C3%A4nder%2C%20Arne%20Autonomous%20reinforcement%20learning%20on%20raw%20visual%20input%20data%20in%20a%20real%20world%20application%202012"
        },
        {
            "id": "22",
            "entry": "[22] Alex Lee, Sergey Levine, and Pieter Abbeel. Learning Visual Servoing with Deep Features and Fitted Q-Iteration. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Alex%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Learning%20Visual%20Servoing%20with%20Deep%20Features%20and%20Fitted%20Q-Iteration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Alex%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Learning%20Visual%20Servoing%20with%20Deep%20Features%20and%20Fitted%20Q-Iteration%202017"
        },
        {
            "id": "23",
            "entry": "[23] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-End Training of Deep Visuomotor Policies. Journal of Machine Learning Research (JMLR), 17(1):1334\u20131373, 2016. ISSN 15337928. doi: 10.1007/s13398-014-0173-7.2.",
            "crossref": "https://dx.doi.org/10.1007/s13398-014-0173-7.2",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s13398-014-0173-7.2"
        },
        {
            "id": "24",
            "entry": "[24] Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. International Journal of Robotics Research, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Pastor%2C%20Peter%20Krizhevsky%2C%20Alex%20Quillen%2C%20Deirdre%20Learning%20Hand-Eye%20Coordination%20for%20Robotic%20Grasping%20with%20Deep%20Learning%20and%20Large-Scale%20Data%20Collection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Pastor%2C%20Peter%20Krizhevsky%2C%20Alex%20Quillen%2C%20Deirdre%20Learning%20Hand-Eye%20Coordination%20for%20Robotic%20Grasping%20with%20Deep%20Learning%20and%20Large-Scale%20Data%20Collection%202017"
        },
        {
            "id": "25",
            "entry": "[25] Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical Actor-Critic. arXiv preprint arXiv:1712.00948, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00948"
        },
        {
            "id": "26",
            "entry": "[26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "27",
            "entry": "[27] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P Lillicrap, David Silver, Koray Kavukcuoglu, Korayk@google Com, and Google Deepmind. Asynchronous Methods for Deep Reinforcement Learning. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%202016"
        },
        {
            "id": "28",
            "entry": "[28] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-Conditional Video Prediction using Deep Networks in Atari Games. In Advances in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20Action-Conditional%20Video%20Prediction%20using%20Deep%20Networks%20in%20Atari%20Games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Junhyuk%20Guo%2C%20Xiaoxiao%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20Action-Conditional%20Video%20Prediction%20using%20Deep%20Networks%20in%20Atari%20Games%202015"
        },
        {
            "id": "29",
            "entry": "[29] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-Driven Exploration by Self-Supervised Prediction. In International Conference on Machine Learning (ICML), pages 488\u2013489. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-Driven%20Exploration%20by%20Self-Supervised%20Prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-Driven%20Exploration%20by%20Self-Supervised%20Prediction%202017"
        },
        {
            "id": "30",
            "entry": "[30] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-Shot Visual Imitation. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-Shot%20Visual%20Imitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Mahmoudieh%2C%20Parsa%20Luo%2C%20Guanghao%20Agrawal%2C%20Pulkit%20Zero-Shot%20Visual%20Imitation%202018"
        },
        {
            "id": "31",
            "entry": "[31] Alexandre P\u00e9r\u00e9, Sebastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=P%C3%A9r%C3%A9%2C%20Alexandre%20Forestier%2C%20Sebastien%20Sigaud%2C%20Olivier%20Oudeyer%2C%20Pierre-Yves%20Unsupervised%20Learning%20of%20Goal%20Spaces%20for%20Intrinsically%20Motivated%20Goal%20Exploration%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=P%C3%A9r%C3%A9%2C%20Alexandre%20Forestier%2C%20Sebastien%20Sigaud%2C%20Olivier%20Oudeyer%2C%20Pierre-Yves%20Unsupervised%20Learning%20of%20Goal%20Spaces%20for%20Intrinsically%20Motivated%20Goal%20Exploration%202018"
        },
        {
            "id": "32",
            "entry": "[32] Lerrel Pinto and Abhinav Gupta. Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours. IEEE International Conference on Robotics and Automation (ICRA), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Supersizing%20Self-supervision%3A%20Learning%20to%20Grasp%20from%2050K%20Tries%20and%20700%20Robot%20Hours%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Supersizing%20Self-supervision%3A%20Learning%20to%20Grasp%20from%2050K%20Tries%20and%20700%20Robot%20Hours%202016"
        },
        {
            "id": "33",
            "entry": "[33] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric Actor Critic for Image-Based Robot Learning. arXiv preprint arXiv:1710.06542, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06542"
        },
        {
            "id": "34",
            "entry": "[34] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob Mcgrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research. arXiv preprint arXiv:1802.09464, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09464"
        },
        {
            "id": "35",
            "entry": "[35] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal Difference Models: Model-Free Deep RL For Model-Based Control. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pong%2C%20Vitchyr%20Gu%2C%20Shixiang%20Dalal%2C%20Murtaza%20Levine%2C%20Sergey%20Temporal%20Difference%20Models%3A%20Model-Free%20Deep%20RL%20For%20Model-Based%20Control%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pong%2C%20Vitchyr%20Gu%2C%20Shixiang%20Dalal%2C%20Murtaza%20Levine%2C%20Sergey%20Temporal%20Difference%20Models%3A%20Model-Free%20Deep%20RL%20For%20Model-Based%20Control%202018"
        },
        {
            "id": "36",
            "entry": "[36] Nikolay Ponomarenko, Lina Jin, Oleg Ieremeiev, Vladimir Lukin, Karen Egiazarian, Jaakko Astola, Benoit Vozel, Kacem Chehdi, Marco Carli, Federica Battisti, and Others. Image database TID2013: Peculiarities, results and perspectives. Signal Processing: Image Communication, 30:57\u201377, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ponomarenko%2C%20Nikolay%20Jin%2C%20Lina%20Ieremeiev%2C%20Oleg%20Lukin%2C%20Vladimir%20Image%20database%20TID2013%3A%20Peculiarities%2C%20results%20and%20perspectives%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ponomarenko%2C%20Nikolay%20Jin%2C%20Lina%20Ieremeiev%2C%20Oleg%20Lukin%2C%20Vladimir%20Image%20database%20TID2013%3A%20Peculiarities%2C%20results%20and%20perspectives%202015"
        },
        {
            "id": "37",
            "entry": "[37] Paulo Rauber, Filipe Mutz, and Juergen J\u00fcrgen Schmidhuber. Hindsight policy gradients. In CoRR, volume abs/1711.0, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rauber%2C%20Paulo%20Mutz%2C%20Filipe%20Schmidhuber%2C%20Juergen%20J%C3%BCrgen%20Hindsight%20policy%20gradients%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rauber%2C%20Paulo%20Mutz%2C%20Filipe%20Schmidhuber%2C%20Juergen%20J%C3%BCrgen%20Hindsight%20policy%20gradients%202017"
        },
        {
            "id": "38",
            "entry": "[38] Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation with manifold interaction. In International Conference on Machine Learning, pages 1431\u20131439, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20Sohn%2C%20Kihyuk%20Zhang%2C%20Yuting%20Lee%2C%20Honglak%20Learning%20to%20disentangle%20factors%20of%20variation%20with%20manifold%20interaction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Scott%20Sohn%2C%20Kihyuk%20Zhang%2C%20Yuting%20Lee%2C%20Honglak%20Learning%20to%20disentangle%20factors%20of%20variation%20with%20manifold%20interaction%202014"
        },
        {
            "id": "39",
            "entry": "[39] Andrei A Rusu, Matej Vecerik, Thomas Roth\u00f6rl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. Conference on Robot Learning (CoRL), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rusu%2C%20Andrei%20A.%20Vecerik%2C%20Matej%20Roth%C3%B6rl%2C%20Thomas%20Heess%2C%20Nicolas%20Sim-to-real%20robot%20learning%20from%20pixels%20with%20progressive%20nets.%20Conference%20on%20Robot%20Learning%20%28CoRL%29%202017"
        },
        {
            "id": "40",
            "entry": "[40] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal Value Function Approximators. In International Conference on Machine Learning (ICML), pages 1312\u20131320, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tom%20Schaul%20Daniel%20Horgan%20Karol%20Gregor%20and%20David%20Silver%20Universal%20Value%20Function%20Approximators%20In%20International%20Conference%20on%20Machine%20Learning%20ICML%20pages%2013121320%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tom%20Schaul%20Daniel%20Horgan%20Karol%20Gregor%20and%20David%20Silver%20Universal%20Value%20Function%20Approximators%20In%20International%20Conference%20on%20Machine%20Learning%20ICML%20pages%2013121320%202015"
        },
        {
            "id": "41",
            "entry": "[41] J\u00fcrgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6): 863\u2013879, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Learning%20factorial%20codes%20by%20predictability%20minimization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J%C3%BCrgen%20Learning%20factorial%20codes%20by%20predictability%20minimization%201992"
        },
        {
            "id": "42",
            "entry": "[42] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-contrastive networks: Self-supervised learning from video. arXiv preprint arXiv:1704.06888, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06888"
        },
        {
            "id": "43",
            "entry": "[43] Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13\u201329, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Linda%20Gasser%2C%20Michael%20The%20development%20of%20embodied%20cognition%3A%20Six%20lessons%20from%20babies%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20Linda%20Gasser%2C%20Michael%20The%20development%20of%20embodied%20cognition%3A%20Six%20lessons%20from%20babies%202005"
        },
        {
            "id": "44",
            "entry": "[44] Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00645"
        },
        {
            "id": "45",
            "entry": "[45] Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction. International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 10:761\u2013768, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Modayil%2C%20Joseph%20Delp%2C%20Michael%20Degris%2C%20Thomas%20Horde%3A%20A%20Scalable%20Real-time%20Architecture%20for%20Learning%20Knowledge%20from%20Unsupervised%20Sensorimotor%20Interaction%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Modayil%2C%20Joseph%20Delp%2C%20Michael%20Degris%2C%20Thomas%20Horde%3A%20A%20Scalable%20Real-time%20Architecture%20for%20Learning%20Knowledge%20from%20Unsupervised%20Sensorimotor%20Interaction%202011"
        },
        {
            "id": "46",
            "entry": "[46] Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently Controllable Factors. In NIPS Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valentin%20Thomas%20Jules%20Pondard%20Emmanuel%20Bengio%20Marc%20Sarfati%20Philippe%20Beaudoin%20MarieJean%20Meurs%20Joelle%20Pineau%20Doina%20Precup%20and%20Yoshua%20Bengio%20Independently%20Controllable%20Factors%20In%20NIPS%20Workshop%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valentin%20Thomas%20Jules%20Pondard%20Emmanuel%20Bengio%20Marc%20Sarfati%20Philippe%20Beaudoin%20MarieJean%20Meurs%20Joelle%20Pineau%20Doina%20Precup%20and%20Yoshua%20Bengio%20Independently%20Controllable%20Factors%20In%20NIPS%20Workshop%202017"
        },
        {
            "id": "47",
            "entry": "[47] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In The IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20MuJoCo%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20MuJoCo%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "48",
            "entry": "[48] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images. In Advances in Neural Information Processing Systems (NIPS), pages 2728\u20132736, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watter%2C%20Manuel%20Springenberg%2C%20Jost%20Tobias%20Boedecker%2C%20Joschka%20Riedmiller%2C%20Martin%20Embed%20to%20Control%3A%20A%20Locally%20Linear%20Latent%20Dynamics%20Model%20for%20Control%20from%20Raw%20Images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watter%2C%20Manuel%20Springenberg%2C%20Jost%20Tobias%20Boedecker%2C%20Joschka%20Riedmiller%2C%20Martin%20Embed%20to%20Control%3A%20A%20Locally%20Linear%20Latent%20Dynamics%20Model%20for%20Control%20from%20Raw%20Images%202015"
        },
        {
            "id": "49",
            "entry": "[49] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. arXiv preprint arXiv:1801.03924, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1801.03924"
        }
    ]
}
