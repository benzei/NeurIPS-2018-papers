{
    "filename": "7373-a-flexible-model-for-training-action-localization-with-varying-levels-of-supervision.pdf",
    "metadata": {
        "title": "A flexible model for training action localization with varying levels of supervision",
        "author": "Guilhem Ch\u00e9ron, Jean-Baptiste Alayrac, Ivan Laptev, Cordelia Schmid",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7373-a-flexible-model-for-training-action-localization-with-varying-levels-of-supervision.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Spatio-temporal action detection in videos is typically addressed in a fullysupervised setup with manual annotation of training videos required at every frame. Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less-demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos."
    },
    "keywords": [
        {
            "term": "bounding box",
            "url": "https://en.wikipedia.org/wiki/bounding_box"
        },
        {
            "term": "Multiple Instance Learning",
            "url": "https://en.wikipedia.org/wiki/Multiple_Instance_Learning"
        },
        {
            "term": "action detection",
            "url": "https://en.wikipedia.org/wiki/action_detection"
        },
        {
            "term": "mean average precision",
            "url": "https://en.wikipedia.org/wiki/mean_average_precision"
        }
    ],
    "highlights": [
        "Action localization aims to find spatial and temporal extents as well as classes of actions in the video, answering questions such as what are the performed actions? when do they happen? and where do they take place? This is a challenging task with many potential applications in surveillance, autonomous driving, video description and search",
        "We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels over temporal points or sparse action bounding boxes to the full per-frame annotation of action bounding boxes",
        "We evaluate our method on two datasets, UCF101-24 and DALY, for the following supervision levels: video level, temporal point, one bounding box, temporal bounds only, temporal bounds and one bounding box, temporal bounds and three bounding boxes",
        "We observe that this is due to two main issues: (i) the quality of images in the UCF101-24 dataset is quite low when compared to DALY which makes the human detection very challenging, and the bounding boxes have been annotated with a large margin around the person whereas a typical off-the-shelf detector produces tight detections",
        "This paper presents a weakly-supervised method for spatio-temporal action localization which aims to reduce the annotation cost associated with fully-supervised learning",
        "We propose a unifying framework that can handle and combine varying types of less-demanding weak supervision"
    ],
    "key_statements": [
        "Action localization aims to find spatial and temporal extents as well as classes of actions in the video, answering questions such as what are the performed actions? when do they happen? and where do they take place? This is a challenging task with many potential applications in surveillance, autonomous driving, video description and search",
        "Typically rely on exhaustive supervision where each frame of a training action has to be manually annotated with an action bounding box",
        "[<a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>] learns action localization from a sparse set of frames with annotated bounding boxes. [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] reduces bounding box annotation to a single spatial point specified for each frame of an action",
        "We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels over temporal points or sparse action bounding boxes to the full per-frame annotation of action bounding boxes",
        "Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods",
        "Given the difficulty of manual action annotation, we investigate how different types and amounts of supervision affect the performance of action localization",
        "We propose a model that can adopt and combine various levels of spatio-temporal action supervision such as (i) video-level only annotations, a single temporal point, a single bounding box, temporal bounds only, (v) temporal bounds with few bounding boxes and full supervision",
        "We introduce a single model that can be trained with various levels and amounts of supervision",
        "Spatial constraints force tracks that overlap with the bounding box to match the action class around this time step",
        "We report results with one and three bounding boxes per action instance",
        "Here an action instance has potentially several annotations, the spatial overlap with the track is defined as the minimum intersection over union between the corresponding bounding boxes",
        "We evaluate our method on two datasets, UCF101-24 and DALY, for the following supervision levels: video level, temporal point, one bounding box, temporal bounds only, temporal bounds and one bounding box, temporal bounds and three bounding boxes",
        "To the best of our knowledge, previously reported results are not available for all levels of supervision considered in this paper",
        "We compare results to two state-of-the-art methods [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>] that are designed to deal with weak supervision for action localization",
        "We observe that this is due to two main issues: (i) the quality of images in the UCF101-24 dataset is quite low when compared to DALY which makes the human detection very challenging, and the bounding boxes have been annotated with a large margin around the person whereas a typical off-the-shelf detector produces tight detections",
        "On the UCF101-24 dataset, we are always better compared to the state of the art when bounding box annotations are available, e.g., +17.2% in \u2018temp + 3 keys\u2019 setting",
        "This paper presents a weakly-supervised method for spatio-temporal action localization which aims to reduce the annotation cost associated with fully-supervised learning",
        "We propose a unifying framework that can handle and combine varying types of less-demanding weak supervision"
    ],
    "summary": [
        "Action localization aims to find spatial and temporal extents as well as classes of actions in the video, answering questions such as what are the performed actions? when do they happen? and where do they take place? This is a challenging task with many potential applications in surveillance, autonomous driving, video description and search.",
        "[<a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>] learns action localization from a sparse set of frames with annotated bounding boxes.",
        "Spatial constraints force tracks that overlap with the bounding box to match the action class around this time step.",
        "Supervised: annotation is defined by the bounding box at each frame of an action.",
        "Its temporal extent is provided while bounding boxes are spatially annotated for a sparse set of frames within action intervals.",
        "Given temporal boundaries for each action instance, we first add all tracklets that are outside of these ground truth intervals to Os in order to assign them to the background class.",
        "Here an action instance has potentially several annotations, the spatial overlap with the track is defined as the minimum IoU between the corresponding bounding boxes.",
        "We evaluate our method on two datasets, UCF101-24 and DALY, for the following supervision levels: video level, temporal point, one bounding box, temporal bounds only, temporal bounds and one bounding box, temporal bounds and three bounding boxes.",
        "For UCF101-24, we report temporal bounds and spatial points [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] and the fully supervised settings.",
        "We compare results to two state-of-the-art methods [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>] that are designed to deal with weak supervision for action localization.",
        "We observe that this is due to two main issues: (i) the quality of images in the UCF101-24 dataset is quite low when compared to DALY which makes the human detection very challenging, and the bounding boxes have been annotated with a large margin around the person whereas a typical off-the-shelf detector produces tight detections.",
        "On the UCF101-24 dataset, we are always better compared to the state of the art when bounding box annotations are available, e.g., +17.2% in \u2018temp + 3 keys\u2019 setting.",
        "We vary the portions of videos with stronger supervision available at the training time, and we evaluate mAP@0.2 on the test set.",
        "This paper presents a weakly-supervised method for spatio-temporal action localization which aims to reduce the annotation cost associated with fully-supervised learning.",
        "The key observations are that (i) dense spatial annotation is not always needed due to the recent advances of human detection and tracking, the performance of \u2018temporal point\u2019 supervision indicates that only annotating an action with a \u2018click\u2019 is very promising to decrease the annotation cost at a moderate performance drop, and mixing levels of supervision is a powerful approach for reducing annotation efforts."
    ],
    "headline": "In this work we propose a unifying framework that can handle and combine varying types of less-demanding weak supervision",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Ivan Laptev, Josef Sivic, and Simon Lacoste Julien. Unsupervised learning from narrated instruction videos. In CVPR, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alayrac%2C%20Jean-Baptiste%20Bojanowski%2C%20Piotr%20Agrawal%2C%20Nishant%20Laptev%2C%20Ivan%20Unsupervised%20learning%20from%20narrated%20instruction%20videos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alayrac%2C%20Jean-Baptiste%20Bojanowski%2C%20Piotr%20Agrawal%2C%20Nishant%20Laptev%2C%20Ivan%20Unsupervised%20learning%20from%20narrated%20instruction%20videos%202016"
        },
        {
            "id": "2",
            "entry": "[2] Pablo Arbel\u00e1ez, Jordi Pont-Tuset, Jonathan T. Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial grouping. In CVPR, 2014. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arbel%C3%A1ez%2C%20Pablo%20Pont-Tuset%2C%20Jordi%20Barron%2C%20Jonathan%20T.%20Marques%2C%20Ferran%20Multiscale%20combinatorial%20grouping%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arbel%C3%A1ez%2C%20Pablo%20Pont-Tuset%2C%20Jordi%20Barron%2C%20Jonathan%20T.%20Marques%2C%20Ferran%20Multiscale%20combinatorial%20grouping%202014"
        },
        {
            "id": "3",
            "entry": "[3] Francis Bach and Za\u00efd Harchaoui. DIFFRAC: A discriminative and flexible framework for clustering. In NIPS, 2007. 3, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Harchaoui%2C%20Za%C3%AFd%20DIFFRAC%3A%20A%20discriminative%20and%20flexible%20framework%20for%20clustering%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Harchaoui%2C%20Za%C3%AFd%20DIFFRAC%3A%20A%20discriminative%20and%20flexible%20framework%20for%20clustering%202007"
        },
        {
            "id": "4",
            "entry": "[4] Piotr Bojanowski, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid, and Josef Sivic. Finding actors and actions in movies. In ICCV, 2013. 2, 3, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20Piotr%20Bach%2C%20Francis%20Laptev%2C%20Ivan%20Ponce%2C%20Jean%20Finding%20actors%20and%20actions%20in%20movies%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojanowski%2C%20Piotr%20Bach%2C%20Francis%20Laptev%2C%20Ivan%20Ponce%2C%20Jean%20Finding%20actors%20and%20actions%20in%20movies%202013"
        },
        {
            "id": "5",
            "entry": "[5] Piotr Bojanowski, R\u00e9mi Lajugie, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid, and Josef Sivic. Weakly supervised action labeling in videos under ordering constraints. In ECCV, 2014. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20Piotr%20Lajugie%2C%20R%C3%A9mi%20Bach%2C%20Francis%20Laptev%2C%20Ivan%20Weakly%20supervised%20action%20labeling%20in%20videos%20under%20ordering%20constraints%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bojanowski%2C%20Piotr%20Lajugie%2C%20R%C3%A9mi%20Bach%2C%20Francis%20Laptev%2C%20Ivan%20Weakly%20supervised%20action%20labeling%20in%20videos%20under%20ordering%20constraints%202014"
        },
        {
            "id": "6",
            "entry": "[6] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the Kinetics dataset. In CVPR, 2017. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20Jo%C3%A3o%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20A%20new%20model%20and%20the%20Kinetics%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20Jo%C3%A3o%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20A%20new%20model%20and%20the%20Kinetics%20dataset%202017"
        },
        {
            "id": "7",
            "entry": "[7] Wei Chen and Jason J. Corso. Action Detection by implicit intentional Motion Clustering. In ICCV, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Wei%20Corso%2C%20Jason%20J.%20Action%20Detection%20by%20implicit%20intentional%20Motion%20Clustering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Wei%20Corso%2C%20Jason%20J.%20Action%20Detection%20by%20implicit%20intentional%20Motion%20Clustering%202015"
        },
        {
            "id": "8",
            "entry": "[8] Olivier Duchenne, Ivan Laptev, Josef Sivic, Francis Bach, and Jean Ponce. Automatic annotation of human actions in video. In CVPR, 2009. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchenne%2C%20Olivier%20Laptev%2C%20Ivan%20Sivic%2C%20Josef%20Bach%2C%20Francis%20Automatic%20annotation%20of%20human%20actions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchenne%2C%20Olivier%20Laptev%2C%20Ivan%20Sivic%2C%20Josef%20Bach%2C%20Francis%20Automatic%20annotation%20of%20human%20actions%202009"
        },
        {
            "id": "9",
            "entry": "[9] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. In Naval Research Logistics Quarterly, 1956. 3, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming"
        },
        {
            "id": "10",
            "entry": "[10] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. Detectron. https://github.com/facebookresearch/detectron, 2018.2, 5",
            "url": "https://github.com/facebookresearch/detectron"
        },
        {
            "id": "11",
            "entry": "[11] Georgia Gkioxari and Jitendra Malik. Finding action tubes. In CVPR, 2015. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gkioxari%2C%20Georgia%20Malik%2C%20Jitendra%20Finding%20action%20tubes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gkioxari%2C%20Georgia%20Malik%2C%20Jitendra%20Finding%20action%20tubes%202015"
        },
        {
            "id": "12",
            "entry": "[12] Alex Gorban, Haroon Idrees, Yu-Gang Jiang, Amir R. Roshan Zamir, Ivan Laptev, Mubarak Shah, and Rahul Sukthankar. THUMOS challenge: Action recognition with a large number of classes. http://www.thumos.info/, 2015.5",
            "url": "http://www.thumos.info/"
        },
        {
            "id": "13",
            "entry": "[13] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Caroline Pantofaru, David A Ross, George Toderici, Yeqing Li, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, et al. AVA: A video dataset of spatio-temporally localized atomic visual actions. In CVPR, 2018. 1, 2, 7, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Chunhui%20Sun%2C%20Chen%20Vijayanarasimhan%2C%20Sudheendra%20Pantofaru%2C%20Caroline%20AVA%3A%20A%20video%20dataset%20of%20spatio-temporally%20localized%20atomic%20visual%20actions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Chunhui%20Sun%2C%20Chen%20Vijayanarasimhan%2C%20Sudheendra%20Pantofaru%2C%20Caroline%20AVA%3A%20A%20video%20dataset%20of%20spatio-temporally%20localized%20atomic%20visual%20actions%202018"
        },
        {
            "id": "14",
            "entry": "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "15",
            "entry": "[15] Rui Hou, Chen Chen, and Mubarak Shah. Tube convolutional neural network (T-CNN) for action detection in videos. In ICCV, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hou%2C%20Rui%20Chen%2C%20Chen%20Shah%2C%20Mubarak%20Tube%20convolutional%20neural%20network%20%28T-CNN%29%20for%20action%20detection%20in%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hou%2C%20Rui%20Chen%2C%20Chen%20Shah%2C%20Mubarak%20Tube%20convolutional%20neural%20network%20%28T-CNN%29%20for%20action%20detection%20in%20videos%202017"
        },
        {
            "id": "16",
            "entry": "[16] De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Connectionist Temporal Modeling for Weakly Supervised Action Labeling. In ECCV, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20De-An%20Fei-Fei%2C%20Li%20Niebles%2C%20Juan%20Carlos%20Connectionist%20Temporal%20Modeling%20for%20Weakly%20Supervised%20Action%20Labeling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20De-An%20Fei-Fei%2C%20Li%20Niebles%2C%20Juan%20Carlos%20Connectionist%20Temporal%20Modeling%20for%20Weakly%20Supervised%20Action%20Labeling%202016"
        },
        {
            "id": "17",
            "entry": "[17] Armand Joulin, Francis Bach, and Jean Ponce. Discriminative clustering for image cosegmentation. In CVPR, 2010. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joulin%2C%20Armand%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Discriminative%20clustering%20for%20image%20cosegmentation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joulin%2C%20Armand%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Discriminative%20clustering%20for%20image%20cosegmentation%202010"
        },
        {
            "id": "18",
            "entry": "[18] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, and Cordelia Schmid. Action tubelet detector for spatio-temporal action localization. In ICCV, 2017. 1, 2, 5, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalogeiton%2C%20Vicky%20Weinzaepfel%2C%20Philippe%20Ferrari%2C%20Vittorio%20Schmid%2C%20Cordelia%20Action%20tubelet%20detector%20for%20spatio-temporal%20action%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalogeiton%2C%20Vicky%20Weinzaepfel%2C%20Philippe%20Ferrari%2C%20Vittorio%20Schmid%2C%20Cordelia%20Action%20tubelet%20detector%20for%20spatio-temporal%20action%20localization%202017"
        },
        {
            "id": "19",
            "entry": "[19] Will Kay, Jo\u00e3o Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. In CoRR, 2017. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Will%20Kay%20Jo%C3%A3o%20Carreira%20Karen%20Simonyan%20Brian%20Zhang%20Chloe%20Hillier%20Sudheendra%20Vijayanarasimhan%20Fabio%20Viola%20Tim%20Green%20Trevor%20Back%20Paul%20Natsev%20et%20al%20The%20kinetics%20human%20action%20video%20dataset%20In%20CoRR%202017%205",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Will%20Kay%20Jo%C3%A3o%20Carreira%20Karen%20Simonyan%20Brian%20Zhang%20Chloe%20Hillier%20Sudheendra%20Vijayanarasimhan%20Fabio%20Viola%20Tim%20Green%20Trevor%20Back%20Paul%20Natsev%20et%20al%20The%20kinetics%20human%20action%20video%20dataset%20In%20CoRR%202017%205"
        },
        {
            "id": "20",
            "entry": "[20] Yan Ke, Rahul Sukthankar, and Martial Hebert. Efficient visual event detection using volumetric features. In ICCV, 2005. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ke%2C%20Yan%20Sukthankar%2C%20Rahul%20Hebert%2C%20Martial%20Efficient%20visual%20event%20detection%20using%20volumetric%20features%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ke%2C%20Yan%20Sukthankar%2C%20Rahul%20Hebert%2C%20Martial%20Efficient%20visual%20event%20detection%20using%20volumetric%20features%202005"
        },
        {
            "id": "21",
            "entry": "[21] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe optimization for structural SVMs. In ICML, 2013. 3, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste-Julien%2C%20S.%20Jaggi%2C%20M.%20Schmidt%2C%20M.%20Pletscher%2C%20P.%20Block-coordinate%20Frank-Wolfe%20optimization%20for%20structural%20SVMs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lacoste-Julien%2C%20S.%20Jaggi%2C%20M.%20Schmidt%2C%20M.%20Pletscher%2C%20P.%20Block-coordinate%20Frank-Wolfe%20optimization%20for%20structural%20SVMs%202013"
        },
        {
            "id": "22",
            "entry": "[22] Ivan Laptev and Patrick P\u00e9rez. Retrieving actions in movies. In ICCV, 2007. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laptev%2C%20Ivan%20P%C3%A9rez%2C%20Patrick%20Retrieving%20actions%20in%20movies%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laptev%2C%20Ivan%20P%C3%A9rez%2C%20Patrick%20Retrieving%20actions%20in%20movies%202007"
        },
        {
            "id": "23",
            "entry": "[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Doll%C3%A1r%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%202014%205",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Doll%C3%A1r%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%202014%205"
        },
        {
            "id": "24",
            "entry": "[24] Bruce D Lucas, Takeo Kanade, et al. An iterative image registration technique with an application to stereo vision. In IJCAI, 1981. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucas%2C%20Bruce%20D.%20Kanade%2C%20Takeo%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucas%2C%20Bruce%20D.%20Kanade%2C%20Takeo%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981"
        },
        {
            "id": "25",
            "entry": "[25] Johan Mathe. Shotdetect. https://github.com/johmathe/Shotdetect, 2012.6",
            "url": "https://github.com/johmathe/Shotdetect"
        },
        {
            "id": "26",
            "entry": "[26] Pascal Mettes, Cees G. M. Snoek, and Shih-Fu Chang. Localizing actions from video labels and pseudo-annotations. In BMVC, 2017. 2, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mettes%2C%20Pascal%20Snoek%2C%20Cees%20G.M.%20Chang%2C%20Shih-Fu%20Localizing%20actions%20from%20video%20labels%20and%20pseudo-annotations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mettes%2C%20Pascal%20Snoek%2C%20Cees%20G.M.%20Chang%2C%20Shih-Fu%20Localizing%20actions%20from%20video%20labels%20and%20pseudo-annotations%202017"
        },
        {
            "id": "27",
            "entry": "[27] Pascal Mettes, Jan C. van Gemert, and Cees G. M. Snoek. Spot on: action localization from pointly-supervised proposals. In ECCV, 2016. 1, 3, 7, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mettes%2C%20Pascal%20van%20Gemert%2C%20Jan%20C.%20Snoek%2C%20Cees%20G.M.%20Spot%20on%3A%20action%20localization%20from%20pointly-supervised%20proposals%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mettes%2C%20Pascal%20van%20Gemert%2C%20Jan%20C.%20Snoek%2C%20Cees%20G.M.%20Spot%20on%3A%20action%20localization%20from%20pointly-supervised%20proposals%202016"
        },
        {
            "id": "28",
            "entry": "[28] Antoine Miech, Jean-Baptiste Alayrac, Piotr Bojanowski, Ivan Laptev, and Josef Sivic. Learning from video and text via large-scale discriminative clustering. In ICCV, 2017. 3, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miech%2C%20Antoine%20Alayrac%2C%20Jean-Baptiste%20Bojanowski%2C%20Piotr%20Laptev%2C%20Ivan%20Learning%20from%20video%20and%20text%20via%20large-scale%20discriminative%20clustering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miech%2C%20Antoine%20Alayrac%2C%20Jean-Baptiste%20Bojanowski%2C%20Piotr%20Laptev%2C%20Ivan%20Learning%20from%20video%20and%20text%20via%20large-scale%20discriminative%20clustering%202017"
        },
        {
            "id": "29",
            "entry": "[29] Dan Oneata, J\u00e9r\u00f4me Revaud, Jakob Verbeek, and Cordelia Schmid. Spatio-temporal object detection proposals. In ECCV, 2014. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oneata%2C%20Dan%20Revaud%2C%20J%C3%A9r%C3%B4me%20Verbeek%2C%20Jakob%20Schmid%2C%20Cordelia%20Spatio-temporal%20object%20detection%20proposals%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oneata%2C%20Dan%20Revaud%2C%20J%C3%A9r%C3%B4me%20Verbeek%2C%20Jakob%20Schmid%2C%20Cordelia%20Spatio-temporal%20object%20detection%20proposals%202014"
        },
        {
            "id": "30",
            "entry": "[30] Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet K. Dokania, and Simon Lacoste-Julien. Minding the gaps for block Frank-Wolfe optimization of structured SVMs. In ICML, 2016. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20Anton%20Alayrac%2C%20Jean-Baptiste%20Lukasewitz%2C%20Isabella%20Dokania%2C%20Puneet%20K.%20Minding%20the%20gaps%20for%20block%20Frank-Wolfe%20optimization%20of%20structured%20SVMs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20Anton%20Alayrac%2C%20Jean-Baptiste%20Lukasewitz%2C%20Isabella%20Dokania%2C%20Puneet%20K.%20Minding%20the%20gaps%20for%20block%20Frank-Wolfe%20optimization%20of%20structured%20SVMs%202016"
        },
        {
            "id": "31",
            "entry": "[31] Xiaojiang Peng and Cordelia Schmid. Multi-region two-stream R-CNN for action detection. In ECCV, 2016. 1, 2, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Xiaojiang%20Schmid%2C%20Cordelia%20Multi-region%20two-stream%20R-CNN%20for%20action%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Xiaojiang%20Schmid%2C%20Cordelia%20Multi-region%20two-stream%20R-CNN%20for%20action%20detection%202016"
        },
        {
            "id": "32",
            "entry": "[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015"
        },
        {
            "id": "33",
            "entry": "[33] Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly supervised action learning with rnn based fine-to-coarse modeling. In CVPR, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richard%2C%20Alexander%20Kuehne%2C%20Hilde%20Gall%2C%20Juergen%20Weakly%20supervised%20action%20learning%20with%20rnn%20based%20fine-to-coarse%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richard%2C%20Alexander%20Kuehne%2C%20Hilde%20Gall%2C%20Juergen%20Weakly%20supervised%20action%20learning%20with%20rnn%20based%20fine-to-coarse%20modeling%202017"
        },
        {
            "id": "34",
            "entry": "[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. In IJCV, 2015. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "35",
            "entry": "[35] Suman Saha, Gurkirt Singh, and Fabio Cuzzolin. Amtnet: Action-micro-tube regression by end-to-end trainable deep architecture. In ICCV, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saha%2C%20Suman%20Singh%2C%20Gurkirt%20Cuzzolin%2C%20Fabio%20Amtnet%3A%20Action-micro-tube%20regression%20by%20end-to-end%20trainable%20deep%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saha%2C%20Suman%20Singh%2C%20Gurkirt%20Cuzzolin%2C%20Fabio%20Amtnet%3A%20Action-micro-tube%20regression%20by%20end-to-end%20trainable%20deep%20architecture%202017"
        },
        {
            "id": "36",
            "entry": "[36] Suman Saha, Gurkirt Singh, Michael Sapienza, Philip HS Torr, and Fabio Cuzzolin. Deep learning for detecting multiple space-time action tubes in videos. In BMVC, 2016. 1, 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saha%2C%20Suman%20Singh%2C%20Gurkirt%20Sapienza%2C%20Michael%20Torr%2C%20Philip%20H.S.%20Deep%20learning%20for%20detecting%20multiple%20space-time%20action%20tubes%20in%20videos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saha%2C%20Suman%20Singh%2C%20Gurkirt%20Sapienza%2C%20Michael%20Torr%2C%20Philip%20H.S.%20Deep%20learning%20for%20detecting%20multiple%20space-time%20action%20tubes%20in%20videos%202016"
        },
        {
            "id": "37",
            "entry": "[37] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip Torr, and Fabio Cuzzolin. Online real time multiple spatiotemporal action localisation and prediction. In ICCV, 2017. 1, 2, 5, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Gurkirt%20Saha%2C%20Suman%20Sapienza%2C%20Michael%20Torr%2C%20Philip%20Online%20real%20time%20multiple%20spatiotemporal%20action%20localisation%20and%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Gurkirt%20Saha%2C%20Suman%20Sapienza%2C%20Michael%20Torr%2C%20Philip%20Online%20real%20time%20multiple%20spatiotemporal%20action%20localisation%20and%20prediction%202017"
        },
        {
            "id": "38",
            "entry": "[38] Krishna Kumar Singh and Yong Jae Lee. Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization. In ICCV, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Krishna%20Kumar%20Lee%2C%20Yong%20Jae%20Hide-and-Seek%3A%20Forcing%20a%20Network%20to%20be%20Meticulous%20for%20Weakly-supervised%20Object%20and%20Action%20Localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Krishna%20Kumar%20Lee%2C%20Yong%20Jae%20Hide-and-Seek%3A%20Forcing%20a%20Network%20to%20be%20Meticulous%20for%20Weakly-supervised%20Object%20and%20Action%20Localization%202017"
        },
        {
            "id": "39",
            "entry": "[39] Parthipan Siva and Tao Xiang. Weakly Supervised Action Detection. In BMVC, 2011. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Siva%2C%20Parthipan%20Xiang%2C%20Tao%20Weakly%20Supervised%20Action%20Detection%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Siva%2C%20Parthipan%20Xiang%2C%20Tao%20Weakly%20Supervised%20Action%20Detection%202011"
        },
        {
            "id": "40",
            "entry": "[40] Khurram Soomro and Mubarak Shah. Unsupervised action discovery and localization in videos. In ICCV, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soomro%2C%20Khurram%20Shah%2C%20Mubarak%20Unsupervised%20action%20discovery%20and%20localization%20in%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soomro%2C%20Khurram%20Shah%2C%20Mubarak%20Unsupervised%20action%20discovery%20and%20localization%20in%20videos%202017"
        },
        {
            "id": "41",
            "entry": "[41] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. In CoRR, 2012. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soomro%2C%20Khurram%20Zamir%2C%20Amir%20Roshan%20Shah%2C%20Mubarak%20UCF101%3A%20A%20dataset%20of%20101%20human%20actions%20classes%20from%20videos%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soomro%2C%20Khurram%20Zamir%2C%20Amir%20Roshan%20Shah%2C%20Mubarak%20UCF101%3A%20A%20dataset%20of%20101%20human%20actions%20classes%20from%20videos%202012"
        },
        {
            "id": "42",
            "entry": "[42] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. In IJCV, 2013. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uijlings%2C%20Jasper%20R.R.%20Sande%2C%20Koen%20E.A.Van%20De%20Gevers%2C%20Theo%20Smeulders%2C%20Arnold%20W.M.%20Selective%20search%20for%20object%20recognition.%20In%20IJCV%202013"
        },
        {
            "id": "43",
            "entry": "[43] Jan C. van Gemert, Mihir Jain, Elia Gati, and Cees G. M. Snoek. APT: Action localization proposals from dense trajectories. In BMVC, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Gemert%2C%20Jan%20C.%20Jain%2C%20Mihir%20Gati%2C%20Elia%20Snoek%2C%20Cees%20G.M.%20APT%3A%20Action%20localization%20proposals%20from%20dense%20trajectories%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Gemert%2C%20Jan%20C.%20Jain%2C%20Mihir%20Gati%2C%20Elia%20Snoek%2C%20Cees%20G.M.%20APT%3A%20Action%20localization%20proposals%20from%20dense%20trajectories%202015"
        },
        {
            "id": "44",
            "entry": "[44] Limin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool. UntrimmedNets for Weakly Supervised Action Recognition and Detection. In CVPR, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Lin%2C%20Dahua%20Gool%2C%20Luc%20Van%20UntrimmedNets%20for%20Weakly%20Supervised%20Action%20Recognition%20and%20Detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Lin%2C%20Dahua%20Gool%2C%20Luc%20Van%20UntrimmedNets%20for%20Weakly%20Supervised%20Action%20Recognition%20and%20Detection%202017"
        },
        {
            "id": "45",
            "entry": "[45] Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. Learning to track for spatiotemporal action localization. In ICCV, 2015. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinzaepfel%2C%20Philippe%20Harchaoui%2C%20Zaid%20Schmid%2C%20Cordelia%20Learning%20to%20track%20for%20spatiotemporal%20action%20localization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinzaepfel%2C%20Philippe%20Harchaoui%2C%20Zaid%20Schmid%2C%20Cordelia%20Learning%20to%20track%20for%20spatiotemporal%20action%20localization%202015"
        },
        {
            "id": "46",
            "entry": "[46] Philippe Weinzaepfel, Xavier Martin, and Cordelia Schmid. Human action localization with sparse spatial supervision. In CoRR, 2016. 1, 3, 5, 6, 7, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinzaepfel%2C%20Philippe%20Martin%2C%20Xavier%20Schmid%2C%20Cordelia%20Human%20action%20localization%20with%20sparse%20spatial%20supervision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinzaepfel%2C%20Philippe%20Martin%2C%20Xavier%20Schmid%2C%20Cordelia%20Human%20action%20localization%20with%20sparse%20spatial%20supervision%202016"
        },
        {
            "id": "47",
            "entry": "[47] Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maximum margin clustering. In NIPS, 2004. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Linli%20Neufeld%2C%20James%20Larson%2C%20Bryce%20Schuurmans%2C%20Dale%20Maximum%20margin%20clustering%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Linli%20Neufeld%2C%20James%20Larson%2C%20Bryce%20Schuurmans%2C%20Dale%20Maximum%20margin%20clustering%202004"
        },
        {
            "id": "48",
            "entry": "[48] Jiong Yang and Junsong Yuan. Common action discovery and localization in unconstrained videos. In ICCV, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Jiong%20Yuan%2C%20Junsong%20Common%20action%20discovery%20and%20localization%20in%20unconstrained%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Jiong%20Yuan%2C%20Junsong%20Common%20action%20discovery%20and%20localization%20in%20unconstrained%20videos%202017"
        },
        {
            "id": "49",
            "entry": "[49] Mohammadreza Zolfaghari, Gabriel L Oliveira, Nima Sedaghat, and Thomas Brox. Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection. In ICCV, 2017. 2 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zolfaghari%2C%20Mohammadreza%20Oliveira%2C%20Gabriel%20L.%20Sedaghat%2C%20Nima%20Brox%2C%20Thomas%20Chained%20multi-stream%20networks%20exploiting%20pose%2C%20motion%2C%20and%20appearance%20for%20action%20classification%20and%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zolfaghari%2C%20Mohammadreza%20Oliveira%2C%20Gabriel%20L.%20Sedaghat%2C%20Nima%20Brox%2C%20Thomas%20Chained%20multi-stream%20networks%20exploiting%20pose%2C%20motion%2C%20and%20appearance%20for%20action%20classification%20and%20detection%202017"
        }
    ]
}
