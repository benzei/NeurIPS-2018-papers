{
    "filename": "7392-how-to-make-the-gradients-small-stochastically-even-faster-convex-and-nonconvex-sgd.pdf",
    "metadata": {
        "title": "How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD",
        "author": "Zeyuan Allen-Zhu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7392-how-to-make-the-gradients-small-stochastically-even-faster-convex-and-nonconvex-sgd.pdf"
        },
        "abstract": "Stochastic gradient descent (SGD) gives an optimal convergence rate when minimizing convex stochastic objectives f (x). However, in terms of making the gradients small, the original SGD does not give an optimal rate, even when f (x) is convex."
    },
    "keywords": [
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "first order",
            "url": "https://en.wikipedia.org/wiki/first_order"
        },
        {
            "term": "convex function",
            "url": "https://en.wikipedia.org/wiki/convex_function"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "positive semidefinite",
            "url": "https://en.wikipedia.org/wiki/positive_semidefinite"
        },
        {
            "term": "stochastic programming",
            "url": "https://en.wikipedia.org/wiki/stochastic_programming"
        },
        {
            "term": "online learning",
            "url": "https://en.wikipedia.org/wiki/online_learning"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "In convex optimization and machine learning, the classical goal is to design algorithms to decrease objective values, that is, to find points x with f (x) \u2212 f (x\u2217) \u2264 \u03b5",
        "In the fullgradient setting, accelerated gradient descent alone is suboptimal for this new goal, and one needs additional tricks to get the fastest rate [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "V \u03c3\u03b4 if f (x) is \u03c3-strongly convex. Both rates are asymptotically optimal in terms of decreasing objective, and V is an absolute bound on the variance of the stochastic gradients",
        "The stochastic gradient descent (SGD) method and all of its variants studied in this paper are online",
        "We summarize this algorithm as SGDsc in Algorithm 2"
    ],
    "key_statements": [
        "In convex optimization and machine learning, the classical goal is to design algorithms to decrease objective values, that is, to find points x with f (x) \u2212 f (x\u2217) \u2264 \u03b5",
        "In the fullgradient setting, accelerated gradient descent alone is suboptimal for this new goal, and one needs additional tricks to get the fastest rate [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "V \u03c3\u03b4 if f (x) is \u03c3-strongly convex. Both rates are asymptotically optimal in terms of decreasing objective, and V is an absolute bound on the variance of the stochastic gradients",
        "The stochastic gradient descent (SGD) method and all of its variants studied in this paper are online",
        "We summarize this algorithm as SGDsc in Algorithm 2"
    ],
    "summary": [
        "In convex optimization and machine learning, the classical goal is to design algorithms to decrease objective values, that is, to find points x with f (x) \u2212 f (x\u2217) \u2264 \u03b5.",
        "If f (x) is convex, to find a point with gradient norm \u03b5, we design an algorithm SGD3 with a near-optimal rate O(\u03b5\u22122), improving the best known rate O(\u03b5\u22128/3) of [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "In the convex stochastic optimization, to the best of our knowledge, tight bounds are not yet known for finding points with small gradients.",
        "Non-convex optimization theory is always in terms of finding points with small gradients.",
        "Consider the stochastic setting where the convex objective f (x) := Ei[fi(x)] and the algorithm can only compute stochastic gradients \u2207fi(x) at any point x for a random i.",
        "It is well-known that stochastic gradient descent (SGD) finds a point x with f (x) \u2212 f (x\u2217) \u2264 \u03b4 in",
        "Both rates are asymptotically optimal in terms of decreasing objective, and V is an absolute bound on the variance of the stochastic gradients.",
        "2 as a regularized version of f (x), and applied the strongly-convex version of AGD to minimize g(x).",
        "SGD2 finds x with \u2207f (x) \u2264 \u03b5 in",
        "SGD3 finds x with \u2207f (x) \u2264 \u03b5 in log3 (L/\u03b5)\u00b7V \u03b52 iterations or log3 (L/\u03c3 )\u00b7V \u03b52 if f (x) is \u03c3-strongly convex.",
        "Our new rates in Theorem 3 not only improve the best known result of T \u221d \u03b5\u22128/3, but are near optimal because \u03a9(V/\u03b52) is clearly a lower bound: even to decide whether a point x has",
        "We apply our techniques to non-convex optimization and give algorithms SGD4 and SGD5 in Section 7.",
        "For composite function F (x) = \u03c8(x) + f (x) where \u03c8(x) is proper convex, given a parameter \u03b7 > 0, the gradient mapping of F (\u00b7) at point x is",
        "The stochastic gradient descent (SGD) method and all of its variants studied in this paper are online.",
        "If f (x) is known to be strongly convex, to get the tightest convergence rate, one can repeatedly apply SGD with decreasing learning rate \u03b1 [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>].",
        "The following theorem describes the rates of convergence in objective values for SGD and SGDsc respectively.",
        "To turn Theorem 4.1 into a rate of convergence for the gradients, we can apply Lemma 2.3 which implies",
        "If f (x) is not strongly parameter \u03c3 > 0, and convex, we regularize it by G(x) apply SGD3sc."
    ],
    "headline": "We review these tricks in Section 1.1",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Open problem session of \u201cfast iterative methods in optimization\u201d workshop. Simons Institute for the Theory of Computing, UC Berkeley, October 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Open%20problem%20session%20of%20%E2%80%9Cfast%20iterative%20methods%20in%20optimization%E2%80%9D%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Open%20problem%20session%20of%20%E2%80%9Cfast%20iterative%20methods%20in%20optimization%E2%80%9D%202017-10"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20First%20Direct%20Acceleration%20of%20Stochastic%20Gradient%20Methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20First%20Direct%20Acceleration%20of%20Stochastic%20Gradient%20Methods%202017"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Natasha%202%3A%20Faster%20Non-Convex%20Optimization%20Than%20SGD%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Natasha%202%3A%20Faster%20Non-Convex%20Optimization%20Than%20SGD%202018"
        },
        {
            "id": "4",
            "entry": "[4] Zeyuan Allen-Zhu and Elad Hazan. Optimal Black-Box Reductions Between Optimization Objectives. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Optimal%20Black-Box%20Reductions%20Between%20Optimization%20Objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Optimal%20Black-Box%20Reductions%20Between%20Optimization%20Objectives%202016"
        },
        {
            "id": "5",
            "entry": "[5] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Follow%20the%20Compressed%20Leader%3A%20Faster%20Online%20Learning%20of%20Eigenvectors%20and%20Faster%20MMWU%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Follow%20the%20Compressed%20Leader%3A%20Faster%20Online%20Learning%20of%20Eigenvectors%20and%20Faster%20MMWU%202017"
        },
        {
            "id": "6",
            "entry": "[6] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding Local Minima via First-Order Oracles. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Neon2%3A%20Finding%20Local%20Minima%20via%20First-Order%20Oracles%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Neon2%3A%20Finding%20Local%20Minima%20via%20First-Order%20Oracles%202018"
        },
        {
            "id": "7",
            "entry": "[7] Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Much Faster Algorithms for Matrix Scaling. In FOCS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Oliveira%2C%20Rafael%20Wigderson%2C%20Avi%20Much%20Faster%20Algorithms%20for%20Matrix%20Scaling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Oliveira%2C%20Rafael%20Wigderson%2C%20Avi%20Much%20Faster%20Algorithms%20for%20Matrix%20Scaling%202017"
        },
        {
            "id": "8",
            "entry": "[8] Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231\u2013357, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20Sebastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20Sebastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015"
        },
        {
            "id": "9",
            "entry": "[9] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated Methods for Non-Convex Optimization. ArXiv e-prints, abs/1611.00756, November 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "10",
            "entry": "[10] M. B. Cohen, A. Madry, D. Tsipras, and A. Vladu. Matrix Scaling and Balancing via Box Constrained Newton\u2019s Method and Interior Point Methods. In FOCS, pages 902\u2013913, Oct 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20M.B.%20Madry%2C%20A.%20Tsipras%2C%20D.%20Vladu%2C%20A.%20Matrix%20Scaling%20and%20Balancing%20via%20Box%20Constrained%20Newton%E2%80%99s%20Method%20and%20Interior%20Point%20Methods%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20M.B.%20Madry%2C%20A.%20Tsipras%2C%20D.%20Vladu%2C%20A.%20Matrix%20Scaling%20and%20Balancing%20via%20Box%20Constrained%20Newton%E2%80%99s%20Method%20and%20Interior%20Point%20Methods%202017-10"
        },
        {
            "id": "11",
            "entry": "[11] Damek Davis and Dmitriy Drusvyatskiy. Complexity of finding near-stationary points of convex functions stochastically. ArXiv e-prints, abs/1802.08556, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08556"
        },
        {
            "id": "12",
            "entry": "[12] Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate o(k\u22121/4) on weakly convex functions. ArXiv e-prints, abs/1802.02988, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02988"
        },
        {
            "id": "13",
            "entry": "[13] John Duchi and Yoram Singer. Efficient Online and Batch Learning Using Forward Backward Splitting. Journal of Machine Learning Research, 10:2899\u20132934, 2009. ISSN 15324435. doi: 10.1561/2400000003.",
            "crossref": "https://dx.doi.org/10.1561/2400000003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1561/2400000003"
        },
        {
            "id": "14",
            "entry": "[14] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, COLT 2015, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition"
        },
        {
            "id": "15",
            "entry": "[15] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization I: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469\u20131492, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Optimal%20stochastic%20approximation%20algorithms%20for%20strongly%20convex%20stochastic%20composite%20optimization%20I%3A%20A%20generic%20algorithmic%20framework%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Optimal%20stochastic%20approximation%20algorithms%20for%20strongly%20convex%20stochastic%20composite%20optimization%20I%3A%20A%20generic%20algorithmic%20framework%202012"
        },
        {
            "id": "16",
            "entry": "[16] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "17",
            "entry": "[17] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, pages 1\u201326, feb 2015. ISSN 00255610.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202015-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202015-02"
        },
        {
            "id": "18",
            "entry": "[18] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157\u2013325, 2016. ISSN 2167-3888. doi: 10.1561/2400000013.",
            "crossref": "https://dx.doi.org/10.1561/2400000013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1561/2400000013"
        },
        {
            "id": "19",
            "entry": "[19] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1): 2489\u20132512, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Beyond%20the%20regret%20minimization%20barrier%3A%20Optimal%20algorithms%20for%20stochastic%20strongly-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Beyond%20the%20regret%20minimization%20barrier%3A%20Optimal%20algorithms%20for%20stochastic%20strongly-convex%20optimization%202014"
        },
        {
            "id": "20",
            "entry": "[20] Martin Idel. A review of matrix scaling and sinkhorn\u2019s normal form for matrices and positive maps. ArXiv e-prints, abs/1609.06349, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.06349"
        },
        {
            "id": "21",
            "entry": "[21] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Nonconvex Finite-Sum Optimization Via SCSG Methods. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Nonconvex%20Finite-Sum%20Optimization%20Via%20SCSG%20Methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Nonconvex%20Finite-Sum%20Optimization%20Via%20SCSG%20Methods%202017"
        },
        {
            "id": "22",
            "entry": "[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In Doklady AN SSSR (translated as Soviet Mathematics Doklady), volume 269, pages 543\u2013547, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983"
        },
        {
            "id": "23",
            "entry": "[23] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course, volume I. Kluwer Academic Publishers, 2004. ISBN 1402075537.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20on%20Convex%20Programming%20Volume%3A%20A%20Basic%20course%2C%20volume%20I%202004"
        },
        {
            "id": "24",
            "entry": "[24] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127\u2013152, December 2005. ISSN 0025-5610. doi: 10.1007/s10107-004-0552-5.",
            "crossref": "https://dx.doi.org/10.1007/s10107-004-0552-5",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10107-004-0552-5"
        },
        {
            "id": "25",
            "entry": "[25] Yurii Nesterov. How to make the gradients small. Optima, 88:10\u201311, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20How%20to%20make%20the%20gradients%20small%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20How%20to%20make%20the%20gradients%20small%202012"
        },
        {
            "id": "26",
            "entry": "[26] Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and Trends in Machine Learning, 4(2):107\u2013194, 2012. ISSN 1935-8237.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Online%20Learning%20and%20Online%20Convex%20Optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Online%20Learning%20and%20Online%20Convex%20Optimization%202012"
        },
        {
            "id": "27",
            "entry": "[27] Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic Cubic Regularization for Fast Nonconvex Optimization. ArXiv e-prints, abs/1711.02838, November 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02838"
        },
        {
            "id": "28",
            "entry": "[28] Blake Woodworth and Nati Srebro. Tight Complexity Bounds for Optimizing Composite Objectives. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodworth%2C%20Blake%20Srebro%2C%20Nati%20Tight%20Complexity%20Bounds%20for%20Optimizing%20Composite%20Objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodworth%2C%20Blake%20Srebro%2C%20Nati%20Tight%20Complexity%20Bounds%20for%20Optimizing%20Composite%20Objectives%202016"
        },
        {
            "id": "29",
            "entry": "[29] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance Reduction. SIAM Journal on Optimization, 24(4):2057\u2014-2075, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20Proximal%20Stochastic%20Gradient%20Method%20with%20Progressive%20Variance%20Reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20Proximal%20Stochastic%20Gradient%20Method%20with%20Progressive%20Variance%20Reduction%202014"
        },
        {
            "id": "30",
            "entry": "[30] Yi Xu and Tianbao Yang. First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. ArXiv e-prints, abs/1711.01944, November 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1711.01944"
        }
    ]
}
