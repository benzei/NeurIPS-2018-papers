{
    "filename": "7907-policy-regret-in-repeated-games.pdf",
    "metadata": {
        "title": "Policy Regret in Repeated Games",
        "author": "Raman Arora, Michael Dinitz, Teodor Vanislavov Marinov, Mehryar Mohri",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7907-policy-regret-in-repeated-games.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The notion of policy regret in online learning is a well defined performance measure for the common scenario of adaptive adversaries, which more traditional quantities such as external regret do not take into account. We revisit the notion of policy regret and first show that there are online learning settings in which policy regret and external regret are incompatible: any sequence of play that achieves a favorable regret with respect to one definition must do poorly with respect to the other. We then focus on the game-theoretic setting where the adversary is a self-interested agent. In that setting, we show that external regret and policy regret are not in conflict and, in fact, that a wide class of algorithms can ensure a favorable regret with respect to both definitions, so long as the adversary is also using such an algorithm. We also show that the sequence of play of no-policy regret algorithms converges to a policy equilibrium, a new notion of equilibrium that we introduce. Relating this back to external regret, we show that coarse correlated equilibria, which no-external regret players converge to, are a strict subset of policy equilibria. Thus, in game-theoretic settings, every sequence of play with no external regret also admits no policy regret, but the converse does not hold."
    },
    "keywords": [
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "equilibrium",
            "url": "https://en.wikipedia.org/wiki/equilibrium"
        },
        {
            "term": "equilibria",
            "url": "https://en.wikipedia.org/wiki/equilibria"
        },
        {
            "term": "online learning",
            "url": "https://en.wikipedia.org/wiki/online_learning"
        }
    ],
    "highlights": [
        "Learning in dynamically evolving environments can be described as a repeated game between a player, an online learning algorithm, and an adversary",
        "It is known that no external regret play converges to a coarse correlated equilibrium (CCE) in such a game, but what happens when players are using no policy regret algorithms? We show in Theorem 4.8 that the average play in repeated games between no policy regret players converges to a policy equilibrium, a new notion of equilibrium that we introduce",
        "We show in Theorems 4.9 and 4.10 that the set of coarse correlated equilibrium is a strict subset of policy equilibria",
        "We show that for any coarse correlated equilibrium we can construct stable no-external regret algorithm which converge to it, and so since stable no-external regret algorithms always converge to policy equilibria (Theorem 3.4), this implies the coarse correlated equilibrium is a policy equilibrium",
        "We presented a new twist on policy regret by examining it in the game setting, where we introduced the notion of policy equilibrium and showed that it captures the behavior of no policy regret players",
        "Even with our current definitions, since we have a broader class of equilibria to consider, it is natural to return to the extensive literature in algorithmic game theory on the price of anarchy and price of stability, and reconsider it in the context of policy equilibria"
    ],
    "key_statements": [
        "Learning in dynamically evolving environments can be described as a repeated game between a player, an online learning algorithm, and an adversary",
        "It is known that no external regret play converges to a coarse correlated equilibrium (CCE) in such a game, but what happens when players are using no policy regret algorithms? We show in Theorem 4.8 that the average play in repeated games between no policy regret players converges to a policy equilibrium, a new notion of equilibrium that we introduce",
        "We show in Theorems 4.9 and 4.10 that the set of coarse correlated equilibrium is a strict subset of policy equilibria",
        "While the motivation for policy regret suggests that this should be a stronger notion compared to external regret, we show that not only that these notions are incomparable in the general adversarial setting, but that they are incompatible in a strong sense",
        "Equipped with the knowledge that no policy regret sequences are obtainable in the game setting under reasonable play from all parties, it is natural to reason how other players would react if player i deviated and what would be the cost of deviation when taking into account possible reactions",
        "We show that for any coarse correlated equilibrium we can construct stable no-external regret algorithm which converge to it, and so since stable no-external regret algorithms always converge to policy equilibria (Theorem 3.4), this implies the coarse correlated equilibrium is a policy equilibrium",
        "We presented a new twist on policy regret by examining it in the game setting, where we introduced the notion of policy equilibrium and showed that it captures the behavior of no policy regret players",
        "Even with our current definitions, since we have a broader class of equilibria to consider, it is natural to return to the extensive literature in algorithmic game theory on the price of anarchy and price of stability, and reconsider it in the context of policy equilibria",
        "For example <a class=\"ref-link\" id=\"cRoughgarden_2015_a\" href=\"#rRoughgarden_2015_a\">Roughgarden (2015</a>) showed that in smooth games the worst coarse correlated equilibrium is no worse than the worst Nash"
    ],
    "summary": [
        "Learning in dynamically evolving environments can be described as a repeated game between a player, an online learning algorithm, and an adversary.",
        "We consider a two-player game where each player is playing a no policy regret algorithm.",
        "It is known that no external regret play converges to a coarse correlated equilibrium (CCE) in such a game, but what happens when players are using no policy regret algorithms?",
        "We adapt a classical notion of stability from the statistical machine learning setting and argue that if the players use no external regret algorithms that are stable, the players will have no policy regret in expectation.",
        "Equipped with the knowledge that no policy regret sequences are obtainable in the game setting under reasonable play from all parties, it is natural to reason how other players would react if player i deviated and what would be the cost of deviation when taking into account possible reactions.",
        "The player is familiar with the guarantees of the algorithm and knows that if, instead, they changed to playing any fixed action a \u2208 A1, the resulting empirical distribution of playa, where player 2 has responded in a memory-bounded way, is such that E(a,b)\u223c\u0302 [u1(a, b)] \u2265 E(a,b)\u223c\u0302a [u1(a, b)] \u2212 \u270f.",
        "This suggests that the equilibrium induced by a no-policy regret play, is a distribution over the functional space defined below.",
        "Since the no policy regret algorithms we work with do not directly induce distributions over the functional space F but rather only distributions over the action space A, we would like to state all of our utility inequalities in terms of distributions over A.",
        "\u21e1 is a policy equilibrium if there exists a stationary distribution of the Markov chain corresponding to \u21e1, such that, when actions are drawn according to , no player has incentive to change their action.",
        "Since the definition of sublinear policy regret does not include a distribution over functional spaces but only works with empirical distributions of play, we would like to present our result in terms of distributions over the action space A.",
        "If the algorithms played by player 1 in the form of ft and player 2 in the form of gt give sub-linear policy regret sequences, the sequence of product distributions (\u0302 \u00d7 \u0302a \u00d7 \u0302b)\u221e T =1 converges weakly to the set S.",
        "For any CCE of a 2-player game G, there exists a policy-equilibrium \u21e1 which induces a Markov chain M with stationary distribution ."
    ],
    "headline": "We focus on the game-theoretic setting where the adversary is a self-interested agent",
    "reference_links": [
        {
            "id": "Allen-Zhu_2016_a",
            "entry": "Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet without agonizing pain. In Advances in Neural Information Processing Systems, pages 974\u2013982, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Lazysvd%3A%20Even%20faster%20svd%20decomposition%20yet%20without%20agonizing%20pain%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Lazysvd%3A%20Even%20faster%20svd%20decomposition%20yet%20without%20agonizing%20pain%202016"
        },
        {
            "id": "Arora_et+al_2012_a",
            "entry": "Raman Arora, Ofer Dekel, and Ambuj Tewari. Deterministic MDPs with adversarial rewards and bandit feedback. In Proceedings on Uncertainty in Artificial Intellegence (UAI), 2012a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Raman%20Dekel%2C%20Ofer%20Tewari%2C%20Ambuj%20Deterministic%20MDPs%20with%20adversarial%20rewards%20and%20bandit%20feedback%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Raman%20Dekel%2C%20Ofer%20Tewari%2C%20Ambuj%20Deterministic%20MDPs%20with%20adversarial%20rewards%20and%20bandit%20feedback%202012"
        },
        {
            "id": "Arora_et+al_2012_b",
            "entry": "Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. In Proceedings of International Conference on Machine Learning (ICML), 2012b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Raman%20Dekel%2C%20Ofer%20Tewari%2C%20Ambuj%20Online%20bandit%20learning%20against%20an%20adaptive%20adversary%3A%20from%20regret%20to%20policy%20regret%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Raman%20Dekel%2C%20Ofer%20Tewari%2C%20Ambuj%20Online%20bandit%20learning%20against%20an%20adaptive%20adversary%3A%20from%20regret%20to%20policy%20regret%202012"
        },
        {
            "id": "Arora_et+al_0000_a",
            "entry": "Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a metaalgorithm and applications. Theory of Computing, 8(1):121\u2013164, 2012c.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20The%20multiplicative%20weights%20update%20method%3A%20a%20metaalgorithm%20and%20applications",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20The%20multiplicative%20weights%20update%20method%3A%20a%20metaalgorithm%20and%20applications"
        },
        {
            "id": "Auer_et+al_2002_a",
            "entry": "Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicolo%20Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicolo%20Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "Avrim_2007_a",
            "entry": "Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning Research, 8:1307\u20131324, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Avrim%20Blum%20and%20Yishay%20Mansour.%20From%20external%20to%20internal%20regret%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Avrim%20Blum%20and%20Yishay%20Mansour.%20From%20external%20to%20internal%20regret%202007"
        },
        {
            "id": "Proceedings_0000_a",
            "entry": "Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 459\u2013467.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proceedings%20of%20the%20fortysixth%20annual%20ACM%20symposium%20on%20Theory%20of%20computing%20pages%20459467",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proceedings%20of%20the%20fortysixth%20annual%20ACM%20symposium%20on%20Theory%20of%20computing%20pages%20459467"
        },
        {
            "id": "Even-Dar_et+al_2009_a",
            "entry": "Eyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online markov decision processes. Mathematics of Operations Research, 34(3):726\u2013736, 2009a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20Eyal%20Kakade%2C%20Sham%20M.%20Mansour%2C%20Yishay%20Online%20markov%20decision%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20Eyal%20Kakade%2C%20Sham%20M.%20Mansour%2C%20Yishay%20Online%20markov%20decision%20processes%202009"
        },
        {
            "id": "Even-Dar_et+al_2009_b",
            "entry": "Eyal Even-Dar, Yishay Mansour, and Uri Nadav. On the convergence of regret minimization dynamics in concave games. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 523\u2013532. ACM, 2009b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20Eyal%20Mansour%2C%20Yishay%20Nadav%2C%20Uri%20On%20the%20convergence%20of%20regret%20minimization%20dynamics%20in%20concave%20games%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20Eyal%20Mansour%2C%20Yishay%20Nadav%2C%20Uri%20On%20the%20convergence%20of%20regret%20minimization%20dynamics%20in%20concave%20games%202009"
        },
        {
            "id": "Farias_2006_a",
            "entry": "Daniela Pucci De Farias and Nimrod Megiddo. Combining expert advice in reactive environments. Journal of the ACM (JACM), 53(5):762\u2013799, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farias%2C%20Daniela%20Pucci%20De%20Megiddo%2C%20Nimrod%20Combining%20expert%20advice%20in%20reactive%20environments%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farias%2C%20Daniela%20Pucci%20De%20Megiddo%2C%20Nimrod%20Combining%20expert%20advice%20in%20reactive%20environments%202006"
        },
        {
            "id": "Foster_1997_a",
            "entry": "Dean P Foster and Rakesh V Vohra. Calibrated learning and correlated equilibrium. Games and Economic Behavior, 21(1-2):40, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foster%2C%20Dean%20P.%20Vohra%2C%20Rakesh%20V.%20Calibrated%20learning%20and%20correlated%20equilibrium%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foster%2C%20Dean%20P.%20Vohra%2C%20Rakesh%20V.%20Calibrated%20learning%20and%20correlated%20equilibrium%201997"
        },
        {
            "id": "Fudenberg_1999_a",
            "entry": "Drew Fudenberg and David K Levine. Conditional universal consistency. Games and Economic Behavior, 29(1-2):104\u2013130, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fudenberg%2C%20Drew%20Levine%2C%20David%20K.%20Conditional%20universal%20consistency%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fudenberg%2C%20Drew%20Levine%2C%20David%20K.%20Conditional%20universal%20consistency%201999"
        },
        {
            "id": "Hart_2000_a",
            "entry": "Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127\u20131150, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hart%2C%20Sergiu%20Mas-Colell%2C%20Andreu%20A%20simple%20adaptive%20procedure%20leading%20to%20correlated%20equilibrium%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hart%2C%20Sergiu%20Mas-Colell%2C%20Andreu%20A%20simple%20adaptive%20procedure%20leading%20to%20correlated%20equilibrium%202000"
        },
        {
            "id": "Hazan_2008_a",
            "entry": "Elad Hazan and Satyen Kale. Computational equivalence of fixed points and no regret algorithms, and convergence to equilibria. In Advances in Neural Information Processing Systems, pages 625\u2013632, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Computational%20equivalence%20of%20fixed%20points%20and%20no%20regret%20algorithms%2C%20and%20convergence%20to%20equilibria%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Computational%20equivalence%20of%20fixed%20points%20and%20no%20regret%20algorithms%2C%20and%20convergence%20to%20equilibria%202008"
        },
        {
            "id": "Heidari_et+al_2016_a",
            "entry": "Hoda Heidari, Michael Kearns, and Aaron Roth. Tight policy regret bounds for improving and decaying bandits. In IJCAI, pages 1562\u20131570, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heidari%2C%20Hoda%20Kearns%2C%20Michael%20Roth%2C%20Aaron%20Tight%20policy%20regret%20bounds%20for%20improving%20and%20decaying%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heidari%2C%20Hoda%20Kearns%2C%20Michael%20Roth%2C%20Aaron%20Tight%20policy%20regret%20bounds%20for%20improving%20and%20decaying%20bandits%202016"
        },
        {
            "id": "Kakade_2003_a",
            "entry": "Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD thesis, University of London London, England, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20Machandranath%20On%20the%20sample%20complexity%20of%20reinforcement%20learning%202003"
        },
        {
            "id": "Koren_et+al_2017_a",
            "entry": "Tomer Koren, Roi Livni, and Yishay Mansour. Bandits with movement costs and adaptive pricing. In Proceedings of the 2017 Conference on Learning Theory, pages 1242\u20131268, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koren%2C%20Tomer%20Livni%2C%20Roi%20Mansour%2C%20Yishay%20Bandits%20with%20movement%20costs%20and%20adaptive%20pricing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koren%2C%20Tomer%20Livni%2C%20Roi%20Mansour%2C%20Yishay%20Bandits%20with%20movement%20costs%20and%20adaptive%20pricing%202017"
        },
        {
            "id": "Koren_et+al_2017_b",
            "entry": "Tomer Koren, Roi Livni, and Yishay Mansour. Multi-armed bandits with metric movement costs. In Advances in Neural Information Processing Systems, pages 4119\u20134128, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koren%2C%20Tomer%20Livni%2C%20Roi%20Mansour%2C%20Yishay%20Multi-armed%20bandits%20with%20metric%20movement%20costs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koren%2C%20Tomer%20Livni%2C%20Roi%20Mansour%2C%20Yishay%20Multi-armed%20bandits%20with%20metric%20movement%20costs%202017"
        },
        {
            "id": "Merhav_et+al_2002_a",
            "entry": "Neri Merhav, Erik Ordentlich, Gadiel Seroussi, and Marcelo J Weinberger. On sequential strategies for loss functions with memory. IEEE Transactions on Information Theory, 48(7):1947\u20131958, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merhav%2C%20Neri%20Ordentlich%2C%20Erik%20Seroussi%2C%20Gadiel%20Weinberger%2C%20Marcelo%20J.%20On%20sequential%20strategies%20for%20loss%20functions%20with%20memory%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merhav%2C%20Neri%20Ordentlich%2C%20Erik%20Seroussi%2C%20Gadiel%20Weinberger%2C%20Marcelo%20J.%20On%20sequential%20strategies%20for%20loss%20functions%20with%20memory%202002"
        },
        {
            "id": "Mohri_2014_a",
            "entry": "Mehryar Mohri and Scott Yang. Conditional swap regret and conditional correlated equilibrium. In Advances in Neural Information Processing Systems, pages 1314\u20131322, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Yang%2C%20Scott%20Conditional%20swap%20regret%20and%20conditional%20correlated%20equilibrium%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohri%2C%20Mehryar%20Yang%2C%20Scott%20Conditional%20swap%20regret%20and%20conditional%20correlated%20equilibrium%202014"
        },
        {
            "id": "Mohri_2017_a",
            "entry": "Mehryar Mohri and Scott Yang. Online learning with transductive regret. In Advances in Neural Information Processing Systems, pages 5220\u20135230, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Yang%2C%20Scott%20Online%20learning%20with%20transductive%20regret%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohri%2C%20Mehryar%20Yang%2C%20Scott%20Online%20learning%20with%20transductive%20regret%202017"
        },
        {
            "id": "Neu_et+al_2010_a",
            "entry": "Gergely Neu, Andras Antos, Andr\u00e1s Gy\u00f6rgy, and Csaba Szepesv\u00e1ri. Online markov decision processes under bandit feedback. In Advances in Neural Information Processing Systems, pages 1804\u20131812, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neu%2C%20Gergely%20Antos%2C%20Andras%20Gy%C3%B6rgy%2C%20Andr%C3%A1s%20Szepesv%C3%A1ri%2C%20Csaba%20Online%20markov%20decision%20processes%20under%20bandit%20feedback%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neu%2C%20Gergely%20Antos%2C%20Andras%20Gy%C3%B6rgy%2C%20Andr%C3%A1s%20Szepesv%C3%A1ri%2C%20Csaba%20Online%20markov%20decision%20processes%20under%20bandit%20feedback%202010"
        },
        {
            "id": "Roughgarden_2015_a",
            "entry": "Tim Roughgarden. Intrinsic robustness of the price of anarchy. Journal of the ACM (JACM), 62(5): 32, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roughgarden%2C%20Tim%20Intrinsic%20robustness%20of%20the%20price%20of%20anarchy%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roughgarden%2C%20Tim%20Intrinsic%20robustness%20of%20the%20price%20of%20anarchy%202015"
        },
        {
            "id": "Saha_et+al_2012_a",
            "entry": "Ankan Saha, Prateek Jain, and Ambuj Tewari. The interplay between stability and regret in online learning. arXiv preprint arXiv:1211.6158, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1211.6158"
        },
        {
            "id": "Stoltz_2007_a",
            "entry": "Gilles Stoltz and G\u00e1bor Lugosi. Learning correlated equilibria in games with compact sets of strategies. Games and Economic Behavior, 59(1):187\u2013208, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stoltz%2C%20Gilles%20Lugosi%2C%20G%C3%A1bor%20Learning%20correlated%20equilibria%20in%20games%20with%20compact%20sets%20of%20strategies%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stoltz%2C%20Gilles%20Lugosi%2C%20G%C3%A1bor%20Learning%20correlated%20equilibria%20in%20games%20with%20compact%20sets%20of%20strategies%202007"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "Szepesv_2010_a",
            "entry": "Csaba Szepesv\u00e1ri. Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, 4(1):1\u2013103, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesv%C3%A1ri%2C%20Csaba%20Algorithms%20for%20reinforcement%20learning.%20Synthesis%20lectures%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szepesv%C3%A1ri%2C%20Csaba%20Algorithms%20for%20reinforcement%20learning.%20Synthesis%20lectures%202010"
        },
        {
            "id": "Yu_et+al_2009_a",
            "entry": "Jia Yuan Yu, Shie Mannor, and Nahum Shimkin. Markov decision processes with arbitrary reward processes. Mathematics of Operations Research, 34(3):737\u2013757, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Jia%20Yuan%20Mannor%2C%20Shie%20Shimkin%2C%20Nahum%20Markov%20decision%20processes%20with%20arbitrary%20reward%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Jia%20Yuan%20Mannor%2C%20Shie%20Shimkin%2C%20Nahum%20Markov%20decision%20processes%20with%20arbitrary%20reward%20processes%202009"
        }
    ]
}
