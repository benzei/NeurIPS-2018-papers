{
    "filename": "7771-a-convex-duality-framework-for-gans.pdf",
    "metadata": {
        "title": "A Convex Duality Framework for GANs",
        "author": "Farzan Farnia, David Tse",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7771-a-convex-duality-framework-for-gans.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generative adversarial network (GAN) is a minimax game between a generator mimicking the true model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function, this game reduces to finding the generative model minimizing a divergence measure, e.g. the Jensen-Shannon (JS) divergence, to the data distribution. However, in practice the discriminator is constrained to be in a smaller class F such as neural nets. Then, a natural question is how the divergence minimization interpretation changes as we constrain F. In this work, we address this question by developing a convex duality framework for analyzing GANs. For a convex set F, this duality framework interprets the original GAN formulation as finding the generative model with minimum JS-divergence to the distributions penalized to match the moments of the data distribution, with the moments specified by the discriminators in F. We show that this interpretation more generally holds for f-GAN and Wasserstein GAN. As a byproduct, we apply the duality framework to a hybrid of f-divergence and Wasserstein distance. Unlike the f-divergence, we prove that the proposed hybrid divergence changes continuously with the generative model, which suggests regularizing the discriminator\u2019s Lipschitz constant in f-GAN and vanilla GAN. We numerically evaluate the power of the suggested regularization schemes for improving GAN\u2019s training performance."
    },
    "keywords": [
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "highlights": [
        "Learning a probability model from data samples is a fundamental task in unsupervised learning",
        "In contrast to traditional methods of parameter fitting like maximum likelihood estimation, the generative adversarial network approach views the problem as a game between a generator G whose goal is to generate fake samples that are close to the real data training samples and a discriminator D whose goal is to distinguish between the real and fake samples",
        "To analyze the dual problem, we develop a convex duality framework for general divergence minimization problems",
        "We leverage our duality framework to prove that the hybrid dJSD,W1 , which possesses the same continuity property as in W1 distance, is the divergence measure minimized in vanilla generative adversarial network with 1-Lipschitz discriminator",
        "We develop a convex duality framework for analyzing divergence minimization problems conditioned to moment-matching constraints",
        "We show that a function in conv(Fnn) is a combination of infinitely-many neural nets, that function can be approximated by uniformly combining boundedly-many neural nets in function in conv"
    ],
    "key_statements": [
        "Learning a probability model from data samples is a fundamental task in unsupervised learning",
        "The recently developed generative adversarial network (GAN) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] leverages the power of deep neural networks to successfully address this task across various domains [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "In contrast to traditional methods of parameter fitting like maximum likelihood estimation, the generative adversarial network approach views the problem as a game between a generator G whose goal is to generate fake samples that are close to the real data training samples and a discriminator D whose goal is to distinguish between the real and fake samples",
        "The generator creates the fake samples by mapping from random noise input",
        "To analyze the dual problem, we develop a convex duality framework for general divergence minimization problems",
        "We show that a similar interpretation applies to generative adversarial network trained over any convex discriminator set F",
        "While a set of neural network functions is not necessarily convex, we prove any convex combination of Lipschitz-bounded neural nets can be approximated by uniformly combining boundedly-many neural nets",
        "We prove that this hybrid divergence enjoys a continuous behavior in distribution P1",
        "We leverage our duality framework to prove that the hybrid dJSD,W1 , which possesses the same continuity property as in W1 distance, is the divergence measure minimized in vanilla generative adversarial network with 1-Lipschitz discriminator",
        "We develop a convex duality framework for analyzing divergence minimization problems conditioned to moment-matching constraints",
        "The following example applies this result to the first-order Wasserstein distance and recovers the Wasserstein GAN problem [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] with a constrained 1-Lipschitz discriminator",
        "We show that a function in conv(Fnn) is a combination of infinitely-many neural nets, that function can be approximated by uniformly combining boundedly-many neural nets in function in conv",
        "We considered vanilla generative adversarial network [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] with the minimax formulation in (16) and DCGAN [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] convolutional architecture for discriminator and generator",
        "After replacing batch normalization with spectral normalization (SN) [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] to ensure the discriminator\u2019s Lipschitzness, the discriminator loss decreased in a desired monotonic fashion (Figure 2-right). This observation is consistent with Theorems 5 and 6 showing that the discriminator loss becomes an estimate for the hybrid dJSD,W1 divergence changing continuously with the generator parameters"
    ],
    "summary": [
        "Learning a probability model from data samples is a fundamental task in unsupervised learning.",
        "The optimization problem (2) can be interpreted as finding the closest generative model to the data distribution PX (Figure 1a), where distance is measured using the JS-divergence.",
        "For this class of discriminator sets, we interpret vanilla GAN as the following JS-divergence minimization between two sets of probability distributions, the set of generative models and the set of discriminator moment-matching distributions (Figure 1b), min min JSD(PG(Z), Q).",
        "We leverage our duality framework to prove that the hybrid dJSD,W1 , which possesses the same continuity property as in W1 distance, is the divergence measure minimized in vanilla GAN with 1-Lipschitz discriminator.",
        "This reduction reveals a divergence minimization problem between generative models and the following set PF (P ) which we call the set of discriminator moment matching distributions, PF (P ) := Q : \u2200D \u2208 F , EQ[D(X)] = EP [D(X)] .",
        "If for function set Fthe corresponding F = {D : D(x) = \u2212 log(1 + exp(D (x))), D \u2208 F} is a convex set, (15) will reduce to the following minimax game which is the vanilla GAN problem (1) with sigmoid activation applied to the discriminator output, exp(D (X))",
        "The following example applies this result to the first-order Wasserstein distance and recovers the WGAN problem [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] with a constrained 1-Lipschitz discriminator.",
        "We discuss two more examples in the Appendix: 1) for the indicator cost cI (x, x ) = I(x = x ) corresponding to the total variation distance we draw the connection to the energy-based GAN [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], 2) for the second-order cost c2(x, x ) = x \u2212 x 2 we recover [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]\u2019s quadratic GAN formulation under the LQG setting assumptions, i.e. linear generator, quadratic discriminator and Gaussian input data.",
        "6 Minimum-sum hybrid of f-divergence and Wasserstein distance: GAN with Lipschitz or adversarially-trained discriminator",
        "Our discussion has so far focused on the mixture of f-divergence and the first order Wasserstein distance, which suggests training f-GAN over Lipschitz-bounded discriminators.",
        "This observation is consistent with Theorems 5 and 6 showing that the discriminator loss becomes an estimate for the hybrid dJSD,W1 divergence changing continuously with the generator parameters.",
        "Several other works including [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>] explore the optimization and stability properties of training GANs. we note that the same convex analysis approach used in this paper has further provided a powerful theoretical framework to analyze various supervised and unsupervised learning problems [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>]."
    ],
    "headline": "We address this question by developing a convex duality framework for analyzing generative adversarial network",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "2",
            "entry": "[2] Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "3",
            "entry": "[3] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pages 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.03906"
        },
        {
            "id": "6",
            "entry": "[6] Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International Conference on Machine Learning, pages 1718\u20131727, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015"
        },
        {
            "id": "7",
            "entry": "[7] Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab\u00e1s P\u00f3czos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pages 2200\u20132210, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017"
        },
        {
            "id": "8",
            "entry": "[8] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03126"
        },
        {
            "id": "9",
            "entry": "[9] Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans: the lqg setting. arXiv preprint arXiv:1710.10793, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10793"
        },
        {
            "id": "10",
            "entry": "[10] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00573"
        },
        {
            "id": "11",
            "entry": "[11] Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. In Advances in Neural Information Processing Systems, pages 5551\u20135559, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Shuang%20Bousquet%2C%20Olivier%20Chaudhuri%2C%20Kamalika%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Shuang%20Bousquet%2C%20Olivier%20Chaudhuri%2C%20Kamalika%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017"
        },
        {
            "id": "12",
            "entry": "[12] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "13",
            "entry": "[13] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "14",
            "entry": "[14] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018"
        },
        {
            "id": "15",
            "entry": "[15] Imre Csisz\u00e1r, Paul C Shields, et al. Information theory and statistics: A tutorial. Foundations and Trends R in Communications and Information Theory, 1(4):417\u2013528, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Csisz%C3%A1r%2C%20Imre%20Shields%2C%20Paul%20C.%20Information%20theory%20and%20statistics%3A%20A%20tutorial.%20Foundations%20and%20Trends%20R%20in%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Csisz%C3%A1r%2C%20Imre%20Shields%2C%20Paul%20C.%20Information%20theory%20and%20statistics%3A%20A%20tutorial.%20Foundations%20and%20Trends%20R%20in%202004"
        },
        {
            "id": "16",
            "entry": "[16] C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C%C3%A9dric%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338%202008"
        },
        {
            "id": "17",
            "entry": "[17] Yasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference via convex duality. In International Conference on Computational Learning Theory, pages 139\u2013153, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altun%2C%20Yasemin%20Smola%2C%20Alex%20Unifying%20divergence%20minimization%20and%20statistical%20inference%20via%20convex%20duality%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altun%2C%20Yasemin%20Smola%2C%20Alex%20Unifying%20divergence%20minimization%20and%20statistical%20inference%20via%20convex%20duality%202006"
        },
        {
            "id": "18",
            "entry": "[18] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Convex%20optimization%202004"
        },
        {
            "id": "19",
            "entry": "[19] Martin Arjovsky and L\u00e9on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "20",
            "entry": "[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "21",
            "entry": "[21] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        },
        {
            "id": "22",
            "entry": "[22] Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "23",
            "entry": "[23] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "24",
            "entry": "[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "25",
            "entry": "[25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "26",
            "entry": "[26] Richard Nock, Zac Cranko, Aditya K Menon, Lizhen Qu, and Robert C Williamson. f-gans in an information geometric nutshell. In Advances in Neural Information Processing Systems, pages 456\u2013464, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nock%2C%20Richard%20Cranko%2C%20Zac%20Menon%2C%20Aditya%20K.%20Qu%2C%20Lizhen%20f-gans%20in%20an%20information%20geometric%20nutshell%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nock%2C%20Richard%20Cranko%2C%20Zac%20Menon%2C%20Aditya%20K.%20Qu%2C%20Lizhen%20f-gans%20in%20an%20information%20geometric%20nutshell%202017"
        },
        {
            "id": "27",
            "entry": "[27] Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08224"
        },
        {
            "id": "28",
            "entry": "[28] Shibani Santurkar, Ludwig Schmidt, and Aleksander Madry. A classification-based perspective on gan distributions. arXiv preprint arXiv:1711.00970, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00970"
        },
        {
            "id": "29",
            "entry": "[29] Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discriminationgeneralization tradeoff in GANs. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Pengchuan%20Liu%2C%20Qiang%20Zhou%2C%20Dengyong%20Xu%2C%20Tao%20On%20the%20discriminationgeneralization%20tradeoff%20in%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Pengchuan%20Liu%2C%20Qiang%20Zhou%2C%20Dengyong%20Xu%2C%20Tao%20On%20the%20discriminationgeneralization%20tradeoff%20in%20GANs%202018"
        },
        {
            "id": "30",
            "entry": "[30] Xu Chen, Jiang Wang, and Hao Ge. Training generative adversarial networks via primal-dual subgradient methods: a Lagrangian perspective on GAN. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xu%20Wang%2C%20Jiang%20Ge%2C%20Hao%20Training%20generative%20adversarial%20networks%20via%20primal-dual%20subgradient%20methods%3A%20a%20Lagrangian%20perspective%20on%20GAN%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xu%20Wang%2C%20Jiang%20Ge%2C%20Hao%20Training%20generative%20adversarial%20networks%20via%20primal-dual%20subgradient%20methods%3A%20a%20Lagrangian%20perspective%20on%20GAN%202018"
        },
        {
            "id": "31",
            "entry": "[31] Shengjia Zhao, Jiaming Song, and Stefano Ermon. The information autoencoding family: A lagrangian perspective on latent variable generative models. arXiv preprint arXiv:1806.06514, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06514"
        },
        {
            "id": "32",
            "entry": "[32] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In Advances in Neural Information Processing Systems, pages 2015\u20132025, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roth%2C%20Kevin%20Lucchi%2C%20Aurelien%20Nowozin%2C%20Sebastian%20Hofmann%2C%20Thomas%20Stabilizing%20training%20of%20generative%20adversarial%20networks%20through%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roth%2C%20Kevin%20Lucchi%2C%20Aurelien%20Nowozin%2C%20Sebastian%20Hofmann%2C%20Thomas%20Stabilizing%20training%20of%20generative%20adversarial%20networks%20through%20regularization%202015"
        },
        {
            "id": "33",
            "entry": "[33] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems, pages 5591\u20135600, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "34",
            "entry": "[34] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in Neural Information Processing Systems, pages 1823\u20131833, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20gans%202017"
        },
        {
            "id": "35",
            "entry": "[35] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. arXiv preprint arXiv:1711.00141, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00141"
        },
        {
            "id": "36",
            "entry": "[36] Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. Solving approximate wasserstein gans to stationarity. arXiv preprint arXiv:1802.08249, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08249"
        },
        {
            "id": "37",
            "entry": "[37] Miroslav Dud\u00edk, Steven J Phillips, and Robert E Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. Journal of Machine Learning Research, 8(Jun):1217\u20131260, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dud%C3%ADk%2C%20Miroslav%20Phillips%2C%20Steven%20J.%20Schapire%2C%20Robert%20E.%20Maximum%20entropy%20density%20estimation%20with%20generalized%20regularization%20and%20an%20application%20to%20species%20distribution%20modeling%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dud%C3%ADk%2C%20Miroslav%20Phillips%2C%20Steven%20J.%20Schapire%2C%20Robert%20E.%20Maximum%20entropy%20density%20estimation%20with%20generalized%20regularization%20and%20an%20application%20to%20species%20distribution%20modeling%202007"
        },
        {
            "id": "38",
            "entry": "[38] Meisam Razaviyayn, Farzan Farnia, and David Tse. Discrete r\u00e9nyi classifiers. In Advances in Neural Information Processing Systems, pages 3276\u20133284, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Razaviyayn%2C%20Meisam%20Farnia%2C%20Farzan%20Tse%2C%20David%20Discrete%20r%C3%A9nyi%20classifiers%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Razaviyayn%2C%20Meisam%20Farnia%2C%20Farzan%20Tse%2C%20David%20Discrete%20r%C3%A9nyi%20classifiers%202015"
        },
        {
            "id": "39",
            "entry": "[39] Farzan Farnia and David Tse. A minimax approach to supervised learning. In Advances in Neural Information Processing Systems, pages 4240\u20134248, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farnia%2C%20Farzan%20Tse%2C%20David%20A%20minimax%20approach%20to%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farnia%2C%20Farzan%20Tse%2C%20David%20A%20minimax%20approach%20to%20supervised%20learning%202016"
        },
        {
            "id": "40",
            "entry": "[40] Rizal Fathony, Anqi Liu, Kaiser Asif, and Brian Ziebart. Adversarial multiclass classification: A risk minimization perspective. In Advances in Neural Information Processing Systems, pages 559\u2013567, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fathony%2C%20Rizal%20Liu%2C%20Anqi%20Asif%2C%20Kaiser%20Ziebart%2C%20Brian%20Adversarial%20multiclass%20classification%3A%20A%20risk%20minimization%20perspective%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fathony%2C%20Rizal%20Liu%2C%20Anqi%20Asif%2C%20Kaiser%20Ziebart%2C%20Brian%20Adversarial%20multiclass%20classification%3A%20A%20risk%20minimization%20perspective%202016"
        },
        {
            "id": "41",
            "entry": "[41] Rizal Fathony, Mohammad Ali Bashiri, and Brian Ziebart. Adversarial surrogate losses for ordinal regression. In Advances in Neural Information Processing Systems, pages 563\u2013573, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fathony%2C%20Rizal%20Bashiri%2C%20Mohammad%20Ali%20Ziebart%2C%20Brian%20Adversarial%20surrogate%20losses%20for%20ordinal%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fathony%2C%20Rizal%20Bashiri%2C%20Mohammad%20Ali%20Ziebart%2C%20Brian%20Adversarial%20surrogate%20losses%20for%20ordinal%20regression%202017"
        }
    ]
}
