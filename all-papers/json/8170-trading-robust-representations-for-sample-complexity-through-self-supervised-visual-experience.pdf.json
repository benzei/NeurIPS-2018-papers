{
    "filename": "8170-trading-robust-representations-for-sample-complexity-through-self-supervised-visual-experience.pdf",
    "metadata": {
        "title": "Trading robust representations for sample complexity through self-supervised visual experience",
        "author": "Andrea Tacchetti, Stephen Voinea, Georgios Evangelopoulos",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8170-trading-robust-representations-for-sample-complexity-through-self-supervised-visual-experience.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning in small sample regimes is among the most remarkable features of the human perceptual system. This ability is related to robustness to transformations, which is acquired through visual experience in the form of weakor self-supervision during development. We explore the idea of allowing artificial systems to learn representations of visual stimuli through weak supervision prior to downstream supervised tasks. We introduce a novel loss function for representation learning using unlabeled image sets and video sequences, and experimentally demonstrate that these representations support one-shot learning and reduce the sample complexity of multiple recognition tasks. We establish the existence of a trade-off between the sizes of weakly supervised, automatically obtained from video sequences, and fully supervised data sets. Our results suggest that equivalence sets other than class labels, which are abundant in unlabeled visual experience, can be used for self-supervised learning of semantically relevant image embeddings."
    },
    "keywords": [
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "perceptual system",
            "url": "https://en.wikipedia.org/wiki/perceptual_system"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "Transformation invariance and learning in small sample regimes are among the most remarkable abilities of the human perceptual system, and arguably the ones that have proven most difficult to replicate in artificial systems",
        "Neuroscientists have long debated whether exposure to specific modes of visual experience, such as spatial proximity or sequential presentation, is necessary to learn visual representations that are robust to complex transformations, or if most of these abilities are innate and independent of visual experience [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "In order to explicitly use the information in such partitions of a training set, we introduce a new loss function that promotes representations that are robust to inter-orbit and selective to intra-orbit relations",
        "We demonstrate how image embeddings learned through weak supervision using our novel loss function and orbit sets support one-shot learning in a variety of settings and reduce the sample complexity of challenging recognition tasks",
        "Motivated by findings in the neuroscience of vision on the necessary role of visual experience in the development of robust visual perception, we considered image representation learning using selfor weaksupervision from unlabeled image sets and videos",
        "We proposed a novel loss function that combines a discriminative and a rectification component of complementary objectives"
    ],
    "key_statements": [
        "Transformation invariance and learning in small sample regimes are among the most remarkable abilities of the human perceptual system, and arguably the ones that have proven most difficult to replicate in artificial systems",
        "Neuroscientists have long debated whether exposure to specific modes of visual experience, such as spatial proximity or sequential presentation, is necessary to learn visual representations that are robust to complex transformations, or if most of these abilities are innate and independent of visual experience [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "Newborn monkeys deprived of any visual experience of faces were found not to exhibit a face selective cortical area [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "In order to explicitly use the information in such partitions of a training set, we introduce a new loss function that promotes representations that are robust to inter-orbit and selective to intra-orbit relations",
        "We demonstrate how image embeddings learned through weak supervision using our novel loss function and orbit sets support one-shot learning in a variety of settings and reduce the sample complexity of challenging recognition tasks",
        "We systematically evaluate representations learned from different datasets and applied to different visual recognition tasks using the same parametrization, different loss functions, and varying degrees of supervision",
        "In Fig. 4 we demonstrate this as a two-dimensional map of classification accuracy for Orbit Joint on the Late Night dataset, by letting the number of labeled examples change between 1-20 per class and the number of orbits between 10-500",
        "Motivated by findings in the neuroscience of vision on the necessary role of visual experience in the development of robust visual perception, we considered image representation learning using selfor weaksupervision from unlabeled image sets and videos",
        "We proposed a novel loss function that combines a discriminative and a rectification component of complementary objectives",
        "Our work suggests that partitioning the training set to equivalence classes defined from sampled or implicitly acquired transformations, is a useful weak-supervision signal for extracting embeddings that are semantically relevant and economically learned"
    ],
    "summary": [
        "Transformation invariance and learning in small sample regimes are among the most remarkable abilities of the human perceptual system, and arguably the ones that have proven most difficult to replicate in artificial systems.",
        "We demonstrate how image embeddings learned through weak supervision using our novel loss function and orbit sets support one-shot learning in a variety of settings and reduce the sample complexity of challenging recognition tasks.",
        "Our results suggest that partitioning observations into equivalence sets, similar to what is achieved through unlabeled visual experience or self-supervision, and using the proposed loss, one can learn semantically relevant embeddings and lower the sample complexity of downstream visual tasks.",
        "Examples of c are the labels of a supervised learning task, the indexes of vector quantization codewords or, for the case of sequential data such as videos, thesequence membership, with C the set of classes, codewords or sequences respectively.",
        "We systematically evaluate representations learned from different datasets and applied to different visual recognition tasks using the same parametrization, different loss functions, and varying degrees of supervision.",
        "Our experimental procedure is explicitly designed to probe the impact of learning robust representations using generic orbit sets on the performance in downstream tasks.",
        "For the affine MNIST evaluations, we used an embedding featuring a Spatial Transformer Networks module [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], trained with orbit supervision (OT-STN) or full supervision (ST-STN).",
        "The weakly-supervised methods (OJ, OT, OE, EX) had only access to the set of 129 \u00d7 20 \u00d7 3 orbits in the embedding set, each corresponding to all 13 viewpoints for a single identity, illumination condition and session.",
        "Figure 2 shows the relative distance landscape, as 2D t-SNE plots [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>], for all images from 10 random subjects of the test set, encoded using embeddings learned with different losses.",
        "By learning robust embeddings from video sequences, we are able to lower the number of labelled examples required to achieve a fixed performance level in a downstream recognition task.",
        "Embedding set: From processing 500 unlabeled source videos, we collected 271 415 images across 500 orbits, which were not manually validated or quality assured in any way.",
        "In Fig. 4 we demonstrate this as a two-dimensional map of classification accuracy for OJ on the Late Night dataset, by letting the number of labeled examples change between 1-20 per class and the number of orbits between 10-500.",
        "Our work suggests that partitioning the training set to equivalence classes defined from sampled or implicitly acquired transformations, is a useful weak-supervision signal for extracting embeddings that are semantically relevant and economically learned.",
        "Future work will involve orbit evaluation and sampling on natural and complex image and video datasets"
    ],
    "headline": "We explore the idea of allowing artificial systems to learn representations of visual stimuli through weak supervision prior to downstream supervised tasks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] F. Anselmi, L. Rosasco, and T. Poggio. On invariance and selectivity in representation learning. Information and Inference, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anselmi%2C%20F.%20Rosasco%2C%20L.%20Poggio%2C%20T.%20On%20invariance%20and%20selectivity%20in%20representation%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anselmi%2C%20F.%20Rosasco%2C%20L.%20Poggio%2C%20T.%20On%20invariance%20and%20selectivity%20in%20representation%20learning%202015"
        },
        {
            "id": "2",
            "entry": "[2] F. Anselmi, J. Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, and T. Poggio. Unsupervised learning of invariant representations. Theoretical Computer Science, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anselmi%2C%20F.%20Leibo%2C%20J.Z.%20Rosasco%2C%20L.%20Mutch%2C%20J.%20Unsupervised%20learning%20of%20invariant%20representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anselmi%2C%20F.%20Leibo%2C%20J.Z.%20Rosasco%2C%20L.%20Mutch%2C%20J.%20Unsupervised%20learning%20of%20invariant%20representations%202016"
        },
        {
            "id": "3",
            "entry": "[3] F. Anselmi, G. Evangelopoulos, L. Rosasco, and T. Poggio. Symmetry-adapted representation learning. Pattern Recognition, 2019.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anselmi%2C%20F.%20Evangelopoulos%2C%20G.%20Rosasco%2C%20L.%20Poggio%2C%20T.%20Symmetry-adapted%20representation%20learning%202019",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anselmi%2C%20F.%20Evangelopoulos%2C%20G.%20Rosasco%2C%20L.%20Poggio%2C%20T.%20Symmetry-adapted%20representation%20learning%202019"
        },
        {
            "id": "4",
            "entry": "[4] M. J. Arcaro, P. F. Schade, J. L. Vincent, C. R. Ponce, and M. S. Livingstone. Seeing faces is necessary for face-domain formation. Nature Neuroscience, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arcaro%2C%20M.J.%20Schade%2C%20P.F.%20Vincent%2C%20J.L.%20Ponce%2C%20C.R.%20Seeing%20faces%20is%20necessary%20for%20face-domain%20formation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arcaro%2C%20M.J.%20Schade%2C%20P.F.%20Vincent%2C%20J.L.%20Ponce%2C%20C.R.%20Seeing%20faces%20is%20necessary%20for%20face-domain%20formation%202017"
        },
        {
            "id": "5",
            "entry": "[5] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE PAMI, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Courville%2C%20A.%20Vincent%2C%20P.%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "6",
            "entry": "[6] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE PAMI, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruna%2C%20J.%20Mallat%2C%20S.%20Invariant%20scattering%20convolution%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruna%2C%20J.%20Mallat%2C%20S.%20Invariant%20scattering%20convolution%20networks%202013"
        },
        {
            "id": "7",
            "entry": "[7] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In IEEE CVPR, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chopra%2C%20S.%20Hadsell%2C%20R.%20LeCun%2C%20Y.%20Learning%20a%20similarity%20metric%20discriminatively%2C%20with%20application%20to%20face%20verification%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chopra%2C%20S.%20Hadsell%2C%20R.%20LeCun%2C%20Y.%20Learning%20a%20similarity%20metric%20discriminatively%2C%20with%20application%20to%20face%20verification%202005"
        },
        {
            "id": "8",
            "entry": "[8] T. S. Cohen and M. Welling. Group equivariant convolutional networks. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=T%20S%20Cohen%20and%20M%20Welling%20Group%20equivariant%20convolutional%20networks%20In%20ICML%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=T%20S%20Cohen%20and%20M%20Welling%20Group%20equivariant%20convolutional%20networks%20In%20ICML%202016"
        },
        {
            "id": "9",
            "entry": "[9] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction. In IEEE ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20C.%20Gupta%2C%20A.%20Efros%2C%20A.A.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20C.%20Gupta%2C%20A.%20Efros%2C%20A.A.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015"
        },
        {
            "id": "10",
            "entry": "[10] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, and T. Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE PAMI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20A.%20Fischer%2C%20P.%20Springenberg%2C%20J.T.%20Riedmiller%2C%20M.%20Discriminative%20unsupervised%20feature%20learning%20with%20exemplar%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "11",
            "entry": "[11] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidaris%2C%20S.%20Singh%2C%20P.%20Komodakis%2C%20N.%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidaris%2C%20S.%20Singh%2C%20P.%20Komodakis%2C%20N.%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018"
        },
        {
            "id": "12",
            "entry": "[12] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Unsupervised learning of spatiotemporally coherent metrics. In IEEE ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goroshin%2C%20R.%20Bruna%2C%20J.%20Tompson%2C%20J.%20Eigen%2C%20D.%20Unsupervised%20learning%20of%20spatiotemporally%20coherent%20metrics%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goroshin%2C%20R.%20Bruna%2C%20J.%20Tompson%2C%20J.%20Eigen%2C%20D.%20Unsupervised%20learning%20of%20spatiotemporally%20coherent%20metrics%202015"
        },
        {
            "id": "13",
            "entry": "[13] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker. Multi-PIE. Image and Vision Computing, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R%20Gross%20I%20Matthews%20J%20Cohn%20T%20Kanade%20and%20S%20Baker%20MultiPIE%20Image%20and%20Vision%20Computing%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R%20Gross%20I%20Matthews%20J%20Cohn%20T%20Kanade%20and%20S%20Baker%20MultiPIE%20Image%20and%20Vision%20Computing%202010"
        },
        {
            "id": "14",
            "entry": "[14] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In IEEE CVPR, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadsell%2C%20R.%20Chopra%2C%20S.%20LeCun%2C%20Y.%20Dimensionality%20reduction%20by%20learning%20an%20invariant%20mapping%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadsell%2C%20R.%20Chopra%2C%20S.%20LeCun%2C%20Y.%20Dimensionality%20reduction%20by%20learning%20an%20invariant%20mapping%202006"
        },
        {
            "id": "15",
            "entry": "[15] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In ICANN, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Wang%2C%20S.D.%20Transforming%20auto-encoders%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Wang%2C%20S.D.%20Transforming%20auto-encoders%202011"
        },
        {
            "id": "16",
            "entry": "[16] M. Jaderberg, K. Simonyan, and A. Zisserman. Spatial transformer networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Spatial%20transformer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Spatial%20transformer%20networks%202015"
        },
        {
            "id": "17",
            "entry": "[17] D. Jayaraman and K. Grauman. Slow and steady feature analysis: Higher order temporal coherence in video. In IEEE ICCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jayaraman%2C%20D.%20Grauman%2C%20K.%20Slow%20and%20steady%20feature%20analysis%3A%20Higher%20order%20temporal%20coherence%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jayaraman%2C%20D.%20Grauman%2C%20K.%20Slow%20and%20steady%20feature%20analysis%3A%20Higher%20order%20temporal%20coherence%202016"
        },
        {
            "id": "18",
            "entry": "[18] G. Larsson, M. Maire, and G. Shakhnarovich. Colorization as a proxy task for visual understanding. In IEEE CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larsson%2C%20G.%20Maire%2C%20M.%20Shakhnarovich%2C%20G.%20Colorization%20as%20a%20proxy%20task%20for%20visual%20understanding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larsson%2C%20G.%20Maire%2C%20M.%20Shakhnarovich%2C%20G.%20Colorization%20as%20a%20proxy%20task%20for%20visual%20understanding%202017"
        },
        {
            "id": "19",
            "entry": "[19] Y. LeCun, F. J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In IEEE CVPR, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Huang%2C%20F.J.%20Bottou%2C%20L.%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Huang%2C%20F.J.%20Bottou%2C%20L.%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004"
        },
        {
            "id": "20",
            "entry": "[20] H. Mobahi, R. Collobert, and J. Weston. Deep learning from temporal coherence in video. In ICML, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mobahi%2C%20H.%20Collobert%2C%20R.%20Weston%2C%20J.%20Deep%20learning%20from%20temporal%20coherence%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mobahi%2C%20H.%20Collobert%2C%20R.%20Weston%2C%20J.%20Deep%20learning%20from%20temporal%20coherence%202009"
        },
        {
            "id": "21",
            "entry": "[21] P. Niyogi, F. Girosi, and T. Poggio. Incorporating prior information in machine learning by creating virtual examples. Proceedings of the IEEE, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niyogi%2C%20P.%20Girosi%2C%20F.%20Poggio%2C%20T.%20Incorporating%20prior%20information%20in%20machine%20learning%20by%20creating%20virtual%20examples%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niyogi%2C%20P.%20Girosi%2C%20F.%20Poggio%2C%20T.%20Incorporating%20prior%20information%20in%20machine%20learning%20by%20creating%20virtual%20examples%201998"
        },
        {
            "id": "22",
            "entry": "[22] D. Novotn\u00fd, S. Albanie, D. Larlus, and A. Vedaldi. Self-supervised learning of geometrically stable features through probabilistic introspection. In IEEE CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Novotn%C3%BD%2C%20D.%20Albanie%2C%20S.%20Larlus%2C%20D.%20Vedaldi%2C%20A.%20Self-supervised%20learning%20of%20geometrically%20stable%20features%20through%20probabilistic%20introspection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novotn%C3%BD%2C%20D.%20Albanie%2C%20S.%20Larlus%2C%20D.%20Vedaldi%2C%20A.%20Self-supervised%20learning%20of%20geometrically%20stable%20features%20through%20probabilistic%20introspection%202018"
        },
        {
            "id": "23",
            "entry": "[23] G. Perry, E. T. Rolls, and S. M. Stringer. Spatial vs temporal continuity in view invariant visual object recognition learning. Vision Research, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perry%2C%20G.%20Rolls%2C%20E.T.%20Stringer%2C%20S.M.%20Spatial%20vs%20temporal%20continuity%20in%20view%20invariant%20visual%20object%20recognition%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perry%2C%20G.%20Rolls%2C%20E.T.%20Stringer%2C%20S.M.%20Spatial%20vs%20temporal%20continuity%20in%20view%20invariant%20visual%20object%20recognition%20learning%202006"
        },
        {
            "id": "24",
            "entry": "[24] G. Perry, E. T. Rolls, and S. M. Stringer. Continuous transformation learning of translation invariant representations. Experimental Brain Research, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perry%2C%20G.%20Rolls%2C%20E.T.%20Stringer%2C%20S.M.%20Continuous%20transformation%20learning%20of%20translation%20invariant%20representations%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perry%2C%20G.%20Rolls%2C%20E.T.%20Stringer%2C%20S.M.%20Continuous%20transformation%20learning%20of%20translation%20invariant%20representations%202010"
        },
        {
            "id": "25",
            "entry": "[25] F. Schroff, D. Kalenichenko, and J. Philbin. FaceNet: A unified embedding for face recognition and clustering. In IEEE CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schroff%2C%20F.%20Kalenichenko%2C%20D.%20Philbin%2C%20J.%20FaceNet%3A%20A%20unified%20embedding%20for%20face%20recognition%20and%20clustering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schroff%2C%20F.%20Kalenichenko%2C%20D.%20Philbin%2C%20J.%20FaceNet%3A%20A%20unified%20embedding%20for%20face%20recognition%20and%20clustering%202015"
        },
        {
            "id": "26",
            "entry": "[26] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202014"
        },
        {
            "id": "27",
            "entry": "[27] S. Soatto and A. Chiuso. Visual representations: Defining properties and deep approximations. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soatto%2C%20S.%20Chiuso%2C%20A.%20Visual%20representations%3A%20Defining%20properties%20and%20deep%20approximations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soatto%2C%20S.%20Chiuso%2C%20A.%20Visual%20representations%3A%20Defining%20properties%20and%20deep%20approximations%202016"
        },
        {
            "id": "28",
            "entry": "[28] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep metric learning via lifted structured feature embedding. In IEEE CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20H.O.%20Xiang%2C%20Y.%20Jegelka%2C%20S.%20Savarese%2C%20S.%20Deep%20metric%20learning%20via%20lifted%20structured%20feature%20embedding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20H.O.%20Xiang%2C%20Y.%20Jegelka%2C%20S.%20Savarese%2C%20S.%20Deep%20metric%20learning%20via%20lifted%20structured%20feature%20embedding%202016"
        },
        {
            "id": "29",
            "entry": "[29] A. Tacchetti, L. Isik, and T. Poggio. Invariant recognition drives neural representations of action sequences. PLOS Computational Biology, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tacchetti%2C%20A.%20Isik%2C%20L.%20Poggio%2C%20T.%20Invariant%20recognition%20drives%20neural%20representations%20of%20action%20sequences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tacchetti%2C%20A.%20Isik%2C%20L.%20Poggio%2C%20T.%20Invariant%20recognition%20drives%20neural%20representations%20of%20action%20sequences%202017"
        },
        {
            "id": "30",
            "entry": "[30] A. Tacchetti, L. Isik, and T. A. Poggio. Invariant recognition shapes neural representations of visual input. Annual Review of Vision Science, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tacchetti%2C%20A.%20Isik%2C%20L.%20Poggio%2C%20T.A.%20Invariant%20recognition%20shapes%20neural%20representations%20of%20visual%20input%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tacchetti%2C%20A.%20Isik%2C%20L.%20Poggio%2C%20T.A.%20Invariant%20recognition%20shapes%20neural%20representations%20of%20visual%20input%202018"
        },
        {
            "id": "31",
            "entry": "[31] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008"
        },
        {
            "id": "32",
            "entry": "[32] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In IEEE CVPR, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Viola%2C%20P.%20Jones%2C%20M.%20Rapid%20object%20detection%20using%20a%20boosted%20cascade%20of%20simple%20features%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Viola%2C%20P.%20Jones%2C%20M.%20Rapid%20object%20detection%20using%20a%20boosted%20cascade%20of%20simple%20features%202001"
        },
        {
            "id": "33",
            "entry": "[33] X. Wang and A. Gupta. Unsupervised learning of visual representations using videos. In IEEE ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Gupta%2C%20A.%20Unsupervised%20learning%20of%20visual%20representations%20using%20videos%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Gupta%2C%20A.%20Unsupervised%20learning%20of%20visual%20representations%20using%20videos%202015"
        },
        {
            "id": "34",
            "entry": "[34] X. Wang, K. He, and A. Gupta. Transitive invariance for self-supervised visual representation learning. In IEEE ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20He%2C%20K.%20Gupta%2C%20A.%20Transitive%20invariance%20for%20self-supervised%20visual%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20He%2C%20K.%20Gupta%2C%20A.%20Transitive%20invariance%20for%20self-supervised%20visual%20representation%20learning%202017"
        },
        {
            "id": "35",
            "entry": "[35] K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20K.Q.%20Saul%2C%20L.K.%20Distance%20metric%20learning%20for%20large%20margin%20nearest%20neighbor%20classification%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20K.Q.%20Saul%2C%20L.K.%20Distance%20metric%20learning%20for%20large%20margin%20nearest%20neighbor%20classification%202009"
        },
        {
            "id": "36",
            "entry": "[36] J. N. Wood and S. M. Wood. The development of invariant object recognition requires visual experience with temporally smooth objects. Cognitive Science, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wood%2C%20J.N.%20Wood%2C%20S.M.%20The%20development%20of%20invariant%20object%20recognition%20requires%20visual%20experience%20with%20temporally%20smooth%20objects%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wood%2C%20J.N.%20Wood%2C%20S.M.%20The%20development%20of%20invariant%20object%20recognition%20requires%20visual%20experience%20with%20temporally%20smooth%20objects%202018"
        },
        {
            "id": "37",
            "entry": "[37] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus. Deconvolutional networks. In IEEE CVPR, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20D%20Zeiler%20D%20Krishnan%20G%20W%20Taylor%20and%20R%20Fergus%20Deconvolutional%20networks%20In%20IEEE%20CVPR%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20D%20Zeiler%20D%20Krishnan%20G%20W%20Taylor%20and%20R%20Fergus%20Deconvolutional%20networks%20In%20IEEE%20CVPR%202010"
        },
        {
            "id": "38",
            "entry": "[38] J. Zhao, M. Mathieu, R. Goroshin, and Y. LeCun. Stacked what-where auto-encoders. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20J.%20Mathieu%2C%20M.%20Goroshin%2C%20R.%20LeCun%2C%20Y.%20Stacked%20what-where%20auto-encoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20J.%20Mathieu%2C%20M.%20Goroshin%2C%20R.%20LeCun%2C%20Y.%20Stacked%20what-where%20auto-encoders%202016"
        },
        {
            "id": "39",
            "entry": "[39] X. Zhu and D. Ramanan. Face detection, pose estimation, and landmark localization in the wild. In IEEE",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20X.%20Ramanan%2C%20D.%20Face%20detection%2C%20pose%20estimation%2C%20and%20landmark%20localization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20X.%20Ramanan%2C%20D.%20Face%20detection%2C%20pose%20estimation%2C%20and%20landmark%20localization"
        },
        {
            "id": "40",
            "entry": "[40] W. Zou, S. Zhu, K. Yu, and A. Ng. Deep learning of invariant features via simulated fixations in video. In ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20W.%20Zhu%2C%20S.%20Yu%2C%20K.%20Ng%2C%20A.%20Deep%20learning%20of%20invariant%20features%20via%20simulated%20fixations%20in%20video",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20W.%20Zhu%2C%20S.%20Yu%2C%20K.%20Ng%2C%20A.%20Deep%20learning%20of%20invariant%20features%20via%20simulated%20fixations%20in%20video"
        }
    ]
}
