{
    "filename": "7792-community-exploration-from-offline-optimization-to-online-learning.pdf",
    "metadata": {
        "title": "Community Exploration: From Offline Optimization to Online Learning",
        "author": "Xiaowei Chen, Weiran Huang, Wei Chen, John C. S. Lui",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7792-community-exploration-from-offline-optimization-to-online-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce the community exploration problem that has many real-world applications such as online advertising. In the problem, an explorer allocates limited budget to explore communities so as to maximize the number of members he could meet. We provide a systematic study of the community exploration problem, from offline optimization to online learning. For the offline setting where the sizes of communities are known, we prove that the greedy methods for both of non-adaptive exploration and adaptive exploration are optimal. For the online setting where the sizes of communities are not known and need to be learned from the multi-round explorations, we propose an \u201cupper confidence\u201d like algorithm that achieves the logarithmic regret bounds. By combining the feedback from different rounds, we can achieve a constant regret bound."
    },
    "keywords": [
        {
            "term": "online advertising",
            "url": "https://en.wikipedia.org/wiki/online_advertising"
        },
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "Multi-armed bandit",
            "url": "https://en.wikipedia.org/wiki/Multi-armed_bandit"
        },
        {
            "term": "online learning",
            "url": "https://en.wikipedia.org/wiki/online_learning"
        }
    ],
    "highlights": [
        "We introduce the community exploration problem, which is abstracted from many real-world applications",
        "We apply the multi-armed bandit (MAB) framework to this task, in which community explorations proceed in multiple rounds, and in each round we explore communities with a budget of K, use the feedback to learn about the community size, and adjust the exploration strategy in future rounds",
        "We consider the online learning problem that consists of T rounds, and during each round, we explore the communities with a budget K",
        "To differentiate the two settings, let\u2019s call the latter one the \u201cinteractive community exploration\u201d, while the former one the \u201crepeated community exploration\u201d. Both the repeated community exploration defined in this paper and the interactive community exploration we will study as the future work have corresponding applications",
        "The former is suitable for online advertising where in each round the advertiser promotes different products",
        "The latter corresponds to the adaptive online advertising for the same product, and the rewards in different rounds are dependent.\n5 Related Work"
    ],
    "key_statements": [
        "We introduce the community exploration problem, which is abstracted from many real-world applications",
        "We provide a systematic study of the above community exploration problem, from offline optimization to online learning",
        "We apply the multi-armed bandit (MAB) framework to this task, in which community explorations proceed in multiple rounds, and in each round we explore communities with a budget of K, use the feedback to learn about the community size, and adjust the exploration strategy in future rounds",
        "We provide theoretical regret bounds of O for both versions, where T is the number of rounds, which is asymptotically tight",
        "We consider the online learning problem that consists of T rounds, and during each round, we explore the communities with a budget K",
        "To differentiate the two settings, let\u2019s call the latter one the \u201cinteractive community exploration\u201d, while the former one the \u201crepeated community exploration\u201d. Both the repeated community exploration defined in this paper and the interactive community exploration we will study as the future work have corresponding applications",
        "The former is suitable for online advertising where in each round the advertiser promotes different products",
        "The latter corresponds to the adaptive online advertising for the same product, and the rewards in different rounds are dependent.\n5 Related Work"
    ],
    "summary": [
        "We introduce the community exploration problem, which is abstracted from many real-world applications.",
        "We provide a systematic study of the above community exploration problem, from offline optimization to online learning.",
        ", km) and a realization \u03c6, we define the reward R as the number of distinct members met, i.e., R(k, \u03c6) = The goal of the non-adaptive exploration m i=1 is to",
        "If we explore the communities with predetermined budget allocation in each round, the T -round regret of a learning algorithm A is defined as",
        "If we explore the communities adaptively in each round, the T -round regret of a learning algorithm A is defined as",
        "Algorithm 1 Non-Adaptive community exploration with optimal budget allocation",
        "Algorithm 2 Adaptive community exploration with greedy policy",
        "The adaptive community exploration with greedy policy is described in Algo.",
        "The following theorem shows that for our community exploration problem, our greedy policy is optimal.",
        "Let F\u03c0(\u03c8, t) denote the expected marginal gain when we further explore communities for t steps with policy \u03c0 starting from a partial realization \u03c8.",
        "The community exploration could either be non-adaptive or adaptive, and the following regret analysis separately discuss these two cases.",
        "We consider the scenario where the member identifiers are fixed over all rounds, and design an algorithm with a constant regret bound.",
        "3 with non-adaptive exploration method has a constant regret bound.",
        "The realization \u03c6 indicates that we will obtain a new member in the first di exploration of community Ci. Let Ui,k denote the number of times community Ci is selected by policy \u03c0g in the first k \u2212 1(k > m) steps under the special full realization \u03c6 we define previously.",
        "We consider the online learning problem that consists of T rounds, and during each round, we explore the communities with a budget K.",
        "Different from the setting defined in this paper, here a member will not contribute to the reward if it has been met in previous rounds.",
        "The result could be applied to our offline adaptive problem, but by an independent analysis we show the better result that the greedy policy is optimal.",
        "The non-adaptive community exploration problem in the online setting fits into the general combinatorial multi-armed bandit (CMAB) framework [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], where the reward is a set function of base arms.",
        "We propose the greedy methods for both of non-adaptive and adaptive exploration problems."
    ],
    "headline": "We introduce the community exploration problem that has many real-world applications such as online advertising",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235\u2013256, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicolo%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicolo%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "2",
            "entry": "[2] Donald A Berry and Bert Fristedt. Bandit problems: sequential allocation of experiments. Chapman and Hall, 5:71\u201387, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berry%2C%20Donald%20A.%20Fristedt%2C%20Bert%20Bandit%20problems%3A%20sequential%20allocation%20of%20experiments%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berry%2C%20Donald%20A.%20Fristedt%2C%20Bert%20Bandit%20problems%3A%20sequential%20allocation%20of%20experiments%201985"
        },
        {
            "id": "3",
            "entry": "[3] Marco Bressan, Enoch Peserico, and Luca Pretto. Simple set cardinality estimation through random sampling. arXiv preprint arXiv:1512.07901, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.07901"
        },
        {
            "id": "4",
            "entry": "[4] S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "5",
            "entry": "[5] S\u00e9bastien Bubeck, Damien Ernst, and Aur\u00e9lien Garivier. Optimal discovery with probabilistic expert advice: finite time analysis and macroscopic optimality. JMLR, 14(Feb):601\u2013623, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%A9bastien%20Bubeck%2C%20Damien%20Ernst%20Garivier%2C%20Aur%C3%A9lien%20Optimal%20discovery%20with%20probabilistic%20expert%20advice%3A%20finite%20time%20analysis%20and%20macroscopic%20optimality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%A9bastien%20Bubeck%2C%20Damien%20Ernst%20Garivier%2C%20Aur%C3%A9lien%20Optimal%20discovery%20with%20probabilistic%20expert%20advice%3A%20finite%20time%20analysis%20and%20macroscopic%20optimality%202013"
        },
        {
            "id": "6",
            "entry": "[6] Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. Combinatorial multi-armed bandit with general reward functions. In NIPS, pages 1659\u20131667, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Wei%20Hu%2C%20Wei%20Li%2C%20Fu%20Li%2C%20Jian%20Combinatorial%20multi-armed%20bandit%20with%20general%20reward%20functions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Wei%20Hu%2C%20Wei%20Li%2C%20Fu%20Li%2C%20Jian%20Combinatorial%20multi-armed%20bandit%20with%20general%20reward%20functions%202016"
        },
        {
            "id": "7",
            "entry": "[7] Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. Journal of Machine Learning Research, 17 (50):1\u201333, 2016. A preliminary version appeared as Chen, Wang, and Yuan, \u201cCombinatorial multi-armed bandit: General framework, results and applications\u201d, ICML\u20192013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Wei%20Wang%2C%20Yajun%20Yuan%2C%20Yang%20Wang%2C%20Qinshi%20Combinatorial%20multi-armed%20bandit%20and%20its%20extension%20to%20probabilistically%20triggered%20arms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Wei%20Wang%2C%20Yajun%20Yuan%2C%20Yang%20Wang%2C%20Qinshi%20Combinatorial%20multi-armed%20bandit%20and%20its%20extension%20to%20probabilistically%20triggered%20arms%202016"
        },
        {
            "id": "8",
            "entry": "[8] Mary C Christman and Tapan K Nayak. Sequential unbiased estimation of the number of classes in a population. Statistica Sinica, pages 335\u2013352, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christman%2C%20Mary%20C.%20Nayak%2C%20Tapan%20K.%20Sequential%20unbiased%20estimation%20of%20the%20number%20of%20classes%20in%20a%20population%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christman%2C%20Mary%20C.%20Nayak%2C%20Tapan%20K.%20Sequential%20unbiased%20estimation%20of%20the%20number%20of%20classes%20in%20a%20population%201994"
        },
        {
            "id": "9",
            "entry": "[9] Devdatt Dubhashi and Alessandro Panconesi. Concentration of Measure for the Analysis of Randomized Algorithms. Cambridge University Press, 1st edition, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dubhashi%2C%20Devdatt%20Panconesi%2C%20Alessandro%20Concentration%20of%20Measure%20for%20the%20Analysis%20of%20Randomized%20Algorithms%202009"
        },
        {
            "id": "10",
            "entry": "[10] Mark Finkelstein, Howard G. Tucker, and Jerry Alan Veeh. Confidence intervals for the number of unseen types. Statistics & Probability Letters, pages 423 \u2013 430, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finkelstein%2C%20Mark%20Tucker%2C%20Howard%20G.%20Veeh%2C%20Jerry%20Alan%20Confidence%20intervals%20for%20the%20number%20of%20unseen%20types%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finkelstein%2C%20Mark%20Tucker%2C%20Howard%20G.%20Veeh%2C%20Jerry%20Alan%20Confidence%20intervals%20for%20the%20number%20of%20unseen%20types%201998"
        },
        {
            "id": "11",
            "entry": "[11] Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, and S Muthukrishnan. Adaptive submodular maximization in bandit setting. In NIPS, pages 2697\u20132705, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gabillon%2C%20Victor%20Kveton%2C%20Branislav%20Wen%2C%20Zheng%20Eriksson%2C%20Brian%20Adaptive%20submodular%20maximization%20in%20bandit%20setting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gabillon%2C%20Victor%20Kveton%2C%20Branislav%20Wen%2C%20Zheng%20Eriksson%2C%20Brian%20Adaptive%20submodular%20maximization%20in%20bandit%20setting%202013"
        },
        {
            "id": "12",
            "entry": "[12] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Trans. Netw., 20(5):1466\u20131478, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gai%2C%20Yi%20Krishnamachari%2C%20Bhaskar%20Jain%2C%20Rahul%20Combinatorial%20network%20optimization%20with%20unknown%20variables%3A%20Multi-armed%20bandits%20with%20linear%20rewards%20and%20individual%20observations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gai%2C%20Yi%20Krishnamachari%2C%20Bhaskar%20Jain%2C%20Rahul%20Combinatorial%20network%20optimization%20with%20unknown%20variables%3A%20Multi-armed%20bandits%20with%20linear%20rewards%20and%20individual%20observations%202012"
        },
        {
            "id": "13",
            "entry": "[13] Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active learning and stochastic optimization. Journal of Artificial Intelligence Research, 42: 427\u2013486, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golovin%2C%20Daniel%20Krause%2C%20Andreas%20Adaptive%20submodularity%3A%20Theory%20and%20applications%20in%20active%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golovin%2C%20Daniel%20Krause%2C%20Andreas%20Adaptive%20submodularity%3A%20Theory%20and%20applications%20in%20active%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "14",
            "entry": "[14] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 58(301):13\u201330, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoeffding%2C%20Wassily%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoeffding%2C%20Wassily%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963"
        },
        {
            "id": "15",
            "entry": "[15] Svante Janson. Large deviations for sums of partly dependent random variables. Random Structures & Algorithms, 24(3):234\u2013248, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Janson%2C%20Svante%20Large%20deviations%20for%20sums%20of%20partly%20dependent%20random%20variables%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Janson%2C%20Svante%20Large%20deviations%20for%20sums%20of%20partly%20dependent%20random%20variables%202004"
        },
        {
            "id": "16",
            "entry": "[16] Liran Katzir, Edo Liberty, and Oren Somekh. Estimating sizes of social networks via biased sampling. In WWW, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katzir%2C%20Liran%20Liberty%2C%20Edo%20Somekh%2C%20Oren%20Estimating%20sizes%20of%20social%20networks%20via%20biased%20sampling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katzir%2C%20Liran%20Liberty%2C%20Edo%20Somekh%2C%20Oren%20Estimating%20sizes%20of%20social%20networks%20via%20biased%20sampling%202011"
        },
        {
            "id": "17",
            "entry": "[17] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Artificial Intelligence and Statistics, pages 535\u2013543, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kveton%2C%20Branislav%20Wen%2C%20Zheng%20Ashkan%2C%20Azin%20Szepesvari%2C%20Csaba%20Tight%20regret%20bounds%20for%20stochastic%20combinatorial%20semi-bandits%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kveton%2C%20Branislav%20Wen%2C%20Zheng%20Ashkan%2C%20Azin%20Szepesvari%2C%20Csaba%20Tight%20regret%20bounds%20for%20stochastic%20combinatorial%20semi-bandits%202015"
        },
        {
            "id": "18",
            "entry": "[18] Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Selected Papers, pages 169\u2013177.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20Herbert%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments"
        },
        {
            "id": "19",
            "entry": "[19] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "20",
            "entry": "[20] Qinshi Wang and Wei Chen. Improving regret bounds for combinatorial semi-bandits with probabilistically triggered arms and its applications. In NIPS, pages 1161\u20131171, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Qinshi%20Chen%2C%20Wei%20Improving%20regret%20bounds%20for%20combinatorial%20semi-bandits%20with%20probabilistically%20triggered%20arms%20and%20its%20applications%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Qinshi%20Chen%2C%20Wei%20Improving%20regret%20bounds%20for%20combinatorial%20semi-bandits%20with%20probabilistically%20triggered%20arms%20and%20its%20applications%202017"
        }
    ]
}
