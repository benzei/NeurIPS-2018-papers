{
    "filename": "7676-scalable-coordinated-exploration-in-concurrent-reinforcement-learning.pdf",
    "metadata": {
        "title": "Scalable Coordinated Exploration in Concurrent Reinforcement Learning",
        "author": "Maria Dimakopoulou, Ian Osband, Benjamin Van Roy",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7676-scalable-coordinated-exploration-in-concurrent-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling[<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and randomized value function learning [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]. We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]. With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes."
    },
    "keywords": [
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Consider a farm of robots operating concurrently, learning how to carry out a task, as studied in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Dimakopolou and Van Roy [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] recently identified properties that are essential to efficient coordinated exploration and proposed suitable tabular model learning methods based on seed sampling",
        "The problem we address is known as concurrent reinforcement learning [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "Figure 1 plots the reward achieved by the evaluation agent for increasing number of posterior sampling for reinforcement learning, seed sampling, concurrent UCRL and Thompson resampling agents operating in parallel in the cartpole environment",
        "The ability of the agents to make decisions in the high-dimensional environments of real systems, where the number of states is enormous or even infinite, is achieved through the value function representations, while coordinating the exploratory effort of the group of agents is achieved through the way that the seeding mechanism controls the fitting of these generalized representations",
        "We have extended the concept of seeding from the non-practical tabular representations to generalized representations and we have proposed an approach for designing scalable concurrent reinforcement learning algorithms that can intelligently coordinate the exploratory effort of agents learning in parallel in potentially enormous state spaces"
    ],
    "key_statements": [
        "Consider a farm of robots operating concurrently, learning how to carry out a task, as studied in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Dimakopolou and Van Roy [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] recently identified properties that are essential to efficient coordinated exploration and proposed suitable tabular model learning methods based on seed sampling",
        "The problem we address is known as concurrent reinforcement learning [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "Figure 1 plots the reward achieved by the evaluation agent for increasing number of posterior sampling for reinforcement learning, seed sampling, concurrent UCRL and Thompson resampling agents operating in parallel in the cartpole environment",
        "The ability of the agents to make decisions in the high-dimensional environments of real systems, where the number of states is enormous or even infinite, is achieved through the value function representations, while coordinating the exploratory effort of the group of agents is achieved through the way that the seeding mechanism controls the fitting of these generalized representations",
        "We have extended the concept of seeding from the non-practical tabular representations to generalized representations and we have proposed an approach for designing scalable concurrent reinforcement learning algorithms that can intelligently coordinate the exploratory effort of agents learning in parallel in potentially enormous state spaces"
    ],
    "summary": [
        "Consider a farm of robots operating concurrently, learning how to carry out a task, as studied in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "Figure 1 plots the reward achieved by the evaluation agent for increasing number of PSRL, seed sampling, concurrent UCRL and Thompson resampling agents operating in parallel in the cartpole environment.",
        "At time tk,m, before taking the mth action, agent k fits its generalized representation model on the history of observations perturbed by the noise seeds zk,j, j = 1, .",
        "The ability of the agents to make decisions in the high-dimensional environments of real systems, where the number of states is enormous or even infinite, is achieved through the value function representations, while coordinating the exploratory effort of the group of agents is achieved through the way that the seeding mechanism controls the fitting of these generalized representations.",
        "When each agent k draws initial parameter seed \u03b8k \u223c N (\u03b8, \u03bbI) and noise seeds zk,1, zk,2, \u00b7 \u00b7 \u00b7 \u223c N (0, v) at each time period it can solve this value-function optimization problem to obtain a posterior parameter sample based on the high-dimensional observations gathered by all agents so far.",
        "We use the concurrent reinforcement learning algorithm of Sections 3.2.2 and 3.2.3 with a neural network value function approximation and we see that our approach explores quickly and achieves high rewards.",
        "As explained in [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], concurrent UCRL and seed sampling are expected to perform in par because they exhibit commitment, but Thompson resampling is detrimental to exploration because resampling a MDP in every time period leads the agents to oscillation around the start vertex.",
        "As explained in [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], Thompson resampling and seed sampling are expected to perform in par because they diversify, but concurrent UCRL is wasteful of the exploratory effort of the agents, because it sends all the agents who have not left the source to the same chain with the most optimistic last edge reward.",
        "We have extended the concept of seeding from the non-practical tabular representations to generalized representations and we have proposed an approach for designing scalable concurrent reinforcement learning algorithms that can intelligently coordinate the exploratory effort of agents learning in parallel in potentially enormous state spaces.",
        "This approach allows the concurrent agents (1) to adapt to each other\u2019s high-dimensional observations via value function learning, (2) to diversify their experience collection via an intrinsic random seed that uniquely initializes each agent\u2019s generalized representation and uniquely interprets the common history of observations, (3) to commit to sequences of actions revealing useful information by maintaining each agent\u2019s seed constant throughout learning."
    ],
    "headline": "We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Maria Dimakopoulou and Benjamin Van Roy. Coordinated exploration in concurrent reinforcement learning. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dimakopoulou%2C%20Maria%20Roy%2C%20Benjamin%20Van%20Coordinated%20exploration%20in%20concurrent%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dimakopoulou%2C%20Maria%20Roy%2C%20Benjamin%20Van%20Coordinated%20exploration%20in%20concurrent%20reinforcement%20learning%202018"
        },
        {
            "id": "2",
            "entry": "[2] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "3",
            "entry": "[3] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In arXiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202016"
        },
        {
            "id": "4",
            "entry": "[4] Z. Guo and E. Brunskill. Concurrent PAC RL. In AAAI Conference on Artificial Intelligence, pages 2624\u20132630, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Z%20Guo%20and%20E%20Brunskill%20Concurrent%20PAC%20RL%20In%20AAAI%20Conference%20on%20Artificial%20Intelligence%20pages%2026242630%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Z%20Guo%20and%20E%20Brunskill%20Concurrent%20PAC%20RL%20In%20AAAI%20Conference%20on%20Artificial%20Intelligence%20pages%2026242630%202015"
        },
        {
            "id": "5",
            "entry": "[5] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11:1563\u20131600, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010"
        },
        {
            "id": "6",
            "entry": "[6] Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209\u2013232, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20Michael%20J.%20Singh%2C%20Satinder%20P.%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20Michael%20J.%20Singh%2C%20Satinder%20P.%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002"
        },
        {
            "id": "7",
            "entry": "[7] Michael Jong Kim. Thompson sampling for stochastic control: The finite parameter case. IEEE Transactions on Automatic Control, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Michael%20Jong%20Thompson%20sampling%20for%20stochastic%20control%3A%20The%20finite%20parameter%20case%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Michael%20Jong%20Thompson%20sampling%20for%20stochastic%20control%3A%20The%20finite%20parameter%20case%202017"
        },
        {
            "id": "8",
            "entry": "[8] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "9",
            "entry": "[9] Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning. arXiv preprint arXiv:1806.03335, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.03335"
        },
        {
            "id": "10",
            "entry": "[10] Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) efficient reinforcement learning via posterior sampling. In NIPS, pages 3003\u20133011. Curran Associates, Inc., 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20%28More%29%20efficient%20reinforcement%20learning%20via%20posterior%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20%28More%29%20efficient%20reinforcement%20learning%20via%20posterior%20sampling%202013"
        },
        {
            "id": "11",
            "entry": "[11] Ian Osband, Daniel Russo, Benjamin Van Roy, and Zheng Wen. Deep exploration via randomized value functions. arXiv preprint arXiv:1608.02731, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.02731"
        },
        {
            "id": "12",
            "entry": "[12] Ian Osband and Benjamin Van Roy. On optimistic versus randomized exploration in reinforcement learning. In The Multi-disciplinary Conference on Reinforcement Learning and Decision Making, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20On%20optimistic%20versus%20randomized%20exploration%20in%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20On%20optimistic%20versus%20randomized%20exploration%20in%20reinforcement%20learning%202017"
        },
        {
            "id": "13",
            "entry": "[13] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Why%20is%20posterior%20sampling%20better%20than%20optimism%20for%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Why%20is%20posterior%20sampling%20better%20than%20optimism%20for%20reinforcement%20learning%202017"
        },
        {
            "id": "14",
            "entry": "[14] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. In Proceedings of The 33rd International Conference on Machine Learning, pages 2377\u20132386, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Wen%2C%20Zheng%20Generalization%20and%20exploration%20via%20randomized%20value%20functions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Wen%2C%20Zheng%20Generalization%20and%20exploration%20via%20randomized%20value%20functions%202016"
        },
        {
            "id": "15",
            "entry": "[15] Jason Pazis and Ronald Parr. PAC optimal exploration in continuous space Markov decision processes. In AAAI. Citeseer, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pazis%2C%20Jason%20Parr%2C%20Ronald%20PAC%20optimal%20exploration%20in%20continuous%20space%20Markov%20decision%20processes.%20In%20AAAI%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pazis%2C%20Jason%20Parr%2C%20Ronald%20PAC%20optimal%20exploration%20in%20continuous%20space%20Markov%20decision%20processes.%20In%20AAAI%202013"
        },
        {
            "id": "16",
            "entry": "[16] Jason Pazis and Ronald Parr. Efficient pac-optimal exploration in concurrent, continuous state mdps with delayed updates. In AAAI. Citeseer, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pazis%2C%20Jason%20Parr%2C%20Ronald%20Efficient%20pac-optimal%20exploration%20in%20concurrent%2C%20continuous%20state%20mdps%20with%20delayed%20updates.%20In%20AAAI%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pazis%2C%20Jason%20Parr%2C%20Ronald%20Efficient%20pac-optimal%20exploration%20in%20concurrent%2C%20continuous%20state%20mdps%20with%20delayed%20updates.%20In%20AAAI%202016"
        },
        {
            "id": "17",
            "entry": "[17] Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. arXiv preprint arXiv:1707.02038, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02038"
        },
        {
            "id": "18",
            "entry": "[18] D. Silver, Barker Newnham, L, S. Weller, and J. McFall. Concurrent reinforcement learning from customer interactions. In Proceedings of The 30th International Conference on Machine Learning, pages 924\u2013932, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Barker%20Newnham%2C%20L.%20Weller%2C%20S.%20McFall%2C%20J.%20Concurrent%20reinforcement%20learning%20from%20customer%20interactions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Barker%20Newnham%2C%20L.%20Weller%2C%20S.%20McFall%2C%20J.%20Concurrent%20reinforcement%20learning%20from%20customer%20interactions%202013"
        },
        {
            "id": "19",
            "entry": "[19] Malcolm J. A. Strens. A Bayesian framework for reinforcement learning. In ICML, pages 943\u2013950, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strens%2C%20Malcolm%20J.A.%20A%20Bayesian%20framework%20for%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strens%2C%20Malcolm%20J.A.%20A%20Bayesian%20framework%20for%20reinforcement%20learning%202000"
        },
        {
            "id": "20",
            "entry": "[20] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20Barto%2C%20Andrew%20Reinforcement%20Learning%3A%20An%20Introduction%202017"
        },
        {
            "id": "21",
            "entry": "[21] Csaba Szepesv\u00e1ri. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesv%C3%A1ri%2C%20Csaba%20Algorithms%20for%20Reinforcement%20Learning.%20Synthesis%20Lectures%20on%20Artificial%20Intelligence%20and%20Machine%20Learning%202010"
        },
        {
            "id": "22",
            "entry": "[22] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1801.00690"
        }
    ]
}
