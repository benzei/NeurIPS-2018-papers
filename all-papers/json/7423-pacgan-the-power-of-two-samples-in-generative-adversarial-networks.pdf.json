{
    "filename": "7423-pacgan-the-power-of-two-samples-in-generative-adversarial-networks.pdf",
    "metadata": {
        "title": "PacGAN: The power of two samples in generative adversarial networks",
        "author": "Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7423-pacgan-the-power-of-two-samples-in-generative-adversarial-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generative adversarial networks (GANs) are a technique for learning generative models of complex data distributions from samples. Despite remarkable advances in generating realistic images, a major shortcoming of GANs is the fact that they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the focus of much recent work. We study a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We draw analysis tools from binary hypothesis testing\u2014in particular the seminal result of Blackwell [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]\u2014to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggest that packing provides significant improvements."
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "Pittsburgh Supercomputing Center",
            "url": "https://en.wikipedia.org/wiki/Pittsburgh_Supercomputing_Center"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Generative adversarial networks (GANs) are a technique for training generative models to produce realistic examples from an unknown data distribution [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "In data-driven generative modeling, this model is typically formulated as a function G : Rd \u2192 Rp that maps a low-dimensional code vector Z \u2208 Rd drawn from a standard distribution to a high-dimensional domain of interest",
        "A breakthrough in training such generative models was achieved by the innovative idea of Generative adversarial networks",
        "We demonstrate on benchmark datasets that PacGAN significantly improves upon competing approaches in mitigating mode collapse (Section 4), notably minibatch discrimination [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "Generative adversarial networks are typically trained with iterative generator-discriminator parameter updates, which can lead to non-convergence [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]\u2014a worse problem than mode collapse",
        "We propose a packing framework that theoretically and empirically mitigates mode collapse with low overhead"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs) are a technique for training generative models to produce realistic examples from an unknown data distribution [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "In data-driven generative modeling, this model is typically formulated as a function G : Rd \u2192 Rp that maps a low-dimensional code vector Z \u2208 Rd drawn from a standard distribution to a high-dimensional domain of interest",
        "A breakthrough in training such generative models was achieved by the innovative idea of Generative adversarial networks",
        "By viewing the discriminator as performing a binary hypothesis test on samples, we can apply classical hypothesis testing results to the analysis of Generative adversarial networks. This view leads to three contributions: (1) Conceptual: We propose a formal definition of mode collapse that abstracts away the geometric properties of the underlying data distributions (Section 3)",
        "(2) Analytical: Through the lens of hypothesis testing and mode collapse regions, we show that if the discriminator is allowed to see samples from the m-th order product distributions P m and Qm instead of the usual target distribution P and generator distribution Q, the corresponding loss when training the generator naturally penalizes generator distributions with strong mode collapse (Section 3)",
        "(3) Algorithmic: We propose a new Generative adversarial networks framework to mitigate mode collapse, which we call PacGAN",
        "We demonstrate on benchmark datasets that PacGAN significantly improves upon competing approaches in mitigating mode collapse (Section 4), notably minibatch discrimination [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "Generative adversarial networks are typically trained with iterative generator-discriminator parameter updates, which can lead to non-convergence [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]\u2014a worse problem than mode collapse",
        "In Appendix E, we show the proposed mode collapse region is equivalent to the ROC curve for binary hypothesis testing",
        "In Figure 4, we evaluate the number of modes recovered and reverse KL divergence for ALI, Generative adversarial networks, minibatch discrimination, and PacGAN, while varying the number of total parameters in each architecture",
        "We propose a packing framework that theoretically and empirically mitigates mode collapse with low overhead"
    ],
    "summary": [
        "Generative adversarial networks (GANs) are a technique for training generative models to produce realistic examples from an unknown data distribution [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>].",
        "By viewing the discriminator as performing a binary hypothesis test on samples, we can apply classical hypothesis testing results to the analysis of GANs. This view leads to three contributions: (1) Conceptual: We propose a formal definition of mode collapse that abstracts away the geometric properties of the underlying data distributions (Section 3).",
        "We provide a new interpretation of the pair of distributions (P, Q) as a two-dimensional region called the mode collapse region, where P is the true data distribution and Q the generated one.",
        "A generator trained with this type of discriminator will choose distributions that exhibit less mode collapse.",
        "GANs are typically trained with iterative generator-discriminator parameter updates, which can lead to non-convergence [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]\u2014a worse problem than mode collapse.",
        "The proposed approach can be applied to any existing GAN architecture and any loss function, as long as it uses a discriminator D(X) that classifies a single input sample.",
        "As in standard GANs, we train the packed discriminator with a bag of samples from the real data and the generator.",
        "The key insight of this work is that by instead considering product distributions, the total variation distance dT V (P m, Qm) varies in a way that is closely tied to the mode collapse regions for (P, Q).",
        "Distributions with strong mode collapse occupy the upper region, and will be penalized by a packed discriminator.",
        "We provide the optimal solution analytically and establish that mode-collapsing pairs occupy the upper part of the total variation region; that is, total variation increases rapidly as we pack more samples together (Figure 3, middle panel).",
        "We compare PacGAN to several baseline GAN architectures, some explicitly designed to mitigate mode collapse: GAN [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], minibatch discrimination (MD) [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], DCGAN [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], VEEGAN [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], Unrolled GANs [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], and ALI [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>].",
        "We use the exactly same choice of architecture, hyperparameters, and loss function as a baseline in each experiment; we change only the discriminator to accept packed samples.",
        "In Figure 4, we evaluate the number of modes recovered and reverse KL divergence for ALI, GAN, MD, and PacGAN, while varying the number of total parameters in each architecture.",
        "We replicate Table 2 from [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], which measured the number of observed modes in a generator trained on the stacked MNIST dataset, as well as the KL divergence of the generated mode distribution."
    ],
    "headline": "We study a principled approach to handling mode collapse, which we call packing",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "2",
            "entry": "[2] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00573"
        },
        {
            "id": "3",
            "entry": "[3] S. Arora and Y. Zhang. Do gans actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08224"
        },
        {
            "id": "4",
            "entry": "[4] D. Blackwell. Equivalent comparisons of experiments. The Annals of Mathematical Statistics, 24(2):265\u2013272, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blackwell%2C%20D.%20Equivalent%20comparisons%20of%20experiments%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blackwell%2C%20D.%20Equivalent%20comparisons%20of%20experiments%201953"
        },
        {
            "id": "5",
            "entry": "[5] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.02136"
        },
        {
            "id": "6",
            "entry": "[6] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pages 3844\u20133852, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defferrard%2C%20M.%20Bresson%2C%20X.%20Vandergheynst%2C%20P.%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defferrard%2C%20M.%20Bresson%2C%20X.%20Vandergheynst%2C%20P.%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016"
        },
        {
            "id": "7",
            "entry": "[7] J. Donahue, P. Kr\u00e4henb\u00fchl, and T. Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09782"
        },
        {
            "id": "8",
            "entry": "[8] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "9",
            "entry": "[9] I. Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "10",
            "entry": "[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "11",
            "entry": "[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "12",
            "entry": "[12] P. Kairouz, S. Oh, and P. Viswanath. The composition theorem for differential privacy. IEEE Transactions on Information Theory, 63(6):4037\u20134049, June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kairouz%2C%20P.%20Oh%2C%20S.%20Viswanath%2C%20P.%20The%20composition%20theorem%20for%20differential%20privacy%202017-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kairouz%2C%20P.%20Oh%2C%20S.%20Viswanath%2C%20P.%20The%20composition%20theorem%20for%20differential%20privacy%202017-06"
        },
        {
            "id": "13",
            "entry": "[13] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "14",
            "entry": "[14] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "15",
            "entry": "[15] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.02907"
        },
        {
            "id": "16",
            "entry": "[16] Y. LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "17",
            "entry": "[17] J. Li, A. Madry, J. Peebles, and L. Schmidt. Towards understanding the dynamics of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.09884"
        },
        {
            "id": "18",
            "entry": "[18] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and convergence properties of generative adversarial learning. arXiv preprint arXiv:1705.08991, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08991"
        },
        {
            "id": "19",
            "entry": "[19] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Z.%20Luo%2C%20P.%20Wang%2C%20X.%20Tang%2C%20X.%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Z.%20Luo%2C%20P.%20Wang%2C%20X.%20Tang%2C%20X.%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "20",
            "entry": "[20] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02163"
        },
        {
            "id": "21",
            "entry": "[21] T. Nguyen, T. Le, H. Vu, and D. Phung. Dual discriminator generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2667\u20132677, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20T.%20Le%2C%20T.%20Vu%2C%20H.%20Phung%2C%20D.%20Dual%20discriminator%20generative%20adversarial%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20T.%20Le%2C%20T.%20Vu%2C%20H.%20Phung%2C%20D.%20Dual%20discriminator%20generative%20adversarial%20nets%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "23",
            "entry": "[23] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.05396"
        },
        {
            "id": "24",
            "entry": "[24] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "25",
            "entry": "[25] A. Srivastava, L. Valkov, C. Russell, M. Gutmann, and C. Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. arXiv preprint arXiv:1705.07761, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07761"
        },
        {
            "id": "26",
            "entry": "[26] K. K. Thekumparampil, C. Wang, S. Oh, and L.-J. Li. Attention-based graph neural network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03735"
        },
        {
            "id": "27",
            "entry": "[27] I. Tolstikhin, S. Gelly, O. Bousquet, C.-J. Simon-Gabriel, and B. Sch\u00f6lkopf. Adagan: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.02386"
        },
        {
            "id": "28",
            "entry": "[28] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. In Advances in Neural Information Processing Systems, pages 3391\u20133401, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20Poczos%2C%20B.%20Deep%20sets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20Poczos%2C%20B.%20Deep%20sets%202017"
        }
    ]
}
