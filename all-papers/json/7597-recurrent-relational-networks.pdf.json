{
    "filename": "7597-recurrent-relational-networks.pdf",
    "metadata": {
        "title": "Recurrent Relational Networks",
        "author": "Rasmus Palm, Ulrich Paquet, Ole Winther",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7597-recurrent-relational-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper is concerned with learning to solve tasks that require a chain of interdependent steps of relational inference, like answering complex questions about the relationships between objects, or solving puzzles where the smaller elements of a solution mutually constrain each other. We introduce the recurrent relational network, a general purpose module that operates on a graph representation of objects. As a generalization of Santoro et al. [2017]\u2019s relational network, it can augment any neural network model with the capacity to do many-step relational reasoning. We achieve state of the art results on the bAbI textual question-answering dataset with the recurrent relational network, consistently solving 20/20 tasks. As bAbI is not particularly challenging from a relational reasoning point of view, we introduce Pretty-CLEVR, a new diagnostic dataset for relational reasoning. In the PrettyCLEVR set-up, we can vary the question to control for the number of relational reasoning steps that are required to obtain the answer. Using Pretty-CLEVR, we probe the limitations of multi-layer perceptrons, relational and recurrent relational networks. Finally, we show how recurrent relational networks can learn to solve Sudoku puzzles from supervised training data, a challenging task requiring upwards of 64 steps of relational reasoning. We achieve state-of-the-art results amongst comparable methods by solving 96.6% of the hardest Sudoku puzzles."
    },
    "keywords": [
        {
            "term": "multilayer perceptron",
            "url": "https://en.wikipedia.org/wiki/multilayer_perceptron"
        },
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "relational network",
            "url": "https://en.wikipedia.org/wiki/relational_network"
        },
        {
            "term": "convolutional neural net",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_net"
        },
        {
            "term": "neural network model",
            "url": "https://en.wikipedia.org/wiki/neural_network_model"
        }
    ],
    "highlights": [
        "A central component of human intelligence is the ability to abstractly reason about objects and their interactions [<a class=\"ref-link\" id=\"cSpelke_et+al_1995_a\" href=\"#rSpelke_et+al_1995_a\"><a class=\"ref-link\" id=\"cSpelke_et+al_1995_a\" href=\"#rSpelke_et+al_1995_a\">Spelke et al, 1995</a></a>, <a class=\"ref-link\" id=\"cSpelke_2007_a\" href=\"#rSpelke_2007_a\"><a class=\"ref-link\" id=\"cSpelke_2007_a\" href=\"#rSpelke_2007_a\">Spelke and Kinzler, 2007</a></a>]",
        "Toward generally realizing the ability to methodically reason about objects and their interactions over many steps, this paper introduces a composite function, the recurrent relational network",
        "We show that it is a powerful architecture for many-step relational reasoning on three varied datasets, achieving state-of-the-art results on bAbI and Sudoku",
        "The graph is the input to the relational reasoning module, and vectors xi would generally be the output of a perceptual front-end, for instance a convolutional neural network",
        "We have proposed a general relational reasoning model for solving tasks requiring an order of magnitude more complex relational reasoning than the current state-of-the art",
        "Our relational reasoning module can be added to any deep learning model to add a powerful relational reasoning capacity"
    ],
    "key_statements": [
        "A central component of human intelligence is the ability to abstractly reason about objects and their interactions [<a class=\"ref-link\" id=\"cSpelke_et+al_1995_a\" href=\"#rSpelke_et+al_1995_a\"><a class=\"ref-link\" id=\"cSpelke_et+al_1995_a\" href=\"#rSpelke_et+al_1995_a\">Spelke et al, 1995</a></a>, <a class=\"ref-link\" id=\"cSpelke_2007_a\" href=\"#rSpelke_2007_a\"><a class=\"ref-link\" id=\"cSpelke_2007_a\" href=\"#rSpelke_2007_a\">Spelke and Kinzler, 2007</a></a>]",
        "Toward generally realizing the ability to methodically reason about objects and their interactions over many steps, this paper introduces a composite function, the recurrent relational network",
        "We show that it is a powerful architecture for many-step relational reasoning on three varied datasets, achieving state-of-the-art results on bAbI and Sudoku",
        "The graph is the input to the relational reasoning module, and vectors xi would generally be the output of a perceptual front-end, for instance a convolutional neural network",
        "To train a recurrent relational network in a supervised manner to solve a Sudoku we introduce an output probability distribution over the digits 1-9 for each of the nodes in the graph",
        "We have proposed a general relational reasoning model for solving tasks requiring an order of magnitude more complex relational reasoning than the current state-of-the art",
        "Our relational reasoning module can be added to any deep learning model to add a powerful relational reasoning capacity"
    ],
    "summary": [
        "A central component of human intelligence is the ability to abstractly reason about objects and their interactions [<a class=\"ref-link\" id=\"cSpelke_et+al_1995_a\" href=\"#rSpelke_et+al_1995_a\"><a class=\"ref-link\" id=\"cSpelke_et+al_1995_a\" href=\"#rSpelke_et+al_1995_a\">Spelke et al, 1995</a></a>, <a class=\"ref-link\" id=\"cSpelke_2007_a\" href=\"#rSpelke_2007_a\"><a class=\"ref-link\" id=\"cSpelke_2007_a\" href=\"#rSpelke_2007_a\">Spelke and Kinzler, 2007</a></a>].",
        "Toward generally realizing the ability to methodically reason about objects and their interactions over many steps, this paper introduces a composite function, the recurrent relational network.",
        "This paper considers many-step relational reasoning, a challenging task for deep learning architectures.",
        "We show that it is a powerful architecture for many-step relational reasoning on three varied datasets, achieving state-of-the-art results on bAbI and Sudoku.",
        "The recurrent relational network will learn to pass messages on a graph.",
        "The graph is the input to the relational reasoning module, and vectors xi would generally be the output of a perceptual front-end, for instance a convolutional neural network.",
        "To train a recurrent relational network in a supervised manner to solve a Sudoku we introduce an output probability distribution over the digits 1-9 for each of the nodes in the graph.",
        "Since the target digits yi are constant over the steps, it encourages the network to learn a convergent message passing algorithm.",
        "We sum the node hidden states and pass that through a MLP to get a single output for the whole graph.",
        "We find that we only need a single step of relational reasoning to solve all the bAbI tasks.",
        "The model learns to compress all the relevant fact-relations into the 128 floats resulting from the sum over the node hidden states, and perform the remaining reasoning steps in the output MLP.",
        "Our network learns to solve 94.1% of even the hardest 17-givens Sudokus after 32 steps.",
        "The node centric correspond exactly to our proposed network with a single step, yet fails to solve any Sudoku.",
        "We have proposed a general relational reasoning model for solving tasks requiring an order of magnitude more complex relational reasoning than the current state-of-the art.",
        "We markedly improve state-of-the-art on the BaBi dataset solving 20/20 tasks in 13 out of 15 runs with a single model trained jointly on all tasks.",
        "In order to solve the tasks that require more than a single step it must compress all the relevant relations into a fixed size vector, perform the remaining relational reasoning in the last forward layers.",
        "<a class=\"ref-link\" id=\"cRoss_et+al_2011_a\" href=\"#rRoss_et+al_2011_a\">Ross et al [2011</a>] trains the inference machine predictors on every step, but there are no hidden states; the node states are the output marginals directly, similar to how belief propagation works.",
        "Inference Machines which ditch the belief propagation algorithm altogether and instead train a series of regressors to output the correct marginals by passing messages on a graph."
    ],
    "headline": "We introduce the recurrent relational network, a general purpose module that operates on a graph representation of objects",
    "reference_links": [
        {
            "id": "Amos_2017_a",
            "entry": "Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. arXiv preprint arXiv:1703.00443, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00443"
        },
        {
            "id": "Battaglia_et+al_2016_a",
            "entry": "Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems, pages 4502\u20134510, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglia%2C%20Peter%20Pascanu%2C%20Razvan%20Lai%2C%20Matthew%20Rezende%2C%20Danilo%20Jimenez%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%20and%20physics%202016"
        },
        {
            "id": "Bauke_2008_a",
            "entry": "Heiko Bauke. Passing messages to lonely numbers. Computing in Science & Engineering, 10(2): 32\u201340, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauke%2C%20Heiko%20Passing%20messages%20to%20lonely%20numbers%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bauke%2C%20Heiko%20Passing%20messages%20to%20lonely%20numbers%202008"
        },
        {
            "id": "Tarek_2017_a",
            "entry": "Tarek R Besold, Artur d\u2019Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe K\u00fchnberger, Luis C Lamb, Daniel Lowd, Priscila Machado Vieira Lima, et al. Neural-symbolic learning and reasoning: A survey and interpretation. arXiv preprint arXiv:1711.03902, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03902"
        },
        {
            "id": "Deng_et+al_2016_a",
            "entry": "Zhiwei Deng, Arash Vahdat, Hexiang Hu, and Greg Mori. Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4772\u20134781, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Zhiwei%20Vahdat%2C%20Arash%20Hu%2C%20Hexiang%20Mori%2C%20Greg%20Structure%20inference%20machines%3A%20Recurrent%20neural%20networks%20for%20analyzing%20relations%20in%20group%20activity%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Zhiwei%20Vahdat%2C%20Arash%20Hu%2C%20Hexiang%20Mori%2C%20Greg%20Structure%20inference%20machines%3A%20Recurrent%20neural%20networks%20for%20analyzing%20relations%20in%20group%20activity%20recognition%202016"
        },
        {
            "id": "Gilmer_et+al_2017_a",
            "entry": "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01212"
        },
        {
            "id": "Heess_et+al_2013_a",
            "entry": "Nicolas Heess, Daniel Tarlow, and John Winn. Learning to pass expectation propagation messages. In Advances in Neural Information Processing Systems, pages 3219\u20133227, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20Nicolas%20Tarlow%2C%20Daniel%20Winn%2C%20John%20Learning%20to%20pass%20expectation%20propagation%20messages%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20Nicolas%20Tarlow%2C%20Daniel%20Winn%2C%20John%20Learning%20to%20pass%20expectation%20propagation%20messages%202013"
        },
        {
            "id": "Henaff_et+al_2016_a",
            "entry": "Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03969"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Khan_et+al_2014_a",
            "entry": "Sheehan Khan, Shahab Jabbari, Shahin Jabbari, and Majid Ghanbarinejad. Solving Sudoku using probabilistic graphical models. 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20Sheehan%20Jabbari%2C%20Shahab%20Jabbari%2C%20Shahin%20Ghanbarinejad%2C%20Majid%20Solving%20Sudoku%20using%20probabilistic%20graphical%20models%202014"
        },
        {
            "id": "Knuth_2000_a",
            "entry": "Donald E Knuth. Dancing links. arXiv preprint cs/0011047, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Knuth%2C%20Donald%20E.%20Dancing%20links%202000"
        },
        {
            "id": "Lake_et+al_2016_a",
            "entry": "Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, pages 1\u2013101, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202016"
        },
        {
            "id": "Lin_et+al_2015_a",
            "entry": "Guosheng Lin, Chunhua Shen, Ian Reid, and Anton van den Hengel. Deeply learning the messages in message passing inference. In Advances in Neural Information Processing Systems, pages 361\u2013369, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Guosheng%20Shen%2C%20Chunhua%20Reid%2C%20Ian%20van%20den%20Hengel%2C%20Anton%20Deeply%20learning%20the%20messages%20in%20message%20passing%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Guosheng%20Shen%2C%20Chunhua%20Reid%2C%20Ian%20van%20den%20Hengel%2C%20Anton%20Deeply%20learning%20the%20messages%20in%20message%20passing%20inference%202015"
        },
        {
            "id": "Mcculloch_1943_a",
            "entry": "Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115\u2013133, 1943.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCulloch%2C%20Warren%20S.%20Pitts%2C%20Walter%20A%20logical%20calculus%20of%20the%20ideas%20immanent%20in%20nervous%20activity%201943",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCulloch%2C%20Warren%20S.%20Pitts%2C%20Walter%20A%20logical%20calculus%20of%20the%20ideas%20immanent%20in%20nervous%20activity%201943"
        },
        {
            "id": "Murphy_et+al_1999_a",
            "entry": "Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 467\u2013475. Morgan Kaufmann Publishers Inc., 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murphy%2C%20Kevin%20P.%20Weiss%2C%20Yair%20Jordan%2C%20Michael%20I.%20Loopy%20belief%20propagation%20for%20approximate%20inference%3A%20An%20empirical%20study%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murphy%2C%20Kevin%20P.%20Weiss%2C%20Yair%20Jordan%2C%20Michael%20I.%20Loopy%20belief%20propagation%20for%20approximate%20inference%3A%20An%20empirical%20study%201999"
        },
        {
            "id": "Norvig_2006_a",
            "entry": "Peter Norvig. Solving every Sudoku puzzle, 2006. URL http://norvig.com/sudoku.html.",
            "url": "http://norvig.com/sudoku.html"
        },
        {
            "id": "Park_2016_a",
            "entry": "Kyubyong Park. Can neural networks crack Sudoku?, 2016. URL https://github.com/ Kyubyong/sudoku.",
            "url": "https://github.com/Kyubyong/sudoku"
        },
        {
            "id": "Rae_et+al_2016_a",
            "entry": "Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves, and Tim Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, pages 3621\u20133629, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rae%2C%20Jack%20Hunt%2C%20Jonathan%20J.%20Danihelka%2C%20Ivo%20Harley%2C%20Timothy%20Scaling%20memory-augmented%20neural%20networks%20with%20sparse%20reads%20and%20writes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rae%2C%20Jack%20Hunt%2C%20Jonathan%20J.%20Danihelka%2C%20Ivo%20Harley%2C%20Timothy%20Scaling%20memory-augmented%20neural%20networks%20with%20sparse%20reads%20and%20writes%202016"
        },
        {
            "id": "Raedt_et+al_2016_a",
            "entry": "Luc De Raedt, Kristian Kersting, Sriraam Natarajan, and David Poole. Statistical relational artificial intelligence: Logic, probability, and computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, 10(2):1\u2013189, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raedt%2C%20Luc%20De%20Kersting%2C%20Kristian%20Natarajan%2C%20Sriraam%20Poole%2C%20David%20Statistical%20relational%20artificial%20intelligence%3A%20Logic%2C%20probability%2C%20and%20computation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raedt%2C%20Luc%20De%20Kersting%2C%20Kristian%20Natarajan%2C%20Sriraam%20Poole%2C%20David%20Statistical%20relational%20artificial%20intelligence%3A%20Logic%2C%20probability%2C%20and%20computation%202016"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "Stephane Ross, Daniel Munoz, Martial Hebert, and J Andrew Bagnell. Learning message-passing inference machines for structured prediction. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 2737\u20132744. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Stephane%20Munoz%2C%20Daniel%20Hebert%2C%20Martial%20Bagnell%2C%20J.Andrew%20Learning%20message-passing%20inference%20machines%20for%20structured%20prediction%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20Stephane%20Munoz%2C%20Daniel%20Hebert%2C%20Martial%20Bagnell%2C%20J.Andrew%20Learning%20message-passing%20inference%20machines%20for%20structured%20prediction%202011"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv preprint arXiv:1706.01427, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01427"
        },
        {
            "id": "Scarselli_et+al_2009_a",
            "entry": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "Serafini_2016_a",
            "entry": "Luciano Serafini and Artur S d\u2019Avila Garcez. Learning and reasoning with logic tensor networks. In AI* IA 2016 Advances in Artificial Intelligence, pages 334\u2013348.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serafini%2C%20Luciano%20d%E2%80%99Avila%20Garcez%2C%20Artur%20S.%20Learning%20and%20reasoning%20with%20logic%20tensor%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Serafini%2C%20Luciano%20d%E2%80%99Avila%20Garcez%2C%20Artur%20S.%20Learning%20and%20reasoning%20with%20logic%20tensor%20networks%202016"
        },
        {
            "id": "Ourek_et+al_2015_a",
            "entry": "Gustav \u0160ourek, Vojtech Aschenbrenner, Filip \u017delezny, and Ondrej Ku\u017eelka. Lifted relational neural networks. In Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches-Volume 1583, pages 52\u201360. CEUR-WS. org, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C5%A0ourek%2C%20Gustav%20Aschenbrenner%2C%20Vojtech%20%C5%BDelezny%2C%20Filip%20Ku%C5%BEelka%2C%20Ondrej%20Lifted%20relational%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=%C5%A0ourek%2C%20Gustav%20Aschenbrenner%2C%20Vojtech%20%C5%BDelezny%2C%20Filip%20Ku%C5%BEelka%2C%20Ondrej%20Lifted%20relational%20neural%20networks%202015"
        },
        {
            "id": "Spelke_2007_a",
            "entry": "Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89\u201396, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spelke%2C%20Elizabeth%20S.%20Kinzler%2C%20Katherine%20D.%20Core%20knowledge%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spelke%2C%20Elizabeth%20S.%20Kinzler%2C%20Katherine%20D.%20Core%20knowledge%202007"
        },
        {
            "id": "Spelke_et+al_1995_a",
            "entry": "Elizabeth S Spelke, Grant Gutheil, and Gretchen Van de Walle. The development of object perception. 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spelke%2C%20Elizabeth%20S.%20Gutheil%2C%20Grant%20de%20Walle%2C%20Gretchen%20Van%20The%20development%20of%20object%20perception%201995"
        },
        {
            "id": "Sukhbaatar_et+al_2015_a",
            "entry": "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pages 2440\u20132448, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015"
        },
        {
            "id": "Sukhbaatar_2016_a",
            "entry": "Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pages 2244\u20132252, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016"
        },
        {
            "id": "Wei_et+al_2016_a",
            "entry": "Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4724\u20134732, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Shih-En%20Ramakrishna%2C%20Varun%20Kanade%2C%20Takeo%20Sheikh%2C%20Yaser%20Convolutional%20pose%20machines%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Shih-En%20Ramakrishna%2C%20Varun%20Kanade%2C%20Takeo%20Sheikh%2C%20Yaser%20Convolutional%20pose%20machines%202016"
        },
        {
            "id": "Weston_et+al_2015_a",
            "entry": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merri\u00ebnboer, Armand Joulin, and Tomas Mikolov. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05698"
        },
        {
            "id": "Yang_2018_a",
            "entry": "Hyochang Yang, Sungzoon Cho, et al. Finding remo (related memory object): A simple neural architecture for text based reasoning. arXiv preprint arXiv:1801.08459, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.08459"
        }
    ]
}
