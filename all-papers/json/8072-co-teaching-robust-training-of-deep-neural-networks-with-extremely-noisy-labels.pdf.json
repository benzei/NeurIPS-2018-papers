{
    "filename": "8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf",
    "metadata": {
        "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
        "author": "Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, Masashi Sugiyama",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called \u201cCo-teaching\u201d for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models."
    },
    "keywords": [
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "clean label",
            "url": "https://en.wikipedia.org/wiki/clean_label"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Learning from noisy labels can date back to three decades ago [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], and still keeps vibrant in recent years [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "Noisy labels are corrupted from ground-truth labels, and they inevitably degenerate the robustness of learned models, especially for deep neural networks [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]",
        "As all deep learning training methods are based on stochastic gradient descent, our Co-teaching works in a mini-batch manner",
        "Since this paper mainly focuses on the robustness of our Co-teaching on extremely noisy supervision, the noise rate is chosen from {0.45, 0.5}",
        "This paper presents a simple but effective learning paradigm called Co-teaching, which trains deep neural networks robustly under noisy supervision",
        "Previous theories for Co-training are very hard to transfer into Co-teaching, since our setting is fundamentally different"
    ],
    "key_statements": [
        "Learning from noisy labels can date back to three decades ago [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], and still keeps vibrant in recent years [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "Noisy labels are corrupted from ground-truth labels, and they inevitably degenerate the robustness of learned models, especially for deep neural networks [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]",
        "We propose a simple but effective learning paradigm called \u201cCo-teaching\u201d, which allows us to train deep networks robustly even with extremely noisy labels (e.g., 45% of noisy labels occur in the fine-grained classification with multiple classes [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>])",
        "Under extremely noisy circumstances (i.e., 45% of noisy labels), the robustness of deep learning models trained by the Co-teaching approach is much superior to state-of-the-art baselines",
        "Under low-level noisy circumstances (i.e., 20% of noisy labels), the robustness of deep learning models trained by the Co-teaching approach is still superior to most baselines.\n2 Related literature",
        "As all deep learning training methods are based on stochastic gradient descent, our Co-teaching works in a mini-batch manner",
        "Since this paper mainly focuses on the robustness of our Co-teaching on extremely noisy supervision, the noise rate is chosen from {0.45, 0.5}",
        "Table 4 reports the accuracy on the testing set",
        "When noisy rate raises to 50%, Standard, Bootstrap, S-model and F-correction fail, and their accuracy decrease lower than 80%",
        "Test accuracy is shown in Table 5",
        "F-correction is the best, and our Co-teaching is comparable with F-correction",
        "The test accuracy is in Table 6",
        "Test accuracy and label precision vs. number of epochs are in Figure 6",
        "This paper presents a simple but effective learning paradigm called Co-teaching, which trains deep neural networks robustly under noisy supervision",
        "Previous theories for Co-training are very hard to transfer into Co-teaching, since our setting is fundamentally different"
    ],
    "summary": [
        "Learning from noisy labels can date back to three decades ago [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], and still keeps vibrant in recent years [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>].",
        "These works try to select clean instances out of the noisy ones, and use them to update the network.",
        "In Co-teaching, since two networks have different learning abilities, they can filter different types of error introduced by noisy labels.",
        "Under extremely noisy circumstances (i.e., 45% of noisy labels), the robustness of deep learning models trained by the Co-teaching approach is much superior to state-of-the-art baselines.",
        "Under low-level noisy circumstances (i.e., 20% of noisy labels), the robustness of deep learning models trained by the Co-teaching approach is still superior to most baselines.",
        "As in Figure 1, in each mini-batch data, each network selects its small-loss instances as the useful knowledge, and teaches such useful instances to its peer network for the further training.",
        "As all deep learning training methods are based on stochastic gradient descent, our Co-teaching works in a mini-batch manner.",
        "If we train our classifier only using small-loss instances in each mini-bach data, it should be resistant to noisy labels.",
        "On noisy data sets, even with the existence of noisy labels, deep networks will learn clean and easy pattern in the initial epochs [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>].",
        "Bootstrap S-model F-correction Decoupling MentorNet Co-teaching large class heavy noise flexibility no pre-train have wrong labels that cannot be learned without additional assumptions.",
        "We compare Co-teaching with the standard deep networks trained on noisy datasets.",
        "Our Co-teaching method does not rely on any specific network architectures, which can deal with a large number of classes and is more robust to noise.",
        "Higher label precision means less noisy instances in the mini-batch after sample selection, and the algorithm with higher label precision is more robust to the label noise.",
        "MentorNet, Decoupling and Co-teaching are considered here, as they are methods do instance selection during training.",
        "All methods, except MentorNet and Co-teaching, fail on harder, i.e., Pair-45% and Symmetry-50% cases.",
        "On test accuracy, we can see Co-teaching strongly hinders neural networks from memorizing noisy labels.",
        "While Decoupling fails to find clean instances, both MentorNet and Co-teaching can do this.",
        "This paper presents a simple but effective learning paradigm called Co-teaching, which trains deep neural networks robustly under noisy supervision.",
        "We conduct simulated experiments to demonstrate that, our proposed Co-teaching can train deep models robustly with the extremely noisy supervision.",
        "We leave the generalization analysis as a future work"
    ],
    "headline": "We propose a new deep learning paradigm called \u201cCo-teaching\u201d for combating with noisy labels",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2(4):343\u2013370, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angluin%2C%20D.%20Laird%2C%20P.%20Learning%20from%20noisy%20examples%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angluin%2C%20D.%20Laird%2C%20P.%20Learning%20from%20noisy%20examples%201988"
        },
        {
            "id": "2",
            "entry": "[2] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. Kanwal, T. Maharaj, A. Fischer, A. Courville, and Y. Bengio. A closer look at memorization in deep networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arpit%2C%20D.%20Jastrzebski%2C%20S.%20Ballas%2C%20N.%20Krueger%2C%20D.%20A%20closer%20look%20at%20memorization%20in%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arpit%2C%20D.%20Jastrzebski%2C%20S.%20Ballas%2C%20N.%20Krueger%2C%20D.%20A%20closer%20look%20at%20memorization%20in%20deep%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] M. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. Journal of Computer and System Sciences, 75(1):78\u201389, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202009"
        },
        {
            "id": "4",
            "entry": "[4] A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statistical query model. Journal of the ACM, 50(4):506\u2013519, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Kalai%2C%20A.%20Wasserman%2C%20H.%20Noise-tolerant%20learning%2C%20the%20parity%20problem%2C%20and%20the%20statistical%20query%20model%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20A.%20Kalai%2C%20A.%20Wasserman%2C%20H.%20Noise-tolerant%20learning%2C%20the%20parity%20problem%2C%20and%20the%20statistical%20query%20model%202003"
        },
        {
            "id": "5",
            "entry": "[5] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Mitchell%2C%20T.%20Combining%20labeled%20and%20unlabeled%20data%20with%20co-training%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20A.%20Mitchell%2C%20T.%20Combining%20labeled%20and%20unlabeled%20data%20with%20co-training%201998"
        },
        {
            "id": "6",
            "entry": "[6] O. Chapelle, B. Scholkopf, and A. Zien. Semi-supervised learning. IEEE Transactions on Neural Networks, 20(3):542\u2013542, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20O.%20Scholkopf%2C%20B.%20Zien%2C%20A.%20Semi-supervised%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20O.%20Scholkopf%2C%20B.%20Zien%2C%20A.%20Semi-supervised%20learning%202009"
        },
        {
            "id": "7",
            "entry": "[7] D. Cohn, Z. Ghahramani, and M. Jordan. Active learning with statistical models. Journal of Artificial Intelligence Research, 4:129\u2013145, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohn%2C%20D.%20Ghahramani%2C%20Z.%20Jordan%2C%20M.%20Active%20learning%20with%20statistical%20models%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohn%2C%20D.%20Ghahramani%2C%20Z.%20Jordan%2C%20M.%20Active%20learning%20with%20statistical%20models%201996"
        },
        {
            "id": "8",
            "entry": "[8] J. Deng, J. Krause, and L. Fei-Fei. Fine-grained crowdsourcing for fine-grained recognition. In CVPR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Krause%2C%20J.%20Fei-Fei%2C%20L.%20Fine-grained%20crowdsourcing%20for%20fine-grained%20recognition%202013"
        },
        {
            "id": "9",
            "entry": "[9] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "10",
            "entry": "[10] Y. Fan, F. Tian, T. Qin, J. Bian, and T. Liu. Learning to teach. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Y.%20Tian%2C%20F.%20Qin%2C%20T.%20Bian%2C%20J.%20Learning%20to%20teach%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Y.%20Tian%2C%20F.%20Qin%2C%20T.%20Bian%2C%20J.%20Learning%20to%20teach%202018"
        },
        {
            "id": "11",
            "entry": "[11] Y. Freund and R. Schapire. A desicion-theoretic generalization of on-line learning and an application to boosting. In European COLT, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Y.%20Schapire%2C%20R.%20A%20desicion-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Y.%20Schapire%2C%20R.%20A%20desicion-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201995"
        },
        {
            "id": "12",
            "entry": "[12] Y. Freund, R. Schapire, and N. Abe. A short introduction to boosting. Journal-Japanese Society For Artificial Intelligence, 14(771-780):1612, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Y.%20Schapire%2C%20R.%20Abe%2C%20N.%20A%20short%20introduction%20to%20boosting%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Y.%20Schapire%2C%20R.%20Abe%2C%20N.%20A%20short%20introduction%20to%20boosting%201999"
        },
        {
            "id": "13",
            "entry": "[13] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberger%2C%20J.%20Ben-Reuven%2C%20E.%20Training%20deep%20neural-networks%20using%20a%20noise%20adaptation%20layer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldberger%2C%20J.%20Ben-Reuven%2C%20E.%20Training%20deep%20neural-networks%20using%20a%20noise%20adaptation%20layer%202017"
        },
        {
            "id": "14",
            "entry": "[14] C. Gong, D. Tao, J. Yang, and W. Liu. Teaching-to-learn and learning-to-teach for multi-label propagation. In AAAI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20C.%20Tao%2C%20D.%20Yang%2C%20J.%20Liu%2C%20W.%20Teaching-to-learn%20and%20learning-to-teach%20for%20multi-label%20propagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20C.%20Tao%2C%20D.%20Yang%2C%20J.%20Liu%2C%20W.%20Teaching-to-learn%20and%20learning-to-teach%20for%20multi-label%20propagation%202016"
        },
        {
            "id": "15",
            "entry": "[15] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "16",
            "entry": "[16] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "17",
            "entry": "[17] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20L.%20Zhou%2C%20Z.%20Leung%2C%20T.%20Li%2C%20L.%20Mentornet%3A%20Learning%20data-driven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20L.%20Zhou%2C%20Z.%20Leung%2C%20T.%20Li%2C%20L.%20Mentornet%3A%20Learning%20data-driven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018"
        },
        {
            "id": "18",
            "entry": "[18] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "19",
            "entry": "[19] R. Kiryo, G. Niu, M. Du Plessis, and M. Sugiyama. Positive-unlabeled learning with nonnegative risk estimator. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiryo%2C%20R.%20Niu%2C%20G.%20Plessis%2C%20M.Du%20Sugiyama%2C%20M.%20Positive-unlabeled%20learning%20with%20nonnegative%20risk%20estimator%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiryo%2C%20R.%20Niu%2C%20G.%20Plessis%2C%20M.Du%20Sugiyama%2C%20M.%20Positive-unlabeled%20learning%20with%20nonnegative%20risk%20estimator%202017"
        },
        {
            "id": "20",
            "entry": "[20] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "21",
            "entry": "[21] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "22",
            "entry": "[22] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, and J. Li. Learning from noisy labels with distillation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Yang%2C%20J.%20Song%2C%20Y.%20Cao%2C%20L.%20Learning%20from%20noisy%20labels%20with%20distillation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Yang%2C%20J.%20Song%2C%20Y.%20Cao%2C%20L.%20Learning%20from%20noisy%20labels%20with%20distillation%202017"
        },
        {
            "id": "23",
            "entry": "[23] T. Liu and D. Tao. Classification with noisy labels by importance reweighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(3):447\u2013461, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20T.%20Tao%2C%20D.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20T.%20Tao%2C%20D.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016"
        },
        {
            "id": "24",
            "entry": "[24] X. Ma, Y. Wang, M. Houle, S. Zhou, S. Erfani, S. Xia, S. Wijewickrema, and J. Bailey. Dimensionality-driven learning with noisy labels. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20X.%20Wang%2C%20Y.%20Houle%2C%20M.%20Zhou%2C%20S.%20Dimensionality-driven%20learning%20with%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20X.%20Wang%2C%20Y.%20Houle%2C%20M.%20Zhou%2C%20S.%20Dimensionality-driven%20learning%20with%20noisy%20labels%202018"
        },
        {
            "id": "25",
            "entry": "[25] A. Maas, A. Hannun, and A. Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20A.%20Hannun%2C%20A.%20Ng%2C%20A.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models.%20In%20ICML%202013"
        },
        {
            "id": "26",
            "entry": "[26] E. Malach and S. Shalev-Shwartz. Decoupling\" when to update\" from\" how to update\". In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Decoupling%22%20when%20to%20update%22%20from%22%20how%20to%20update%22%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Decoupling%22%20when%20to%20update%22%20from%22%20how%20to%20update%22%202017"
        },
        {
            "id": "27",
            "entry": "[27] H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In NIPS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Masnadi-Shirazi%2C%20H.%20Vasconcelos%2C%20N.%20On%20the%20design%20of%20loss%20functions%20for%20classification%3A%20theory%2C%20robustness%20to%20outliers%2C%20and%20savageboost%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Masnadi-Shirazi%2C%20H.%20Vasconcelos%2C%20N.%20On%20the%20design%20of%20loss%20functions%20for%20classification%3A%20theory%2C%20robustness%20to%20outliers%2C%20and%20savageboost%202009"
        },
        {
            "id": "28",
            "entry": "[28] A. Menon, B. Van Rooyen, C. Ong, and B. Williamson. Learning from corrupted binary labels via class-probability estimation. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20A.%20Rooyen%2C%20B.Van%20Ong%2C%20C.%20Williamson%2C%20B.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20A.%20Rooyen%2C%20B.Van%20Ong%2C%20C.%20Williamson%2C%20B.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015"
        },
        {
            "id": "29",
            "entry": "[29] T. Miyato, A. Dai, and I. Goodfellow. Virtual adversarial training for semi-supervised text classification. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Dai%2C%20A.%20Goodfellow%2C%20I.%20Virtual%20adversarial%20training%20for%20semi-supervised%20text%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Dai%2C%20A.%20Goodfellow%2C%20I.%20Virtual%20adversarial%20training%20for%20semi-supervised%20text%20classification%202016"
        },
        {
            "id": "30",
            "entry": "[30] N. Natarajan, I. Dhillon, P. Ravikumar, and A. Tewari. Learning with noisy labels. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20N.%20Dhillon%2C%20I.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20N.%20Dhillon%2C%20I.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013"
        },
        {
            "id": "31",
            "entry": "[31] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patrini%2C%20G.%20Rozza%2C%20A.%20Menon%2C%20A.%20Nock%2C%20R.%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20A%20loss%20correction%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patrini%2C%20G.%20Rozza%2C%20A.%20Menon%2C%20A.%20Nock%2C%20R.%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20A%20loss%20correction%20approach%202017"
        },
        {
            "id": "32",
            "entry": "[32] V. Raykar, S. Yu, L. Zhao, G. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. Journal of Machine Learning Research, 11(Apr):1297\u20131322, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raykar%2C%20V.%20Yu%2C%20S.%20Zhao%2C%20L.%20Valadez%2C%20G.%20Learning%20from%20crowds%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raykar%2C%20V.%20Yu%2C%20S.%20Zhao%2C%20L.%20Valadez%2C%20G.%20Learning%20from%20crowds%202010"
        },
        {
            "id": "33",
            "entry": "[33] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Lee%2C%20H.%20Anguelov%2C%20D.%20Szegedy%2C%20C.%20Training%20deep%20neural%20networks%20on%20noisy%20labels%20with%20bootstrapping%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Lee%2C%20H.%20Anguelov%2C%20D.%20Szegedy%2C%20C.%20Training%20deep%20neural%20networks%20on%20noisy%20labels%20with%20bootstrapping%202015"
        },
        {
            "id": "34",
            "entry": "[34] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20M.%20Zeng%2C%20W.%20Yang%2C%20B.%20Urtasun%2C%20R.%20Learning%20to%20reweight%20examples%20for%20robust%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20M.%20Zeng%2C%20W.%20Yang%2C%20B.%20Urtasun%2C%20R.%20Learning%20to%20reweight%20examples%20for%20robust%20deep%20learning%202018"
        },
        {
            "id": "35",
            "entry": "[35] F. Rodrigues and F. Pereira. Deep learning from crowds. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodrigues%2C%20F.%20Pereira%2C%20F.%20Deep%20learning%20from%20crowds%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodrigues%2C%20F.%20Pereira%2C%20F.%20Deep%20learning%20from%20crowds%202018"
        },
        {
            "id": "36",
            "entry": "[36] T. Sanderson and C. Scott. Class proportion estimation with application to multiclass anomaly rejection. In AISTATS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sanderson%2C%20T.%20Scott%2C%20C.%20Class%20proportion%20estimation%20with%20application%20to%20multiclass%20anomaly%20rejection%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sanderson%2C%20T.%20Scott%2C%20C.%20Class%20proportion%20estimation%20with%20application%20to%20multiclass%20anomaly%20rejection%202014"
        },
        {
            "id": "37",
            "entry": "[37] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "38",
            "entry": "[38] D. Tanaka, D. Ikami, T. Yamasaki, and K. Aizawa. Joint optimization framework for learning with noisy labels. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tanaka%2C%20D.%20Ikami%2C%20D.%20Yamasaki%2C%20T.%20Aizawa%2C%20K.%20Joint%20optimization%20framework%20for%20learning%20with%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tanaka%2C%20D.%20Ikami%2C%20D.%20Yamasaki%2C%20T.%20Aizawa%2C%20K.%20Joint%20optimization%20framework%20for%20learning%20with%20noisy%20labels%202018"
        },
        {
            "id": "39",
            "entry": "[39] B. Van Rooyen, A. Menon, and B. Williamson. Learning with symmetric label noise: The importance of being unhinged. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rooyen%2C%20B.Van%20Menon%2C%20A.%20Williamson%2C%20B.%20Learning%20with%20symmetric%20label%20noise%3A%20The%20importance%20of%20being%20unhinged%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rooyen%2C%20B.Van%20Menon%2C%20A.%20Williamson%2C%20B.%20Learning%20with%20symmetric%20label%20noise%3A%20The%20importance%20of%20being%20unhinged%202015"
        },
        {
            "id": "40",
            "entry": "[40] A. Veit, N. Alldrin, G. Chechik, I. Krasin, A. Gupta, and S. Belongie. Learning from noisy large-scale datasets with minimal supervision. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veit%2C%20A.%20Alldrin%2C%20N.%20Chechik%2C%20G.%20Krasin%2C%20I.%20Learning%20from%20noisy%20large-scale%20datasets%20with%20minimal%20supervision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veit%2C%20A.%20Alldrin%2C%20N.%20Chechik%2C%20G.%20Krasin%2C%20I.%20Learning%20from%20noisy%20large-scale%20datasets%20with%20minimal%20supervision%202017"
        },
        {
            "id": "41",
            "entry": "[41] Y. Wang, W. Liu, X. Ma, J. Bailey, H. Zha, L. Song, and S. Xia. Iterative learning with open-set noisy labels. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Liu%2C%20W.%20Ma%2C%20X.%20Bailey%2C%20J.%20Iterative%20learning%20with%20open-set%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Liu%2C%20W.%20Ma%2C%20X.%20Bailey%2C%20J.%20Iterative%20learning%20with%20open-set%20noisy%20labels%202018"
        },
        {
            "id": "42",
            "entry": "[42] Y. Yan, R. Rosales, G. Fung, R. Subramanian, and J. Dy. Learning from multiple annotators with varying expertise. Machine Learning, 95(3):291\u2013327, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20Y.%20Rosales%2C%20R.%20Fung%2C%20G.%20Subramanian%2C%20R.%20Learning%20from%20multiple%20annotators%20with%20varying%20expertise%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20Y.%20Rosales%2C%20R.%20Fung%2C%20G.%20Subramanian%2C%20R.%20Learning%20from%20multiple%20annotators%20with%20varying%20expertise%202014"
        },
        {
            "id": "43",
            "entry": "[43] X. Yu, T. Liu, M. Gong, K. Batmanghelich, and D. Tao. An efficient and provable approach for mixture proportion estimation using linear independence assumption. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20X.%20Liu%2C%20T.%20Gong%2C%20M.%20Batmanghelich%2C%20K.%20An%20efficient%20and%20provable%20approach%20for%20mixture%20proportion%20estimation%20using%20linear%20independence%20assumption%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20X.%20Liu%2C%20T.%20Gong%2C%20M.%20Batmanghelich%2C%20K.%20An%20efficient%20and%20provable%20approach%20for%20mixture%20proportion%20estimation%20using%20linear%20independence%20assumption%202018"
        },
        {
            "id": "44",
            "entry": "[44] X. Yu, T. Liu, M. Gong, and D. Tao. Learning with biased complementary labels. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20X.%20Liu%2C%20T.%20Gong%2C%20M.%20Tao%2C%20D.%20Learning%20with%20biased%20complementary%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20X.%20Liu%2C%20T.%20Gong%2C%20M.%20Tao%2C%20D.%20Learning%20with%20biased%20complementary%20labels%202018"
        },
        {
            "id": "45",
            "entry": "[45] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        }
    ]
}
