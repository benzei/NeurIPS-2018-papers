{
    "filename": "7374-a-deep-bayesian-policy-reuse-approach-against-non-stationary-agents.pdf",
    "metadata": {
        "title": "A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents",
        "author": "YAN ZHENG, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, Changjie Fan",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7374-a-deep-bayesian-policy-reuse-approach-against-non-stationary-agents.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In multiagent domains, coping with non-stationary agents that change behaviors from time to time is a challenging problem, where an agent is usually required to be able to quickly detect the other agent\u2019s policy during online interaction, and then adapt its own policy accordingly. This paper studies efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games. We propose a new deep BPR+ algorithm by extending the recent BPR+ algorithm with a neural network as the value-function approximator. To detect policy accurately, we propose the rectified belief model taking advantage of the opponent model to infer the other agent\u2019s policy from reward signals and its behaviors. Instead of directly storing individual policies as BPR+, we introduce distilled policy network that serves as the policy library in BPR+, using policy distillation to achieve efficient online policy learning and reuse. Deep BPR+ inherits all the advantages of BPR+ and empirically shows better performance in terms of detection accuracy, cumulative rewards and speed of convergence compared to existing algorithms in complex Markov games with raw visual inputs."
    },
    "keywords": [
        {
            "term": "policy network",
            "url": "https://en.wikipedia.org/wiki/Policy_Network"
        },
        {
            "term": "multiagent systems",
            "url": "https://en.wikipedia.org/wiki/multiagent_systems"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "With recent advance of deep reinforcement learning (DRL) techniques [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], a large number of deep reinforcement learning algorithms have been successfully applied to solve challenging problems such as game playing [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], robotics [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] and recommendation [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "As for storing and reusing policies, we introduce the distilled policy network using policy distillation [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], which plays a role of the policy library in Bayesian policy reuse+, providing convenient policy switching",
        "Deep Bayesian policy reuse+ selects a response policy \u03c0t using the new belief model \u03b2t rectified by the opponent model, receives cumulative reward ut, estimates the opponent\u2019s online policy \u03c4ot, and updates \u03b2t using ut and \u03c4ot",
        "We propose the opponent model/policy \u03c4parameterized by \u03b8, a neural network approximator to an opponent\u2019s true policy \u03c4 , depicting its behaviors from its past sequence of moves",
        "This paper proposes a deep Bayesian policy reuse+ algorithm to play against non-stationary agents in multiagent scenarios",
        "The distilled policy network is proposed as the policy library, achieving efficient learning against unknown policies, convenient policy reuse and efficient policy storing"
    ],
    "key_statements": [
        "With recent advance of deep reinforcement learning (DRL) techniques [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], a large number of deep reinforcement learning algorithms have been successfully applied to solve challenging problems such as game playing [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], robotics [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] and recommendation [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "To address above limitations in Bayesian policy reuse+, we propose a new algorithm, named deep Bayesian policy reuse+, combining Bayesian policy reuse+ with recent deep reinforcement learning techniques",
        "As for storing and reusing policies, we introduce the distilled policy network using policy distillation [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], which plays a role of the policy library in Bayesian policy reuse+, providing convenient policy switching",
        "Deep Bayesian policy reuse+ selects a response policy \u03c0t using the new belief model \u03b2t rectified by the opponent model, receives cumulative reward ut, estimates the opponent\u2019s online policy \u03c4ot, and updates \u03b2t using ut and \u03c4ot",
        "The estimated opponent\u2019s policy \u03c4ot, that depicts the opponent\u2019s new policy, will be added into the known opponent policy set T , the corresponding learned response policy will be added into the policy library \u03a0, and the performance model P (U |T , \u03a0) will be updated using the corresponding received cumulative reward",
        "We propose the opponent model/policy \u03c4parameterized by \u03b8, a neural network approximator to an opponent\u2019s true policy \u03c4 , depicting its behaviors from its past sequence of moves",
        "In all games involving a non-stationary agent, deep Bayesian policy reuse+ is empirically verified in terms of detection accuracy, cumulative reward and learning speed of a new response policy",
        "This paper proposes a deep Bayesian policy reuse+ algorithm to play against non-stationary agents in multiagent scenarios",
        "The distilled policy network is proposed as the policy library, achieving efficient learning against unknown policies, convenient policy reuse and efficient policy storing",
        "We would like to investigate how to act optimally in face of the adaptive agents whose behaviors are continuously changing over time"
    ],
    "summary": [
        "With recent advance of deep reinforcement learning (DRL) techniques [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], a large number of DRL algorithms have been successfully applied to solve challenging problems such as game playing [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], robotics [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] and recommendation [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "Compared to existing algorithms, deep BPR+ can achieve more accurate policy detection, more efficient policy reuse and higher speed of convergence towards optimal policies in face of a non-stationary agent using a new unseen policy in complex Markov games with raw visual inputs.",
        "It consists of two stages: 1) a reuse stage selecting the most appropriate policy to execute; and 2) a learning stage obtaining an optimal response policy against a new opponent.",
        "Deep BPR+ selects a response policy \u03c0t using the new belief model \u03b2t rectified by the opponent model, receives cumulative reward ut, estimates the opponent\u2019s online policy \u03c4ot, and updates \u03b2t using ut and \u03c4ot.",
        "The estimated opponent\u2019s policy \u03c4ot, that depicts the opponent\u2019s new policy, will be added into the known opponent policy set T , the corresponding learned response policy will be added into the policy library \u03a0, and the performance model P (U |T , \u03a0) will be updated using the corresponding received cumulative reward.",
        "DPIQN [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] learns features for different opponent\u2019s policies, but uses them to train a generalized Q-network to execute, rather than directly reusing a more advantageous response policy.",
        "In deep BPR+, we explicitly categorize opponent\u2019s policy and try to achieve better performance by reusing the best response policy.",
        "Both belief and opponent models can be understood as the posterior probabilities measuring the opponent\u2019s policy, based on received reward signal u and observed online behaviors \u03c4respectively.",
        "Compared to training an independent convolution layer for each response policy, this architecture forces the shared convolution layer to learn more generalized features against different opponents, better describing the environment.",
        "By optimizing from the starting policy, deep BPR+ learns more efficiently and robustly against different opponents than directly reusing the response policy.",
        "In all games involving a non-stationary agent, deep BPR+ is empirically verified in terms of detection accuracy, cumulative reward and learning speed of a new response policy.",
        "Due to the RMB, deep BPR+(D) performs better by choosing the best response policy to execute, especially when the opponent switches at a fast frequency.",
        "BPR performs poorly in the three games due to its inability to detect and respond to the agent O using unknown policies, while all the others can detect and learn the corresponding response policies against it.",
        "The rectified belief model is introduced by utilizing the opponent model, achieving accurate policy detection from the perspectives of received signal and opponent\u2019s behaviors.",
        "We would like to investigate how to act optimally in face of the adaptive agents whose behaviors are continuously changing over time"
    ],
    "headline": "This paper studies efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intelligence, 258:66\u201395, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Albrecht%2C%20Stefano%20V.%20Stone%2C%20Peter%20Autonomous%20agents%20modelling%20other%20agents%3A%20A%20comprehensive%20survey%20and%20open%20problems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Albrecht%2C%20Stefano%20V.%20Stone%2C%20Peter%20Autonomous%20agents%20modelling%20other%20agents%3A%20A%20comprehensive%20survey%20and%20open%20problems%202018"
        },
        {
            "id": "2",
            "entry": "[2] Georgios Chalkiadakis and Craig Boutilier. Coordination in multiagent reinforcement learning: a baayesian approach. In Proceedings of the 2nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 709\u2013716, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chalkiadakis%2C%20Georgios%20Boutilier%2C%20Craig%20Coordination%20in%20multiagent%20reinforcement%20learning%3A%20a%20baayesian%20approach%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chalkiadakis%2C%20Georgios%20Boutilier%2C%20Craig%20Coordination%20in%20multiagent%20reinforcement%20learning%3A%20a%20baayesian%20approach%202003"
        },
        {
            "id": "3",
            "entry": "[3] Jacob W. Crandall. Just add pepper: extending learning algorithms for repeated matrix games to repeated markov games. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 399\u2013406, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crandall%2C%20Jacob%20W.%20Just%20add%20pepper%3A%20extending%20learning%20algorithms%20for%20repeated%20matrix%20games%20to%20repeated%20markov%20games%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crandall%2C%20Jacob%20W.%20Just%20add%20pepper%3A%20extending%20learning%20algorithms%20for%20repeated%20matrix%20games%20to%20repeated%20markov%20games%202012"
        },
        {
            "id": "4",
            "entry": "[4] Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 122\u2013130, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Chen%2C%20Richard%20Y.%20Al-Shedivat%2C%20Maruan%20Whiteson%2C%20Shimon%20Learning%20with%20opponent-learning%20awareness%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Chen%2C%20Richard%20Y.%20Al-Shedivat%2C%20Maruan%20Whiteson%2C%20Shimon%20Learning%20with%20opponent-learning%20awareness%202018"
        },
        {
            "id": "5",
            "entry": "[5] Jayesh K. Gupta, Maxim Egorov, and Mykel J. Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Adaptive Learning Agents Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Jayesh%20K.%20Egorov%2C%20Maxim%20Kochenderfer%2C%20Mykel%20J.%20Cooperative%20multi-agent%20control%20using%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Jayesh%20K.%20Egorov%2C%20Maxim%20Kochenderfer%2C%20Mykel%20J.%20Cooperative%20multi-agent%20control%20using%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "6",
            "entry": "[6] He He and Jordan L. Boyd-Graber. Opponent modeling in deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 1804\u2013 1813, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20He%20Boyd-Graber%2C%20Jordan%20L.%20Opponent%20modeling%20in%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20He%20Boyd-Graber%2C%20Jordan%20L.%20Opponent%20modeling%20in%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "7",
            "entry": "[7] Pablo Hernandez-Leal and Michael Kaisers. Learning against sequential opponents in repeated stochastic games. In The 3rd Multi-disciplinary Conference on Reinforcement Learning and Decision Making, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandez-Leal%2C%20Pablo%20Kaisers%2C%20Michael%20Learning%20against%20sequential%20opponents%20in%20repeated%20stochastic%20games%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandez-Leal%2C%20Pablo%20Kaisers%2C%20Michael%20Learning%20against%20sequential%20opponents%20in%20repeated%20stochastic%20games%202017"
        },
        {
            "id": "8",
            "entry": "[8] Pablo Hernandez-Leal and Michael Kaisers. Towards a fast detection of opponents in repeated stochastic games. In Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS) 2017 Workshops, pages 239\u2013257, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandez-Leal%2C%20Pablo%20Kaisers%2C%20Michael%20Towards%20a%20fast%20detection%20of%20opponents%20in%20repeated%20stochastic%20games%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandez-Leal%2C%20Pablo%20Kaisers%2C%20Michael%20Towards%20a%20fast%20detection%20of%20opponents%20in%20repeated%20stochastic%20games%202017"
        },
        {
            "id": "9",
            "entry": "[9] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learning in multiagent environments: Dealing with non-stationarity. CoRR, abs/1707.09183, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09183"
        },
        {
            "id": "10",
            "entry": "[10] Pablo Hernandez-Leal, Benjamin Rosman, Matthew E. Taylor, Luis Enrique Sucar, and Enrique Munoz de Cote. A Bayesian approach for learning and tracking switching, non-stationary opponents (extended abstract). In Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 1315\u20131316, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hernandez-Leal%2C%20Pablo%20Rosman%2C%20Benjamin%20Taylor%2C%20Matthew%20E.%20Sucar%2C%20Luis%20Enrique%20A%20Bayesian%20approach%20for%20learning%20and%20tracking%20switching%2C%20non-stationary%20opponents%20%28extended%20abstract%29%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hernandez-Leal%2C%20Pablo%20Rosman%2C%20Benjamin%20Taylor%2C%20Matthew%20E.%20Sucar%2C%20Luis%20Enrique%20A%20Bayesian%20approach%20for%20learning%20and%20tracking%20switching%2C%20non-stationary%20opponents%20%28extended%20abstract%29%202016"
        },
        {
            "id": "11",
            "entry": "[11] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "12",
            "entry": "[12] Zhang-Wei Hong, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, and Chun-Yi Lee. A deep policy inference q-network for multi-agent systems. In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 1388\u20131396, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20Zhang-Wei%20Su%2C%20Shih-Yang%20Shann%2C%20Tzu-Yun%20Chang%2C%20Yi-Hsiang%20A%20deep%20policy%20inference%20q-network%20for%20multi-agent%20systems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20Zhang-Wei%20Su%2C%20Shih-Yang%20Shann%2C%20Tzu-Yun%20Chang%2C%20Yi-Hsiang%20A%20deep%20policy%20inference%20q-network%20for%20multi-agent%20systems%202018"
        },
        {
            "id": "13",
            "entry": "[13] Junling Hu and Michael P. Wellman. Multiagent reinforcement learning: Theoretical framework and an algorithm. In Proceedings of the 15th International Conference on Machine Learning (ICML), pages 242\u2013250, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Junling%20Wellman%2C%20Michael%20P.%20Multiagent%20reinforcement%20learning%3A%20Theoretical%20framework%20and%20an%20algorithm%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Junling%20Wellman%2C%20Michael%20P.%20Multiagent%20reinforcement%20learning%3A%20Theoretical%20framework%20and%20an%20algorithm%201998"
        },
        {
            "id": "14",
            "entry": "[14] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "15",
            "entry": "[15] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the 11th International Conference on Machine Learning (ICML), pages 157\u2013163, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994"
        },
        {
            "id": "16",
            "entry": "[16] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems 30, (NIPS), pages 6382\u20136393, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017"
        },
        {
            "id": "17",
            "entry": "[17] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "18",
            "entry": "[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness anda Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "19",
            "entry": "[19] Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep reinforcement learning. In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 443\u2013451, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Palmer%2C%20Gregory%20Tuyls%2C%20Karl%20Bloembergen%2C%20Daan%20Savani%2C%20Rahul%20Lenient%20multi-agent%20deep%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Palmer%2C%20Gregory%20Tuyls%2C%20Karl%20Bloembergen%2C%20Daan%20Savani%2C%20Rahul%20Lenient%20multi-agent%20deep%20reinforcement%20learning%202018"
        },
        {
            "id": "20",
            "entry": "[20] Benjamin Rosman, Majd Hawasly, and Subramanian Ramamoorthy. Bayesian policy reuse. Machine Learning, 104(1):99\u2013127, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosman%2C%20Benjamin%20Hawasly%2C%20Majd%20Ramamoorthy%2C%20Subramanian%20Bayesian%20policy%20reuse%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosman%2C%20Benjamin%20Hawasly%2C%20Majd%20Ramamoorthy%2C%20Subramanian%20Bayesian%20policy%20reuse%202016"
        },
        {
            "id": "21",
            "entry": "[21] Andrei A. Rusu, Sergio Gomez Colmenarejo, \u00c7aglar G\u00fcl\u00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. CoRR, abs/1511.06295, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06295"
        },
        {
            "id": "22",
            "entry": "[22] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tom%20Schaul%20John%20Quan%20Ioannis%20Antonoglou%20and%20David%20Silver%20Prioritized%20experience%20replay%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tom%20Schaul%20John%20Quan%20Ioannis%20Antonoglou%20and%20David%20Silver%20Prioritized%20experience%20replay%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202016"
        },
        {
            "id": "23",
            "entry": "[23] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. PLOS ONE, 12(4):1\u201315, 04 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tampuu%2C%20Ardi%20Matiisen%2C%20Tambet%20Kodelja%2C%20Dorian%20Kuzovkin%2C%20Ilya%20Multiagent%20cooperation%20and%20competition%20with%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tampuu%2C%20Ardi%20Matiisen%2C%20Tambet%20Kodelja%2C%20Dorian%20Kuzovkin%2C%20Ilya%20Multiagent%20cooperation%20and%20competition%20with%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "24",
            "entry": "[24] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-learning. In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), pages 2094\u20132100, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Hasselt%2C%20Hado%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20Q-learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Hasselt%2C%20Hado%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20Q-learning%202016"
        },
        {
            "id": "25",
            "entry": "[25] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling network architectures for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 1995\u20132003, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20van%20Hasselt%2C%20Hado%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20van%20Hasselt%2C%20Hado%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%201995"
        },
        {
            "id": "26",
            "entry": "[26] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Dawei Yin, Yihong Zhao, and Jiliang Tang. Deep reinforcement learning for list-wise recommendations. CoRR, abs/1801.00209, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1801.00209"
        }
    ]
}
