{
    "filename": "8000-breaking-the-activation-function-bottleneck-through-adaptive-parameterization.pdf",
    "metadata": {
        "title": "Breaking the Activation Function Bottleneck through Adaptive Parameterization",
        "author": "Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8000-breaking-the-activation-function-bottleneck-through-adaptive-parameterization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half the number of iterations."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "statistical efficiency",
            "url": "https://en.wikipedia.org/wiki/statistical_efficiency"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "penn treebank",
            "url": "https://en.wikipedia.org/wiki/Penn_Treebank"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        }
    ],
    "highlights": [
        "While a two-layer feed-forward neural network is sufficient to approximate any function (<a class=\"ref-link\" id=\"cCybenko_1989_a\" href=\"#rCybenko_1989_a\"><a class=\"ref-link\" id=\"cCybenko_1989_a\" href=\"#rCybenko_1989_a\">Cybenko, 1989</a></a>; <a class=\"ref-link\" id=\"cHornik_1991_a\" href=\"#rHornik_1991_a\"><a class=\"ref-link\" id=\"cHornik_1991_a\" href=\"#rHornik_1991_a\">Hornik, 1991</a></a>), in practice much deeper networks are necessary to learn a good approximation to a complex function",
        "We show that deep neural networks learn a family of compositions of linear maps and because the activation function is static, the inherent flexibility in this family is weak",
        "We evaluate the aLSTM using the same settings as on Penn Treebank corpus, and test a version with larger hidden state size to match the parameter count of current state of the art models",
        "By viewing deep neural networks as adaptive compositions of linear maps, we have showed that standard activation functions induce an activation function bottleneck because they fail to have significant non-linear effect on a non-trivial subset of inputs",
        "We have developed an adaptive feed-forward layer and showed empirically that it can learn patterns where a deep feed-forward network fails whilst using fewer parameters",
        "Extending the adaptive feed-forward layer to Recurrent Neural Networks, we presented an adaptive Long Short-Term Memory model that significantly increases model capacity and statistical efficiency while being more robust to hyper-parameters"
    ],
    "key_statements": [
        "While a two-layer feed-forward neural network is sufficient to approximate any function (<a class=\"ref-link\" id=\"cCybenko_1989_a\" href=\"#rCybenko_1989_a\"><a class=\"ref-link\" id=\"cCybenko_1989_a\" href=\"#rCybenko_1989_a\">Cybenko, 1989</a></a>; <a class=\"ref-link\" id=\"cHornik_1991_a\" href=\"#rHornik_1991_a\"><a class=\"ref-link\" id=\"cHornik_1991_a\" href=\"#rHornik_1991_a\">Hornik, 1991</a></a>), in practice much deeper networks are necessary to learn a good approximation to a complex function",
        "For a randomly sampled input to break linearity, layers must be wide and the network deep to ensure some elements lie in non-linear regions of the activation function",
        "To overcome the bias towards linear behavior, more sophisticated activation functions have been designed (<a class=\"ref-link\" id=\"cClevert_et+al_2015_a\" href=\"#rClevert_et+al_2015_a\">Clevert et al, 2015</a>; <a class=\"ref-link\" id=\"cHe_et+al_2015_a\" href=\"#rHe_et+al_2015_a\">He et al, 2015</a>; <a class=\"ref-link\" id=\"cKlambauer_et+al_2017_a\" href=\"#rKlambauer_et+al_2017_a\">Klambauer et al, 2017</a>; <a class=\"ref-link\" id=\"cDauphin_et+al_2017_a\" href=\"#rDauphin_et+al_2017_a\">Dauphin et al, 2017</a>). These still limit all non-linearity to sit in the activation function",
        "We show that deep neural networks learn a family of compositions of linear maps and because the activation function is static, the inherent flexibility in this family is weak",
        "We focus on the feed-forward layer, f (x) := \u03c6(W x + b), for some activation function \u03c6 : R \u2192 R",
        "We evaluate the aLSTM on word-level modeling following standard practice in training setup (e.g. <a class=\"ref-link\" id=\"cZaremba_et+al_2015_a\" href=\"#rZaremba_et+al_2015_a\">Zaremba et al, 2015</a>)",
        "We evaluate the aLSTM using the same settings as on Penn Treebank corpus, and test a version with larger hidden state size to match the parameter count of current state of the art models",
        "We use the same hyper-parameters for all models except for (a) the standard Long Short-Term Memory model and (b) the aLSTM under an output-adaptation policy and a feed-forward adaptation model, as this configuration needed slightly lower dropout rates to converge to good performance",
        "We further study the robustness of the aLSTM with respect to hyper-parameters",
        "By viewing deep neural networks as adaptive compositions of linear maps, we have showed that standard activation functions induce an activation function bottleneck because they fail to have significant non-linear effect on a non-trivial subset of inputs",
        "We have developed an adaptive feed-forward layer and showed empirically that it can learn patterns where a deep feed-forward network fails whilst using fewer parameters",
        "Extending the adaptive feed-forward layer to Recurrent Neural Networks, we presented an adaptive Long Short-Term Memory model that significantly increases model capacity and statistical efficiency while being more robust to hyper-parameters"
    ],
    "summary": [
        "While a two-layer feed-forward neural network is sufficient to approximate any function (<a class=\"ref-link\" id=\"cCybenko_1989_a\" href=\"#rCybenko_1989_a\"><a class=\"ref-link\" id=\"cCybenko_1989_a\" href=\"#rCybenko_1989_a\">Cybenko, 1989</a></a>; <a class=\"ref-link\" id=\"cHornik_1991_a\" href=\"#rHornik_1991_a\"><a class=\"ref-link\" id=\"cHornik_1991_a\" href=\"#rHornik_1991_a\">Hornik, 1991</a></a>), in practice much deeper networks are necessary to learn a good approximation to a complex function.",
        "By relaxing the element-wise non-linearity constraint imposed on the standard feed-forward layer, it can learn behaviors that would otherwise be very hard or impossible to model, such as contextual rotations and shears, and adaptive feature activation.",
        "Our goal is to break the activation function bottleneck by generalizing G into a parameterized adaptation policy, thereby enabling the network to specialize parameters in A to encode global, input invariant information while parameters in G encode local, contextual information.",
        "We demonstrate adaptive parameterization in the context of Recurrent Neural Networks (RNNs), where feed-forward layers are predominant.",
        "To extend the adaptive feed-forward layer to the LSTM, index sub-policies with a tuple (s, j) \u2208 {i, f, o, z} \u00d7 {0, 1, 2, 3, 4} such that Dt(s,j) = diag(\u03c0(s,j)).",
        "An alternative method pre-defines the adaptation policy as gradient descent and meta-learns an initialization such that performing gradient descent on a given input from some new task yields good task-specific parameters (<a class=\"ref-link\" id=\"cFinn_et+al_2017_a\" href=\"#rFinn_et+al_2017_a\">Finn et al, 2017</a>; <a class=\"ref-link\" id=\"cLee_2017_a\" href=\"#rLee_2017_a\">Lee & Choi, 2017</a>; <a class=\"ref-link\" id=\"cAl-Shedivat_et+al_2018_a\" href=\"#rAl-Shedivat_et+al_2018_a\">Al-Shedivat et al, 2018</a>).",
        "Most closely related to the aLSTM are HyperNetworks (<a class=\"ref-link\" id=\"cHa_et+al_2017_a\" href=\"#rHa_et+al_2017_a\">Ha et al, 2017</a>; <a class=\"ref-link\" id=\"cSuarez_2017_a\" href=\"#rSuarez_2017_a\">Suarez, 2017</a>); these implement output adaptation conditioned on both the input and the state of the system using a recurrent adaptation policy.",
        "We use the same hyper-parameters for all models except for (a) the standard LSTM and (b) the aLSTM under an output-adaptation policy and a feed-forward adaptation model, as this configuration needed slightly lower dropout rates to converge to good performance.",
        "Going from a feed-forward adaptation model to a recurrent adaptation model yields a significant improvement irrespective of policy, and our hybrid RHN-LSTM provides a further boost.",
        "By viewing deep neural networks as adaptive compositions of linear maps, we have showed that standard activation functions induce an activation function bottleneck because they fail to have significant non-linear effect on a non-trivial subset of inputs.",
        "We have developed an adaptive feed-forward layer and showed empirically that it can learn patterns where a deep feed-forward network fails whilst using fewer parameters.",
        "Extending the adaptive feed-forward layer to RNNs, we presented an adaptive LSTM that significantly increases model capacity and statistical efficiency while being more robust to hyper-parameters.",
        "We obtain new state of the art results on the Penn Treebank and the WikiText-2 word-modeling tasks, using ~20\u201330% fewer parameters and converging in less than half as many iterations."
    ],
    "headline": "We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly",
    "reference_links": [
        {
            "id": "Al-Shedivat_et+al_2018_a",
            "entry": "Al-Shedivat, Maruan, Bansal, Trapit, Burda, Yuri, Sutskever, Ilya, Mordatch, Igor, and Abbeel, Pieter. Continuous adaptation via meta-learning in nonstationary and competitive environments. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Al-Shedivat%2C%20Maruan%20Bansal%2C%20Trapit%20Burda%2C%20Yuri%20Sutskever%2C%20Ilya%20Continuous%20adaptation%20via%20meta-learning%20in%20nonstationary%20and%20competitive%20environments%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Shedivat%2C%20Maruan%20Bansal%2C%20Trapit%20Burda%2C%20Yuri%20Sutskever%2C%20Ilya%20Continuous%20adaptation%20via%20meta-learning%20in%20nonstationary%20and%20competitive%20environments%202018"
        },
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "Andrychowicz, Marcin, Denil, Misha, G\u00f3mez, Sergio, Hoffman, Matthew W, Pfau, David, Schaul, Tom, and de Freitas, Nando. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pp. 3981\u20133989, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20G%C3%B3mez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20G%C3%B3mez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "Bengio_et+al_1995_a",
            "entry": "Bengio, Samy, Bengio, Yoshua, Cloutier, Jocelyn, and Gecsei, Jan. On the optimization of a synaptic learning rule. In Optimality in Biological and Artificial Networks, pp. 6\u20138, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Cloutier%2C%20Jocelyn%20Gecsei%2C%20Jan%20On%20the%20optimization%20of%20a%20synaptic%20learning%20rule%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Cloutier%2C%20Jocelyn%20Gecsei%2C%20Jan%20On%20the%20optimization%20of%20a%20synaptic%20learning%20rule%201995"
        },
        {
            "id": "Bengio_et+al_1991_a",
            "entry": "Bengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn. Learning a synaptic learning rule. Universit\u00e9 de Montr\u00e9al, D\u00e9partement d\u2019informatique et de recherche op\u00e9rationnelle, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Bengio%2C%20Samy%20Cloutier%2C%20Jocelyn%20Learning%20a%20synaptic%20learning%20rule.%20Universit%C3%A9%20de%20Montr%C3%A9al%2C%20D%C3%A9partement%20d%E2%80%99informatique%20et%20de%20recherche%20op%C3%A9rationnelle%201991"
        },
        {
            "id": "Bertinetto_et+al_2016_a",
            "entry": "Bertinetto, Luca, Henriques, Jo\u00e3o F, Valmadre, Jack, Torr, Philip, and Vedaldi, Andrea. Learning feedforward one-shot learners. In Advances in neural information processing systems, pp. 523\u2013531, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertinetto%2C%20Luca%20Henriques%2C%20Jo%C3%A3o%20F.%20Valmadre%2C%20Jack%20Torr%2C%20Philip%20Learning%20feedforward%20one-shot%20learners%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertinetto%2C%20Luca%20Henriques%2C%20Jo%C3%A3o%20F.%20Valmadre%2C%20Jack%20Torr%2C%20Philip%20Learning%20feedforward%20one-shot%20learners%202016"
        },
        {
            "id": "Brock_et+al_2018_a",
            "entry": "Brock, Andrew, Lim, Theo, Ritchie, J.M., and Weston, Nick. SMASH: One-shot model architecture search through hypernetworks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Lim%2C%20Theo%20Ritchie%2C%20J.M.%20Weston%2C%20Nick%20SMASH%3A%20One-shot%20model%20architecture%20search%20through%20hypernetworks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Lim%2C%20Theo%20Ritchie%2C%20J.M.%20Weston%2C%20Nick%20SMASH%3A%20One-shot%20model%20architecture%20search%20through%20hypernetworks%202018"
        },
        {
            "id": "Canziani_et+al_2016_a",
            "entry": "Canziani, Alfredo, Paszke, Adam, and Culurciello, Eugenio. An analysis of deep neural network models for practical applications. arXiv preprint, arXiv:1605.07678, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07678"
        },
        {
            "id": "Cho_2014_a",
            "entry": "Cho, Kyunghyun, van Merrienboer, Bart, G\u00fcl\u00e7ehre, \u00c7aglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using RNN encoder-decoder for statistical machine translation. Proceedings of Emperical Methods in Natural Language Processing, pp. 1724\u20131734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Chung_et+al_2014_a",
            "entry": "Chung, Junyoung, G\u00fcl\u00e7ehre, \u00c7aglar, Cho, Kyunghyun, and Bengio, Yoshua. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv preprint, arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Clevert, Djork-Arn\u00e9, Unterthiner, Thomas, and Hochreiter, Sepp. Fast and accurate deep network learning by exponential linear units (elus). In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clevert%2C%20Djork-Arn%C3%A9%20Unterthiner%2C%20Thomas%20Hochreiter%2C%20Sepp%20Fast%20and%20accurate%20deep%20network%20learning%20by%20exponential%20linear%20units%20%28elus%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clevert%2C%20Djork-Arn%C3%A9%20Unterthiner%2C%20Thomas%20Hochreiter%2C%20Sepp%20Fast%20and%20accurate%20deep%20network%20learning%20by%20exponential%20linear%20units%20%28elus%29%202015"
        },
        {
            "id": "Cooijmans_et+al_2016_a",
            "entry": "Cooijmans, Tim, Ballas, Nicolas, Laurent, C\u00e9sar, and Courville, Aaron. Recurrent Batch Normalization. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cooijmans%20Tim%20Ballas%20Nicolas%20Laurent%20C%C3%A9sar%20and%20Courville%20Aaron%20Recurrent%20Batch%20Normalization%20In%20International%20Conference%20on%20Learning%20Representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cooijmans%20Tim%20Ballas%20Nicolas%20Laurent%20C%C3%A9sar%20and%20Courville%20Aaron%20Recurrent%20Batch%20Normalization%20In%20International%20Conference%20on%20Learning%20Representations%202016"
        },
        {
            "id": "Cybenko_1989_a",
            "entry": "Cybenko, George. Approximation by superpositions of a sigmoidal function. MCSS, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20George%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20George%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989"
        },
        {
            "id": "Dauphin_et+al_2017_a",
            "entry": "Dauphin, Yann N, Fan, Angela, Auli, Michael, and Grangier, David. Language Modeling with Gated Convolutional Networks. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20Modeling%20with%20Gated%20Convolutional%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20Modeling%20with%20Gated%20Convolutional%20Networks%202017"
        },
        {
            "id": "Denil_et+al_2013_a",
            "entry": "Denil, Misha, Shakibi, Babak, Dinh, Laurent, De Freitas, Nando, et al. Predicting parameters in deep learning. In Advances in neural information processing systems, pp. 2148\u20132156, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20De%20Predicting%20parameters%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20De%20Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "Fernando_et+al_2016_a",
            "entry": "Fernando, Chrisantha, Banarse, Dylan, Reynolds, Malcolm, Besse, Frederic, Pfau, David, Jaderberg, Max, Lanctot, Marc, and Wierstra, Daan. Convolution by Evolution - Differentiable Pattern Producing Networks. GECCO, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fernando%2C%20Chrisantha%20Banarse%2C%20Dylan%20Reynolds%2C%20Malcolm%20Besse%2C%20Frederic%20Convolution%20by%20Evolution%20-%20Differentiable%20Pattern%20Producing%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fernando%2C%20Chrisantha%20Banarse%2C%20Dylan%20Reynolds%2C%20Malcolm%20Besse%2C%20Frederic%20Convolution%20by%20Evolution%20-%20Differentiable%20Pattern%20Producing%20Networks%202016"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Finn, Chelsea, Abbeel, Pieter, and Levine, Sergey. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017"
        },
        {
            "id": "Frankle_2018_a",
            "entry": "Frankle, Jonathan and Carbin, Michael. The lottery ticket hypothesis: Training pruned neural networks. arXiv preprint, arXiv:1803.03635, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03635"
        },
        {
            "id": "Gers_et+al_2000_a",
            "entry": "Gers, Felix A, Schmidhuber, J\u00fcrgen, and Cummins, Fred. Learning to Forget: Continual Prediction with LSTM. Neural Computation, 12(10):2451\u20132471, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gers%2C%20Felix%20A.%20Schmidhuber%2C%20J%C3%BCrgen%20Cummins%2C%20Fred%20Learning%20to%20Forget%3A%20Continual%20Prediction%20with%20LSTM%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gers%2C%20Felix%20A.%20Schmidhuber%2C%20J%C3%BCrgen%20Cummins%2C%20Fred%20Learning%20to%20Forget%3A%20Continual%20Prediction%20with%20LSTM%202000"
        },
        {
            "id": "Grave_et+al_2017_a",
            "entry": "Grave, Edouard, Joulin, Armand, and Usunier, Nicolas. Improving Neural Language Models with a Continuous Cache. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grave%2C%20Edouard%20Joulin%2C%20Armand%20Usunier%2C%20Nicolas%20Improving%20Neural%20Language%20Models%20with%20a%20Continuous%20Cache%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grave%2C%20Edouard%20Joulin%2C%20Armand%20Usunier%2C%20Nicolas%20Improving%20Neural%20Language%20Models%20with%20a%20Continuous%20Cache%202017"
        },
        {
            "id": "Graves_2013_a",
            "entry": "Graves, Alex. Generating Sequences With Recurrent Neural Networks. arXiv preprint, arXiv:1308.0850, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.0850"
        },
        {
            "id": "Ha_2018_a",
            "entry": "Ha, David and Eck, Douglas. A neural representation of sketch drawings. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ha%2C%20David%20Eck%2C%20Douglas%20A%20neural%20representation%20of%20sketch%20drawings%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ha%2C%20David%20Eck%2C%20Douglas%20A%20neural%20representation%20of%20sketch%20drawings%202018"
        },
        {
            "id": "Ha_et+al_2017_a",
            "entry": "Ha, David, Dai, Andrew, and Le, Quoc V. HyperNetworks. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ha%20David%20Dai%20Andrew%20and%20Le%20Quoc%20V%20HyperNetworks%20International%20Conference%20on%20Learning%20Representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ha%20David%20Dai%20Andrew%20and%20Le%20Quoc%20V%20HyperNetworks%20International%20Conference%20on%20Learning%20Representations%202017"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification. In International Conference on Computer Vision, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20Deep%20into%20Rectifiers%20Surpassing%20Human-Level%20Performance%20on%20ImageNet%20Classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20Deep%20into%20Rectifiers%20Surpassing%20Human-Level%20Performance%20on%20ImageNet%20Classification%202015"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Hochreiter, Sepp and Schmidhuber, J\u00fcrgen. Long short-term memory. Neural Computation, 9: 1735\u201380, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hornik_1991_a",
            "entry": "Hornik, Kurt. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4 (2):251\u2013257, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20Kurt%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks.%20Neural%20Networks%2C%204%201991"
        },
        {
            "id": "Inan_et+al_2017_a",
            "entry": "Inan, Hakan, Khosravi, Khashayar, and Socher, Richard. Tying word vectors and word classifiers: A loss framework for language modeling. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Inan%2C%20Hakan%20Khosravi%2C%20Khashayar%20Socher%2C%20Richard%20Tying%20word%20vectors%20and%20word%20classifiers%3A%20A%20loss%20framework%20for%20language%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Inan%2C%20Hakan%20Khosravi%2C%20Khashayar%20Socher%2C%20Richard%20Tying%20word%20vectors%20and%20word%20classifiers%3A%20A%20loss%20framework%20for%20language%20modeling%202017"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Jaderberg, Max, Czarnecki, Wojciech Marian, Osindero, Simon, Vinyals, Oriol, Graves, Alex, and Kavukcuoglu, Koray. Decoupled neural interfaces using synthetic gradients. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Czarnecki%2C%20Wojciech%20Marian%20Osindero%2C%20Simon%20Vinyals%2C%20Oriol%20Decoupled%20neural%20interfaces%20using%20synthetic%20gradients%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Czarnecki%2C%20Wojciech%20Marian%20Osindero%2C%20Simon%20Vinyals%2C%20Oriol%20Decoupled%20neural%20interfaces%20using%20synthetic%20gradients%202017"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Kingma, Diederik P. and Ba, Jimmy. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015"
        },
        {
            "id": "Klambauer_et+al_2017_a",
            "entry": "Klambauer, G\u00fcnter, Unterthiner, Thomas, Mayr, Andreas, and Hochreiter, Sepp. Self-normalizing neural networks. In Advances in Neural Information Processing Systems, pp. 972\u2013981, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klambauer%2C%20G%C3%BCnter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Self-normalizing%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klambauer%2C%20G%C3%BCnter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Self-normalizing%20neural%20networks%202017"
        },
        {
            "id": "Krause_et+al_2016_a",
            "entry": "Krause, Ben, Lu, Liang, Murray, Iain, and Renals, Steve. Multiplicative lstm for sequence modelling. arXiv preprint, arXiv:1609:07959, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20Ben%20Lu%2C%20Liang%20Murray%2C%20Iain%20Renals%2C%20Steve%20Multiplicative%20lstm%20for%20sequence%20modelling.%20arXiv%20preprint%2C%20arXiv%3A%201609%3A%2007959%202016"
        },
        {
            "id": "Krause_et+al_2017_a",
            "entry": "Krause, Ben, Kahembwe, Emmanuel, Murray, Iain, and Renals, Steve. Dynamic Evaluation of Neural Sequence Models. arXiv preprint, arXiv:1709:07432, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%20Ben%20Kahembwe%20Emmanuel%20Murray%20Iain%20and%20Renals%20Steve%20Dynamic%20Evaluation%20of%20Neural%20Sequence%20Models%20arXiv%20preprint%20arXiv170907432%202017"
        },
        {
            "id": "Lee_2017_a",
            "entry": "Lee, Yoonho and Choi, Seungjin. Meta-Learning with Adaptive Layerwise Metric and Subspace. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Yoonho%20Choi%2C%20Seungjin%20Meta-Learning%20with%20Adaptive%20Layerwise%20Metric%20and%20Subspace%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Yoonho%20Choi%2C%20Seungjin%20Meta-Learning%20with%20Adaptive%20Layerwise%20Metric%20and%20Subspace%202017"
        },
        {
            "id": "Marcus_et+al_1993_a",
            "entry": "Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini, Beatrice. Building a large annotated corpus of english: the Penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20the%20Penn%20treebank%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20Mitchell%20P.%20Marcinkiewicz%2C%20Mary%20Ann%20Santorini%2C%20Beatrice%20Building%20a%20large%20annotated%20corpus%20of%20english%3A%20the%20Penn%20treebank%201993"
        },
        {
            "id": "Melis_et+al_2018_a",
            "entry": "Melis, G\u00e1bor, Dyer, Chris, and Blunsom, Phil. On the State of the Art of Evaluation in Neural Language Models. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20State%20of%20the%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20State%20of%20the%202018"
        },
        {
            "id": "Merity_et+al_2017_a",
            "entry": "Merity, Stephen, Xiong, Caiming, Bradbury, James, and Socher, Richard. Pointer sentinel mixture models. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Bradbury%2C%20James%20Socher%2C%20Richard%20Pointer%20sentinel%20mixture%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Bradbury%2C%20James%20Socher%2C%20Richard%20Pointer%20sentinel%20mixture%20models%202017"
        },
        {
            "id": "Merity_et+al_2018_a",
            "entry": "Merity, Stephen, Keskar, Nitish Shirish, and Socher, Richard. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20LSTM%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Keskar%2C%20Nitish%20Shirish%20Socher%2C%20Richard%20Regularizing%20and%20optimizing%20LSTM%20language%20models%202018"
        },
        {
            "id": "Mikolov_2012_a",
            "entry": "Mikolov, Tom\u00e1\u0161. Statistical language models based on neural networks. PhD thesis, Brno University of Technology, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tom%C3%A1%C5%A1%20Statistical%20language%20models%20based%20on%20neural%20networks%202012"
        },
        {
            "id": "Mikolov_et+al_2010_a",
            "entry": "Mikolov, Tomas, Karafiat, Martin, Burget, Lukas, Cernocky, Jan, and Khudanpur, Sanjeev. Recurrent neural network based language model. Interspeech, 2:3, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "Mikolov_et+al_2012_b",
            "entry": "Mikolov, Tom\u00e1\u0161, Sutskever, Ilya, Deoras, Anoop, Le, Hai-Son, Kombrink, Stefan, and Cernocky, Jan. Subword language modeling with neural networks. Preprint, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tom%C3%A1%C5%A1%20Sutskever%2C%20Ilya%20Deoras%2C%20Anoop%20Le%2C%20Hai-Son%20Subword%20language%20modeling%20with%20neural%20networks%202012"
        },
        {
            "id": "Novak_et+al_2018_a",
            "entry": "Novak, Roman, Bahri, Yasaman, Abolafia, Daniel A., Pennington, Jeffrey, and Sohl-Dickstein, Jascha. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Novak%2C%20Roman%20Bahri%2C%20Yasaman%20Abolafia%2C%20Daniel%20A.%20Pennington%2C%20Jeffrey%20Sensitivity%20and%20generalization%20in%20neural%20networks%3A%20an%20empirical%20study%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novak%2C%20Roman%20Bahri%2C%20Yasaman%20Abolafia%2C%20Daniel%20A.%20Pennington%2C%20Jeffrey%20Sensitivity%20and%20generalization%20in%20neural%20networks%3A%20an%20empirical%20study%202018"
        },
        {
            "id": "Press_2017_a",
            "entry": "Press, Ofir and Wolf, Lior. Using the output embedding to improve language models. In Proceedings of the European Chapter of the Association for Computational Linguistics, volume 2, pp. 157\u2013163, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Press%2C%20Ofir%20Wolf%2C%20Lior%20Using%20the%20output%20embedding%20to%20improve%20language%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Press%2C%20Ofir%20Wolf%2C%20Lior%20Using%20the%20output%20embedding%20to%20improve%20language%20models%202017"
        },
        {
            "id": "Radford_et+al_2017_a",
            "entry": "Radford, Alec, Jozefowicz, Rafal, and Sutskever, Ilya. Learning to Generate Reviews and Discovering Sentiment. arXiv preprint, arXiv:1704.01444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01444"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Ravi, Sachin and Larochelle, Hugo. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Saxe_et+al_2013_a",
            "entry": "Saxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint, arXiv:1312.6120, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6120"
        },
        {
            "id": "Schmidhuber_1992_a",
            "entry": "Schmidhuber, J\u00fcrgen. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131\u2013139, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J%C3%BCrgen%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20dynamic%20recurrent%20networks%201992"
        },
        {
            "id": "Stanley_et+al_2009_a",
            "entry": "Stanley, Kenneth O., D\u2019Ambrosio, David B., and Gauci, Jason. A hypercube-based encoding for evolving large-scale neural networks. Artificial Life, 15(2):185\u2013212, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20Kenneth%20O.%20D%E2%80%99Ambrosio%2C%20David%20B.%20Gauci%2C%20Jason%20A%20hypercube-based%20encoding%20for%20evolving%20large-scale%20neural%20networks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanley%2C%20Kenneth%20O.%20D%E2%80%99Ambrosio%2C%20David%20B.%20Gauci%2C%20Jason%20A%20hypercube-based%20encoding%20for%20evolving%20large-scale%20neural%20networks%202009"
        },
        {
            "id": "Suarez_2017_a",
            "entry": "Suarez, Joseph. Character-level language modeling with recurrent highway hypernetworks. In Advances in neural information processing systems, pp. 3269\u20133278, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suarez%2C%20Joseph%20Character-level%20language%20modeling%20with%20recurrent%20highway%20hypernetworks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suarez%2C%20Joseph%20Character-level%20language%20modeling%20with%20recurrent%20highway%20hypernetworks%202017"
        },
        {
            "id": "Sutskever_et+al_2011_a",
            "entry": "Sutskever, Ilya, Martens, James, and Hinton, Geoffrey E. Generating text with recurrent neural networks. In International Conference on Machine Learning, pp. 1017\u20131024, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Hinton%2C%20Geoffrey%20E.%20Generating%20text%20with%20recurrent%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Hinton%2C%20Geoffrey%20E.%20Generating%20text%20with%20recurrent%20neural%20networks%202011"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Wu, Yuhuai, Zhang, Saizheng, Zhang, Ying, Bengio, Yoshua, and Salakhutdinov, Ruslan. On Multiplicative Integration with Recurrent Neural Networks. In Advances in neural information processing systems, pp. 2864\u20132872, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Zhang%2C%20Saizheng%20Zhang%2C%20Ying%20Bengio%2C%20Yoshua%20On%20Multiplicative%20Integration%20with%20Recurrent%20Neural%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Zhang%2C%20Saizheng%20Zhang%2C%20Ying%20Bengio%2C%20Yoshua%20On%20Multiplicative%20Integration%20with%20Recurrent%20Neural%20Networks%202016"
        },
        {
            "id": "Yang_et+al_2018_a",
            "entry": "Yang, Zhilin, Dai, Zihang, Salakhutdinov, Ruslan, and Cohen, William W. Breaking the Softmax Bottleneck: A High-Rank RNN Language Model. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zhilin%20Dai%2C%20Zihang%20Salakhutdinov%2C%20Ruslan%20Cohen%2C%20William%20W.%20Breaking%20the%20Softmax%20Bottleneck%3A%20A%20High-Rank%20RNN%20Language%20Model%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zhilin%20Dai%2C%20Zihang%20Salakhutdinov%2C%20Ruslan%20Cohen%2C%20William%20W.%20Breaking%20the%20Softmax%20Bottleneck%3A%20A%20High-Rank%20RNN%20Language%20Model%202018"
        },
        {
            "id": "Zaremba_et+al_2015_a",
            "entry": "Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol. Recurrent Neural Network Regularization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaremba%20Wojciech%20Sutskever%20Ilya%20and%20Vinyals%20Oriol%20Recurrent%20Neural%20Network%20Regularization%20In%20International%20Conference%20on%20Learning%20Representations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaremba%20Wojciech%20Sutskever%20Ilya%20and%20Vinyals%20Oriol%20Recurrent%20Neural%20Network%20Regularization%20In%20International%20Conference%20on%20Learning%20Representations%202015"
        },
        {
            "id": "Zilly_et+al_2016_a",
            "entry": "Zilly, Julian Georg, Srivastava, Rupesh Kumar, Koutnik, Jan, and Schmidhuber, Jurgen. Recurrent Highway Networks. arXiv preprint, arXiv:1607.03474, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.03474"
        },
        {
            "id": "Zoph_2017_a",
            "entry": "Zoph, Barret and Le, Quoc V. Neural Architecture Search with Reinforcement Learning. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%2C%20Barret%20Le%2C%20Quoc%20V.%20Neural%20Architecture%20Search%20with%20Reinforcement%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoph%2C%20Barret%20Le%2C%20Quoc%20V.%20Neural%20Architecture%20Search%20with%20Reinforcement%20Learning%202017"
        }
    ]
}
