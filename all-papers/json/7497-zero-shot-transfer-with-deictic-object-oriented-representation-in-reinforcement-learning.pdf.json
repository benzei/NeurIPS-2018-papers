{
    "filename": "7497-zero-shot-transfer-with-deictic-object-oriented-representation-in-reinforcement-learning.pdf",
    "metadata": {
        "title": "Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning",
        "author": "Ofir Marom, Benjamin Rosman",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7497-zero-shot-transfer-with-deictic-object-oriented-representation-in-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Object-oriented representations in reinforcement learning have shown promise in transfer learning, with previous research introducing a propositional objectoriented framework that has provably efficient learning bounds with respect to sample complexity. However, this framework has limitations in terms of the classes of tasks it can efficiently learn. In this paper we introduce a novel deictic objectoriented framework that has provably efficient learning bounds and can solve a broader range of tasks. Additionally, we show that this framework is capable of zero-shot transfer of transition dynamics across tasks and demonstrate this empirically for the Taxi and Sokoban domains."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "Most commonly an reinforcement learning task is described as a discrete-time, finite-state and finite-action Markov decision process (MDP) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "As we formalise in section 2, Deictic OO-Markov decision process are defined in terms of a schema and so for all tasks instantiated from that schema the number of preconditions that the transition dynamics depend on remains constant",
        "The KWIK [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] framework generalises Rmax to a broader range of representations, such that if the transition dynamics can be learned with polynomial sample complexity under the KWIK protocol, it is possible to construct an Rmax type algorithm to learn a near-optimal policy for M",
        "In this paper we have introduced a novel deictic object-oriented representation for reinforcement learning",
        "We have shown that this representation can be described in terms of a schema that allows reuse of transition dynamics across grounded Markov decision process instantiated from that schema and that this allows for zero-shot transfer across such Markov decision process",
        "We conducted experiments on an extension of the Taxi domain as well as a more challenging Sokoban domain and have shown that zero-shot transfer of transition dynamics is possible with our representation"
    ],
    "key_statements": [
        "Most commonly an reinforcement learning task is described as a discrete-time, finite-state and finite-action Markov decision process (MDP) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "In this paper we propose to overcome this limitation of Propositional OO-Markov decision process with a novel deictic object-oriented representation, Deictic OO-Markov decision process",
        "As we formalise in section 2, Deictic OO-Markov decision process are defined in terms of a schema and so for all tasks instantiated from that schema the number of preconditions that the transition dynamics depend on remains constant",
        "Most commonly an reinforcement learning task is described as a discrete-time, finite-state and finite-action Markov decision process (MDP) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "The KWIK [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] framework generalises Rmax to a broader range of representations, such that if the transition dynamics can be learned with polynomial sample complexity under the KWIK protocol, it is possible to construct an Rmax type algorithm to learn a near-optimal policy for M",
        "In this paper we have introduced a novel deictic object-oriented representation for reinforcement learning",
        "We have shown that this representation can be described in terms of a schema that allows reuse of transition dynamics across grounded Markov decision process instantiated from that schema and that this allows for zero-shot transfer across such Markov decision process",
        "We conducted experiments on an extension of the Taxi domain as well as a more challenging Sokoban domain and have shown that zero-shot transfer of transition dynamics is possible with our representation"
    ],
    "summary": [
        "Most commonly an RL task is described as a discrete-time, finite-state and finite-action Markov decision process (MDP) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>].",
        "As we formalise in section 2, Deictic OO-MDPs are defined in terms of a schema and so for all tasks instantiated from that schema the number of preconditions that the transition dynamics depend on remains constant.",
        "Deictic OO-MDPs allow for the efficient learning of transition dynamics for classes of tasks not possible with propositional frameworks, as discussed in section 3.",
        "If an agent is currently in state s \u2208 SO and takes action a, the transition dynamics for MO,\u03c1 operate as follows: for each object o in s that is an instance of C and each attribute C.\u03b1 we compute the Boolean truth values B = {fi(o, s)}Di=a1,C.\u03b1 for the deictic predicates in Fa,C.\u03b1.",
        "The transition dynamics for this attribute depend on three preconditions for which we require a deictic passenger object: is a taxi on the same square as passenger?",
        "It is beneficial to contrast the Deictic OO-MDP representation introduced in subsection 2.2 to Propositional OO-MDPs. Propositions alone are insufficient to represent the transition dynamics for the all-passenger any-destination Taxi domain.",
        "Given a set of D preconditions we want to learn the transition dynamics for each attribute C.\u03b1 and action a.",
        "The main assumption required for DOORM AX to be correct is that the transition dynamics for each attribute and action must be representable as a full binary tree with propositions at the non-leaf nodes and effects at the leaf nodes.",
        "For attribute C.\u03b1 and action a let Fbe a set of size D that contains hypothesised deictic predicate preconditions on which the transition dynamics of C.\u03b1 when taking action a may depend.",
        "The essence of the procedure is that for an attribute C.\u03b1 and action a the input is a deictic object o that is an instance of C and a state s, and it is required that for each effect type in Gwe can make a prediction that is not \u22a5 and further that all the predictions of o.\u03b1 are the same, otherwise return \u22a5.",
        "In these experiments we stop after 100 steps for each episode of the training MDPs. for the deictic representation with transfer we transfer the learned transition dynamics of n = 4 passengers and do no additional learning on the new larger gridworld.",
        "This simple toy MDP with 7, 961 states is enough to completely learn the schema transition dynamics of this domain under the Deictic OO-MDP representation.",
        "We conducted experiments on an extension of the Taxi domain as well as a more challenging Sokoban domain and have shown that zero-shot transfer of transition dynamics is possible with our representation"
    ],
    "headline": "In this paper we introduce a novel deictic objectoriented framework that has provably efficient learning bounds and can solve a broader range of tasks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3:213\u2013231, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brafman%2C%20R.I.%20Tennenholtz%2C%20M.%20R-max%20-%20a%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brafman%2C%20R.I.%20Tennenholtz%2C%20M.%20R-max%20-%20a%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202002"
        },
        {
            "id": "2",
            "entry": "[2] T. G. Dietterich. The maxq method for hierarchical reinforcement learning. Proceedings of the 15th International Conference on Machine Learning, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dietterich%2C%20T.G.%20The%20maxq%20method%20for%20hierarchical%20reinforcement%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dietterich%2C%20T.G.%20The%20maxq%20method%20for%20hierarchical%20reinforcement%20learning%201998"
        },
        {
            "id": "3",
            "entry": "[3] C. Diuk. An object-oriented representation for efficient reinforcement learning. PhD thesis, Rutgers The State University of New Jersey-New Brunswick, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diuk%2C%20C.%20An%20object-oriented%20representation%20for%20efficient%20reinforcement%20learning%202010"
        },
        {
            "id": "4",
            "entry": "[4] C. Diuk, A. Cohen, and M. L. Littman. An object-oriented representation for efficient reinforcement learning. Proceedings of the 25th International Conference on Machine learning, pages 240\u2013247, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diuk%2C%20C.%20Cohen%2C%20A.%20Littman%2C%20M.L.%20An%20object-oriented%20representation%20for%20efficient%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diuk%2C%20C.%20Cohen%2C%20A.%20Littman%2C%20M.L.%20An%20object-oriented%20representation%20for%20efficient%20reinforcement%20learning%202008"
        },
        {
            "id": "5",
            "entry": "[5] S. D\u017eeroski, L. DeRaedt, and K. Driessens. Relational reinforcement learning. Machine Learning Journal, 43:7\u201352, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%C5%BEeroski%2C%20S.%20DeRaedt%2C%20L.%20Driessens%2C%20K.%20Relational%20reinforcement%20learning%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%C5%BEeroski%2C%20S.%20DeRaedt%2C%20L.%20Driessens%2C%20K.%20Relational%20reinforcement%20learning%202001"
        },
        {
            "id": "6",
            "entry": "[6] S. Finney, N. Gardiol, L. P. Kaelbling, and T. Oates. Learning with deictic representations. Technical report, Massachusetts Institute of Technology, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finney%2C%20S.%20Gardiol%2C%20N.%20Kaelbling%2C%20L.P.%20Oates%2C%20T.%20Learning%20with%20deictic%20representations%202002"
        },
        {
            "id": "7",
            "entry": "[7] C. Guestrin, D. Koller, C. Gearhart, and N. Kanodia. Generalizing plans to new environments in relational mdps. Proceedings of the 18th Joint Conference on Artificial Intelligence, pages 1003\u20131010, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guestrin%2C%20C.%20Koller%2C%20D.%20Gearhart%2C%20C.%20Kanodia%2C%20N.%20Generalizing%20plans%20to%20new%20environments%20in%20relational%20mdps%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guestrin%2C%20C.%20Koller%2C%20D.%20Gearhart%2C%20C.%20Kanodia%2C%20N.%20Generalizing%20plans%20to%20new%20environments%20in%20relational%20mdps%202003"
        },
        {
            "id": "8",
            "entry": "[8] K. Kansky, T. Silver, D. A. M\u00e8ly, M. Eldawy, M. L\u00e0zaro-Gredilla, X. Lou, N. Dorfman, S. Sidor, S. Phoenix, and D. George. Schema networks: Zero-shot transfer with a generative causal model of intuitive physics. Proceedings of the 34th International Conference on Machine Learning, pages 1809\u20131818, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schema%20networks%3A%20Zero-shot%20transfer%20with%20a%20generative%20causal%20model%20of%20intuitive%20physics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schema%20networks%3A%20Zero-shot%20transfer%20with%20a%20generative%20causal%20model%20of%20intuitive%20physics%202017"
        },
        {
            "id": "9",
            "entry": "[9] G. Konidaris and A. Barto. Building portable options: Skill transfer in reinforcement learning. Proceedings of the Twentieth International Joint Conference on Artificial Intelligence, pages 895\u2013900, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20G.%20Barto%2C%20A.%20Building%20portable%20options%3A%20Skill%20transfer%20in%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20G.%20Barto%2C%20A.%20Building%20portable%20options%3A%20Skill%20transfer%20in%20reinforcement%20learning%202007"
        },
        {
            "id": "10",
            "entry": "[10] L. Li, M. L. Littman, and T. J. Walsh. Knows what it knows: A framework for self-aware learning. Proceedings of the 25th International Conference on Machine Learning, pages 568\u2013575, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20Littman%2C%20M.L.%20Walsh%2C%20T.J.%20Knows%20what%20it%20knows%3A%20A%20framework%20for%20self-aware%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20Littman%2C%20M.L.%20Walsh%2C%20T.J.%20Knows%20what%20it%20knows%3A%20A%20framework%20for%20self-aware%20learning%202008"
        },
        {
            "id": "11",
            "entry": "[11] B. Ravindran and A. Barto. Relativized options: choosing the right transformation. Proceedings of the 12th Yale Workshop on Adaptive and Learning Systems, pages 109\u2013114, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravindran%2C%20B.%20Barto%2C%20A.%20Relativized%20options%3A%20choosing%20the%20right%20transformation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravindran%2C%20B.%20Barto%2C%20A.%20Relativized%20options%3A%20choosing%20the%20right%20transformation%202003"
        },
        {
            "id": "12",
            "entry": "[12] J. Scholz, M. Levihn, C. L. Isbell, and D. Wingate. A physics-based model prior for objectoriented mdps. Proceedings of the 31st International Conference on Machine Learning, pages 1089\u20131097, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholz%2C%20J.%20Levihn%2C%20M.%20Isbell%2C%20C.L.%20Wingate%2C%20D.%20A%20physics-based%20model%20prior%20for%20objectoriented%20mdps%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scholz%2C%20J.%20Levihn%2C%20M.%20Isbell%2C%20C.L.%20Wingate%2C%20D.%20A%20physics-based%20model%20prior%20for%20objectoriented%20mdps%202014"
        },
        {
            "id": "13",
            "entry": "[13] R. S. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.%20Reinforcement%20Learning%3A%20An%20Introduction%201998"
        },
        {
            "id": "14",
            "entry": "[14] M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(1):1633\u20131685, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20M.E.%20Stone%2C%20P.%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20M.E.%20Stone%2C%20P.%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009"
        }
    ]
}
