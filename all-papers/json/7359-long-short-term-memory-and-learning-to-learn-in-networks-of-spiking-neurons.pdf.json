{
    "filename": "7359-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons.pdf",
    "metadata": {
        "title": "Long short-term memory and Learning-to-learn in networks of spiking neurons",
        "author": "Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, Wolfgang Maass",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7359-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with ANNs. We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning."
    },
    "keywords": [
        {
            "term": "artificial neural networks",
            "url": "https://en.wikipedia.org/wiki/artificial_neural_networks"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "long short term memory",
            "url": "https://en.wikipedia.org/wiki/long_short_term_memory"
        },
        {
            "term": "short term memory",
            "url": "https://en.wikipedia.org/wiki/short_term_memory"
        },
        {
            "term": "pre-frontal cortex",
            "url": "https://en.wikipedia.org/wiki/pre-frontal_cortex"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "backpropagation through time",
            "url": "https://en.wikipedia.org/wiki/backpropagation_through_time"
        },
        {
            "term": "BPTT",
            "url": "https://en.wikipedia.org/wiki/BPTT"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Recurrent networks of spiking neurons (RSNNs) are frequently studied as models for networks of neurons in the brain",
        "Altogether this demo shows that meta reinforcement learning can be applied to Recurrent networks of spiking neurons, and produces previously not seen capabilities of sparsely firing Recurrent networks of spiking neurons to extract abstract knowledge from experimentation, and to use it in clever ways for controlling behaviour",
        "We have demonstrated that deep learning provides a useful new tool for the investigation of networks of spiking neurons: It allows us to create architectures and learning algorithms for Recurrent networks of spiking neurons with enhanced computing and learning capabilities",
        "We have shown in section 4 that this method allows us to create sparsely connected Recurrent networks of spiking neurons that approach the performance of LSTM networks on common benchmark tasks for the classification of spatio-temporal patterns",
        "This qualitative jump in the computational power of Recurrent networks of spiking neurons was supported by the introduction of adapting neurons into the model",
        "Adapting neurons introduce a spread of longer time constants into Recurrent networks of spiking neurons, as they do in the neocortex according to [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]"
    ],
    "key_statements": [
        "Recurrent networks of spiking neurons (RSNNs) are frequently studied as models for networks of neurons in the brain",
        "One difference between Recurrent networks of spiking neurons in the brain and Recurrent networks of spiking neurons models is that Recurrent networks of spiking neurons in the brain have been optimized for their function through long evolutionary processes, complemented by a sophisticated learning curriculum during development",
        "We show that it makes L2L applicable to Recurrent networks of spiking neurons, as for LSTM networks",
        "We reduced (\u201cdampened\u201d) the amplitude of the pseudo-derivative by a factor < 1. This enhances the performance of backpropagation through time for Recurrent networks of spiking neurons that compute during larger time spans, that require backpropagation through several 1000 layers of an unrolled feedforward network of spiking neurons",
        "Results: Most of the functions that are computed by target network from the class F are nonlinear, as illustrated in Fig. 2G for the case of inputs (x1, x2) with x1 = x2",
        "Learning the input/output behaviour of any such target network with biologically realistic local plasticity mechanisms presents a daunting challenge for a SNN",
        "In view of the fact that each target network is defined by 40 parameters, it comes at some surprise that the resulting network learning algorithm of the LSNN for learning the input/output behaviour of a new target network produces in general a good approximation of the target network after just 5 to 20 trials, where in each trial one randomly drawn labelled example is presented",
        "Fig. 2H shows the fast evolution of internal models of the LSNN for the target network during the first trials",
        "One sees that the internal model of the LSNN is from the beginning a smooth function, of the same type as the ones defined by the target network in F",
        "The LSNN had acquired during the training in the outer loop of L2L a prior for the types of functions that are to be learnt, that was encoded in its synaptic weights. This prior was quite efficient, since Fig. 2E and F show that the LSNN was able to learn a target network with substantially fewer trials than a generic learning algorithm for learning the target network directly in an artificial neural network as in Fig. 2A: BP with a prior that favored small weights and biases",
        "Altogether this demo shows that meta reinforcement learning can be applied to Recurrent networks of spiking neurons, and produces previously not seen capabilities of sparsely firing Recurrent networks of spiking neurons to extract abstract knowledge from experimentation, and to use it in clever ways for controlling behaviour",
        "We have demonstrated that deep learning provides a useful new tool for the investigation of networks of spiking neurons: It allows us to create architectures and learning algorithms for Recurrent networks of spiking neurons with enhanced computing and learning capabilities",
        "We have shown in section 4 that this method allows us to create sparsely connected Recurrent networks of spiking neurons that approach the performance of LSTM networks on common benchmark tasks for the classification of spatio-temporal patterns",
        "This qualitative jump in the computational power of Recurrent networks of spiking neurons was supported by the introduction of adapting neurons into the model",
        "Adapting neurons introduce a spread of longer time constants into Recurrent networks of spiking neurons, as they do in the neocortex according to [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]",
        "We have shown in Fig. 2 that an application of deep learning (BPTT and DEEP R) in the outer loop of L2L provides a new paradigm for learning of nonlinear input/output mappings by a Recurrent networks of spiking neurons"
    ],
    "summary": [
        "Recurrent networks of spiking neurons (RSNNs) are frequently studied as models for networks of neurons in the brain.",
        "We refer to the resulting type of RSNNs as Long short-term memory Spiking Neural Networks (LSNNs).",
        "This enhances the performance of BPTT for RSNNs that compute during larger time spans, that require backpropagation through several 1000 layers of an unrolled feedforward network of spiking neurons.",
        "The network is forced to encode all results from learning the current task C in its internal state, in particular in its firing activity and the thresholds of adapting neurons.",
        "The L2L results of [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] show that LSTM networks can learn nonlinear functions from a teacher without modifying their synaptic weights, using their short-term memory instead.",
        "It produced outputs in the form of weighted spike counts during 20 ms windows from all neurons in the network, where the weights for this linear readout were trained, like all weights inside the LSNN, in the outer loop, and remained fixed during learning of a particular TN.",
        "The weights of the LSNN remained fixed, and it was required to learn the input/output behaviour of TNs from F that it had never seen before in an online manner by just using its short-term memory and dynamics.",
        "Fig. 2C shows that after a few thousand training iterations in the outer loop, the LSNN achieves low MSE for learning new TNs from the family F, significantly surpassing the performance of an optimal linear approximator that was trained on all 500 pairs of inputs and target outputs, see orange curve in Fig. 2C,E.",
        "In view of the fact that each TN is defined by 40 parameters, it comes at some surprise that the resulting network learning algorithm of the LSNN for learning the input/output behaviour of a new TN produces in general a good approximation of the TN after just 5 to 20 trials, where in each trial one randomly drawn labelled example is presented.",
        "We have demonstrated that deep learning provides a useful new tool for the investigation of networks of spiking neurons: It allows us to create architectures and learning algorithms for RSNNs with enhanced computing and learning capabilities.",
        "We have shown in section 4 that this method allows us to create sparsely connected RSNNs that approach the performance of LSTM networks on common benchmark tasks for the classification of spatio-temporal patterns.",
        "We have shown in Fig. 2 that an application of deep learning (BPTT and DEEP R) in the outer loop of L2L provides a new paradigm for learning of nonlinear input/output mappings by a RSNN."
    ],
    "headline": "We address two possible reasons for that",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "2",
            "entry": "[2] Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, and Dharmendra S. Modha. Convolutional networks for fast, energy-efficient neuromorphic computing. Proceedings of the National Academy of Sciences, 113(41):11441\u201311446, November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esser%2C%20Steven%20K.%20Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Cassidy%2C%20Andrew%20S.%20Convolutional%20networks%20for%20fast%2C%20energy-efficient%20neuromorphic%20computing%202016-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esser%2C%20Steven%20K.%20Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Cassidy%2C%20Andrew%20S.%20Convolutional%20networks%20for%20fast%2C%20energy-efficient%20neuromorphic%20computing%202016-11"
        },
        {
            "id": "3",
            "entry": "[3] David Kappel, Robert Legenstein, Stefan Habenschuss, Michael Hsieh, and Wolfgang Maass. Rewardbased stochastic self-configuration of neural circuits. eNEURO, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kappel%2C%20David%20Legenstein%2C%20Robert%20Habenschuss%2C%20Stefan%20Hsieh%2C%20Michael%20Rewardbased%20stochastic%20self-configuration%20of%20neural%20circuits%202018"
        },
        {
            "id": "4",
            "entry": "[4] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellec%2C%20Guillaume%20Kappel%2C%20David%20Maass%2C%20Wolfgang%20Legenstein%2C%20Robert%20Deep%20rewiring%3A%20Training%20very%20sparse%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellec%2C%20Guillaume%20Kappel%2C%20David%20Maass%2C%20Wolfgang%20Legenstein%2C%20Robert%20Deep%20rewiring%3A%20Training%20very%20sparse%20deep%20networks%202018"
        },
        {
            "id": "5",
            "entry": "[5] Uri Hasson, Janice Chen, and Christopher J Honey. Hierarchical process memory: memory as an integral component of information processing. Trends in cognitive sciences, 19(6):304\u2013313, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hasson%2C%20Uri%20Chen%2C%20Janice%20Honey%2C%20Christopher%20J.%20Hierarchical%20process%20memory%3A%20memory%20as%20an%20integral%20component%20of%20information%20processing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hasson%2C%20Uri%20Chen%2C%20Janice%20Honey%2C%20Christopher%20J.%20Hierarchical%20process%20memory%3A%20memory%20as%20an%20integral%20component%20of%20information%20processing%202015"
        },
        {
            "id": "6",
            "entry": "[6] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05763"
        },
        {
            "id": "7",
            "entry": "[7] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "8",
            "entry": "[8] Jane X Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Demis Hassabis, and Matthew Botvinick. Prefrontal cortex as a meta-reinforcement learning system. Nature Neuroscience, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jane%20X.%20Kurth-Nelson%2C%20Zeb%20Kumaran%2C%20Dharshan%20Tirumala%2C%20Dhruva%20Prefrontal%20cortex%20as%20a%20meta-reinforcement%20learning%20system%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jane%20X.%20Kurth-Nelson%2C%20Zeb%20Kumaran%2C%20Dharshan%20Tirumala%2C%20Dhruva%20Prefrontal%20cortex%20as%20a%20meta-reinforcement%20learning%20system%202018"
        },
        {
            "id": "9",
            "entry": "[9] Johannes Schemmel, Daniel Bruderle, Andreas Grubl, Matthias Hock, Karlheinz Meier, and Sebastian Millner. A wafer-scale neuromorphic hardware system for large-scale neural modeling. In Circuits and systems (ISCAS), proceedings of 2010 IEEE international symposium on, pages 1947\u20131950. IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schemmel%2C%20Johannes%20Bruderle%2C%20Daniel%20Grubl%2C%20Andreas%20Hock%2C%20Matthias%20A%20wafer-scale%20neuromorphic%20hardware%20system%20for%20large-scale%20neural%20modeling%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schemmel%2C%20Johannes%20Bruderle%2C%20Daniel%20Grubl%2C%20Andreas%20Hock%2C%20Matthias%20A%20wafer-scale%20neuromorphic%20hardware%20system%20for%20large-scale%20neural%20modeling%202010"
        },
        {
            "id": "10",
            "entry": "[10] Steve B Furber, David R Lester, Luis A Plana, Jim D Garside, Eustace Painkras, Steve Temple, and Andrew D Brown. Overview of the spinnaker system architecture. IEEE Transactions on Computers, 62(12):2454\u20132467, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Furber%2C%20Steve%20B.%20Lester%2C%20David%20R.%20Plana%2C%20Luis%20A.%20Garside%2C%20Jim%20D.%20Overview%20of%20the%20spinnaker%20system%20architecture%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Furber%2C%20Steve%20B.%20Lester%2C%20David%20R.%20Plana%2C%20Luis%20A.%20Garside%2C%20Jim%20D.%20Overview%20of%20the%20spinnaker%20system%20architecture%202013"
        },
        {
            "id": "11",
            "entry": "[11] Ning Qiao, Hesham Mostafa, Federico Corradi, Marc Osswald, Fabio Stefanini, Dora Sumislawska, and Giacomo Indiveri. A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses. Frontiers in neuroscience, 9:141, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qiao%2C%20Ning%20Mostafa%2C%20Hesham%20Corradi%2C%20Federico%20Osswald%2C%20Marc%20A%20reconfigurable%20on-line%20learning%20spiking%20neuromorphic%20processor%20comprising%20256%20neurons%20and%20128k%20synapses%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qiao%2C%20Ning%20Mostafa%2C%20Hesham%20Corradi%2C%20Federico%20Osswald%2C%20Marc%20A%20reconfigurable%20on-line%20learning%20spiking%20neuromorphic%20processor%20comprising%20256%20neurons%20and%20128k%20synapses%202015"
        },
        {
            "id": "12",
            "entry": "[12] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro, 38(1):82\u201399, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davies%2C%20Mike%20Srinivasa%2C%20Narayan%20Lin%2C%20Tsung-Han%20Chinya%2C%20Gautham%20Loihi%3A%20A%20neuromorphic%20manycore%20processor%20with%20on-chip%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davies%2C%20Mike%20Srinivasa%2C%20Narayan%20Lin%2C%20Tsung-Han%20Chinya%2C%20Gautham%20Loihi%3A%20A%20neuromorphic%20manycore%20processor%20with%20on-chip%20learning%202018"
        },
        {
            "id": "13",
            "entry": "[13] Chris Eliasmith. How to build a brain: A neural architecture for biological cognition. Oxford University Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eliasmith%2C%20Chris%20How%20to%20build%20a%20brain%3A%20A%20neural%20architecture%20for%20biological%20cognition%202013"
        },
        {
            "id": "14",
            "entry": "[14] Brian DePasquale, Mark M Churchland, and LF Abbott. Using firing-rate dynamics to train recurrent networks of spiking model neurons. arXiv preprint arXiv:1601.07620, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.07620"
        },
        {
            "id": "15",
            "entry": "[15] Dongsung Huh and Terrence J Sejnowski. Gradient descent for spiking neural networks. arXiv preprint arXiv:1706.04698, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04698"
        },
        {
            "id": "16",
            "entry": "[16] Wilten Nicola and Claudia Clopath. Supervised learning in spiking neural networks with force training. Nature communications, 8(1):2208, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nicola%2C%20Wilten%20Clopath%2C%20Claudia%20Supervised%20learning%20in%20spiking%20neural%20networks%20with%20force%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nicola%2C%20Wilten%20Clopath%2C%20Claudia%20Supervised%20learning%20in%20spiking%20neural%20networks%20with%20force%20training%202017"
        },
        {
            "id": "17",
            "entry": "[17] Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. Computational properties of networks of spiking neurons with adapting neurons; in preparation. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellec%2C%20Guillaume%20Salaj%2C%20Darjan%20Subramoney%2C%20Anand%20Legenstein%2C%20Robert%20Computational%20properties%20of%20networks%20of%20spiking%20neurons%20with%20adapting%20neurons%3B%20in%20preparation%202018"
        },
        {
            "id": "18",
            "entry": "[18] Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerstner%2C%20Wulfram%20Kistler%2C%20Werner%20M.%20Naud%2C%20Richard%20Paninski%2C%20Liam%20Neuronal%20dynamics%3A%20From%20single%20neurons%20to%20networks%20and%20models%20of%20cognition%202014"
        },
        {
            "id": "19",
            "entry": "[19] Christian Pozzorini, Skander Mensi, Olivier Hagens, Richard Naud, Christof Koch, and Wulfram Gerstner. Automated high-throughput characterization of single neurons by means of simplified spiking models. PLoS computational biology, 11(6):e1004275, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pozzorini%2C%20Christian%20Mensi%2C%20Skander%20Hagens%2C%20Olivier%20Naud%2C%20Richard%20Automated%20high-throughput%20characterization%20of%20single%20neurons%20by%20means%20of%20simplified%20spiking%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pozzorini%2C%20Christian%20Mensi%2C%20Skander%20Hagens%2C%20Olivier%20Naud%2C%20Richard%20Automated%20high-throughput%20characterization%20of%20single%20neurons%20by%20means%20of%20simplified%20spiking%20models%202015"
        },
        {
            "id": "20",
            "entry": "[20] Nathan W Gouwens, Jim Berg, David Feng, Staci A Sorensen, Hongkui Zeng, Michael J Hawrylycz, Christof Koch, and Anton Arkhipov. Systematic generation of biophysically detailed models for diverse cortical neuron types. Nature communications, 9(1), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gouwens%2C%20Nathan%20W.%20Berg%2C%20Jim%20Feng%2C%20David%20Sorensen%2C%20Staci%20A.%20Systematic%20generation%20of%20biophysically%20detailed%20models%20for%20diverse%20cortical%20neuron%20types%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gouwens%2C%20Nathan%20W.%20Berg%2C%20Jim%20Feng%2C%20David%20Sorensen%2C%20Staci%20A.%20Systematic%20generation%20of%20biophysically%20detailed%20models%20for%20diverse%20cortical%20neuron%20types%202018"
        },
        {
            "id": "21",
            "entry": "[21] Corinne Teeter, Ramakrishnan Iyer, Vilas Menon, Nathan Gouwens, David Feng, Jim Berg, Aaron Szafer, Nicholas Cain, Hongkui Zeng, Michael Hawrylycz, et al. Generalized leaky integrate-and-fire models classify multiple neuron types. Nature communications, 1(1):1\u201315, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teeter%2C%20Corinne%20Iyer%2C%20Ramakrishnan%20Menon%2C%20Vilas%20Gouwens%2C%20Nathan%20Generalized%20leaky%20integrate-and-fire%20models%20classify%20multiple%20neuron%20types%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teeter%2C%20Corinne%20Iyer%2C%20Ramakrishnan%20Menon%2C%20Vilas%20Gouwens%2C%20Nathan%20Generalized%20leaky%20integrate-and-fire%20models%20classify%20multiple%20neuron%20types%202018"
        },
        {
            "id": "22",
            "entry": "[22] Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc\u2019Aurelio Ranzato. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.7753"
        },
        {
            "id": "23",
            "entry": "[23] David Kappel, Stefan Habenschuss, Robert Legenstein, and Wolfgang Maass. Network Plasticity as Bayesian Inference. PLOS Computational Biology, 11(11):e1004485, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kappel%2C%20David%20Habenschuss%2C%20Stefan%20Legenstein%2C%20Robert%20Maass%2C%20Wolfgang%20Network%20Plasticity%20as%20Bayesian%20Inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kappel%2C%20David%20Habenschuss%2C%20Stefan%20Legenstein%2C%20Robert%20Maass%2C%20Wolfgang%20Network%20Plasticity%20as%20Bayesian%20Inference%202015"
        },
        {
            "id": "24",
            "entry": "[24] Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent networks of rectified linear units. CoRR, abs/1504.00941, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00941"
        },
        {
            "id": "25",
            "entry": "[25] Rui Costa, Ioannis Alexandros Assael, Brendan Shillingford, Nando de Freitas, and Tim Vogels. Cortical microcircuits as gated-recurrent neural networks. In Advances in Neural Information Processing Systems, pages 272\u2013283, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Costa%2C%20Rui%20Assael%2C%20Ioannis%20Alexandros%20Shillingford%2C%20Brendan%20de%20Freitas%2C%20Nando%20Cortical%20microcircuits%20as%20gated-recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Costa%2C%20Rui%20Assael%2C%20Ioannis%20Alexandros%20Shillingford%2C%20Brendan%20de%20Freitas%2C%20Nando%20Cortical%20microcircuits%20as%20gated-recurrent%20neural%20networks%202017"
        },
        {
            "id": "26",
            "entry": "[26] Klaus Greff, Rupesh K Srivastava, Jan Koutn\u0131k, Bas R Steunebrink, and Jurgen Schmidhuber. LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greff%2C%20Klaus%20Srivastava%2C%20Rupesh%20K.%20Koutn%C4%B1k%2C%20Jan%20Steunebrink%2C%20Bas%20R.%20LSTM%3A%20A%20search%20space%20odyssey%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greff%2C%20Klaus%20Srivastava%2C%20Rupesh%20K.%20Koutn%C4%B1k%2C%20Jan%20Steunebrink%2C%20Bas%20R.%20LSTM%3A%20A%20search%20space%20odyssey%202017"
        },
        {
            "id": "27",
            "entry": "[27] Anand Subramoney, Guillaume Bellec, Franz Scherr, Robert Legenstein, and Wolfgang Maass. Recurrent networks of spiking neurons learn to learn; in preparation. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Subramoney%2C%20Anand%20Bellec%2C%20Guillaume%20Scherr%2C%20Franz%20Legenstein%2C%20Robert%20Recurrent%20networks%20of%20spiking%20neurons%20learn%20to%20learn%3B%20in%20preparation%202018"
        },
        {
            "id": "28",
            "entry": "[28] Matthew G Perich, Juan A Gallego, and Lee E Miller. A neural population mechanism for rapid learning. Neuron, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perich%2C%20Matthew%20G.%20Gallego%2C%20Juan%20A.%20Miller%2C%20Lee%20E.%20A%20neural%20population%20mechanism%20for%20rapid%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perich%2C%20Matthew%20G.%20Gallego%2C%20Juan%20A.%20Miller%2C%20Lee%20E.%20A%20neural%20population%20mechanism%20for%20rapid%20learning%202018"
        },
        {
            "id": "29",
            "entry": "[29] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks, pages 87\u201394.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sepp%20Hochreiter%2C%20A.Steven%20Younger%20Conwell%2C%20Peter%20R.%20Learning%20to%20learn%20using%20gradient%20descent",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sepp%20Hochreiter%2C%20A.Steven%20Younger%20Conwell%2C%20Peter%20R.%20Learning%20to%20learn%20using%20gradient%20descent"
        },
        {
            "id": "30",
            "entry": "[30] Richard Morris. Developments of a water-maze procedure for studying spatial learning in the rat. Journal of neuroscience methods, 11(1):47\u201360, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morris%2C%20Richard%20Developments%20of%20a%20water-maze%20procedure%20for%20studying%20spatial%20learning%20in%20the%20rat%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morris%2C%20Richard%20Developments%20of%20a%20water-maze%20procedure%20for%20studying%20spatial%20learning%20in%20the%20rat%201984"
        },
        {
            "id": "31",
            "entry": "[31] Eleni Vasilaki, Nicolas Fremaux, Robert Urbanczik, Walter Senn, and Wulfram Gerstner. Spike-based reinforcement learning in continuous state and action space: when policy gradient methods fail. PLoS computational biology, 5(12):e1000586, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vasilaki%2C%20Eleni%20Fremaux%2C%20Nicolas%20Urbanczik%2C%20Robert%20Senn%2C%20Walter%20Spike-based%20reinforcement%20learning%20in%20continuous%20state%20and%20action%20space%3A%20when%20policy%20gradient%20methods%20fail%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vasilaki%2C%20Eleni%20Fremaux%2C%20Nicolas%20Urbanczik%2C%20Robert%20Senn%2C%20Walter%20Spike-based%20reinforcement%20learning%20in%20continuous%20state%20and%20action%20space%3A%20when%20policy%20gradient%20methods%20fail%202009"
        },
        {
            "id": "32",
            "entry": "[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "33",
            "entry": "[33] Allen Institute. c 2018 Allen Institute for Brain Science. Allen Cell Types Database, cell feature search. Available from: celltypes.brain-map.org/data. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen%20Institute%20c%202018%20Allen%20Institute%20for%20Brain%20Science%20Allen%20Cell%20Types%20Database%20cell%20feature%20search%20Available%20from%20celltypesbrainmaporgdata%202018"
        },
        {
            "id": "34",
            "entry": "[34] Gianluigi Mongillo, Omri Barak, and Misha Tsodyks. Synaptic theory of working memory. Science (New York, N.Y.), 319(5869):1543\u20131546, March 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mongillo%2C%20Gianluigi%20Barak%2C%20Omri%20Tsodyks%2C%20Misha%20Synaptic%20theory%20of%20working%20memory%205869-03"
        },
        {
            "id": "35",
            "entry": "[35] Mark G. Stokes. \u2018Activity-silent\u2019 working memory in prefrontal cortex: a dynamic coding framework. Trends in Cognitive Sciences, 19(7):394\u2013405, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stokes%2C%20Mark%20G.%20%E2%80%98Activity-silent%E2%80%99%20working%20memory%20in%20prefrontal%20cortex%3A%20a%20dynamic%20coding%20framework%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stokes%2C%20Mark%20G.%20%E2%80%98Activity-silent%E2%80%99%20working%20memory%20in%20prefrontal%20cortex%3A%20a%20dynamic%20coding%20framework%202015"
        }
    ]
}
