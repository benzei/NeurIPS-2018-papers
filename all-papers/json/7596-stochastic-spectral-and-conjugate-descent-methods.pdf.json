{
    "filename": "7596-stochastic-spectral-and-conjugate-descent-methods.pdf",
    "metadata": {
        "title": "Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems",
        "author": "Yu. Nesterov",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7596-stochastic-spectral-and-conjugate-descent-methods.pdf",
            "doi": "10.1137/100802001"
        },
        "journal": "SIAM Journal on Optimization",
        "volume": "22",
        "abstract": "The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.",
        "date": 2012,
        "pages": "341-362"
    },
    "keywords": [
        {
            "term": "importance sampling",
            "url": "https://en.wikipedia.org/wiki/importance_sampling"
        },
        {
            "term": "random permutation",
            "url": "https://en.wikipedia.org/wiki/random_permutation"
        },
        {
            "term": "linear rate",
            "url": "https://en.wikipedia.org/wiki/linear_rate"
        },
        {
            "term": "semidefinite programming",
            "url": "https://en.wikipedia.org/wiki/semidefinite_programming"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "linear system",
            "url": "https://en.wikipedia.org/wiki/linear_system"
        }
    ],
    "highlights": [
        "An increasing array of learning and training tasks reduce to optimization problem in very large dimensions",
        "The state-of-the-art algorithms in this regime are based on randomized coordinate descent (RCD)",
        "Various acceleration strategies were proposed for Randomized Coordinate Descent in the literature in recent years, based on techniques such as Nesterov\u2019s momentum [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], heavy ball momentum [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], importance sampling [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], adaptive sampling [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], random permutations [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], greedy rules [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], mini-batching [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], and locality breaking [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "In this paper we introduce a fundamentally new type of acceleration strategy for Randomized Coordinate Descent which relies on the idea of enriching the set of coordinate directions {e1, e2, . . . , en} in Rn, which are used in Randomized Coordinate Descent as directions of descent, via the addition of a few spectral or conjugate directions",
        "As a warm-up, we first ask: how important is importance sampling? More precisely, we investigate Randomized Coordinate Descent with probabilities pi \u221d Aii, and Randomized Coordinate Descent with probabilities pi \u221d Ai: 2, considered as Randomized Coordinate Descent with \u201cimportance sampling\u201d, and compare these with the baseline Randomized Coordinate Descent with uniform probabilities",
        "We show that mini-batch SSCD is optimal among a certain parametric family of methods, and that its rate improves as k increases"
    ],
    "key_statements": [
        "An increasing array of learning and training tasks reduce to optimization problem in very large dimensions",
        "The state-of-the-art algorithms in this regime are based on randomized coordinate descent (RCD)",
        "Various acceleration strategies were proposed for Randomized Coordinate Descent in the literature in recent years, based on techniques such as Nesterov\u2019s momentum [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], heavy ball momentum [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], importance sampling [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], adaptive sampling [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], random permutations [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], greedy rules [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], mini-batching [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], and locality breaking [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "In this paper we introduce a fundamentally new type of acceleration strategy for Randomized Coordinate Descent which relies on the idea of enriching the set of coordinate directions {e1, e2, . . . , en} in Rn, which are used in Randomized Coordinate Descent as directions of descent, via the addition of a few spectral or conjugate directions",
        "The algorithms we develop and analyze in this paper randomize over this enriched larger set of directions",
        "[<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] that if the probabilities are proportional to the diagonal elements of A, the random iterates of Randomized Coordinate Descent satisfy E",
        "We extend all results discussed above for Stochastic Spectral Descent, including the rate (6), to the more general class of methods we call stochastic conjugate descent (SconD), for which D is the uniform distribution over vectors v1, . . . , vn which are mutually A conjugate: vi Avj = 0 for i = j and vi Avi = 1.\n1.4",
        "As a warm-up, we first ask: how important is importance sampling? More precisely, we investigate Randomized Coordinate Descent with probabilities pi \u221d Aii, and Randomized Coordinate Descent with probabilities pi \u221d Ai: 2, considered as Randomized Coordinate Descent with \u201cimportance sampling\u201d, and compare these with the baseline Randomized Coordinate Descent with uniform probabilities",
        "We show that for any n \u2265 2 and any T > 0, there is a matrix A such that the rate of Randomized Coordinate Descent with any probabilities is O(T log 1 )",
        "We show that in contrast with the situation above, adding a few of the \u201clargest\u201d eigenvectors to the coordinate directions of Randomized Coordinate Descent does not help",
        "We extend Stochastic spectral coordinate descent to a mini-batch setting; we call the new method mini-batch SSCD",
        "We show that the rate of mini-batch SSCD interpolates between the rate of mini-batch Randomized Coordinate Descent and rate of Stochastic Spectral Descent",
        "We show that mini-batch SSCD is optimal among a certain parametric family of methods, and that its rate improves as k increases",
        "We formalize it as Algorithm 1, and equip it with a stepsize, which will be useful in Section A.1, where we study mini-batch version of Stochastic Descent",
        "The same rate as in Theorem 2 holds for the stochastic conjugate descent (SconD) method, which arises as a special case of stochastic descent for \u03c9 = 1 and D being a uniform distribution over a set of A-orthogonal vectors",
        "We show that there is no hope for adjustment of probabilities in Randomized Coordinate Descent to lead to a rate independent of the data A, as is the case for Stochastic Spectral Descent",
        "The optimal parameters suggest that Randomized Coordinate Descent has better rate without these directions",
        "In Figure 2 we report on the behavior of mini-batch SSCD, the mini-batch version of Stochastic spectral coordinate descent, for four choices of the mini-batch parameter \u03c4 , and several choices of k",
        "In Figure 3 we report on an experiment using a synthetic problem with data matrix A of dimension n = 105"
    ],
    "summary": [
        "An increasing array of learning and training tasks reduce to optimization problem in very large dimensions.",
        "Note that RCD arises as a special case with D being a discrete probability distribution over the set {e1, .",
        "We extend all results discussed above for SSD, including the rate (6), to the more general class of methods we call stochastic conjugate descent (SconD), for which D is the uniform distribution over vectors v1, .",
        "Motivated by the nature of SSD, we ask the following question: in order to obtain a condition-number-independent rate such as (6), do we have to consider new descent directions, such as eigenvectors of A, or can a similar effect be obtained using RCD with a better selection of probabilities?",
        "We show that for any n \u2265 2 and any T > 0, there is a matrix A such that the rate of RCD with any probabilities is O(T log 1 ).",
        "RCD and SSD lie on opposite ends of a continuum of stochastic descent methods for solving (1).",
        "We call this new method stochastic spectral coordinate descent (SSCD).",
        "We prove that the rate of SSCD with optimal probabilities is",
        "Let {xk} be the sequence of random iterates produced by stochastic spectral descent (Algorithm 2).",
        "The same rate as in Theorem 2 holds for the stochastic conjugate descent (SconD) method, which arises as a special case of stochastic descent for \u03c9 = 1 and D being a uniform distribution over a set of A-orthogonal vectors.",
        "RCD (Algorithm 3) arises as a special case of SD with unit stepsize (\u03c9 = 1) and distribution D given by st = ei with probability pi > 0.",
        "Uniform probabilities optimize the rate of RCD in (11).",
        "We show that there is no hope for adjustment of probabilities in RCD to lead to a rate independent of the data A, as is the case for SSD.",
        "For every n \u2265 2 and T > 0, there exists an n \u00d7 n positive definite matrix A and starting point x0, such that the number of iterations of RCD with any choice probabilities p1, .",
        ", \u03b2k) gives rise to a new specific method which we call stochastic spectral coordinate descent (SSCD).",
        "Algorithm 4 Stochastic Spectral Coordinate Descent (SSCD)",
        "Consider Stochastic Spectral Coordinate Descent (Algorithm 4) for fixed k \u2208",
        "The optimal parameters suggest that RCD has better rate without these directions."
    ],
    "headline": "In this paper we introduce a fundamentally new type of acceleration strategy for Randomized Coordinate Descent based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\u00e1rik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In ICML, pages 1110\u20131119, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Yuan%2C%20Yang%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Yuan%2C%20Yang%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016"
        },
        {
            "id": "2",
            "entry": "[2] Jonathan Barzilai and Borwein Jonathan M. Two point step size gradient methods. IMA Journal of Numerical Analysis, 8:141\u2013148, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barzilai%2C%20Jonathan%20M%2C%20Borwein%20Jonathan%20Two%20point%20step%20size%20gradient%20methods%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barzilai%2C%20Jonathan%20M%2C%20Borwein%20Jonathan%20Two%20point%20step%20size%20gradient%20methods%201988"
        },
        {
            "id": "3",
            "entry": "[3] Ernesto G. Birgin, Jos\u00e9 Mario Mart\u00ednez, and Marcos Raydan. Spectral projected gradient methods: Review and perspectives. Journal of Statistical Software, 60(3):1\u201321, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Birgin%2C%20Ernesto%20G.%20Mart%C3%ADnez%2C%20Jos%C3%A9%20Mario%20Raydan%2C%20Marcos%20Spectral%20projected%20gradient%20methods%3A%20Review%20and%20perspectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Birgin%2C%20Ernesto%20G.%20Mart%C3%ADnez%2C%20Jos%C3%A9%20Mario%20Raydan%2C%20Marcos%20Spectral%20projected%20gradient%20methods%3A%20Review%20and%20perspectives%202014"
        },
        {
            "id": "4",
            "entry": "[4] Dominik Csiba, Zheng Qu, and Peter Richt\u00e1rik. Stochastic dual coordinate ascent with adaptive probabilities. In ICML, pages 674\u2013683, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Csiba%2C%20Dominik%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Stochastic%20dual%20coordinate%20ascent%20with%20adaptive%20probabilities%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Csiba%2C%20Dominik%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Stochastic%20dual%20coordinate%20ascent%20with%20adaptive%20probabilities%202015"
        },
        {
            "id": "5",
            "entry": "[5] Olivier Fercoq and Peter Richt\u00e1rik. Accelerated, parallel, and proximal coordinate descent. SIAM Journal on Optimization, 25(4):1997\u20132023, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fercoq%2C%20Olivier%20Accelerated%2C%20Peter%20Richt%C3%A1rik%20parallel%20and%20proximal%20coordinate%20descent%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fercoq%2C%20Olivier%20Accelerated%2C%20Peter%20Richt%C3%A1rik%20parallel%20and%20proximal%20coordinate%20descent%202015"
        },
        {
            "id": "6",
            "entry": "[6] Robert M Gower and Peter Richt\u00e1rik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660\u20131690, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gower%2C%20Robert%20M.%20Richt%C3%A1rik%2C%20Peter%20Randomized%20iterative%20methods%20for%20linear%20systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gower%2C%20Robert%20M.%20Richt%C3%A1rik%2C%20Peter%20Randomized%20iterative%20methods%20for%20linear%20systems%202015"
        },
        {
            "id": "7",
            "entry": "[7] Robert Mansel Gower and Peter Richt\u00e1rik. Stochastic dual ascent for solving linear systems. arXiv preprint arXiv:1512.06890, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.06890"
        },
        {
            "id": "8",
            "entry": "[8] Ching-Pei Lee and Stephen J. Wright. Random permutations fix a worst case for cyclic coordinate descent. arXiv:1607.08320, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08320"
        },
        {
            "id": "9",
            "entry": "[9] Yin Tat Lee and Aaron Sidford. Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems. In FOCS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Efficient%20accelerated%20coordinate%20descent%20methods%20and%20faster%20algorithms%20for%20solving%20linear%20systems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Efficient%20accelerated%20coordinate%20descent%20methods%20and%20faster%20algorithms%20for%20solving%20linear%20systems%202013"
        },
        {
            "id": "10",
            "entry": "[10] Dennis Leventhal and Adrian Lewis. Randomized methods for linear constraints: convergence rates and conditioning. Mathematics of Operations Research, 35:641\u2013654, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leventhal%2C%20Dennis%20Lewis%2C%20Adrian%20Randomized%20methods%20for%20linear%20constraints%3A%20convergence%20rates%20and%20conditioning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leventhal%2C%20Dennis%20Lewis%2C%20Adrian%20Randomized%20methods%20for%20linear%20constraints%3A%20convergence%20rates%20and%20conditioning%202010"
        },
        {
            "id": "11",
            "entry": "[11] Nicolas Loizou and Peter Richt\u00e1rik. Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods. arXiv preprint arXiv:1712.09677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09677"
        },
        {
            "id": "12",
            "entry": "[12] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(2):372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20o%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20o%281/k2%29%201983"
        },
        {
            "id": "13",
            "entry": "[13] Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012. doi: 10.1137/100802001. URL https://doi.org/10.1137/100802001. First appeared in 2010 as CORE discussion paper 2010/2.",
            "crossref": "https://dx.doi.org/10.1137/100802001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/100802001"
        },
        {
            "id": "14",
            "entry": "[14] Yurii Nesterov and Sebastian Stich. Efficiency of accelerated coordinate descent method on structured optimization problems. SIAM Journal on Optimization, 27(1):110\u2013123, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Stich%2C%20Sebastian%20Efficiency%20of%20accelerated%20coordinate%20descent%20method%20on%20structured%20optimization%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Stich%2C%20Sebastian%20Efficiency%20of%20accelerated%20coordinate%20descent%20method%20on%20structured%20optimization%20problems%202017"
        },
        {
            "id": "15",
            "entry": "[15] Julie Nutini, Mark Schmidt, Issam H. Laradji, Michael Friedlander, and Hoyt Koepke. Coordinate descent converges faster with the Gauss-Southwell rule than random selection. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Laradji%2C%20Issam%20H.%20Friedlander%2C%20Michael%20Coordinate%20descent%20converges%20faster%20with%20the%20Gauss-Southwell%20rule%20than%20random%20selection%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Laradji%2C%20Issam%20H.%20Friedlander%2C%20Michael%20Coordinate%20descent%20converges%20faster%20with%20the%20Gauss-Southwell%20rule%20than%20random%20selection%202015"
        },
        {
            "id": "16",
            "entry": "[16] Boris Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1 \u2013 17, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964"
        },
        {
            "id": "17",
            "entry": "[17] Zheng Qu and Peter Richt\u00e1rik. Coordinate descent with arbitrary sampling I: algorithms and complexity. Optimization Methods and Software, 31(5):829\u2013857, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20I%3A%20algorithms%20and%20complexity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20I%3A%20algorithms%20and%20complexity%202016"
        },
        {
            "id": "18",
            "entry": "[18] Peter Richt\u00e1rik and Martin Tak\u00e1c. Stochastic reformulations of linear systems: Algorithms and convergence theory. arXiv preprint arXiv:1706.01108, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01108"
        },
        {
            "id": "19",
            "entry": "[19] Peter Richt\u00e1rik and Martin Tak\u00e1c. On optimal probabilities in stochastic coordinate descent methods. Optimization Letters, 10(6):1233\u20131243, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20On%20optimal%20probabilities%20in%20stochastic%20coordinate%20descent%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20On%20optimal%20probabilities%20in%20stochastic%20coordinate%20descent%20methods%202016"
        },
        {
            "id": "20",
            "entry": "[20] Peter Richt\u00e1rik and Peter Tak\u00e1c. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1):433\u2013484, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Peter%20Parallel%20coordinate%20descent%20methods%20for%20big%20data%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Peter%20Parallel%20coordinate%20descent%20methods%20for%20big%20data%20optimization%202016"
        },
        {
            "id": "21",
            "entry": "[21] Stephen Tu, Shivaram Venkataraman, Ashia C. Wilson, Alex Gittens, Michael I. Jordan, and Benjamin Recht. Breaking locality accelerates block Gauss-Seidel. In ICML, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tu%2C%20Stephen%20Venkataraman%2C%20Shivaram%20Wilson%2C%20Ashia%20C.%20Gittens%2C%20Alex%20Breaking%20locality%20accelerates%20block%20Gauss-Seidel%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tu%2C%20Stephen%20Venkataraman%2C%20Shivaram%20Wilson%2C%20Ashia%20C.%20Gittens%2C%20Alex%20Breaking%20locality%20accelerates%20block%20Gauss-Seidel%202017"
        }
    ]
}
