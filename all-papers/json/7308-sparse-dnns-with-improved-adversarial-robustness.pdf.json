{
    "filename": "7308-sparse-dnns-with-improved-adversarial-robustness.pdf",
    "metadata": {
        "title": "Sparse DNNs with Improved Adversarial Robustness",
        "author": "Yiwen Guo, Chao Zhang, Changshui Zhang, Yurong Chen",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7308-sparse-dnns-with-improved-adversarial-robustness.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under l2 attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples."
    },
    "keywords": [
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "sparse representation",
            "url": "https://en.wikipedia.org/wiki/sparse_representation"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Deep neural networks (DNNs) have advanced the state-of-the-art of many artificial intelligence techniques, some undesired properties may hinder them from being deployed in real-world applications",
        "In addition to the fast gradient sign [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and DeepFool [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] attacks which have been thoroughly discussed in Section 3, we introduce two more attacks for extensive comparisons of the model robustness",
        "Unlike previous linear models which behave differently under l\u221e and l2 attacks, nonlinear deep neural networks models show a consistent trend of adversarial robustness with respect to the sparsity",
        "It can be necessary to evaluate the adversarial robustness of deep neural networks during an aggressive surgery, even though the prediction accuracy of compressed models may remain competitive with their references on benign test-sets",
        "We study some intrinsic relationships between the adversarial robustness and the sparsity of classifiers, both theoretically and empirically",
        "We demonstrate that unlike some linear models which behave differently under l\u221e and l2 attacks, sparse nonlinear deep neural networks can be consistently more robust to both of them than their corresponding dense references, until their sparsity reaches certain thresholds and inevitably causes harm to the network capacity"
    ],
    "key_statements": [
        "Deep neural networks (DNNs) have advanced the state-of-the-art of many artificial intelligence techniques, some undesired properties may hinder them from being deployed in real-world applications",
        "With continued proliferation of deep learning powered applications, one major concern raised recently is the heavy computation and storage burden that deep neural networks models shall lay upon mobile platforms",
        "We reveal, somewhat surprising, that there is a discrepancy between the robustness of sparse linear classifiers and nonlinear deep neural networks, under l2 attacks",
        "In addition to the fast gradient sign [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and DeepFool [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] attacks which have been thoroughly discussed in Section 3, we introduce two more attacks for extensive comparisons of the model robustness",
        "Unlike previous linear models which behave differently under l\u221e and l2 attacks, nonlinear deep neural networks models show a consistent trend of adversarial robustness with respect to the sparsity",
        "It can be necessary to evaluate the adversarial robustness of deep neural networks during an aggressive surgery, even though the prediction accuracy of compressed models may remain competitive with their references on benign test-sets",
        "We study some intrinsic relationships between the adversarial robustness and the sparsity of classifiers, both theoretically and empirically",
        "We demonstrate that unlike some linear models which behave differently under l\u221e and l2 attacks, sparse nonlinear deep neural networks can be consistently more robust to both of them than their corresponding dense references, until their sparsity reaches certain thresholds and inevitably causes harm to the network capacity"
    ],
    "summary": [
        "Deep neural networks (DNNs) have advanced the state-of-the-art of many artificial intelligence techniques, some undesired properties may hinder them from being deployed in real-world applications.",
        "We reveal, somewhat surprising, that there is a discrepancy between the robustness of sparse linear classifiers and nonlinear DNNs, under l2 attacks.",
        "Our results demonstrate that an appropriately higher sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples, under both the l\u221e and l2 circumstances.",
        "Our theoretical and empirical analyses shall cover both linear classifiers and nonlinear DNNs, in which the middle-layer activations and connection weights can all become sparse.",
        "The introduced two metrics evaluate robustness of classifiers from two different perspectives: r\u221e calculates the expected accuracy on (FGS) adversarial examples and r2 measures a decision margin between benign examples from the two classes.",
        "We shall later discuss its connections with our rps, for p \u2208 {\u221e, 2}, and we try providing a local Lipschitz constant of function gy(x) \u2212 gk(x), to help us delve deeper into the robustness of nonlinear DNNs. Without loss of generality, we will let the following discussion be made under a fixed radius R > 0 and a given instance x \u2208 Rn. Some modern DNNs can be structurally very complex.",
        "Which is a diagonal matrix whose entries taking value one correspond to nonzero activations within the j-th layer, and Mj \u2208 {0, 1}nj\u22121\u00d7nj , which is a binary mask corresponding to each Wj. Along with some analyses, the following lemma and theorem present intrinsic relationships between the adversarial robustness and sparsity of nonlinear DNNs. Lemma 3.1.",
        "Unlike previous linear models which behave differently under l\u221e and l2 attacks, nonlinear DNN models show a consistent trend of adversarial robustness with respect to the sparsity.",
        "We prune dense network models in the progressive manner and illustrate quantitative relationships between the robustness and weight sparsity in Figure 2 (e)-(h).",
        "It can be necessary to evaluate the adversarial robustness of DNNs during an aggressive surgery, even though the prediction accuracy of compressed models may remain competitive with their references on benign test-sets.",
        "We demonstrate that unlike some linear models which behave differently under l\u221e and l2 attacks, sparse nonlinear DNNs can be consistently more robust to both of them than their corresponding dense references, until their sparsity reaches certain thresholds and inevitably causes harm to the network capacity.",
        "Our results demonstrate that such sparsity, including sparse connections and middle-layer neuron activations, can be effectively imposed using network pruning and l1 regularization of weight tensors."
    ],
    "headline": "This paper studies classification models, especially deep neural networks-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "2",
            "entry": "[2] Emmanuel J Cand\u00e8s, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2):489\u2013509, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20Emmanuel%20J.%20Romberg%2C%20Justin%20Tao%2C%20Terence%20Robust%20uncertainty%20principles%3A%20Exact%20signal%20reconstruction%20from%20highly%20incomplete%20frequency%20information%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20Emmanuel%20J.%20Romberg%2C%20Justin%20Tao%2C%20Terence%20Robust%20uncertainty%20principles%3A%20Exact%20signal%20reconstruction%20from%20highly%20incomplete%20frequency%20information%202006"
        },
        {
            "id": "3",
            "entry": "[3] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In SP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017"
        },
        {
            "id": "5",
            "entry": "[5] Alexandre d\u2019Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions for sparse principal component analysis. JMLR, 9(July):1269\u20131294, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d%E2%80%99Aspremont%2C%20Alexandre%20Bach%2C%20Francis%20Ghaoui%2C%20Laurent%20El%20Optimal%20solutions%20for%20sparse%20principal%20component%20analysis%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=d%E2%80%99Aspremont%2C%20Alexandre%20Bach%2C%20Francis%20Ghaoui%2C%20Laurent%20El%20Optimal%20solutions%20for%20sparse%20principal%20component%20analysis%202008"
        },
        {
            "id": "6",
            "entry": "[6] Misha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio Ranzato, and Nando De Freitas. Predicting parameters in deep learning. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Predicting%20parameters%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "7",
            "entry": "[7] Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhillon%2C%20Guneet%20S.%20Azizzadenesheli%2C%20Kamyar%20Lipton%2C%20Zachary%20C.%20Bernstein%2C%20Jeremy%20and%20Anima%20Anandkumar.%20Stochastic%20activation%20pruning%20for%20robust%20adversarial%20defense%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhillon%2C%20Guneet%20S.%20Azizzadenesheli%2C%20Kamyar%20Lipton%2C%20Zachary%20C.%20Bernstein%2C%20Jeremy%20and%20Anima%20Anandkumar.%20Stochastic%20activation%20pruning%20for%20robust%20adversarial%20defense%202018"
        },
        {
            "id": "8",
            "entry": "[8] David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Compressed%20sensing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Compressed%20sensing%202006"
        },
        {
            "id": "9",
            "entry": "[9] Angus Galloway, Graham W Taylor, and Medhat Moussa. Attacking binarized neural networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Galloway%2C%20Angus%20Taylor%2C%20Graham%20W.%20Moussa%2C%20Medhat%20Attacking%20binarized%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Galloway%2C%20Angus%20Taylor%2C%20Graham%20W.%20Moussa%2C%20Medhat%20Attacking%20binarized%20neural%20networks%202018"
        },
        {
            "id": "10",
            "entry": "[10] Ji Gao, Beilun Wang, Zeming Lin, Weilin Xu, and Yanjun Qi. Deepcloak: Masking deep neural network models for robustness against adversarial samples. In ICLR Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Ji%20Wang%2C%20Beilun%20Lin%2C%20Zeming%20Xu%2C%20Weilin%20Deepcloak%3A%20Masking%20deep%20neural%20network%20models%20for%20robustness%20against%20adversarial%20samples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Ji%20Wang%2C%20Beilun%20Lin%2C%20Zeming%20Xu%2C%20Weilin%20Deepcloak%3A%20Masking%20deep%20neural%20network%20models%20for%20robustness%20against%20adversarial%20samples%202017"
        },
        {
            "id": "11",
            "entry": "[11] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "12",
            "entry": "[12] Soorya Gopalakrishnan, Zhinus Marzi, Upamanyu Madhow, and Ramtin Pedarsani. Combating adversarial attacks using sparse representations. In ICLR Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopalakrishnan%2C%20Soorya%20Marzi%2C%20Zhinus%20Madhow%2C%20Upamanyu%20Pedarsani%2C%20Ramtin%20Combating%20adversarial%20attacks%20using%20sparse%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopalakrishnan%2C%20Soorya%20Marzi%2C%20Zhinus%20Madhow%2C%20Upamanyu%20Pedarsani%2C%20Ramtin%20Combating%20adversarial%20attacks%20using%20sparse%20representations%202018"
        },
        {
            "id": "13",
            "entry": "[13] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016"
        },
        {
            "id": "14",
            "entry": "[14] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "15",
            "entry": "[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition"
        },
        {
            "id": "16",
            "entry": "[16] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "17",
            "entry": "[17] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In MM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20Yangqing%20Shelhamer%2C%20Evan%20Donahue%2C%20Jeff%20Karayev%2C%20Sergey%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014"
        },
        {
            "id": "18",
            "entry": "[18] Jiajun Lu, Theerasit Issaranon, and David Forsyth. Safetynet: Detecting and rejecting adversarial examples robustly. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Jiajun%20Issaranon%2C%20Theerasit%20Forsyth%2C%20David%20Safetynet%3A%20Detecting%20and%20rejecting%20adversarial%20examples%20robustly%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Jiajun%20Issaranon%2C%20Theerasit%20Forsyth%2C%20David%20Safetynet%3A%20Detecting%20and%20rejecting%20adversarial%20examples%20robustly%202017"
        },
        {
            "id": "19",
            "entry": "[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "20",
            "entry": "[20] Zhinus Marzi, Soorya Gopalakrishnan, Upamanyu Madhow, and Ramtin Pedarsani. Sparsity-based defense against adversarial attacks on linear classifiers. arXiv preprint arXiv:1801.04695, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04695"
        },
        {
            "id": "21",
            "entry": "[21] Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017"
        },
        {
            "id": "22",
            "entry": "[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple and accurate method to fool deep neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "23",
            "entry": "[23] Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry P Vetrov. Structured bayesian pruning via log-normal multiplicative noise. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neklyudov%2C%20Kirill%20Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20P.%20Structured%20bayesian%20pruning%20via%20log-normal%20multiplicative%20noise%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neklyudov%2C%20Kirill%20Molchanov%2C%20Dmitry%20Ashukha%2C%20Arsenii%20Vetrov%2C%20Dmitry%20P.%20Structured%20bayesian%20pruning%20via%20log-normal%20multiplicative%20noise%202017"
        },
        {
            "id": "24",
            "entry": "[24] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In SP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "25",
            "entry": "[25] Jongsoo Park, Sheng Li, Wei Wen, Ping Tak Peter Tang, Hai Li, Yiran Chen, and Pradeep Dubey. Faster cnns with direct sparse convolutions and guided pruning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Park%2C%20Jongsoo%20Li%2C%20Sheng%20Wen%2C%20Wei%20Tang%2C%20Ping%20Tak%20Peter%20Faster%20cnns%20with%20direct%20sparse%20convolutions%20and%20guided%20pruning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Park%2C%20Jongsoo%20Li%2C%20Sheng%20Wen%2C%20Wei%20Tang%2C%20Ping%20Tak%20Peter%20Faster%20cnns%20with%20direct%20sparse%20convolutions%20and%20guided%20pruning%202017"
        },
        {
            "id": "26",
            "entry": "[26] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "27",
            "entry": "[27] Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tram%C3%A8r%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Boneh%2C%20Dan%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tram%C3%A8r%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Boneh%2C%20Dan%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018"
        },
        {
            "id": "28",
            "entry": "[28] Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullrich%2C%20Karen%20Meeds%2C%20Edward%20Welling%2C%20Max%20Soft%20weight-sharing%20for%20neural%20network%20compression%202017"
        },
        {
            "id": "29",
            "entry": "[29] Luyu Wang, Gavin Weiguang Ding, Ruitong Huang, Yanshuai Cao, and Yik Chau Lui. Adversarial robustness of pruned neural networks. In ICLR Workshop submission, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Luyu%20Ding%2C%20Gavin%20Weiguang%20Huang%2C%20Ruitong%20Cao%2C%20Yanshuai%20Adversarial%20robustness%20of%20pruned%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Luyu%20Ding%2C%20Gavin%20Weiguang%20Huang%2C%20Ruitong%20Cao%2C%20Yanshuai%20Adversarial%20robustness%20of%20pruned%20neural%20networks%202018"
        },
        {
            "id": "30",
            "entry": "[30] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%202018"
        },
        {
            "id": "31",
            "entry": "[31] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Ren%2C%20Zhou%20Mitigating%20adversarial%20effects%20through%20randomization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Ren%2C%20Zhou%20Mitigating%20adversarial%20effects%20through%20randomization%202018"
        },
        {
            "id": "32",
            "entry": "[32] Shaokai Ye, Siyue Wang, Xiao Wang, Bo Yuan, Wujie Wen, and Xue Lin. Defending DNN adversarial attacks with pruning and logits augmentation. In ICLR Workshop submission, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20Shaokai%20Wang%2C%20Siyue%20Wang%2C%20Xiao%20Yuan%2C%20Bo%20Defending%20DNN%20adversarial%20attacks%20with%20pruning%20and%20logits%20augmentation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20Shaokai%20Wang%2C%20Siyue%20Wang%2C%20Xiao%20Yuan%2C%20Bo%20Defending%20DNN%20adversarial%20attacks%20with%20pruning%20and%20logits%20augmentation%202018"
        }
    ]
}
