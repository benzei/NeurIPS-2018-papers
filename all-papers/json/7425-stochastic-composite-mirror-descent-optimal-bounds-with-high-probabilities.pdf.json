{
    "filename": "7425-stochastic-composite-mirror-descent-optimal-bounds-with-high-probabilities.pdf",
    "metadata": {
        "title": "Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities",
        "author": "Yunwen Lei, Ke Tang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7425-stochastic-composite-mirror-descent-optimal-bounds-with-high-probabilities.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives with non-smooth loss functions, for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to study the generalization performance of multi-pass stochastic gradient descent (SGD) in a non-parametric setting. Our high-probability generalization bounds enjoy a logarithmical dependency on the number of passes provided that the step size sequence is square-summable, which improves the existing bounds in expectation with a polynomial dependency and therefore gives a strong justification on the ability of multi-pass SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our theoretical findings."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "Stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "Stochastic gradient descent (SGD) has found wide applications in machine learning problems due to its simplicity in implementation, low memory requirement and low computational complexity per iteration, as well as good practical behavior [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>]",
        "We consider both general convex and strongly convex objectives, for each of which we show that Stochastic composite mirror descent can achieve almost optimal convergence rates with high probability, which match the minimax lower rates for stochastic approximation up to a logarithmic factor [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We apply our high-probability convergence rates for Stochastic composite mirror descent to establish generalization error bounds for Stochastic gradient descent",
        "We show bounds scaling logarithmically w.r.t. the number of passes for E) \u2212 Ez) (Theorem 10). This implies that estimation errors will never essentially dominate the other two errors and one can run Stochastic gradient descent with a sufficient number of passes with little overfitting if step sizes are square-summable, due to the key observation on the almost boundedness of iterates established in Theorem 3",
        "Our analysis implies that Stochastic gradient descent can be run with a sufficient number of iterations with little overfitting if step sizes are square-summable, which can achieve similar generalization performance with different computational complexities",
        "We justify the immunity of multi-pass Stochastic gradient descent to overfitting by giving estimation error bounds with a logarithmic dependency on the number of passes for square-summable step sizes, while existing bounds scale polynomially [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]"
    ],
    "key_statements": [
        "Stochastic gradient descent (SGD) has found wide applications in machine learning problems due to its simplicity in implementation, low memory requirement and low computational complexity per iteration, as well as good practical behavior [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>]",
        "Stochastic gradient descent minimizes empirical errors by moving iterates along the direction of a negative gradient calculated based on a loss function on a single training example or a batch of few examples",
        "This strategy of processing few examples per iteration makes Stochastic gradient descent particularly suitable for large scale applications with very large data points [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], which are becoming ubiquitous in the big data era",
        "We consider both general convex and strongly convex objectives, for each of which we show that Stochastic composite mirror descent can achieve almost optimal convergence rates with high probability, which match the minimax lower rates for stochastic approximation up to a logarithmic factor [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We identify a constraint on step sizes to guarantee the boundedness of iterates with high probability",
        "We show that estimation errors scale logarithmically with respect to (w.r.t.) the number of passes provided that the step size sequence is square-summable, which implies that Stochastic gradient descent may be immune to overfitting",
        "We introduce Stochastic composite mirror descent and state convergence rates in Section 2 and Section 3, respectively",
        "We study the behavior of Stochastic composite mirror descent for convex objectives with \u03c3\u03c6 = 0",
        "We apply our high-probability convergence rates for Stochastic composite mirror descent to establish generalization error bounds for Stochastic gradient descent",
        "The high-probability bounds in [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] were established for the specific Stochastic gradient descent based on loss functions with H\u00f6lder continuous gradients, and involve w\u2217",
        "We show bounds scaling logarithmically w.r.t. the number of passes for E) \u2212 Ez) (Theorem 10). This implies that estimation errors will never essentially dominate the other two errors and one can run Stochastic gradient descent with a sufficient number of passes with little overfitting if step sizes are square-summable, due to the key observation on the almost boundedness of iterates established in Theorem 3",
        "Our analysis implies that Stochastic gradient descent can be run with a sufficient number of iterations with little overfitting if step sizes are square-summable, which can achieve similar generalization performance with different computational complexities",
        "We justify the immunity of multi-pass Stochastic gradient descent to overfitting by giving estimation error bounds with a logarithmic dependency on the number of passes for square-summable step sizes, while existing bounds scale polynomially [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "This improvement is based on the key observation on the almost boundedness of iterates with high probability"
    ],
    "summary": [
        "Stochastic gradient descent (SGD) has found wide applications in machine learning problems due to its simplicity in implementation, low memory requirement and low computational complexity per iteration, as well as good practical behavior [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>].",
        "In Theorem 8, we establish high-probability bounds for both wt \u2212 w\u2217 2 and \u03c6) \u2212 \u03c6(w\u2217) with wt(2) being another weighted average of the first t iterates, for each of which we derive optimal convergence rates up to a logarithmic factor [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "We apply our high-probability convergence rates for SCMD to establish generalization error bounds for SGD.",
        "High-probability bounds were established for stochastic dual averaging under the boundedness assumption on iterates and subgradients [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>].",
        "We show that the same high-probability convergence rate holds without any boundedness assumptions on either the iterates {wt} or the associated subgradients.",
        "The high-probability bounds in [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] were established for the specific SGD based on loss functions with H\u00f6lder continuous gradients, and involve w\u2217.",
        "For \u03bb-exp-concave loss functions, a regret bound O(\u03bb\u22121 log T ) was established for an online Newton method [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], which implies convergence rates O\u22121 log T for some average of iterates produced by the stochastic counterpart.",
        "This implies that estimation errors will never essentially dominate the other two errors and one can run SGD with a sufficient number of passes with little overfitting if step sizes are square-summable, due to the key observation on the almost boundedness of iterates established in Theorem 3.",
        "The high-probability bounds there require to impose Lipschitz continuity, smoothness and strong convexity assumptions on loss functions, and ignore computational and approximation errors [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "Our analysis implies that SGD can be run with a sufficient number of iterations with little overfitting if step sizes are square-summable, which can achieve similar generalization performance with different computational complexities.",
        "We establish a rigorous theoretical foundation for SCMD by providing optimal convergence rates in the stochastic optimization setting without boundedness assumptions on either subgradients or iterates, which in turn shed new insights on the generalization behavior of the multi-pass SGD in the statistical learning theory setting.",
        "We justify the immunity of multi-pass SGD to overfitting by giving estimation error bounds with a logarithmic dependency on the number of passes for square-summable step sizes, while existing bounds scale polynomially [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "Our generalization analysis of SGD substantially improves learning rates in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], removes bounded subgradient assumptions in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], removes smoothness assumptions in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] and is performed in high probability instead of in expectation [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "It would be interesting to extend our results to a non-convex setting [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] and to general mirror descent algorithms with a non-differentiable mirror map [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]"
    ],
    "headline": "We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agarwal, M. J. Wainwright, P. L. Bartlett, and P. K. Ravikumar. Information-theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, pages 1\u20139, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Wainwright%2C%20M.J.%20Bartlett%2C%20P.L.%20Ravikumar%2C%20P.K.%20Information-theoretic%20lower%20bounds%20on%20the%20oracle%20complexity%20of%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Wainwright%2C%20M.J.%20Bartlett%2C%20P.L.%20Ravikumar%2C%20P.K.%20Information-theoretic%20lower%20bounds%20on%20the%20oracle%20complexity%20of%20convex%20optimization%202009"
        },
        {
            "id": "2",
            "entry": "[2] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n). In Advances in Neural Information Processing Systems, pages 773\u2013781, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Moulines%2C%20E.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20O%281/n%29%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Moulines%2C%20E.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20O%281/n%29%202013"
        },
        {
            "id": "3",
            "entry": "[3] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "4",
            "entry": "[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20A.%20Teboulle%2C%20M.%20Mirror%20descent%20and%20nonlinear%20projected%20subgradient%20methods%20for%20convex%20optimization%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20A.%20Teboulle%2C%20M.%20Mirror%20descent%20and%20nonlinear%20projected%20subgradient%20methods%20for%20convex%20optimization%202003"
        },
        {
            "id": "5",
            "entry": "[5] L. Bottou. On-line learning and stochastic approximations. In D. Saad, editor, On-line Learning in Neural Networks, pages 9\u201342. Cambridge University Press, New York, NY, USA, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20On-line%20learning%20and%20stochastic%20approximations%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20On-line%20learning%20and%20stochastic%20approximations%201998"
        },
        {
            "id": "6",
            "entry": "[6] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018"
        },
        {
            "id": "7",
            "entry": "[7] O. Bousquet and L. Bottou. The tradeoffs of large scale learning. In Advances in Neural Information Processing Systems, pages 161\u2013168, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20O.%20Bottou%2C%20L.%20The%20tradeoffs%20of%20large%20scale%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20O.%20Bottou%2C%20L.%20The%20tradeoffs%20of%20large%20scale%20learning%202008"
        },
        {
            "id": "8",
            "entry": "[8] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20C.-C.%20Lin%2C%20C.-J.%20LIBSVM%3A%20a%20library%20for%20support%20vector%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20C.-C.%20Lin%2C%20C.-J.%20LIBSVM%3A%20a%20library%20for%20support%20vector%20machines%202011"
        },
        {
            "id": "9",
            "entry": "[9] F. Cucker and D.-X. Zhou. Learning Theory: an Approximation Theory Viewpoint. Cambridge University Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cucker%2C%20F.%20Zhou%2C%20D.-X.%20Learning%20Theory%3A%20an%20Approximation%20Theory%20Viewpoint%202007"
        },
        {
            "id": "10",
            "entry": "[10] A. Dieuleveut and F. Bach. Nonparametric stochastic approximation with large step-sizes. Annals of Statistics, 44(4):1363\u20131399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieuleveut%2C%20A.%20Bach%2C%20F.%20Nonparametric%20stochastic%20approximation%20with%20large%20step-sizes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieuleveut%2C%20A.%20Bach%2C%20F.%20Nonparametric%20stochastic%20approximation%20with%20large%20step-sizes%202016"
        },
        {
            "id": "11",
            "entry": "[11] J. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. In Advances in Neural Information Processing Systems, pages 495\u2013503, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Singer%2C%20Y.%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Singer%2C%20Y.%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009"
        },
        {
            "id": "12",
            "entry": "[12] J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Conference on Learning Theory, pages 14\u201326, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Tewari%2C%20A.%20Composite%20objective%20mirror%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Tewari%2C%20A.%20Composite%20objective%20mirror%20descent%202010"
        },
        {
            "id": "13",
            "entry": "[13] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, pages 1225\u20131234, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20M.%20Recht%2C%20B.%20Singer%2C%20Y.%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20M.%20Recht%2C%20B.%20Singer%2C%20Y.%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "14",
            "entry": "[14] E. Hazan and S. Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic stronglyconvex optimization. Journal of Machine Learning Research, 15(1):2489\u20132512, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Kale%2C%20S.%20Beyond%20the%20regret%20minimization%20barrier%3A%20optimal%20algorithms%20for%20stochastic%20stronglyconvex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Kale%2C%20S.%20Beyond%20the%20regret%20minimization%20barrier%3A%20optimal%20algorithms%20for%20stochastic%20stronglyconvex%20optimization%202014"
        },
        {
            "id": "15",
            "entry": "[15] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2):169\u2013192, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Agarwal%2C%20A.%20Kale%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Agarwal%2C%20A.%20Kale%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007"
        },
        {
            "id": "16",
            "entry": "[16] S. Lacoste-Julien, M. Schmidt, and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.2002"
        },
        {
            "id": "17",
            "entry": "[17] Y. Lei and D.-X. Zhou. Convergence of online mirror descent. Applied and Computational Harmonic Analysis, 2018. doi: https://doi.org/10.1016/j.acha.2018.05.005.",
            "crossref": "https://dx.doi.org/10.1016/j.acha.2018.05.005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.acha.2018.05.005"
        },
        {
            "id": "18",
            "entry": "[18] Y. Lei and D.-X. Zhou. Learning theory of randomized sparse Kaczmarz method. SIAM Journal on Imaging Sciences, 11(1):547\u2013574, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Y.%20Zhou%2C%20D.-X.%20Learning%20theory%20of%20randomized%20sparse%20Kaczmarz%20method%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Y.%20Zhou%2C%20D.-X.%20Learning%20theory%20of%20randomized%20sparse%20Kaczmarz%20method%202018"
        },
        {
            "id": "19",
            "entry": "[19] Y. Lei, L. Shi, and Z.-C. Guo. Convergence of unregularized online learning algorithms. Journal of Machine Learning Research, 18(171):1\u201333, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Y.%20Shi%2C%20L.%20Guo%2C%20Z.-C.%20Convergence%20of%20unregularized%20online%20learning%20algorithms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Y.%20Shi%2C%20L.%20Guo%2C%20Z.-C.%20Convergence%20of%20unregularized%20online%20learning%20algorithms%202018"
        },
        {
            "id": "20",
            "entry": "[20] J. Lin and L. Rosasco. Optimal learning for multi-pass stochastic gradient methods. In Advances in Neural Information Processing Systems, pages 4556\u20134564, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20J.%20Rosasco%2C%20L.%20Optimal%20learning%20for%20multi-pass%20stochastic%20gradient%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20J.%20Rosasco%2C%20L.%20Optimal%20learning%20for%20multi-pass%20stochastic%20gradient%20methods%202016"
        },
        {
            "id": "21",
            "entry": "[21] J. Lin, R. Camoriano, and L. Rosasco. Generalization properties and implicit regularization for multiple passes SGM. In International Conference on Machine Learning, pages 2340\u20132348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20J.%20Camoriano%2C%20R.%20Rosasco%2C%20L.%20Generalization%20properties%20and%20implicit%20regularization%20for%20multiple%20passes%20SGM%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20J.%20Camoriano%2C%20R.%20Rosasco%2C%20L.%20Generalization%20properties%20and%20implicit%20regularization%20for%20multiple%20passes%20SGM%202016"
        },
        {
            "id": "22",
            "entry": "[22] B. London. A PAC-bayesian analysis of randomized learning with application to stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 2931\u20132940, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=London%2C%20B.%20A%20PAC-bayesian%20analysis%20of%20randomized%20learning%20with%20application%20to%20stochastic%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=London%2C%20B.%20A%20PAC-bayesian%20analysis%20of%20randomized%20learning%20with%20application%20to%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "23",
            "entry": "[23] E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451\u2013459, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moulines%2C%20E.%20Bach%2C%20F.%20Non-asymptotic%20analysis%20of%20stochastic%20approximation%20algorithms%20for%20machine%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moulines%2C%20E.%20Bach%2C%20F.%20Non-asymptotic%20analysis%20of%20stochastic%20approximation%20algorithms%20for%20machine%20learning%202011"
        },
        {
            "id": "24",
            "entry": "[24] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "25",
            "entry": "[25] A.-S. Nemirovsky and D.-B. Yudin. Problem Complexity and Method Efficiency in Optimization. John Wiley & Sons, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovsky%2C%20A.-S.%20Yudin%2C%20D.-B.%20Problem%20Complexity%20and%20Method%20Efficiency%20in%20Optimization%201983"
        },
        {
            "id": "26",
            "entry": "[26] L. M. Nguyen, P. H. Nguyen, M. van Dijk, P. Richt\u00e1rik, K. Scheinberg, and M. Tak\u00e1c. SGD and hogwild! convergence without the bounded gradients assumption. arXiv preprint arXiv:1802.03801, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03801"
        },
        {
            "id": "27",
            "entry": "[27] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. In Advances in Neural Information Processing Systems, pages 1116\u20131124, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orabona%2C%20F.%20Simultaneous%20model%20selection%20and%20optimization%20through%20parameter-free%20stochastic%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orabona%2C%20F.%20Simultaneous%20model%20selection%20and%20optimization%20through%20parameter-free%20stochastic%20learning%202014"
        },
        {
            "id": "28",
            "entry": "[28] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In International Conference on Machine Learning, pages 449\u2013456, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20A.%20Shamir%2C%20O.%20Sridharan%2C%20K.%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakhlin%2C%20A.%20Shamir%2C%20O.%20Sridharan%2C%20K.%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012"
        },
        {
            "id": "29",
            "entry": "[29] L. Rosasco and S. Villa. Learning with incremental iterative regularization. In Advances in Neural Information Processing Systems, pages 1630\u20131638, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosasco%2C%20L.%20Villa%2C%20S.%20Learning%20with%20incremental%20iterative%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosasco%2C%20L.%20Villa%2C%20S.%20Learning%20with%20incremental%20iterative%20regularization%202015"
        },
        {
            "id": "30",
            "entry": "[30] S. Shalev-Shwartz and A. Tewari. Stochastic methods for 1-regularized loss minimization. Journal of Machine Learning Research, 12:1865\u20131892, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Tewari%2C%20A.%20Stochastic%20methods%20for%201-regularized%20loss%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Tewari%2C%20A.%20Stochastic%20methods%20for%201-regularized%20loss%20minimization%202011"
        },
        {
            "id": "31",
            "entry": "[31] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In International Conference on Machine Learning, pages 807\u2013814. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20SVM%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20SVM%202007"
        },
        {
            "id": "32",
            "entry": "[32] O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization convergence results and optimal averaging schemes. In International Conference on Machine Learning, pages 71\u201379, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20O.%20Zhang%2C%20T.%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%20convergence%20results%20and%20optimal%20averaging%20schemes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20O.%20Zhang%2C%20T.%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%20convergence%20results%20and%20optimal%20averaging%20schemes%202013"
        },
        {
            "id": "33",
            "entry": "[33] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Analysis and Applications, 1(01):17\u201341, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smale%2C%20S.%20Zhou%2C%20D.-X.%20Estimating%20the%20approximation%20error%20in%20learning%20theory%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smale%2C%20S.%20Zhou%2C%20D.-X.%20Estimating%20the%20approximation%20error%20in%20learning%20theory%202003"
        },
        {
            "id": "34",
            "entry": "[34] I. Steinwart and A. Christmann. Support Vector Machines. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20I.%20Christmann%2C%20A.%20Support%20Vector%20Machines%202008"
        },
        {
            "id": "35",
            "entry": "[35] I. Steinwart and C. Scovel. Fast rates for support vector machines using gaussian kernels. Annals of Statistics, 35(2):575\u2013607, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20I.%20Scovel%2C%20C.%20Fast%20rates%20for%20support%20vector%20machines%20using%20gaussian%20kernels%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20I.%20Scovel%2C%20C.%20Fast%20rates%20for%20support%20vector%20machines%20using%20gaussian%20kernels%202007"
        },
        {
            "id": "36",
            "entry": "[36] P. Tarres and Y. Yao. Online learning as stochastic approximation of regularization paths: optimality and almost-sure convergence. IEEE Transactions on Information Theory, 60(9):5716\u20135735, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tarres%2C%20P.%20Yao%2C%20Y.%20Online%20learning%20as%20stochastic%20approximation%20of%20regularization%20paths%3A%20optimality%20and%20almost-sure%20convergence%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tarres%2C%20P.%20Yao%2C%20Y.%20Online%20learning%20as%20stochastic%20approximation%20of%20regularization%20paths%3A%20optimality%20and%20almost-sure%20convergence%202014"
        },
        {
            "id": "37",
            "entry": "[37] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11:2543\u20132596, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010"
        },
        {
            "id": "38",
            "entry": "[38] Y. Ying and M. Pontil. Online gradient descent learning algorithms. Foundations of Computational Mathematics, 8(5):561\u2013596, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ying%2C%20Y.%20Pontil%2C%20M.%20Online%20gradient%20descent%20learning%20algorithms%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ying%2C%20Y.%20Pontil%2C%20M.%20Online%20gradient%20descent%20learning%20algorithms%202008"
        },
        {
            "id": "39",
            "entry": "[39] Y. Ying and D.-X. Zhou. Online regularized classification algorithms. IEEE Transactions on Information Theory, 52(11):4775\u20134788, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ying%2C%20Y.%20Zhou%2C%20D.-X.%20Online%20regularized%20classification%20algorithms%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ying%2C%20Y.%20Zhou%2C%20D.-X.%20Online%20regularized%20classification%20algorithms%202006"
        },
        {
            "id": "40",
            "entry": "[40] Y. Ying and D.-X. Zhou. Unregularized online learning algorithms with general loss functions. Applied and Computational Harmonic Analysis, 42(2):224\u2013244, 2017. International Conference on Machine Learning, pages 919\u2013926, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ying%2C%20Y.%20Zhou%2C%20D.-X.%20Unregularized%20online%20learning%20algorithms%20with%20general%20loss%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ying%2C%20Y.%20Zhou%2C%20D.-X.%20Unregularized%20online%20learning%20algorithms%20with%20general%20loss%20functions%202017"
        },
        {
            "id": "42",
            "entry": "[42] P. Zhao and T. Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In International Conference on Machine Learning, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20P.%20Zhang%2C%20T.%20Stochastic%20optimization%20with%20importance%20sampling%20for%20regularized%20loss%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20P.%20Zhang%2C%20T.%20Stochastic%20optimization%20with%20importance%20sampling%20for%20regularized%20loss%20minimization%202015"
        },
        {
            "id": "43",
            "entry": "[43] Z. Zhou, P. Mertikopoulos, N. Bambos, S. Boyd, and P. W. Glynn. Stochastic mirror descent in variationally coherent optimization problems. In Advances in Neural Information Processing Systems, pages 7043\u20137052, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Z.%20Mertikopoulos%2C%20P.%20Bambos%2C%20N.%20Boyd%2C%20S.%20Stochastic%20mirror%20descent%20in%20variationally%20coherent%20optimization%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Z.%20Mertikopoulos%2C%20P.%20Bambos%2C%20N.%20Boyd%2C%20S.%20Stochastic%20mirror%20descent%20in%20variationally%20coherent%20optimization%20problems%202017"
        },
        {
            "id": "44",
            "entry": "[44] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In International Conference on Machine Learning, pages 928\u2013936, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
