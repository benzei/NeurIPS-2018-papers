{
    "filename": "7557-playing-hard-exploration-games-by-watching-youtube.pdf",
    "metadata": {
        "title": "Playing hard exploration games by watching YouTube",
        "author": "Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, Nando de Freitas",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7557-playing-hard-exploration-games-by-watching-youtube.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent\u2019s exact environment setup and the demonstrator\u2019s action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games MONTEZUMA\u2019S REVENGE, PITFALL! and PRIVATE EYE for the first time, even if the agent is not presented with any environment rewards."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "IMPALA",
            "url": "https://en.wikipedia.org/wiki/IMPALA"
        }
    ],
    "highlights": [
        "From knitting to dancing to playing games, by watching videos online",
        "Despite the recent advancements in deep reinforcement learning algorithms [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and architectures [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>], there are many \u201chard exploration\u201d challenges, characterized by particularly sparse environment rewards, that continue to pose a difficult challenge for existing reinforcement learning agents",
        "Providing a standard reinforcement learning agent with an imitation reward learnt from a single YouTube video, we are the first to convincingly exceed human-level performance on three of Atari\u2019s hardest exploration games: MONTEZUMA\u2019S REVENGE, PITFALL! and PRIVATE EYE",
        "We demonstrate how a sequence of checkpoints placed along the embedding of a single YouTube video can be presented as a reward signal to a standard reinforcement learning agent (IMPALA for our experiments [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]), allowing successful one-shot imitation even in the complete absence of the environment rewards",
        "We propose a method of guiding agent exploration through hard exploration challenges by watching YouTube videos",
        "We have proposed novel self-supervised objectives that allow a domain-invariant representation to be learnt across videos, and described a one-shot imitation mechanism for guiding agent exploration by embedding checkpoints throughout this space"
    ],
    "key_statements": [
        "From knitting to dancing to playing games, by watching videos online",
        "We show how this proposed research agenda enables us to make some initial progress in self-supervised alignment of noisy demonstration sequences for reinforcement learning agents, enabling human-level performance on the most complex and previously unsolved Atari 2600 games",
        "Despite the recent advancements in deep reinforcement learning algorithms [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and architectures [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>], there are many \u201chard exploration\u201d challenges, characterized by particularly sparse environment rewards, that continue to pose a difficult challenge for existing reinforcement learning agents",
        "This paper proposes a method for overcoming domain gaps between the observation sequences of multiple demonstrations, by using self-supervised classification tasks that are constructed over both time and modality to learn a common representation",
        "Providing a standard reinforcement learning agent with an imitation reward learnt from a single YouTube video, we are the first to convincingly exceed human-level performance on three of Atari\u2019s hardest exploration games: MONTEZUMA\u2019S REVENGE, PITFALL! and PRIVATE EYE",
        "We propose two novel self-supervised objectives: temporal distance classification (TDC), described in Section 3.1 and cross-modal temporal distance classification (CMC), described in Section 3.2",
        "We demonstrate how a sequence of checkpoints placed along the embedding of a single YouTube video can be presented as a reward signal to a standard reinforcement learning agent (IMPALA for our experiments [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]), allowing successful one-shot imitation even in the complete absence of the environment rewards",
        "We consider both (a) the 2-way cycle-consistency, P\u03c6, between the test video and the first training video, and (b) the 3-way cycle-consistency, P\u03c63, between the test video and the first two training videos. These results are presented in Figure 5, comparing the cycle-consistencies of our Temporal distance classification, Cross-modal temporal distance classification and combined methods to a naive l2-distance in pixel space, single-view timecontrastive networks (TCN) [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] and L3-Net [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We propose a method of guiding agent exploration through hard exploration challenges by watching YouTube videos",
        "Unlike traditional methods of imitation learning, where demonstrations are generated under controlled conditions with access to action and reward sequences, YouTube videos contain only unaligned and often noisy audio-visual sequences",
        "We have proposed novel self-supervised objectives that allow a domain-invariant representation to be learnt across videos, and described a one-shot imitation mechanism for guiding agent exploration by embedding checkpoints throughout this space"
    ],
    "summary": [
        "From knitting to dancing to playing games, by watching videos online.",
        "This paper proposes a method for overcoming domain gaps between the observation sequences of multiple demonstrations, by using self-supervised classification tasks that are constructed over both time and modality to learn a common representation.",
        "We propose an auxiliary imitation loss that allows an agent to successfully play hard exploration games without requiring the knowledge of the demonstrator\u2019s action trajectory.",
        "Providing a standard RL agent with an imitation reward learnt from a single YouTube video, we are the first to convincingly exceed human-level performance on three of Atari\u2019s hardest exploration games: MONTEZUMA\u2019S REVENGE, PITFALL!",
        "Imitation learning methods such as DQfD have yielded promising results for guiding agent exploration in sparse-reward tasks, both in game-playing [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and robotics domains [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>].",
        "We demonstrate how a sequence of checkpoints placed along the embedding of a single YouTube video can be presented as a reward signal to a standard reinforcement learning agent (IMPALA for our experiments [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]), allowing successful one-shot imitation even in the complete absence of the environment rewards.",
        "As described in Section 4, our imitation loss is constructed by generating checkpoints every N = 16 frames along the \u03c6-embedded observation sequence of a single YouTube video.",
        "To usefully close the domain gap between YouTube gameplay footage and our environment observations, our learnt embedding space should exhibit two desirable properties: (1) one-to-one alignment capacity and (2) meaningful abstraction.",
        "These results are presented in Figure 5, comparing the cycle-consistencies of our TDC, CMC and combined methods to a naive l2-distance in pixel space, single-view timecontrastive networks (TCN) [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] and L3-Net [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "The embeddings learnt using purely cross-modal alignment (i.e. L3-Net and CMC) perform better but still yield particularly scattered and disjoint trajectories, which is an undesirable property likely due to the sparsity of salient audio signals.",
        "Using the method described in Section 4, we train an IMPALA agent to play the hard exploration Atari games MONTEZUMA\u2019S REVENGE, PITFALL!",
        "To test the impact of the choice of expert trajectory, we generate checkpoints from two additional videos of MONTEZUMA\u2019S REVENGE from our set, and train agents with those sequences.",
        "Unlike traditional methods of imitation learning, where demonstrations are generated under controlled conditions with access to action and reward sequences, YouTube videos contain only unaligned and often noisy audio-visual sequences.",
        "We have proposed novel self-supervised objectives that allow a domain-invariant representation to be learnt across videos, and described a one-shot imitation mechanism for guiding agent exploration by embedding checkpoints throughout this space.",
        "Combining these methods with a standard IMPALA agent, we demonstrate"
    ],
    "headline": "We propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "2",
            "entry": "[2] Theodore Wilbur Anderson. An introduction to multivariate statistical analysis, volume 2. Wiley New York, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Theodore%20Wilbur%20An%20introduction%20to%20multivariate%20statistical%20analysis%2C%20volume%202%201958"
        },
        {
            "id": "3",
            "entry": "[3] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 609\u2013617. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Relja%20Arandjelovic%20and%20Andrew%20Zisserman%20Look%20listen%20and%20learn%20In%202017%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%20pages%20609617%20IEEE%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Relja%20Arandjelovic%20and%20Andrew%20Zisserman%20Look%20listen%20and%20learn%20In%202017%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%20pages%20609617%20IEEE%202017"
        },
        {
            "id": "4",
            "entry": "[4] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20and%20Brett%20Browning.%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009"
        },
        {
            "id": "5",
            "entry": "[5] Yusuf Aytar, Lluis Castrejon, Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Cross-modal scene networks. IEEE transactions on pattern analysis and machine intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aytar%2C%20Yusuf%20Castrejon%2C%20Lluis%20Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Cross-modal%20scene%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aytar%2C%20Yusuf%20Castrejon%2C%20Lluis%20Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Cross-modal%20scene%20networks%202017"
        },
        {
            "id": "6",
            "entry": "[6] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. See, hear, and read: Deep aligned representations. arXiv preprint arXiv:1706.00932, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00932"
        },
        {
            "id": "7",
            "entry": "[7] Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barth-Maron%2C%20Gabriel%20Hoffman%2C%20Matthew%20W.%20Budden%2C%20David%20Dabney%2C%20Will%20Distributed%20distributional%20deterministic%20policy%20gradients.%20International%20Conference%20on%20Learning%20Representations%20%28ICLR%29%202018"
        },
        {
            "id": "8",
            "entry": "[8] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471\u20131479, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "9",
            "entry": "[9] Marc G Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marc%20G%20Bellemare%2C%20Will%20Dabney%20Munos%2C%20R%C3%A9mi%20A%20distributional%20perspective%20on%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marc%20G%20Bellemare%2C%20Will%20Dabney%20Munos%2C%20R%C3%A9mi%20A%20distributional%20perspective%20on%20reinforcement%20learning%202017"
        },
        {
            "id": "10",
            "entry": "[10] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "11",
            "entry": "[11] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pages 1422\u20131430, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20Carl%20Gupta%2C%20Abhinav%20and%20Alexei%20A%20Efros.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20Carl%20Gupta%2C%20Abhinav%20and%20Alexei%20A%20Efros.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015"
        },
        {
            "id": "12",
            "entry": "[12] Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In The IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20Carl%20Zisserman%2C%20Andrew%20Multi-task%20self-supervised%20visual%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20Carl%20Zisserman%2C%20Andrew%20Multi-task%20self-supervised%20visual%20learning%202017"
        },
        {
            "id": "13",
            "entry": "[13] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in neural information processing systems, pages 1087\u20131098, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20OpenAI%20Jonathan%20One-shot%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20OpenAI%20Jonathan%20One-shot%20imitation%20learning%202017"
        },
        {
            "id": "14",
            "entry": "[14] Lasse Espeholt, Hubert Soyer, R\u00e9mi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. CoRR, abs/1802.01561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "15",
            "entry": "[15] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. arXiv preprint arXiv:1709.04905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04905"
        },
        {
            "id": "16",
            "entry": "[16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096\u20132030, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] Audrunas Gruslys, Mohammad Gheshlaghi Azar, Marc G Bellemare, and Remi Munos. The reactor: A sample-efficient actor-critic architecture. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gruslys%2C%20Audrunas%20Azar%2C%20Mohammad%20Gheshlaghi%20Bellemare%2C%20Marc%20G.%20Munos%2C%20Remi%20The%20reactor%3A%20A%20sample-efficient%20actor-critic%20architecture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gruslys%2C%20Audrunas%20Azar%2C%20Mohammad%20Gheshlaghi%20Bellemare%2C%20Marc%20G.%20Munos%2C%20Remi%20The%20reactor%3A%20A%20sample-efficient%20actor-critic%20architecture%202017"
        },
        {
            "id": "18",
            "entry": "[18] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design. In Advances in Neural Information Processing Systems, pages 6768\u20136777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dylan%20HadfieldMenell%20Smitha%20Milli%20Pieter%20Abbeel%20Stuart%20J%20Russell%20and%20Anca%20Dragan%20Inverse%20reward%20design%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2067686777%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dylan%20HadfieldMenell%20Smitha%20Milli%20Pieter%20Abbeel%20Stuart%20J%20Russell%20and%20Anca%20Dragan%20Inverse%20reward%20design%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2067686777%202017"
        },
        {
            "id": "19",
            "entry": "[19] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hessel%2C%20Matteo%20Modayil%2C%20Joseph%20Hasselt%2C%20Hado%20Van%20Schaul%2C%20Tom%20Rainbow%3A%20Combining%20improvements%20in%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hessel%2C%20Matteo%20Modayil%2C%20Joseph%20Hasselt%2C%20Hado%20Van%20Schaul%2C%20Tom%20Rainbow%3A%20Combining%20improvements%20in%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Gabriel Dulac-Arnold, et al. Deep q-learning from demonstrations. Proceedings of the AAAI Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hester%2C%20Todd%20Vecerik%2C%20Matej%20Pietquin%2C%20Olivier%20Lanctot%2C%20Marc%20Deep%20q-learning%20from%20demonstrations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hester%2C%20Todd%20Vecerik%2C%20Matej%20Pietquin%2C%20Olivier%20Lanctot%2C%20Marc%20Deep%20q-learning%20from%20demonstrations%202017"
        },
        {
            "id": "21",
            "entry": "[21] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pages 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "22",
            "entry": "[22] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver. Distributed prioritized experience replay. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horgan%2C%20Dan%20Quan%2C%20John%20Budden%2C%20David%20Barth-Maron%2C%20Gabriel%20Distributed%20prioritized%20experience%20replay%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Horgan%2C%20Dan%20Quan%2C%20John%20Budden%2C%20David%20Barth-Maron%2C%20Gabriel%20Distributed%20prioritized%20experience%20replay%202018"
        },
        {
            "id": "23",
            "entry": "[23] Ionel-Alexandru Hosu and Traian Rebedea. Playing atari games with deep reinforcement learning and human checkpoint replay. CoRR, abs/1607.05077, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.05077"
        },
        {
            "id": "24",
            "entry": "[24] Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. arXiv preprint arXiv:1707.03374, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03374"
        },
        {
            "id": "25",
            "entry": "[25] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "26",
            "entry": "[26] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.03748"
        },
        {
            "id": "27",
            "entry": "[27] Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized value functions. arXiv preprint arXiv:1703.07608, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07608"
        },
        {
            "id": "28",
            "entry": "[28] Andrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Torralba. Ambient sound provides supervision for visual learning. In European Conference on Computer Vision, pages 801\u2013816.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Owens%2C%20Andrew%20Wu%2C%20Jiajun%20McDermott%2C%20Josh%20H.%20Freeman%2C%20William%20T.%20Ambient%20sound%20provides%20supervision%20for%20visual%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Owens%2C%20Andrew%20Wu%2C%20Jiajun%20McDermott%2C%20Josh%20H.%20Freeman%2C%20William%20T.%20Ambient%20sound%20provides%20supervision%20for%20visual%20learning"
        },
        {
            "id": "29",
            "entry": "[29] Tom Le Paine, Sergio G\u00f3mez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar, Tobias Pfaff, Matt W Hoffman, Gabriel Barth-Maron, Serkan Cabi, David Budden, et al. One-shot high-fidelity imitation: Training large-scale deep nets with rl. arXiv preprint arXiv:1810.05017, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.05017"
        },
        {
            "id": "30",
            "entry": "[30] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction"
        },
        {
            "id": "31",
            "entry": "[31] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. arXiv preprint arXiv:1804.08606, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08606"
        },
        {
            "id": "32",
            "entry": "[32] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Vecer\u00edk, et al. Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11593"
        },
        {
            "id": "33",
            "entry": "[33] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.00653"
        },
        {
            "id": "34",
            "entry": "[34] Pierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey Levine. Time-contrastive networks: Selfsupervised learning from multi-view observation. arXiv preprint arXiv:1704.06888, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06888"
        },
        {
            "id": "35",
            "entry": "[35] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "36",
            "entry": "[36] Satinder Singh, Richard L Lewis, and Andrew G Barto. Where do rewards come from. In Proceedings of the annual conference of the cognitive science society, pages 2601\u20132606, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Satinder%20Lewis%2C%20Richard%20L.%20Barto%2C%20Andrew%20G.%20Where%20do%20rewards%20come%20from%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Satinder%20Lewis%2C%20Richard%20L.%20Barto%2C%20Andrew%20G.%20Where%20do%20rewards%20come%20from%202009"
        },
        {
            "id": "37",
            "entry": "[37] Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv preprint arXiv:1703.01703, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01703"
        },
        {
            "id": "38",
            "entry": "[38] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint arXiv:1805.01954, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01954"
        },
        {
            "id": "39",
            "entry": "[39] George Trigeorgis, Mihalis A Nicolaou, Bj\u00f6rn W Schuller, and Stefanos Zafeiriou. Deep canonical time warping for simultaneous alignment and representation learning of sequences. IEEE transactions on pattern analysis and machine intelligence, 40(5):1128\u20131138, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trigeorgis%2C%20George%20Nicolaou%2C%20Mihalis%20A.%20Schuller%2C%20Bj%C3%B6rn%20W.%20Zafeiriou%2C%20Stefanos%20Deep%20canonical%20time%20warping%20for%20simultaneous%20alignment%20and%20representation%20learning%20of%20sequences%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trigeorgis%2C%20George%20Nicolaou%2C%20Mihalis%20A.%20Schuller%2C%20Bj%C3%B6rn%20W.%20Zafeiriou%2C%20Stefanos%20Deep%20canonical%20time%20warping%20for%20simultaneous%20alignment%20and%20representation%20learning%20of%20sequences%202018"
        },
        {
            "id": "40",
            "entry": "[40] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3474"
        },
        {
            "id": "41",
            "entry": "[41] Matej Vecer\u00edk, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08817"
        },
        {
            "id": "42",
            "entry": "[42] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating the future by watching unlabeled video. arXiv preprint arXiv:1504.08023, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.08023"
        },
        {
            "id": "43",
            "entry": "[43] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20Hasselt%2C%20Hado%20Van%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20Hasselt%2C%20Hado%20Van%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "44",
            "entry": "[44] Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01557"
        },
        {
            "id": "45",
            "entry": "[45] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Conference on Computer Vision, pages 649\u2013666.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Richard%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Colorful%20image%20colorization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Richard%20Isola%2C%20Phillip%20and%20Alexei%20A%20Efros.%20Colorful%20image%20colorization"
        },
        {
            "id": "46",
            "entry": "[46] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. arXiv preprint arXiv:1412.6856, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6856"
        },
        {
            "id": "47",
            "entry": "[47] Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A Efros. Learning dense correspondence via 3d-guided cycle consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 117\u2013126, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Tinghui%20Krahenbuhl%2C%20Philipp%20Aubry%2C%20Mathieu%20Qixing%20Huang%2C%20and%20Alexei%20A%20Efros.%20Learning%20dense%20correspondence%20via%203d-guided%20cycle%20consistency%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Tinghui%20Krahenbuhl%2C%20Philipp%20Aubry%2C%20Mathieu%20Qixing%20Huang%2C%20and%20Alexei%20A%20Efros.%20Learning%20dense%20correspondence%20via%203d-guided%20cycle%20consistency%202016"
        },
        {
            "id": "48",
            "entry": "[48] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        },
        {
            "id": "49",
            "entry": "[49] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        }
    ]
}
