{
    "filename": "7395-evolution-guided-policy-gradient-in-reinforcement-learning.pdf",
    "metadata": {
        "title": "Evolution-Guided Policy Gradient in Reinforcement Learning",
        "author": "Shauharda Khadka, Kagan Tumer",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7395-evolution-guided-policy-gradient-in-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA\u2019s ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL\u2019s ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods."
    },
    "keywords": [
        {
            "term": "Evolutionary algorithms",
            "url": "https://en.wikipedia.org/wiki/Evolutionary_algorithms"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "hybrid algorithm",
            "url": "https://en.wikipedia.org/wiki/hybrid_algorithm"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        }
    ],
    "highlights": [
        "A standard reinforcement learning setting is formalized as a Markov Decision Process (MDP) and consists of an agent interacting with an environment E over a number of discrete time steps",
        "Results: Figure 3 shows the comparative performance of Evolutionary Reinforcement Learning, Evolutionary algorithms, Deep Deterministic Policy Gradient and Policy Optimization",
        "Evolutionary Reinforcement Learning is able to learn on the 3D quadruped locomotion Ant benchmark where Deep Deterministic Policy Gradient normally fails to make any learning progress [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We presented Evolutionary Reinforcement Learning, a hybrid algorithm that leverages the population of an Evolutionary algorithms to generate diverse experiences to train an Reinforcement learning agent, and reinserts the Reinforcement learning agent into the Evolutionary algorithms population sporadically to inject gradient information into the Evolutionary algorithms",
        "Evolutionary Reinforcement Learning recycles the date generated by the evolutionary population and leverages the replay buffer to learn from them repeatedly, allowing maximal information extraction from each experience leading to improved sample efficiency",
        "Results in a range of challenging continuous control benchmarks demonstrate that Evolutionary Reinforcement Learning outperforms state-of-the-art Deep Reinforcement Learning algorithms including Policy Optimization and Deep Deterministic Policy Gradient"
    ],
    "key_statements": [
        "A standard reinforcement learning setting is formalized as a Markov Decision Process (MDP) and consists of an agent interacting with an environment E over a number of discrete time steps",
        "We introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that incorporates Evolutionary algorithms\u2019s population-based approach to generate diverse experiences to train an Reinforcement learning agent, and transfers the Reinforcement learning agent into the Evolutionary algorithms population periodically to inject gradient information into the Evolutionary algorithms",
        "Experiments in a range of challenging continuous control benchmarks demonstrate that Evolutionary Reinforcement Learning significantly outperforms prior Deep Reinforcement Learning and Evolutionary algorithms methods",
        "A standard reinforcement learning setting is formalized as a Markov Decision Process (MDP) and consists of an agent interacting with an environment E over a number of discrete time steps",
        "A general flow of the Evolutionary Reinforcement Learning algorithm proceeds as follow: a population of actor networks is initialized with random weights",
        "For Evolutionary Reinforcement Learning, during each training generation, the actor network with the highest fitness was selected as the champion",
        "Results: Figure 3 shows the comparative performance of Evolutionary Reinforcement Learning, Evolutionary algorithms, Deep Deterministic Policy Gradient and Policy Optimization",
        "The performances of Deep Deterministic Policy Gradient and Policy Optimization were verified to have matched the ones reported in their original papers [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>]",
        "Evolutionary Reinforcement Learning is able to learn on the 3D quadruped locomotion Ant benchmark where Deep Deterministic Policy Gradient normally fails to make any learning progress [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We presented Evolutionary Reinforcement Learning, a hybrid algorithm that leverages the population of an Evolutionary algorithms to generate diverse experiences to train an Reinforcement learning agent, and reinserts the Reinforcement learning agent into the Evolutionary algorithms population sporadically to inject gradient information into the Evolutionary algorithms",
        "Evolutionary Reinforcement Learning recycles the date generated by the evolutionary population and leverages the replay buffer to learn from them repeatedly, allowing maximal information extraction from each experience leading to improved sample efficiency",
        "Results in a range of challenging continuous control benchmarks demonstrate that Evolutionary Reinforcement Learning outperforms state-of-the-art Deep Reinforcement Learning algorithms including Policy Optimization and Deep Deterministic Policy Gradient",
        "From a reinforcement learning perspective, Evolutionary Reinforcement Learning can be viewed as a form of \u2018population-driven guide\u2019 that biases exploration towards states with higher long-term returns, promotes diversity of explored policies, and introduces redundancies for stability",
        "Evolutionary Reinforcement Learning can be viewed as a Lamarckian mechanism that enables incorporation of powerful gradient-based methods to learn at the resolution of an agent\u2019s individual experiences",
        "The principal mechanism behind Evolutionary Reinforcement Learning is the capability to incorporate both modes of learning: learning directly from the high resolution of individual experiences while being aligned to maximize long term return by leveraging the low resolution fitness metric"
    ],
    "summary": [
        "A standard reinforcement learning setting is formalized as a Markov Decision Process (MDP) and consists of an agent interacting with an environment E over a number of discrete time steps.",
        "We introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that incorporates EA\u2019s population-based approach to generate diverse experiences to train an RL agent, and transfers the RL agent into the EA population periodically to inject gradient information into the EA.",
        "The sampled policy gradient with respect to the actor\u2019s parameters \u03b8\u03c0 is computed by backpropagation through the combined actor and critic network.",
        "The principal idea behind Evolutionary Reinforcement Learning (ERL) is to incorporate EA\u2019s population-based approach to generate a diverse set of experiences while leveraging powerful gradientbased methods from DRL to learn from them.",
        "A general flow of the ERL algorithm proceeds as follow: a population of actor networks is initialized with random weights.",
        "In contrast to a standard EA which would extract the fitness metric from these experiences and disregard them immediately, ERL retains them in the buffer and engages the rlactor and critic to learn from them repeatedly using powerful gradient-based methods.",
        "As the buffer is populated by the experiences collected by these individuals, this process biases the state distribution towards regions that have higher episode-wide return.",
        "Ideas within RL have been used to improve EAs. Gangwani and Peng devised a genetic algorithm using imitation learning and policy gradients as crossover and mutation operator, respectively [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "ERL inherits EA\u2019s invariance to sparse rewards with long time horizons, ability for diverse exploration, and stability of a population-based approach and complements it with DRL\u2019s ability to leverage gradients for lower sample complexity.",
        "ERL recycles the date generated by the evolutionary population and leverages the replay buffer to learn from them repeatedly, allowing maximal information extraction from each experience leading to improved sample efficiency.",
        "Results in a range of challenging continuous control benchmarks demonstrate that ERL outperforms state-of-the-art DRL algorithms including PPO and DDPG.",
        "From a reinforcement learning perspective, ERL can be viewed as a form of \u2018population-driven guide\u2019 that biases exploration towards states with higher long-term returns, promotes diversity of explored policies, and introduces redundancies for stability.",
        "ERL can be viewed as a Lamarckian mechanism that enables incorporation of powerful gradient-based methods to learn at the resolution of an agent\u2019s individual experiences.",
        "The principal mechanism behind ERL is the capability to incorporate both modes of learning: learning directly from the high resolution of individual experiences while being aligned to maximize long term return by leveraging the low resolution fitness metric."
    ],
    "headline": "We introduce Evolutionary Reinforcement Learning , a hybrid algorithm that leverages the population of an Evolutionary algorithms to provide diversified data to train an Reinforcement learning agent, and reinserts the Reinforcement learning agent into the Evolutionary algorithms population periodically to inject gradient information into the Evolutionary algorithms",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Ackley and M. Littman. Interactions between learning and evolution. Artificial life II, 10: 487\u2013509, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ackley%2C%20D.%20Littman%2C%20M.%20Interactions%20between%20learning%20and%20evolution.%20Artificial%20life%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ackley%2C%20D.%20Littman%2C%20M.%20Interactions%20between%20learning%20and%20evolution.%20Artificial%20life%201991"
        },
        {
            "id": "2",
            "entry": "[2] C. W. Ahn and R. S. Ramakrishna. Elitism-based compact genetic algorithms. IEEE Transactions on Evolutionary Computation, 7(4):367\u2013385, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahn%2C%20C.W.%20Ramakrishna%2C%20R.S.%20Elitism-based%20compact%20genetic%20algorithms%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahn%2C%20C.W.%20Ramakrishna%2C%20R.S.%20Elitism-based%20compact%20genetic%20algorithms%202003"
        },
        {
            "id": "3",
            "entry": "[3] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048\u20135058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20M.%20Wolski%2C%20F.%20Ray%2C%20A.%20Schneider%2C%20J.%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20M.%20Wolski%2C%20F.%20Ray%2C%20A.%20Schneider%2C%20J.%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "4",
            "entry": "[4] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471\u20131479, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.%20Srinivasan%2C%20S.%20Ostrovski%2C%20G.%20Schaul%2C%20T.%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.%20Srinivasan%2C%20S.%20Ostrovski%2C%20G.%20Schaul%2C%20T.%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "5",
            "entry": "[5] S. Bhatnagar, D. Precup, D. Silver, R. S. Sutton, H. R. Maei, and C. Szepesv\u00e1ri. Convergent temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems, pages 1204\u20131212, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhatnagar%2C%20S.%20Precup%2C%20D.%20Silver%2C%20D.%20Sutton%2C%20R.S.%20Convergent%20temporal-difference%20learning%20with%20arbitrary%20smooth%20function%20approximation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhatnagar%2C%20S.%20Precup%2C%20D.%20Silver%2C%20D.%20Sutton%2C%20R.S.%20Convergent%20temporal-difference%20learning%20with%20arbitrary%20smooth%20function%20approximation%202009"
        },
        {
            "id": "6",
            "entry": "[6] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "7",
            "entry": "[7] C. Colas, O. Sigaud, and P.-Y. Oudeyer. Gep-pg: Decoupling exploration and exploitation in deep reinforcement learning algorithms. arXiv preprint arXiv:1802.05054, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05054"
        },
        {
            "id": "8",
            "entry": "[8] E. Conti, V. Madhavan, F. P. Such, J. Lehman, K. O. Stanley, and J. Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. arXiv preprint arXiv:1712.06560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06560"
        },
        {
            "id": "9",
            "entry": "[9] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret. Robots that can adapt like animals. Nature, 521(7553):503, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cully%2C%20A.%20Clune%2C%20J.%20Tarapore%2C%20D.%20Mouret%2C%20J.-B.%20Robots%20that%20can%20adapt%20like%20animals%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cully%2C%20A.%20Clune%2C%20J.%20Tarapore%2C%20D.%20Mouret%2C%20J.-B.%20Robots%20that%20can%20adapt%20like%20animals%202015"
        },
        {
            "id": "10",
            "entry": "[10] K. De Asis, J. F. Hernandez-Garcia, G. Z. Holland, and R. S. Sutton. Multi-step reinforcement learning: A unifying algorithm. arXiv preprint arXiv:1703.01327, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01327"
        },
        {
            "id": "11",
            "entry": "[11] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "12",
            "entry": "[12] M. M. Drugan. Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms. Swarm and Evolutionary Computation, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drugan%2C%20M.M.%20Reinforcement%20learning%20versus%20evolutionary%20computation%3A%20A%20survey%20on%20hybrid%20algorithms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drugan%2C%20M.M.%20Reinforcement%20learning%20versus%20evolutionary%20computation%3A%20A%20survey%20on%20hybrid%20algorithms%202018"
        },
        {
            "id": "13",
            "entry": "[13] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u2013 1338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "14",
            "entry": "[14] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "15",
            "entry": "[15] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06070"
        },
        {
            "id": "16",
            "entry": "[16] C. Fernando, D. Banarse, M. Reynolds, F. Besse, D. Pfau, M. Jaderberg, M. Lanctot, and D. Wierstra. Convolution by evolution: Differentiable pattern producing networks. In Proceedings of the Genetic and Evolutionary Computation Conference 2016, pages 109\u2013116. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fernando%2C%20C.%20Banarse%2C%20D.%20Reynolds%2C%20M.%20Besse%2C%20F.%20Convolution%20by%20evolution%3A%20Differentiable%20pattern%20producing%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fernando%2C%20C.%20Banarse%2C%20D.%20Reynolds%2C%20M.%20Besse%2C%20F.%20Convolution%20by%20evolution%3A%20Differentiable%20pattern%20producing%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "18",
            "entry": "[18] D. Floreano, P. D\u00fcrr, and C. Mattiussi. Neuroevolution: from architectures to learning. Evolutionary Intelligence, 1(1):47\u201362, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Floreano%2C%20D.%20D%C3%BCrr%2C%20P.%20Mattiussi%2C%20C.%20Neuroevolution%3A%20from%20architectures%20to%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Floreano%2C%20D.%20D%C3%BCrr%2C%20P.%20Mattiussi%2C%20C.%20Neuroevolution%3A%20from%20architectures%20to%20learning%202008"
        },
        {
            "id": "19",
            "entry": "[19] D. B. Fogel. Evolutionary computation: toward a new philosophy of machine intelligence, volume 1. John Wiley & Sons, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fogel%2C%20D.B.%20Evolutionary%20computation%3A%20toward%20a%20new%20philosophy%20of%20machine%20intelligence%2C%20volume%201%202006"
        },
        {
            "id": "20",
            "entry": "[20] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.10295"
        },
        {
            "id": "21",
            "entry": "[21] S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09477"
        },
        {
            "id": "22",
            "entry": "[22] T. Gangwani and J. Peng. Genetic policy optimization. arXiv preprint arXiv:1711.01012, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01012"
        },
        {
            "id": "23",
            "entry": "[23] S. Gu, T. Lillicrap, R. E. Turner, Z. Ghahramani, B. Sch\u00f6lkopf, and S. Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3849\u20133858, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20S.%20Lillicrap%2C%20T.%20Turner%2C%20R.E.%20Ghahramani%2C%20Z.%20Interpolated%20policy%20gradient%3A%20Merging%20on-policy%20and%20off-policy%20gradient%20estimation%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20S.%20Lillicrap%2C%20T.%20Turner%2C%20R.E.%20Ghahramani%2C%20Z.%20Interpolated%20policy%20gradient%3A%20Merging%20on-policy%20and%20off-policy%20gradient%20estimation%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "24",
            "entry": "[24] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01290"
        },
        {
            "id": "25",
            "entry": "[25] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06560"
        },
        {
            "id": "26",
            "entry": "[26] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109\u20131117, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houthooft%2C%20R.%20Chen%2C%20X.%20Duan%2C%20Y.%20Schulman%2C%20J.%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Houthooft%2C%20R.%20Chen%2C%20X.%20Duan%2C%20Y.%20Schulman%2C%20J.%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016"
        },
        {
            "id": "27",
            "entry": "[27] R. Islam, P. Henderson, M. Gomrokchi, and D. Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04133"
        },
        {
            "id": "28",
            "entry": "[28] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09846"
        },
        {
            "id": "29",
            "entry": "[29] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "30",
            "entry": "[30] J. Lehman and K. O. Stanley. Exploiting open-endedness to solve problems through the search for novelty. In ALIFE, pages 329\u2013336, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20J.%20Stanley%2C%20K.O.%20Exploiting%20open-endedness%20to%20solve%20problems%20through%20the%20search%20for%20novelty%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20J.%20Stanley%2C%20K.O.%20Exploiting%20open-endedness%20to%20solve%20problems%20through%20the%20search%20for%20novelty%202008"
        },
        {
            "id": "31",
            "entry": "[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "32",
            "entry": "[32] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu. Hierarchical representations for efficient architecture search. arXiv preprint arXiv:1711.00436, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00436"
        },
        {
            "id": "33",
            "entry": "[33] B. L\u00fcders, M. Schl\u00e4ger, A. Korach, and S. Risi. Continual and one-shot learning through neural networks with dynamic external memory. In European Conference on the Applications of Evolutionary Computation, pages 886\u2013901.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B.%20L%C3%BCders%2C%20M.%20Schl%C3%A4ger%2C%20A.%20Korach%20Risi%2C%20S.%20Continual%20and%20one-shot%20learning%20through%20neural%20networks%20with%20dynamic%20external%20memory",
            "oa_query": "https://api.scholarcy.com/oa_version?query=B.%20L%C3%BCders%2C%20M.%20Schl%C3%A4ger%2C%20A.%20Korach%20Risi%2C%20S.%20Continual%20and%20one-shot%20learning%20through%20neural%20networks%20with%20dynamic%20external%20memory"
        },
        {
            "id": "34",
            "entry": "[34] A. R. Mahmood, H. Yu, and R. S. Sutton. Multi-step off-policy learning without importance sampling ratios. arXiv preprint arXiv:1702.03006, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03006"
        },
        {
            "id": "35",
            "entry": "[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "36",
            "entry": "[36] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "37",
            "entry": "[37] R. Munos. Q (\u03bb) with off-policy corrections. In Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings, volume 9925, page 305.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R.%20Q%20%28%CE%BB%29%20with%20off-policy%20corrections%202016-10-19",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R.%20Q%20%28%CE%BB%29%20with%20off-policy%20corrections%202016-10-19"
        },
        {
            "id": "38",
            "entry": "[38] G. Ostrovski, M. G. Bellemare, A. v. d. Oord, and R. Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01310"
        },
        {
            "id": "39",
            "entry": "[39] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20A.%20Gross%2C%20S.%20Chintala%2C%20S.%20Chanan%2C%20G.%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "40",
            "entry": "[40] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by selfsupervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction"
        },
        {
            "id": "41",
            "entry": "[41] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen, T. Asfour, P. Abbeel, and M. Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "42",
            "entry": "[42] J. K. Pugh, L. B. Soros, and K. O. Stanley. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pugh%2C%20J.K.%20Soros%2C%20L.B.%20Stanley%2C%20K.O.%20Quality%20diversity%3A%20A%20new%20frontier%20for%20evolutionary%20computation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pugh%2C%20J.K.%20Soros%2C%20L.B.%20Stanley%2C%20K.O.%20Quality%20diversity%3A%20A%20new%20frontier%20for%20evolutionary%20computation%202016"
        },
        {
            "id": "43",
            "entry": "[43] S. Risi and J. Togelius. Neuroevolution in games: State of the art and open challenges. IEEE Transactions on Computational Intelligence and AI in Games, 9(1):25\u201341, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Risi%2C%20S.%20Togelius%2C%20J.%20Neuroevolution%20in%20games%3A%20State%20of%20the%20art%20and%20open%20challenges%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Risi%2C%20S.%20Togelius%2C%20J.%20Neuroevolution%20in%20games%3A%20State%20of%20the%20art%20and%20open%20challenges%202017"
        },
        {
            "id": "44",
            "entry": "[44] T. Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "45",
            "entry": "[45] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "46",
            "entry": "[46] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "47",
            "entry": "[47] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "48",
            "entry": "[48] C. Sherstan, B. Bennett, K. Young, D. R. Ashley, A. White, M. White, and R. S. Sutton. Directly estimating the variance of the {\\lambda}-return using temporal-difference methods. arXiv preprint arXiv:1801.08287, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.08287"
        },
        {
            "id": "49",
            "entry": "[49] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "50",
            "entry": "[50] W. M. Spears, K. A. De Jong, T. B\u00e4ck, D. B. Fogel, and H. De Garis. An overview of evolutionary computation. In European Conference on Machine Learning, pages 442\u2013459.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spears%2C%20W.M.%20Jong%2C%20K.A.De%20B%C3%A4ck%2C%20T.%20Fogel%2C%20D.B.%20An%20overview%20of%20evolutionary%20computation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spears%2C%20W.M.%20Jong%2C%20K.A.De%20B%C3%A4ck%2C%20T.%20Fogel%2C%20D.B.%20An%20overview%20of%20evolutionary%20computation"
        },
        {
            "id": "51",
            "entry": "[51] A. Stafylopatis and K. Blekas. Autonomous vehicle navigation using evolutionary reinforcement learning. European Journal of Operational Research, 108(2):306\u2013318, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stafylopatis%2C%20A.%20Blekas%2C%20K.%20Autonomous%20vehicle%20navigation%20using%20evolutionary%20reinforcement%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stafylopatis%2C%20A.%20Blekas%2C%20K.%20Autonomous%20vehicle%20navigation%20using%20evolutionary%20reinforcement%20learning%201998"
        },
        {
            "id": "52",
            "entry": "[52] K. O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary computation, 10(2):99\u2013127, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20K.O.%20Miikkulainen%2C%20R.%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanley%2C%20K.O.%20Miikkulainen%2C%20R.%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002"
        },
        {
            "id": "53",
            "entry": "[53] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06567"
        },
        {
            "id": "54",
            "entry": "[54] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "55",
            "entry": "[55] H. Tang, R. Houthooft, D. Foote, A. Stooke, O. X. Chen, Y. Duan, J. Schulman, F. DeTurck, and P. Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 2750\u20132759, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20H.%20Houthooft%2C%20R.%20Foote%2C%20D.%20Stooke%2C%20A.%20Abbeel.%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20H.%20Houthooft%2C%20R.%20Foote%2C%20D.%20Stooke%2C%20A.%20Abbeel.%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "56",
            "entry": "[56] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "57",
            "entry": "[57] P. Turney, D. Whitley, and R. W. Anderson. Evolution, learning, and instinct: 100 years of the baldwin effect. Evolutionary Computation, 4(3):iv\u2013viii, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Turney%2C%20P.%20Whitley%2C%20D.%20Anderson%2C%20R.W.%20Evolution%2C%20learning%2C%20and%20instinct%3A%20100%20years%20of%20the%20baldwin%20effect%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Turney%2C%20P.%20Whitley%2C%20D.%20Anderson%2C%20R.W.%20Evolution%2C%20learning%2C%20and%20instinct%3A%20100%20years%20of%20the%20baldwin%20effect%201996"
        },
        {
            "id": "58",
            "entry": "[58] G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion. Physical review, 36 (5):823, 1930.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uhlenbeck%2C%20G.E.%20Ornstein%2C%20L.S.%20On%20the%20theory%20of%20the%20brownian%20motion%201930"
        },
        {
            "id": "59",
            "entry": "[59] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01224"
        },
        {
            "id": "60",
            "entry": "[60] S. Whiteson and P. Stone. Evolutionary function approximation for reinforcement learning. Journal of Machine Learning Research, 7(May):877\u2013917, 2006. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Whiteson%2C%20S.%20Stone%2C%20P.%20Evolutionary%20function%20approximation%20for%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Whiteson%2C%20S.%20Stone%2C%20P.%20Evolutionary%20function%20approximation%20for%20reinforcement%20learning%202006"
        }
    ]
}
