{
    "filename": "7766-channelnets-compact-and-efficient-convolutional-neural-networks-via-channel-wise-convolutions.pdf",
    "metadata": {
        "date": 2018,
        "title": "ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions",
        "author": "Hongyang Gao Texas A&M University",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7766-channelnets-compact-and-efficient-convolutional-neural-networks-via-channel-wise-convolutions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. ChannelNets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods."
    },
    "keywords": [
        {
            "term": "computational cost",
            "url": "https://en.wikipedia.org/wiki/computational_cost"
        },
        {
            "term": "convolutional layer",
            "url": "https://en.wikipedia.org/wiki/convolutional_layer"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "mobile device",
            "url": "https://en.wikipedia.org/wiki/mobile_device"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "The trainable layers of Convolutional neural networks are commonly composed of convolutional layers and fully-connected layers",
        "By observing that the fully-connected pattern accounts for most parameters in Convolutional neural networks, we propose channel-wise convolutions, which are used to replace dense connections among feature maps with sparse ones",
        "ChannelNets achieve a better trade-off between efficiency and accuracy than prior compact Convolutional neural networks, as demonstrated by experimental results on the ImageNet ILSVRC 2012 dataset",
        "Through analysis of the generalized method, we propose a convolutional classification layer to replace the fully-connected output layer in Section 3.4, which further reduces the amounts of parameters and computations",
        "The convolutional classification layer is the first attempt in the field of model compression to compress the the fully-connected classification layer",
        "The current study evaluates the proposed methods on image classification tasks, but the methods can be applied to other tasks, such as detection and segmentation"
    ],
    "key_statements": [
        "The trainable layers of Convolutional neural networks are commonly composed of convolutional layers and fully-connected layers",
        "By observing that the fully-connected pattern accounts for most parameters in Convolutional neural networks, we propose channel-wise convolutions, which are used to replace dense connections among feature maps with sparse ones",
        "ChannelNets achieve a better trade-off between efficiency and accuracy than prior compact Convolutional neural networks, as demonstrated by experimental results on the ImageNet ILSVRC 2012 dataset",
        "Since the convolutional kernel is shared for each spatial location, for any pair of input and output feature maps, the connections are sparse and weighted by dk \u21e5 dk shared parameters",
        "Through analysis of the generalized method, we propose a convolutional classification layer to replace the fully-connected output layer in Section 3.4, which further reduces the amounts of parameters and computations",
        "We propose channel-wise convolutions to perform model compression by replacing dense connections in deep networks",
        "The convolutional classification layer is the first attempt in the field of model compression to compress the the fully-connected classification layer",
        "ChannelNets achieve a better trade-off between efficiency and accuracy",
        "The current study evaluates the proposed methods on image classification tasks, but the methods can be applied to other tasks, such as detection and segmentation"
    ],
    "summary": [
        "The trainable layers of CNNs are commonly composed of convolutional layers and fully-connected layers.",
        "The depth-wise convolution applies a single convolutional kernel independently for each input feature map, generating the same number of output channels.",
        "While MobileNets successfully employed depth-wise separable convolutions to perform model compression and achieve competitive results, it is noted that the m \u21e5 n term still dominates the number of parameters in the models.",
        "Through analysis of the generalized method, we propose a convolutional classification layer to replace the fully-connected output layer in Section 3.4, which further reduces the amounts of parameters and computations.",
        "The number of parameters in a channel-wise convolution with a kernel size of dc is dc and the computational cost is dc \u21e5 n \u21e5 df \u21e5 df .",
        "Given n input feature maps that are divided into g groups, this operation performs g independent channel-wise convolutions.",
        "Our ChannelNets with the convolutional classification layer achieve much better results than other models with similar amounts of parameters.",
        "In our ChannelNet-v1, we choose to replace depth-wise separable convolutions with larger numbers of input and output channels.",
        "Six consecutive depth-wise separable convolutional layers with 512 input and output channels are replaced by two GCWMs followed by one GM.",
        "We use the depth-wise separable channel-wise convolution to replace this layer, leading to ChannelNet-v2.",
        "We compare different versions of ChannelNets with other compact CNNs. Ablation studies are conducted to show the effect of group channel-wise convolutions.",
        "The speed is even slower than AlexNet and the accuracy is not competitive to other compact CNNs. By replacing depth-wise separable convolutions with GMs and GCWMs, ChannelNet-v1 achieves nearly the same perfor-",
        "The width multiplier is proposed in [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] to make the Table 2: Comparison between ChannelNets network architecture thinner by reducing the number and other compact CNNs with width multiof input and output channels in each layer, thereby inpliers in terms of the top-1 accuracy on the creasing the compression level.",
        "We can observe from the results that ChannelNet-v2 outperforms 0.75 MobileNet with an absolute 1.1% gain in accuracy, which demonstrates the effect of our depth-wise separable channel-wise convolutions.",
        "We propose channel-wise convolutions to perform model compression by replacing dense connections in deep networks.",
        "We build a new family of compact and efficient CNNs, known as ChannelNets, by using three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer."
    ],
    "headline": "We propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in Convolutional neural networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. In International Conference on Machine Learning, pages 2285\u20132294, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Wenlin%20Wilson%2C%20James%20Tyree%2C%20Stephen%20Weinberger%2C%20Kilian%20Compressing%20neural%20networks%20with%20the%20hashing%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Wenlin%20Wilson%2C%20James%20Tyree%2C%20Stephen%20Weinberger%2C%20Kilian%20Compressing%20neural%20networks%20with%20the%20hashing%20trick%202015"
        },
        {
            "id": "2",
            "entry": "[2] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chollet%2C%20Fran%C3%A7ois%20Xception%3A%20Deep%20learning%20with%20depthwise%20separable%20convolutions.%20arXiv%20p%202016"
        },
        {
            "id": "3",
            "entry": "[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "4",
            "entry": "[4] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202015"
        },
        {
            "id": "5",
            "entry": "[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "6",
            "entry": "[6] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "7",
            "entry": "[7] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "8",
            "entry": "[8] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "9",
            "entry": "[9] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Vedaldi%2C%20Andrea%20Zisserman%2C%20Andrew%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014"
        },
        {
            "id": "10",
            "entry": "[10] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3D convolutional neural networks for human action recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):221\u2013231, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ji%2C%20Shuiwang%20Xu%2C%20Wei%20Yang%2C%20Ming%20Yu%2C%20Kai%203D%20convolutional%20neural%20networks%20for%20human%20action%20recognition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ji%2C%20Shuiwang%20Xu%2C%20Wei%20Yang%2C%20Ming%20Yu%2C%20Kai%203D%20convolutional%20neural%20networks%20for%20human%20action%20recognition%202013"
        },
        {
            "id": "11",
            "entry": "[11] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "12",
            "entry": "[12] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6553"
        },
        {
            "id": "13",
            "entry": "[13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86 (11):2278\u20132324, November 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11-11"
        },
        {
            "id": "14",
            "entry": "[14] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.4400"
        },
        {
            "id": "15",
            "entry": "[15] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525\u2013542.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rastegari%2C%20Mohammad%20Ordonez%2C%20Vicente%20Redmon%2C%20Joseph%20Farhadi%2C%20Ali%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks"
        },
        {
            "id": "16",
            "entry": "[16] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. arXiv preprint arXiv:1801.04381, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04381"
        },
        {
            "id": "17",
            "entry": "[17] Abigail See, Minh-Thang Luong, and Christopher D Manning. Compression of neural machine translation models via pruning. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 291\u2013301, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=See%2C%20Abigail%20Luong%2C%20Minh-Thang%20Manning%2C%20Christopher%20D.%20Compression%20of%20neural%20machine%20translation%20models%20via%20pruning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=See%2C%20Abigail%20Luong%2C%20Minh-Thang%20Manning%2C%20Christopher%20D.%20Compression%20of%20neural%20machine%20translation%20models%20via%20pruning%202016"
        },
        {
            "id": "18",
            "entry": "[18] Laurent Sifre and PS Mallat. Rigid-motion scattering for image classification. PhD thesis, Citeseer, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sifre%2C%20Laurent%20Mallat%2C%20P.S.%20Rigid-motion%20scattering%20for%20image%20classification%202014"
        },
        {
            "id": "19",
            "entry": "[19] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. Proceedings of the International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "20",
            "entry": "[20] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "21",
            "entry": "[21] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "22",
            "entry": "[22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "23",
            "entry": "[23] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4820\u20134828, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20convolutional%20neural%20networks%20for%20mobile%20devices%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiaxiang%20Leng%2C%20Cong%20Wang%2C%20Yuhang%20Hu%2C%20Qinghao%20Quantized%20convolutional%20neural%20networks%20for%20mobile%20devices%202016"
        },
        {
            "id": "24",
            "entry": "[24] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1707.01083"
        }
    ]
}
