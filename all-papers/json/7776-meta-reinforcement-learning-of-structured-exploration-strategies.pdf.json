{
    "filename": "7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf",
    "metadata": {
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies",
        "author": "Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey Levine",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm \u2013 model agnostic exploration with structured noise (MAESN) \u2013 to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation."
    },
    "keywords": [
        {
            "term": "thompson sampling",
            "url": "https://en.wikipedia.org/wiki/thompson_sampling"
        },
        {
            "term": "intrinsic motivation",
            "url": "https://en.wikipedia.org/wiki/intrinsic_motivation"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "Deep reinforcement learning methods have been shown to learn complex tasks ranging from games [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] to robotic control [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] with minimal supervision, by exploring the environment and experiencing rewards",
        "Prior works have proposed guiding exploration based on criteria such as intrinsic motivation [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], state-visitation counts [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Thompson sampling and bootstrapped models [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], optimism in the face of uncertainty [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], and parameter space exploration [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "Since model-agnostic meta-learning reverts to conventional policy gradient when faced with out-of-distribution tasks, it provides a natural starting point for us to consider the design of a meta-exploration algorithm: by starting with a method that is essentially on par with task-agnostic reinforcement learning methods that learn from scratch in the worst case, we can improve on it to incorporate the ability to acquire stochastic exploration strategies from experience, while preserving asymptotic performance.\n3 Model Agnostic Exploration with Structured Noise",
        "While meta-learning has been shown to be effective for fast adaptation on several reinforcement learning problems [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], the prior methods generally focus on tasks where exploration is trivial and a few random trials are sufficient to identify the goals of the task [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], or the policy should acquire a consistent \u201csearch\u201d strategy, for example to find the exit in new mazes [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We presented MAESN, a meta-reinforcement learning algorithm that explicitly learns to explore by combining gradientbased meta-learning with a learned latent exploration space"
    ],
    "key_statements": [
        "Deep reinforcement learning methods have been shown to learn complex tasks ranging from games [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] to robotic control [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] with minimal supervision, by exploring the environment and experiencing rewards",
        "Prior works have proposed guiding exploration based on criteria such as intrinsic motivation [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], state-visitation counts [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Thompson sampling and bootstrapped models [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], optimism in the face of uncertainty [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], and parameter space exploration [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "Since model-agnostic meta-learning reverts to conventional policy gradient when faced with out-of-distribution tasks, it provides a natural starting point for us to consider the design of a meta-exploration algorithm: by starting with a method that is essentially on par with task-agnostic reinforcement learning methods that learn from scratch in the worst case, we can improve on it to incorporate the ability to acquire stochastic exploration strategies from experience, while preserving asymptotic performance.\n3 Model Agnostic Exploration with Structured Noise",
        "While meta-learning has been shown to be effective for fast adaptation on several reinforcement learning problems [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], the prior methods generally focus on tasks where exploration is trivial and a few random trials are sufficient to identify the goals of the task [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], or the policy should acquire a consistent \u201csearch\u201d strategy, for example to find the exit in new mazes [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "To understand the exploration strategies learned by MAESN, we visualize the trajectories obtained by sampling from the meta-learned latentconditioned policy \u03c0\u03b8 with the latent distribution q\u03c9(z) set to the prior N (0, I)",
        "We evaluate whether the noise injected from the latent space learned by MAESN is used for exploration",
        "We presented MAESN, a meta-reinforcement learning algorithm that explicitly learns to explore by combining gradientbased meta-learning with a learned latent exploration space"
    ],
    "summary": [
        "Deep reinforcement learning methods have been shown to learn complex tasks ranging from games [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] to robotic control [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] with minimal supervision, by exploring the environment and experiencing rewards.",
        "We aim to address these challenges by devising a meta-RL algorithm that adapts to new tasks by following the policy gradient, while injecting learned structured stochasticity into a latent space to enable effective exploration.",
        "Which we call model agnostic exploration with structured noise (MAESN), uses prior experience both to initialize a policy and to learn a latent exploration space from which it can sample temporally coherent structured behaviors.",
        "Since MAML reverts to conventional policy gradient when faced with out-of-distribution tasks, it provides a natural starting point for us to consider the design of a meta-exploration algorithm: by starting with a method that is essentially on par with task-agnostic RL methods that learn from scratch in the worst case, we can improve on it to incorporate the ability to acquire stochastic exploration strategies from experience, while preserving asymptotic performance.",
        "While meta-learning has been shown to be effective for fast adaptation on several RL problems [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], the prior methods generally focus on tasks where exploration is trivial and a few random trials are sufficient to identify the goals of the task [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], or the policy should acquire a consistent \u201csearch\u201d strategy, for example to find the exit in new mazes [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "Both the policy and the latent space are trained with meta-learning to explicitly provide for fast adaptation to new tasks.",
        "Given a latent variable conditioned policy as described above, our goal is to train it so as to capture coherent exploration strategies from a family of training tasks that enable fast adaptation to new tasks from a similar distribution.",
        "Our aim is to meta-train the policy parameters \u03b8 so that they can make use of the latent variables to perform coherent exploration on a new task and the behavior can be adapted as fast as possible.",
        "Our experiments aim to comparatively evaluate our meta-learning method and study the following questions: (1) Can meta-learned exploration strategies with MAESN explore coherently and adapt quickly to new tasks, providing a significant advantage over learning from scratch?",
        "To understand the exploration strategies learned by MAESN, we visualize the trajectories obtained by sampling from the meta-learned latentconditioned policy \u03c0\u03b8 with the latent distribution q\u03c9(z) set to the prior N (0, I).",
        "MAESN learns a latent space that can be used to inject temporally correlated, coherent stochasticity into the policy to explore effectively at meta-test time.",
        "It\u2019s worth noting, that our approach is not mutually exclusive with these methods, and a promising direction for future work would be to combine our approach with these methods [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]"
    ],
    "headline": "We study how prior tasks can inform an agent about how to explore effectively in new situations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning to learn by gradient descent by gradient descent. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20M.%20Denil%2C%20M.%20Colmenarejo%2C%20S.G.%20Hoffman%2C%20M.W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "2",
            "entry": "[2] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Srinivasan%2C%20S.%20Ostrovski%2C%20G.%20Schaul%2C%20T.%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.G.%20Srinivasan%2C%20S.%20Ostrovski%2C%20G.%20Schaul%2C%20T.%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "3",
            "entry": "[3] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3:213\u2013231, Mar. 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brafman%2C%20R.I.%20Tennenholtz%2C%20M.%20R-max%20-%20a%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202003-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brafman%2C%20R.I.%20Tennenholtz%2C%20M.%20R-max%20-%20a%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202003-03"
        },
        {
            "id": "4",
            "entry": "[4] O. Chapelle and L. Li. An empirical evaluation of thompson sampling. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2249\u20132257. Curran Associates, Inc., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20O.%20Li%2C%20L.%20An%20empirical%20evaluation%20of%20thompson%20sampling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20O.%20Li%2C%20L.%20An%20empirical%20evaluation%20of%20thompson%20sampling%202011"
        },
        {
            "id": "5",
            "entry": "[5] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl$\u02c62$: Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "6",
            "entry": "[6] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In D. Precup and Y. W. Teh, editors, ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "7",
            "entry": "[7] C. Florensa, Y. Duan, and P. Abbeel. Stochastic neural networks for hierarchical reinforcement learning. CoRR, abs/1704.03012, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03012"
        },
        {
            "id": "8",
            "entry": "[8] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin, C. Blundell, and S. Legg. Noisy networks for exploration. CoRR, abs/1706.10295, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.10295"
        },
        {
            "id": "9",
            "entry": "[9] K. Hausman, J. T. Springenberg, N. H. Ziyu Wang, and M. Riedmiller. Learning an embedding space for transferable robot skills. In Proceedings of the International Conference on Learning Representations, ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausman%2C%20K.%20Springenberg%2C%20J.T.%20Wang%2C%20N.H.Ziyu%20Riedmiller%2C%20M.%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausman%2C%20K.%20Springenberg%2C%20J.T.%20Wang%2C%20N.H.Ziyu%20Riedmiller%2C%20M.%20Learning%20an%20embedding%20space%20for%20transferable%20robot%20skills%202018"
        },
        {
            "id": "10",
            "entry": "[10] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In Artificial Neural Networks - ICANN 2001, International Conference Vienna, Austria, August 21-25, 2001 Proceedings, pages 87\u201394, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Younger%2C%20A.S.%20Conwell%2C%20P.R.%20Learning%20to%20learn%20using%20gradient%20descent%202001-08-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Younger%2C%20A.S.%20Conwell%2C%20P.R.%20Learning%20to%20learn%20using%20gradient%20descent%202001-08-21"
        },
        {
            "id": "11",
            "entry": "[11] R. Houthooft, X. Chen, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational information maximizing exploration. In NIPS. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houthooft%2C%20R.%20Chen%2C%20X.%20Chen%2C%20X.%20Duan%2C%20Y.%20Vime%3A%20Variational%20information%20maximizing%20exploration.%20In%20NIPS%202016"
        },
        {
            "id": "12",
            "entry": "[12] M. J. Kearns and S. P. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209\u2013232, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20M.J.%20Singh%2C%20S.P.%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20M.J.%20Singh%2C%20S.P.%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002"
        },
        {
            "id": "13",
            "entry": "[13] J. Z. Kolter and A. Y. Ng. Learning omnidirectional path following using dimensionality reduction. In RSS, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolter%2C%20J.Z.%20Ng%2C%20A.Y.%20Learning%20omnidirectional%20path%20following%20using%20dimensionality%20reduction%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolter%2C%20J.Z.%20Ng%2C%20A.Y.%20Learning%20omnidirectional%20path%20following%20using%20dimensionality%20reduction%202007"
        },
        {
            "id": "14",
            "entry": "[14] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. JMLR, 17(39):1\u201340, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "15",
            "entry": "[15] Z. Li, F. Zhou, F. Chen, and H. Li. Meta-sgd: Learning to learn quickly for few shot learning. CoRR, abs/1707.09835, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09835"
        },
        {
            "id": "16",
            "entry": "[16] M. Lopes, T. Lang, M. Toussaint, and P. yves Oudeyer. Exploration in model-based reinforcement learning by empirically estimating learning progress. In NIPS. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopes%2C%20M.%20Lang%2C%20T.%20Toussaint%2C%20M.%20yves%20Oudeyer%2C%20P.%20Exploration%20in%20model-based%20reinforcement%20learning%20by%20empirically%20estimating%20learning%20progress.%20In%20NIPS%202012"
        },
        {
            "id": "17",
            "entry": "[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "18",
            "entry": "[18] I. Osband, C. Blundell, A. Pritzel, and B. V. Roy. Deep exploration via bootstrapped DQN. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20I.%20Blundell%2C%20C.%20Pritzel%2C%20A.%20Roy%2C%20B.V.%20Deep%20exploration%20via%20bootstrapped%20DQN%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20I.%20Blundell%2C%20C.%20Pritzel%2C%20A.%20Roy%2C%20B.V.%20Deep%20exploration%20via%20bootstrapped%20DQN%202016"
        },
        {
            "id": "19",
            "entry": "[19] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen, T. Asfour, P. Abbeel, and M. Andrychowicz. Parameter space noise for exploration. CoRR, abs/1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "20",
            "entry": "[20] A. Rajeswaran, V. Kumar, A. Gupta, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. CoRR, abs/1709.10087, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10087"
        },
        {
            "id": "21",
            "entry": "[21] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In Proceedings of the International Conference on Learning Representations, ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. P. Lillicrap. Meta-learning with memory-augmented neural networks. In M. Balcan and K. Q. Weinberger, editors, ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20A.%20Bartunov%2C%20S.%20Botvinick%2C%20M.%20Wierstra%2C%20D.%20Meta-learning%20with%20memory-augmented%20neural%20networks%202016"
        },
        {
            "id": "23",
            "entry": "[23] J. Schmidhuber. Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta...-hook. Diploma thesis, Technische Universitat Munchen, Germany, 14 May 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Evolutionary%20principles%20in%20self-referential%20learning.%20on%20learning%20now%20to%20learn%3A%20The%20meta-meta-meta...-hook.%20Diploma%20thesis%2C%20Technische%20Universitat%20Munchen%2C%20Germany%201987-05-14"
        },
        {
            "id": "24",
            "entry": "[24] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.I.%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.I.%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "25",
            "entry": "[25] S. P. Singh, A. G. Barto, and N. Chentanez. Intrinsically motivated reinforcement learning. In NIPS, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20S.P.%20Barto%2C%20A.G.%20Chentanez%2C%20N.%20Intrinsically%20motivated%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20S.P.%20Barto%2C%20A.G.%20Chentanez%2C%20N.%20Intrinsically%20motivated%20reinforcement%20learning%202004"
        },
        {
            "id": "26",
            "entry": "[26] B. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. CoRR, abs/1507.00814, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00814"
        },
        {
            "id": "27",
            "entry": "[27] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision processes. J. Comput. Syst. Sci., 74(8):1309\u20131331, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20A.L.%20Littman%2C%20M.L.%20An%20analysis%20of%20model-based%20interval%20estimation%20for%20markov%20decision%20processes%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20A.L.%20Littman%2C%20M.L.%20An%20analysis%20of%20model-based%20interval%20estimation%20for%20markov%20decision%20processes%202008"
        },
        {
            "id": "28",
            "entry": "[28] S. Thrun and L. Pratt. Learning to learn. chapter Learning to Learn: Introduction and Overview, pages 3\u201317. Kluwer Academic Publishers, Norwell, MA, USA, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20S.%20Pratt%2C%20L.%20Learning%20to%20learn.%20chapter%20Learning%20to%20Learn%3A%20Introduction%20and%20Overview%201998"
        },
        {
            "id": "29",
            "entry": "[29] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot learning. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Blundell%2C%20C.%20Lillicrap%2C%20T.%20Kavukcuoglu%2C%20K.%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "30",
            "entry": "[30] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. CoRR, abs/1611.05763, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05763"
        },
        {
            "id": "31",
            "entry": "[31] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229\u2013256, 1992. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        }
    ]
}
