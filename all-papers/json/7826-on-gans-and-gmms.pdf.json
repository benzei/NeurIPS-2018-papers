{
    "filename": "7826-on-gans-and-gmms.pdf",
    "metadata": {
        "title": "Eitan Richardson School of Computer Science and Engineering",
        "author": "The Hebrew University of Jerusalem Jerusalem, Israel eitanrich@cs.huji.ac.il",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7826-on-gans-and-gmms.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution (\"mode collapse\") and that using the learned models for anything other than generating samples may be very difficult. In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images. 1"
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "statistical model",
            "url": "https://en.wikipedia.org/wiki/statistical_model"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "Birthday Paradox",
            "url": "https://en.wikipedia.org/wiki/Birthday_Paradox"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        }
    ],
    "highlights": [
        "Natural images take up only a tiny fraction of the space of possible images",
        "While Gaussian Mixture Model have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality",
        "number of statistically-different bins evaluation shows that, like Generative Adversarial Networks, adversarial Gaussian Mixture Model suffers from mode collapse and futhermore the log likelihood this Mixture of Factor Analyzers model gives to the data is far worse than traditional, maximum likelihood training",
        "In this paper we investigated the utility of Generative Adversarial Networks for learning statistical models of images by comparing them to the humble Gaussian Mixture Model",
        "We showed that it is possible to efficiently train Gaussian Mixture Model on the same datasets that are usually used with Generative Adversarial Networks",
        "We showed that the Gaussian Mixture Model generates realistic samples ( not as sharp as the Generative Adversarial Networks samples) but unlike Generative Adversarial Networks it does an excellent job of capturing the underlying distribution and provides explicit representation of the statistical structure"
    ],
    "key_statements": [
        "Natural images take up only a tiny fraction of the space of possible images",
        "Finding a way to explicitly model the statistical structure of such images is a longstanding problem with applications to engineering and to computational neuroscience",
        "Perhaps the most dramatic success in modeling full images has been achieved by Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], which can learn to generate remarkably realistic samples at high resolution [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], (Fig. 1)",
        "The focus on the quality of the generated images has perhaps decreased the focus on the original question: to what extent are Generative Adversarial Networks learning useful statistical models of the data? In this paper, we try to address this question more directly by comparing Generative Adversarial Networks to perhaps the simplest statistical model, the Gaussian Mixture Model",
        "We present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins",
        "We compare the performance of Generative Adversarial Networks to Gaussian Mixture Model trained on the same datasets",
        "While Gaussian Mixture Model have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality",
        "Our results show that Gaussian Mixture Model can generate realistic samples ( less sharp than those of Generative Adversarial Networks) but capture the full distribution which Generative Adversarial Networks fail to do",
        "number of statistically-different bins evaluation shows that, like Generative Adversarial Networks, adversarial Gaussian Mixture Model suffers from mode collapse and futhermore the log likelihood this Mixture of Factor Analyzers model gives to the data is far worse than traditional, maximum likelihood training",
        "In this paper we investigated the utility of Generative Adversarial Networks for learning statistical models of images by comparing them to the humble Gaussian Mixture Model",
        "We showed that it is possible to efficiently train Gaussian Mixture Model on the same datasets that are usually used with Generative Adversarial Networks",
        "We showed that the Gaussian Mixture Model generates realistic samples ( not as sharp as the Generative Adversarial Networks samples) but unlike Generative Adversarial Networks it does an excellent job of capturing the underlying distribution and provides explicit representation of the statistical structure"
    ],
    "summary": [
        "Natural images take up only a tiny fraction of the space of possible images.",
        "Perhaps the most dramatic success in modeling full images has been achieved by Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], which can learn to generate remarkably realistic samples at high resolution [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], (Fig. 1).",
        "As an alternative to log likelihood, one could calculate the Wasserstein distance betweeen generated samples and the training data, but this is again intractable in high dimensions so approximations must be used [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "The focus on the quality of the generated images has perhaps decreased the focus on the original question: to what extent are GANs learning useful statistical models of the data?",
        "Our results show that GMMs can generate realistic samples but capture the full distribution which GANs fail to do.",
        "In order to provide context on the utility of GANs in learning statistical models of images, we compare it to perhaps the simplest possible statistical model: the Gaussian Mixture Model (GMM) trained on the same datasets.",
        "Using equations 4 - 6, the complexity of the log-likelihood computation is linear in the image dimension d, allowing to train the MFA model efficiently on full-image datasets.",
        "We compare the GMM model to the GAN models along three attributes: (1) visual quality of samples (2) our quantitative NDB score and (3) ability to capture the statistical structure and perform efficient inference.",
        "Random samples from our MFA models trained on the three datasets are shown in Fig. 4.",
        "Since the latent variable z is sampled from a standard-normal distribution, any linear combination of column vectors from the component should result in a realistic image, as guaranteed by the log-likelihood training objective.",
        "We experiment with the idea of combining the benefits of GMM with the fine-details of GAN in order to generate sharp images while still being loyal to the data distribution.",
        "Higher-resolution samples generated by our MFA+pix2pix models are shown in Fig. 8 and in the supplementary material.",
        "NDB evaluation shows that, like GANs, adversarial GMM suffers from mode collapse and futhermore the log likelihood this MFA model gives to the data is far worse than traditional, maximum likelihood training.",
        "We showed that it is possible to efficiently train GMMs on the same datasets that are usually used with GANs. We showed that the GMM generates realistic samples but unlike GANs it does an excellent job of capturing the underlying distribution and provides explicit representation of the statistical structure.",
        "We showed that the GMM generates realistic samples but unlike GANs it does an excellent job of capturing the underlying distribution and provides explicit representation of the statistical structure"
    ],
    "headline": "We examine the utility of Generative Adversarial Networks in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. TensorFlow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein Generative Adversarial Networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20Generative%20Adversarial%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20Generative%20Adversarial%20Networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202018"
        },
        {
            "id": "4",
            "entry": "[4] Anthony J Bell and Terrence J Sejnowski. Edges are the \u2019independent components\u2019 of natural scenes. In Advances in neural information processing systems, pages 831\u2013837, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bell%2C%20Anthony%20J.%20Sejnowski%2C%20Terrence%20J.%20Edges%20are%20the%E2%80%99independent%20components%E2%80%99%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bell%2C%20Anthony%20J.%20Sejnowski%2C%20Terrence%20J.%20Edges%20are%20the%E2%80%99independent%20components%E2%80%99%201997"
        },
        {
            "id": "5",
            "entry": "[5] David Berthelot, Tom Schumm, and Luke Metz. BEGAN: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "6",
            "entry": "[6] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "7",
            "entry": "[7] Wayne W Daniel and Chad Lee Cross. Biostatistics: a foundation for analysis in the health sciences. 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20Wayne%20W.%20Cross%2C%20Chad%20Lee%20Biostatistics%3A%20a%20foundation%20for%20analysis%20in%20the%20health%20sciences%201995"
        },
        {
            "id": "8",
            "entry": "[8] Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Comparison of maximum likelihood and GAN-based training of Real NVPs. arXiv preprint arXiv:1705.05263, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.05263"
        },
        {
            "id": "9",
            "entry": "[9] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20Laurent%20Sohl-Dickstein%2C%20Jascha%20Bengio%2C%20Samy%20Density%20estimation%20using%20Real%20NVP%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Sohl-Dickstein%2C%20Jascha%20Bengio%2C%20Samy%20Density%20estimation%20using%20Real%20NVP%202017"
        },
        {
            "id": "10",
            "entry": "[10] William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018"
        },
        {
            "id": "11",
            "entry": "[11] Zoubin Ghahramani, Geoffrey E Hinton, et al. The EM algorithm for mixtures of factor analyzers. Technical report, Technical Report CRG-TR-96-1, University of Toronto, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Zoubin%20Hinton%2C%20Geoffrey%20E.%20The%20EM%20algorithm%20for%20mixtures%20of%20factor%20analyzers%201996"
        },
        {
            "id": "12",
            "entry": "[12] Ian Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1701.00160"
        },
        {
            "id": "13",
            "entry": "[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "14",
            "entry": "[14] Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-GAN: Combining maximum likelihood and adversarial learning in generative models. In AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grover%2C%20Aditya%20Dhar%2C%20Manik%20Ermon%2C%20Stefano%20Flow-GAN%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grover%2C%20Aditya%20Dhar%2C%20Manik%20Ermon%2C%20Stefano%20Flow-GAN%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018"
        },
        {
            "id": "15",
            "entry": "[15] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20GANs%202017"
        },
        {
            "id": "16",
            "entry": "[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626\u20136637, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017"
        },
        {
            "id": "17",
            "entry": "[17] Xianxu Hou, Linlin Shen, Ke Sun, and Guoping Qiu. Deep feature consistent variational autoencoder. In Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on, pages 1133\u20131141. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hou%2C%20Xianxu%20Shen%2C%20Linlin%20Sun%2C%20Ke%20Qiu%2C%20Guoping%20Deep%20feature%20consistent%20variational%20autoencoder%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hou%2C%20Xianxu%20Shen%2C%20Linlin%20Sun%2C%20Ke%20Qiu%2C%20Guoping%20Deep%20feature%20consistent%20variational%20autoencoder%202017"
        },
        {
            "id": "18",
            "entry": "[18] Daniel Jiwoong Im, He Ma, Graham Taylor, and Kristin Branson. Quantitatively evaluating GANs with divergences proposed for training. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Im%2C%20Daniel%20Jiwoong%20Ma%2C%20He%20Taylor%2C%20Graham%20Branson%2C%20Kristin%20Quantitatively%20evaluating%20GANs%20with%20divergences%20proposed%20for%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Im%2C%20Daniel%20Jiwoong%20Ma%2C%20He%20Taylor%2C%20Graham%20Branson%2C%20Kristin%20Quantitatively%20evaluating%20GANs%20with%20divergences%20proposed%20for%20training%202018"
        },
        {
            "id": "19",
            "entry": "[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20Efros%2C%20Alexei%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20Efros%2C%20Alexei%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017-07"
        },
        {
            "id": "20",
            "entry": "[20] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20GANs%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "21",
            "entry": "[21] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "22",
            "entry": "[22] Martin Knott and David J Bartholomew. Latent variable models and factor analysis. Number 7. Edward Arnold, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Knott%2C%20Martin%20Bartholomew%2C%20David%20J.%20Latent%20variable%20models%20and%20factor%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Knott%2C%20Martin%20Bartholomew%2C%20David%20J.%20Latent%20variable%20models%20and%20factor%20analysis%201999"
        },
        {
            "id": "23",
            "entry": "[23] Alexander Kolesnikov and Christoph H Lampert. PixelCNN models with auxiliary variables for natural image modeling. In International Conference on Machine Learning, pages 1905\u20131914, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolesnikov%2C%20Alexander%20Lampert%2C%20Christoph%20H.%20PixelCNN%20models%20with%20auxiliary%20variables%20for%20natural%20image%20modeling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolesnikov%2C%20Alexander%20Lampert%2C%20Christoph%20H.%20PixelCNN%20models%20with%20auxiliary%20variables%20for%20natural%20image%20modeling%202017"
        },
        {
            "id": "24",
            "entry": "[24] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "25",
            "entry": "[25] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "26",
            "entry": "[26] Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMD GAN: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems 30, pages 2203\u20132213. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20MMD%20GAN%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20MMD%20GAN%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017"
        },
        {
            "id": "27",
            "entry": "[27] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "28",
            "entry": "[28] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs created equal? a large-scale study. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucic%2C%20Mario%20Kurach%2C%20Karol%20Michalski%2C%20Marcin%20Gelly%2C%20Sylvain%20Are%20GANs%20created%20equal%3F%20a%20large-scale%20study%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucic%2C%20Mario%20Kurach%2C%20Karol%20Michalski%2C%20Marcin%20Gelly%2C%20Sylvain%20Are%20GANs%20created%20equal%3F%20a%20large-scale%20study%202018"
        },
        {
            "id": "29",
            "entry": "[29] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metz%2C%20Luke%20Poole%2C%20Ben%20Pfau%2C%20David%20Sohl-Dickstein%2C%20Jascha%20Unrolled%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metz%2C%20Luke%20Poole%2C%20Ben%20Pfau%2C%20David%20Sohl-Dickstein%2C%20Jascha%20Unrolled%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "30",
            "entry": "[30] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "31",
            "entry": "[31] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In Proceedings of the 34th International Conference on Machine Learning, pages 2642\u20132651, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017"
        },
        {
            "id": "32",
            "entry": "[32] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996"
        },
        {
            "id": "33",
            "entry": "[33] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "34",
            "entry": "[34] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "35",
            "entry": "[35] Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. VEEGAN: Reducing mode collapse in GANs using implicit variational learning. In Advances in Neural Information Processing Systems, pages 3308\u20133318, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20VEEGAN%3A%20Reducing%20mode%20collapse%20in%20GANs%20using%20implicit%20variational%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20VEEGAN%3A%20Reducing%20mode%20collapse%20in%20GANs%20using%20implicit%20variational%20learning%202017"
        },
        {
            "id": "36",
            "entry": "[36] Michael E Tipping and Christopher M Bishop. Mixtures of probabilistic principal component analyzers. Neural computation, 11(2):443\u2013482, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tipping%2C%20Michael%20E.%20Bishop%2C%20Christopher%20M.%20Mixtures%20of%20probabilistic%20principal%20component%20analyzers%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tipping%2C%20Michael%20E.%20Bishop%2C%20Christopher%20M.%20Mixtures%20of%20probabilistic%20principal%20component%20analyzers%201999"
        },
        {
            "id": "37",
            "entry": "[37] Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611\u2013622, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tipping%2C%20Michael%20E.%20Bishop%2C%20Christopher%20M.%20Probabilistic%20principal%20component%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tipping%2C%20Michael%20E.%20Bishop%2C%20Christopher%20M.%20Probabilistic%20principal%20component%20analysis%201999"
        },
        {
            "id": "38",
            "entry": "[38] Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive density-estimator. In Advances in Neural Information Processing Systems, pages 2175\u20132183, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uria%2C%20Benigno%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20Rnade%3A%20The%20real-valued%20neural%20autoregressive%20density-estimator%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uria%2C%20Benigno%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20Rnade%3A%20The%20real-valued%20neural%20autoregressive%20density-estimator%202013"
        },
        {
            "id": "39",
            "entry": "[39] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, pages 4790\u20134798, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016"
        },
        {
            "id": "40",
            "entry": "[40] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International Conference on Machine Learning, pages 1747\u20131756, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oord%2C%20Aaron%20Van%20Kalchbrenner%2C%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oord%2C%20Aaron%20Van%20Kalchbrenner%2C%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "41",
            "entry": "[41] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017"
        },
        {
            "id": "42",
            "entry": "[42] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 479\u2013 486. IEEE, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoran%2C%20Daniel%20Weiss%2C%20Yair%20From%20learning%20models%20of%20natural%20image%20patches%20to%20whole%20image%20restoration%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoran%2C%20Daniel%20Weiss%2C%20Yair%20From%20learning%20models%20of%20natural%20image%20patches%20to%20whole%20image%20restoration%202011"
        }
    ]
}
