{
    "filename": "7406-revisiting-multi-task-learning-with-rock-a-deep-residual-auxiliary-block-for-visual-detection.pdf",
    "metadata": {
        "title": "Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection",
        "author": "Taylor Mordan, Nicolas THOME, Gilles Henaff, Matthieu Cord",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7406-revisiting-multi-task-learning-with-rock-a-deep-residual-auxiliary-block-for-visual-detection.pdf"
        },
        "abstract": "Multi-Task Learning (MTL) is appealing for deep learning regularization. In this paper, we tackle a specific MTL context denoted as primary MTL, where the ultimate goal is to improve the performance of a given primary task by leveraging several other auxiliary tasks. Our main methodological contribution is to introduce ROCK, a new generic multi-modal fusion block for deep learning tailored to the primary MTL context. ROCK architecture is based on a residual connection, which makes forward prediction explicitly impacted by the intermediate auxiliary representations. The auxiliary predictor\u2019s architecture is also specifically designed to our primary MTL context, by incorporating intensive pooling operators for maximizing complementarity of intermediate representations. Extensive experiments on NYUv2 dataset (object detection with scene classification, depth prediction, and surface normal estimation as auxiliary tasks) validate the relevance of the approach and its superiority to flat MTL approaches. Our method outperforms state-of-the-art object detection models on NYUv2 by a large margin, and is also able to handle large-scale heterogeneous inputs (real and synthetic images) with missing annotation modalities."
    },
    "keywords": [
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "object detection",
            "url": "https://en.wikipedia.org/wiki/object_detection"
        },
        {
            "term": "ROCK",
            "url": "https://en.wikipedia.org/wiki/ROCK"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "The outstanding success of ConvNets for image classification in the ILSVRC challenge [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] has heralded a new era for deep learning",
        "In flat Multi-Task Learning, all tasks are at the same level and are able to benefit each other through implicit common feature learning only, i.e. their mutual influence is implicit in the models",
        "In order to evaluate the importance of each auxiliary task, Table 2 presents another ablation study with results obtained when one task is dropped at a time, both for the flat Multi-Task Learning baseline and ROCK",
        "We introduced ROCK, a generic multi-modal fusion block for deep networks, to tackle the primary Multi-Task Learning context, where auxiliary tasks are leveraged during training to improve performance on a primary task",
        "We show that exploiting additional supervision with ROCK yields the same performance than having around 30% additional examples with a single-task model, encouraging to fully exploit available data in contexts where images are difficult to gather",
        "The design of ROCK has been kept fairly simple to prove the relevance of the approach"
    ],
    "key_statements": [
        "The outstanding success of ConvNets for image classification in the ILSVRC challenge [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] has heralded a new era for deep learning",
        "An appealing option to limit overfitting is to rely on Transfer Learning (TL), which aims at leveraging different objectives and datasets for improving predictive performances",
        "We introduce a new model for leveraging auxiliary supervision and improving performance on a primary task, in a primary Multi-Task Learning setup",
        "The residual formulation allows the base feature map to keep its content while focusing it more on relevant details of the images, yielding better features for the primary task. This feature merging step is key in ROCK to improve upon flat Multi-Task Learning, and these two modules are the main difference between flat and primary Multi-Task Learning",
        "In flat Multi-Task Learning, all tasks are at the same level and are able to benefit each other through implicit common feature learning only, i.e. their mutual influence is implicit in the models",
        "Instantiation of ROCK We describe how ROCK is instantiated for the three tasks utilized with NYUv2 dataset [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>]",
        "We present an ablation study of ROCK in Table 1 to identify the influence of each component",
        "In order to evaluate the importance of each auxiliary task, Table 2 presents another ablation study with results obtained when one task is dropped at a time, both for the flat Multi-Task Learning baseline and ROCK",
        "The results are shown on the left of Table 5 and are close to those of the detection baseline, indicating that the auxiliary block is only useful to learn from auxiliary tasks in an effective way",
        "Training ROCK on around 70% of the train set roughly gives similar results than the detection baseline, i.e. having the additional three auxiliary tasks to learn from compensates for the loss of 30% of examples",
        "We introduced ROCK, a generic multi-modal fusion block for deep networks, to tackle the primary Multi-Task Learning context, where auxiliary tasks are leveraged during training to improve performance on a primary task",
        "We show that exploiting additional supervision with ROCK yields the same performance than having around 30% additional examples with a single-task model, encouraging to fully exploit available data in contexts where images are difficult to gather",
        "The design of ROCK has been kept fairly simple to prove the relevance of the approach"
    ],
    "summary": [
        "The outstanding success of ConvNets for image classification in the ILSVRC challenge [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] has heralded a new era for deep learning.",
        "The idea behind ROCK is to add a new residual auxiliary block between the two existing components, in order to leverage T other, auxiliary tasks {ti}iT=1 to extract useful information and inject it into the base feature map X to yield a refined version Xof it.",
        "The fusion step merges all these task-specific features with the base one in a residual manner to yield the refined feature map X , which encodes both primary and auxiliary information: T",
        "This feature merging step is key in ROCK to improve upon flat MTL, and these two modules are the main difference between flat and primary MTLs. In flat MTL, all tasks are at the same level and are able to benefit each other through implicit common feature learning only, i.e. their mutual influence is implicit in the models.",
        "We incorporate ROCK for object detection as the primary task, using multi-modal auxiliary information: scene classification, depth prediction and surface normal estimation (Figure 1).",
        "In order to evaluate the importance of each auxiliary task, Table 2 presents another ablation study with results obtained when one task is dropped at a time, both for the flat MTL baseline and ROCK.",
        "By specifically designing the architecture to leverage this auxiliary supervision to improve the primary object detection performance, ROCK even outperforms methods using similar kinds of annotations, but at test-time too, in contrast with the privileged context of ROCK.",
        "The results are shown on the left of Table 5 and are close to those of the detection baseline, indicating that the auxiliary block is only useful to learn from auxiliary tasks in an effective way.",
        "Training ROCK on around 70% of the train set roughly gives similar results than the detection baseline, i.e. having the additional three auxiliary tasks to learn from compensates for the loss of 30% of examples.",
        "We introduced ROCK, a generic multi-modal fusion block for deep networks, to tackle the primary MTL context, where auxiliary tasks are leveraged during training to improve performance on a primary task.",
        "We show that exploiting additional supervision with ROCK yields the same performance than having around 30% additional examples with a single-task model, encouraging to fully exploit available data in contexts where images are difficult to gather.",
        "By pre-training our model on a large-scale synthetic dataset with different classes and auxiliary modalities, we set a new state of the art on NYUv2 and demonstrate ROCK is flexible and can adapt to various challenging setups.",
        "The fusion operation could be studied more thoroughly"
    ],
    "headline": "We introduce a new model for leveraging auxiliary supervision and improving performance on a primary task, in a primary Multi-Task Learning setup",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson. Factors of transferability for a generic ConvNet representation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 38(9):1790\u20131802, 2016. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azizpour%2C%20Hossein%20Razavian%2C%20Ali%20Sharif%20Sullivan%2C%20Josephine%20Maki%2C%20Atsuto%20Factors%20of%20transferability%20for%20a%20generic%20ConvNet%20representation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azizpour%2C%20Hossein%20Razavian%2C%20Ali%20Sharif%20Sullivan%2C%20Josephine%20Maki%2C%20Atsuto%20Factors%20of%20transferability%20for%20a%20generic%20ConvNet%20representation%202016"
        },
        {
            "id": "2",
            "entry": "[2] Hedi Ben-younes, R\u00e9mi Cadene, Matthieu Cord, and Nicolas Thome. MUTAN: Multimodal tucker fusion for visual question answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-younes%2C%20Hedi%20Cadene%2C%20R%C3%A9mi%20Cord%2C%20Matthieu%20Nicolas%20Thome.%20MUTAN%3A%20Multimodal%20tucker%20fusion%20for%20visual%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-younes%2C%20Hedi%20Cadene%2C%20R%C3%A9mi%20Cord%2C%20Matthieu%20Nicolas%20Thome.%20MUTAN%3A%20Multimodal%20tucker%20fusion%20for%20visual%20question%20answering%202017"
        },
        {
            "id": "3",
            "entry": "[3] Hakan Bilen and Andrea Vedaldi. Integrated perception with recurrent multi-task neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 235\u2013243, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Integrated%20perception%20with%20recurrent%20multi-task%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Integrated%20perception%20with%20recurrent%20multi-task%20neural%20networks%202016"
        },
        {
            "id": "4",
            "entry": "[4] Rich Caruana. Multitask learning. Machine Learning, 28(1):41\u201375, 1997. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20Rich%20Multitask%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20Rich%20Multitask%20learning%201997"
        },
        {
            "id": "5",
            "entry": "[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan Yuille. DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Liang-Chieh%20Papandreou%2C%20George%20Kokkinos%2C%20Iasonas%20Murphy%2C%20Kevin%20DeepLab%3A%20Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%2C%20atrous%20convolution%2C%20and%20fully%20connected%20CRFs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Liang-Chieh%20Papandreou%2C%20George%20Kokkinos%2C%20Iasonas%20Murphy%2C%20Kevin%20DeepLab%3A%20Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%2C%20atrous%20convolution%2C%20and%20fully%20connected%20CRFs%202018"
        },
        {
            "id": "6",
            "entry": "[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cordts%2C%20Marius%20Omran%2C%20Mohamed%20Ramos%2C%20Sebastian%20Rehfeld%2C%20Timo%20The%20Cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cordts%2C%20Marius%20Omran%2C%20Mohamed%20Ramos%2C%20Sebastian%20Rehfeld%2C%20Timo%20The%20Cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016"
        },
        {
            "id": "7",
            "entry": "[7] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: Object detection via region-based fully convolutional networks. In Advances in Neural Information Processing Systems (NIPS), 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Jifeng%20Li%2C%20Yi%20He%2C%20Kaiming%20Sun%2C%20Jian%20R-FCN%3A%20Object%20detection%20via%20region-based%20fully%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Jifeng%20Li%2C%20Yi%20He%2C%20Kaiming%20Sun%2C%20Jian%20R-FCN%3A%20Object%20detection%20via%20region-based%20fully%20convolutional%20networks%202016"
        },
        {
            "id": "8",
            "entry": "[8] Thanuja Dharmasiri, Andrew Spek, and Tom Drummond. Joint prediction of depths, normals and surface curvature from RGB images using CNNs. In Proceedings of the International Conference on Intelligent Robots and Systems (IROS), 2017. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dharmasiri%2C%20Thanuja%20Spek%2C%20Andrew%20Drummond%2C%20Tom%20Joint%20prediction%20of%20depths%2C%20normals%20and%20surface%20curvature%20from%20RGB%20images%20using%20CNNs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dharmasiri%2C%20Thanuja%20Spek%2C%20Andrew%20Drummond%2C%20Tom%20Joint%20prediction%20of%20depths%2C%20normals%20and%20surface%20curvature%20from%20RGB%20images%20using%20CNNs%202017"
        },
        {
            "id": "9",
            "entry": "[9] Alain Droniou and Olivier Sigaud. Gated autoencoders with tied input weights. In Proceedings of the International Conference on Machine Learning (ICML), 2013. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Droniou%2C%20Alain%20Sigaud%2C%20Olivier%20Gated%20autoencoders%20with%20tied%20input%20weights%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Droniou%2C%20Alain%20Sigaud%2C%20Olivier%20Gated%20autoencoders%20with%20tied%20input%20weights%202013"
        },
        {
            "id": "10",
            "entry": "[10] Thibaut Durand, Taylor Mordan, Nicolas Thome, and Matthieu Cord. WILDCAT: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durand%2C%20Thibaut%20Mordan%2C%20Taylor%20Thome%2C%20Nicolas%20Cord%2C%20Matthieu%20WILDCAT%3A%20Weakly%20supervised%20learning%20of%20deep%20convnets%20for%20image%20classification%2C%20pointwise%20localization%20and%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durand%2C%20Thibaut%20Mordan%2C%20Taylor%20Thome%2C%20Nicolas%20Cord%2C%20Matthieu%20WILDCAT%3A%20Weakly%20supervised%20learning%20of%20deep%20convnets%20for%20image%20classification%2C%20pointwise%20localization%20and%20segmentation%202017"
        },
        {
            "id": "11",
            "entry": "[11] Thibaut Durand, Nicolas Thome, and Matthieu Cord. MANTRA: Minimum maximum latent structural SVM for image classification and ranking. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2713\u20132721, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durand%2C%20Thibaut%20Thome%2C%20Nicolas%20Cord%2C%20Matthieu%20MANTRA%3A%20Minimum%20maximum%20latent%20structural%20SVM%20for%20image%20classification%20and%20ranking%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durand%2C%20Thibaut%20Thome%2C%20Nicolas%20Cord%2C%20Matthieu%20MANTRA%3A%20Minimum%20maximum%20latent%20structural%20SVM%20for%20image%20classification%20and%20ranking%202015"
        },
        {
            "id": "12",
            "entry": "[12] Thibaut Durand, Nicolas Thome, and Matthieu Cord. WELDON: Weakly supervised learning of deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4743\u20134752, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durand%2C%20Thibaut%20Thome%2C%20Nicolas%20Cord%2C%20Matthieu%20WELDON%3A%20Weakly%20supervised%20learning%20of%20deep%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durand%2C%20Thibaut%20Thome%2C%20Nicolas%20Cord%2C%20Matthieu%20WELDON%3A%20Weakly%20supervised%20learning%20of%20deep%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "13",
            "entry": "[13] David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2650\u20132658, 2015. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eigen%2C%20David%20Fergus%2C%20Rob%20Predicting%20depth%2C%20surface%20normals%20and%20semantic%20labels%20with%20a%20common%20multi-scale%20convolutional%20architecture%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eigen%2C%20David%20Fergus%2C%20Rob%20Predicting%20depth%2C%20surface%20normals%20and%20semantic%20labels%20with%20a%20common%20multi-scale%20convolutional%20architecture%202015"
        },
        {
            "id": "14",
            "entry": "[14] Huan Fu, Mingming Gong, Chaohui Wang, and Dacheng Tao. A compromise principle in deep monocular depth estimation. arXiv preprint arXiv:1708.08267, 2017. 2",
            "arxiv_url": "https://arxiv.org/pdf/1708.08267"
        },
        {
            "id": "15",
            "entry": "[15] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv:1606.01847, 2016. 5",
            "arxiv_url": "https://arxiv.org/pdf/1606.01847"
        },
        {
            "id": "16",
            "entry": "[16] Ross Girshick. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1440\u20131448, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%20Girshick%20Fast%20RCNN%20In%20Proceedings%20of%20the%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%20pages%2014401448%202015%202",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%20Girshick%20Fast%20RCNN%20In%20Proceedings%20of%20the%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%20pages%2014401448%202015%202"
        },
        {
            "id": "17",
            "entry": "[17] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 580\u2013587, 2014. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girshick%2C%20Ross%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Malik%2C%20Jitendra%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girshick%2C%20Ross%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20Malik%2C%20Jitendra%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation%202014"
        },
        {
            "id": "18",
            "entry": "[18] Saurabh Gupta, Pablo Arbel\u00e1ez, Ross Girshick, and Jitendra Malik. Inferring 3D object pose in RGB-D images. arXiv preprint arXiv:1502.04652, 2015. 5, 7",
            "arxiv_url": "https://arxiv.org/pdf/1502.04652"
        },
        {
            "id": "19",
            "entry": "[19] Saurabh Gupta, Ross Girshick, Pablo Arbel\u00e1ez, and Jitendra Malik. Learning rich features from RGB-D images for object detection and segmentation. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), pages 345\u2013360, 2014. 5, 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Saurabh%20Girshick%2C%20Ross%20Arbel%C3%A1ez%2C%20Pablo%20Malik%2C%20Jitendra%20Learning%20rich%20features%20from%20RGB-D%20images%20for%20object%20detection%20and%20segmentation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Saurabh%20Girshick%2C%20Ross%20Arbel%C3%A1ez%2C%20Pablo%20Malik%2C%20Jitendra%20Learning%20rich%20features%20from%20RGB-D%20images%20for%20object%20detection%20and%20segmentation%202014"
        },
        {
            "id": "20",
            "entry": "[20] Christian Hane, Lubor Ladicky, and Marc Pollefeys. Direction matters: Depth estimation with a surface normal classifier. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 381\u2013389, 2015. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hane%2C%20Christian%20Ladicky%2C%20Lubor%20Pollefeys%2C%20Marc%20Direction%20matters%3A%20Depth%20estimation%20with%20a%20surface%20normal%20classifier%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hane%2C%20Christian%20Ladicky%2C%20Lubor%20Pollefeys%2C%20Marc%20Direction%20matters%3A%20Depth%20estimation%20with%20a%20surface%20normal%20classifier%202015"
        },
        {
            "id": "21",
            "entry": "[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "22",
            "entry": "[22] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "23",
            "entry": "[23] Judy Hoffman, Saurabh Gupta, and Trevor Darrell. Learning with side information through modality hallucination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 826\u2013834, 2016. 2, 5, 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Judy%20Gupta%2C%20Saurabh%20Darrell%2C%20Trevor%20Learning%20with%20side%20information%20through%20modality%20hallucination%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Judy%20Gupta%2C%20Saurabh%20Darrell%2C%20Trevor%20Learning%20with%20side%20information%20through%20modality%20hallucination%202016"
        },
        {
            "id": "24",
            "entry": "[24] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "25",
            "entry": "[25] Iasonas Kokkinos. UberNet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kokkinos%2C%20Iasonas%20UberNet%3A%20Training%20a%20universal%20convolutional%20neural%20network%20for%20low-%2C%20mid-%2C%20and%20high-level%20vision%20using%20diverse%20datasets%20and%20limited%20memory%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kokkinos%2C%20Iasonas%20UberNet%3A%20Training%20a%20universal%20convolutional%20neural%20network%20for%20low-%2C%20mid-%2C%20and%20high-level%20vision%20using%20diverse%20datasets%20and%20limited%20memory%202017"
        },
        {
            "id": "26",
            "entry": "[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "27",
            "entry": "[27] Lubor Ladicky, Jianbo Shi, and Marc Pollefeys. Pulling things out of perspective. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 89\u201396, 2014. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ladicky%2C%20Lubor%20Shi%2C%20Jianbo%20Pollefeys%2C%20Marc%20Pulling%20things%20out%20of%20perspective%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ladicky%2C%20Lubor%20Shi%2C%20Jianbo%20Pollefeys%2C%20Marc%20Pulling%20things%20out%20of%20perspective%202014"
        },
        {
            "id": "28",
            "entry": "[28] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In Proceedings of the IEEE International Conference on 3D Vision (3DV), pages 239\u2013248, 2016. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laina%2C%20Iro%20Rupprecht%2C%20Christian%20Belagiannis%2C%20Vasileios%20Tombari%2C%20Federico%20Deeper%20depth%20prediction%20with%20fully%20convolutional%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laina%2C%20Iro%20Rupprecht%2C%20Christian%20Belagiannis%2C%20Vasileios%20Tombari%2C%20Federico%20Deeper%20depth%20prediction%20with%20fully%20convolutional%20residual%20networks%202016"
        },
        {
            "id": "29",
            "entry": "[29] Jun Li, Reinhard Klein, and Angela Yao. A two-streamed network for estimating fine-scaled depth maps from single RGB images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3372\u20133380, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Jun%20Klein%2C%20Reinhard%20Yao%2C%20Angela%20A%20two-streamed%20network%20for%20estimating%20fine-scaled%20depth%20maps%20from%20single%20RGB%20images%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Jun%20Klein%2C%20Reinhard%20Yao%2C%20Angela%20A%20two-streamed%20network%20for%20estimating%20fine-scaled%20depth%20maps%20from%20single%20RGB%20images%202017"
        },
        {
            "id": "30",
            "entry": "[30] Beyang Liu, Stephen Gould, and Daphne Koller. Single image depth estimation from predicted semantic labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1253\u20131260, 2010. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Beyang%20Gould%2C%20Stephen%20Koller%2C%20Daphne%20Single%20image%20depth%20estimation%20from%20predicted%20semantic%20labels%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Beyang%20Gould%2C%20Stephen%20Koller%2C%20Daphne%20Single%20image%20depth%20estimation%20from%20predicted%20semantic%20labels%202010"
        },
        {
            "id": "31",
            "entry": "[31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, and Scott Reed. SSD: Single shot multibox detector. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2016. 2, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Wei%20Anguelov%2C%20Dragomir%20Erhan%2C%20Dumitru%20Szegedy%2C%20Christian%20SSD%3A%20Single%20shot%20multibox%20detector%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Wei%20Anguelov%2C%20Dragomir%20Erhan%2C%20Dumitru%20Szegedy%2C%20Christian%20SSD%3A%20Single%20shot%20multibox%20detector%202016"
        },
        {
            "id": "32",
            "entry": "[32] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rog\u00e9rio Feris. Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1131\u20131140, 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fully-adaptive%20feature%20sharing%20in%20multi-task%20networks%20with%20applications%20in%20person%20attribute%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fully-adaptive%20feature%20sharing%20in%20multi-task%20networks%20with%20applications%20in%20person%20attribute%20classification%202017"
        },
        {
            "id": "33",
            "entry": "[33] Diogo Luvizon, David Picard, and Hedi Tabia. 2D/3D pose estimation and action recognition using multitask deep learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luvizon%2C%20Diogo%20Picard%2C%20David%20Tabia%2C%20Hedi%202D/3D%20pose%20estimation%20and%20action%20recognition%20using%20multitask%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luvizon%2C%20Diogo%20Picard%2C%20David%20Tabia%2C%20Hedi%202D/3D%20pose%20estimation%20and%20action%20recognition%20using%20multitask%20deep%20learning%202018"
        },
        {
            "id": "34",
            "entry": "[34] Elliot Meyerson and Risto Miikkulainen. Beyond shared hierarchies: Deep multitask learning through soft layer ordering. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meyerson%2C%20Elliot%20Miikkulainen%2C%20Risto%20Beyond%20shared%20hierarchies%3A%20Deep%20multitask%20learning%20through%20soft%20layer%20ordering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meyerson%2C%20Elliot%20Miikkulainen%2C%20Risto%20Beyond%20shared%20hierarchies%3A%20Deep%20multitask%20learning%20through%20soft%20layer%20ordering%202018"
        },
        {
            "id": "35",
            "entry": "[35] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3994\u20134003, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Ishan%20Shrivastava%2C%20Abhinav%20Gupta%2C%20Abhinav%20Hebert%2C%20Martial%20Cross-stitch%20networks%20for%20multi-task%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Ishan%20Shrivastava%2C%20Abhinav%20Gupta%2C%20Abhinav%20Hebert%2C%20Martial%20Cross-stitch%20networks%20for%20multi-task%20learning%202016"
        },
        {
            "id": "36",
            "entry": "[36] Taylor Mordan, Nicolas Thome, Gilles Henaff, and Matthieu Cord. End-to-end learning of latent deformable part-based representations for object detection. International Journal of Computer Vision (IJCV), pages 1\u201321, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mordan%2C%20Taylor%20Thome%2C%20Nicolas%20Henaff%2C%20Gilles%20Cord%2C%20Matthieu%20End-to-end%20learning%20of%20latent%20deformable%20part-based%20representations%20for%20object%20detection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mordan%2C%20Taylor%20Thome%2C%20Nicolas%20Henaff%2C%20Gilles%20Cord%2C%20Matthieu%20End-to-end%20learning%20of%20latent%20deformable%20part-based%20representations%20for%20object%20detection%202018"
        },
        {
            "id": "37",
            "entry": "[37] Seong-Jin Park, Ki-Sang Hong, and Seungyong Lee. RDFNet: RGB-D multi-level residual feature fusion for indoor semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Park%2C%20Seong-Jin%20Hong%2C%20Ki-Sang%20Lee%2C%20Seungyong%20RDFNet%3A%20RGB-D%20multi-level%20residual%20feature%20fusion%20for%20indoor%20semantic%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Park%2C%20Seong-Jin%20Hong%2C%20Ki-Sang%20Lee%2C%20Seungyong%20RDFNet%3A%20RGB-D%20multi-level%20residual%20feature%20fusion%20for%20indoor%20semantic%20segmentation%202017"
        },
        {
            "id": "38",
            "entry": "[38] Dmitry Pechyony and Vladimir Vapnik. On the theory of learnining with privileged information. In Advances in Neural Information Processing Systems (NIPS), pages 1894\u20131902, 2010. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pechyony%2C%20Dmitry%20Vapnik%2C%20Vladimir%20On%20the%20theory%20of%20learnining%20with%20privileged%20information%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pechyony%2C%20Dmitry%20Vapnik%2C%20Vladimir%20On%20the%20theory%20of%20learnining%20with%20privileged%20information%202010"
        },
        {
            "id": "39",
            "entry": "[39] Zhongzheng Ren and Yong Jae Lee. Cross-domain self-supervised multi-task feature learning using synthetic imagery. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Zhongzheng%20Lee%2C%20Yong%20Jae%20Cross-domain%20self-supervised%20multi-task%20feature%20learning%20using%20synthetic%20imagery%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Zhongzheng%20Lee%2C%20Yong%20Jae%20Cross-domain%20self-supervised%20multi-task%20feature%20learning%20using%20synthetic%20imagery%202018"
        },
        {
            "id": "40",
            "entry": "[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. 1, 5, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "41",
            "entry": "[41] Viktoriia Sharmanska, Novi Quadrianto, and Christoph Lampert. Learning to rank using privileged information. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharmanska%2C%20Viktoriia%20Quadrianto%2C%20Novi%20Lampert%2C%20Christoph%20Learning%20to%20rank%20using%20privileged%20information%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharmanska%2C%20Viktoriia%20Quadrianto%2C%20Novi%20Lampert%2C%20Christoph%20Learning%20to%20rank%20using%20privileged%20information%202013"
        },
        {
            "id": "42",
            "entry": "[42] Viktoriia Sharmanska, Novi Quadrianto, and Christoph Lampert. Learning to transfer privileged information. In arXiv:1410.0389, 2014. 2",
            "arxiv_url": "https://arxiv.org/pdf/1410.0389"
        },
        {
            "id": "43",
            "entry": "[43] Zhiyuan Shi and Tae-Kyun Kim. Learning and refining of privileged information-based RNNs for action recognition from depth sequences. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Zhiyuan%20Kim%2C%20Tae-Kyun%20Learning%20and%20refining%20of%20privileged%20information-based%20RNNs%20for%20action%20recognition%20from%20depth%20sequences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Zhiyuan%20Kim%2C%20Tae-Kyun%20Learning%20and%20refining%20of%20privileged%20information-based%20RNNs%20for%20action%20recognition%20from%20depth%20sequences%202017"
        },
        {
            "id": "44",
            "entry": "[44] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from RGBD images. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2012. 3, 5, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silberman%2C%20Nathan%20Hoiem%2C%20Derek%20Kohli%2C%20Pushmeet%20Fergus%2C%20Rob%20Indoor%20segmentation%20and%20support%20inference%20from%20RGBD%20images%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silberman%2C%20Nathan%20Hoiem%2C%20Derek%20Kohli%2C%20Pushmeet%20Fergus%2C%20Rob%20Indoor%20segmentation%20and%20support%20inference%20from%20RGBD%20images%202012"
        },
        {
            "id": "45",
            "entry": "[45] Luciano Spinello and Kai Arras. Leveraging RGB-D data: Adaptive fusion and domain adaptation for object detection. In Proceedings of the IEEE Conference on Robotics and Automation (ICRA), pages 4469\u20134474, 2012. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spinello%2C%20Luciano%20Arras%2C%20Kai%20Leveraging%20RGB-D%20data%3A%20Adaptive%20fusion%20and%20domain%20adaptation%20for%20object%20detection%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spinello%2C%20Luciano%20Arras%2C%20Kai%20Leveraging%20RGB-D%20data%3A%20Adaptive%20fusion%20and%20domain%20adaptation%20for%20object%20detection%202012"
        },
        {
            "id": "46",
            "entry": "[46] Vladimir Vapnik and Rauf Izmailov. Learning using privileged information: Similarity control and knowledge transfer. Journal of Machine Learning Research (JMLR), 16:2023\u20132049, 2015. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20Izmailov%2C%20Rauf%20Learning%20using%20privileged%20information%3A%20Similarity%20control%20and%20knowledge%20transfer%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vapnik%2C%20Vladimir%20Izmailov%2C%20Rauf%20Learning%20using%20privileged%20information%3A%20Similarity%20control%20and%20knowledge%20transfer%202015"
        },
        {
            "id": "47",
            "entry": "[47] Vladimir Vapnik and Akshay Vashist. A new learning paradigm: Learning using privileged information. Neural Networks, 22(5-6):544\u2013557, 2009. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20Vashist%2C%20Akshay%20A%20new%20learning%20paradigm%3A%20Learning%20using%20privileged%20information%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vapnik%2C%20Vladimir%20Vashist%2C%20Akshay%20A%20new%20learning%20paradigm%3A%20Learning%20using%20privileged%20information%202009"
        },
        {
            "id": "48",
            "entry": "[48] Chu Wang and Kaleem Siddiqi. Differential geometry boosts convolutional neural networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 51\u201358, 2016. 5, 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Chu%20Siddiqi%2C%20Kaleem%20Differential%20geometry%20boosts%20convolutional%20neural%20networks%20for%20object%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Chu%20Siddiqi%2C%20Kaleem%20Differential%20geometry%20boosts%20convolutional%20neural%20networks%20for%20object%20detection%202016"
        },
        {
            "id": "49",
            "entry": "[49] Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, and Alan L Yuille. Towards unified depth and semantic prediction from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2800\u20132809, 2015. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Peng%20Shen%2C%20Xiaohui%20Lin%2C%20Zhe%20Cohen%2C%20Scott%20Towards%20unified%20depth%20and%20semantic%20prediction%20from%20a%20single%20image%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Peng%20Shen%2C%20Xiaohui%20Lin%2C%20Zhe%20Cohen%2C%20Scott%20Towards%20unified%20depth%20and%20semantic%20prediction%20from%20a%20single%20image%202015"
        },
        {
            "id": "50",
            "entry": "[50] Yongxin Yang and Timothy Hospedales. Deep multi-task representation learning: A tensor factorisation approach. In Proceedings of the International Conference on Learning Representations (ICLR), 2017. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yongxin%20Hospedales%2C%20Timothy%20Deep%20multi-task%20representation%20learning%3A%20A%20tensor%20factorisation%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yongxin%20Hospedales%2C%20Timothy%20Deep%20multi-task%20representation%20learning%3A%20A%20tensor%20factorisation%20approach%202017"
        },
        {
            "id": "51",
            "entry": "[51] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, and Thomas Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 7 13 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yinda%20Song%2C%20Shuran%20Yumer%2C%20Ersin%20Savva%2C%20Manolis%20Physically-based%20rendering%20for%20indoor%20scene%20understanding%20using%20convolutional%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yinda%20Song%2C%20Shuran%20Yumer%2C%20Ersin%20Savva%2C%20Manolis%20Physically-based%20rendering%20for%20indoor%20scene%20understanding%20using%20convolutional%20neural%20networks%202017"
        }
    ]
}
