{
    "filename": "7460-random-feature-stein-discrepancies.pdf",
    "metadata": {
        "date": 2018,
        "title": "Random Feature Stein Discrepancies",
        "author": "Jonathan H. Huggins\u21e4 Department of Biostatistics, Harvard jhuggins@mit.edu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7460-random-feature-stein-discrepancies.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Computable Stein discrepancies have been deployed for a variety of applications, ranging from sampler selection in posterior inference to approximate Bayesian inference to goodness-of-fit testing. Existing convergence-determining Stein discrepancies admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time Stein discrepancies have been proposed for goodness-of-fit testing, they exhibit avoidable degradations in testing power\u2014even when power is explicitly optimized. To address these shortcomings, we introduce feature Stein discrepancies ( SDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct SDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations\u2014random SDs (R SDs)\u2014which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, R SDs perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute."
    },
    "keywords": [
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "quality measure",
            "url": "https://en.wikipedia.org/wiki/quality_measure"
        },
        {
            "term": "Gaussian mixture model",
            "url": "https://en.wikipedia.org/wiki/Gaussian_mixture_model"
        },
        {
            "term": "Markov chain Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"
        },
        {
            "term": "fit testing",
            "url": "https://en.wikipedia.org/wiki/fit_testing"
        },
        {
            "term": "importance sampling",
            "url": "https://en.wikipedia.org/wiki/importance_sampling"
        },
        {
            "term": "linear time",
            "url": "https://en.wikipedia.org/wiki/linear_time"
        }
    ],
    "highlights": [
        "Motivated by the intractable integration problems arising from Bayesian inference and maximum likelihood estimation [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], Gorham and Mackey [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] introduced the notion of a Stein discrepancy as a quality measure that can potentially be computed even when direct integration under the distribution of interest is unavailable",
        "We introduce a fast importance sampling-based approximation we call random SDs (R SDs)",
        "We provide conditions under which, with an appropriate choice of proposal distribution, an random feature Stein discrepancies is close in relative error to the SD with high probability",
        "We considered the random feature Stein discrepancies described in Examples 3.3 and 3.4: the tilted sech kernel using r",
        "We target the bimodal Gaussian mixture model (GMM) posterior of Welling and Teh [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and compare the step size selection made by the two random feature Stein discrepancies to that of inverse multiquadric kernel Stein discrepancy [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] when N = 1000",
        "We explored the power of random feature Stein discrepancies-based tests compared to finite-set Stein discrepancy [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], random Fourier feature [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] (Gaussian and Cauchy kernels with bandwidth = mded2), and kernel Stein discrepancy-based tests [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] (Gaussian kernel with bandwidth"
    ],
    "key_statements": [
        "Motivated by the intractable integration problems arising from Bayesian inference and maximum likelihood estimation [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], Gorham and Mackey [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] introduced the notion of a Stein discrepancy as a quality measure that can potentially be computed even when direct integration under the distribution of interest is unavailable",
        "We introduce a fast importance sampling-based approximation we call random SDs (R SDs)",
        "We provide conditions under which, with an appropriate choice of proposal distribution, an random feature Stein discrepancies is close in relative error to the SD with high probability",
        "We can recommend the following fairly simple default procedure for choosing an random feature Stein discrepancies based on a reference kernel Stein discrepancy admitting the form in Assumption A. (1) Choose any > 0, and set \u21b5 = /3, \u0304 = 1 \u21b5/2, and \u21e0 = 4\u21b5/(2 + \u21b5)",
        "We investigate the importance-sample and computational efficiency of our proposed random feature Stein discrepancies and evaluate their benefits in Markov chain Monte Carlo hyperparameter selection and goodness-of-fit testing.4",
        "We considered the random feature Stein discrepancies described in Examples 3.3 and 3.4: the tilted sech kernel using r",
        "We target the bimodal Gaussian mixture model (GMM) posterior of Welling and Teh [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and compare the step size selection made by the two random feature Stein discrepancies to that of inverse multiquadric kernel Stein discrepancy [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] when N = 1000",
        "Goodness-of-fit testing we investigated the performance of random feature Stein discrepancies for goodness-of-fit testing",
        "We explored the power of random feature Stein discrepancies-based tests compared to finite-set Stein discrepancy [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], random Fourier feature [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] (Gaussian and Cauchy kernels with bandwidth = mded2), and kernel Stein discrepancy-based tests [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] (Gaussian kernel with bandwidth",
        "We have introduced feature Stein discrepancies, a family of computable Stein discrepancies that can be cheaply approximated using importance sampling"
    ],
    "summary": [
        "Motivated by the intractable integration problems arising from Bayesian inference and maximum likelihood estimation [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], Gorham and Mackey [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] introduced the notion of a Stein discrepancy as a quality measure that can potentially be computed even when direct integration under the distribution of interest is unavailable.",
        "Jitkrittum et al [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] introduced a special form of KSD termed the finite-set Stein discrepancy (FSSD) to test goodness-of-fit in linear time.",
        "P (h)|, measuring the maximum discrepancy between target and sample expectations over a class of test functions.",
        "We will approximate the SD in expression (1) using an importance sample of size M drawn from a proposal distribution with (Lebesgue) density \u232b.",
        "Gorham and Mackey [11, Thm. 8] proved that when 2 ( 1, 0), KSDs with an IMQ base kernel determine weak convergence on RD whenever P 2 P, the set of distantly dissipative distributions for which r log p is Lipschitz.2",
        "We will consider two R SDs that determine convergence by Propositions 3.1 and 3.3 and that yield (C, ) second moments for any 2 (0, 1] using Theorem 3.8.",
        "In the future it would be interesting to construct other R SDs. We can recommend the following fairly simple default procedure for choosing an R SD based on a reference KSD admitting the form in Assumption A.",
        "We investigate the importance-sample and computational efficiency of our proposed R SDs and evaluate their benefits in MCMC hyperparameter selection and goodness-of-fit testing.4 In our experiments, we considered the R SDs described in Examples 3.3 and 3.4: the tilted sech kernel using r",
        "Finding hyperparameter settings for the L1 IMQ that were stable across dimension and appropriately controlled the size for goodness-of-fit testing required some care.",
        "We target the bimodal Gaussian mixture model (GMM) posterior of Welling and Teh [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and compare the step size selection made by the two R SDs to that of IMQ KSD [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] when N = 1000.",
        "Random feature Stein discrepancies (R SDs), combine the computational benefits of linear-time discrepancy measures with the convergence-determining properties of quadratic-time Stein discrepancies.",
        "We validated the benefits of R SDs on two applications where kernel Stein discrepancies have shown excellent performance: measuring sample quality and goodness-of-fit testing.",
        "The L1 IMQ R SD performed particularly well: it outperformed existing linear-time KSD approximations in high dimensions and performed as well or better than the state-of-the-art quadratic-time KSDs. R SDs could be used as drop-in replacements for KSDs in applications to Monte Carlo variance reduction with control functionals [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], probabilistic inference using Stein variational gradient descent [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and kernel quadrature [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]."
    ],
    "headline": "We introduce feature Stein discrepancies , a new family of quality measures that can be cheaply approximated using importance sampling",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abramowitz and I. Stegun, editors. Handbook of Mathematical Functions. Dover Publications, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Handbook%20of%20Mathematical%20Functions%201964"
        },
        {
            "id": "2",
            "entry": "[2] F. Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. Journal of Machine Learning Research, 18:1\u201338, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20On%20the%20Equivalence%20between%20Kernel%20Quadrature%20Rules%20and%20Random%20Feature%20Expansions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20On%20the%20Equivalence%20between%20Kernel%20Quadrature%20Rules%20and%20Random%20Feature%20Expansions%202017"
        },
        {
            "id": "3",
            "entry": "[3] F.-X. Briol, C. J. Oates, J. Cockayne, W. Y. Chen, and M. A. Girolami. On the Sampling Problem for Kernel Quadrature. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Briol%2C%20F.-X.%20Oates%2C%20C.J.%20Cockayne%2C%20J.%20Chen%2C%20W.Y.%20On%20the%20Sampling%20Problem%20for%20Kernel%20Quadrature%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Briol%2C%20F.-X.%20Oates%2C%20C.J.%20Cockayne%2C%20J.%20Chen%2C%20W.Y.%20On%20the%20Sampling%20Problem%20for%20Kernel%20Quadrature%202017"
        },
        {
            "id": "4",
            "entry": "[4] C. Carmeli, E. De Vito, A. Toigo, and V. Umanita. Vector valued reproducing kernel hilbert spaces and universality. Analysis and Applications, 8(01):19\u201361, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmeli%2C%20C.%20Vito%2C%20E.De%20Toigo%2C%20A.%20Umanita%2C%20V.%20Vector%20valued%20reproducing%20kernel%20hilbert%20spaces%20and%20universality%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmeli%2C%20C.%20Vito%2C%20E.De%20Toigo%2C%20A.%20Umanita%2C%20V.%20Vector%20valued%20reproducing%20kernel%20hilbert%20spaces%20and%20universality%202010"
        },
        {
            "id": "5",
            "entry": "[5] F. Chung and L. Lu. Complex Graphs and Networks, volume 107. American Mathematical Society, Providence, Rhode Island, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20F.%20Lu%2C%20L.%20Complex%20Graphs%20and%20Networks%2C%20volume%20107%202006"
        },
        {
            "id": "6",
            "entry": "[6] K. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast Two-Sample Testing with Analytic Representations of Probability Measures. In Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chwialkowski%2C%20K.%20Ramdas%2C%20A.%20Sejdinovic%2C%20D.%20Gretton%2C%20A.%20Fast%20Two-Sample%20Testing%20with%20Analytic%20Representations%20of%20Probability%20Measures%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chwialkowski%2C%20K.%20Ramdas%2C%20A.%20Sejdinovic%2C%20D.%20Gretton%2C%20A.%20Fast%20Two-Sample%20Testing%20with%20Analytic%20Representations%20of%20Probability%20Measures%202015"
        },
        {
            "id": "7",
            "entry": "[7] K. Chwialkowski, H. Strathmann, and A. Gretton. A Kernel Test of Goodness of Fit. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chwialkowski%2C%20K.%20Strathmann%2C%20H.%20Gretton%2C%20A.%20A%20Kernel%20Test%20of%20Goodness%20of%20Fit%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chwialkowski%2C%20K.%20Strathmann%2C%20H.%20Gretton%2C%20A.%20A%20Kernel%20Test%20of%20Goodness%20of%20Fit%202016"
        },
        {
            "id": "8",
            "entry": "[8] A. Eberle. Reflection couplings and contraction rates for diffusions. Probability Theory and Related Fields, 166(3-4):851\u2013886, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eberle%2C%20A.%20Reflection%20couplings%20and%20contraction%20rates%20for%20diffusions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eberle%2C%20A.%20Reflection%20couplings%20and%20contraction%20rates%20for%20diffusions%202016"
        },
        {
            "id": "9",
            "entry": "[9] C. J. Geyer. Markov Chain Monte Carlo Maximum Likelihood. In Computing Science and Statistics, Proceedings of the 23rd Symposium on the Interface, pages 156\u2013163, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20J%20Geyer%20Markov%20Chain%20Monte%20Carlo%20Maximum%20Likelihood%20In%20Computing%20Science%20and%20Statistics%20Proceedings%20of%20the%2023rd%20Symposium%20on%20the%20Interface%20pages%20156163%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20J%20Geyer%20Markov%20Chain%20Monte%20Carlo%20Maximum%20Likelihood%20In%20Computing%20Science%20and%20Statistics%20Proceedings%20of%20the%2023rd%20Symposium%20on%20the%20Interface%20pages%20156163%201991"
        },
        {
            "id": "10",
            "entry": "[10] J. Gorham and L. Mackey. Measuring Sample Quality with Stein\u2019s Method. In Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gorham%2C%20J.%20Mackey%2C%20L.%20Measuring%20Sample%20Quality%20with%20Stein%E2%80%99s%20Method%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gorham%2C%20J.%20Mackey%2C%20L.%20Measuring%20Sample%20Quality%20with%20Stein%E2%80%99s%20Method%202015"
        },
        {
            "id": "11",
            "entry": "[11] J. Gorham and L. Mackey. Measuring Sample Quality with Kernels. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gorham%2C%20J.%20Mackey%2C%20L.%20Measuring%20Sample%20Quality%20with%20Kernels%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gorham%2C%20J.%20Mackey%2C%20L.%20Measuring%20Sample%20Quality%20with%20Kernels%202017"
        },
        {
            "id": "12",
            "entry": "[12] J. Gorham, A. B. Duncan, S. J. Vollmer, and L. Mackey. Measuring Sample Quality with Diffusions. arXiv.org, Nov. 2016, 1611.06972v3.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06972v3"
        },
        {
            "id": "13",
            "entry": "[13] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, and A. J. Smola. A Kernel Two-Sample Test. Journal of Machine Learning Research, 13:723\u2013773, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Scholkopf%2C%20B.%20A%20Kernel%20Two-Sample%20Test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Scholkopf%2C%20B.%20A%20Kernel%20Two-Sample%20Test%202012"
        },
        {
            "id": "14",
            "entry": "[14] R. Herb and P. Sally Jr. The Plancherel formula, the Plancherel theorem, and the Fourier transform of orbital integrals. In Representation Theory and Mathematical Physics: Conference in Honor of Gregg Zuckerman\u2019s 60th Birthday, October 24\u201327, 2009, Yale University, volume 557, page 1. American Mathematical Soc., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herb%2C%20R.%20Sally%2C%20Jr%2C%20P.%20The%20Plancherel%20formula%2C%20the%20Plancherel%20theorem%2C%20and%20the%20Fourier%20transform%20of%20orbital%20integrals.%20In%20Representation%20Theory%20and%20Mathematical%20Physics%3A%20Conference%20in%20Honor%20of%20Gregg%20Zuckerman%E2%80%99s%2060th%20Birthday%202009-10-24",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herb%2C%20R.%20Sally%2C%20Jr%2C%20P.%20The%20Plancherel%20formula%2C%20the%20Plancherel%20theorem%2C%20and%20the%20Fourier%20transform%20of%20orbital%20integrals.%20In%20Representation%20Theory%20and%20Mathematical%20Physics%3A%20Conference%20in%20Honor%20of%20Gregg%20Zuckerman%E2%80%99s%2060th%20Birthday%202009-10-24"
        },
        {
            "id": "15",
            "entry": "[15] J. Honorio and Y.-J. Li. The Error Probability of Random Fourier Features is Dimensionality Independent. arXiv.org, Oct. 2017, 1710.09953v1.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09953v1"
        },
        {
            "id": "16",
            "entry": "[16] W. Jitkrittum, W. Xu, Z. Szabo, K. Fukumizu, and A. Gretton. A Linear-Time Kernel Goodnessof-Fit Test. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jitkrittum%2C%20W.%20Xu%2C%20W.%20Szabo%2C%20Z.%20Fukumizu%2C%20K.%20A%20Linear-Time%20Kernel%20Goodnessof-Fit%20Test%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jitkrittum%2C%20W.%20Xu%2C%20W.%20Szabo%2C%20Z.%20Fukumizu%2C%20K.%20A%20Linear-Time%20Kernel%20Goodnessof-Fit%20Test%202017"
        },
        {
            "id": "17",
            "entry": "[17] Q. Liu and J. D. Lee. Black-box Importance Sampling. In International Conference on Artificial Intelligence and Statistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Q.%20Lee%2C%20J.D.%20Black-box%20Importance%20Sampling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Q.%20Lee%2C%20J.D.%20Black-box%20Importance%20Sampling%202017"
        },
        {
            "id": "18",
            "entry": "[18] Q. Liu and D. Wang. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Q.%20Wang%2C%20D.%20Stein%20Variational%20Gradient%20Descent%3A%20A%20General%20Purpose%20Bayesian%20Inference%20Algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Q.%20Wang%2C%20D.%20Stein%20Variational%20Gradient%20Descent%3A%20A%20General%20Purpose%20Bayesian%20Inference%20Algorithm%202016"
        },
        {
            "id": "19",
            "entry": "[19] Q. Liu, J. D. Lee, and M. I. Jordan. A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Q.%20Lee%2C%20J.D.%20Jordan%2C%20M.I.%20A%20Kernelized%20Stein%20Discrepancy%20for%20Goodness-of-fit%20Tests%20and%20Model%20Evaluation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Q.%20Lee%2C%20J.D.%20Jordan%2C%20M.I.%20A%20Kernelized%20Stein%20Discrepancy%20for%20Goodness-of-fit%20Tests%20and%20Model%20Evaluation%202016"
        },
        {
            "id": "20",
            "entry": "[20] A. Muller. Integral probability metrics and their generating classes of functions. Ann. Appl. Probab., 29(2):pp. 429\u2013443, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muller%2C%20A.%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muller%2C%20A.%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997"
        },
        {
            "id": "21",
            "entry": "[21] C. J. Oates, M. Girolami, and N. Chopin. Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):695\u2013718, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oates%2C%20C.J.%20Girolami%2C%20M.%20Chopin%2C%20N.%20Control%20functionals%20for%20Monte%20Carlo%20integration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oates%2C%20C.J.%20Girolami%2C%20M.%20Chopin%2C%20N.%20Control%20functionals%20for%20Monte%20Carlo%20integration%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "23",
            "entry": "[23] D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-based and RKHS-based statistics in hypothesis testing. The Annals of Statistics, 41(5):2263\u20132291, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sejdinovic%2C%20D.%20Sriperumbudur%2C%20B.%20Gretton%2C%20A.%20Fukumizu%2C%20K.%20Equivalence%20of%20distance-based%20and%20RKHS-based%20statistics%20in%20hypothesis%20testing%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sejdinovic%2C%20D.%20Sriperumbudur%2C%20B.%20Gretton%2C%20A.%20Fukumizu%2C%20K.%20Equivalence%20of%20distance-based%20and%20RKHS-based%20statistics%20in%20hypothesis%20testing%202013"
        },
        {
            "id": "24",
            "entry": "[24] R. J. Serfling. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, New York, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serfling%2C%20R.J.%20Approximation%20Theorems%20of%20Mathematical%20Statistics%201980"
        },
        {
            "id": "25",
            "entry": "[25] B. K. Sriperumbudur and Z. Szabo. Optimal rates for random Fourier features. In Advances in Neural Information Processing Systems, pages 1144\u20131152, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sriperumbudur%2C%20B.K.%20Szabo%2C%20Z.%20Optimal%20rates%20for%20random%20Fourier%20features%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sriperumbudur%2C%20B.K.%20Szabo%2C%20Z.%20Optimal%20rates%20for%20random%20Fourier%20features%202015"
        },
        {
            "id": "26",
            "entry": "[26] D. J. Sutherland and J. Schneider. On the error of random Fourier features. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pages 862\u2013871, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutherland%2C%20D.J.%20Schneider%2C%20J.%20On%20the%20error%20of%20random%20Fourier%20features%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutherland%2C%20D.J.%20Schneider%2C%20J.%20On%20the%20error%20of%20random%20Fourier%20features%202015"
        },
        {
            "id": "27",
            "entry": "[27] D. Wang and Q. Liu. Learning to Draw Samples - With Application to Amortized MLE for Generative Adversarial Learning. arXiv, stat.ML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20D.%20Liu%2C%20Q.%20Learning%20to%20Draw%20Samples%20-%20With%20Application%20to%20Amortized%20MLE%20for%20Generative%20Adversarial%20Learning.%20arXiv%2C%20stat%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20D.%20Liu%2C%20Q.%20Learning%20to%20Draw%20Samples%20-%20With%20Application%20to%20Amortized%20MLE%20for%20Generative%20Adversarial%20Learning.%20arXiv%2C%20stat%202016"
        },
        {
            "id": "28",
            "entry": "[28] M. Welling and Y. W. Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In International Conference on Machine Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20Learning%20via%20Stochastic%20Gradient%20Langevin%20Dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20Learning%20via%20Stochastic%20Gradient%20Langevin%20Dynamics%202011"
        },
        {
            "id": "29",
            "entry": "[29] H. Wendland. Scattered Data Approximation. Cambridge University Press, New York, NY, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wendland%2C%20H.%20Scattered%20Data%20Approximation%202005"
        },
        {
            "id": "30",
            "entry": "[30] J. Zhao and D. Meng. FastMMD: Ensemble of circular discrepancy for efficient two-sample test. Neural computation, 27(6):1345\u20131372, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20J.%20Meng%2C%20D.%20FastMMD%3A%20Ensemble%20of%20circular%20discrepancy%20for%20efficient%20two-sample%20test%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20J.%20Meng%2C%20D.%20FastMMD%3A%20Ensemble%20of%20circular%20discrepancy%20for%20efficient%20two-sample%20test%202015"
        }
    ]
}
