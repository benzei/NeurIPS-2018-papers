{
    "filename": "7851-recurrent-transformer-networks-for-semantic-correspondence.pdf",
    "metadata": {
        "title": "Recurrent Transformer Networks for Semantic Correspondence",
        "author": "Seungryong Kim, Stephen Lin, SANG RYUL JEON, Dongbo Min, Kwanghoon Sohn",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7851-recurrent-transformer-networks-for-semantic-correspondence.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence."
    },
    "keywords": [
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "receptive field",
            "url": "https://en.wikipedia.org/wiki/receptive_field"
        }
    ],
    "highlights": [
        "Let us denote semantically similar source and target images as Is and It, respectively",
        "TSS Benchmark We evaluated recurrent transformer networks on the TSS benchmark [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], which consists of 400 image pairs divided into three groups: FG3DCar [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], JODS [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], and PASCAL [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "As in [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], flow accuracy was measured by computing the proportion of foreground pixels with an absolute flow endpoint error that is smaller than a threshold of T = 5, after resizing each image so that its larger dimension is 100 pixels",
        "In comparison to the geometric matching-based methods GMat. [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] and GMat. w/Inl. [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], recurrent transformer networks consisting of feature extraction with ResNet and recurrent\n1For these experiments, we utilized the hierarchical dual-layer belief propagation of SIFT flow [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] together with alternative dense descriptors",
        "We presented recurrent transformer networks, which learn to infer locally-varying geometric fields for semantic correspondence in an end-to-end and weakly-supervised fashion",
        "A technique is presented for weakly-supervised training of recurrent transformer networks"
    ],
    "key_statements": [
        "Let us denote semantically similar source and target images as Is and It, respectively",
        "We present recurrent transformer networks (RTNs) for overcoming the aforementioned limitations of current semantic correspondence techniques",
        "They often fail to handle complicated deformations caused by scale, rotation, or skew that may exist among object instances",
        "We present an effective and efficient integration of the two existing approaches for geometric invariance, i.e., spatial transformer networks-based feature extraction networks [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and geometric matching networks [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>], that includes a novel weakly-supervised loss function tailored to our objective",
        "We comprehensively evaluated our recurrent transformer networks through comparisons to state-of-the-art methods for semantic correspondence, including SF [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], deformable spatial pyramids [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], Zhou et al [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>], Taniai et al [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], (b)",
        "TSS Benchmark We evaluated recurrent transformer networks on the TSS benchmark [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], which consists of 400 image pairs divided into three groups: FG3DCar [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], JODS [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], and PASCAL [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "As in [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], flow accuracy was measured by computing the proportion of foreground pixels with an absolute flow endpoint error that is smaller than a threshold of T = 5, after resizing each image so that its larger dimension is 100 pixels",
        "Methods that use spatial transformer networks-based feature transformations, namely UCN-spatial transformer [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] and CAT-FCSS [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], show improved ability to deal with geometric variations",
        "In comparison to the geometric matching-based methods GMat. [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] and GMat. w/Inl. [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], recurrent transformer networks consisting of feature extraction with ResNet and recurrent\n1For these experiments, we utilized the hierarchical dual-layer belief propagation of SIFT flow [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] together with alternative dense descriptors",
        "We presented recurrent transformer networks, which learn to infer locally-varying geometric fields for semantic correspondence in an end-to-end and weakly-supervised fashion",
        "A technique is presented for weakly-supervised training of recurrent transformer networks"
    ],
    "summary": [
        "Let us denote semantically similar source and target images as Is and It, respectively.",
        "The STN-based methods infer geometric deformation fields within a deep network and transform the convolutional activations to provide geometric-invariant features [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>].",
        "A major limitation of these learning-based methods is the lack of ground-truth geometric fields Ti\u2217 between source and target images.",
        "The objective of our networks is to learn and infer locally-varying affine deformation fields Ti in an end-to-end and weakly-supervised fashion using only image pairs without ground-truth transformations Ti\u2217.",
        "We present an effective and efficient integration of the two existing approaches for geometric invariance, i.e., STN-based feature extraction networks [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and geometric matching networks [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>], that includes a novel weakly-supervised loss function tailored to our objective.",
        "Our networks are split into two parts, as shown in Fig. 3: a feature extraction network to extract source Dis and target Dt(Ti) features, and a geometric matching network to infer the geometric fields Ti. To learn these networks in a weakly-supervised manner, we formulate a novel classification loss defined without ground-truth T\u2217i based on the assumption that the transformation which maximizes the similarity of the source features Dis and transformed target features Dt(Ti) at a pixel i should be correct, while the matching scores of other transformation candidates should be minimized.",
        "Extracting each feature by transforming local receptive fields within the target image It according to Ti for each pixel i and passing it through the networks would be time-consuming when iterating the networks.",
        "To learn the networks using only weak supervision in the form of image pairs, we formulate the loss function based on the intuition that the matching score between the source feature Dis at each pixel i and the target feature Dt(Ti) should be maximized while keeping the scores of other transformation candidates low.",
        "Average flow accuracy the matching accuracy for different numbers of iterations, with various window sizes of Ni, for different back-0.7 bone feature extraction networks such as VGGNet [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>], CAT-FCSS [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], and ResNet [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], and with pretrained or 0.6 learned backbone networks.",
        "Methods that use STN-based feature transformations, namely UCN-ST [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] and CAT-FCSS [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], show improved ability to deal with geometric variations.",
        "Similar to the experiments on the PF-WILLOW benchmark [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], CNN-based methods [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] including our RTNs yield better performance, with RTNs providing the highest matching accuracy.",
        "We presented RTNs, which learn to infer locally-varying geometric fields for semantic correspondence in an end-to-end and weakly-supervised fashion.",
        "A direction for further study is to examine how the semantic correspondence of RTNs could benefit single-image 3D reconstruction and instance-level object detection and segmentation"
    ],
    "headline": "We present recurrent transformer networks  for obtaining dense correspondences between semantically similar images",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. Barnes, E. Shechtman, D. B. Goldman, and A. Finkelstein. The generalized patchmatch correspondence algorithm. In: ECCV, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barnes%2C%20C.%20Shechtman%2C%20E.%20Goldman%2C%20D.B.%20Finkelstein%2C%20A.%20The%20generalized%20patchmatch%20correspondence%20algorithm%202010"
        },
        {
            "id": "2",
            "entry": "[2] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations,. In: ICCV, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bourdev%2C%20L.%20Malik%2C%20J.%20Poselets%3A%20Body%20part%20detectors%20trained%20using%203d%20human%20pose%20annotations%202009"
        },
        {
            "id": "3",
            "entry": "[3] H. Bristow, J. Valmadre, and S. Lucey. Dense semantic correspondence where every pixel is a classifier. In: ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bristow%2C%20H.%20Valmadre%2C%20J.%20Lucey%2C%20S.%20Dense%20semantic%20correspondence%20where%20every%20pixel%20is%20a%20classifier%202015"
        },
        {
            "id": "4",
            "entry": "[4] D. Butler, J. Wulff, G. Stanley, and M. Black. A naturalistic open source movie for optical flow evaluation. In: ECCV, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Butler%2C%20D.%20Wulff%2C%20J.%20Stanley%2C%20G.%20Black%2C%20M.%20A%20naturalistic%20open%20source%20movie%20for%20optical%20flow%20evaluation%202012"
        },
        {
            "id": "5",
            "entry": "[5] C. B. Choy, Y. Gwak, and S. Savarese. Universal correspondence network. In:NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choy%2C%20C.B.%20Gwak%2C%20Y.%20Savarese%2C%20S.%20Universal%20correspondence%20network%202016"
        },
        {
            "id": "6",
            "entry": "[6] Y. HaCohen, E. Shechtman, D. B. Goldman, and D. Lischinski. Non-rigid dense correspondence with applications for image enhancement. In: SIGGRAPH, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HaCohen%2C%20Y.%20Shechtman%2C%20E.%20Goldman%2C%20D.B.%20Lischinski%2C%20D.%20Non-rigid%20dense%20correspondence%20with%20applications%20for%20image%20enhancement%202011"
        },
        {
            "id": "7",
            "entry": "[7] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal flow. In: CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ham%2C%20B.%20Cho%2C%20M.%20Schmid%2C%20C.%20Ponce%2C%20J.%20Proposal%20flow%202016"
        },
        {
            "id": "8",
            "entry": "[8] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal flow: Semantic correspondences from object proposals. IEEE Trans. PAMI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ham%2C%20B.%20Cho%2C%20M.%20Schmid%2C%20C.%20Ponce%2C%20J.%20Proposal%20flow%3A%20Semantic%20correspondences%20from%20object%20proposals%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ham%2C%20B.%20Cho%2C%20M.%20Schmid%2C%20C.%20Ponce%2C%20J.%20Proposal%20flow%3A%20Semantic%20correspondences%20from%20object%20proposals%202017"
        },
        {
            "id": "9",
            "entry": "[9] K. Han, R. S. Rezende, B. Ham, K. Wong, M. Cho, C. Schmid, and J. Ponce. Scnet: Learning semantic correspondence. In: ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20K.%20Rezende%2C%20R.S.%20Ham%2C%20B.%20Wong%2C%20K.%20Scnet%3A%20Learning%20semantic%20correspondence%202017"
        },
        {
            "id": "10",
            "entry": "[10] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse detectors. In: ICCV, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hariharan%2C%20B.%20Arbelaez%2C%20P.%20Bourdev%2C%20L.%20Maji%2C%20S.%20Semantic%20contours%20from%20inverse%20detectors%202011"
        },
        {
            "id": "11",
            "entry": "[11] T. Hassner, V. Mayzels, and L. Zelnik-Manor. On sifts and their scales. In: CVPR, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassner%2C%20T.%20Mayzels%2C%20V.%20Zelnik-Manor%2C%20L.%20On%20sifts%20and%20their%20scales%202012"
        },
        {
            "id": "12",
            "entry": "[12] K. He, X. Zhang, S. Ren, and Sun. J. Deep residual learning for image recognition. In: CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20J%2C%20Sun%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "13",
            "entry": "[13] J. Hur, H. Lim, C. Park, and S. C. Ahn. Generalized deformable spatial pyramid: Geometrypreserving dense correspondence estimation. In: CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hur%2C%20J.%20Lim%2C%20H.%20Park%2C%20C.%20Ahn%2C%20S.C.%20Generalized%20deformable%20spatial%20pyramid%3A%20Geometrypreserving%20dense%20correspondence%20estimation%202015"
        },
        {
            "id": "14",
            "entry": "[14] Philbin. J., O. Chum, M. Isard, J. Sivic, and A. Zisserman. Object retrieval with large vocabularies and fast spatial matching. In: CVPR, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J.%2C%20Philbin%20Chum%2C%20O.%20Isard%2C%20M.%20Sivic%2C%20J.%20Object%20retrieval%20with%20large%20vocabularies%20and%20fast%20spatial%20matching%202007"
        },
        {
            "id": "15",
            "entry": "[15] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks. In: NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Kavukcuoglu%2C%20K.%20Spatial%20transformer%20networks%202015"
        },
        {
            "id": "16",
            "entry": "[16] A. Kanazawa, D. W. Jacobs, and M. Chandraker. Warpnet: Weakly supervised matching for single-view reconstruction. In: CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kanazawa%2C%20A.%20Jacobs%2C%20D.W.%20Chandraker%2C%20M.%20Warpnet%3A%20Weakly%20supervised%20matching%20for%20single-view%20reconstruction%202016"
        },
        {
            "id": "17",
            "entry": "[17] J. Kim, C. Liu, F. Sha, and K. Grauman. Deformable spatial pyramid matching for fast dense correspondences. In: CVPR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20J.%20Liu%2C%20C.%20Sha%2C%20F.%20Grauman%2C%20K.%20Deformable%20spatial%20pyramid%20matching%20for%20fast%20dense%20correspondences%202013"
        },
        {
            "id": "18",
            "entry": "[18] S. Kim, D. Min, B. Ham, S. Jeon, S. Lin, and K. Sohn. Fcss: Fully convolutional self-similarity for dense semantic correspondence. In: CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.%20Min%2C%20D.%20Ham%2C%20B.%20Jeon%2C%20S.%20Fcss%3A%20Fully%20convolutional%20self-similarity%20for%20dense%20semantic%20correspondence%202017"
        },
        {
            "id": "19",
            "entry": "[19] S. Kim, D. Min, B. Ham, S. Lin, and K. Sohn. Fcss: Fully convolutional self-similarity for dense semantic correspondence. TPAMI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.%20Min%2C%20D.%20Ham%2C%20B.%20Lin%2C%20S.%20Fcss%3A%20Fully%20convolutional%20self-similarity%20for%20dense%20semantic%20correspondence%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20S.%20Min%2C%20D.%20Ham%2C%20B.%20Lin%2C%20S.%20Fcss%3A%20Fully%20convolutional%20self-similarity%20for%20dense%20semantic%20correspondence%202018"
        },
        {
            "id": "20",
            "entry": "[20] S. Kim, D. Min, S. Lin, and K. Sohn. Dctm: Discrete-continuous transformation matching for semantic flow. In: ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.%20Min%2C%20D.%20Lin%2C%20S.%20Sohn%2C%20K.%20Dctm%3A%20Discrete-continuous%20transformation%20matching%20for%20semantic%20flow%202017"
        },
        {
            "id": "21",
            "entry": "[21] S. Kim, D. Min, S. Lin, and K. Sohn. Discrete-continuous transformation matching for dense semantic correspondence. IEEE Trans. PAMI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.%20Min%2C%20D.%20Lin%2C%20S.%20Sohn%2C%20K.%20Discrete-continuous%20transformation%20matching%20for%20dense%20semantic%20correspondence%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20S.%20Min%2C%20D.%20Lin%2C%20S.%20Sohn%2C%20K.%20Discrete-continuous%20transformation%20matching%20for%20dense%20semantic%20correspondence%202018"
        },
        {
            "id": "22",
            "entry": "[22] J. Liao, Y. Yao, L. Yuan, G. Hua, and S. B. Kang. Visual attribute transfer through deep image analogy. In: SIGGRAPH, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liao%2C%20J.%20Yao%2C%20Y.%20Yuan%2C%20L.%20Hua%2C%20G.%20Visual%20attribute%20transfer%20through%20deep%20image%20analogy%202017"
        },
        {
            "id": "23",
            "entry": "[23] Chen-Hsuan Lin and Simon Lucey. Inverse compositional spatial transformer networks. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Chen-Hsuan%20Lucey%2C%20Simon%20Inverse%20compositional%20spatial%20transformer%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Chen-Hsuan%20Lucey%2C%20Simon%20Inverse%20compositional%20spatial%20transformer%20networks%202017"
        },
        {
            "id": "24",
            "entry": "[24] Y. L. Lin, V. I. Morariu, W. Hsu, and L. S. Davis. Jointly optimizing 3d model fitting and fine-grained classification. In: ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Y.L.%20Morariu%2C%20V.I.%20Hsu%2C%20W.%20Davis%2C%20L.S.%20Jointly%20optimizing%203d%20model%20fitting%20and%20fine-grained%20classification%202014"
        },
        {
            "id": "25",
            "entry": "[25] C. Liu, J. Yuen, and A Torralba. Sift flow: Dense correspondence across scenes and its applications. IEEE Trans. PAMI, 33(5):815\u2013830, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20C.%20Yuen%2C%20J.%20Torralba%2C%20A.%20Sift%20flow%3A%20Dense%20correspondence%20across%20scenes%20and%20its%20applications%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20C.%20Yuen%2C%20J.%20Torralba%2C%20A.%20Sift%20flow%3A%20Dense%20correspondence%20across%20scenes%20and%20its%20applications%202011"
        },
        {
            "id": "26",
            "entry": "[26] J. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence? In: NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20J.%20Zhang%2C%20N.%20Darrell%2C%20T.%20Do%20convnets%20learn%20correspondence%3F%202014"
        },
        {
            "id": "27",
            "entry": "[27] D.G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91\u2013110, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lowe%2C%20D.G.%20Distinctive%20image%20features%20from%20scale-invariant%20keypoints%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowe%2C%20D.G.%20Distinctive%20image%20features%20from%20scale-invariant%20keypoints%202004"
        },
        {
            "id": "28",
            "entry": "[28] D. Novotny, D. Larlus, and A. Vedaldi. Anchornet: A weakly supervised network to learn geometry-sensitive features for semantic matching. In:CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Novotny%2C%20D.%20Larlus%2C%20D.%20Vedaldi%2C%20A.%20Anchornet%3A%20A%20weakly%20supervised%20network%20to%20learn%20geometry-sensitive%20features%20for%20semantic%20matching%202017"
        },
        {
            "id": "29",
            "entry": "[29] W. Qiu, X. Wang, X. Bai, A. Yuille, and Z. Tu. Scale-space sift flow. In: WACV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qiu%2C%20W.%20Wang%2C%20X.%20Bai%2C%20X.%20Yuille%2C%20A.%20Scale-space%20sift%20flow%202014"
        },
        {
            "id": "30",
            "entry": "[30] I. Rocco, R. Arandjelovic, and J. Sivic. Convolutional neural network architecture for geometric matching. In:CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rocco%2C%20I.%20Arandjelovic%2C%20R.%20Sivic%2C%20J.%20Convolutional%20neural%20network%20architecture%20for%20geometric%20matching%202017"
        },
        {
            "id": "31",
            "entry": "[31] I. Rocco, R. Arandjelovic, and J. Sivic. End-to-end weakly-supervised semantic alignment. In: CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rocco%2C%20I.%20Arandjelovic%2C%20R.%20Sivic%2C%20J.%20End-to-end%20weakly-supervised%20semantic%20alignment%202018"
        },
        {
            "id": "32",
            "entry": "[32] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In: MICCAI, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ronneberger%2C%20O.%20Fischer%2C%20P.%20Brox%2C%20T.%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%202015"
        },
        {
            "id": "33",
            "entry": "[33] M. Rubinstein, A. Joulin, J. Kopf, and C. Liu. Unsupervised joint object discovery and segmentation in internet images. In: CVPR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20M.%20Joulin%2C%20A.%20Kopf%2C%20J.%20Liu%2C%20C.%20Unsupervised%20joint%20object%20discovery%20and%20segmentation%20in%20internet%20images%202013"
        },
        {
            "id": "34",
            "entry": "[34] D. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. IJCV, 47(1):7\u201342, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scharstein%2C%20D.%20Szeliski%2C%20R.%20A%20taxonomy%20and%20evaluation%20of%20dense%20two-frame%20stereo%20correspondence%20algorithms%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scharstein%2C%20D.%20Szeliski%2C%20R.%20A%20taxonomy%20and%20evaluation%20of%20dense%20two-frame%20stereo%20correspondence%20algorithms%202002"
        },
        {
            "id": "35",
            "entry": "[35] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In: ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "36",
            "entry": "[36] T. Taniai, S. N. Sinha, and Y. Sato. Joint recovery of dense correspondence and cosegmentation in two images. In: CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taniai%2C%20T.%20Sinha%2C%20S.N.%20Sato%2C%20Y.%20Joint%20recovery%20of%20dense%20correspondence%20and%20cosegmentation%20in%20two%20images%202016"
        },
        {
            "id": "37",
            "entry": "[37] J. Thewlis1, H. Bilen, and A. Vedald. Unsupervised learning of object frames by dense equivariant image labelling. In: NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thewlis1%2C%20J.%20Bilen%2C%20H.%20Vedald%2C%20A.%20Unsupervised%20learning%20of%20object%20frames%20by%20dense%20equivariant%20image%20labelling%202017"
        },
        {
            "id": "38",
            "entry": "[38] N. Ufer and B. Ommer. Deep semantic feature matching. In:CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ufer%2C%20N.%20Ommer%2C%20B.%20Deep%20semantic%20feature%20matching%202017"
        },
        {
            "id": "39",
            "entry": "[39] F. Yang, X. Li, H. Cheng, J. Li, and L. Chen. Object-aware dense semantic correspondence. In:CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20F.%20Li%2C%20X.%20Cheng%2C%20H.%20Li%2C%20J.%20Object-aware%20dense%20semantic%20correspondence%202017"
        },
        {
            "id": "40",
            "entry": "[40] H. Yang, W. Y. Lin, and J. Lu. Daisy filter flow: A generalized discrete approach to dense correspondences. In: CVPR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20H.%20Lin%2C%20W.Y.%20Lu%2C%20J.%20Daisy%20filter%20flow%3A%20A%20generalized%20discrete%20approach%20to%20dense%20correspondences%202014"
        },
        {
            "id": "41",
            "entry": "[41] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. Lift: Learned invariant feature transform. In: ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20K.M.%20Trulls%2C%20E.%20Lepetit%2C%20V.%20Fua%2C%20P.%20Lift%3A%20Learned%20invariant%20feature%20transform%202016"
        },
        {
            "id": "42",
            "entry": "[42] K. M. Yi, Y. Verdie, P. Fua, and V. Lepetit. Learning to assign orientations to feature points. In: CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20K.M.%20Verdie%2C%20Y.%20Fua%2C%20P.%20Lepetit%2C%20V.%20Learning%20to%20assign%20orientations%20to%20feature%20points%202016"
        },
        {
            "id": "43",
            "entry": "[43] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In: ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20F.%20Koltun%2C%20V.%20Multi-scale%20context%20aggregation%20by%20dilated%20convolutions%202016"
        },
        {
            "id": "44",
            "entry": "[44] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A. Efros. Learning dense correspondence via 3d-guided cycle consistency. In: CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20T.%20Krahenbuhl%2C%20P.%20Aubry%2C%20M.%20Huang%2C%20Q.%20Learning%20dense%20correspondence%20via%203d-guided%20cycle%20consistency%202016"
        },
        {
            "id": "45",
            "entry": "[45] T. Zhou, Y. J. Lee, S. X. Yu, and A. A. Efros. Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences. In: CVPR, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20T.%20Lee%2C%20Y.J.%20Yu%2C%20S.X.%20Efros%2C%20A.A.%20Flowweb%3A%20Joint%20image%20set%20alignment%20by%20weaving%20consistent%2C%20pixel-wise%20correspondences%202015"
        }
    ]
}
