{
    "filename": "7319-self-supervised-generation-of-spatial-audio-for-360-video.pdf",
    "metadata": {
        "title": "Self-Supervised Generation of Spatial Audio for 360\u00b0 Video",
        "author": "Pedro Morgado, Nuno Nvasconcelos, Timothy Langlois, Oliver Wang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7319-self-supervised-generation-of-spatial-audio-for-360-video.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce an approach to convert mono audio recorded by a 360\u25e6 video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360\u25e6 video viewing, but spatial audio microphones are still rare in current 360\u25e6 video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360\u25e6 video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360\u25e6 videos uploaded with spatial audio. During training, ground-truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360\u25e6 video and a mono audio track."
    },
    "keywords": [
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "spatial audio",
            "url": "https://en.wikipedia.org/wiki/spatial_audio"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Log-spectral distance",
            "url": "https://en.wikipedia.org/wiki/Log-spectral_distance"
        },
        {
            "term": "audio visual",
            "url": "https://en.wikipedia.org/wiki/audio_visual"
        },
        {
            "term": "head-mounted display",
            "url": "https://en.wikipedia.org/wiki/head-mounted_display"
        }
    ],
    "highlights": [
        "360\u25e6 video provides viewers an immersive viewing experience where they are free to look in any direction, either by turning their heads with a Head-Mounted Display (HMD), or by mouse-control while watching the video in a browser (e.g., YouTube)",
        "As humans rely on audio localization cues for full scene awareness, spatial audio is a crucial component of 360\u25e6 video",
        "In order to train and validate our approach, we introduce two 360\u25e6 video datasets with spatial audio, one recorded by ourselves in a constrained domain, and a large-scale dataset collected in-the-wild from YouTube",
        "We demonstrate the advantages of our approach, inspired by the ambisonics encoding process in controlled environments, over a generic U-Net architecture for spatial audio generation",
        "Conclusion We presented the first approach for up-converting conventional mono recordings into spatial audio given a 360\u25e6 video, and introduced an end-to-end trainable network tailored to this task"
    ],
    "key_statements": [
        "360\u25e6 video provides viewers an immersive viewing experience where they are free to look in any direction, either by turning their heads with a Head-Mounted Display (HMD), or by mouse-control while watching the video in a browser (e.g., YouTube)",
        "As humans rely on audio localization cues for full scene awareness, spatial audio is a crucial component of 360\u25e6 video",
        "In order to train and validate our approach, we introduce two 360\u25e6 video datasets with spatial audio, one recorded by ourselves in a constrained domain, and a large-scale dataset collected in-the-wild from YouTube",
        "Experiments conducted in both datasets show that the proposed neural network can generate plausible spatial audio for 360\u25e6 video",
        "We show that the generation of ambisonic audio can be learned using a dataset of 360\u25e6 video with spatial audio collected in-the-wild without additional human intervention",
        "We demonstrate the advantages of our approach, inspired by the ambisonics encoding process in controlled environments, over a generic U-Net architecture for spatial audio generation",
        "first-order ambisonics consists of four channels that store the first-order coefficients, \u03c600, \u03c6\u22121 1, \u03c610 and \u03c611, of the spherical harmonic expansion in (Eq 1)",
        "In order to measure the localization accuracy of the generated spatial audio, we propose to compute the Earth Mover\u2019s Distance [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] between the energy maps E(\u03b8, t) associated with \u03c6(t) and \u03c6(t)",
        "We identified 489 videos that were recorded with a \u201chorizontal\u201d spatial audio microphone,\u03c6x(t) and \u03c6y(t) channels)",
        "Qualitative results Designing robust metrics for comparing spatial audio is an open problem, and we found that only so much can be determined by these metrics alone",
        "The results shown in Table 1 and Fig. 3 use videos recorded with ambisonic microphones and converted to mono audio",
        "To validate whether our method extends to real mono microphones, we scraped additional videos from YouTube that were not recorded with ambisonics, and show that we can still generate convincing spatial audio",
        "While general purpose spatial audio generation is still an open problem, we provide a first approach",
        "Conclusion We presented the first approach for up-converting conventional mono recordings into spatial audio given a 360\u25e6 video, and introduced an end-to-end trainable network tailored to this task"
    ],
    "summary": [
        "360\u25e6 video provides viewers an immersive viewing experience where they are free to look in any direction, either by turning their heads with a Head-Mounted Display (HMD), or by mouse-control while watching the video in a browser (e.g., YouTube).",
        "We seek to generate spatial audio in the form of a popular encoding format called first-order ambisonics (FOA), given the mono audio and corresponding 360\u25e6 video as inputs.",
        "The proposed procedure is based on a novel neural network architecture that disentangles two fundamental challenges in audio spatialization: the separation of sound sources from a mixed audio input and respective localization of these sources.",
        "Experiments conducted in both datasets show that the proposed neural network can generate plausible spatial audio for 360\u25e6 video.",
        "We show that the generation of ambisonic audio can be learned using a dataset of 360\u25e6 video with spatial audio collected in-the-wild without additional human intervention.",
        "[<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] proposes a recurrent neural-network for monaural separation of two speakers, [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] seek to isolate sound sources by leveraging synchronized visual information in addition to the audio input, and [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>] studies a wide range of frequency-based separation methods.",
        "Self-supervised audio spatialization Converting mono to FOA ideally requires learning from videos with paired mono and ambisonics recordings, which are difficult to collect in-the-wild.",
        "Localization To localize the sounds f i(t) extracted by the separation network, we implement a module that generates, at each time t, the localization weights wi(t) =, wyi (t), wzi (t)) associated with each of the k sources, through a series of fully-connected layers applied to the multi-modal feature vectors of the analysis stage.",
        "We modify the U-NET architecture by setting the number of units in the output layer to the number of ambisonic channels, and concatenate video features to the U-Net bottleneck.",
        "The results shown in Table 1 and Fig. 3 use videos recorded with ambisonic microphones and converted to mono audio.",
        "To validate whether our method extends to real mono microphones, we scraped additional videos from YouTube that were not recorded with ambisonics, and show that we can still generate convincing spatial audio.",
        "Participants watched 20 randomly selected videos whose audio was generated by one of four methods: GT, the original ground-truth recorded spatial audio; MONO, just the mono track; U-NET, the baseline method; and OURS, the result of our full method.",
        "Conclusion We presented the first approach for up-converting conventional mono recordings into spatial audio given a 360\u25e6 video, and introduced an end-to-end trainable network tailored to this task.",
        "We demonstrate the benefits of each component of our network and show that the proposed generator performs substantially better than a domain independent baseline"
    ],
    "headline": "We introduce an approach to convert mono audio recorded by a 360\u25e6 video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] T. Afouras, J. S. Chung, and A. Zisserman. The conversation: Deep audio-visual speech enhancement. In Interspeech, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Afouras%2C%20T.%20Chung%2C%20J.S.%20Zisserman%2C%20A.%20The%20conversation%3A%20Deep%20audio-visual%20speech%20enhancement%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Afouras%2C%20T.%20Chung%2C%20J.S.%20Zisserman%2C%20A.%20The%20conversation%3A%20Deep%20audio-visual%20speech%20enhancement%202018"
        },
        {
            "id": "2",
            "entry": "[2] S. Amari, A. Cichocki, and H. H. Yang. A new learning algorithm for blind signal separation. In Advances in Neural Information and Processing Systems (NIPS), 1996. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.%20Cichocki%2C%20A.%20Yang%2C%20H.H.%20A%20new%20learning%20algorithm%20for%20blind%20signal%20separation%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.%20Cichocki%2C%20A.%20Yang%2C%20H.H.%20A%20new%20learning%20algorithm%20for%20blind%20signal%20separation%201996"
        },
        {
            "id": "3",
            "entry": "[3] S. Argentieri, P. Dan\u00e8s, and P. Sou\u00e8res. A survey on sound source localization in robotics: From binaural to array processing methods. Computer Speech & Language, 34(1):87\u2013112, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argentieri%2C%20S.%20Dan%C3%A8s%2C%20P.%20Sou%C3%A8res%2C%20P.%20A%20survey%20on%20sound%20source%20localization%20in%20robotics%3A%20From%20binaural%20to%20array%20processing%20methods%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argentieri%2C%20S.%20Dan%C3%A8s%2C%20P.%20Sou%C3%A8res%2C%20P.%20A%20survey%20on%20sound%20source%20localization%20in%20robotics%3A%20From%20binaural%20to%20array%20processing%20methods%202015"
        },
        {
            "id": "4",
            "entry": "[4] Y. Aytar, C. Vondrick, and A. Torralba. Soundnet: Learning sound representations from unlabeled video. In Advances in Neural Information and Processing Systems (NIPS), 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aytar%2C%20Y.%20Vondrick%2C%20C.%20Torralba%2C%20A.%20Soundnet%3A%20Learning%20sound%20representations%20from%20unlabeled%20video%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aytar%2C%20Y.%20Vondrick%2C%20C.%20Torralba%2C%20A.%20Soundnet%3A%20Learning%20sound%20representations%20from%20unlabeled%20video%202016"
        },
        {
            "id": "5",
            "entry": "[5] Z. Barzelay and Y. Y. Schechner. Harmony in motion. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2007. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barzelay%2C%20Z.%20Schechner%2C%20Y.Y.%20Harmony%20in%20motion%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barzelay%2C%20Z.%20Schechner%2C%20Y.Y.%20Harmony%20in%20motion%202007"
        },
        {
            "id": "6",
            "entry": "[6] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and blind deconvolution. Neural computation, 7(6):1129\u20131159, 1995. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bell%2C%20A.J.%20Sejnowski%2C%20T.J.%20An%20information-maximization%20approach%20to%20blind%20separation%20and%20blind%20deconvolution%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bell%2C%20A.J.%20Sejnowski%2C%20T.J.%20An%20information-maximization%20approach%20to%20blind%20separation%20and%20blind%20deconvolution%201995"
        },
        {
            "id": "7",
            "entry": "[7] P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):287 \u2013 314, 1994. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Comon%2C%20P.%20Independent%20component%20analysis%2C%20a%20new%20concept%3F%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Comon%2C%20P.%20Independent%20component%20analysis%2C%20a%20new%20concept%3F%201994"
        },
        {
            "id": "8",
            "entry": "[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2009. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "9",
            "entry": "[9] G. Dickins and R. Kennedy. Towards optimal soundfield representation. In Audio Engineering Society Convention, 1999. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dickins%2C%20G.%20Kennedy%2C%20R.%20Towards%20optimal%20soundfield%20representation%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dickins%2C%20G.%20Kennedy%2C%20R.%20Towards%20optimal%20soundfield%20representation%201999"
        },
        {
            "id": "10",
            "entry": "[10] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In European Conference on Computer Vision (ECCV), 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20C.%20Loy%2C%20C.C.%20He%2C%20K.%20Tang%2C%20X.%20Learning%20a%20deep%20convolutional%20network%20for%20image%20super-resolution%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20C.%20Loy%2C%20C.C.%20He%2C%20K.%20Tang%2C%20X.%20Learning%20a%20deep%20convolutional%20network%20for%20image%20super-resolution%202014"
        },
        {
            "id": "11",
            "entry": "[11] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. Freeman, and M. Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. In ACM SIGGRAPH, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ephrat%2C%20A.%20Mosseri%2C%20I.%20Lang%2C%20O.%20Dekel%2C%20T.%20Looking%20to%20listen%20at%20the%20cocktail%20party%3A%20A%20speaker-independent%20audio-visual%20model%20for%20speech%20separation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ephrat%2C%20A.%20Mosseri%2C%20I.%20Lang%2C%20O.%20Dekel%2C%20T.%20Looking%20to%20listen%20at%20the%20cocktail%20party%3A%20A%20speaker-independent%20audio-visual%20model%20for%20speech%20separation%202018"
        },
        {
            "id": "12",
            "entry": "[12] A. Gabbay, A. Shamir, and S. Peleg. Visual speech enhancement using noise-invariant training. In Interspeech, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gabbay%2C%20A.%20Shamir%2C%20A.%20Peleg%2C%20S.%20Visual%20speech%20enhancement%20using%20noise-invariant%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gabbay%2C%20A.%20Shamir%2C%20A.%20Peleg%2C%20S.%20Visual%20speech%20enhancement%20using%20noise-invariant%20training%202018"
        },
        {
            "id": "13",
            "entry": "[13] M. A. Gerzon. Periphony: With-height sound reproduction. J. Audio Eng. Soc, 21(1):2\u201310, 1973. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerzon%2C%20M.A.%20Periphony%3A%20With-height%20sound%20reproduction%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gerzon%2C%20M.A.%20Periphony%3A%20With-height%20sound%20reproduction%201973"
        },
        {
            "id": "14",
            "entry": "[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information and Processing Systems (NIPS), 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "15",
            "entry": "[15] A. Gray and J. Markel. Distance measures for speech processing. IEEE Transactions on Acoustics, Speech, and Signal Processing, 24(5):380\u2013391, 1976. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gray%2C%20A.%20Markel%2C%20J.%20Distance%20measures%20for%20speech%20processing%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gray%2C%20A.%20Markel%2C%20J.%20Distance%20measures%20for%20speech%20processing%201976"
        },
        {
            "id": "16",
            "entry": "[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), 2016. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20mappings%20in%20deep%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20mappings%20in%20deep%20residual%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, et al. CNN architectures for large-scale audio classification. In IEEE International Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2017. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hershey%2C%20S.%20Chaudhuri%2C%20S.%20Ellis%2C%20D.P.%20Gemmeke%2C%20J.F.%20CNN%20architectures%20for%20large-scale%20audio%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hershey%2C%20S.%20Chaudhuri%2C%20S.%20Ellis%2C%20D.P.%20Gemmeke%2C%20J.F.%20CNN%20architectures%20for%20large-scale%20audio%20classification%202017"
        },
        {
            "id": "18",
            "entry": "[18] J. Hornstein, M. Lopes, J. Santos-Victor, and F. Lacerda. Sound localization for humanoid robots-building audio-motor maps based on the HRTF. In IEEE/RSJ International Conf. on Intelligent Robots and Systems, 2006. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornstein%2C%20J.%20Lopes%2C%20M.%20Santos-Victor%2C%20J.%20Lacerda%2C%20F.%20Sound%20localization%20for%20humanoid%20robots-building%20audio-motor%20maps%20based%20on%20the%20HRTF%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornstein%2C%20J.%20Lopes%2C%20M.%20Santos-Victor%2C%20J.%20Lacerda%2C%20F.%20Sound%20localization%20for%20humanoid%20robots-building%20audio-motor%20maps%20based%20on%20the%20HRTF%202006"
        },
        {
            "id": "19",
            "entry": "[19] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis. Deep learning for monaural speech separation. In IEEE International Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20P.-S.%20Kim%2C%20M.%20Hasegawa-Johnson%2C%20M.%20Smaragdis%2C%20P.%20Deep%20learning%20for%20monaural%20speech%20separation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20P.-S.%20Kim%2C%20M.%20Hasegawa-Johnson%2C%20M.%20Smaragdis%2C%20P.%20Deep%20learning%20for%20monaural%20speech%20separation%202014"
        },
        {
            "id": "20",
            "entry": "[20] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification. ACM Transactions on Graphics (TOG), 35(4):110, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Let%20there%20be%20color%21%3A%20joint%20end-to-end%20learning%20of%20global%20and%20local%20image%20priors%20for%20automatic%20image%20colorization%20with%20simultaneous%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iizuka%2C%20S.%20Simo-Serra%2C%20E.%20Ishikawa%2C%20H.%20Let%20there%20be%20color%21%3A%20joint%20end-to-end%20learning%20of%20global%20and%20local%20image%20priors%20for%20automatic%20image%20colorization%20with%20simultaneous%20classification%202016"
        },
        {
            "id": "21",
            "entry": "[21] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ilg%2C%20E.%20Mayer%2C%20N.%20Saikia%2C%20T.%20Keuper%2C%20M.%20Flownet%202.0%3A%20Evolution%20of%20optical%20flow%20estimation%20with%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ilg%2C%20E.%20Mayer%2C%20N.%20Saikia%2C%20T.%20Keuper%2C%20M.%20Flownet%202.0%3A%20Evolution%20of%20optical%20flow%20estimation%20with%20deep%20networks%202017"
        },
        {
            "id": "22",
            "entry": "[22] H. Izadinia, I. Saleemi, and M. Shah. Multimodal analysis for identification and segmentation of movingsounding objects. IEEE Transactions on Multimedia, 15(2):378\u2013390, 2013. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Izadinia%2C%20H.%20Saleemi%2C%20I.%20Shah%2C%20M.%20Multimodal%20analysis%20for%20identification%20and%20segmentation%20of%20movingsounding%20objects%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Izadinia%2C%20H.%20Saleemi%2C%20I.%20Shah%2C%20M.%20Multimodal%20analysis%20for%20identification%20and%20segmentation%20of%20movingsounding%20objects%202013"
        },
        {
            "id": "23",
            "entry": "[23] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. 3",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "24",
            "entry": "[24] C. Jutten and J. Herault. Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. Signal Processing, 24(1), 1991. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jutten%2C%20C.%20Herault%2C%20J.%20Blind%20separation%20of%20sources%2C%20part%20I%3A%20An%20adaptive%20algorithm%20based%20on%20neuromimetic%20architecture%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jutten%2C%20C.%20Herault%2C%20J.%20Blind%20separation%20of%20sources%2C%20part%20I%3A%20An%20adaptive%20algorithm%20based%20on%20neuromimetic%20architecture%201991"
        },
        {
            "id": "25",
            "entry": "[25] E. Kidron, Y. Y. Schechner, and M. Elad. Pixels that sound. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2005. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=E%20Kidron%20Y%20Y%20Schechner%20and%20M%20Elad%20Pixels%20that%20sound%20In%20IEEE%20Conf%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202005%203",
            "oa_query": "https://api.scholarcy.com/oa_version?query=E%20Kidron%20Y%20Y%20Schechner%20and%20M%20Elad%20Pixels%20that%20sound%20In%20IEEE%20Conf%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%202005%203"
        },
        {
            "id": "26",
            "entry": "[26] E. Kidron, Y. Y. Schechner, and M. Elad. Cross-modal localization via sparsity. IEEE Trans. on Signal Processing (TIP), 55(4):1390\u20131404, 2007. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kidron%2C%20E.%20Schechner%2C%20Y.Y.%20Elad%2C%20M.%20Cross-modal%20localization%20via%20sparsity%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kidron%2C%20E.%20Schechner%2C%20Y.Y.%20Elad%2C%20M.%20Cross-modal%20localization%20via%20sparsity%202007"
        },
        {
            "id": "27",
            "entry": "[27] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-resolution using very deep convolutional networks. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20J.%20Lee%2C%20J.Kwon%20K.%20Mu%20Lee.%20Accurate%20image%20super-resolution%20using%20very%20deep%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20J.%20Lee%2C%20J.Kwon%20K.%20Mu%20Lee.%20Accurate%20image%20super-resolution%20using%20very%20deep%20convolutional%20networks%202016"
        },
        {
            "id": "28",
            "entry": "[28] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. 6",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "29",
            "entry": "[29] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations (ICLR), 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "30",
            "entry": "[30] M. Kronlachner. Spatial transformations for the alteration of ambisonic recordings. Master\u2019s thesis, Graz University of Technology, 2014. 2, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kronlachner%2C%20M.%20Spatial%20transformations%20for%20the%20alteration%20of%20ambisonic%20recordings%202014"
        },
        {
            "id": "31",
            "entry": "[31] V. Kuleshov, S. Z. Enam, and S. Ermon. Audio super resolution using neural networks. In Workshops at International Conference on Learning Representations (ICLR), 2017. 3, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuleshov%2C%20V.%20Enam%2C%20S.Z.%20Ermon%2C%20S.%20Audio%20super%20resolution%20using%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuleshov%2C%20V.%20Enam%2C%20S.Z.%20Ermon%2C%20S.%20Audio%20super%20resolution%20using%20neural%20networks%202017"
        },
        {
            "id": "32",
            "entry": "[32] E. Levina and P. Bickel. The earth mover\u2019s distance is the mallows distance: Some insights from statistics. In IEEE International Conference on Computer Vision (ICCV), 2001. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levina%2C%20E.%20Bickel%2C%20P.%20The%20earth%20mover%E2%80%99s%20distance%20is%20the%20mallows%20distance%3A%20Some%20insights%20from%20statistics%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levina%2C%20E.%20Bickel%2C%20P.%20The%20earth%20mover%E2%80%99s%20distance%20is%20the%20mallows%20distance%3A%20Some%20insights%20from%20statistics%202001"
        },
        {
            "id": "33",
            "entry": "[33] A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: A large-scale speaker identification dataset. In Interspeech, 2017. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagrani%2C%20A.%20Chung%2C%20J.S.%20Zisserman%2C%20A.%20Voxceleb%3A%20A%20large-scale%20speaker%20identification%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagrani%2C%20A.%20Chung%2C%20J.S.%20Zisserman%2C%20A.%20Voxceleb%3A%20A%20large-scale%20speaker%20identification%20dataset%202017"
        },
        {
            "id": "34",
            "entry": "[34] K. Nakadai, H. G. Okuno, and H. Kitano. Real-time sound source localization and separation for robot audition. In International Conference on Spoken Language Processing, 2002. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nakadai%2C%20K.%20Okuno%2C%20H.G.%20Kitano%2C%20H.%20Real-time%20sound%20source%20localization%20and%20separation%20for%20robot%20audition%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nakadai%2C%20K.%20Okuno%2C%20H.G.%20Kitano%2C%20H.%20Real-time%20sound%20source%20localization%20and%20separation%20for%20robot%20audition%202002"
        },
        {
            "id": "35",
            "entry": "[35] K. Nakamura, K. Nakadai, F. Asano, and G. Ince. Intelligent sound source localization and its application to multimodal human tracking. In IEEE/RSJ International Conf. on Intelligent Robots and Systems (IROS), 2011. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nakamura%2C%20K.%20Nakadai%2C%20K.%20Asano%2C%20F.%20Ince%2C%20G.%20Intelligent%20sound%20source%20localization%20and%20its%20application%20to%20multimodal%20human%20tracking%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nakamura%2C%20K.%20Nakadai%2C%20K.%20Asano%2C%20F.%20Ince%2C%20G.%20Intelligent%20sound%20source%20localization%20and%20its%20application%20to%20multimodal%20human%20tracking%202011"
        },
        {
            "id": "36",
            "entry": "[36] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. In 9th ISCA Speech Synthesis Workshop, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=v.%20d.%20Oord%2C%20A.%20Dieleman%2C%20S.%20Zen%2C%20H.%20Simonyan%2C%20K.%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=v.%20d.%20Oord%2C%20A.%20Dieleman%2C%20S.%20Zen%2C%20H.%20Simonyan%2C%20K.%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016"
        },
        {
            "id": "37",
            "entry": "[37] A. Owens and A. A. Efros. Audio-visual scene analysis with self-supervised multisensory features. In European Conference on Computer Vision (ECCV), 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Owens%2C%20A.%20Efros%2C%20A.A.%20Audio-visual%20scene%20analysis%20with%20self-supervised%20multisensory%20features%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Owens%2C%20A.%20Efros%2C%20A.A.%20Audio-visual%20scene%20analysis%20with%20self-supervised%20multisensory%20features%202018"
        },
        {
            "id": "38",
            "entry": "[38] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adelson, and W. T. Freeman. Visually indicated sounds. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Owens%2C%20A.%20Isola%2C%20P.%20McDermott%2C%20J.%20Torralba%2C%20A.%20Visually%20indicated%20sounds%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Owens%2C%20A.%20Isola%2C%20P.%20McDermott%2C%20J.%20Torralba%2C%20A.%20Visually%20indicated%20sounds%202016"
        },
        {
            "id": "39",
            "entry": "[39] O. Santala and V. Pulkki. Directional perception of distributed sound sources. The Journal of the Acoustical Society of America, 129(3):1522\u20131530, 2011. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santala%2C%20O.%20Pulkki%2C%20V.%20Directional%20perception%20of%20distributed%20sound%20sources%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santala%2C%20O.%20Pulkki%2C%20V.%20Directional%20perception%20of%20distributed%20sound%20sources%202011"
        },
        {
            "id": "40",
            "entry": "[40] J. O. Smith. Mathematics of the discrete Fourier transform (DFT): with audio applications, chapter The Analytic Signal and Hilbert Transform Filters. Julius Smith, 2007. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20J.O.%20Mathematics%20of%20the%20discrete%20Fourier%20transform%20%28DFT%29%3A%20with%20audio%20applications%2C%20chapter%20The%20Analytic%20Signal%20and%20Hilbert%20Transform%20Filters%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20J.O.%20Mathematics%20of%20the%20discrete%20Fourier%20transform%20%28DFT%29%3A%20with%20audio%20applications%2C%20chapter%20The%20Analytic%20Signal%20and%20Hilbert%20Transform%20Filters%202007"
        },
        {
            "id": "41",
            "entry": "[41] M. Soler, J.-C. Bazin, O. Wang, A. Krause, and A. Sorkine-Hornung. Suggesting sounds for images from video collections. In European Conference on Computer Vision (ECCV), 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soler%2C%20M.%20Bazin%2C%20J.-C.%20Wang%2C%20O.%20Krause%2C%20A.%20Suggesting%20sounds%20for%20images%20from%20video%20collections%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soler%2C%20M.%20Bazin%2C%20J.-C.%20Wang%2C%20O.%20Krause%2C%20A.%20Suggesting%20sounds%20for%20images%20from%20video%20collections%202016"
        },
        {
            "id": "42",
            "entry": "[42] N. Strobel, S. Spors, and R. Rabenstein. Joint audio-video object localization and tracking. IEEE Signal Processing Magazine, 18(1):22\u201331, 2001. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strobel%2C%20N.%20Spors%2C%20S.%20Rabenstein%2C%20R.%20Joint%20audio-video%20object%20localization%20and%20tracking%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strobel%2C%20N.%20Spors%2C%20S.%20Rabenstein%2C%20R.%20Joint%20audio-video%20object%20localization%20and%20tracking%202001"
        },
        {
            "id": "43",
            "entry": "[43] J.-M. Valin, F. Michaud, and J. Rouat. Robust localization and tracking of simultaneous moving sound sources using beamforming and particle filtering. Robotics and Autonomous Systems, 55(3):216\u2013228, 2007. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valin%2C%20J.-M.%20Michaud%2C%20F.%20Rouat%2C%20J.%20Robust%20localization%20and%20tracking%20of%20simultaneous%20moving%20sound%20sources%20using%20beamforming%20and%20particle%20filtering%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valin%2C%20J.-M.%20Michaud%2C%20F.%20Rouat%2C%20J.%20Robust%20localization%20and%20tracking%20of%20simultaneous%20moving%20sound%20sources%20using%20beamforming%20and%20particle%20filtering%202007"
        },
        {
            "id": "44",
            "entry": "[44] D. Wang and J. Chen. Supervised speech separation based on deep learning: An overview. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20D.%20Chen%2C%20J.%20Supervised%20speech%20separation%20based%20on%20deep%20learning%3A%20An%20overview%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20D.%20Chen%2C%20J.%20Supervised%20speech%20separation%20based%20on%20deep%20learning%3A%20An%20overview%202018"
        },
        {
            "id": "45",
            "entry": "[45] E. B. Wilson. Probable inference, the law of succession, and statistical inference. Journal of the American Statistical Association, 22(158):209\u2013212, 1927. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20E.B.%20Probable%20inference%2C%20the%20law%20of%20succession%2C%20and%20statistical%20inference%201927",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20E.B.%20Probable%20inference%2C%20the%20law%20of%20succession%2C%20and%20statistical%20inference%201927"
        },
        {
            "id": "46",
            "entry": "[46] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In European Conference on Computer Vision (ECCV), 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Colorful%20image%20colorization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Colorful%20image%20colorization%202016"
        },
        {
            "id": "47",
            "entry": "[47] H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. McDermott, and A. Torralba. The sound of pixels. In European Conference on Computer Vision (ECCV), 2018. 3 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20H.%20Gan%2C%20C.%20Rouditchenko%2C%20A.%20Vondrick%2C%20C.%20The%20sound%20of%20pixels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20H.%20Gan%2C%20C.%20Rouditchenko%2C%20A.%20Vondrick%2C%20C.%20The%20sound%20of%20pixels%202018"
        }
    ]
}
