{
    "filename": "8279-support-recovery-for-orthogonal-matching-pursuit-upper-and-lower-bounds.pdf",
    "metadata": {
        "title": "Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds",
        "author": "Raghav Somani, Chirag Gupta, Prateek Jain, Praneeth Netrapalli",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8279-support-recovery-for-orthogonal-matching-pursuit-upper-and-lower-bounds.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the problem of sparse regression where the goal is to learn a sparse vector that best optimizes a given objective function. Under the assumption that the objective function satisfies restricted strong convexity (RSC), we analyze orthogonal matching pursuit (OMP), a greedy algorithm that is used heavily in applications, and obtain a support recovery result as well as a tight generalization error bound for the OMP estimator. Further, we show a lower bound for OMP, demonstrating that both our results on support recovery and generalization error are tight up to logarithmic factors. To the best of our knowledge, these are the first such tight upper and lower bounds for any sparse regression algorithm under the RSC assumption."
    },
    "keywords": [
        {
            "term": "generalization error",
            "url": "https://en.wikipedia.org/wiki/generalization_error"
        },
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "strong convexity",
            "url": "https://en.wikipedia.org/wiki/strong_convexity"
        },
        {
            "term": "greedy algorithm",
            "url": "https://en.wikipedia.org/wiki/greedy_algorithm"
        },
        {
            "term": "restricted isometry property",
            "url": "https://en.wikipedia.org/wiki/restricted_isometry_property"
        },
        {
            "term": "sparse vector",
            "url": "https://en.wikipedia.org/wiki/sparse_vector"
        }
    ],
    "highlights": [
        "The goal in sparse regression is to find the optimal sparse vector that minimizes a given objective function",
        "We focus on the problem of sparse linear regression (SLR), which is a representative problem in this domain",
        "In Section 4, we provide a matching lower bound which shows that there exist certain sparse linear regression problems on which orthogonal matching pursuit cannot perform significantly better than the error bounds given by our analysis",
        "For every value of d, n and s\u2217 where s\u2217 \u2264 d \u2264 n, there exists a design matrix A \u2208 Rn\u00d7d and a s\u2217-sparse vector x, (4.1)) such that the following holds true for orthogonal matching pursuit when applied to the sparse linear regression problem with y = Axand when orthogonal matching pursuit is executed for s \u2264 d \u2212 s\u2217 iterations:",
        "We provide lower bounds for orthogonal matching pursuit showing that our results are tight up to logarithmic factors",
        "We note that our results significantly improve upon a long list of existing results for greedy methods and match the best known results for sparse regression that use nonconvex penalty based methods"
    ],
    "key_statements": [
        "The goal in sparse regression is to find the optimal sparse vector that minimizes a given objective function",
        "Sparse regression is an important problem in statistical machine learning since sparse models lead to better generalization guarantees when the feature dimension is high or data is less, eg, high-dimensional statistics [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], bioinformatics [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], etc",
        "We focus on the problem of sparse linear regression (SLR), which is a representative problem in this domain",
        "We focus on orthogonal matching pursuit which is a greedy method that incrementally adds elements to support based on the amount of reduction in training error",
        "In Section 4, we provide a matching lower bound which shows that there exist certain sparse linear regression problems on which orthogonal matching pursuit cannot perform significantly better than the error bounds given by our analysis",
        "We show that in the noisy case, support recovery fails and the generalization error of orthogonal matching pursuit is large and matches the upper bound provided in Theorem 3.3",
        "For every value of d, n and s\u2217 where s\u2217 \u2264 d \u2264 n, there exists a design matrix A \u2208 Rn\u00d7d and a s\u2217-sparse vector x, (4.1)) such that the following holds true for orthogonal matching pursuit when applied to the sparse linear regression problem with y = Axand when orthogonal matching pursuit is executed for s \u2264 d \u2212 s\u2217 iterations:",
        "We show that with high probability, orthogonal matching pursuit starts recovering the correct support only after d1\u2212\u03b1 iterations for some constant \u03b1 > 0. This further implies that the generalization error cannot be better than the lower bound on generalization error we showed in Theorem 3.3",
        "We provide lower bounds for orthogonal matching pursuit showing that our results are tight up to logarithmic factors",
        "We note that our results significantly improve upon a long list of existing results for greedy methods and match the best known results for sparse regression that use nonconvex penalty based methods"
    ],
    "summary": [
        "The goal in sparse regression is to find the optimal sparse vector that minimizes a given objective function.",
        "We provide matching lower bounds for our support recovery and generalization error results.",
        "These lower bound results are restricted to algorithms which recover exactly s\u2217-sparse vectors, where s\u2217 = |supp(x)| and do not apply to OMP if it adds more than s\u2217 elements to the support set, which is the more meaningful scenario to consider.",
        "While the generalization error lower bound of [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] holds, it does not preclude the OMP algorithm from recovering the correct support.",
        "In Section 4, we provide a matching lower bound which shows that there exist certain sparse linear regression problems on which OMP cannot perform significantly better than the error bounds given by our analysis.",
        "We provide lower bounds on the performance of OMP, both in terms of support recovery and generalization error.",
        "We show that in the noisy case, support recovery fails and the generalization error of OMP is large and matches the upper bound provided in Theorem 3.3.",
        "For every value of d, n and s\u2217 where s\u2217 \u2264 d \u2264 n, there exists a design matrix A \u2208 Rn\u00d7d and a s\u2217-sparse vector x, (4.1)) such that the following holds true for OMP when applied to the sparse linear regression problem with y = Axand when OMP is executed for s \u2264 d \u2212 s\u2217 iterations:",
        "This theorem guarantees that it is the case, i.e., if design matrix A is ill-conditioned OMP has to work with support sets of size s \u2265 \u03bas+s\u2217 s\u2217.",
        "I.e., \u03c3 = 0, we can study both support recovery as well as generalization error behavior with respect to the restricted condition number \u03bas+s\u2217 .",
        "For every value of d and s\u2217, and any constants \u03b1 \u2208 (0, 1), \u03b4 \u2208 (0, 1), such that 8 \u2264 s\u2217 \u2264 s \u2264 d1\u2212\u03b1 and d \u2265 max 32 log (1/\u03b4) , 41/\u03b1 , there exists a sparse linear regression problem with y = Ax + \u03b7, \u03b7 \u223c N 0, \u03c32In\u00d7n , with design matrix A, and a s\u2217-sparse vector xdefined in (4.2),(4.1) such that the following holds:",
        "Note that we can compute the actual value of \u03ba for M( ); in general the restricted condition number of M( ) increases with decreasing , increasing the difficulty of the support recovery problem."
    ],
    "headline": "We study the problem of sparse regression where the goal is to learn a sparse vector that best optimizes a given objective function",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alekh Agarwal, Sahand Negahban, and Martin J. Wainwright. Fast global convergence of gradient methods for high-dimensional statistical recovery. Ann. Statist., 40(5):2452\u20132482, 10 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Fast%20global%20convergence%20of%20gradient%20methods%20for%20high-dimensional%20statistical%20recovery%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Fast%20global%20convergence%20of%20gradient%20methods%20for%20high-dimensional%20statistical%20recovery%202012"
        },
        {
            "id": "2",
            "entry": "[2] Thomas Blumensath and Mike E Davies. Gradient pursuit for non-linear sparse signal modelling. In Signal Processing Conference, 2008 16th European, pages 1\u20135. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Gradient%20pursuit%20for%20non-linear%20sparse%20signal%20modelling%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Gradient%20pursuit%20for%20non-linear%20sparse%20signal%20modelling%202008"
        },
        {
            "id": "3",
            "entry": "[3] St\u00e9phane Boucheron and Maud Thomas. Concentration inequalities for order statistics. arXiv preprint arXiv:1207.7209, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1207.7209"
        },
        {
            "id": "4",
            "entry": "[4] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theor., 51(12):4203\u2013 4215, December 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20E.J.%20Tao%2C%20T.%20Decoding%20by%20linear%20programming%204215-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20E.J.%20Tao%2C%20T.%20Decoding%20by%20linear%20programming%204215-12"
        },
        {
            "id": "5",
            "entry": "[5] Emmanuel Candes and Justin Romberg. Sparsity and incoherence in compressive sampling. Inverse problems, 23(3):969, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20Romberg%2C%20Justin%20Sparsity%20and%20incoherence%20in%20compressive%20sampling%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20Romberg%2C%20Justin%20Sparsity%20and%20incoherence%20in%20compressive%20sampling%202007"
        },
        {
            "id": "6",
            "entry": "[6] Emmanuel Candes, Terence Tao, et al. The dantzig selector: Statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20Tao%2C%20Terence%20The%20dantzig%20selector%3A%20Statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20Tao%2C%20Terence%20The%20dantzig%20selector%3A%20Statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007"
        },
        {
            "id": "7",
            "entry": "[7] David L Donoho, Michael Elad, and Vladimir N Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on information theory, 52(1):6\u201318, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Elad%2C%20Michael%20Temlyakov%2C%20Vladimir%20N.%20Stable%20recovery%20of%20sparse%20overcomplete%20representations%20in%20the%20presence%20of%20noise%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Elad%2C%20Michael%20Temlyakov%2C%20Vladimir%20N.%20Stable%20recovery%20of%20sparse%20overcomplete%20representations%20in%20the%20presence%20of%20noise%202006"
        },
        {
            "id": "8",
            "entry": "[8] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing, volume 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foucart%2C%20Simon%20Rauhut%2C%20Holger%20A%20mathematical%20introduction%20to%20compressive%20sensing%2C%20volume%201"
        },
        {
            "id": "9",
            "entry": "[9] Chirag Gupta, Arun Sai Suggala, Ankit Goyal, Harsha Vardhan Simhadri, Bhargavi Paranjape, Ashish Kumar, Saurabh Goyal, Raghavendra Udupa, Manik Varma, and Prateek Jain. Protonn: Compressed and accurate knn for resource-scarce devices. In International Conference on Machine Learning, pages 1331\u20131340, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Chirag%20Suggala%2C%20Arun%20Sai%20Goyal%2C%20Ankit%20Simhadri%2C%20Harsha%20Vardhan%20Protonn%3A%20Compressed%20and%20accurate%20knn%20for%20resource-scarce%20devices%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Chirag%20Suggala%2C%20Arun%20Sai%20Goyal%2C%20Ankit%20Simhadri%2C%20Harsha%20Vardhan%20Protonn%3A%20Compressed%20and%20accurate%20knn%20for%20resource-scarce%20devices%202017"
        },
        {
            "id": "10",
            "entry": "[10] D. Hsu, S. M. Kakade, and T. Zhang. A tail inequality for quadratic forms of subgaussian random vectors. ArXiv e-prints, October 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20D.%20Kakade%2C%20S.M.%20Zhang%2C%20T.%20A%20tail%20inequality%20for%20quadratic%20forms%20of%20subgaussian%20random%20vectors.%20ArXiv%20e-prints%202011-10"
        },
        {
            "id": "11",
            "entry": "[11] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-dimensional m-estimation. In Advances in Neural Information Processing Systems, pages 685\u2013693, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Prateek%20Tewari%2C%20Ambuj%20Kar%2C%20Purushottam%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Prateek%20Tewari%2C%20Ambuj%20Kar%2C%20Purushottam%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014"
        },
        {
            "id": "12",
            "entry": "[12] Ali Jalali, Christopher C Johnson, and Pradeep K Ravikumar. On learning discrete graphical models using greedy methods. In Advances in Neural Information Processing Systems, pages 1935\u20131943, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jalali%2C%20Ali%20Johnson%2C%20Christopher%20C.%20Ravikumar%2C%20Pradeep%20K.%20On%20learning%20discrete%20graphical%20models%20using%20greedy%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jalali%2C%20Ali%20Johnson%2C%20Christopher%20C.%20Ravikumar%2C%20Pradeep%20K.%20On%20learning%20discrete%20graphical%20models%20using%20greedy%20methods%202011"
        },
        {
            "id": "13",
            "entry": "[13] Po-Ling Loh, Martin J Wainwright, et al. Support recovery without incoherence: A case for nonconvex regularization. The Annals of Statistics, 45(6):2455\u20132482, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loh%2C%20Po-Ling%20Wainwright%2C%20Martin%20J.%20Support%20recovery%20without%20incoherence%3A%20A%20case%20for%20nonconvex%20regularization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loh%2C%20Po-Ling%20Wainwright%2C%20Martin%20J.%20Support%20recovery%20without%20incoherence%3A%20A%20case%20for%20nonconvex%20regularization%202017"
        },
        {
            "id": "14",
            "entry": "[14] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on computing, 24(2):227\u2013234, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20Balas%20Kausik%20Sparse%20approximate%20solutions%20to%20linear%20systems%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20Balas%20Kausik%20Sparse%20approximate%20solutions%20to%20linear%20systems%201995"
        },
        {
            "id": "15",
            "entry": "[15] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May):1665\u2013 1697, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Restricted%20strong%20convexity%20and%20weighted%20matrix%20completion%3A%20Optimal%20bounds%20with%20noise%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Restricted%20strong%20convexity%20and%20weighted%20matrix%20completion%3A%20Optimal%20bounds%20with%20noise%202012"
        },
        {
            "id": "16",
            "entry": "[16] Y. C. Pati, R. Rezaiifar, Y. C. Pati R. Rezaiifar, and P. S. Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In Proceedings of the 27 th Annual Asilomar Conference on Signals, Systems, and Computers, pages 40\u201344, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pati%2C%20Y.C.%20Rezaiifar%2C%20R.%20Rezaiifar%2C%20Y.C.Pati%20R.%20Krishnaprasad%2C%20P.S.%20Orthogonal%20matching%20pursuit%3A%20Recursive%20function%20approximation%20with%20applications%20to%20wavelet%20decomposition%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pati%2C%20Y.C.%20Rezaiifar%2C%20R.%20Rezaiifar%2C%20Y.C.Pati%20R.%20Krishnaprasad%2C%20P.S.%20Orthogonal%20matching%20pursuit%3A%20Recursive%20function%20approximation%20with%20applications%20to%20wavelet%20decomposition%201993"
        },
        {
            "id": "17",
            "entry": "[17] Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, pages 40\u201344. IEEE, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pati%2C%20Yagyensh%20Chandra%20Rezaiifar%2C%20Ramin%20Krishnaprasad%2C%20Perinkulam%20Sambamurthy%20Orthogonal%20matching%20pursuit%3A%20Recursive%20function%20approximation%20with%20applications%20to%20wavelet%20decomposition%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pati%2C%20Yagyensh%20Chandra%20Rezaiifar%2C%20Ramin%20Krishnaprasad%2C%20Perinkulam%20Sambamurthy%20Orthogonal%20matching%20pursuit%3A%20Recursive%20function%20approximation%20with%20applications%20to%20wavelet%20decomposition%201993"
        },
        {
            "id": "18",
            "entry": "[18] Amela Prelic, Stefan Bleuler, Philip Zimmermann, Anja Wille, Peter B\u00fchlmann, Wilhelm Gruissem, Lars Hennig, Lothar Thiele, and Eckart Zitzler. A systematic comparison and evaluation of biclustering methods for gene expression data. Bioinformatics, 22(9):1122\u20131129, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20systematic%20comparison%20and%20evaluation%20of%20biclustering%20methods%20for%20gene%20expression%20data%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20systematic%20comparison%20and%20evaluation%20of%20biclustering%20methods%20for%20gene%20expression%20data%202006"
        },
        {
            "id": "19",
            "entry": "[19] Pradeep Ravikumar, Martin J Wainwright, John D Lafferty, et al. High-dimensional ising model selection using l1-regularized logistic regression. The Annals of Statistics, 38(3):1287\u20131319, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravikumar%2C%20Pradeep%20Wainwright%2C%20Martin%20J.%20Lafferty%2C%20John%20D.%20High-dimensional%20ising%20model%20selection%20using%20l1-regularized%20logistic%20regression%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravikumar%2C%20Pradeep%20Wainwright%2C%20Martin%20J.%20Lafferty%2C%20John%20D.%20High-dimensional%20ising%20model%20selection%20using%20l1-regularized%20logistic%20regression%202010"
        },
        {
            "id": "20",
            "entry": "[20] Jie Shen and Ping Li. On the iteration complexity of support recovery via hard thresholding pursuit. In International Conference on Machine Learning, pages 3115\u20133124, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Jie%20Li%2C%20Ping%20On%20the%20iteration%20complexity%20of%20support%20recovery%20via%20hard%20thresholding%20pursuit%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Jie%20Li%2C%20Ping%20On%20the%20iteration%20complexity%20of%20support%20recovery%20via%20hard%20thresholding%20pursuit%202017"
        },
        {
            "id": "21",
            "entry": "[21] Jie Shen and Ping Li. Partial hard thresholding: Towards a principled analysis of support recovery. In Advances in Neural Information Processing Systems, pages 3127\u20133137, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Jie%20Li%2C%20Ping%20Partial%20hard%20thresholding%3A%20Towards%20a%20principled%20analysis%20of%20support%20recovery%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Jie%20Li%2C%20Ping%20Partial%20hard%20thresholding%3A%20Towards%20a%20principled%20analysis%20of%20support%20recovery%202017"
        },
        {
            "id": "22",
            "entry": "[22] Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Transactions on information theory, 53(12):4655\u20134666, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20Gilbert%2C%20Anna%20C.%20Signal%20recovery%20from%20random%20measurements%20via%20orthogonal%20matching%20pursuit%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20Gilbert%2C%20Anna%20C.%20Signal%20recovery%20from%20random%20measurements%20via%20orthogonal%20matching%20pursuit%202007"
        },
        {
            "id": "23",
            "entry": "[23] Joel A. Tropp and Anna C. Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE TRANS. INFORM. THEORY, 53:4655\u20134666, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20Gilbert%2C%20Anna%20C.%20Signal%20recovery%20from%20random%20measurements%20via%20orthogonal%20matching%20pursuit%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20Gilbert%2C%20Anna%20C.%20Signal%20recovery%20from%20random%20measurements%20via%20orthogonal%20matching%20pursuit%202007"
        },
        {
            "id": "24",
            "entry": "[24] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained quadratic programming (lasso). IEEE transactions on information theory, 55(5):2183\u20132202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using%20l1-constrained%20quadratic%20programming%20%28lasso%29%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using%20l1-constrained%20quadratic%20programming%20%28lasso%29%202009"
        },
        {
            "id": "25",
            "entry": "[25] Xiaotong Yuan, Ping Li, and Tong Zhang. Exact recovery of hard thresholding pursuit. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3558\u20133566. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Xiaotong%20Li%2C%20Ping%20Zhang%2C%20Tong%20Exact%20recovery%20of%20hard%20thresholding%20pursuit%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Xiaotong%20Li%2C%20Ping%20Zhang%2C%20Tong%20Exact%20recovery%20of%20hard%20thresholding%20pursuit%202016"
        },
        {
            "id": "26",
            "entry": "[26] Tong Zhang. On the consistency of feature selection using greedy least squares regression. Journal of Machine Learning Research, 10(Mar):555\u2013568, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20On%20the%20consistency%20of%20feature%20selection%20using%20greedy%20least%20squares%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20On%20the%20consistency%20of%20feature%20selection%20using%20greedy%20least%20squares%20regression%202009"
        },
        {
            "id": "27",
            "entry": "[27] Tong Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations. IEEE transactions on information theory, 57(7):4689\u20134708, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Adaptive%20forward-backward%20greedy%20algorithm%20for%20learning%20sparse%20representations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Adaptive%20forward-backward%20greedy%20algorithm%20for%20learning%20sparse%20representations%202011"
        },
        {
            "id": "28",
            "entry": "[28] Tong Zhang. Sparse recovery with orthogonal matching pursuit under rip. IEEE Transactions on Information Theory, 57(9):6215\u20136221, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Sparse%20recovery%20with%20orthogonal%20matching%20pursuit%20under%20rip%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Sparse%20recovery%20with%20orthogonal%20matching%20pursuit%20under%20rip%202011"
        },
        {
            "id": "29",
            "entry": "[29] Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Lower bounds on the performance of polynomial-time algorithms for sparse linear regression. In Conference on Learning Theory, pages 921\u2013948, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Lower%20bounds%20on%20the%20performance%20of%20polynomial-time%20algorithms%20for%20sparse%20linear%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Lower%20bounds%20on%20the%20performance%20of%20polynomial-time%20algorithms%20for%20sparse%20linear%20regression%202014"
        },
        {
            "id": "30",
            "entry": "[30] Yuchen Zhang, Martin J Wainwright, Michael I Jordan, et al. Optimal prediction for sparse linear models? lower bounds for coordinate-separable m-estimators. Electronic Journal of Statistics, 11(1):752\u2013799, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Optimal%20prediction%20for%20sparse%20linear%20models%3F%20lower%20bounds%20for%20coordinate-separable%20m-estimators%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Optimal%20prediction%20for%20sparse%20linear%20models%3F%20lower%20bounds%20for%20coordinate-separable%20m-estimators%202017"
        }
    ]
}
