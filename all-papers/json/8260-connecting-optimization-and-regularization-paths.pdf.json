{
    "filename": "8260-connecting-optimization-and-regularization-paths.pdf",
    "metadata": {
        "date": 2018,
        "title": "Connecting Optimization and Regularization Paths",
        "author": "Arun Sai Suggala Carnegie Mellon University",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8260-connecting-optimization-and-regularization-paths.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the implicit regularization properties of optimization techniques by explicitly connecting their optimization paths to the regularization paths of \u201ccorresponding\u201d regularized problems. This surprising connection shows that iterates of optimization techniques such as gradient descent and mirror descent are pointwise close to solutions of appropriately regularized objectives. While such a tight connection between optimization and regularization is of independent intellectual interest, it also has important implications for machine learning: we can port results from regularized estimators to optimization, and vice versa. We investigate one key consequence, that borrows from the well-studied analysis of regularized estimators, to then obtain tight excess risk bounds of the iterates generated by optimization techniques."
    },
    "keywords": [
        {
            "term": "logistic regression",
            "url": "https://en.wikipedia.org/wiki/logistic_regression"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "implicit bias",
            "url": "https://en.wikipedia.org/wiki/implicit_bias"
        },
        {
            "term": "matrix factorization",
            "url": "https://en.wikipedia.org/wiki/matrix_factorization"
        },
        {
            "term": "Ordinary Differential Equation",
            "url": "https://en.wikipedia.org/wiki/Ordinary_Differential_Equation"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "With the recent success of optimization techniques in training over-parametrized deep neural networks, there has been a growing interest in understanding the implicit regularization properties of various optimization techniques",
        "Our results explicitly show that the sequence of iterates produced by iterative optimization techniques such as gradient descent, mirror descent on strongly convex functions, lie pointwise close to the regularization path of a corresponding regularized objective",
        "The tighter bound in Theorem 1 helps us obtain tight generalization bounds and early stopping rules for the iterates of gradient descent",
        "As shown by our theory, excess risk bounds for gradient descent on OLS are composed of two terms, one which increases with t and the other which decreases with t",
        "We studied the connections between the trajectory of the iterates of optimization techniques such as gradient descent, Mirror Descent and regularization path of the corresponding regularized objective",
        "While our analysis in this paper focused on gradient descent, it\u2019d be interesting to study if similar connections hold for other non-stochastic methods such as steepest descent, accelerated gradient descent, Newton\u2019s method and stochastic methods such as stochastic gradient descent"
    ],
    "key_statements": [
        "With the recent success of optimization techniques in training over-parametrized deep neural networks, there has been a growing interest in understanding the implicit regularization properties of various optimization techniques",
        "Our results explicitly show that the sequence of iterates produced by iterative optimization techniques such as gradient descent, mirror descent on strongly convex functions, lie pointwise close to the regularization path of a corresponding regularized objective",
        "For the problem of classification with separable data, we show that for losses with exponentially decaying tails, the optimization path of gradient descent is close to the regularization path of the corresponding regularized objective.\n2 Strongly Convex Loss",
        "The tighter bound in Theorem 1 helps us obtain tight generalization bounds and early stopping rules for the iterates of gradient descent",
        "We utilize the connection between optimization and regularization paths derived in Theorem 1 to provide excess risk bounds of gradient descent iterates",
        "Recent works by <a class=\"ref-link\" id=\"cJi_2018_a\" href=\"#rJi_2018_a\">Ji and Telgarsky [2018</a>], <a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\">Soudry et al [2017</a>] study the behavior of gradient descent on un-regularized logistic regression and show that when the data is separable, gradient descent converges to a max margin solution",
        "As shown by our theory, excess risk bounds for gradient descent on OLS are composed of two terms, one which increases with t and the other which decreases with t",
        "We studied the connections between the trajectory of the iterates of optimization techniques such as gradient descent, Mirror Descent and regularization path of the corresponding regularized objective",
        "For the popularly studied problem of classification with separable data, we showed that the optimization and regularization paths are close to each other",
        "While our analysis in this paper focused on gradient descent, it\u2019d be interesting to study if similar connections hold for other non-stochastic methods such as steepest descent, accelerated gradient descent, Newton\u2019s method and stochastic methods such as stochastic gradient descent"
    ],
    "summary": [
        "With the recent success of optimization techniques in training over-parametrized deep neural networks, there has been a growing interest in understanding the implicit regularization properties of various optimization techniques.",
        "We focus on a particular consequence of our connection: we derive excess risk bounds of the iterates of optimization techniques.",
        "We utilize these results to derive excess risk bounds of iterates of optimization techniques.",
        "We explicitly connect the optimization path of GD and regularization path of L22 penalized objective on strongly convex and smooth functions.",
        "We utilize the connection between optimization and regularization paths derived in Theorem 1 to provide excess risk bounds of GD iterates.",
        "We consider the problem of linear regression and show how a better understanding of the regularized problem, coupled with our connection, helps us obtain a tighter parameter estimation error bound for the iterates of GD.",
        "We provide the following result from <a class=\"ref-link\" id=\"cHsu_et+al_2012_a\" href=\"#rHsu_et+al_2012_a\">Hsu et al [2012</a>] which obtains tight upper bounds on the parameter estimation error of the solution of ridge regression.",
        "Under the assumption that the loss function(\u2713, x) at each x 2 X is m strongly convex and M smooth, stability gives us the following expected risk bounds for \u2713(t)",
        "We focus on classification losses and show that the optimization path of GD and the corresponding regularization path of L22 penalized risk are close to each other.",
        "Recent works by <a class=\"ref-link\" id=\"cJi_2018_a\" href=\"#rJi_2018_a\">Ji and Telgarsky [2018</a>], <a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\"><a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\">Soudry et al [2017</a></a>] study the behavior of gradient descent on un-regularized logistic regression and show that when the data is separable, GD converges to a max margin solution.",
        "Our result shows that the minimizer of the regularized problem (2) converges to max-margin solution at a slow rate.",
        "<a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\"><a class=\"ref-link\" id=\"cSoudry_et+al_2017_a\" href=\"#rSoudry_et+al_2017_a\">Soudry et al [2017</a></a>] analyze gradient descent on exponential loss, with separable data and obtained similar bounds for the iterates of GD.",
        "We studied the connections between the trajectory of the iterates of optimization techniques such as GD, Mirror Descent and regularization path of the corresponding regularized objective.",
        "For the popularly studied problem of classification with separable data, we showed that the optimization and regularization paths are close to each other.",
        "We believe that our results on strongly convex losses can be further improved to obtain tighter connections and better generalization bounds of the iterates.",
        "An interesting direction for future work would be to see if similar connections hold for non-convex problems and specifically the optimization objectives that arise in deep learning."
    ],
    "headline": "We study the implicit regularization properties of optimization techniques by explicitly connecting their optimization paths to the regularization paths of \u201ccorresponding\u201d regularized problems",
    "reference_links": [
        {
            "id": "Banerjee_et+al_2005_a",
            "entry": "Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with bregman divergences. Journal of machine learning research, 6(Oct):1705\u20131749, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20Arindam%20Merugu%2C%20Srujana%20Dhillon%2C%20Inderjit%20S.%20Ghosh%2C%20Joydeep%20Clustering%20with%20bregman%20divergences%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20Arindam%20Merugu%2C%20Srujana%20Dhillon%2C%20Inderjit%20S.%20Ghosh%2C%20Joydeep%20Clustering%20with%20bregman%20divergences%202005"
        },
        {
            "id": "Bousquet_2002_a",
            "entry": "Olivier Bousquet and Andr\u00e9 Elisseeff. Stability and generalization. Journal of machine learning research, 2 (Mar):499\u2013526, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andr%C3%A9%20Stability%20and%20generalization.%20Journal%20of%20machine%20learning%20research%202002-03-02"
        },
        {
            "id": "Bubeck_2015_a",
            "entry": "S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4):231\u2013357, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Y. Chen, C. Jin, and B. Yu. Stability and Convergence Trade-off of Iterative Optimization Algorithms. ArXiv e-prints, April 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Jin%2C%20C.%20Yu%2C%20B.%20Stability%20and%20Convergence%20Trade-off%20of%20Iterative%20Optimization%20Algorithms.%20ArXiv%20e-prints%202018-04"
        },
        {
            "id": "Corless_et+al_1996_a",
            "entry": "Robert M Corless, Gaston H Gonnet, David EG Hare, David J Jeffrey, and Donald E Knuth. On the lambertw function. Advances in Computational mathematics, 5(1):329\u2013359, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Corless%2C%20Robert%20M.%20Gonnet%2C%20Gaston%20H.%20Hare%2C%20David%20E.G.%20Jeffrey%2C%20David%20J.%20On%20the%20lambertw%20function%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Corless%2C%20Robert%20M.%20Gonnet%2C%20Gaston%20H.%20Hare%2C%20David%20E.G.%20Jeffrey%2C%20David%20J.%20On%20the%20lambertw%20function%201996"
        },
        {
            "id": "Friedman_2003_a",
            "entry": "J Friedman and Bogdan E Popescu. Gradient directed regularization for linear regression and classification. Technical report, Citeseer, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20J.%20Popescu%2C%20Bogdan%20E.%20Gradient%20directed%20regularization%20for%20linear%20regression%20and%20classification%202003"
        },
        {
            "id": "Gunasekar_et+al_2018_a",
            "entry": "S. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing Implicit Bias in Terms of Optimization Geometry. ArXiv e-prints, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20S.%20Lee%2C%20J.%20Soudry%2C%20D.%20Srebro%2C%20N.%20Characterizing%20Implicit%20Bias%20in%20Terms%20of%202018-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20S.%20Lee%2C%20J.%20Soudry%2C%20D.%20Srebro%2C%20N.%20Characterizing%20Implicit%20Bias%20in%20Terms%20of%202018-02"
        },
        {
            "id": "Gunasekar_et+al_2017_a",
            "entry": "Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pages 6152\u2013 6160, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017"
        },
        {
            "id": "Hardt_et+al_2015_a",
            "entry": "Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.01240"
        },
        {
            "id": "Hoorfar_2008_a",
            "entry": "Abdolhossein Hoorfar and Mehdi Hassani. Inequalities on the lambert w function and hyperpower function. J. Inequal. Pure and Appl. Math, 9(2):5\u20139, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoorfar%2C%20Abdolhossein%20Hassani%2C%20Mehdi%20Inequalities%20on%20the%20lambert%20w%20function%20and%20hyperpower%20function%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoorfar%2C%20Abdolhossein%20Hassani%2C%20Mehdi%20Inequalities%20on%20the%20lambert%20w%20function%20and%20hyperpower%20function%202008"
        },
        {
            "id": "Hsu_et+al_2012_a",
            "entry": "Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In Conference on Learning Theory, pages 9\u20131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20Random%20design%20analysis%20of%20ridge%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20Random%20design%20analysis%20of%20ridge%20regression%202012"
        },
        {
            "id": "Ji_2018_a",
            "entry": "Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07300"
        },
        {
            "id": "Nacson_et+al_2018_a",
            "entry": "Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01905"
        },
        {
            "id": "Negahban_et+al_2009_a",
            "entry": "Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework for highdimensional analysis of m-estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, pages 1348\u20131356, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20Sahand%20Yu%2C%20Bin%20Wainwright%2C%20Martin%20J.%20Ravikumar%2C%20Pradeep%20K.%20A%20unified%20framework%20for%20highdimensional%20analysis%20of%20m-estimators%20with%20decomposable%20regularizers%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20Sahand%20Yu%2C%20Bin%20Wainwright%2C%20Martin%20J.%20Ravikumar%2C%20Pradeep%20K.%20A%20unified%20framework%20for%20highdimensional%20analysis%20of%20m-estimators%20with%20decomposable%20regularizers%202009"
        },
        {
            "id": "Neu_2018_a",
            "entry": "G. Neu and L. Rosasco. Iterate averaging as regularization for stochastic gradient descent. ArXiv e-prints, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neu%2C%20G.%20Rosasco%2C%20L.%20Iterate%20averaging%20as%20regularization%20for%20stochastic%20gradient%20descent.%20ArXiv%20e-prints%202018-02"
        },
        {
            "id": "Ohayon_2013_a",
            "entry": "Ben Ohayon and Guy Ron. New approaches in designing a zeeman slower. Journal of Instrumentation, 8(02): P02016, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ohayon%2C%20Ben%20Ron%2C%20Guy%20New%20approaches%20in%20designing%20a%20zeeman%20slower%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ohayon%2C%20Ben%20Ron%2C%20Guy%20New%20approaches%20in%20designing%20a%20zeeman%20slower%202013"
        },
        {
            "id": "Raskutti_et+al_2014_a",
            "entry": "Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. Journal of Machine Learning Research, 15(1):335\u2013366, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Early%20stopping%20and%20non-parametric%20regression%3A%20an%20optimal%20data-dependent%20stopping%20rule%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Early%20stopping%20and%20non-parametric%20regression%3A%20an%20optimal%20data-dependent%20stopping%20rule%202014"
        },
        {
            "id": "Rosasco_2015_a",
            "entry": "Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization. In Advances in Neural Information Processing Systems, pages 1630\u20131638, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosasco%2C%20Lorenzo%20Villa%2C%20Silvia%20Learning%20with%20incremental%20iterative%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosasco%2C%20Lorenzo%20Villa%2C%20Silvia%20Learning%20with%20incremental%20iterative%20regularization%202015"
        },
        {
            "id": "Rosset_et+al_2004_a",
            "entry": "Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin classifier. Journal of Machine Learning Research, 5(Aug):941\u2013973, 2004a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosset%2C%20Saharon%20Zhu%2C%20Ji%20Hastie%2C%20Trevor%20Boosting%20as%20a%20regularized%20path%20to%20a%20maximum%20margin%20classifier%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosset%2C%20Saharon%20Zhu%2C%20Ji%20Hastie%2C%20Trevor%20Boosting%20as%20a%20regularized%20path%20to%20a%20maximum%20margin%20classifier%202004"
        },
        {
            "id": "Rosset_et+al_2004_b",
            "entry": "Saharon Rosset, Ji Zhu, and Trevor J Hastie. Margin maximizing loss functions. In Advances in neural information processing systems, pages 1237\u20131244, 2004b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosset%2C%20Saharon%20Zhu%2C%20Ji%20Hastie%2C%20Trevor%20J.%20Margin%20maximizing%20loss%20functions%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosset%2C%20Saharon%20Zhu%2C%20Ji%20Hastie%2C%20Trevor%20J.%20Margin%20maximizing%20loss%20functions%202004"
        },
        {
            "id": "Rudelson_2009_a",
            "entry": "Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics, 62(12):1707\u20131739, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudelson%2C%20Mark%20Vershynin%2C%20Roman%20Smallest%20singular%20value%20of%20a%20random%20rectangular%20matrix%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudelson%2C%20Mark%20Vershynin%2C%20Roman%20Smallest%20singular%20value%20of%20a%20random%20rectangular%20matrix%202009"
        },
        {
            "id": "Soudry_et+al_2017_a",
            "entry": "Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345"
        },
        {
            "id": "Valluri_et+al_2000_a",
            "entry": "Sree Ram Valluri, David J Jeffrey, and Robert M Corless. Some applications of the lambert w function to physics. Canadian Journal of Physics, 78(9):823\u2013831, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valluri%2C%20Sree%20Ram%20Jeffrey%2C%20David%20J.%20Corless%2C%20Robert%20M.%20Some%20applications%20of%20the%20lambert%20w%20function%20to%20physics%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valluri%2C%20Sree%20Ram%20Jeffrey%2C%20David%20J.%20Corless%2C%20Robert%20M.%20Some%20applications%20of%20the%20lambert%20w%20function%20to%20physics%202000"
        },
        {
            "id": "Yao_et+al_2007_a",
            "entry": "Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007"
        }
    ]
}
