{
    "filename": "7983-towards-text-generation-with-adversarially-learned-neural-outlines.pdf",
    "metadata": {
        "title": "Towards Text Generation with Adversarially Learned Neural Outlines",
        "author": "Sandeep Subramanian, Sai Rajeswar Mudumba, Alessandro Sordoni, Adam Trischler, Aaron C. Courville, Chris Pal",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7983-towards-text-generation-with-adversarially-learned-neural-outlines.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Recent progress in deep generative models has been fueled by two paradigms \u2013 autoregressive and adversarial models. We propose a combination of both approaches with the goal of learning generative models of text. Our method first produces a high-level sentence outline and then generates words sequentially, conditioning on both the outline and the previous outputs. We generate outlines with an adversarial model trained to approximate the distribution of sentences in a latent space induced by general-purpose sentence encoders. This provides strong, informative conditioning for the autoregressive stage. Our quantitative evaluations suggests that conditioning information from generated outlines is able to guide the autoregressive model to produce realistic samples, comparable to maximum-likelihood trained language models, even at high temperatures with multinomial sampling. Qualitative results also demonstrate that this generative procedure yields natural-looking sentences and interpolations."
    },
    "keywords": [
        {
            "term": "general purpose",
            "url": "https://en.wikipedia.org/wiki/general_purpose"
        },
        {
            "term": "adversarial model",
            "url": "https://en.wikipedia.org/wiki/adversarial_model"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "high level",
            "url": "https://en.wikipedia.org/wiki/high_level"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "kernel density estimator",
            "url": "https://en.wikipedia.org/wiki/kernel_density_estimator"
        },
        {
            "term": "language model",
            "url": "https://en.wikipedia.org/wiki/language_model"
        },
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "text generation",
            "url": "https://en.wikipedia.org/wiki/text_generation"
        }
    ],
    "highlights": [
        "Deep neural networks are powerful tools for modeling sequential data [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We model this distribution using a Generative Adversarial Network (GAN), which enables us to build a fully generative model of sentences as follows: we first sample a sentence embedding from the Generative Adversarial Network generator, decode it to the observed space using a conditional GRU-based language model",
        "We propose the use of fixed-length representations induced by general-purpose sentence encoders for training generative models of text, and demonstrate their potential both qualitatively and quantitatively",
        "We investigate and demonstrate the potential of leveraging general purpose sentence encoders that produce fixed-length sentence representations to train generative models of sentences",
        "We show that it is possible to train conditional generative models of text that operate by manipulating and transforming sentences entirely in the latent space",
        "We show that smooth transitions arise in the observed space when moving linearly along the input space of the Generative Adversarial Network generator"
    ],
    "key_statements": [
        "Deep neural networks are powerful tools for modeling sequential data [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We study how pre-trained sentence representations, obtained by general purpose sentence encoders, can be leveraged to train better models for text generation",
        "We model this distribution using a Generative Adversarial Network (GAN), which enables us to build a fully generative model of sentences as follows: we first sample a sentence embedding from the Generative Adversarial Network generator, decode it to the observed space using a conditional GRU-based language model",
        "We propose the use of fixed-length representations induced by general-purpose sentence encoders for training generative models of text, and demonstrate their potential both qualitatively and quantitatively",
        "We investigate and demonstrate the potential of leveraging general purpose sentence encoders that produce fixed-length sentence representations to train generative models of sentences",
        "We show that it is possible to train conditional generative models of text that operate by manipulating and transforming sentences entirely in the latent space",
        "We show that smooth transitions arise in the observed space when moving linearly along the input space of the Generative Adversarial Network generator"
    ],
    "summary": [
        "Deep neural networks are powerful tools for modeling sequential data [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>].",
        "General-purpose sentence encoders have been shown to produce representations that are useful across a wide range of natural language processing (NLP) tasks [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>].",
        "We propose the use of fixed-length representations induced by general-purpose sentence encoders for training generative models of text, and demonstrate their potential both qualitatively and quantitatively.",
        "We train a conditional GAN that learns to transform a given hypothesis representation in the sentence embedding space into a premise embedding that satisfies a specified entailment relationship.",
        "We argue that generative adversarial training on latent representations of a sentence not only alleviates the non-differentiability issue of regular GAN training on discrete sequences, but eases the learning process by instead modeling an already smoothed manifold of the underlying discrete data distribution.",
        "Extensions of the skip-gram model to sentences [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], sequential de-noising autoencoders [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], features learned from Natural Language Inference models [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], and large-scale multi-task models that are trained with multiple objectives [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] have been shown to learn useful [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c56\" href=\"#r56\">56</a>] fixed-length representations of text.",
        "We will use the pre-trained sentence encoder from [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], which consists of an embedding look-up table learned from scratch and a single layer GRU [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] with 2048 hidden units.",
        "Given generated or real sentences in their fixed-length latent space, we would like to train a model that maps these vectors back to their respective sentences.",
        "We train a conditional GRU language model that, for each sentence x, conditions on the sentence vector representation, hx, as well as previously generated words at each step to reconstruct x.",
        "To encourage better generalization for samples from G(z), during the training of the decoder we smooth the distribution P by adding a small amount of additive isotropic Gaussian noise to each vector hx.",
        "We show samples from decoders trained with sensitive to small perturbations to examples different amounts of noise but always conditioned on on the data manifold.",
        "We partition the dataset into sized halves, one on which we train our generative model (GAN & Decoder) and the other for evaluation.",
        "Qualitative examples shown in in Table 8 and Supplementary Table 4 indicate that the model is able to capture some simple aspects of the mapping from sentiment or entailment labels to generated sentences/premises.",
        "We show that it is possible to train conditional generative models of text that operate by manipulating and transforming sentences entirely in the latent space.",
        "Ablating the coefficient of the adversarial term in this context, can shed some light on the impact of the autoregressive component and we hope to look into this in future work"
    ],
    "headline": "We propose a combination of both approaches with the goal of learning generative models of text",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. arXiv preprint arXiv:1608.04207, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04207"
        },
        {
            "id": "2",
            "entry": "[2] Guozhong An. The effects of adding noise during backpropagation training on a generalization performance. Neural computation, 8(3):643\u2013674, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=An%2C%20Guozhong%20The%20effects%20of%20adding%20noise%20during%20backpropagation%20training%20on%20a%20generalization%20performance%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=An%2C%20Guozhong%20The%20effects%20of%20adding%20noise%20during%20backpropagation%20training%20on%20a%20generalization%20performance%201996"
        },
        {
            "id": "3",
            "entry": "[3] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "4",
            "entry": "[4] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.05326"
        },
        {
            "id": "5",
            "entry": "[5] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06349"
        },
        {
            "id": "6",
            "entry": "[6] Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07983"
        },
        {
            "id": "7",
            "entry": "[7] Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1657\u20131668, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Qian%20Zhu%2C%20Xiaodan%20Ling%2C%20Zhen-Hua%20Wei%2C%20Si%20Enhanced%20lstm%20for%20natural%20language%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Qian%20Zhu%2C%20Xiaodan%20Ling%2C%20Zhen-Hua%20Wei%2C%20Si%20Enhanced%20lstm%20for%20natural%20language%20inference%202017"
        },
        {
            "id": "8",
            "entry": "[8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "9",
            "entry": "[9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "10",
            "entry": "[10] Alexi Conneau, German Kruszewski, Guillaume Lample, Lo\u0131c Barrault, and Marco Baroni. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01070"
        },
        {
            "id": "11",
            "entry": "[11] Alexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representations. arXiv preprint arXiv:1803.05449, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05449"
        },
        {
            "id": "12",
            "entry": "[12] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02364"
        },
        {
            "id": "13",
            "entry": "[13] Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.05053"
        },
        {
            "id": "14",
            "entry": "[14] Jesse Engel, Matthew Hoffman, and Adam Roberts. Latent constraints: Learning to generate conditionally from unconditional generative models. arXiv preprint arXiv:1711.05772, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05772"
        },
        {
            "id": "15",
            "entry": "[15] William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via filling in the . arXiv preprint arXiv:1801.07736, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07736"
        },
        {
            "id": "16",
            "entry": "[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "17",
            "entry": "[17] Anirudh Goyal, Nan Rosemary Ke, Alex Lamb, R Devon Hjelm, Chris Pal, Joelle Pineau, and Yoshua Bengio. Actual: Actor-critic under adversarial learning. arXiv preprint arXiv:1711.04755, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04755"
        },
        {
            "id": "18",
            "entry": "[18] Anirudh Goyal ALIAS PARTH GOYAL, Alessandro Sordoni, Marc-Alexandre Cote, Nan Ke, and Yoshua Bengio. Z-forcing: Training stochastic recurrent networks. In Advances in Neural Information Processing Systems, pages 6716\u20136726, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GOYAL%2C%20Anirudh%20Goyal%20A.L.I.A.S.P.A.R.T.H.%20Sordoni%2C%20Alessandro%20Cote%2C%20Marc-Alexandre%20Ke%2C%20Nan%20Z-forcing%3A%20Training%20stochastic%20recurrent%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GOYAL%2C%20Anirudh%20Goyal%20A.L.I.A.S.P.A.R.T.H.%20Sordoni%2C%20Alessandro%20Cote%2C%20Marc-Alexandre%20Ke%2C%20Nan%20Z-forcing%3A%20Training%20stochastic%20recurrent%20networks%202017"
        },
        {
            "id": "19",
            "entry": "[19] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "20",
            "entry": "[20] Kenneth Heafield. Kenlm: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187\u2013197. Association for Computational Linguistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heafield%2C%20Kenneth%20Kenlm%3A%20Faster%20and%20smaller%20language%20model%20queries%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heafield%2C%20Kenneth%20Kenlm%3A%20Faster%20and%20smaller%20language%20model%20queries%202011"
        },
        {
            "id": "21",
            "entry": "[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08500"
        },
        {
            "id": "22",
            "entry": "[22] Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.03483"
        },
        {
            "id": "23",
            "entry": "[23] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, and Tomas Mikolov. Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03651"
        },
        {
            "id": "24",
            "entry": "[24] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "25",
            "entry": "[25] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "26",
            "entry": "[26] Yoon Kim, Kelly Zhang, Alexander M Rush, Yann LeCun, et al. Adversarially regularized autoencoders for generating discrete structures. arXiv preprint arXiv:1706.04223, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04223"
        },
        {
            "id": "27",
            "entry": "[27] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "28",
            "entry": "[28] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "29",
            "entry": "[29] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015"
        },
        {
            "id": "30",
            "entry": "[30] Vladyslav Kolesnyk, Tim Rocktaschel, and Sebastian Riedel. Generating natural language inference chains. arXiv preprint arXiv:1606.01404, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01404"
        },
        {
            "id": "31",
            "entry": "[31] Matt J Kusner and Jose Miguel Hernandez-Lobato. Gans for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint arXiv:1611.04051, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04051"
        },
        {
            "id": "32",
            "entry": "[32] Guillaume Lample, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00043"
        },
        {
            "id": "33",
            "entry": "[33] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05644"
        },
        {
            "id": "34",
            "entry": "[34] Alexander Patrick Mathews, Lexing Xie, and Xuming He. Senticap: Generating image descriptions with sentiments. In AAAI, pages 3574\u20133580, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathews%2C%20Alexander%20Patrick%20Lexing%20Xie%2C%20and%20Xuming%20He.%20Senticap%3A%20Generating%20image%20descriptions%20with%20sentiments%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathews%2C%20Alexander%20Patrick%20Lexing%20Xie%2C%20and%20Xuming%20He.%20Senticap%3A%20Generating%20image%20descriptions%20with%20sentiments%202016"
        },
        {
            "id": "35",
            "entry": "[35] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02182"
        },
        {
            "id": "36",
            "entry": "[36] Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Karafiat%2C%20Martin%20Burget%2C%20Lukas%20Cernocky%2C%20Jan%20Recurrent%20neural%20network%20based%20language%20model"
        },
        {
            "id": "37",
            "entry": "[37] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "38",
            "entry": "[38] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "39",
            "entry": "[39] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2536\u20132544, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Krahenbuhl%2C%20Philipp%20Donahue%2C%20Jeff%20Darrell%2C%20Trevor%20and%20Alexei%20A%20Efros.%20Context%20encoders%3A%20Feature%20learning%20by%20inpainting%202016"
        },
        {
            "id": "40",
            "entry": "[40] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "41",
            "entry": "[41] Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. Analyzing noise in autoencoders and deep networks. CoRR, abs/1406.1831, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1831"
        },
        {
            "id": "42",
            "entry": "[42] Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, and Lior Wolf. Language generation with recurrent generative adversarial networks without pre-training. arXiv preprint arXiv:1706.01399, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01399"
        },
        {
            "id": "43",
            "entry": "[43] Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal, and Aaron Courville. Adversarial generation of natural language. arXiv preprint arXiv:1705.10929, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10929"
        },
        {
            "id": "44",
            "entry": "[44] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive autoencoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on International Conference on Machine Learning, pages 833\u2013840. Omnipress, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rifai%2C%20Salah%20Vincent%2C%20Pascal%20Muller%2C%20Xavier%20Glorot%2C%20Xavier%20Contractive%20autoencoders%3A%20Explicit%20invariance%20during%20feature%20extraction%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rifai%2C%20Salah%20Vincent%2C%20Pascal%20Muller%2C%20Xavier%20Glorot%2C%20Xavier%20Contractive%20autoencoders%3A%20Explicit%20invariance%20during%20feature%20extraction%202011"
        },
        {
            "id": "45",
            "entry": "[45] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "46",
            "entry": "[46] Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serban%2C%20Iulian%20Vlad%20Sordoni%2C%20Alessandro%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20C.%20Building%20end-to-end%20dialogue%20systems%20using%20generative%20hierarchical%20neural%20network%20models%202016"
        },
        {
            "id": "47",
            "entry": "[47] Samira Shabanian, Devansh Arpit, Adam Trischler, and Yoshua Bengio. Variational bi-lstms. arXiv preprint arXiv:1711.05717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05717"
        },
        {
            "id": "48",
            "entry": "[48] Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from nonparallel text by cross-alignment. In Advances in Neural Information Processing Systems, pages 6833\u20136844, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Tianxiao%20Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Style%20transfer%20from%20nonparallel%20text%20by%20cross-alignment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Tianxiao%20Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Style%20transfer%20from%20nonparallel%20text%20by%20cross-alignment%202017"
        },
        {
            "id": "49",
            "entry": "[49] Yikang Shen, Shawn Tan, Chin-Wei Huang, and Aaron Courville. Generating contradictory, neutral, and entailing sentences. arXiv preprint arXiv:1803.02710, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02710"
        },
        {
            "id": "50",
            "entry": "[50] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "51",
            "entry": "[51] Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00079"
        },
        {
            "id": "52",
            "entry": "[52] Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.01844"
        },
        {
            "id": "53",
            "entry": "[53] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein autoencoders. arXiv preprint arXiv:1711.01558, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01558"
        },
        {
            "id": "54",
            "entry": "[54] Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "55",
            "entry": "[55] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "56",
            "entry": "[56] Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07461"
        },
        {
            "id": "57",
            "entry": "[57] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pages 5\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning"
        },
        {
            "id": "58",
            "entry": "[58] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852\u20132858, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Lantao%20Zhang%2C%20Weinan%20Wang%2C%20Jun%20Yu%2C%20Yong%20Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Lantao%20Zhang%2C%20Weinan%20Wang%2C%20Jun%20Yu%2C%20Yong%20Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient%202017"
        },
        {
            "id": "59",
            "entry": "[59] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Yukun%20Kiros%2C%20Ryan%20Zemel%2C%20Rich%20Salakhutdinov%2C%20Ruslan%20Aligning%20books%20and%20movies%3A%20Towards%20story-like%20visual%20explanations%20by%20watching%20movies%20and%20reading%20books%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Yukun%20Kiros%2C%20Ryan%20Zemel%2C%20Rich%20Salakhutdinov%2C%20Ruslan%20Aligning%20books%20and%20movies%3A%20Towards%20story-like%20visual%20explanations%20by%20watching%20movies%20and%20reading%20books%202015"
        }
    ]
}
