{
    "filename": "7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.pdf",
    "metadata": {
        "date": 2018,
        "title": "Where Do You Think You\u2019re Going?: Inferring Beliefs about Dynamics from Behavior",
        "author": "Siddharth Reddy, Anca D. Dragan, Sergey Levine Department of Electrical Engineering and Computer Science",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules \u2013 the dynamics \u2013 governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user\u2019s internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences."
    },
    "keywords": [
        {
            "term": "actions per minute",
            "url": "https://en.wikipedia.org/wiki/actions_per_minute"
        },
        {
            "term": "inverse planning",
            "url": "https://en.wikipedia.org/wiki/inverse_planning"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "internal model",
            "url": "https://en.wikipedia.org/wiki/internal_model"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "brain-machine interface",
            "url": "https://en.wikipedia.org/wiki/brain-machine_interface"
        }
    ],
    "highlights": [
        "Inferring intent in sequential decision-making problems has been heavily studied under the framework of inverse reinforcement learning (IRL), which we build on in this work",
        "Subject to \u03b4\u03b8i,\u03c6(s, a) = 0 \u2200i \u2208 {1, 2, ..., n}, s \u2208 S, a \u2208 A, where Didemo are the demonstrations for task i, and \u03c0\u03b8i denotes the action likelihood given by Q\u03b8i and Equation 1.\n1Our algorithm can in principle learn from demonstrations even when the rewards are unknown, but in practice we find that this relaxation usually makes learning the correct internal dynamics too difficult.\n2We assume a discrete action space to simplify our exposition and experiments",
        "We explore another way to regularize the learned internal dynamics: imposing the prior that the learned internal dynamics T\u03c6 should be similar to the known real dynamics T real by restricting the support of T\u03c6(\u00b7|s, a) to states s that are reachable in the real dynamics",
        "MaxCausalEnt inverse reinforcement learning cannot learn the user\u2019s reward function from misguided demonstrations when it makes the standard assumption that the internal dynamics are equal to the real dynamics, but can learn accurate rewards when it instead uses the learned internal dynamics model produced by our algorithm.\n5 User Study and Simulation Experiments",
        "Simulation experiments and a small-scale user study demonstrate the effectiveness of our method at recovering a dynamics model that explains human actions, as well as its utility for applications in shared autonomy and inverse reinforcement learning"
    ],
    "key_statements": [
        "Inferring intent in sequential decision-making problems has been heavily studied under the framework of inverse reinforcement learning (IRL), which we build on in this work",
        "Subject to \u03b4\u03b8i,\u03c6(s, a) = 0 \u2200i \u2208 {1, 2, ..., n}, s \u2208 S, a \u2208 A, where Didemo are the demonstrations for task i, and \u03c0\u03b8i denotes the action likelihood given by Q\u03b8i and Equation 1.\n1Our algorithm can in principle learn from demonstrations even when the rewards are unknown, but in practice we find that this relaxation usually makes learning the correct internal dynamics too difficult.\n2We assume a discrete action space to simplify our exposition and experiments",
        "We explore another way to regularize the learned internal dynamics: imposing the prior that the learned internal dynamics T\u03c6 should be similar to the known real dynamics T real by restricting the support of T\u03c6(\u00b7|s, a) to states s that are reachable in the real dynamics",
        "We explore two applications: (1) shared autonomy, in which a human and robot collaborate to solve a challenging real-time control task, and (2) learning the reward function of a user who generates suboptimal demonstrations due to internal model misspecification",
        "We instantiate prior work with MaxCausalEnt inverse reinforcement learning [<a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>], which inverts the behavioral model from Equation 1 to infer rewards from demonstrations",
        "MaxCausalEnt inverse reinforcement learning cannot learn the user\u2019s reward function from misguided demonstrations when it makes the standard assumption that the internal dynamics are equal to the real dynamics, but can learn accurate rewards when it instead uses the learned internal dynamics model produced by our algorithm.\n5 User Study and Simulation Experiments",
        "Simulation experiments and a small-scale user study demonstrate the effectiveness of our method at recovering a dynamics model that explains human actions, as well as its utility for applications in shared autonomy and inverse reinforcement learning"
    ],
    "summary": [
        "Inferring intent in sequential decision-making problems has been heavily studied under the framework of inverse reinforcement learning (IRL), which we build on in this work.",
        "We explore two applications: (1) shared autonomy, in which a human and robot collaborate to solve a challenging real-time control task, and (2) learning the reward function of a user who generates suboptimal demonstrations due to internal model misspecification.",
        "We propose an alternative algorithm that assists the user without knowing their reward function by leveraging the internal dynamics model learned by our method.",
        "Most existing inverse reinforcement learning algorithms assume that the user\u2019s internal dynamics are equivalent to the real dynamics, and learn their reward function from near-optimal demonstrations.",
        "MaxCausalEnt IRL cannot learn the user\u2019s reward function from misguided demonstrations when it makes the standard assumption that the internal dynamics are equal to the real dynamics, but can learn accurate rewards when it instead uses the learned internal dynamics model produced by our algorithm.",
        "Standard IRL algorithms, such as MaxCausalEnt IRL [<a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>], can fail to learn rewards from user demonstrations that are \u2018misguided\u2019, i.e., systematically suboptimal in the real world but near-optimal with respect to the user\u2019s internal dynamics.",
        "Figure 3 shows that our algorithm learns an internal dynamics model characterized by a slower game speed than the real dynamics, which makes sense since a slower game speed induces smaller forces and slower motion \u2013 conditions under which the users\u2019 action demonstrations would have been closer to optimal.",
        "These results support our claim that our algorithm can learn an internal dynamics model that explains user actions better than the real dynamics.",
        "The simultaneous estimation of rewards and dynamics (SERD) instantiation of MaxCausalEnt IRL [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] aims to improve the sample efficiency of IRL by forcing the learned real dynamics model to explain observed state transitions as well as actions.",
        "We propose two new internal dynamics regularization techniques, multi-task training and the action intent prior, and demonstrate their utility for learning an internal dynamics model that differs from the real dynamics.",
        "We conduct a user experiment that shows human actions in a game environment can be better explained by a learned internal dynamics model than by the real dynamics, and that augmenting user control with internal-to-real dynamics transfer results in improved game play.",
        "Simulation experiments and a small-scale user study demonstrate the effectiveness of our method at recovering a dynamics model that explains human actions, as well as its utility for applications in shared autonomy and inverse reinforcement learning.",
        "It enables applications that involve intent inference, including adaptive brain-computer interfaces for prosthetic limbs [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>] that help users perform control tasks that are difficult to fully specify"
    ],
    "headline": "We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] George A Akerlof. Procrastination and obedience. The American Economic Review, 81(2):1\u201319, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20George%20A%20Akerlof.%20Procrastination%20and%20obedience%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20George%20A%20Akerlof.%20Procrastination%20and%20obedience%201991"
        },
        {
            "id": "2",
            "entry": "[2] Stuart Armstrong and S\u00f6ren Mindermann. Impossibility of deducing preferences and rationality from human policy. arXiv preprint arXiv:1712.05812, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05812"
        },
        {
            "id": "3",
            "entry": "[3] Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. Action understanding as inverse planning. Cognition, 113(3):329\u2013349, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Chris%20L.%20Saxe%2C%20Rebecca%20Tenenbaum%2C%20Joshua%20B.%20Action%20understanding%20as%20inverse%20planning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Chris%20L.%20Saxe%2C%20Rebecca%20Tenenbaum%2C%20Joshua%20B.%20Action%20understanding%20as%20inverse%20planning%202009"
        },
        {
            "id": "4",
            "entry": "[4] Leon Bergen, Owain Evans, and Joshua Tenenbaum. Learning structured preferences. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 32, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergen%2C%20Leon%20Evans%2C%20Owain%20Tenenbaum%2C%20Joshua%20Learning%20structured%20preferences%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergen%2C%20Leon%20Evans%2C%20Owain%20Tenenbaum%2C%20Joshua%20Learning%20structured%20preferences%202010"
        },
        {
            "id": "5",
            "entry": "[5] Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Constrained%20optimization%20and%20Lagrange%20multiplier%20methods%202014"
        },
        {
            "id": "6",
            "entry": "[6] Michael Bloem and Nicholas Bambos. Infinite time horizon maximum causal entropy inverse reinforcement learning. In Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on, pages 4911\u20134916. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bloem%2C%20Michael%20Bambos%2C%20Nicholas%20Infinite%20time%20horizon%20maximum%20causal%20entropy%20inverse%20reinforcement%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bloem%2C%20Michael%20Bambos%2C%20Nicholas%20Infinite%20time%20horizon%20maximum%20causal%20entropy%20inverse%20reinforcement%20learning%202014"
        },
        {
            "id": "7",
            "entry": "[7] Matthew Botvinick and James An. Goal-directed decision making in prefrontal cortex: a computational framework. In Advances in neural information processing systems, pages 169\u2013176, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Botvinick%2C%20Matthew%20An%2C%20James%20Goal-directed%20decision%20making%20in%20prefrontal%20cortex%3A%20a%20computational%20framework%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Botvinick%2C%20Matthew%20An%2C%20James%20Goal-directed%20decision%20making%20in%20prefrontal%20cortex%3A%20a%20computational%20framework%202009"
        },
        {
            "id": "8",
            "entry": "[8] Alexander Broad, TD Murphey, and Brenna Argall. Learning models for shared control of human-machine systems with unknown dynamics. Robotics: Science and Systems Proceedings, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alexander%20Broad%2C%20T.D.Murphey%20Argall%2C%20Brenna%20Learning%20models%20for%20shared%20control%20of%20human-machine%20systems%20with%20unknown%20dynamics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alexander%20Broad%2C%20T.D.Murphey%20Argall%2C%20Brenna%20Learning%20models%20for%20shared%20control%20of%20human-machine%20systems%20with%20unknown%20dynamics%202017"
        },
        {
            "id": "9",
            "entry": "[9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016"
        },
        {
            "id": "10",
            "entry": "[10] Giuseppe Calafiore and Fabrizio Dabbene. Probabilistic and randomized methods for design under uncertainty. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calafiore%2C%20Giuseppe%20Dabbene%2C%20Fabrizio%20Probabilistic%20and%20randomized%20methods%20for%20design%20under%20uncertainty%202006"
        },
        {
            "id": "11",
            "entry": "[11] Alfonso Caramazza, Michael McCloskey, and Bert Green. Naive beliefs in \u201csophisticated\u201d subjects: Misconceptions about trajectories of objects. Cognition, 9(2):117\u2013123, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caramazza%2C%20Alfonso%20McCloskey%2C%20Michael%20Green%2C%20Bert%20Naive%20beliefs%20in%20%E2%80%9Csophisticated%E2%80%9D%20subjects%3A%20Misconceptions%20about%20trajectories%20of%20objects%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caramazza%2C%20Alfonso%20McCloskey%2C%20Michael%20Green%2C%20Bert%20Naive%20beliefs%20in%20%E2%80%9Csophisticated%E2%80%9D%20subjects%3A%20Misconceptions%20about%20trajectories%20of%20objects%201981"
        },
        {
            "id": "12",
            "entry": "[12] Jose M Carmena. Advances in neuroprosthetic learning and control. PLoS biology, 11(5):e1001561, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmena%2C%20Jose%20M.%20Advances%20in%20neuroprosthetic%20learning%20and%20control%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmena%2C%20Jose%20M.%20Advances%20in%20neuroprosthetic%20learning%20and%20control%202013"
        },
        {
            "id": "13",
            "entry": "[13] Mark Cutler and Jonathan P How. Efficient reinforcement learning for robots using informative simulated priors. In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pages 2605\u20132612. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cutler%2C%20Mark%20How%2C%20Jonathan%20P.%20Efficient%20reinforcement%20learning%20for%20robots%20using%20informative%20simulated%20priors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cutler%2C%20Mark%20How%2C%20Jonathan%20P.%20Efficient%20reinforcement%20learning%20for%20robots%20using%20informative%20simulated%20priors%202015"
        },
        {
            "id": "14",
            "entry": "[14] Michel Desmurget and Scott Grafton. Forward modeling allows feedback control for fast reaching movements. Trends in cognitive sciences, 4(11):423\u2013431, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Desmurget%2C%20Michel%20Grafton%2C%20Scott%20Forward%20modeling%20allows%20feedback%20control%20for%20fast%20reaching%20movements%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Desmurget%2C%20Michel%20Grafton%2C%20Scott%20Forward%20modeling%20allows%20feedback%20control%20for%20fast%20reaching%20movements%202000"
        },
        {
            "id": "15",
            "entry": "[15] Owain Evans and Noah D Goodman. Learning the preferences of bounded agents. In NIPS Workshop on Bounded Optimality, volume 6, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evans%2C%20Owain%20Goodman%2C%20Noah%20D.%20Learning%20the%20preferences%20of%20bounded%20agents%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evans%2C%20Owain%20Goodman%2C%20Noah%20D.%20Learning%20the%20preferences%20of%20bounded%20agents%202015"
        },
        {
            "id": "16",
            "entry": "[16] Owain Evans, Andreas Stuhlm\u00fcller, and Noah D Goodman. Learning the preferences of ignorant, inconsistent agents. In AAAI, pages 323\u2013329, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evans%2C%20Owain%20Stuhlm%C3%BCller%2C%20Andreas%20Goodman%2C%20Noah%20D.%20Learning%20the%20preferences%20of%20ignorant%2C%20inconsistent%20agents%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evans%2C%20Owain%20Stuhlm%C3%BCller%2C%20Andreas%20Goodman%2C%20Noah%20D.%20Learning%20the%20preferences%20of%20ignorant%2C%20inconsistent%20agents%202016"
        },
        {
            "id": "17",
            "entry": "[17] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, pages 49\u201358, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%202016"
        },
        {
            "id": "18",
            "entry": "[18] Jerry A Fodor. A theory of the child\u2019s theory of mind. Cognition, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Jerry%20A%20Fodor.%20A%20theory%20of%20the%20child%E2%80%99s%20theory%20of%20mind%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Jerry%20A%20Fodor.%20A%20theory%20of%20the%20child%E2%80%99s%20theory%20of%20mind%201992"
        },
        {
            "id": "19",
            "entry": "[19] David Fridovich-Keil, Sylvia L Herbert, Jaime F Fisac, Sampada Deglurkar, and Claire J Tomlin. Planning, fast and slow: A framework for adaptive real-time safe trajectory planning. arXiv preprint arXiv:1710.04731, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.04731"
        },
        {
            "id": "20",
            "entry": "[20] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11248"
        },
        {
            "id": "21",
            "entry": "[21] Tobias Gerstenberg and Joshua B Tenenbaum. Intuitive theories. Oxford handbook of causal reasoning, pages 515\u2013548, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerstenberg%2C%20Tobias%20Tenenbaum%2C%20Joshua%20B.%20Intuitive%20theories%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gerstenberg%2C%20Tobias%20Tenenbaum%2C%20Joshua%20B.%20Intuitive%20theories%202017"
        },
        {
            "id": "22",
            "entry": "[22] Matthew Golub, Steven Chase, and M Yu Byron. Learning an internal dynamics model from control demonstration. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 606\u2013614, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20Matthew%20Chase%2C%20Steven%20Byron%2C%20M.Yu%20Learning%20an%20internal%20dynamics%20model%20from%20control%20demonstration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golub%2C%20Matthew%20Chase%2C%20Steven%20Byron%2C%20M.Yu%20Learning%20an%20internal%20dynamics%20model%20from%20control%20demonstration%202013"
        },
        {
            "id": "23",
            "entry": "[23] Alison Gopnik and Henry M Wellman. The theory theory. Mapping the mind: Domain specificity in cognition and culture, page 257, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopnik%2C%20Alison%20Wellman%2C%20Henry%20M.%20The%20theory%20theory.%20Mapping%20the%20mind%3A%20Domain%20specificity%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopnik%2C%20Alison%20Wellman%2C%20Henry%20M.%20The%20theory%20theory.%20Mapping%20the%20mind%3A%20Domain%20specificity%201994"
        },
        {
            "id": "24",
            "entry": "[24] Thomas L Griffiths, Falk Lieder, and Noah D Goodman. Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic. Topics in cognitive science, 7(2):217\u2013229, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffiths%2C%20Thomas%20L.%20Lieder%2C%20Falk%20Goodman%2C%20Noah%20D.%20Rational%20use%20of%20cognitive%20resources%3A%20Levels%20of%20analysis%20between%20the%20computational%20and%20the%20algorithmic%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffiths%2C%20Thomas%20L.%20Lieder%2C%20Falk%20Goodman%2C%20Noah%20D.%20Rational%20use%20of%20cognitive%20resources%3A%20Levels%20of%20analysis%20between%20the%20computational%20and%20the%20algorithmic%202015"
        },
        {
            "id": "25",
            "entry": "[25] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08165"
        },
        {
            "id": "26",
            "entry": "[26] Jessica Hamrick, Peter Battaglia, and Joshua B Tenenbaum. Internal physics models guide probabilistic judgments about object dynamics. In Proceedings of the 33rd annual conference of the cognitive science society, pages 1545\u20131550. Cognitive Science Society Austin, TX, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamrick%2C%20Jessica%20Battaglia%2C%20Peter%20Tenenbaum%2C%20Joshua%20B.%20Internal%20physics%20models%20guide%20probabilistic%20judgments%20about%20object%20dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamrick%2C%20Jessica%20Battaglia%2C%20Peter%20Tenenbaum%2C%20Joshua%20B.%20Internal%20physics%20models%20guide%20probabilistic%20judgments%20about%20object%20dynamics%202011"
        },
        {
            "id": "27",
            "entry": "[27] Sylvia L Herbert, Mo Chen, SooJean Han, Somil Bansal, Jaime F Fisac, and Claire J Tomlin. Fastrack: a modular framework for fast and guaranteed safe motion planning. arXiv preprint arXiv:1703.07373, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07373"
        },
        {
            "id": "28",
            "entry": "[28] Michael Herman, Tobias Gindele, J\u00f6rg Wagner, Felix Schmitt, and Wolfram Burgard. Inverse reinforcement learning with simultaneous estimation of rewards and dynamics. In Artificial Intelligence and Statistics, pages 102\u2013110, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herman%2C%20Michael%20Gindele%2C%20Tobias%20Wagner%2C%20J%C3%B6rg%20Schmitt%2C%20Felix%20Inverse%20reinforcement%20learning%20with%20simultaneous%20estimation%20of%20rewards%20and%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herman%2C%20Michael%20Gindele%2C%20Tobias%20Wagner%2C%20J%C3%B6rg%20Schmitt%2C%20Felix%20Inverse%20reinforcement%20learning%20with%20simultaneous%20estimation%20of%20rewards%20and%20dynamics%202016"
        },
        {
            "id": "29",
            "entry": "[29] Shervin Javdani, Siddhartha S Srinivasa, and J Andrew Bagnell. Shared autonomy via hindsight optimization. arXiv preprint arXiv:1503.07619, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.07619"
        },
        {
            "id": "30",
            "entry": "[30] Mitsuo Kawato. Internal models for motor control and trajectory planning. Current opinion in neurobiology, 9(6):718\u2013727, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawato%2C%20Mitsuo%20Internal%20models%20for%20motor%20control%20and%20trajectory%20planning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawato%2C%20Mitsuo%20Internal%20models%20for%20motor%20control%20and%20trajectory%20planning%201999"
        },
        {
            "id": "31",
            "entry": "[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "32",
            "entry": "[32] Jon Kleinberg and Sigal Oren. Time-inconsistent planning: a computational problem in behavioral economics. In Proceedings of the fifteenth ACM conference on Economics and computation, pages 547\u2013564. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20Jon%20Oren%2C%20Sigal%20Time-inconsistent%20planning%3A%20a%20computational%20problem%20in%20behavioral%20economics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20Jon%20Oren%2C%20Sigal%20Time-inconsistent%20planning%3A%20a%20computational%20problem%20in%20behavioral%20economics%202014"
        },
        {
            "id": "33",
            "entry": "[33] Anirudha Majumdar, Sumeet Singh, Ajay Mandlekar, and Marco Pavone. Risk-sensitive inverse reinforcement learning via coherent risk models. In Robotics: Science and Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Majumdar%2C%20Anirudha%20Singh%2C%20Sumeet%20Mandlekar%2C%20Ajay%20Pavone%2C%20Marco%20Risk-sensitive%20inverse%20reinforcement%20learning%20via%20coherent%20risk%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Majumdar%2C%20Anirudha%20Singh%2C%20Sumeet%20Mandlekar%2C%20Ajay%20Pavone%2C%20Marco%20Risk-sensitive%20inverse%20reinforcement%20learning%20via%20coherent%20risk%20models%202017"
        },
        {
            "id": "34",
            "entry": "[34] Biren Mehta and Stefan Schaal. Forward models in visuomotor control. Journal of Neurophysiology, 88(2):942\u2013953, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mehta%2C%20Biren%20Schaal%2C%20Stefan%20Forward%20models%20in%20visuomotor%20control%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mehta%2C%20Biren%20Schaal%2C%20Stefan%20Forward%20models%20in%20visuomotor%20control%202002"
        },
        {
            "id": "35",
            "entry": "[35] Katharina Muelling, Arun Venkatraman, Jean-Sebastien Valois, John E Downey, Jeffrey Weiss, Shervin Javdani, Martial Hebert, Andrew B Schwartz, Jennifer L Collinger, and J Andrew Bagnell. Autonomy infused teleoperation with application to brain computer interface controlled manipulation. Autonomous Robots, pages 1\u201322, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muelling%2C%20Katharina%20Venkatraman%2C%20Arun%20Valois%2C%20Jean-Sebastien%20Downey%2C%20John%20E.%20Autonomy%20infused%20teleoperation%20with%20application%20to%20brain%20computer%20interface%20controlled%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muelling%2C%20Katharina%20Venkatraman%2C%20Arun%20Valois%2C%20Jean-Sebastien%20Downey%2C%20John%20E.%20Autonomy%20infused%20teleoperation%20with%20application%20to%20brain%20computer%20interface%20controlled%20manipulation%202017"
        },
        {
            "id": "36",
            "entry": "[36] Gergely Neu and Csaba Szepesv\u00e1ri. Apprenticeship learning using inverse reinforcement learning and gradient methods. arXiv preprint arXiv:1206.5264, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.5264"
        },
        {
            "id": "37",
            "entry": "[37] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, pages 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "38",
            "entry": "[38] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "39",
            "entry": "[39] David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515\u2013526, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Premack%2C%20David%20Woodruff%2C%20Guy%20Does%20the%20chimpanzee%20have%20a%20theory%20of%20mind%3F%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Premack%2C%20David%20Woodruff%2C%20Guy%20Does%20the%20chimpanzee%20have%20a%20theory%20of%20mind%3F%201978"
        },
        {
            "id": "40",
            "entry": "[40] Dennis R Proffitt and David L Gilden. Understanding natural dynamics. Journal of Experimental Psychology: Human Perception and Performance, 15(2):384, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Proffitt%2C%20Dennis%20R.%20Gilden%2C%20David%20L.%20Understanding%20natural%20dynamics%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Proffitt%2C%20Dennis%20R.%20Gilden%2C%20David%20L.%20Understanding%20natural%20dynamics%201989"
        },
        {
            "id": "41",
            "entry": "[41] Anna N Rafferty and Thomas L Griffiths. Diagnosing algebra understanding via inverse planning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rafferty%2C%20Anna%20N.%20Griffiths%2C%20Thomas%20L.%20Diagnosing%20algebra%20understanding%20via%20inverse%20planning"
        },
        {
            "id": "42",
            "entry": "[42] Anna N Rafferty, Rachel Jansen, and Thomas L Griffiths. Using inverse planning for personalized feedback. In EDM, pages 472\u2013477, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rafferty%2C%20Anna%20N.%20Jansen%2C%20Rachel%20Griffiths%2C%20Thomas%20L.%20Using%20inverse%20planning%20for%20personalized%20feedback%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rafferty%2C%20Anna%20N.%20Jansen%2C%20Rachel%20Griffiths%2C%20Thomas%20L.%20Using%20inverse%20planning%20for%20personalized%20feedback%202016"
        },
        {
            "id": "43",
            "entry": "[43] Anna N Rafferty, Michelle M LaMar, and Thomas L Griffiths. Inferring learners\u2019 knowledge from their actions. Cognitive Science, 39(3):584\u2013618, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rafferty%2C%20Anna%20N.%20LaMar%2C%20Michelle%20M.%20Griffiths%2C%20Thomas%20L.%20Inferring%20learners%E2%80%99%20knowledge%20from%20their%20actions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rafferty%2C%20Anna%20N.%20LaMar%2C%20Michelle%20M.%20Griffiths%2C%20Thomas%20L.%20Inferring%20learners%E2%80%99%20knowledge%20from%20their%20actions%202015"
        },
        {
            "id": "44",
            "entry": "[44] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. Urbana, 51(61801):1\u20134, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramachandran%2C%20Deepak%20Amir%2C%20Eyal%20Bayesian%20inverse%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramachandran%2C%20Deepak%20Amir%2C%20Eyal%20Bayesian%20inverse%20reinforcement%20learning%202007"
        },
        {
            "id": "45",
            "entry": "[45] Siddharth Reddy, Sergey Levine, and Anca Dragan. Shared autonomy via deep reinforcement learning. arXiv preprint arXiv:1802.01744, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01744"
        },
        {
            "id": "46",
            "entry": "[46] Wilko Schwarting, Javier Alonso-Mora, Liam Pauli, Sertac Karaman, and Daniela Rus. Parallel autonomy in automated vehicles: Safe motion generation with minimal intervention. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 1928\u20131935. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarting%2C%20Wilko%20Alonso-Mora%2C%20Javier%20Pauli%2C%20Liam%20Karaman%2C%20Sertac%20Parallel%20autonomy%20in%20automated%20vehicles%3A%20Safe%20motion%20generation%20with%20minimal%20intervention%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarting%2C%20Wilko%20Alonso-Mora%2C%20Javier%20Pauli%2C%20Liam%20Karaman%2C%20Sertac%20Parallel%20autonomy%20in%20automated%20vehicles%3A%20Safe%20motion%20generation%20with%20minimal%20intervention%202017"
        },
        {
            "id": "47",
            "entry": "[47] Krishna V Shenoy and Jose M Carmena. Combining decoder design and neural adaptation in brain-machine interfaces. Neuron, 84(4):665\u2013680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shenoy%2C%20Krishna%20V.%20Carmena%2C%20Jose%20M.%20Combining%20decoder%20design%20and%20neural%20adaptation%20in%20brain-machine%20interfaces%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shenoy%2C%20Krishna%20V.%20Carmena%2C%20Jose%20M.%20Combining%20decoder%20design%20and%20neural%20adaptation%20in%20brain-machine%20interfaces%202014"
        },
        {
            "id": "48",
            "entry": "[48] Herbert A Simon. Bounded rationality and organizational learning. Organization science, 2(1):125\u2013134, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simon%2C%20Herbert%20A.%20Bounded%20rationality%20and%20organizational%20learning%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simon%2C%20Herbert%20A.%20Bounded%20rationality%20and%20organizational%20learning%201991"
        },
        {
            "id": "49",
            "entry": "[49] Emanuel Todorov. Optimality principles in sensorimotor control. Nature neuroscience, 7(9):907, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Optimality%20principles%20in%20sensorimotor%20control%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Optimality%20principles%20in%20sensorimotor%20control%202004"
        },
        {
            "id": "50",
            "entry": "[50] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases. science, 185(4157):1124\u20131131, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tversky%2C%20Amos%20Kahneman%2C%20Daniel%20Judgment%20under%20uncertainty%3A%20Heuristics%20and%20biases%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tversky%2C%20Amos%20Kahneman%2C%20Daniel%20Judgment%20under%20uncertainty%3A%20Heuristics%20and%20biases%201974"
        },
        {
            "id": "51",
            "entry": "[51] Eiji Uchibe. Model-free deep inverse reinforcement learning by logistic regression. Neural Processing Letters, pages 1\u201315, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uchibe%2C%20Eiji%20Model-free%20deep%20inverse%20reinforcement%20learning%20by%20logistic%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uchibe%2C%20Eiji%20Model-free%20deep%20inverse%20reinforcement%20learning%20by%20logistic%20regression%202017"
        },
        {
            "id": "52",
            "entry": "[52] Friedrich Wilkening and Trix Cacchione. Children\u2019s intuitive physics. The Wiley-Blackwell Handbook of Childhood Cognitive Development, Second edition, pages 473\u2013496, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilkening%2C%20Friedrich%20Cacchione%2C%20Trix%20Children%E2%80%99s%20intuitive%20physics.%20The%20Wiley-Blackwell%20Handbook%20of%20Childhood%20Cognitive%20Development%2C%20Second%20edition%202010"
        },
        {
            "id": "53",
            "entry": "[53] Daniel M Wolpert, Zoubin Ghahramani, and Michael I Jordan. An internal model for sensorimotor integration. Science, 269(5232):1880\u20131882, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolpert%2C%20Daniel%20M.%20Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20An%20internal%20model%20for%20sensorimotor%20integration%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolpert%2C%20Daniel%20M.%20Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20An%20internal%20model%20for%20sensorimotor%20integration%201995"
        },
        {
            "id": "54",
            "entry": "[54] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforcement learning. arXiv preprint arXiv:1507.04888, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.04888"
        },
        {
            "id": "55",
            "entry": "[55] Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of maximum causal entropy. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1255\u20131262. Omnipress, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Modeling%20interaction%20via%20the%20principle%20of%20maximum%20causal%20entropy%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Modeling%20interaction%20via%20the%20principle%20of%20maximum%20causal%20entropy%202010"
        },
        {
            "id": "56",
            "entry": "[56] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        },
        {
            "id": "57",
            "entry": "[57] Brian D Ziebart, Nathan Ratliff, Garratt Gallagher, Christoph Mertz, Kevin Peterson, J Andrew Bagnell, Martial Hebert, Anind K Dey, and Siddhartha Srinivasa. Planning-based prediction for pedestrians. In Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, pages 3931\u20133936. IEEE, 2009. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Ratliff%2C%20Nathan%20Gallagher%2C%20Garratt%20Mertz%2C%20Christoph%20Planning-based%20prediction%20for%20pedestrians%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Ratliff%2C%20Nathan%20Gallagher%2C%20Garratt%20Mertz%2C%20Christoph%20Planning-based%20prediction%20for%20pedestrians%202009"
        }
    ]
}
