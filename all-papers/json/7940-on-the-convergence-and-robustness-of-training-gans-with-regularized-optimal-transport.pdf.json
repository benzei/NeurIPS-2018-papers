{
    "filename": "7940-on-the-convergence-and-robustness-of-training-gans-with-regularized-optimal-transport.pdf",
    "metadata": {
        "date": 2018,
        "title": "On the Convergence and Robustness of Training GANs with Regularized Optimal Transport",
        "author": "Maziar Sanjabi University of Southern California sanjabi@usc.edu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7940-on-the-convergence-and-robustness-of-training-gans-with-regularized-optimal-transport.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10 images. Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power."
    },
    "keywords": [
        {
            "term": "gradient flow",
            "url": "https://en.wikipedia.org/wiki/gradient_flow"
        },
        {
            "term": "wasserstein distance",
            "url": "https://en.wikipedia.org/wiki/wasserstein_distance"
        },
        {
            "term": "probability distribution",
            "url": "https://en.wikipedia.org/wiki/probability_distribution"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "time scale",
            "url": "https://en.wikipedia.org/wiki/time_scale"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "The introduction of Wasserstein distance as a metric for Generative Adversarial Networks re-surged the interest in the field of optimal transport [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>]. [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] provided a game-representation for their proposed Wasserstein Generative Adversarial Networks formulation based on the dual form of the resulting optimal transport problem",
        "It is worth noting that the dual formulation of our regularized Wasserstein Generative Adversarial Networks is an unconstrained smooth convex problem in the functional domain",
        "We prove that the regularized Wasserstein distance, when used in Generative Adversarial Networks problems, is smooth with respect to the generator parameters",
        "Having approximate first order information and smoothness, we prove the convergence of vanilla stochastic gradient descent (SGD) method to a stationary solution",
        "Specially [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] which uses Sinkhorn algorithm to solve regularized Optimal Transport, Smoothed WGAN methods are capable of generating higher quality images"
    ],
    "key_statements": [
        "[<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] provided a game-representation for their proposed Wasserstein Generative Adversarial Networks formulation based on the dual form of the resulting optimal transport problem",
        "We propose to use a smooth surrogate for the Wasserstein distance",
        "The introduction of Wasserstein distance as a metric for Generative Adversarial Networks re-surged the interest in the field of optimal transport [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>]. [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] provided a game-representation for their proposed Wasserstein Generative Adversarial Networks formulation based on the dual form of the resulting optimal transport problem",
        "It is worth noting that the dual formulation of our regularized Wasserstein Generative Adversarial Networks is an unconstrained smooth convex problem in the functional domain",
        "We study the problem of solving Wasserstein Generative Adversarial Networks from optimization perspective",
        "We prove that the regularized Wasserstein distance, when used in Generative Adversarial Networks problems, is smooth with respect to the generator parameters",
        "Having approximate first order information and smoothness, we prove the convergence of vanilla stochastic gradient descent (SGD) method to a stationary solution",
        "Our results suggests that converging to stationarity of the final solution not only depends on the number of steps in the generator, but depends on the quality of solving the discriminator problem",
        "Note that our convergence result relies on the smoothness of regularized Wasserstein distance with respect to the generator parameters",
        "Blondel et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] noted that using strongly convex regularizers on the optimal transport problem would result in an unconstrained dual formulation which is computationally easier to solve",
        "We show that this loss does not introduce bias into finding the correct generative model regardless of the amount of regularization",
        "We describe one such algorithm based on the vanilla mini-batch stochastic gradient descent and state its convergence guarantee",
        "Comparing Smoothed WGAN methods with and without latent representation, we find that the ones with latent representation perform better",
        "Specially [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] which uses Sinkhorn algorithm to solve regularized Optimal Transport, Smoothed WGAN methods are capable of generating higher quality images",
        "Due to high computational cost, we only evaluate latent Sinkhorn loss Smoothed WGAN with L1 and Cosine cost on CIFAR-10"
    ],
    "summary": [
        "[<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] provided a game-representation for their proposed Wasserstein GAN formulation based on the dual form of the resulting optimal transport problem.",
        "It is worth noting that the dual formulation of our regularized Wasserstein GAN is an unconstrained smooth convex problem in the functional domain.",
        "We prove that the regularized Wasserstein distance, when used in GAN problems, is smooth with respect to the generator parameters.",
        "We prove that by approximately solving the regularized Wasserstein distance, we can control the error in the computation of the gradients for the generator.",
        "Our results suggests that converging to stationarity of the final solution not only depends on the number of steps in the generator, but depends on the quality of solving the discriminator problem.",
        "Note that our convergence result relies on the smoothness of regularized Wasserstein distance with respect to the generator parameters.",
        "Blondel et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] noted that using strongly convex regularizers on the optimal transport problem would result in an unconstrained dual formulation which is computationally easier to solve.",
        "This unconstrained form is essential in using parametric methods, such as neural networks, for solving regularized optimal transport problems, as observed in [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>].",
        "Note that \u21e1 may not be a feasible transport plan, (6) can be used to compute an approximate gradient of the generator problem, as discussed in Section 4.",
        "We assume that at each iteration of the procedure for finding the optimal generator, we have access to an oracle which solves the resulting dual of regularized optimal transport to some predefined accuracy.",
        "Under the same assumptions as in Theorem 3.1, let ( , ) be an \u270f-accurate solution to the dual formulation of regularized optimal transport for a given \u2713.",
        "When using regularized Wasserstein distance h (\u2713) as an objective for generative models, one needs to use very small value of as noted by [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>].",
        "By replacing and in the convergence guarantees of Theorem 4.2 withandrespectively, we obtain a convergence guarantee for the SGD based method, described above, for solving the generative Sinkhorn loss optimization problem (10).",
        "Specially [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] which uses Sinkhorn algorithm to solve regularized OT, SWGAN methods are capable of generating higher quality images.",
        "Learning CIFAR-10 images is a more challenging problem than MNIST; and as we predicted in Section 4.1 SWGAN methods with regularized OT objective cannot generate high quality samples, even with carefully tuned hyper-parameters; see Fig. 4 in Appendix."
    ],
    "headline": "We show that obtaining gradient information of the smoothed Wasserstein Generative Adversarial Networks formulation, which is based on regularized Optimal Transport , is computationally effortless and one can apply first order optimization methods to minimize this objective",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] Z. Allen-Zhu and Y. Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex objectives. In International conference on machine learning, pages 1080\u20131089, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Yuan%2C%20Y.%20Improved%20svrg%20for%20non-strongly-convex%20or%20sum-of-non-convex%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Yuan%2C%20Y.%20Improved%20svrg%20for%20non-strongly-convex%20or%20sum-of-non-convex%20objectives%202016"
        },
        {
            "id": "3",
            "entry": "[3] J. Altschuler, J. Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal transport via sinkhorn iteration. In Advances in Neural Information Processing Systems, pages 1961\u20131971, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altschuler%2C%20J.%20Weed%2C%20J.%20Rigollet%2C%20P.%20Near-linear%20time%20approximation%20algorithms%20for%20optimal%20transport%20via%20sinkhorn%20iteration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altschuler%2C%20J.%20Weed%2C%20J.%20Rigollet%2C%20P.%20Near-linear%20time%20approximation%20algorithms%20for%20optimal%20transport%20via%20sinkhorn%20iteration%202017"
        },
        {
            "id": "4",
            "entry": "[4] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "5",
            "entry": "[5] M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer, and R. Munos. The cramer distance as a solution to biased wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10743"
        },
        {
            "id": "6",
            "entry": "[6] P. Bernhard and A. Rapaport. On a theorem of danskin with an application to a theorem of von neumannsion. Nonlinear analysis, 24(8):1163\u20131182, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bernhard%2C%20P.%20Rapaport%2C%20A.%20On%20a%20theorem%20of%20danskin%20with%20an%20application%20to%20a%20theorem%20of%20von%20neumannsion%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bernhard%2C%20P.%20Rapaport%2C%20A.%20On%20a%20theorem%20of%20danskin%20with%20an%20application%20to%20a%20theorem%20of%20von%20neumannsion%201995"
        },
        {
            "id": "7",
            "entry": "[7] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. arXiv preprint arXiv:1801.01401, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01401"
        },
        {
            "id": "8",
            "entry": "[8] M. Blondel, V. Seguy, and A. Rolet. Smooth and sparse optimal transport. arXiv preprint arXiv:1710.06276, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06276"
        },
        {
            "id": "9",
            "entry": "[9] V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291\u2013294, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borkar%2C%20V.S.%20Stochastic%20approximation%20with%20two%20time%20scales%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borkar%2C%20V.S.%20Stochastic%20approximation%20with%20two%20time%20scales%201997"
        },
        {
            "id": "10",
            "entry": "[10] O. Bousquet, S. Gelly, I. Tolstikhin, C.-J. Simon-Gabriel, and B. Schoelkopf. From optimal transport to generative modeling: the VEGAN cookbook. arXiv preprint arXiv:1705.07642, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07642"
        },
        {
            "id": "11",
            "entry": "[11] G. Carlier, V. Duval, G. Peyr\u00e9, and B. Schmitzer. Convergence of entropic schemes for optimal transport and gradient flows. SIAM Journal on Mathematical Analysis, 49(2):1385\u20131418, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G.%20Carlier%2C%20V.%20Duval%2C%20G.%20Peyr%C3%A9%20Schmitzer%2C%20B.%20Convergence%20of%20entropic%20schemes%20for%20optimal%20transport%20and%20gradient%20flows%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G.%20Carlier%2C%20V.%20Duval%2C%20G.%20Peyr%C3%A9%20Schmitzer%2C%20B.%20Convergence%20of%20entropic%20schemes%20for%20optimal%20transport%20and%20gradient%20flows%202017"
        },
        {
            "id": "12",
            "entry": "[12] G. Carlier, V. Duval, G. Peyr\u00e9, and B. Schmitzer. Convergence of entropic schemes for optimal transport and gradient flows. SIAM Journal on Mathematical Analysis, 49(2):1385\u20131418, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G.%20Carlier%2C%20V.%20Duval%2C%20G.%20Peyr%C3%A9%20Schmitzer%2C%20B.%20Convergence%20of%20entropic%20schemes%20for%20optimal%20transport%20and%20gradient%20flows%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G.%20Carlier%2C%20V.%20Duval%2C%20G.%20Peyr%C3%A9%20Schmitzer%2C%20B.%20Convergence%20of%20entropic%20schemes%20for%20optimal%20transport%20and%20gradient%20flows%202017"
        },
        {
            "id": "13",
            "entry": "[13] F. Cicalese, L. Gargano, and U. Vaccaro. How to find a joint probability distribution of minimum entropy (almost), given the marginals. arXiv preprint arXiv:1701.05243, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.05243"
        },
        {
            "id": "14",
            "entry": "[14] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pages 2292\u20132300, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20M.%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20M.%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013"
        },
        {
            "id": "15",
            "entry": "[15] C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with optimism. arXiv preprint arXiv:1711.00141, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00141"
        },
        {
            "id": "16",
            "entry": "[16] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.03906"
        },
        {
            "id": "17",
            "entry": "[17] S. Feizi, C. Suh, F. Xia, and D. Tse. Understanding GANs: the LQG setting. arXiv preprint arXiv:1710.10793, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10793"
        },
        {
            "id": "18",
            "entry": "[18] A. Genevay, M. Cuturi, G. Peyr\u00e9, and F. Bach. Stochastic optimization for large-scale optimal transport. In Advances in Neural Information Processing Systems, pages 3440\u20133448, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20Genevay%2C%20M.%20Cuturi%2C%20G.%20Peyr%C3%A9%20Bach%2C%20F.%20Stochastic%20optimization%20for%20large-scale%20optimal%20transport%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20Genevay%2C%20M.%20Cuturi%2C%20G.%20Peyr%C3%A9%20Bach%2C%20F.%20Stochastic%20optimization%20for%20large-scale%20optimal%20transport%202016"
        },
        {
            "id": "19",
            "entry": "[19] A. Genevay, G. Peyr\u00e9, and M. Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics, pages 1608\u20131617, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20Genevay%2C%20G.%20Peyr%C3%A9%20Cuturi%2C%20M.%20Learning%20generative%20models%20with%20sinkhorn%20divergences%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20Genevay%2C%20G.%20Peyr%C3%A9%20Cuturi%2C%20M.%20Learning%20generative%20models%20with%20sinkhorn%20divergences%202018"
        },
        {
            "id": "20",
            "entry": "[20] S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "21",
            "entry": "[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "22",
            "entry": "[22] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20GANs%202017"
        },
        {
            "id": "23",
            "entry": "[23] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6629\u20136640, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "24",
            "entry": "[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "25",
            "entry": "[25] P. A. Knight. The sinkhorn\u2013knopp algorithm: convergence and applications. SIAM Journal on Matrix Analysis and Applications, 30(1):261\u2013275, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Knight%2C%20P.A.%20The%20sinkhorn%E2%80%93knopp%20algorithm%3A%20convergence%20and%20applications%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Knight%2C%20P.A.%20The%20sinkhorn%E2%80%93knopp%20algorithm%3A%20convergence%20and%20applications%202008"
        },
        {
            "id": "26",
            "entry": "[26] A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset. online: http://www.cs.toronto.edu/kriz/cifar.html, 2014.",
            "url": "http://www.cs.toronto.edu/kriz/cifar.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Nair%2C%20V.%20Hinton%2C%20G.%20The%20cifar-10%20dataset%202014"
        },
        {
            "id": "27",
            "entry": "[27] J. Li, A. Madry, J. Peebles, and L. Schmidt. Towards understanding the dynamics of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.09884"
        },
        {
            "id": "28",
            "entry": "[28] J. H. Lim and J. C. Ye. Geometric GAN. arXiv preprint arXiv:1705.02894, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02894"
        },
        {
            "id": "29",
            "entry": "[29] L. Mescheder. On the convergence properties of GAN training. arXiv preprint arXiv:1801.04406, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04406"
        },
        {
            "id": "30",
            "entry": "[30] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge? arXiv preprint arXiv:1801.04406, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04406"
        },
        {
            "id": "31",
            "entry": "[31] L. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. In Advances in Neural Information Processing Systems, pages 1823\u20131833, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20numerics%20of%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20numerics%20of%20GANs%202017"
        },
        {
            "id": "32",
            "entry": "[32] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02163"
        },
        {
            "id": "33",
            "entry": "[33] V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. In Advances in Neural Information Processing Systems, pages 5591\u20135600, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20V.%20Kolter%2C%20J.Z.%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20V.%20Kolter%2C%20J.Z.%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "34",
            "entry": "[34] A. Nemirovski. Prox-method with rate of convergence O (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Prox-method%20with%20rate%20of%20convergence%20O%20%281/t%29%20for%20variational%20inequalities%20with%20lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Prox-method%20with%20rate%20of%20convergence%20O%20%281/t%29%20for%20variational%20inequalities%20with%20lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004"
        },
        {
            "id": "35",
            "entry": "[35] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127\u2013152, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Smooth%20minimization%20of%20non-smooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Smooth%20minimization%20of%20non-smooth%20functions%202005"
        },
        {
            "id": "36",
            "entry": "[36] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "37",
            "entry": "[37] S. Nowozin, B. Cseke, and R. Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pages 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20S.%20Cseke%2C%20B.%20Tomioka%2C%20R.%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20S.%20Cseke%2C%20B.%20Tomioka%2C%20R.%20f-GAN%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "38",
            "entry": "[38] E. Posner. Random coding strategies for minimum entropy. IEEE Transactions on Information Theory, 21(4):388\u2013391, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Posner%2C%20E.%20Random%20coding%20strategies%20for%20minimum%20entropy%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Posner%2C%20E.%20Random%20coding%20strategies%20for%20minimum%20entropy%201975"
        },
        {
            "id": "39",
            "entry": "[39] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "40",
            "entry": "[40] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "41",
            "entry": "[41] T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs using optimal transport. arXiv preprint arXiv:1803.05573, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05573"
        },
        {
            "id": "42",
            "entry": "[42] V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet, and M. Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02283"
        },
        {
            "id": "43",
            "entry": "[43] A. Thibault, L. Chizat, C. Dossal, and N. Papadakis. Overrelaxed Sinkhorn-Knopp algorithm for regularized optimal transport. arXiv preprint arXiv:1711.01851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01851"
        },
        {
            "id": "44",
            "entry": "[44] C. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C.%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338%202008"
        }
    ]
}
