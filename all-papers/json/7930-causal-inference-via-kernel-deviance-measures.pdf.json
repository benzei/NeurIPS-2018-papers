{
    "filename": "7930-causal-inference-via-kernel-deviance-measures.pdf",
    "metadata": {
        "title": "Causal Inference via Kernel Deviance Measures",
        "author": "Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7930-causal-inference-via-kernel-deviance-measures.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Discovering the causal structure among a set of variables is a fundamental problem in many areas of science. In this paper, we propose Kernel Conditional Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery method based on purely observational data. From a novel interpretation of the notion of asymmetry between cause and effect, we derive a corresponding asymmetry measure using the framework of reproducing kernel Hilbert spaces. Based on this, we propose three decision rules for causal discovery. We demonstrate the wide applicability and robustness of our method across a range of diverse synthetic datasets. Furthermore, we test our method on real-world time series data and the real-world benchmark dataset T\u00fcbingen Cause-Effect Pairs where we outperform state-of-the-art approaches."
    },
    "keywords": [
        {
            "term": "causal inference",
            "url": "https://en.wikipedia.org/wiki/causal_inference"
        },
        {
            "term": "decision rule",
            "url": "https://en.wikipedia.org/wiki/decision_rule"
        },
        {
            "term": "gross domestic product",
            "url": "https://en.wikipedia.org/wiki/gross_domestic_product"
        },
        {
            "term": "conditional distribution",
            "url": "https://en.wikipedia.org/wiki/conditional_distribution"
        },
        {
            "term": "reproducing kernel Hilbert spaces",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_spaces"
        },
        {
            "term": "directed acyclic graph",
            "url": "https://en.wikipedia.org/wiki/directed_acyclic_graph"
        }
    ],
    "highlights": [
        "Let (X , BX ) and (Y, BY ) be measurable spaces with BX and BY the associated Borel \u03c3-algebras.<br/><br/>Denote by (HX , k) and (HY , l) the reproducing kernel Hilbert spaces of functions defined on X and Y, respectively, and their corresponding kernels",
        "The conditional distributions are represented with conditional entropy, mutual information and a quantification of their variability in terms of the spread of the entropy, variance and skewness for different values of the conditioning variable. This differs from our approach where we base our causal inference method on a novel interpretation of the asymmetry between cause and effect, and based on this derive three decision rules with one of these decision rules relying on classifying feature representations",
        "We consider feature representations based only on conditional distributions which we argue to be more discriminative for inferring the causal direction.\n2All directed acyclic graph that encode the same set of conditional independence relations constitute a Markov equivalence class.\n3 Kernel Conditional Deviance for Causal Inference\n3.1",
        "We proposed a fully nonparametric causal inference method that uses purely observational data and does not postulate a priori assumptions on the functional relationship between the variables or the noise structure",
        "We proposed a novel interpretation of the notion of asymmetry between cause and effect in terms of the variability, across different values of the input, of the minimal description length of programs implementing the data-generating process of conditional distributions",
        "In order to quantify the description length variability, we proposed a flexible measure in terms of the within-set deviance of the reproducing kernel Hilbert spaces norms of conditional mean embeddings and presented three decision rules for causal inference based on direct comparison, ensembling and classification, respectively"
    ],
    "key_statements": [
        "Let (X , BX ) and (Y, BY ) be measurable spaces with BX and BY the associated Borel \u03c3-algebras.<br/><br/>Denote by (HX , k) and (HY , l) the reproducing kernel Hilbert spaces of functions defined on X and Y, respectively, and their corresponding kernels",
        "We develop a fully nonparametric causal inference method to automatically discover causal relationships from purely observational data",
        "Whereas previous work [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] quantifies the asymmetry in terms of the Kolmogorov complexity of the factorization of the joint distribution, we propose to interpret the asymmetry based on the Kolmogorov complexity of the conditional distribution",
        "We propose three decision rules for causal inference based on this criterion",
        "The conditional distributions are represented with conditional entropy, mutual information and a quantification of their variability in terms of the spread of the entropy, variance and skewness for different values of the conditioning variable. This differs from our approach where we base our causal inference method on a novel interpretation of the asymmetry between cause and effect, and based on this derive three decision rules with one of these decision rules relying on classifying feature representations",
        "We consider feature representations based only on conditional distributions which we argue to be more discriminative for inferring the causal direction.\n2All directed acyclic graph that encode the same set of conditional independence relations constitute a Markov equivalence class.\n3 Kernel Conditional Deviance for Causal Inference\n3.1",
        "We develop a fully nonparametric causal discovery method that relies only on observational data",
        "Given the postulate of minimal description length independence which is the basis for causal discovery in KCDC, the only case when the causal direction will not be identifiable for KCDC is the situation where the description length of conditional distributions in both the causal and anticausal direction does not vary with the value of the cause and effect, respectively",
        "In order to showcase the wide applicability and robustness of our proposed approach, we test it extensively on several synthetic datasets spanning a wide range of functional dependencies between cause and effect and different interaction patterns with different kinds of noise",
        "We proposed a fully nonparametric causal inference method that uses purely observational data and does not postulate a priori assumptions on the functional relationship between the variables or the noise structure",
        "We proposed a novel interpretation of the notion of asymmetry between cause and effect in terms of the variability, across different values of the input, of the minimal description length of programs implementing the data-generating process of conditional distributions",
        "In order to quantify the description length variability, we proposed a flexible measure in terms of the within-set deviance of the reproducing kernel Hilbert spaces norms of conditional mean embeddings and presented three decision rules for causal inference based on direct comparison, ensembling and classification, respectively"
    ],
    "summary": [
        "Let (X , BX ) and (Y, BY ) be measurable spaces with BX and BY the associated Borel \u03c3-algebras.<br/><br/>Denote by (HX , k) and (HY , l) the RKHSs of functions defined on X and Y, respectively, and their corresponding kernels.",
        "Using the RKHS framework makes our method readily applicable in situations when trying to infer the causal direction between pairs of random variables taking values in structured or non-Euclidean domains on which a kernel can be defined.",
        "Constraint-based methods assume that the true causal structure can be represented with a directed acyclic graph (DAG) G which they try to infer by analyzing conditional independencies present in the observational data distribution P .",
        "This differs from our approach where we base our causal inference method on a novel interpretation of the asymmetry between cause and effect, and based on this derive three decision rules with one of these decision rules relying on classifying feature representations.",
        "Kernel Conditional Deviance for Causal Inference (KCDC), is based on the assumption that there exists an asymmetry between cause and effect that is inherent in the data-generating process.",
        "Using the RKHS framework makes our method readily applicable in situations when trying to infer the causal direction between two random vectors or pairs of other types of random variables taking values in structured or non-Euclidean domains on which a kernel can be defined.",
        "Due to the equivalence of conditional mean embeddings and vector-valued regressors [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], this is analogous to the requirement for constraining the assumed functional class in structural equation models in order to ensure identifiability [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "Given the postulate of minimal description length independence which is the basis for causal discovery in KCDC, the only case when the causal direction will not be identifiable for KCDC is the situation where the description length of conditional distributions in both the causal and anticausal direction does not vary with the value of the cause and effect, respectively.",
        "We proposed a fully nonparametric causal inference method that uses purely observational data and does not postulate a priori assumptions on the functional relationship between the variables or the noise structure.",
        "We proposed a novel interpretation of the notion of asymmetry between cause and effect in terms of the variability, across different values of the input, of the minimal description length of programs implementing the data-generating process of conditional distributions.",
        "In order to quantify the description length variability, we proposed a flexible measure in terms of the within-set deviance of the RKHS norms of conditional mean embeddings and presented three decision rules for causal inference based on direct comparison, ensembling and classification, respectively.",
        "We tested our method on real-world time series data and the real-world benchmark dataset T\u00fcbingen Cause-Effect Pairs where we outperformed existing state-of-the-art methods by a significant margin"
    ],
    "headline": "We propose Kernel Conditional Deviance for Causal Inference  a fully nonparametric causal discovery method based on purely observational data",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Bauer, B. Sch\u00f6lkopf, and J. Peters. The arrow of time in multivariate time series. In International Conference on Machine Learning, pages 2043\u20132051, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauer%2C%20S.%20Sch%C3%B6lkopf%2C%20B.%20Peters%2C%20J.%20The%20arrow%20of%20time%20in%20multivariate%20time%20series%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bauer%2C%20S.%20Sch%C3%B6lkopf%2C%20B.%20Peters%2C%20J.%20The%20arrow%20of%20time%20in%20multivariate%20time%20series%202016"
        },
        {
            "id": "2",
            "entry": "[2] K. Budhathoki and J. Vreeken. Causal inference by stochastic complexity. arXiv:1702.06776, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06776"
        },
        {
            "id": "3",
            "entry": "[3] D. M. Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507\u2013554, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chickering%2C%20D.M.%20Optimal%20structure%20identification%20with%20greedy%20search%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chickering%2C%20D.M.%20Optimal%20structure%20identification%20with%20greedy%20search%202002"
        },
        {
            "id": "4",
            "entry": "[4] P. Daniusis, D. Janzing, J. Mooij, J. Zscheischler, B. Steudel, K. Zhang, and B. Sch\u00f6lkopf. Inferring deterministic causal relations. arXiv preprint arXiv:1203.3475, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1203.3475"
        },
        {
            "id": "5",
            "entry": "[5] J. A. Fonollosa. Conditional distribution variability measures for causality detection. arXiv preprint arXiv:1601.06680, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06680"
        },
        {
            "id": "6",
            "entry": "[6] T. G\u00e4rtner, J. W. Lloyd, and P. A. Flach. Kernels for structured data. In International Conference on",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%C3%A4rtner%2C%20T.%20Lloyd%2C%20J.W.%20Flach%2C%20P.A.%20Kernels%20for%20structured%20data",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%C3%A4rtner%2C%20T.%20Lloyd%2C%20J.W.%20Flach%2C%20P.A.%20Kernels%20for%20structured%20data"
        },
        {
            "id": "7",
            "entry": "[7] O. Goudet, D. Kalainathan, P. Caillou, D. Lopez-Paz, I. Guyon, M. Sebag, A. Tritas, and P. Tubaro. Learning functional causal models with generative neural networks. arXiv preprint arXiv:1709.05321, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.05321"
        },
        {
            "id": "8",
            "entry": "[8] P. D. Gr\u00fcnwald and P. M. Vit\u00e1nyi. Algorithmic information theory. Handbook of the Philosophy of Information, pages 281\u2013320, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%BCnwald%2C%20P.D.%20Vit%C3%A1nyi%2C%20P.M.%20Algorithmic%20information%20theory%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gr%C3%BCnwald%2C%20P.D.%20Vit%C3%A1nyi%2C%20P.M.%20Algorithmic%20information%20theory%202008"
        },
        {
            "id": "9",
            "entry": "[9] P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Sch\u00f6lkopf. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pages 689\u2013696, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoyer%2C%20P.O.%20Janzing%2C%20D.%20Mooij%2C%20J.M.%20Peters%2C%20J.%20Nonlinear%20causal%20discovery%20with%20additive%20noise%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoyer%2C%20P.O.%20Janzing%2C%20D.%20Mooij%2C%20J.M.%20Peters%2C%20J.%20Nonlinear%20causal%20discovery%20with%20additive%20noise%20models%202009"
        },
        {
            "id": "10",
            "entry": "[10] D. Janzing and B. Scholkopf. Causal inference using the algorithmic markov condition. IEEE Transactions on Information Theory, 56(10):5168\u20135194, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Janzing%2C%20D.%20Scholkopf%2C%20B.%20Causal%20inference%20using%20the%20algorithmic%20markov%20condition%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Janzing%2C%20D.%20Scholkopf%2C%20B.%20Causal%20inference%20using%20the%20algorithmic%20markov%20condition%202010"
        },
        {
            "id": "11",
            "entry": "[11] J. Lemeire and E. Dirkx. Causal models as minimal descriptions of multivariate systems, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lemeire%2C%20J.%20Dirkx%2C%20E.%20Causal%20models%20as%20minimal%20descriptions%20of%20multivariate%20systems%202006"
        },
        {
            "id": "12",
            "entry": "[12] G. Lever, L. Baldassarre, S. Patterson, A. Gretton, M. Pontil, and S. Gr\u00fcnew\u00e4lder. Conditional mean embeddings as regressors. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1823\u20131830, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lever%2C%20G.%20Baldassarre%2C%20L.%20Patterson%2C%20S.%20Gretton%2C%20A.%20Conditional%20mean%20embeddings%20as%20regressors%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lever%2C%20G.%20Baldassarre%2C%20L.%20Patterson%2C%20S.%20Gretton%2C%20A.%20Conditional%20mean%20embeddings%20as%20regressors%202012"
        },
        {
            "id": "13",
            "entry": "[13] D. Lopez-Paz, K. Muandet, B. Sch\u00f6lkopf, and I. Tolstikhin. Towards a learning theory of cause-effect inference. In International Conference on Machine Learning, pages 1452\u20131461, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20D.%20Muandet%2C%20K.%20Sch%C3%B6lkopf%2C%20B.%20Tolstikhin%2C%20I.%20Towards%20a%20learning%20theory%20of%20cause-effect%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20D.%20Muandet%2C%20K.%20Sch%C3%B6lkopf%2C%20B.%20Tolstikhin%2C%20I.%20Towards%20a%20learning%20theory%20of%20cause-effect%20inference%202015"
        },
        {
            "id": "14",
            "entry": "[14] J. Mooij, D. Janzing, J. Peters, and B. Sch\u00f6lkopf. Regression by dependence minimization and its application to causal inference in additive noise models. In Proceedings of the 26th annual international conference on machine learning, pages 745\u2013752. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mooij%2C%20J.%20Janzing%2C%20D.%20Peters%2C%20J.%20Sch%C3%B6lkopf%2C%20B.%20Regression%20by%20dependence%20minimization%20and%20its%20application%20to%20causal%20inference%20in%20additive%20noise%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mooij%2C%20J.%20Janzing%2C%20D.%20Peters%2C%20J.%20Sch%C3%B6lkopf%2C%20B.%20Regression%20by%20dependence%20minimization%20and%20its%20application%20to%20causal%20inference%20in%20additive%20noise%20models%202009"
        },
        {
            "id": "15",
            "entry": "[15] J. M. Mooij, D. Janzing, J. Zscheischler, and B. Sch\u00f6lkopf. Cause-effect pairs repository. 2015. http://webdav.tuebingen.mpg.de/cause-effect/.",
            "url": "http://webdav.tuebingen.mpg.de/cause-effect/"
        },
        {
            "id": "16",
            "entry": "[16] J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Sch\u00f6lkopf. Distinguishing cause from effect using observational data: methods and benchmarks. Journal of Machine Learning Research, 17(32):1\u2013102, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mooij%2C%20J.M.%20Peters%2C%20J.%20Janzing%2C%20D.%20Zscheischler%2C%20J.%20Distinguishing%20cause%20from%20effect%20using%20observational%20data%3A%20methods%20and%20benchmarks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mooij%2C%20J.M.%20Peters%2C%20J.%20Janzing%2C%20D.%20Zscheischler%2C%20J.%20Distinguishing%20cause%20from%20effect%20using%20observational%20data%3A%20methods%20and%20benchmarks%202016"
        },
        {
            "id": "17",
            "entry": "[17] J. Pearl. Causality: models, reasoning, and inference. Cambridge University Press Cambridge, UK:, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearl%2C%20J.%20Causality%3A%20models%2C%20reasoning%2C%20and%20inference%202000"
        },
        {
            "id": "18",
            "entry": "[18] B. Scholkopf and A. J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholkopf%2C%20B.%20Smola%2C%20A.J.%20Learning%20with%20kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202001"
        },
        {
            "id": "19",
            "entry": "[19] S. Shimizu, P. O. Hoyer, A. Hyv\u00e4rinen, and A. Kerminen. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003\u20132030, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shimizu%2C%20S.%20Hoyer%2C%20P.O.%20Hyv%C3%A4rinen%2C%20A.%20Kerminen%2C%20A.%20A%20linear%20non-gaussian%20acyclic%20model%20for%20causal%20discovery%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shimizu%2C%20S.%20Hoyer%2C%20P.O.%20Hyv%C3%A4rinen%2C%20A.%20Kerminen%2C%20A.%20A%20linear%20non-gaussian%20acyclic%20model%20for%20causal%20discovery%202006"
        },
        {
            "id": "20",
            "entry": "[20] P. Spirtes, C. Glymour, R. Scheines, et al. Causation, prediction, and search. MIT Press Books, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spirtes%2C%20P.%20Glymour%2C%20C.%20Scheines%2C%20R.%20Causation%2C%20prediction%2C%20and%20search%202000"
        },
        {
            "id": "21",
            "entry": "[21] O. Stegle, D. Janzing, K. Zhang, J. M. Mooij, and B. Sch\u00f6lkopf. Probabilistic latent variable models for distinguishing between cause and effect. In Advances in Neural Information Processing Systems, pages 1687\u20131695, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stegle%2C%20O.%20Janzing%2C%20D.%20Zhang%2C%20K.%20Mooij%2C%20J.M.%20Probabilistic%20latent%20variable%20models%20for%20distinguishing%20between%20cause%20and%20effect%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stegle%2C%20O.%20Janzing%2C%20D.%20Zhang%2C%20K.%20Mooij%2C%20J.M.%20Probabilistic%20latent%20variable%20models%20for%20distinguishing%20between%20cause%20and%20effect%202010"
        },
        {
            "id": "22",
            "entry": "[22] X. Sun, D. Janzing, and B. Sch\u00f6lkopf. Distinguishing between cause and effect via kernel-based complexity measures for conditional distributions. In ESANN, pages 441\u2013446, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20X.%20Janzing%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Distinguishing%20between%20cause%20and%20effect%20via%20kernel-based%20complexity%20measures%20for%20conditional%20distributions%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20X.%20Janzing%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Distinguishing%20between%20cause%20and%20effect%20via%20kernel-based%20complexity%20measures%20for%20conditional%20distributions%202007"
        },
        {
            "id": "23",
            "entry": "[23] X. Sun, D. Janzing, B. Sch\u00f6lkopf, and K. Fukumizu. A kernel-based causal learning algorithm. In Proceedings of the 24th international conference on Machine learning, pages 855\u2013862. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20X.%20Janzing%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Fukumizu%2C%20K.%20A%20kernel-based%20causal%20learning%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20X.%20Janzing%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Fukumizu%2C%20K.%20A%20kernel-based%20causal%20learning%20algorithm%202007"
        },
        {
            "id": "24",
            "entry": "[24] I. Tsamardinos, L. E. Brown, and C. F. Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. Machine learning, 65(1):31\u201378, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsamardinos%2C%20I.%20Brown%2C%20L.E.%20Aliferis%2C%20C.F.%20The%20max-min%20hill-climbing%20bayesian%20network%20structure%20learning%20algorithm%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsamardinos%2C%20I.%20Brown%2C%20L.E.%20Aliferis%2C%20C.F.%20The%20max-min%20hill-climbing%20bayesian%20network%20structure%20learning%20algorithm%202006"
        },
        {
            "id": "25",
            "entry": "[25] K. Zhang and A. Hyv\u00e4rinen. On the identifiability of the post-nonlinear causal model. In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence, pages 647\u2013655. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20K.%20Hyv%C3%A4rinen%2C%20A.%20On%20the%20identifiability%20of%20the%20post-nonlinear%20causal%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20K.%20Hyv%C3%A4rinen%2C%20A.%20On%20the%20identifiability%20of%20the%20post-nonlinear%20causal%20model%202009"
        },
        {
            "id": "26",
            "entry": "[26] K. Zhang, J. Peters, D. Janzing, and B. Sch\u00f6lkopf. Kernel-based conditional independence test and application in causal discovery. In Proceedings of the 27th Annual Conference on Uncertainty in Artificial Intelligence (uai), 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20K.%20Peters%2C%20J.%20Janzing%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Kernel-based%20conditional%20independence%20test%20and%20application%20in%20causal%20discovery%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20K.%20Peters%2C%20J.%20Janzing%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Kernel-based%20conditional%20independence%20test%20and%20application%20in%20causal%20discovery%202011"
        }
    ]
}
