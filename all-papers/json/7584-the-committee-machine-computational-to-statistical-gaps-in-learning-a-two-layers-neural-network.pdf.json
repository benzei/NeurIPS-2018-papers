{
    "filename": "7584-the-committee-machine-computational-to-statistical-gaps-in-learning-a-two-layers-neural-network.pdf",
    "metadata": {
        "title": "The committee machine: Computational to statistical gaps in learning a two-layers neural network",
        "author": "Benjamin Aubin, Antoine Maillard, jean barbier, Florent Krzakala, Nicolas Macris, Lenka Zdeborov\u00e1",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7584-the-committee-machine-computational-to-statistical-gaps-in-learning-a-two-layers-neural-network.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap."
    },
    "keywords": [
        {
            "term": "phase transition",
            "url": "https://en.wikipedia.org/wiki/phase_transition"
        },
        {
            "term": "committee machine",
            "url": "https://en.wikipedia.org/wiki/committee_machine"
        },
        {
            "term": "statistical physics",
            "url": "https://en.wikipedia.org/wiki/statistical_physics"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and Rademacher [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] worst-case type bounds, there has been a considerable body of theoretical work on calculating the generalization ability of neural networks for data arising from a probabilistic model within the framework of statistical mechanics [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "In other problems where a hard phase was identified, its study boosted the development of algorithms that are able to match the predicted thresholds and we anticipate this will translate to the present model",
        "One of the contributions of this paper is the design of an approximate message passing-type algorithm that is able to achieve the Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of the so-called hard phase",
        "The hard phase is associated with first order phase transitions appearing in the solution of the model",
        "In the case of the committee machine with a large number of hidden neurons we identify a large hard phase in which learning is possible information-theoretically but not efficiently",
        "In other problems where such a hard phase was identified, its study boosted the development of algorithms that are able to match the predicted threshold"
    ],
    "key_statements": [
        "While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and Rademacher [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] worst-case type bounds, there has been a considerable body of theoretical work on calculating the generalization ability of neural networks for data arising from a probabilistic model within the framework of statistical mechanics [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "Of particular interest is the so-called teacher-student approach, where labels are generated by feeding i.i.d. random samples to a neural network architecture and are presented to another neural network that is trained using these data",
        "In other problems where a hard phase was identified, its study boosted the development of algorithms that are able to match the predicted thresholds and we anticipate this will translate to the present model",
        "Two neurons \u2014 Let us discuss how the above results can be used to study the optimal learning in the simplest non-trivial case of a two-layers neural network with two hidden neurons, i.e. when model (1) is Y\u03bc = sign[ sign(",
        "The full line is obtained from the fixed point of the state evolution (SE) of the approximate message passing algorithm (10), corresponding to the extremizer of the replica free entropy (7)",
        "While the information-theoretic and specialization phase transitions were identified in the physics literature on the committee machine [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], the gap between the information-theoretic performance and the performance of approximate message passing \u2014that is conjectured to be optimal among polynomial algorithms\u2014 was not yet discussed in the context of this model",
        "approximate message passing is conjectured to be optimal among all polynomial algorithms and analyzing its state evolution sheds light on possible computational-to-statistical gaps that come hand in hand with 1st order phase transitions",
        "One of the contributions of this paper is the design of an approximate message passing-type algorithm that is able to achieve the Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of the so-called hard phase",
        "The hard phase is associated with first order phase transitions appearing in the solution of the model",
        "In the case of the committee machine with a large number of hidden neurons we identify a large hard phase in which learning is possible information-theoretically but not efficiently",
        "In other problems where such a hard phase was identified, its study boosted the development of algorithms that are able to match the predicted threshold",
        "Note that for larger K > 2 the present approximate message passing algorithm includes higher-dimensional integrals that hamper the speed of the algorithm",
        "Detailed account of the corresponding results are left for future work",
        "Even though we focused in this paper on a two-layers neural network, the analysis and algorithm can be readily extended to a multi-layer setting, see [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], as long as the number of layers as well as the number of hidden neurons in each layer is held constant, and as long as one learns only weights of the first layer, for which the proof already applies"
    ],
    "summary": [
        "While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and Rademacher [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] worst-case type bounds, there has been a considerable body of theoretical work on calculating the generalization ability of neural networks for data arising from a probabilistic model within the framework of statistical mechanics [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "Recent developments in statistical estimation and information theory \u2014in particular of approximate message passing algorithms (AMP) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and a rigorous proof of the replica formula for the optimal generalization error [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]\u2014 allowed to settle these two missing points for single-layer neural networks.",
        "The free entropy \u2014 The central object of study leading to the optimal learning and generalization errors in the present setting is the posterior distribution of the weights: P ({wil}ni,,lK=1 | {X\u03bci, Y\u03bc}m \u03bc,,in=1) =",
        "Two neurons \u2014 Let us discuss how the above results can be used to study the optimal learning in the simplest non-trivial case of a two-layers neural network with two hidden neurons, i.e. when model (1) is Y\u03bc = sign[ sign(",
        "The full line is obtained from the fixed point of the state evolution (SE) of the AMP algorithm (10), corresponding to the extremizer of the replica free entropy (7).",
        "This means that for \u03b1 < \u03b1sGpec(K = 2) the number of samples is too small, and the student-neural network is not able to learn that two different teacher-vectors W1 and W2 were used to generate the observed labels.",
        "While the information-theoretic and specialization phase transitions were identified in the physics literature on the committee machine [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], the gap between the information-theoretic performance and the performance of AMP \u2014that is conjectured to be optimal among polynomial algorithms\u2014 was not yet discussed in the context of this model.",
        "AMP is conjectured to be optimal among all polynomial algorithms and analyzing its state evolution sheds light on possible computational-to-statistical gaps that come hand in hand with 1st order phase transitions.",
        "In the regime of \u03b1 = \u0398(K) for large K the non-specialized fixed point is always stable implying that AMP will not be able to give a lower generalization error than \u03b5plateau.",
        "One of the contributions of this paper is the design of an AMP-type algorithm that is able to achieve the Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of the so-called hard phase.",
        "We conjecture that in the present model over-parametrization will not improve the generalization error achieved by AMP in the Bayes-optimal case.",
        "Even though we focused in this paper on a two-layers neural network, the analysis and algorithm can be readily extended to a multi-layer setting, see [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], as long as the number of layers as well as the number of hidden neurons in each layer is held constant, and as long as one learns only weights of the first layer, for which the proof already applies."
    ],
    "headline": "We provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Vladimir Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20Statistical%20learning%20theory%201998"
        },
        {
            "id": "2",
            "entry": "[2] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "3",
            "entry": "[3] Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning from examples. Physical Review A, 45(8):6056, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seung%2C%20Sebastian%20Sompolinsky%2C%20Haim%20Tishby%2C%20Naftali%20Statistical%20mechanics%20of%20learning%20from%20examples%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seung%2C%20Sebastian%20Sompolinsky%2C%20Haim%20Tishby%2C%20Naftali%20Statistical%20mechanics%20of%20learning%20from%20examples%201992"
        },
        {
            "id": "4",
            "entry": "[4] Timothy LH Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a rule. Reviews of Modern Physics, 65(2):499, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watkin%2C%20Timothy%20L.H.%20Rau%2C%20Albrecht%20Biehl%2C%20Michael%20The%20statistical%20mechanics%20of%20learning%20a%20rule%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watkin%2C%20Timothy%20L.H.%20Rau%2C%20Albrecht%20Biehl%2C%20Michael%20The%20statistical%20mechanics%20of%20learning%20a%20rule%201993"
        },
        {
            "id": "5",
            "entry": "[5] R\u00e9mi Monasson and Riccardo Zecchina. Learning and generalization theories of large committee-machines. Modern Physics Letters B, 9(30):1887\u20131897, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Monasson%2C%20R%C3%A9mi%20Zecchina%2C%20Riccardo%20Learning%20and%20generalization%20theories%20of%20large%20committee-machines%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Monasson%2C%20R%C3%A9mi%20Zecchina%2C%20Riccardo%20Learning%20and%20generalization%20theories%20of%20large%20committee-machines%201995"
        },
        {
            "id": "6",
            "entry": "[6] R\u00e9mi Monasson and Riccardo Zecchina. Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks. Physical review letters, 75(12):2432, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Monasson%2C%20R%C3%A9mi%20Zecchina%2C%20Riccardo%20Weight%20space%20structure%20and%20internal%20representations%3A%20a%20direct%20approach%20to%20learning%20and%20generalization%20in%20multilayer%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Monasson%2C%20R%C3%A9mi%20Zecchina%2C%20Riccardo%20Weight%20space%20structure%20and%20internal%20representations%3A%20a%20direct%20approach%20to%20learning%20and%20generalization%20in%20multilayer%20neural%20networks%201995"
        },
        {
            "id": "7",
            "entry": "[7] Andreas Engel and Christian PL Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Andreas%20den%20Broeck%2C%20Christian%20P.L.Van%20Statistical%20Mechanics%20of%20Learning%202001"
        },
        {
            "id": "8",
            "entry": "[8] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. in ICLR 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "9",
            "entry": "[9] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016. in ICLR 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01838"
        },
        {
            "id": "10",
            "entry": "[10] Charles H Martin and Michael W Mahoney. Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior. arXiv preprint arXiv:1710.09553, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09553"
        },
        {
            "id": "11",
            "entry": "[11] Jean Barbier, Florent Krzakala, Nicolas Macris, L\u00e9o Miolane, and Lenka Zdeborov\u00e1. Phase transitions, optimal errors and optimality of message-passing in generalized linear models. arXiv preprint arXiv:1708.03395, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03395"
        },
        {
            "id": "12",
            "entry": "[12] Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, G\u00e9rard Ben-Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06969"
        },
        {
            "id": "13",
            "entry": "[13] Marc M\u00e9zard, Giorgio Parisi, and Miguel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing Company, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A9zard%2C%20Marc%20Parisi%2C%20Giorgio%20Virasoro%2C%20Miguel%20Spin%20glass%20theory%20and%20beyond%3A%20An%20Introduction%20to%20the%20Replica%20Method%20and%20Its%20Applications%2C%20volume%209%201987"
        },
        {
            "id": "14",
            "entry": "[14] Marc M\u00e9zard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A9zard%2C%20Marc%20Montanari%2C%20Andrea%20Information%2C%20physics%2C%20and%20computation%202009"
        },
        {
            "id": "15",
            "entry": "[15] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914\u201318919, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Maleki%2C%20Arian%20Montanari%2C%20Andrea%20Message-passing%20algorithms%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Maleki%2C%20Arian%20Montanari%2C%20Andrea%20Message-passing%20algorithms%20for%20compressed%20sensing%202009"
        },
        {
            "id": "16",
            "entry": "[16] Sundeep Rangan. Generalized approximate message passing for estimation with random linear mixing. In Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2168\u20132172. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rangan%2C%20Sundeep%20Generalized%20approximate%20message%20passing%20for%20estimation%20with%20random%20linear%20mixing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rangan%2C%20Sundeep%20Generalized%20approximate%20message%20passing%20for%20estimation%20with%20random%20linear%20mixing%202011"
        },
        {
            "id": "17",
            "entry": "[17] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bayati%2C%20Mohsen%20Montanari%2C%20Andrea%20The%20dynamics%20of%20message%20passing%20on%20dense%20graphs%2C%20with%20applications%20to%20compressed%20sensing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bayati%2C%20Mohsen%20Montanari%2C%20Andrea%20The%20dynamics%20of%20message%20passing%20on%20dense%20graphs%2C%20with%20applications%20to%20compressed%20sensing%202011"
        },
        {
            "id": "18",
            "entry": "[18] Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115\u2013144, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Javanmard%2C%20Adel%20Montanari%2C%20Andrea%20State%20evolution%20for%20general%20approximate%20message%20passing%20algorithms%2C%20with%20applications%20to%20spatial%20coupling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Javanmard%2C%20Adel%20Montanari%2C%20Andrea%20State%20evolution%20for%20general%20approximate%20message%20passing%20algorithms%2C%20with%20applications%20to%20spatial%20coupling%202013"
        },
        {
            "id": "19",
            "entry": "[19] Henry Schwarze. Learning a rule in a multilayer neural network. Journal of Physics A: Mathematical and General, 26(21):5781, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarze%2C%20Henry%20Learning%20a%20rule%20in%20a%20multilayer%20neural%20network%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarze%2C%20Henry%20Learning%20a%20rule%20in%20a%20multilayer%20neural%20network%201993"
        },
        {
            "id": "20",
            "entry": "[20] Henry Schwarze and John Hertz. Generalization in a large committee machine. EPL (Europhysics Letters), 20(4):375, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarze%2C%20Henry%20Hertz%2C%20John%20Generalization%20in%20a%20large%20committee%20machine%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarze%2C%20Henry%20Hertz%2C%20John%20Generalization%20in%20a%20large%20committee%20machine%201992"
        },
        {
            "id": "21",
            "entry": "[21] Henry Schwarze and John Hertz. Generalization in fully connected committee machines. EPL (Europhysics Letters), 21(7):785, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarze%2C%20Henry%20Hertz%2C%20John%20Generalization%20in%20fully%20connected%20committee%20machines%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarze%2C%20Henry%20Hertz%2C%20John%20Generalization%20in%20fully%20connected%20committee%20machines%201993"
        },
        {
            "id": "22",
            "entry": "[22] German Mato and Nestor Parga. Generalization properties of multilayered neural networks. Journal of Physics A: Mathematical and General, 25(19):5047, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mato%2C%20German%20Parga%2C%20Nestor%20Generalization%20properties%20of%20multilayered%20neural%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mato%2C%20German%20Parga%2C%20Nestor%20Generalization%20properties%20of%20multilayered%20neural%20networks%201992"
        },
        {
            "id": "23",
            "entry": "[23] David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review E, 52(4):4225, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saad%2C%20David%20Solla%2C%20Sara%20A.%20On-line%20learning%20in%20soft%20committee%20machines%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saad%2C%20David%20Solla%2C%20Sara%20A.%20On-line%20learning%20in%20soft%20committee%20machines%201995"
        },
        {
            "id": "24",
            "entry": "[24] Jean Barbier and Nicolas Macris. The adaptive interpolation method: A simple scheme to prove replica formulas in bayesian inference. To appear in Probability Theory and Related Fields, arXiv preprint http://arxiv.org/abs/1705.02780 [cs.IT], 2017.",
            "url": "http://arxiv.org/abs/1705.02780",
            "arxiv_url": "https://arxiv.org/pdf/1705.02780"
        },
        {
            "id": "25",
            "entry": "[25] David L Donoho, Iain Johnstone, and Andrea Montanari. Accurate prediction of phase transitions in compressed sensing via a connection to minimax denoising. IEEE transactions on information theory, 59(6):3396\u20133433, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Johnstone%2C%20Iain%20Montanari%2C%20Andrea%20Accurate%20prediction%20of%20phase%20transitions%20in%20compressed%20sensing%20via%20a%20connection%20to%20minimax%20denoising%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Johnstone%2C%20Iain%20Montanari%2C%20Andrea%20Accurate%20prediction%20of%20phase%20transitions%20in%20compressed%20sensing%20via%20a%20connection%20to%20minimax%20denoising%202013"
        },
        {
            "id": "26",
            "entry": "[26] Lenka Zdeborov\u00e1 and Florent Krzakala. Statistical physics of inference: thresholds and algorithms. Advances in Physics, 65(5):453\u2013552, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zdeborov%C3%A1%2C%20Lenka%20Krzakala%2C%20Florent%20Statistical%20physics%20of%20inference%3A%20thresholds%20and%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zdeborov%C3%A1%2C%20Lenka%20Krzakala%2C%20Florent%20Statistical%20physics%20of%20inference%3A%20thresholds%20and%20algorithms%202016"
        },
        {
            "id": "27",
            "entry": "[27] Yash Deshpande and Andrea Montanari. Finding hidden cliques of size \\sqrt {N/e} n/e in nearly linear time. Foundations of Computational Mathematics, 15(4):1069\u20131128, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Finding%20hidden%20cliques%20of%20size%20%5Csqrt%20%7BN/e%7D%20n/e%20in%20nearly%20linear%20time%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Finding%20hidden%20cliques%20of%20size%20%5Csqrt%20%7BN/e%7D%20n/e%20in%20nearly%20linear%20time%202015"
        },
        {
            "id": "28",
            "entry": "[28] Afonso S Bandeira, Amelia Perry, and Alexander S Wein. Notes on computational-to-statistical gaps: predictions using statistical physics. arXiv preprint arXiv:1803.11132, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.11132"
        },
        {
            "id": "29",
            "entry": "[29] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08968"
        },
        {
            "id": "30",
            "entry": "[30] Ahmed El Alaoui, Aaditya Ramdas, Florent Krzakala, Lenka Zdeborov\u00e1, and Michael I Jordan. Decoding from pooled data: Sharp information-theoretic bounds. arXiv preprint arXiv:1611.09981, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09981"
        },
        {
            "id": "31",
            "entry": "[31] Ahmed El Alaoui, Aaditya Ramdas, Florent Krzakala, Lenka Zdeborov\u00e1, and Michael I Jordan. Decoding from pooled data: Phase transitions of message passing. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages 2780\u20132784. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alaoui%2C%20Ahmed%20El%20Ramdas%2C%20Aaditya%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Decoding%20from%20pooled%20data%3A%20Phase%20transitions%20of%20message%20passing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alaoui%2C%20Ahmed%20El%20Ramdas%2C%20Aaditya%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Decoding%20from%20pooled%20data%3A%20Phase%20transitions%20of%20message%20passing%202017"
        },
        {
            "id": "32",
            "entry": "[32] Junan Zhu, Dror Baron, and Florent Krzakala. Performance limits for noisy multimeasurement vector problems. IEEE Transactions on Signal Processing, 65(9):2444\u20132454, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Junan%20Baron%2C%20Dror%20Krzakala%2C%20Florent%20Performance%20limits%20for%20noisy%20multimeasurement%20vector%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Junan%20Baron%2C%20Dror%20Krzakala%2C%20Florent%20Performance%20limits%20for%20noisy%20multimeasurement%20vector%20problems%202017"
        },
        {
            "id": "33",
            "entry": "[33] Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas Macris, and Lenka Zdeborov\u00e1. The committee machine: Computational to statistical gaps in learning a two-layers neural network. arXiv:1806.05451, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.05451"
        },
        {
            "id": "34",
            "entry": "[34] Francesco Guerra. Broken replica symmetry bounds in the mean field spin glass model. Communications in mathematical physics, 233(1):1\u201312, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guerra%2C%20Francesco%20Broken%20replica%20symmetry%20bounds%20in%20the%20mean%20field%20spin%20glass%20model%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guerra%2C%20Francesco%20Broken%20replica%20symmetry%20bounds%20in%20the%20mean%20field%20spin%20glass%20model%202003"
        },
        {
            "id": "35",
            "entry": "[35] Michel Talagrand. Spin glasses: a challenge for mathematicians: cavity and mean field models, volume 46. Springer Science & Business Media, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20Michel%20Spin%20glasses%3A%20a%20challenge%20for%20mathematicians%3A%20cavity%20and%20mean%20field%20models%2C%20volume%2046%202003"
        },
        {
            "id": "36",
            "entry": "[36] David J Thouless, Philip W Anderson, and Robert G Palmer. Solution of\u2019solvable model of a spin glass\u2019. Philosophical Magazine, 35(3):593\u2013601, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thouless%2C%20David%20J.%20Anderson%2C%20Philip%20W.%20Palmer%2C%20Robert%20G.%20Solution%20of%E2%80%99solvable%20model%20of%20a%20spin%20glass%E2%80%99%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thouless%2C%20David%20J.%20Anderson%2C%20Philip%20W.%20Palmer%2C%20Robert%20G.%20Solution%20of%E2%80%99solvable%20model%20of%20a%20spin%20glass%E2%80%99%201977"
        },
        {
            "id": "37",
            "entry": "[37] Marc M\u00e9zard. The space of interactions in neural networks: Gardner\u2019s computation with the cavity method. Journal of Physics A: Mathematical and General, 22(12):2181\u20132190, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A9zard%2C%20Marc%20The%20space%20of%20interactions%20in%20neural%20networks%3A%20Gardner%E2%80%99s%20computation%20with%20the%20cavity%20method%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%A9zard%2C%20Marc%20The%20space%20of%20interactions%20in%20neural%20networks%3A%20Gardner%E2%80%99s%20computation%20with%20the%20cavity%20method%201989"
        },
        {
            "id": "38",
            "entry": "[38] Manfred Opper and Ole Winther. Mean field approach to bayes learning in feed-forward neural networks. Physical review letters, 76(11):1964, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opper%2C%20Manfred%20Winther%2C%20Ole%20Mean%20field%20approach%20to%20bayes%20learning%20in%20feed-forward%20neural%20networks%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Opper%2C%20Manfred%20Winther%2C%20Ole%20Mean%20field%20approach%20to%20bayes%20learning%20in%20feed-forward%20neural%20networks%201996"
        },
        {
            "id": "39",
            "entry": "[39] Yoshiyuki Kabashima. Inference from correlated patterns: a unified theory for perceptron learning and linear vector channels. Journal of Physics: Conference Series, 95(1):012001, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kabashima%2C%20Yoshiyuki%20Inference%20from%20correlated%20patterns%3A%20a%20unified%20theory%20for%20perceptron%20learning%20and%20linear%20vector%20channels%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kabashima%2C%20Yoshiyuki%20Inference%20from%20correlated%20patterns%3A%20a%20unified%20theory%20for%20perceptron%20learning%20and%20linear%20vector%20channels%202008"
        },
        {
            "id": "40",
            "entry": "[40] Carlo Baldassi, Alfredo Braunstein, Nicolas Brunel, and Riccardo Zecchina. Efficient supervised learning in networks with binary synapses. Proceedings of the National Academy of Sciences, 104(26):11079\u201311084, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldassi%2C%20Carlo%20Braunstein%2C%20Alfredo%20Brunel%2C%20Nicolas%20Zecchina%2C%20Riccardo%20Efficient%20supervised%20learning%20in%20networks%20with%20binary%20synapses%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldassi%2C%20Carlo%20Braunstein%2C%20Alfredo%20Brunel%2C%20Nicolas%20Zecchina%2C%20Riccardo%20Efficient%20supervised%20learning%20in%20networks%20with%20binary%20synapses%202007"
        },
        {
            "id": "41",
            "entry": "[41] Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas Macris, and Lenka Zdeborov\u00e1. AMP implementation of the committee machine. https://github.com/benjaminaubin/TheCommitteeMachine, 2018.",
            "url": "https://github.com/benjaminaubin/TheCommitteeMachine"
        },
        {
            "id": "42",
            "entry": "[42] Philip Schniter, Sundeep Rangan, and Alyson K Fletcher. Vector approximate message passing for the generalized linear model. In Signals, Systems and Computers, 2016 50th Asilomar Conference on, pages 1525\u20131529. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schniter%2C%20Philip%20Rangan%2C%20Sundeep%20Fletcher%2C%20Alyson%20K.%20Vector%20approximate%20message%20passing%20for%20the%20generalized%20linear%20model%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schniter%2C%20Philip%20Rangan%2C%20Sundeep%20Fletcher%2C%20Alyson%20K.%20Vector%20approximate%20message%20passing%20for%20the%20generalized%20linear%20model%202016"
        },
        {
            "id": "43",
            "entry": "[43] Florent Krzakala, Marc M\u00e9zard, Francois Sausset, Yifan Sun, and Lenka Zdeborov\u00e1. Probabilistic reconstruction in compressed sensing: algorithms, phase diagrams, and threshold achieving matrices. Journal of Statistical Mechanics: Theory and Experiment, 2012(08):P08009, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krzakala%2C%20Florent%20M%C3%A9zard%2C%20Marc%20Sausset%2C%20Francois%20Sun%2C%20Yifan%20Probabilistic%20reconstruction%20in%20compressed%20sensing%3A%20algorithms%2C%20phase%20diagrams%2C%20and%20threshold%20achieving%20matrices%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krzakala%2C%20Florent%20M%C3%A9zard%2C%20Marc%20Sausset%2C%20Francois%20Sun%2C%20Yifan%20Probabilistic%20reconstruction%20in%20compressed%20sensing%3A%20algorithms%2C%20phase%20diagrams%2C%20and%20threshold%20achieving%20matrices%202012"
        },
        {
            "id": "44",
            "entry": "[44] Ulugbek Kamilov, Sundeep Rangan, Michael Unser, and Alyson K Fletcher. Approximate message passing with consistent parameter estimation and applications to sparse learning. In Advances in Neural Information Processing Systems, pages 2438\u20132446, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamilov%2C%20Ulugbek%20Rangan%2C%20Sundeep%20Unser%2C%20Michael%20Fletcher%2C%20Alyson%20K.%20Approximate%20message%20passing%20with%20consistent%20parameter%20estimation%20and%20applications%20to%20sparse%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamilov%2C%20Ulugbek%20Rangan%2C%20Sundeep%20Unser%2C%20Michael%20Fletcher%2C%20Alyson%20K.%20Approximate%20message%20passing%20with%20consistent%20parameter%20estimation%20and%20applications%20to%20sparse%20learning%202012"
        }
    ]
}
