{
    "filename": "7904-on-gradient-regularizers-for-mmd-gans.pdf",
    "metadata": {
        "date": 2018,
        "title": "On gradient regularizers for MMD GANs",
        "author": "Michael Arbel Gatsby Computational Neuroscience Unit",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7904-on-gradient-regularizers-for-mmd-gans.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on 160 \u00d7 160 CelebA and 64 \u00d7 64 unconditional ImageNet."
    },
    "keywords": [
        {
            "term": "ICML",
            "url": "https://en.wikipedia.org/wiki/ICML"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "NIPS",
            "url": "https://en.wikipedia.org/wiki/NIPS"
        },
        {
            "term": "RKHS",
            "url": "https://en.wikipedia.org/wiki/RKHS"
        }
    ],
    "highlights": [
        "There has been an explosion of interest in implicit generative models (IGMs) over the last few years, especially after the introduction of generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "On CelebA, spectral normalization-SWGAN and spectral normalization-SMMDGAN dramatically outperformed the other methods with the same architecture in all three metrics",
        "It is worth noting that spectral normalization-SWGAN far outperformed WGAN-GP on both datasets",
        "Table 1b presents the scores for SMMDGAN and spectral normalization-SMMDGAN trained on ImageNet, and the scores of pre-trained models using BGAN [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] and spectral normalization-generative adversarial networks [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>].5",
        "We studied gradient regularization for Maximum Mean Discrepancy-based critics in implicit generative models, clarifying how previous techniques relate to the DM\u03a8MD loss",
        "We proposed the GradientConstrained Maximum Mean Discrepancy and its approximation the Scaled Maximum Mean Discrepancy, a new loss function for implicit generative models that controls gradient behavior in a principled way and obtains excellent performance in practice"
    ],
    "key_statements": [
        "There has been an explosion of interest in implicit generative models (IGMs) over the last few years, especially after the introduction of generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "We introduce a novel regularization for the Maximum Mean Discrepancy generative adversarial networks critic of [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], which directly targets generator performance, rather than adopting regularization methods intended to approximate Wasserstein distances [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "The correspondence to a Sobolev norm is lost in higher dimensions [52, Ch. 10], but we found the first term to be beneficial in practice",
        "Results Table 1a presents the scores for models trained on both CIFAR-10 and CelebA datasets",
        "On CelebA, spectral normalization-SWGAN and spectral normalization-SMMDGAN dramatically outperformed the other methods with the same architecture in all three metrics",
        "It is worth noting that spectral normalization-SWGAN far outperformed WGAN-GP on both datasets",
        "Table 1b presents the scores for SMMDGAN and spectral normalization-SMMDGAN trained on ImageNet, and the scores of pre-trained models using BGAN [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] and spectral normalization-generative adversarial networks [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>].5",
        "Normalized WGANs / MMDGANs To control for the contribution of the spectral parametrization to the performance, we evaluated variants of MMDGANs, WGANs and SobolevGAN using spectral normalization",
        "WGAN and Sobolev-generative adversarial networks led to unstable training and didn\u2019t converge at all (Figure 10) despite many attempts, while MMDGAN converged but didn\u2019t achieve comparable performance to the proposed method with spectral normalization",
        "We studied gradient regularization for Maximum Mean Discrepancy-based critics in implicit generative models, clarifying how previous techniques relate to the DM\u03a8MD loss",
        "We proposed the GradientConstrained Maximum Mean Discrepancy and its approximation the Scaled Maximum Mean Discrepancy, a new loss function for implicit generative models that controls gradient behavior in a principled way and obtains excellent performance in practice",
        "One interesting area of future study for these distances is their behavior when used to diffuse particles distributed as Q towards particles distributed as P"
    ],
    "summary": [
        "There has been an explosion of interest in implicit generative models (IGMs) over the last few years, especially after the introduction of generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "The recent Sobolev GAN [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] uses a similar constraint on the expected gradient norm, but phrases it as estimating a Sobolev IPM rather than loosely approximating Wasserstein.",
        "We introduce a novel regularization for the MMD GAN critic of [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], which directly targets generator performance, rather than adopting regularization methods intended to approximate Wasserstein distances [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "We first discuss in Section 2 how MMD-based losses can be used to learn implicit generative models, and how a naive approach could fail.",
        "2 Learning implicit generative models with MMD-based losses",
        "More recent MMD GANs [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] achieve better results by using a parameterized family of kernels, {k\u03c8}\u03c8\u2208\u03a8, in the Optimized MMD loss previously studied by [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]: DM\u03a8MD(P, Q) := sup MMDk\u03c8 (P, Q).",
        "If we put a box constraint on \u03c8 [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] or regularize the gradient of the critic function [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], the resulting MMD GAN generally matches or outperforms WGAN-based models.",
        "Proposition 2 shows that an MMD-like discrepancy can be continuous under the weak topology even when optimizing over kernels, if we directly restrict the critic functions to be Lipschitz.",
        "This discrepancy works well in practice: Appendix F.2 shows that optimizing our estimate of DG\u03bc,C\u03a8M,\u03bbMD = sup\u03c8\u2208\u03a8 GCMMD\u03bc,k\u03c8,\u03bb yields a good generative model on MNIST.",
        "Optimizing the SMMD over a kernel family \u03a8, DS\u03bcM,\u03a8M,\u03bbD(P, Q) := sup\u03c8\u2208\u03a8 SMMD\u03bc,k\u03c8,\u03bb(P, Q), gives a distance very different from DM\u03a8MD (3).",
        "Models without explicit gradient control (SN-GAN, SN-MMDGAN, SN-MMGAN-L2, SN-WGAN) fix \u03b3 = 1, for spectral normalization; others learn \u03b3, using a spectral parameterization.",
        "Evaluation To compare the sample quality of different models, we considered three different scores based on the Inception network [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>] trained for ImageNet classification, all using default parameters in the implementation of [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "WGAN and Sobolev-GAN led to unstable training and didn\u2019t converge at all (Figure 10) despite many attempts, while MMDGAN converged but didn\u2019t achieve comparable performance to the proposed method with SN.",
        "This is reinforced by Figure 2 (c), which shows that the expected gradient of the critic network is much better-controlled by SMMD, even when SN is used.",
        "We studied gradient regularization for MMD-based critics in implicit generative models, clarifying how previous techniques relate to the DM\u03a8MD loss.",
        "We proposed the GradientConstrained MMD and its approximation the Scaled MMD, a new loss function for IGMs that controls gradient behavior in a principled way and obtains excellent performance in practice.",
        "Mroueh et al [32, Appendix A.1] began such a study for the Sobolev GAN loss; [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] proved convergence and studied discrete-time approximations"
    ],
    "headline": "We propose a principled method for gradient-based regularization of the critic of generative adversarial networks-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] B. Amos and J. Z. Kolter. \u201cOptNet: Differentiable Optimization as a Layer in Neural Networks.\u201d In: ICML. 2017. arXiv: 1703.00443.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00443"
        },
        {
            "id": "2",
            "entry": "[2] M. Arjovsky and L. Bottou. \u201cTowards Principled Methods for Training Generative Adversarial Networks.\u201d In: ICLR. 2017. arXiv: 1701.04862.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "3",
            "entry": "[3] M. Arjovsky, S. Chintala, and L. Bottou. \u201cWasserstein Generative Adversarial Networks.\u201d In: ICML. 2017. arXiv: 1701.07875.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "4",
            "entry": "[4] S. Barratt and R. Sharma. A Note on the Inception Score. 2018. arXiv: 1801.01973.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "5",
            "entry": "[5] M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer, and R. Munos. The Cramer Distance as a Solution to Biased Wasserstein Gradients. 2017. arXiv: 1705.10743.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10743"
        },
        {
            "id": "6",
            "entry": "[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary Equilibrium Generative Adversarial Networks. 2017. arXiv: 1703.10717.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "7",
            "entry": "[7] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. \u201cDemystifying MMD GANs.\u201d In: ICLR. 2018. arXiv: 1801.01401.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01401"
        },
        {
            "id": "8",
            "entry": "[8] L. Bottou, M. Arjovsky, D. Lopez-Paz, and M. Oquab. \u201cGeometrical Insights for Implicit Generative Modeling.\u201d In: Braverman Readings in Machine Learning: Key Iedas from Inception to Current State. Ed. by L. Rozonoer, B. Mirkin, and I. Muchnik. LNAI Vol. 11100. Springer, 2018, pp. 229\u2013268. arXiv: 1712.07822.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07822"
        },
        {
            "id": "9",
            "entry": "[9] W. Bounliphone, E. Belilovsky, M. B. Blaschko, I. Antonoglou, and A. Gretton. \u201cA Test of Relative Similarity For Model Selection in Generative Models.\u201d In: ICLR. 2016. arXiv: 1511.04581.",
            "arxiv_url": "https://arxiv.org/pdf/1511.04581"
        },
        {
            "id": "10",
            "entry": "[10] O. Bousquet, O. Chapelle, and M. Hein. \u201cMeasure Based Regularization.\u201d In: NIPS. 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20O.%20Chapelle%2C%20O.%20Hein%2C%20M.%20Measure%20Based%20Regularization.%202004"
        },
        {
            "id": "11",
            "entry": "[11] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. \u201cNeural Photo Editing with Introspective Adversarial Networks.\u201d In: ICLR. 2017. arXiv: 1609.07093.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07093"
        },
        {
            "id": "12",
            "entry": "[12] R. M. Dudley. Real Analysis and Probability. 2nd ed. Cambridge University Press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dudley%2C%20R.M.%20Real%20Analysis%20and%20Probability%202002"
        },
        {
            "id": "13",
            "entry": "[13] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. \u201cTraining generative neural networks via Maximum Mean Discrepancy optimization.\u201d In: UAI. 2015. arXiv: 1505.03906.",
            "arxiv_url": "https://arxiv.org/pdf/1505.03906"
        },
        {
            "id": "14",
            "entry": "[14] A. Genevay, G. Peyr\u00e9, and M. Cuturi. \u201cLearning Generative Models with Sinkhorn Divergences.\u201d In: AISTATS. 2018. arXiv: 1706.00292.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00292"
        },
        {
            "id": "15",
            "entry": "[15] T. Gneiting and A. E. Raftery. \u201cStrictly proper scoring rules, prediction, and estimation.\u201d In: JASA 102.477 (2007), pp. 359\u2013378.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gneiting%2C%20T.%20Raftery%2C%20A.E.%20Strictly%20proper%20scoring%20rules%2C%20prediction%2C%20and%20estimation.%202007"
        },
        {
            "id": "16",
            "entry": "[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. \u201cGenerative Adversarial Nets.\u201d In: NIPS. 2014. arXiv: 1406.2661.",
            "arxiv_url": "https://arxiv.org/pdf/1406.2661"
        },
        {
            "id": "17",
            "entry": "[17] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. J. Smola. \u201cA Kernel TwoSample Test.\u201d In: JMLR 13 (2012).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Sch%C3%B6lkopf%2C%20B.%20A%20Kernel%20TwoSample%20Test.%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Sch%C3%B6lkopf%2C%20B.%20A%20Kernel%20TwoSample%20Test.%202012"
        },
        {
            "id": "18",
            "entry": "[18] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. \u201cImproved Training of Wasserstein GANs.\u201d In: NIPS. 2017. arXiv: 1704.00028.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00028"
        },
        {
            "id": "19",
            "entry": "[19] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. \u201cGANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium.\u201d In: NIPS. 2017. arXiv: 1706.08500.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08500"
        },
        {
            "id": "20",
            "entry": "[20] G. Huang, Y. Yuan, Q. Xu, C. Guo, Y. Sun, F. Wu, and K. Weinberger. An empirical study on evaluation metrics of generative adversarial networks. 2018. arXiv: 1806.07755.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07755"
        },
        {
            "id": "21",
            "entry": "[21] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. \u201cMultimodal Unsupervised Image-to-Image Translation.\u201d In: ECCV. 2018. arXiv: 1804.04732.",
            "arxiv_url": "https://arxiv.org/pdf/1804.04732"
        },
        {
            "id": "22",
            "entry": "[22] Y. Jin, K. Zhang, M. Li, Y. Tian, H. Zhu, and Z. Fang. Towards the Automatic Anime Characters Creation with Generative Adversarial Networks. 2017. arXiv: 1708.05509.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05509"
        },
        {
            "id": "23",
            "entry": "[23] T. Karras, T. Aila, S. Laine, and J. Lehtinen. \u201cProgressive Growing of GANs for Improved Quality, Stability, and Variation.\u201d In: ICLR. 2018. arXiv: 1710.10196.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "24",
            "entry": "[24] D. Kingma and J. Ba. \u201cAdam: A Method for Stochastic Optimization.\u201d In: ICLR. 2015. arXiv: 1412.6980.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "25",
            "entry": "[25] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images%202009"
        },
        {
            "id": "26",
            "entry": "[26] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. P\u00f3czos. \u201cMMD GAN: Towards Deeper Understanding of Moment Matching Network.\u201d In: NIPS. 2017. arXiv: 1705.08584.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08584"
        },
        {
            "id": "27",
            "entry": "[27] Y. Li, K. Swersky, and R. Zemel. \u201cGenerative Moment Matching Networks.\u201d In: ICML. 2015. arXiv: 1502.02761.",
            "arxiv_url": "https://arxiv.org/pdf/1502.02761"
        },
        {
            "id": "28",
            "entry": "[28] Z. Liu, P. Luo, X. Wang, and X. Tang. \u201cDeep learning face attributes in the wild.\u201d In: ICCV. 2015. arXiv: 1411.7766.",
            "arxiv_url": "https://arxiv.org/pdf/1411.7766"
        },
        {
            "id": "29",
            "entry": "[29] L. Mescheder, A. Geiger, and S. Nowozin. \u201cWhich Training Methods for GANs do actually Converge?\u201d In: ICML. 2018. arXiv: 1801.04406.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04406"
        },
        {
            "id": "30",
            "entry": "[30] P. Milgrom and I. Segal. \u201cEnvelope theorems for arbitrary choice sets.\u201d In: Econometrica 70.2 (2002), pp. 583\u2013601.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Milgrom%2C%20P.%20Segal%2C%20I.%20Envelope%20theorems%20for%20arbitrary%20choice%20sets.%202002"
        },
        {
            "id": "31",
            "entry": "[31] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. \u201cSpectral Normalization for Generative Adversarial Networks.\u201d In: ICLR. 2018. arXiv: 1802.05927.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05927"
        },
        {
            "id": "32",
            "entry": "[32] Y. Mroueh, C.-L. Li, T. Sercu, A. Raj, and Y. Cheng. \u201cSobolev GAN.\u201d In: ICLR. 2018. arXiv: 1711.04894.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04894"
        },
        {
            "id": "33",
            "entry": "[33] Y. Mroueh and T. Sercu. \u201cFisher GAN.\u201d In: NIPS. 2017. arXiv: 1705.09675.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09675"
        },
        {
            "id": "34",
            "entry": "[34] Y. Mroueh, T. Sercu, and A. Raj. Regularized Kernel and Neural Sobolev Descent: Dynamic MMD Transport. 2018. arXiv: 1805.12062.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12062"
        },
        {
            "id": "35",
            "entry": "[35] A. M\u00fcller. \u201cIntegral Probability Metrics and their Generating Classes of Functions.\u201d In: Advances in Applied Probability 29.2 (1997), pp. 429\u2013443.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%BCller%2C%20A.%20Integral%20Probability%20Metrics%20and%20their%20Generating%20Classes%20of%20Functions.%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%BCller%2C%20A.%20Integral%20Probability%20Metrics%20and%20their%20Generating%20Classes%20of%20Functions.%201997"
        },
        {
            "id": "36",
            "entry": "[36] S. Nowozin, B. Cseke, and R. Tomioka. \u201cf-GAN: Training Generative Neural Samplers using Variational Divergence Minimization.\u201d In: NIPS. 2016. arXiv: 1606.00709.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00709"
        },
        {
            "id": "37",
            "entry": "[37] A. Radford, L. Metz, and S. Chintala. \u201cUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.\u201d In: ICLR. 2016. arXiv: 1511.06434.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "Soc_1978_a",
            "entry": "Soc. 84.4 (July 1978), pp. 681\u2013685.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soc%20844%20July%201978%20pp%20681685"
        },
        {
            "id": "39",
            "entry": "[39] K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. \u201cStabilizing Training of Generative Adversarial Networks through Regularization.\u201d In: NIPS. 2017. arXiv: 1705.09367.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09367"
        },
        {
            "id": "40",
            "entry": "[40] O. Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge. 2014. arXiv: 1409.0575.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0575"
        },
        {
            "id": "41",
            "entry": "[41] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. \u201cImproved Techniques for Training GANs.\u201d In: NIPS. 2016. arXiv: 1606.03498.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03498"
        },
        {
            "id": "42",
            "entry": "[42] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shawe-Taylor%2C%20J.%20Cristianini%2C%20N.%20Kernel%20Methods%20for%20Pattern%20Analysis"
        },
        {
            "id": "43",
            "entry": "[43] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Sch\u00f6lkopf. \u201cKernel choice and classifiability for RKHS embeddings of probability distributions.\u201d In: NIPS. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sriperumbudur%2C%20B.K.%20Fukumizu%2C%20K.%20Gretton%2C%20A.%20Lanckriet%2C%20G.R.G.%20Kernel%20choice%20and%20classifiability%20for%20RKHS%20embeddings%20of%20probability%20distributions.%202009"
        },
        {
            "id": "44",
            "entry": "[44] B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. \u201cUniversality, Characteristic Kernels and RKHS Embedding of Measures.\u201d In: JMLR 12 (2011), pp. 2389\u20132410. arXiv: 1003.0887.",
            "arxiv_url": "https://arxiv.org/pdf/1003.0887"
        },
        {
            "id": "45",
            "entry": "[45] B. Sriperumbudur. \u201cOn the optimal estimation of probability mesaures in weak and strong topologies.\u201d In: Bernoulli 22.3 (2016), pp. 1839\u20131893. arXiv: 1310.8240.",
            "arxiv_url": "https://arxiv.org/pdf/1310.8240"
        },
        {
            "id": "46",
            "entry": "[46] I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics. Springer, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20I.%20Christmann%2C%20A.%20Support%20Vector%20Machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20I.%20Christmann%2C%20A.%20Support%20Vector%20Machines%202008"
        },
        {
            "id": "47",
            "entry": "[47] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton. \u201cGenerative Models and Model Criticism via Optimized Maximum Mean Discrepancy.\u201d In: ICLR. 2017. arXiv: 1611.04488.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04488"
        },
        {
            "id": "48",
            "entry": "[48] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. \u201cRethinking the Inception Architecture for Computer Vision.\u201d In: CVPR. 2016. arXiv: 1512.00567.",
            "arxiv_url": "https://arxiv.org/pdf/1512.00567"
        },
        {
            "id": "49",
            "entry": "[49] T. Unterthiner, B. Nessler, C. Seward, G. Klambauer, M. Heusel, H. Ramsauer, and S. Hochreiter. \u201cCoulomb GANs: Provably Optimal Nash Equilibria via Potential Fields.\u201d In: ICLR. 2018. arXiv: 1708.08819.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08819"
        },
        {
            "id": "50",
            "entry": "[50] C. Villani. Optimal Transport: Old and New. Springer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C.%20Optimal%20Transport%3A%20Old%20and%20New%202009"
        },
        {
            "id": "51",
            "entry": "[51] J. Weed and F. Bach. \u201cSharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance.\u201d In: Bernoulli (forthcoming). arXiv: 1707.00087.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00087"
        },
        {
            "id": "52",
            "entry": "[52] H. Wendland. Scattered Data Approximation. Cambridge, UK: Cambridge University Press, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wendland%2C%20H.%20Scattered%20Data%20Approximation%202005"
        },
        {
            "id": "53",
            "entry": "[53] W. Zaremba, A. Gretton, and M. B. Blaschko. \u201cB-tests: Low Variance Kernel Two-Sample Tests.\u201d In: NIPS. 2013. arXiv: 1307.1954. ",
            "arxiv_url": "https://arxiv.org/pdf/1307.1954"
        }
    ]
}
