{
    "filename": "7878-on-controllable-sparse-alternatives-to-softmax.pdf",
    "metadata": {
        "title": "On Controllable Sparse Alternatives to Softmax",
        "author": "Anirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra, Karthik Sankaranarayanan, Harish G. Ramaswamy",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7878-on-controllable-sparse-alternatives-to-softmax.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Jensen-Shannon divergence",
            "url": "https://en.wikipedia.org/wiki/Jensen-Shannon_divergence"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "multiclass classification",
            "url": "https://en.wikipedia.org/wiki/multiclass_classification"
        },
        {
            "term": "probability distribution",
            "url": "https://en.wikipedia.org/wiki/probability_distribution"
        }
    ],
    "highlights": [
        "Various widely used probability mapping functions such as sum-normalization, softmax, and spherical softmax enable mapping of vectors from the euclidean space to probability distributions",
        "While there have been several such mapping functions proposed in literature such as softmax [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], spherical softmax [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] and sparsemax [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], very little is understood in terms of how they relate to each other and their theoretical underpinnings",
        "In this paper, we introduce a general formulation encompassing all such probability mapping functions which serves as a unifying framework to understand individual formulations such as hardmax, softmax, sum-normalization, spherical softmax and sparsemax as special cases, while at the same time helps in providing explicit control over degree of sparsity",
        "Definition: We propose a generic probability mapping function inspired from the sparsemax formulation which we call sparsegen: \u03c1(z) = sparsegen(z; g, \u03bb) = argmin p \u2212 g(z)",
        "We investigated a family of sparse probability mapping functions, unifying them under a general framework",
        "We have proposed convex loss functions, which helped us to achieve better accuracies in the multilabel classification setting"
    ],
    "key_statements": [
        "Various widely used probability mapping functions such as sum-normalization, softmax, and spherical softmax enable mapping of vectors from the euclidean space to probability distributions",
        "The need for such functions arises in multiple problem settings like multiclass classification [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], reinforcement learning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] and more recently in attention mechanism [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] in deep neural networks, amongst others",
        "While there have been several such mapping functions proposed in literature such as softmax [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], spherical softmax [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] and sparsemax [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], very little is understood in terms of how they relate to each other and their theoretical underpinnings",
        "In this paper, we introduce a general formulation encompassing all such probability mapping functions which serves as a unifying framework to understand individual formulations such as hardmax, softmax, sum-normalization, spherical softmax and sparsemax as special cases, while at the same time helps in providing explicit control over degree of sparsity",
        "Definition: We propose a generic probability mapping function inspired from the sparsemax formulation which we call sparsegen: \u03c1(z) = sparsegen(z; g, \u03bb) = argmin p \u2212 g(z)",
        "We investigated a family of sparse probability mapping functions, unifying them under a general framework",
        "We have proposed convex loss functions, which helped us to achieve better accuracies in the multilabel classification setting"
    ],
    "summary": [
        "Various widely used probability mapping functions such as sum-normalization, softmax, and spherical softmax enable mapping of vectors from the euclidean space to probability distributions.",
        "In this paper, we introduce a general formulation encompassing all such probability mapping functions which serves as a unifying framework to understand individual formulations such as hardmax, softmax, sum-normalization, spherical softmax and sparsemax as special cases, while at the same time helps in providing explicit control over degree of sparsity.",
        "A general framework of formulations producing probability distributions with connections to hardmax, softmax, sparsemax, spherical softmax and sum-normalization (Sec.3).",
        "New formulations like sparsegen-lin and sparsehourglass as special cases of the general framework which enable explicit control over the desired degree of sparsity (Sec.3.2,3.5).",
        "Sum-normalization and spherical softmax satisfy this property whereas sparsemax and softmax are not scale invariant.",
        "Only sum-normalization and spherical softmax satisfy scale invariance.",
        "It can be shown that any probability mapping function with the scale invariance property will not be Lipschitz continuous and will be undefined for z = 0.",
        "From the translation invariance property of sparsemax, the resultant mapping function can be shown equivalent to considering g(z) = \u03b1(z)z in Eq.1.",
        "The above loss function for sparsegen-lin can be used to derive a multilabel loss for sparsemax by setting \u03bb = 0 .",
        "We consider the following activation-loss pairs: (1) softmax+log: KL-divergence loss applied on top of softmax outputs, (2) sparsemax+huber: multilabel classification method from [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], (3) sparsemax+hinge: hinge loss as in Eq.8 with \u03bb = 0 is used during training compared to Huber loss in (2), and (4) sparsehg+hinge: for sparsehourglass, loss in Eq.7 is used during training.",
        "Please note as we have a convex system of equations due to an underlying linear prediction model, applying Eq.8 in training and applying sparsegen-lin activation during test time produces the same result as sparsemax+hinge.",
        "We can observe that sparsemax+hinge and sparsehg+hinge consistently perform better than sparse-8 sparsity max+huber in all three cases, especially the label distributions are sparser.",
        "The purpose of these experiments are two fold: firstly, effectiveness of our proposed formulations sparsegen-lin and sparsehourglass in attention framework on these tasks, and secondly, control over sparsity leads to enhanced interpretability.",
        "We see that sparsegen-lin surpasses BLEU scores of softmax and sparsemax for FR-EN translation, whereas sparsehg formulations yield comparable performance.",
        "The results in Table.2 show that sparsegen-lin stands out in performance with other formulations closely following and comparable to softmax and sparsemax.",
        "Our proposed probability mapping functions enabled us to provide explicit control over sparsity to achieve higher interpretability.",
        "We have proposed convex loss functions, which helped us to achieve better accuracies in the multilabel classification setting.",
        "We would like to investigate application of these proposed sparse formulations in knowledge distillation and reinforcement learning settings"
    ],
    "headline": "We develop a unified framework that encompasses all these formulations as special cases",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Aly. Survey on multiclass classification methods. Neural networks, pages 1\u20139, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aly%2C%20M.%20Survey%20on%20multiclass%20classification%20methods%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aly%2C%20M.%20Survey%20on%20multiclass%20classification%20methods%202005"
        },
        {
            "id": "2",
            "entry": "[2] John S. Bridle. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Fran\u00e7oise Fogelman Souli\u00e9 and Jeanny H\u00e9rault, editors, Neurocomputing, pages 227\u2013236, Berlin, Heidelberg, 1990. Springer Berlin Heidelberg.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bridle%2C%20John%20S.%20Probabilistic%20interpretation%20of%20feedforward%20classification%20network%20outputs%2C%20with%20relationships%20to%20statistical%20pattern%20recognition%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bridle%2C%20John%20S.%20Probabilistic%20interpretation%20of%20feedforward%20classification%20network%20outputs%2C%20with%20relationships%20to%20statistical%20pattern%20recognition%201990"
        },
        {
            "id": "3",
            "entry": "[3] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Introduction%20to%20Reinforcement%20Learning%201998"
        },
        {
            "id": "4",
            "entry": "[4] B. Gao and L. Pavel. On the properties of the softmax function with application in game theory and reinforcement learning. ArXiv e-prints, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20B.%20Pavel%2C%20L.%20On%20the%20properties%20of%20the%20softmax%20function%20with%20application%20in%20game%20theory%20and%20reinforcement%20learning.%20ArXiv%20e-prints%202017"
        },
        {
            "id": "5",
            "entry": "[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR), San Diego, CA, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "6",
            "entry": "[6] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Francis R. Bach and David M. Blei, editors, ICML, volume 37 of JMLR Workshop and Conference Proceedings, pages 2048\u20132057. JMLR.org, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Kelvin%20Ba%2C%20Jimmy%20Kiros%2C%20Ryan%20Cho%2C%20Kyunghyun%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015"
        },
        {
            "id": "7",
            "entry": "[7] K. Cho, A. Courville, and Y. Bengio. Describing multimedia content using attention-based encoder-decoder networks. IEEE Transactions on Multimedia, 17(11):1875\u20131886, Nov 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20K.%20Courville%2C%20A.%20Bengio%2C%20Y.%20Describing%20multimedia%20content%20using%20attention-based%20encoder-decoder%20networks%202015-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20K.%20Courville%2C%20A.%20Bengio%2C%20Y.%20Describing%20multimedia%20content%20using%20attention-based%20encoder-decoder%20networks%202015-11"
        },
        {
            "id": "8",
            "entry": "[8] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389, Lisbon, Portugal, September 2015. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rush%2C%20Alexander%20M.%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20A%20neural%20attention%20model%20for%20abstractive%20sentence%20summarization%202015-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rush%2C%20Alexander%20M.%20Chopra%2C%20Sumit%20Weston%2C%20Jason%20A%20neural%20attention%20model%20for%20abstractive%20sentence%20summarization%202015-09"
        },
        {
            "id": "9",
            "entry": "[9] Preksha Nema, Mitesh Khapra, Anirban Laha, and Balaraman Ravindran. Diversity driven attention model for query-based abstractive summarization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, August 2017. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nema%2C%20Preksha%20Khapra%2C%20Mitesh%20Laha%2C%20Anirban%20Ravindran%2C%20Balaraman%20Diversity%20driven%20attention%20model%20for%20query-based%20abstractive%20summarization%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nema%2C%20Preksha%20Khapra%2C%20Mitesh%20Laha%2C%20Anirban%20Ravindran%2C%20Balaraman%20Diversity%20driven%20attention%20model%20for%20query-based%20abstractive%20summarization%202017-08"
        },
        {
            "id": "10",
            "entry": "[10] Francis Bach, Rodolphe Jenatton, and Julien Mairal. Optimization with Sparsity-Inducing Penalties (Foundations and Trends(R) in Machine Learning). Now Publishers Inc., Hanover, MA, USA, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Jenatton%2C%20Rodolphe%20Mairal%2C%20Julien%20Optimization%20with%20Sparsity-Inducing%20Penalties%20%28Foundations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Jenatton%2C%20Rodolphe%20Mairal%2C%20Julien%20Optimization%20with%20Sparsity-Inducing%20Penalties%20%28Foundations%202011"
        },
        {
            "id": "11",
            "entry": "[11] Mohammad S. Sorower. A literature survey on algorithms for multi-label learning. 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sorower%2C%20Mohammad%20S.%20A%20literature%20survey%20on%20algorithms%20for%20multi-label%20learning%202010"
        },
        {
            "id": "12",
            "entry": "[12] Pascal Vincent, Alexandre de Br\u00e9bisson, and Xavier Bouthillier. Efficient exact gradient update for training deep networks with very large sparse targets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201915, pages 1108\u20131116, Cambridge, MA, USA, 2015. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20de%20Br%C3%A9bisson%2C%20Alexandre%20Bouthillier%2C%20Xavier%20Efficient%20exact%20gradient%20update%20for%20training%20deep%20networks%20with%20very%20large%20sparse%20targets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20de%20Br%C3%A9bisson%2C%20Alexandre%20Bouthillier%2C%20Xavier%20Efficient%20exact%20gradient%20update%20for%20training%20deep%20networks%20with%20very%20large%20sparse%20targets%202015"
        },
        {
            "id": "13",
            "entry": "[13] Alexandre de Br\u00e9bisson and Pascal Vincent. An exploration of softmax alternatives belonging to the spherical loss family. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Br%C3%A9bisson%2C%20Alexandre%20Vincent%2C%20Pascal%20An%20exploration%20of%20softmax%20alternatives%20belonging%20to%20the%20spherical%20loss%20family%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Br%C3%A9bisson%2C%20Alexandre%20Vincent%2C%20Pascal%20An%20exploration%20of%20softmax%20alternatives%20belonging%20to%20the%20spherical%20loss%20family%202016"
        },
        {
            "id": "14",
            "entry": "[14] Andr\u00e9 F. T. Martins and Ram\u00f3n F. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pages 1614\u20131623. JMLR.org, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martins%2C%20Andr%C3%A9%20F.T.%20Astudillo%2C%20Ram%C3%B3n%20F.%20From%20softmax%20to%20sparsemax%3A%20A%20sparse%20model%20of%20attention%20and%20multi-label%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martins%2C%20Andr%C3%A9%20F.T.%20Astudillo%2C%20Ram%C3%B3n%20F.%20From%20softmax%20to%20sparsemax%3A%20A%20sparse%20model%20of%20attention%20and%20multi-label%20classification%202016"
        },
        {
            "id": "15",
            "entry": "[15] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the l1-ball for learning in high dimensions. In Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 272\u2013279, New York, NY, USA, 2008. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Chandra%2C%20Tushar%20Efficient%20projections%20onto%20the%20l1-ball%20for%20learning%20in%20high%20dimensions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Chandra%2C%20Tushar%20Efficient%20projections%20onto%20the%20l1-ball%20for%20learning%20in%20high%20dimensions%202008"
        },
        {
            "id": "16",
            "entry": "[16] Vlad Niculae and Mathieu Blondel. A regularized framework for sparse and structured neural attention. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3340\u20133350. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niculae%2C%20Vlad%20Blondel%2C%20Mathieu%20A%20regularized%20framework%20for%20sparse%20and%20structured%20neural%20attention%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niculae%2C%20Vlad%20Blondel%2C%20Mathieu%20A%20regularized%20framework%20for%20sparse%20and%20structured%20neural%20attention%202017"
        },
        {
            "id": "17",
            "entry": "[17] Ashish Kapoor, Raajay Viswanathan, and Prateek Jain. Multilabel classification using bayesian compressed sensing. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2645\u20132653. Curran Associates, Inc., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kapoor%2C%20Ashish%20Viswanathan%2C%20Raajay%20Jain%2C%20Prateek%20Multilabel%20classification%20using%20bayesian%20compressed%20sensing%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kapoor%2C%20Ashish%20Viswanathan%2C%20Raajay%20Jain%2C%20Prateek%20Multilabel%20classification%20using%20bayesian%20compressed%20sensing%202012"
        },
        {
            "id": "18",
            "entry": "[18] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. Opennmt: Open-source toolkit for neural machine translation. CoRR, abs/1701.02810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.02810"
        },
        {
            "id": "19",
            "entry": "[19] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20arthur%20szlam%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20arthur%20szlam%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015"
        }
    ]
}
