{
    "filename": "7988-videocapsulenet-a-simplified-network-for-action-detection.pdf",
    "metadata": {
        "title": "VideoCapsuleNet: A Simplified Network for Action Detection",
        "author": "Kevin Duarte, Yogesh Rawat, Mubarak Shah",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7988-videocapsulenet-a-simplified-network-for-action-detection.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue and make the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves state-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive \u223c20% improvement on UCF-101 and \u223c15% improvement on J-HMDB in terms of v-mAP scores."
    },
    "keywords": [
        {
            "term": "action detection",
            "url": "https://en.wikipedia.org/wiki/action_detection"
        },
        {
            "term": "video frame",
            "url": "https://en.wikipedia.org/wiki/video_frame"
        },
        {
            "term": "Intelligence Advanced Research Projects Activity",
            "url": "https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity"
        },
        {
            "term": "human action",
            "url": "https://en.wikipedia.org/wiki/human_action"
        },
        {
            "term": "optical flow",
            "url": "https://en.wikipedia.org/wiki/optical_flow"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "Human action detection is a challenging computer vision problem, which involves detecting human actions in a long video as well as localizing these actions both spatially and temporally",
        "Great progress have been achieved in solving action detection problem using deep learning methods [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We present a novel capsule-pooling procedure for capsule routing, which greatly reduces the computational cost of routing in convolutional capsule layers",
        "We propose a new voting procedure for convolutional capsule layers to reduce the number of computations used in capsule routing",
        "In this work we propose VideoCapsuleNet, a generalization of capsule network from 2D images to 3D videos, for action detection",
        "We introduce capsule-pooling to optimize the voting algorithm in the convolutional capsule layers which makes the routing feasible"
    ],
    "key_statements": [
        "Human action detection is a challenging computer vision problem, which involves detecting human actions in a long video as well as localizing these actions both spatially and temporally",
        "Great progress have been achieved in solving action detection problem using deep learning methods [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We present a novel capsule-pooling procedure for capsule routing, which greatly reduces the computational cost of routing in convolutional capsule layers",
        "We propose a new voting procedure for convolutional capsule layers to reduce the number of computations used in capsule routing",
        "Computational Cost and Training Speed capsule networks tend to be computationally expensive, capsule-pooling allows VideoCapsuleNet to run on a single Titan X GPU using a batch size of 8",
        "In this work we propose VideoCapsuleNet, a generalization of capsule network from 2D images to 3D videos, for action detection",
        "We introduce capsule-pooling to optimize the voting algorithm in the convolutional capsule layers which makes the routing feasible"
    ],
    "summary": [
        "Human action detection is a challenging computer vision problem, which involves detecting human actions in a long video as well as localizing these actions both spatially and temporally.",
        "We aim at generalizing the capsule network from images to videos for the task of action detection.",
        "The proposed network, VideoCapsuleNet, uses 3D convolutions along with capsules to learn semantic information necessary for action detection.",
        "Apart from action classification and pixel-wise localization, the predicted capsules in the network are capable of explaining different characteristics of the action in the video.",
        "This is not enough to reduce the computational cost if (i) the kernel/receptive field volume is large, as in our case when using 3-dimensional kernels, or the spatial/temporal dimensions of the convolutional capsule layer is large.",
        "We propose a new voting procedure for convolutional capsule layers to reduce the number of computations used in capsule routing.",
        "Localization Network To obtain frame-level action localizations, we want to leverage the actionbased representation found in the class capsule layer\u2019s pose matrices.",
        "Current SOTA methods usually localize actions at a frame level: a region proposal network generates bounding box proposals, which are linked together over time and regressed to improve results.",
        "These experiments show us that the addition of a reconstruction network, when no localization information are available, do help the capsules learn better representations: there is a 10% increase in performance (Table 2).",
        "These additional skip connections result in similar classification and localization results as the base VideoCapsuleNet (Table 2), but they increase the number of network parameters as well as the training time.",
        "Coordinate Addition Coordinate Addition allows the class capsules to encode positional information about the actions which they represent, by adding the capsules\u2019 coordinates to the vote matrices of the final convolutional capsule layer.",
        "VideoCapsuleNet is trained on these randomly generated videos, and we measure the dimensions of the class capsules\u2019 pose matrices when varying different properties of the generated videos.",
        "This helps explain why VideoCapsuleNet is able to achieve such good localization results; the capsules learn to represent the different spatio-temporal properties necessary for accurate action localizations.",
        "5.5 Computational Cost and Training Speed capsule networks tend to be computationally expensive, capsule-pooling allows VideoCapsuleNet to run on a single Titan X GPU using a batch size of 8.",
        "In this work we propose VideoCapsuleNet, a generalization of capsule network from 2D images to 3D videos, for action detection.",
        "The proposed network takes video frames as input and predicts an action class as well as a pixel-wise localization for the input video clip.",
        "The proposed network has a localization component which generates pixel-wise localization considering the predicted class-specific capsules.",
        "The results we have achieved in this paper on videos are promising and indicate the potential of capsules for videos, which makes it worth exploring"
    ],
    "headline": "We present a more elegant solution for action detection based on the recently developed capsule network",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Samitha Herath, Mehrtash Harandi, and Fatih Porikli. Going deeper into action recognition: A survey. Image and vision computing, 60:4\u201321, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herath%2C%20Samitha%20Harandi%2C%20Mehrtash%20Porikli%2C%20Fatih%20Going%20deeper%20into%20action%20recognition%3A%20A%20survey%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herath%2C%20Samitha%20Harandi%2C%20Mehrtash%20Porikli%2C%20Fatih%20Going%20deeper%20into%20action%20recognition%3A%20A%20survey%202017"
        },
        {
            "id": "2",
            "entry": "[2] Rui Hou, Chen Chen, and Mubarak Shah. Tube convolutional neural network (t-cnn) for action detection in videos. In IEEE International Conference on Computer Vision, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hou%2C%20Rui%20Chen%2C%20Chen%20Shah%2C%20Mubarak%20Tube%20convolutional%20neural%20network%20%28t-cnn%29%20for%20action%20detection%20in%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hou%2C%20Rui%20Chen%2C%20Chen%20Shah%2C%20Mubarak%20Tube%20convolutional%20neural%20network%20%28t-cnn%29%20for%20action%20detection%20in%20videos%202017"
        },
        {
            "id": "3",
            "entry": "[3] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Caroline Pantofaru, David A Ross, George Toderici, Yeqing Li, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Chunhui%20Sun%2C%20Chen%20Vijayanarasimhan%2C%20Sudheendra%20Pantofaru%2C%20Caroline%20Ava%3A%20A%20video%20dataset%20of%20spatio-temporally%20localized%20atomic%20visual%20actions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Chunhui%20Sun%2C%20Chen%20Vijayanarasimhan%2C%20Sudheendra%20Pantofaru%2C%20Caroline%20Ava%3A%20A%20video%20dataset%20of%20spatio-temporally%20localized%20atomic%20visual%20actions%202018"
        },
        {
            "id": "4",
            "entry": "[4] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, and Cordelia Schmid. Action tubelet detector for spatio-temporal action localization. In ICCV-IEEE International Conference on Computer Vision, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalogeiton%2C%20Vicky%20Weinzaepfel%2C%20Philippe%20Ferrari%2C%20Vittorio%20Schmid%2C%20Cordelia%20Action%20tubelet%20detector%20for%20spatio-temporal%20action%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalogeiton%2C%20Vicky%20Weinzaepfel%2C%20Philippe%20Ferrari%2C%20Vittorio%20Schmid%2C%20Cordelia%20Action%20tubelet%20detector%20for%20spatio-temporal%20action%20localization%202017"
        },
        {
            "id": "5",
            "entry": "[5] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In Advances in neural information processing systems, pages 568\u2013576, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014"
        },
        {
            "id": "6",
            "entry": "[6] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4724\u20134733. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20Joao%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20Joao%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017"
        },
        {
            "id": "7",
            "entry": "[7] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in Neural Information Processing Systems, pages 3859\u20133869, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017"
        },
        {
            "id": "8",
            "entry": "[8] Xiaojiang Peng and Cordelia Schmid. Multi-region two-stream r-cnn for action detection. In European Conference on Computer Vision, pages 744\u2013759.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Xiaojiang%20Schmid%2C%20Cordelia%20Multi-region%20two-stream%20r-cnn%20for%20action%20detection",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Xiaojiang%20Schmid%2C%20Cordelia%20Multi-region%20two-stream%20r-cnn%20for%20action%20detection"
        },
        {
            "id": "9",
            "entry": "[9] Zhenheng Yang, Jiyang Gao, and Ram Nevatia. Spatio-temporal action detection with cascade proposal and location anticipation. BMVC, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zhenheng%20Gao%2C%20Jiyang%20Nevatia%2C%20Ram%20Spatio-temporal%20action%20detection%20with%20cascade%20proposal%20and%20location%20anticipation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zhenheng%20Gao%2C%20Jiyang%20Nevatia%2C%20Ram%20Spatio-temporal%20action%20detection%20with%20cascade%20proposal%20and%20location%20anticipation%202017"
        },
        {
            "id": "10",
            "entry": "[10] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Computer Vision (ICCV), 2015 IEEE International Conference on, pages 4489\u20134497. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015"
        },
        {
            "id": "11",
            "entry": "[11] Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with EM routing. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Matrix%20capsules%20with%20EM%20routing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Matrix%20capsules%20with%20EM%20routing%202018"
        },
        {
            "id": "12",
            "entry": "[12] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "13",
            "entry": "[13] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014"
        },
        {
            "id": "14",
            "entry": "[14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "15",
            "entry": "[15] Mikel D Rodriguez, Javed Ahmed, and Mubarak Shah. Action mach a spatio-temporal maximum average correlation height filter for action recognition. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodriguez%2C%20Mikel%20D.%20Ahmed%2C%20Javed%20Shah%2C%20Mubarak%20Action%20mach%20a%20spatio-temporal%20maximum%20average%20correlation%20height%20filter%20for%20action%20recognition%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodriguez%2C%20Mikel%20D.%20Ahmed%2C%20Javed%20Shah%2C%20Mubarak%20Action%20mach%20a%20spatio-temporal%20maximum%20average%20correlation%20height%20filter%20for%20action%20recognition%202008"
        },
        {
            "id": "16",
            "entry": "[16] Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, and Michael J Black. Towards understanding action recognition. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 3192\u20133199. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jhuang%2C%20Hueihan%20Gall%2C%20Juergen%20Zuffi%2C%20Silvia%20Schmid%2C%20Cordelia%20Towards%20understanding%20action%20recognition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jhuang%2C%20Hueihan%20Gall%2C%20Juergen%20Zuffi%2C%20Silvia%20Schmid%2C%20Cordelia%20Towards%20understanding%20action%20recognition%202013"
        },
        {
            "id": "17",
            "entry": "[17] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.0402"
        },
        {
            "id": "18",
            "entry": "[18] Suman Saha, Gurkirt Singh, Michael Sapienza, Philip HS Torr, and Fabio Cuzzolin. Deep learning for detecting multiple space-time action tubes in videos. BMVC, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saha%2C%20Suman%20Singh%2C%20Gurkirt%20Sapienza%2C%20Michael%20Torr%2C%20Philip%20H.S.%20Deep%20learning%20for%20detecting%20multiple%20space-time%20action%20tubes%20in%20videos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saha%2C%20Suman%20Singh%2C%20Gurkirt%20Sapienza%2C%20Michael%20Torr%2C%20Philip%20H.S.%20Deep%20learning%20for%20detecting%20multiple%20space-time%20action%20tubes%20in%20videos%202016"
        },
        {
            "id": "19",
            "entry": "[19] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip Torr, and Fabio Cuzzolin. Online real time multiple spatiotemporal action localisation and prediction. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Gurkirt%20Saha%2C%20Suman%20Sapienza%2C%20Michael%20Torr%2C%20Philip%20Online%20real%20time%20multiple%20spatiotemporal%20action%20localisation%20and%20prediction%202017"
        },
        {
            "id": "20",
            "entry": "[20] Jiawei He, Mostafa S Ibrahim, Zhiwei Deng, and Greg Mori. Generic tubelet proposals for action localization. WACV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Jiawei%20Ibrahim%2C%20Mostafa%20S.%20Deng%2C%20Zhiwei%20Mori%2C%20Greg%20Generic%20tubelet%20proposals%20for%20action%20localization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Jiawei%20Ibrahim%2C%20Mostafa%20S.%20Deng%2C%20Zhiwei%20Mori%2C%20Greg%20Generic%20tubelet%20proposals%20for%20action%20localization%202018"
        },
        {
            "id": "21",
            "entry": "[21] Tian Lan, Yang Wang, and Greg Mori. Discriminative figure-centric models for joint action localization and recognition. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2003\u20132010. IEEE, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Tian%20Wang%2C%20Yang%20Mori%2C%20Greg%20Discriminative%20figure-centric%20models%20for%20joint%20action%20localization%20and%20recognition%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Tian%20Wang%2C%20Yang%20Mori%2C%20Greg%20Discriminative%20figure-centric%20models%20for%20joint%20action%20localization%20and%20recognition%202003"
        }
    ]
}
