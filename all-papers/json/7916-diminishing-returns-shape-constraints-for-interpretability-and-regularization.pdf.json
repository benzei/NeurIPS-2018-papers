{
    "filename": "7916-diminishing-returns-shape-constraints-for-interpretability-and-regularization.pdf",
    "metadata": {
        "title": "Diminishing Returns Shape Constraints for Interpretability and Regularization",
        "author": "Maya Gupta, Dara Bahri, Andrew Cotter, Kevin Canini",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7916-diminishing-returns-shape-constraints-for-interpretability-and-regularization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs. We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of concavity/convexity and monotonicity constraints on any subsets of features, and compare to new shape-constrained neural networks. We demonstrate on real-world examples that these shape constrained models can provide tuning-free regularization and improve model understandability."
    },
    "keywords": [
        {
            "term": "square meter",
            "url": "https://en.wikipedia.org/wiki/square_meter"
        },
        {
            "term": "Generalized additive models",
            "url": "https://en.wikipedia.org/wiki/Generalized_Additive_Model"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Diminishing returns are common in physical systems, human perception and psychology [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], and have been recognized in economics [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] and agriculture [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] for centuries",
        "We investigate diminishing returns shape constraints for two flexible, nonlinear function classes: neural networks and lattices",
        "While our proposal can be extended to deep lattice networks, we focus on the two-layer lattice network formed by an ensemble of L calibrated lattices [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]: L",
        "Table 1 shows the Test MSE is slightly better with the added shape constraint for all three models (SCNN, random tiny lattices, Cal",
        "We specified the additional constraints needed to learn flexible lattice models that can impose any mixture of convexity/concavity/monotonicity constraints over subsets of features, ceterus paribus, and showed we can stably train models with these extra linear inequality constraints",
        "We extended neural networks to handle partial diminishing returns constraints"
    ],
    "key_statements": [
        "Diminishing returns are common in physical systems, human perception and psychology [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], and have been recognized in economics [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] and agriculture [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] for centuries",
        "We investigate diminishing returns shape constraints for two flexible, nonlinear function classes: neural networks and lattices",
        "Calibrated linear models are Generalized additive models, with the special case of setting K = N corresponds to a nonparametric model, though in practice we find the validated choice of K is usually 5 \u2212 50, making calibrated linear models much more efficient to evaluate than non-parametric models",
        "While our proposal can be extended to deep lattice networks, we focus on the two-layer lattice network formed by an ensemble of L calibrated lattices [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]: L",
        "Table 1 shows the Test MSE is slightly better with the added shape constraint for all three models (SCNN, random tiny lattices, Cal",
        "We specified the additional constraints needed to learn flexible lattice models that can impose any mixture of convexity/concavity/monotonicity constraints over subsets of features, ceterus paribus, and showed we can stably train models with these extra linear inequality constraints",
        "We extended neural networks to handle partial diminishing returns constraints"
    ],
    "summary": [
        "Diminishing returns are common in physical systems, human perception and psychology [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], and have been recognized in economics [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] and agriculture [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] for centuries.",
        "We show that one can shape-constrain lattice models for any mixture of monotonicity and concavity/convexity constraints on any subsets of the features, and achieve similar test metrics than with unconstrained deep neural networks on a breadth of real-world problems.",
        "We extend partial monotonic lattice ensemble models [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] to enable ceterus paribus partial convexity or concavity constraints, and partial diminishing/accelerating returns constraints.",
        "If one interpolates the look-up table with the standard multilinear interpolation, the fitted surface forms a multilinear polynomial over [0, 1]S, and is ceterus paribus linear in each feature, and the lattice layer does not affect convexity/concavity for more on that).",
        "Figure 2 shows the more constrained models are smoother and more interpretable, because a human can summarize the machine learned as, \u201cHigher price cars decrease sales, but absolute price differences matter less the higher the price.\u201d Table 1 shows the Test MSE is slightly better for the calibrated linear model with the added convexity constraint.",
        "Table 1 shows the Test MSE is slightly better with the added shape constraint for all three models (SCNN, RTL, Cal. Linear).",
        "In this experiment (18 features, 1,522 training, 435 validation, and 217 test examples), we illustrate the use of concavity/convexity constraints without any monotonicity constraints.",
        "The results in Table 2 show the extra concavity constraint slightly improves both the validation error and the test error for the RTL and Calibrated Linear model.",
        "Table 2 shows that constraining the price feature does not have much effect on the Test MSE for the RTL and Calibrated Linear models; visual inspection of the learned calibrators showed they both picked up the correct shape with or without the shape constraint.",
        "We constrain the most important feature, relatedness to be concave, based on observing that the shape of its calibrator in an unconstrained calibrated linear model appears to exhibit noisy diminishing returns, as shown in Figure 2.",
        "We specified the additional constraints needed to learn flexible lattice models that can impose any mixture of convexity/concavity/monotonicity constraints over subsets of features, ceterus paribus, and showed we can stably train models with these extra linear inequality constraints.",
        "Experimental results on real-world problems showed the extra convexity/concaveity shape constraints either reduced or did not affect test MSE on IID test sets, and provided the most value for the non-IID Puzzles experiment, where shape constraint regularization is expected to be most valuable.",
        "The resulting SCNNs enable a less flexible menu of shape constraint choices, and their experimental results were more sensitive to hyperparameter choices and stochasticity in mini-batch sampling"
    ],
    "headline": "We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Kahneman and A. Tversky. Choices, Values and Frames. Cambridge University Press, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kahneman%2C%20D.%20Choices%2C%20A.Tversky%20Values%20and%20Frames%202000"
        },
        {
            "id": "2",
            "entry": "[2] A. Smith. On the Principles of Political Economy and Taxation. 1817.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20A.%20On%20the%20Principles%20of%20Political%20Economy%20and%20Taxation%201817"
        },
        {
            "id": "3",
            "entry": "[3] F. L. Patton. Diminishing Returns in Agriculture. 1926.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patton%2C%20F.L.%20Diminishing%20Returns%20in%20Agriculture%201926"
        },
        {
            "id": "4",
            "entry": "[4] A. Smith. An Inquiry into the Nature and Causes of the Wealth of Nations. 1776.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20A.%20An%20Inquiry%20into%20the%20Nature%20and%20Causes%20of%20the%20Wealth%20of%20Nations%201776"
        },
        {
            "id": "5",
            "entry": "[5] S. Shingo. Zero quality control: source inspection and the poka-yoke system. Productivity Press, Portland, USA, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shingo%2C%20S.%20Zero%20quality%20control%3A%20source%20inspection%20and%20the%20poka-yoke%20system%201986"
        },
        {
            "id": "6",
            "entry": "[6] T. Hastie and R. Tibshirani. Generalized Additive Models. Chapman Hall, New York, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20T.%20Tibshirani%2C%20R.%20Generalized%20Additive%20Models%201990"
        },
        {
            "id": "7",
            "entry": "[7] P. Groeneboom and G. Jongbloed. Nonparametric estimation under shape constraints. Cambridge Press, New York, USA, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Groeneboom%2C%20P.%20Jongbloed%2C%20G.%20Nonparametric%20estimation%20under%20shape%20constraints%202014"
        },
        {
            "id": "8",
            "entry": "[8] R. E. Barlow, D. J. Bartholomew, J. M. Bremner, and H. D. Brunk. Statistical inference under order restrictions; the theory and application of isotonic regression. Wiley, New York, USA, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barlow%2C%20R.E.%20Bartholomew%2C%20D.J.%20Bremner%2C%20J.M.%20Brunk%2C%20H.D.%20Statistical%20inference%20under%20order%20restrictions%3B%20the%20theory%20and%20application%20of%20isotonic%20regression%201972"
        },
        {
            "id": "9",
            "entry": "[9] Y. Chen and R. J. Samworth. Generalized additive and index models with shape constraints. Journal Royal Statistical Society B, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Samworth%2C%20R.J.%20Generalized%20additive%20and%20index%20models%20with%20shape%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Y.%20Samworth%2C%20R.J.%20Generalized%20additive%20and%20index%20models%20with%20shape%20constraints%202016"
        },
        {
            "id": "10",
            "entry": "[10] N. Pya and S. N. Wood. Shape constrained additive models. Statistics and Computing, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pya%2C%20N.%20Wood%2C%20S.N.%20Shape%20constrained%20additive%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pya%2C%20N.%20Wood%2C%20S.N.%20Shape%20constrained%20additive%20models%202015"
        },
        {
            "id": "11",
            "entry": "[11] J. Kim, J. Lee, L. Vandenberghe, and C.-K. Yang. Techniques for improving the accuracy of geometric-programming based analog circuit design optimization. Proc. IEEE International Conference on Computer-aided Design, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20J.%20Lee%2C%20J.%20Vandenberghe%2C%20L.%20Yang%2C%20C.-K.%20Techniques%20for%20improving%20the%20accuracy%20of%20geometric-programming%20based%20analog%20circuit%20design%20optimization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20J.%20Lee%2C%20J.%20Vandenberghe%2C%20L.%20Yang%2C%20C.-K.%20Techniques%20for%20improving%20the%20accuracy%20of%20geometric-programming%20based%20analog%20circuit%20design%20optimization%202004"
        },
        {
            "id": "12",
            "entry": "[12] A. Magnani and S. P. Boyd. Convex piecewise-linear fitting. Optimization and Engineering, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magnani%2C%20A.%20Boyd%2C%20S.P.%20Convex%20piecewise-linear%20fitting%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Magnani%2C%20A.%20Boyd%2C%20S.P.%20Convex%20piecewise-linear%20fitting%202009"
        },
        {
            "id": "13",
            "entry": "[13] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20Optimization%202008"
        },
        {
            "id": "14",
            "entry": "[14] J. Sill. Monotonic networks. Advances in Neural Information Processing Systems (NIPS), 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sill%2C%20J.%20Monotonic%20networks%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sill%2C%20J.%20Monotonic%20networks%201998"
        },
        {
            "id": "15",
            "entry": "[15] N. P. Archer and S. Wang. Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems. Decision Sciences, 24(1):60\u201375, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Archer%2C%20N.P.%20Wang%2C%20S.%20Application%20of%20the%20back%20propagation%20neural%20network%20algorithm%20with%20monotonicity%20constraints%20for%20two-group%20classification%20problems%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Archer%2C%20N.P.%20Wang%2C%20S.%20Application%20of%20the%20back%20propagation%20neural%20network%20algorithm%20with%20monotonicity%20constraints%20for%20two-group%20classification%20problems%201993"
        },
        {
            "id": "16",
            "entry": "[16] H. Kay and L. H. Ungar. Estimating monotonic functions and their bounds. AIChE Journal, 46(12):2426\u20132434, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kay%2C%20H.%20Ungar%2C%20L.H.%20Estimating%20monotonic%20functions%20and%20their%20bounds%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kay%2C%20H.%20Ungar%2C%20L.H.%20Estimating%20monotonic%20functions%20and%20their%20bounds%202000"
        },
        {
            "id": "17",
            "entry": "[17] A. Minin, M. Velikova, B. Lang, and H. Daniels. Comparison of universal approximators incorporating partial monotonicity by structure. Neural Networks, 23(4):471\u2013475, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minin%2C%20A.%20Velikova%2C%20M.%20Lang%2C%20B.%20Daniels%2C%20H.%20Comparison%20of%20universal%20approximators%20incorporating%20partial%20monotonicity%20by%20structure%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minin%2C%20A.%20Velikova%2C%20M.%20Lang%2C%20B.%20Daniels%2C%20H.%20Comparison%20of%20universal%20approximators%20incorporating%20partial%20monotonicity%20by%20structure%202010"
        },
        {
            "id": "18",
            "entry": "[18] C. Dugas, Y. Bengio, F. B\u00e9lisle, C. Nadeau, and R. Garcia. Incorporating functional knowledge in neural networks. Journal Machine Learning Research, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dugas%2C%20C.%20Bengio%2C%20Y.%20B%C3%A9lisle%2C%20F.%20Nadeau%2C%20C.%20Incorporating%20functional%20knowledge%20in%20neural%20networks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dugas%2C%20C.%20Bengio%2C%20Y.%20B%C3%A9lisle%2C%20F.%20Nadeau%2C%20C.%20Incorporating%20functional%20knowledge%20in%20neural%20networks%202009"
        },
        {
            "id": "19",
            "entry": "[19] Y.-J. Qu and B.-G. Hu. Generalized constraint neural network regression model subject to linear priors. IEEE Trans. on Neural Networks, 22(11):2447\u20132459, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Y.-J.%20Hu%2C%20B.-G.%20Generalized%20constraint%20neural%20network%20regression%20model%20subject%20to%20linear%20priors%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Y.-J.%20Hu%2C%20B.-G.%20Generalized%20constraint%20neural%20network%20regression%20model%20subject%20to%20linear%20priors%202011"
        },
        {
            "id": "20",
            "entry": "[20] H. Daniels and M. Velikova. Monotone and partially monotone neural networks. IEEE Trans. Neural Networks, 21(6):906\u2013917, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniels%2C%20H.%20Velikova%2C%20M.%20Monotone%20and%20partially%20monotone%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniels%2C%20H.%20Velikova%2C%20M.%20Monotone%20and%20partially%20monotone%20neural%20networks%202010"
        },
        {
            "id": "21",
            "entry": "[21] S. You, D. Ding, K. Canini, J. Pfeifer, and M. R. Gupta. Deep lattice networks and partial monotonic functions. Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=You%2C%20S.%20Ding%2C%20D.%20Canini%2C%20K.%20Pfeifer%2C%20J.%20Deep%20lattice%20networks%20and%20partial%20monotonic%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=You%2C%20S.%20Ding%2C%20D.%20Canini%2C%20K.%20Pfeifer%2C%20J.%20Deep%20lattice%20networks%20and%20partial%20monotonic%20functions%202017"
        },
        {
            "id": "22",
            "entry": "[22] B. Amos, L. Xu, and J. Z. Kolter. Input convex neural networks. Proc. ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amos%2C%20B.%20Xu%2C%20L.%20Kolter%2C%20J.Z.%20Input%20convex%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amos%2C%20B.%20Xu%2C%20L.%20Kolter%2C%20J.Z.%20Input%20convex%20neural%20networks%202017"
        },
        {
            "id": "23",
            "entry": "[23] K. Canini, A. Cotter, M. M. Fard, M. R. Gupta, and J. Pfeifer. Fast and flexible monotonic functions with ensembles of lattices. Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Canini%2C%20K.%20Cotter%2C%20A.%20Fard%2C%20M.M.%20Gupta%2C%20M.R.%20Fast%20and%20flexible%20monotonic%20functions%20with%20ensembles%20of%20lattices%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Canini%2C%20K.%20Cotter%2C%20A.%20Fard%2C%20M.M.%20Gupta%2C%20M.R.%20Fast%20and%20flexible%20monotonic%20functions%20with%20ensembles%20of%20lattices%202016"
        },
        {
            "id": "24",
            "entry": "[24] M. R. Gupta, A. Cotter, J. Pfeifer, K. Voevodski, K. Canini, A. Mangylov, W. Moczydlowski, and A. Van Esbroeck. Monotonic calibrated interpolated look-up tables. Journal of Machine Learning Research, 17(109):1\u201347, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20M.R.%20Cotter%2C%20A.%20Pfeifer%2C%20J.%20Voevodski%2C%20K.%20Monotonic%20calibrated%20interpolated%20look-up%20tables%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20M.R.%20Cotter%2C%20A.%20Pfeifer%2C%20J.%20Voevodski%2C%20K.%20Monotonic%20calibrated%20interpolated%20look-up%20tables%202016"
        },
        {
            "id": "25",
            "entry": "[25] E. K. Garcia and M. R. Gupta. Lattice regression. In Advances in Neural Information Processing Systems (NIPS), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garcia%2C%20E.K.%20Gupta%2C%20M.R.%20Lattice%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garcia%2C%20E.K.%20Gupta%2C%20M.R.%20Lattice%20regression%202009"
        },
        {
            "id": "26",
            "entry": "[26] E. K. Garcia, R. Arora, and M. R. Gupta. Optimized regression for efficient function evaluation. IEEE Trans. Image Processing, 21(9):4128\u20134140, Sept. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garcia%2C%20E.K.%20Arora%2C%20R.%20Gupta%2C%20M.R.%20Optimized%20regression%20for%20efficient%20function%20evaluation%202012-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garcia%2C%20E.K.%20Arora%2C%20R.%20Gupta%2C%20M.R.%20Optimized%20regression%20for%20efficient%20function%20evaluation%202012-09"
        },
        {
            "id": "27",
            "entry": "[27] A. Cotter, M. R. Gupta, and J. Pfeifer. A light touch for heavily constrained SGD. In 29th Annual Conference on Learning Theory, pages 729\u2013771, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cotter%2C%20A.%20Gupta%2C%20M.R.%20Pfeifer%2C%20J.%20A%20light%20touch%20for%20heavily%20constrained%20SGD%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cotter%2C%20A.%20Gupta%2C%20M.R.%20Pfeifer%2C%20J.%20A%20light%20touch%20for%20heavily%20constrained%20SGD%202016"
        },
        {
            "id": "28",
            "entry": "[28] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal Machine Learning Research, 12:2121\u20132159, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        }
    ]
}
