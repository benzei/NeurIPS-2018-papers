{
    "filename": "7910-provable-gaussian-embedding-with-one-observation.pdf",
    "metadata": {
        "title": "Provable Gaussian Embedding with One Observation",
        "author": "Ming Yu, Zhuoran Yang, Tuo Zhao, Mladen Kolar, Princeton Zhaoran Wang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7910-provable-gaussian-embedding-with-one-observation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The success of machine learning methods heavily relies on having an appropriate representation for data at hand. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about data. However, recently there has been a surge in approaches that learn how to encode the data automatically in a low dimensional space. Exponential family embedding provides a probabilistic framework for learning low-dimensional representation for various types of high-dimensional data [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]. Though successful in practice, theoretical underpinnings for exponential family embeddings have not been established. In this paper, we study the Gaussian embedding model and develop the first theoretical results for exponential family embedding models. First, we show that, under mild condition, the embedding structure can be learned from one observation by leveraging the parameter sharing between different contexts even though the data are dependent with each other. Second, we study properties of two algorithms used for learning the embedding structure and establish convergence results for each of them. The first algorithm is based on a convex relaxation, while the other solved the non-convex formulation of the problem directly. Experiments demonstrate the effectiveness of our approach."
    },
    "keywords": [
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "ising model",
            "url": "https://en.wikipedia.org/wiki/ising_model"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "We briefly review the exponential family embedding framework",
        "Not every graph structure can be learned from one observation, here we provide sufficient conditions on the graph that allow us to learn Gaussian embedding from one observation.\n3) we develop two methods for learning the embedding",
        "We focus on Gaussian embedding and develop the first theoretical result for exponential family embedding model",
        "We show that for various kinds of context structures, we are able to learn the embedding structure with only one observation",
        "It is useful to point out that, the theoretical framework we proposed is for general exponential family embedding models",
        "As long as the similar conditions are satisfied, the framework and theoretical results hold for any general exponential family embedding model as well"
    ],
    "key_statements": [
        "We briefly review the exponential family embedding framework",
        "Not every graph structure can be learned from one observation, here we provide sufficient conditions on the graph that allow us to learn Gaussian embedding from one observation.\n3) we develop two methods for learning the embedding",
        "The exponential family embedding is naturally related to the literature on probabilistic graphical models as the context structure forms a conditional dependence graph among the nodes",
        "In word embedding, x represents a document consisting of m words, xj is a one-hot vector representation of the j-th word, and p is the size of the dictionary",
        "We focus on Gaussian embedding and develop the first theoretical result for exponential family embedding model",
        "We show that for various kinds of context structures, we are able to learn the embedding structure with only one observation",
        "It is useful to point out that, the theoretical framework we proposed is for general exponential family embedding models",
        "As long as the similar conditions are satisfied, the framework and theoretical results hold for any general exponential family embedding model as well"
    ],
    "summary": [
        "We briefly review the exponential family embedding framework.",
        "The exponential family distribution is used to model the conditional distribution of xj given the context {xk, (k, j) 2 E} specified by the neighborhood of the node j in the graph G.",
        "In order for the learning of the embedding to be possible, one assumes how the parameters of the conditional distributions are shared across different nodes in the graph.",
        "The graph structure, conditional exponential family, and the way parameters are shared across the nodes are modeling choices and are application specific.",
        "The exponential family embedding is naturally related to the literature on probabilistic graphical models as the context structure forms a conditional dependence graph among the nodes.",
        "Much of the research effort on graphical model focus on learning the graph structure and the conditional dependency among nodes [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "We consider the case of Gaussian embedding, where the conditional distribution of xj given its context xcj is given in (2.3) with the conditional covariance matrix \u2303j unknown.",
        "In order to show that a minimizer of (3.5) gives a good estimator for M , we first show that the objective function L(\u00b7) is strongly convex under the assumption that the data are distributed according to (2.3) with the true parameter M \u21e4 = V \u21e4V \u21e4> with V \u21e4 2 Rp\u21e5r.",
        "Assumption (SC) gives sufficient condition on the graph structure, and it requires that the dependency among nodes is weak.",
        "The condition kAk2 \u00b7 kM k2 < 1 is satisfied naturally for a valid Gaussian embedding model.",
        "Assumption (SC) gives a sufficient condition on the graph structure so that the embedding is learnable.",
        "The following lemma proves that the minimum and maximum eigenvalues of the sample Hessian matrix H are bounded from below and above with high probability.",
        "We obtain the following main result for the non-convex optimization approach, which establishes a linear rate of convergence to a point that has the same statistical error rate as the convex relaxation approach studied in Theorem 4.2.",
        "All the data we observe are dependent, we show that the objective function is still well-behaved and we can learn the embedding structure reasonably well.",
        "It is useful to point out that, the theoretical framework we proposed is for general exponential family embedding models.",
        "As long as the similar conditions are satisfied, the framework and theoretical results hold for any general exponential family embedding model as well.",
        "Extending the result to other embedding models, for example the Ising model, is work in progress"
    ],
    "headline": "We study the Gaussian embedding model and develop the first theoretical results for exponential family embedding models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137\u20131155, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Ducharme%2C%20R%C3%A9jean%20Vincent%2C%20Pascal%20Jauvin%2C%20Christian%20A%20neural%20probabilistic%20language%20model%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Ducharme%2C%20R%C3%A9jean%20Vincent%2C%20Pascal%20Jauvin%2C%20Christian%20A%20neural%20probabilistic%20language%20model%202003"
        },
        {
            "id": "2",
            "entry": "[2] Florentina Bunea, Yiyuan She, and Marten H Wegkamp. Optimal selection of reduced rank estimators of high-dimensional matrices. The Annals of Statistics, pages 1282\u20131309, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bunea%2C%20Florentina%20She%2C%20Yiyuan%20Wegkamp%2C%20Marten%20H.%20Optimal%20selection%20of%20reduced%20rank%20estimators%20of%20high-dimensional%20matrices%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bunea%2C%20Florentina%20She%2C%20Yiyuan%20Wegkamp%2C%20Marten%20H.%20Optimal%20selection%20of%20reduced%20rank%20estimators%20of%20high-dimensional%20matrices%202011"
        },
        {
            "id": "3",
            "entry": "[3] Emmanuel J Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009"
        },
        {
            "id": "4",
            "entry": "[4] Shizhe Chen, Daniela M Witten, and Ali Shojaie. Selection and estimation for mixed graphical models. Biometrika, 102(1):47\u201364, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Shizhe%20Witten%2C%20Daniela%20M.%20Shojaie%2C%20Ali%20Selection%20and%20estimation%20for%20mixed%20graphical%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Shizhe%20Witten%2C%20Daniela%20M.%20Shojaie%2C%20Ali%20Selection%20and%20estimation%20for%20mixed%20graphical%20models%202014"
        },
        {
            "id": "5",
            "entry": "[5] GY Hu and Robert F O\u2019Connell. Analytical inversion of symmetric tridiagonal matrices. Journal of Physics A: Mathematical and General, 29(7):1511, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20G.Y.%20O%E2%80%99Connell%2C%20Robert%20F.%20Analytical%20inversion%20of%20symmetric%20tridiagonal%20matrices%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20G.Y.%20O%E2%80%99Connell%2C%20Robert%20F.%20Analytical%20inversion%20of%20symmetric%20tridiagonal%20matrices%201996"
        },
        {
            "id": "6",
            "entry": "[6] Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lauritzen%2C%20Steffen%20L.%20Graphical%20models%2C%20volume%2017%201996"
        },
        {
            "id": "7",
            "entry": "[7] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20Michel%20Talagrand%2C%20Michel%20Probability%20in%20Banach%20Spaces%3A%20isoperimetry%20and%20processes%202013"
        },
        {
            "id": "8",
            "entry": "[8] Jason D Lee and Trevor J Hastie. Learning the structure of mixed graphical models. Journal of Computational and Graphical Statistics, 24(1):230\u2013253, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Hastie%2C%20Trevor%20J.%20Learning%20the%20structure%20of%20mixed%20graphical%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Hastie%2C%20Trevor%20J.%20Learning%20the%20structure%20of%20mixed%20graphical%20models%202015"
        },
        {
            "id": "9",
            "entry": "[9] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pages 2177\u20132185, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Neural%20word%20embedding%20as%20implicit%20matrix%20factorization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Neural%20word%20embedding%20as%20implicit%20matrix%20factorization%202014"
        },
        {
            "id": "10",
            "entry": "[10] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Dagan%2C%20Ido%20Improving%20distributional%20similarity%20with%20lessons%20learned%20from%20word%20embeddings%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Dagan%2C%20Ido%20Improving%20distributional%20similarity%20with%20lessons%20learned%20from%20word%20embeddings%202015"
        },
        {
            "id": "11",
            "entry": "[11] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1301.3781"
        },
        {
            "id": "12",
            "entry": "[12] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746\u2013751, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Yih%2C%20Wen-tau%20Zweig%2C%20Geoffrey%20Linguistic%20regularities%20in%20continuous%20space%20word%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Yih%2C%20Wen-tau%20Zweig%2C%20Geoffrey%20Linguistic%20regularities%20in%20continuous%20space%20word%20representations%202013"
        },
        {
            "id": "13",
            "entry": "[13] Tanmoy Mukherjee and Timothy Hospedales. Gaussian visual-linguistic embedding for zeroshot recognition. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 912\u2013918, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mukherjee%2C%20Tanmoy%20Hospedales%2C%20Timothy%20Gaussian%20visual-linguistic%20embedding%20for%20zeroshot%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mukherjee%2C%20Tanmoy%20Hospedales%2C%20Timothy%20Gaussian%20visual-linguistic%20embedding%20for%20zeroshot%20recognition%202016"
        },
        {
            "id": "14",
            "entry": "[14] Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, pages 1069\u20131097, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Estimation%20of%20%28near%29%20low-rank%20matrices%20with%20noise%20and%20high-dimensional%20scaling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Estimation%20of%20%28near%29%20low-rank%20matrices%20with%20noise%20and%20high-dimensional%20scaling%202011"
        },
        {
            "id": "15",
            "entry": "[15] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and Trends R in Optimization, 1(3):127\u2013239, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parikh%2C%20Neal%20Boyd%2C%20Stephen%20Proximal%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parikh%2C%20Neal%20Boyd%2C%20Stephen%20Proximal%20algorithms%202014"
        },
        {
            "id": "16",
            "entry": "[16] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. Journal of Machine Learning Research, 11(Aug):2241\u20132259, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010"
        },
        {
            "id": "17",
            "entry": "[17] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471\u2013501, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Fazel%2C%20Maryam%20and%20Pablo%20A%20Parrilo.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Fazel%2C%20Maryam%20and%20Pablo%20A%20Parrilo.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010"
        },
        {
            "id": "18",
            "entry": "[18] Maja Rudolph and David Blei. Dynamic bernoulli embeddings for language evolution. arXiv preprint arXiv:1703.08052, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.08052"
        },
        {
            "id": "19",
            "entry": "[19] Maja Rudolph, Francisco Ruiz, Susan Athey, and David Blei. Structured embedding models for grouped data. In Advances in Neural Information Processing Systems, pages 250\u2013260, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudolph%2C%20Maja%20Ruiz%2C%20Francisco%20Athey%2C%20Susan%20Blei%2C%20David%20Structured%20embedding%20models%20for%20grouped%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudolph%2C%20Maja%20Ruiz%2C%20Francisco%20Athey%2C%20Susan%20Blei%2C%20David%20Structured%20embedding%20models%20for%20grouped%20data%202017"
        },
        {
            "id": "20",
            "entry": "[20] Maja Rudolph, Francisco Ruiz, Stephan Mandt, and David Blei. Exponential family embeddings. In Advances in Neural Information Processing Systems, pages 478\u2013486, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudolph%2C%20Maja%20Ruiz%2C%20Francisco%20Mandt%2C%20Stephan%20Blei%2C%20David%20Exponential%20family%20embeddings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudolph%2C%20Maja%20Ruiz%2C%20Francisco%20Mandt%2C%20Stephan%20Blei%2C%20David%20Exponential%20family%20embeddings%202016"
        },
        {
            "id": "21",
            "entry": "[21] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. IEEE Transactions on Information Theory, 62(11):6535\u20136579, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ruoyu%20Luo%2C%20Zhi-Quan%20Guaranteed%20matrix%20completion%20via%20non-convex%20factorization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ruoyu%20Luo%2C%20Zhi-Quan%20Guaranteed%20matrix%20completion%20via%20non-convex%20factorization%202016"
        },
        {
            "id": "22",
            "entry": "[22] Jialei Wang and Mladen Kolar. Inference for high-dimensional exponential family graphical models. In Artificial Intelligence and Statistics, pages 1042\u20131050, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jialei%20Kolar%2C%20Mladen%20Inference%20for%20high-dimensional%20exponential%20family%20graphical%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jialei%20Kolar%2C%20Mladen%20Inference%20for%20high-dimensional%20exponential%20family%20graphical%20models%202016"
        },
        {
            "id": "23",
            "entry": "[23] Yuchung J Wang and Edward H Ip. Conditionally specified continuous distributions. Biometrika, 95(3):735\u2013746, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yuchung%20J.%20Ip%2C%20Edward%20H.%20Conditionally%20specified%20continuous%20distributions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yuchung%20J.%20Ip%2C%20Edward%20H.%20Conditionally%20specified%20continuous%20distributions%202008"
        },
        {
            "id": "24",
            "entry": "[24] Zhaoran Wang, Huanran Lu, and Han Liu. Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial time. arXiv preprint arXiv:1408.5352, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1408.5352"
        },
        {
            "id": "25",
            "entry": "[25] Eunho Yang, Pradeep Ravikumar, Genevera I Allen, and Zhandong Liu. Graphical models via univariate exponential family distributions. Journal of Machine Learning Research, 16(1):3813\u2013 3847, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Eunho%20Ravikumar%2C%20Pradeep%20Allen%2C%20Genevera%20I.%20Liu%2C%20Zhandong%20Graphical%20models%20via%20univariate%20exponential%20family%20distributions",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Eunho%20Ravikumar%2C%20Pradeep%20Allen%2C%20Genevera%20I.%20Liu%2C%20Zhandong%20Graphical%20models%20via%20univariate%20exponential%20family%20distributions"
        },
        {
            "id": "26",
            "entry": "[26] Ming Yu, Varun Gupta, and Mladen Kolar. An influence-receptivity model for topic based information cascades. 2017 IEEE International Conference on Data Mining (ICDM), pages 1141\u20131146, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Ming%20Gupta%2C%20Varun%20Kolar%2C%20Mladen%20An%20influence-receptivity%20model%20for%20topic%20based%20information%20cascades%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Ming%20Gupta%2C%20Varun%20Kolar%2C%20Mladen%20An%20influence-receptivity%20model%20for%20topic%20based%20information%20cascades%202017"
        },
        {
            "id": "27",
            "entry": "[27] Ming Yu, Varun Gupta, and Mladen Kolar. Learning influence-receptivity network structure with guarantee. arXiv preprint arXiv:1806.05730, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.05730"
        },
        {
            "id": "28",
            "entry": "[28] Ming Yu, Mladen Kolar, and Varun Gupta. Statistical inference for pairwise graphical models using score matching. In Advances in Neural Information Processing Systems, pages 2829\u20132837, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Ming%20Kolar%2C%20Mladen%20Gupta%2C%20Varun%20Statistical%20inference%20for%20pairwise%20graphical%20models%20using%20score%20matching%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Ming%20Kolar%2C%20Mladen%20Gupta%2C%20Varun%20Statistical%20inference%20for%20pairwise%20graphical%20models%20using%20score%20matching%202016"
        },
        {
            "id": "29",
            "entry": "[29] Ming Yu, Zhaoran Wang, Varun Gupta, and Mladen Kolar. Recovery of simultaneous low rank and two-way sparse coefficient matrices, a nonconvex approach. arXiv preprint arXiv:1802.06967, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06967"
        },
        {
            "id": "30",
            "entry": "[30] Tuo Zhao, Zhaoran Wang, and Han Liu. Nonconvex low rank matrix factorization via inexact first order oracle. Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Tuo%20Wang%2C%20Zhaoran%20Liu%2C%20Han%20Nonconvex%20low%20rank%20matrix%20factorization%20via%20inexact%20first%20order%20oracle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Tuo%20Wang%2C%20Zhaoran%20Liu%2C%20Han%20Nonconvex%20low%20rank%20matrix%20factorization%20via%20inexact%20first%20order%20oracle%202015"
        },
        {
            "id": "31",
            "entry": "[31] Will Y Zou, Richard Socher, Daniel Cer, and Christopher D Manning. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393\u20131398, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Will%20Y.%20Socher%2C%20Richard%20Cer%2C%20Daniel%20Manning%2C%20Christopher%20D.%20Bilingual%20word%20embeddings%20for%20phrase-based%20machine%20translation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Will%20Y.%20Socher%2C%20Richard%20Cer%2C%20Daniel%20Manning%2C%20Christopher%20D.%20Bilingual%20word%20embeddings%20for%20phrase-based%20machine%20translation%202013"
        }
    ]
}
