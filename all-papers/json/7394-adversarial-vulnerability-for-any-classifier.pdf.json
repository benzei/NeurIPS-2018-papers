{
    "filename": "7394-adversarial-vulnerability-for-any-classifier.pdf",
    "metadata": {
        "title": "Adversarial vulnerability for any classifier",
        "author": "Alhussein Fawzi, Hamza Fawzi, Omar Fawzi",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7394-adversarial-vulnerability-for-any-classifier.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets."
    },
    "keywords": [
        {
            "term": "upper bound",
            "url": "https://en.wikipedia.org/wiki/upper_bound"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "unsupervised feature learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_feature_learning"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Deep neural networks are powerful models that achieve state-of-the-art performance across several domains, such as bioinformatics [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], speech [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and computer vision [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "When the data is defined according to a distribution which can be approximated by a smooth, high-dimensional generative model, our results show that arbitrary classifiers will have small adversarial examples in expectation",
        "We have shown the existence of a baseline robustness that no classifier can surpass, whenever the distribution is approximable by a generative model mapping latent representations to images",
        "The bounds lead to informative numerical results: for example, on the CIFAR-10 task, our upper bound shows that a significant portion of datapoints can be fooled with a perturbation of magnitude 10% that of an image",
        "We expect the design of more robust classifiers to get closer to this upper bound",
        "The existence of a baseline robustness is fundamental in that context in order to measure the progress made and compare to the optimal robustness we can hope to achieve"
    ],
    "key_statements": [
        "Deep neural networks are powerful models that achieve state-of-the-art performance across several domains, such as bioinformatics [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], speech [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and computer vision [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We assume that the data distribution is defined by a smooth generative model, and study theoretically the existence of small adversarial perturbations for arbitrary classifiers",
        "We show fundamental upper bounds on the robustness of any classifier to perturbations, which provides a baseline to the maximal achievable robustness",
        "We prove the existence of adversarial perturbations that transfer across different classifiers",
        "On the other hand, it is the case that the human visual system is inherently robust to small perturbations, our analysis shows that a distribution over natural images cannot be modeled by smooth and high-dimensional generative models",
        "Motivated by the success of generative models mapping latent representations with a normal prior, we instead study the existence of robust classifiers under this general data-generating procedure and derive bounds on the robustness that hold for any classification function",
        "When the data is defined according to a distribution which can be approximated by a smooth, high-dimensional generative model, our results show that arbitrary classifiers will have small adversarial examples in expectation",
        "We have shown the existence of a baseline robustness that no classifier can surpass, whenever the distribution is approximable by a generative model mapping latent representations to images",
        "The bounds lead to informative numerical results: for example, on the CIFAR-10 task, our upper bound shows that a significant portion of datapoints can be fooled with a perturbation of magnitude 10% that of an image",
        "We expect the design of more robust classifiers to get closer to this upper bound",
        "The existence of a baseline robustness is fundamental in that context in order to measure the progress made and compare to the optimal robustness we can hope to achieve"
    ],
    "summary": [
        "Deep neural networks are powerful models that achieve state-of-the-art performance across several domains, such as bioinformatics [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], speech [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and computer vision [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "We assume that the data distribution is defined by a smooth generative model, and study theoretically the existence of small adversarial perturbations for arbitrary classifiers.",
        "On the other hand, it is the case that the human visual system is inherently robust to small perturbations, our analysis shows that a distribution over natural images cannot be modeled by smooth and high-dimensional generative models.",
        "Motivated by the success of generative models mapping latent representations with a normal prior, we instead study the existence of robust classifiers under this general data-generating procedure and derive bounds on the robustness that hold for any classification function.",
        "We state a general bound on the robustness to perturbations and derive two special cases to make more explicit the dependence on the distribution and number of classes.",
        "We specifically assume that the generated distribution g\u2217(\u03bd) provides an approximation to the true underlying distribution in the 1-Wasserstein sense on the metric space (X , \u00b7 ); i.e., W (g\u2217(\u03bd), \u03bc) \u2264 \u03b4, and derive upper bounds on the robustness.",
        "When the data is defined according to a distribution which can be approximated by a smooth, high-dimensional generative model, our results show that arbitrary classifiers will have small adversarial examples in expectation.",
        "We train a DCGAN [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] generative model on this dataset, with a latent vector dimension d = 100, and further consider several neural networks architectures for classification.10 For each classifier, the empirical robustness is compared to our upper bound.11 In addition to reporting the in-distribution and unconstrained robustness, we report the robustness in the latent space: rZ = minr r 2 s.t. f (g(z + r)) = f (g(z)).",
        "Our bounds notably predict that any classifier defined on this task will have perturbations not exceeding 1/10 of the norm of the image, for 25% of the datapoints in the distribution.",
        "We have shown the existence of a baseline robustness that no classifier can surpass, whenever the distribution is approximable by a generative model mapping latent representations to images.",
        "To construct classifiers with better robustness, our analysis suggests that these should have linear decision boundaries in the latent space; in particular, classifiers with multiple disconnected classification regions will be more prone to small perturbations.",
        "If we take as a premise that human visual system classifiers require large-norm perturbations to be fooled, our work shows that natural image distributions cannot be modeled as very high dimensional and smooth mappings."
    ],
    "headline": "We study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Spencer, J. Eickholt, and J. Cheng, \u201cA deep learning network approach to ab initio protein secondary structure prediction,\u201d IEEE/ACM Trans. Comput. Biol. Bioinformatics, vol. 12, no. 1, pp. 103\u2013112, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spencer%2C%20M.%20Eickholt%2C%20J.%20Cheng%2C%20J.%20A%20deep%20learning%20network%20approach%20to%20ab%20initio%20protein%20secondary%20structure%20prediction%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spencer%2C%20M.%20Eickholt%2C%20J.%20Cheng%2C%20J.%20A%20deep%20learning%20network%20approach%20to%20ab%20initio%20protein%20secondary%20structure%20prediction%2C%202015"
        },
        {
            "id": "2",
            "entry": "[2] D. Chicco, P. Sadowski, and P. Baldi, \u201cDeep autoencoder neural networks for gene ontology annotation predictions,\u201d in ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, pp. 533\u2013540, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chicco%2C%20D.%20Sadowski%2C%20P.%20Baldi%2C%20P.%20Deep%20autoencoder%20neural%20networks%20for%20gene%20ontology%20annotation%20predictions%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chicco%2C%20D.%20Sadowski%2C%20P.%20Baldi%2C%20P.%20Deep%20autoencoder%20neural%20networks%20for%20gene%20ontology%20annotation%20predictions%2C%202014"
        },
        {
            "id": "3",
            "entry": "[3] G. E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%2C%202012"
        },
        {
            "id": "4",
            "entry": "[4] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d arXiv preprint arXiv:1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "5",
            "entry": "[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems (NIPS), pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%2C%202012"
        },
        {
            "id": "6",
            "entry": "[6] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d in International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Zaremba%2C%20W.%20Sutskever%2C%20I.%20Bruna%2C%20J.%20Intriguing%20properties%20of%20neural%20networks%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Zaremba%2C%20W.%20Sutskever%2C%20I.%20Bruna%2C%20J.%20Intriguing%20properties%20of%20neural%20networks%2C%202014"
        },
        {
            "id": "7",
            "entry": "[7] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndic, P. Laskov, G. Giacinto, and F. Roli, \u201cEvasion attacks against machine learning at test time,\u201d in Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387\u2013402, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20B.%20Corona%2C%20I.%20Maiorca%2C%20D.%20Nelson%2C%20B.%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20B.%20Corona%2C%20I.%20Maiorca%2C%20D.%20Nelson%2C%20B.%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time%2C%202013"
        },
        {
            "id": "8",
            "entry": "[8] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d in International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%2C%202015"
        },
        {
            "id": "9",
            "entry": "[9] U. Shaham, Y. Yamada, and S. Negahban, \u201cUnderstanding adversarial training: Increasing local stability of neural nets through robust optimization,\u201d arXiv preprint arXiv:1511.05432, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05432"
        },
        {
            "id": "10",
            "entry": "[10] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "11",
            "entry": "[11] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier, \u201cParseval networks: Improving robustness to adversarial examples,\u201d in International Conference on Machine Learning, pp. 854\u2013863, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20M.%20Bojanowski%2C%20P.%20Grave%2C%20E.%20Dauphin%2C%20Y.%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20M.%20Bojanowski%2C%20P.%20Grave%2C%20E.%20Dauphin%2C%20Y.%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%2C%202017"
        },
        {
            "id": "12",
            "entry": "[12] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, \u201cDistillation as a defense to adversarial perturbations against deep neural networks,\u201d arXiv preprint arXiv:1511.04508, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.04508"
        },
        {
            "id": "13",
            "entry": "[13] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, \u201cDeep variational information bottleneck,\u201d arXiv preprint arXiv:1612.00410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00410"
        },
        {
            "id": "14",
            "entry": "[14] N. Carlini and D. Wagner, \u201cAdversarial examples are not easily detected: Bypassing ten detection methods,\u201d in Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 3\u201314, ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%2C%202017"
        },
        {
            "id": "15",
            "entry": "[15] J. Uesato, B. O\u2019Donoghue, A. v. d. Oord, and P. Kohli, \u201cAdversarial risk and the dangers of evaluating against weak attacks,\u201d arXiv preprint arXiv:1802.05666, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05666"
        },
        {
            "id": "16",
            "entry": "[16] J. Rauber and W. Brendel, \u201cThe robust vision benchmark.\u201d http://robust.vision, 2017.",
            "url": "http://robust.vision"
        },
        {
            "id": "17",
            "entry": "[17] A. Ilyas, A. Jalal, E. Asteri, C. Daskalakis, and A. G. Dimakis, \u201cThe robust manifold defense: Adversarial training using generative models,\u201d arXiv preprint arXiv:1712.09196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09196"
        },
        {
            "id": "18",
            "entry": "[18] J. Gilmer, L. Metz, F. Faghri, S. S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow, \u201cAdversarial spheres,\u201d arXiv preprint arXiv:1801.02774, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "19",
            "entry": "[19] A. Fawzi, O. Fawzi, and P. Frossard, \u201cAnalysis of classifiers\u2019 robustness to adversarial perturbations,\u201d CoRR, vol. abs/1502.02590, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.02590"
        },
        {
            "id": "20",
            "entry": "[20] A. Fawzi, S. Moosavi-Dezfooli, and P. Frossard, \u201cRobustness of classifiers: from adversarial to random noise,\u201d in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20A.%20Moosavi-Dezfooli%2C%20S.%20Frossard%2C%20P.%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20A.%20Moosavi-Dezfooli%2C%20S.%20Frossard%2C%20P.%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%2C%202016"
        },
        {
            "id": "21",
            "entry": "[21] T. Tanay and L. Griffin, \u201cA boundary tilting persepective on the phenomenon of adversarial examples,\u201d arXiv preprint arXiv:1608.07690, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.07690"
        },
        {
            "id": "22",
            "entry": "[22] M. Hein and M. Andriushchenko, \u201cFormal guarantees on the robustness of a classifier against adversarial manipulation,\u201d in Advances in Neural Information Processing Systems, pp. 2263\u20132273, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20M.%20Andriushchenko%2C%20M.%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20M.%20Andriushchenko%2C%20M.%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%2C%202017"
        },
        {
            "id": "23",
            "entry": "[23] J. Peck, J. Roels, B. Goossens, and Y. Saeys, \u201cLower bounds on the robustness to adversarial perturbations,\u201d in Advances in Neural Information Processing Systems, pp. 804\u2013813, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peck%2C%20J.%20Roels%2C%20J.%20Goossens%2C%20B.%20Saeys%2C%20Y.%20Lower%20bounds%20on%20the%20robustness%20to%20adversarial%20perturbations%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peck%2C%20J.%20Roels%2C%20J.%20Goossens%2C%20B.%20Saeys%2C%20Y.%20Lower%20bounds%20on%20the%20robustness%20to%20adversarial%20perturbations%2C%202017"
        },
        {
            "id": "24",
            "entry": "[24] A. Sinha, H. Namkoong, and J. Duchi, \u201cCertifiable distributional robustness with principled adversarial training,\u201d arXiv preprint arXiv:1710.10571, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10571"
        },
        {
            "id": "25",
            "entry": "[25] A. Raghunathan, J. Steinhardt, and P. Liang, \u201cCertified defenses against adversarial examples,\u201d arXiv preprint arXiv:1801.09344, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09344"
        },
        {
            "id": "26",
            "entry": "[26] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, \u201cA dual approach to scalable verification of deep networks,\u201d arXiv preprint arXiv:1803.06567, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06567"
        },
        {
            "id": "27",
            "entry": "[27] J. Kos, I. Fischer, and D. Song, \u201cAdversarial examples for generative models,\u201d arXiv preprint arXiv:1702.06832, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06832"
        },
        {
            "id": "28",
            "entry": "[28] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "29",
            "entry": "[29] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d in Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014"
        },
        {
            "id": "30",
            "entry": "[30] A. Radford, L. Metz, and S. Chintala, \u201cUnsupervised representation learning with deep convolutional generative adversarial networks,\u201d arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "31",
            "entry": "[31] M. Arjovsky, S. Chintala, and L. Bottou, \u201cWasserstein generative adversarial networks,\u201d in International Conference on Machine Learning, pp. 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%2C%202017"
        },
        {
            "id": "32",
            "entry": "[32] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, \u201cImproved training of wasserstein gans,\u201d in Advances in Neural Information Processing Systems, pp. 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%2C%202017"
        },
        {
            "id": "33",
            "entry": "[33] C. Borell, \u201cThe Brunn-Minkowski inequality in Gauss space,\u201d Inventiones mathematicae, vol. 30, no. 2, pp. 207\u2013216, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borell%2C%20C.%20The%20Brunn-Minkowski%20inequality%20in%20Gauss%20space%2C%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borell%2C%20C.%20The%20Brunn-Minkowski%20inequality%20in%20Gauss%20space%2C%201975"
        },
        {
            "id": "34",
            "entry": "[34] V. N. Sudakov and B. S. Tsirel\u2019son, \u201cExtremal properties of half-spaces for spherically invariant measures,\u201d Journal of Soviet Mathematics, vol. 9, no. 1, pp. 9\u201318, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sudakov%2C%20V.N.%20Tsirel%E2%80%99son%2C%20B.S.%20Extremal%20properties%20of%20half-spaces%20for%20spherically%20invariant%20measures%2C%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sudakov%2C%20V.N.%20Tsirel%E2%80%99son%2C%20B.S.%20Extremal%20properties%20of%20half-spaces%20for%20spherically%20invariant%20measures%2C%201978"
        },
        {
            "id": "35",
            "entry": "[35] P. Samangouei, M. Kabkab, and R. Chellappa, \u201cDefense-gan: Protecting classifiers against adversarial attacks using generative models,\u201d in International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samangouei%2C%20P.%20Kabkab%2C%20M.%20Chellappa%2C%20R.%20Defense-gan%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samangouei%2C%20P.%20Kabkab%2C%20M.%20Chellappa%2C%20R.%20Defense-gan%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%2C%202018"
        },
        {
            "id": "36",
            "entry": "[36] Y. Liu, X. Chen, C. Liu, and D. Song, \u201cDelving into transferable adversarial examples and black-box attacks,\u201d arXiv preprint arXiv:1611.02770, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02770"
        },
        {
            "id": "37",
            "entry": "[37] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, \u201cReading digits in natural images with unsupervised feature learning,\u201d in NIPS workshop on deep learning and unsupervised feature learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Y.%20Wang%2C%20T.%20Coates%2C%20A.%20Bissacco%2C%20A.%20%E2%80%9CReading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%2C%E2%80%9D%20in%20NIPS%20workshop%20on%20deep%20learning%20and%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "38",
            "entry": "[38] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, \u201cDeepfool: a simple and accurate method to fool deep neural networks,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.-M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20S.-M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%2C%202016"
        },
        {
            "id": "39",
            "entry": "[39] A. Krizhevsky and G. Hinton, \u201cLearning multiple layers of features from tiny images,\u201d Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%2C%202009"
        },
        {
            "id": "40",
            "entry": "[40] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d in International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%2C%202014"
        },
        {
            "id": "41",
            "entry": "[41] S. Zagoruyko and N. Komodakis, \u201cWide residual networks,\u201d arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "42",
            "entry": "[42] G. F. Elsayed, S. Shankar, B. Cheung, N. Papernot, A. Kurakin, I. Goodfellow, and J. Sohl-Dickstein, \u201cAdversarial examples that fool both human and computer vision,\u201d arXiv preprint arXiv:1802.08195, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1802.08195"
        }
    ]
}
