{
    "filename": "7514-wasserstein-variational-inference.pdf",
    "metadata": {
        "title": "Wasserstein Variational Inference",
        "author": "Luca Ambrogioni, Umut G\u00fc\u00e7l\u00fc, Ya\u011fmur G\u00fc\u00e7l\u00fct\u00fcrk, Max Hinne, Marcel A. J. van Gerven, Eric Maris",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7514-wasserstein-variational-inference.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques."
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "probability mass",
            "url": "https://en.wikipedia.org/wiki/probability_mass"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "new form",
            "url": "https://en.wikipedia.org/wiki/New_Form"
        },
        {
            "term": "wasserstein distance",
            "url": "https://en.wikipedia.org/wiki/wasserstein_distance"
        }
    ],
    "highlights": [
        "Optimal transport divergences quantify the distance between two probability distributions as the cost of transporting probability mass from one to the other",
        "Using this family of divergences we introduce the new framework of Wasserstein variational inference, which exploits the celebrated Sinkhorn iterations [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and automatic differentiation",
        "Optimal transport divergences quantify the distance between two probability distributions as the cost of transporting probability mass from one to the other",
        "In our notation the subscript p in gp denotes the fact that the distribution p(z|x) and the function gp depend on a common set of parameters which are optimized during variational inference",
        "In the previous sections we showed that variational inference based on f-divergences is a special case of Wasserstein variational inference",
        "In this paper we showed that Wasserstein variational inference offers an effective and robust method for black-box variational Bayesian inference"
    ],
    "key_statements": [
        "Optimal transport divergences quantify the distance between two probability distributions as the cost of transporting probability mass from one to the other",
        "Using this family of divergences we introduce the new framework of Wasserstein variational inference, which exploits the celebrated Sinkhorn iterations [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and automatic differentiation",
        "Optimal transport divergences quantify the distance between two probability distributions as the cost of transporting probability mass from one to the other",
        "In our notation the subscript p in gp denotes the fact that the distribution p(z|x) and the function gp depend on a common set of parameters which are optimized during variational inference",
        "In the previous sections we showed that variational inference based on f-divergences is a special case of Wasserstein variational inference",
        "In this paper we showed that Wasserstein variational inference offers an effective and robust method for black-box variational Bayesian inference"
    ],
    "summary": [
        "Optimal transport divergences quantify the distance between two probability distributions as the cost of transporting probability mass from one to the other.",
        "Using this family of divergences we introduce the new framework of Wasserstein variational inference, which exploits the celebrated Sinkhorn iterations [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and automatic differentiation.",
        "Using this family of distributions we can define the entropy regularized optimal transport divergence: Wc, (p, q) = inf c(x1, x2) du(x1, x2) .",
        "When p and q are discrete distributions the regularized optimal transport cost can be efficiently obtained using the Sinkhorn iterations [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "All optimal transport divergences are part of the c-Wasserstein family, where Cp,q(x, y) reduces to a non-negative valued function c(x1, x2) independent from p and q.",
        "In order to apply optimal transport divergences to a Bayesian variational problem we need to assign a metric, or more generally a transport cost, to the latent space of the Bayesian model.",
        "In our notation the subscript p in gp denotes the fact that the distribution p(z|x) and the function gp depend on a common set of parameters which are optimized during variational inference.",
        "Another interesting special case of c-Wasserstein divergence can be obtained by considering the distribution of the residuals of an autoencoder.",
        "We can define the observable autoencoder cost functional as follows: COp A(x1, z1; x2, z2) = d(x1 \u2212 gp(z1), x2 \u2212 gp(z2)) , (18)",
        "From Theorem 2 it follows that this cost functional defines a valid c-Wasserstein divergence.",
        "This is a generalized form of operator variational loss where the functional family can depend on p and q.",
        "In the case of optimal transport divergences, where Cp,q(x1, z1; x2, z2) = c(x1, z1; x2, z2), the resulting loss is a special case of the regular operator variational loss.",
        "The recently introduced Wasserstein autoencoder (WAE) uses a regularized optimal transport divergence between p(x) and k(x) in order to train a generative model [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>].",
        "When D(p(z) q(z)) is a c-Wasserstein divergence, we can show that the LW A is a Wasserstein variational inference loss and that Wasserstein autoencoders are approximate Bayesian methods.",
        "Our reformulation suggests another way of training the latent space using a metric optimal transport divergence and the Sinkhorn iterations.",
        "The cost functional of our Wasserstein variational autoencoder had the weights w1, w2, w3 and w4 different from zero.",
        "These methods are special cases of Wasserstein variational autoencoders with non-zero w5 weight and where the f function is chosen to give either the reverse KL divergence or the Jensen-Shannon divergence respectively.",
        "These features make Wasserstein variational inference particularly suitable for probabilistic programming, where the aim is to combine declarative general purpose programming and automatic probabilistic inference"
    ],
    "headline": "This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "2",
            "entry": "[2] R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. International Conference on Artificial Intelligence and Statistic, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "3",
            "entry": "[3] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "4",
            "entry": "[4] A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei. Automatic differentiation variational inference. The Journal of Machine Learning Research, 18(1):430\u2013474, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kucukelbir%2C%20A.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Gelman%2C%20A.%20Automatic%20differentiation%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kucukelbir%2C%20A.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Gelman%2C%20A.%20Automatic%20differentiation%20variational%20inference%202017"
        },
        {
            "id": "5",
            "entry": "[5] D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Edward: A library for probabilistic modeling, inference, and criticism. arXiv preprint arXiv:1610.09787, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09787"
        },
        {
            "id": "6",
            "entry": "[6] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20D.M.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20D.M.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "7",
            "entry": "[7] C. Zhang, J. Butepage, H. Kjellstrom, and S. Mandt. Advances in variational inference. arXiv preprint arXiv:1711.05597, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05597"
        },
        {
            "id": "8",
            "entry": "[8] Y. Li and R. E. Turner. R\u00e9nyi divergence variational inference. Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Turner%2C%20R.E.%20R%C3%A9nyi%20divergence%20variational%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Turner%2C%20R.E.%20R%C3%A9nyi%20divergence%20variational%20inference%202016"
        },
        {
            "id": "9",
            "entry": "[9] L. Ambrogioni, U. G\u00fc\u00e7l\u00fc, J. Berezutskaya, E. W.P. van den Borne, Y. G\u00fc\u00e7l\u00fct\u00fcrk, M. Hinne, E. Maris, and M. A.J. van Gerven. Forward amortized inference for likelihood-free variational marginalization. arXiv preprint arXiv:1805.11542, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11542"
        },
        {
            "id": "10",
            "entry": "[10] R. Ranganath, D. Tran, J. Altosaar, and D. Blei. Operator variational inference. Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tran%2C%20D.%20Altosaar%2C%20J.%20Blei%2C%20D.%20Operator%20variational%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Tran%2C%20D.%20Altosaar%2C%20J.%20Blei%2C%20D.%20Operator%20variational%20inference%202016"
        },
        {
            "id": "11",
            "entry": "[11] A. B. Dieng, D. Tran, R. Ranganath, J. Paisley, and D. Blei. Variational inference via chi upper bound minimization. Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieng%2C%20A.B.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Paisley%2C%20J.%20Variational%20inference%20via%20chi%20upper%20bound%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieng%2C%20A.B.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Paisley%2C%20J.%20Variational%20inference%20via%20chi%20upper%20bound%20minimization%202017"
        },
        {
            "id": "12",
            "entry": "[12] R. Bamler, C. Zhang, M. Opper, and S. Mandt. Perturbative black box variational inference. Advances in Neural Information Processing Systems, pages 5086\u20135094, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bamler%2C%20R.%20Zhang%2C%20C.%20Opper%2C%20M.%20Mandt%2C%20S.%20Perturbative%20black%20box%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bamler%2C%20R.%20Zhang%2C%20C.%20Opper%2C%20M.%20Mandt%2C%20S.%20Perturbative%20black%20box%20variational%20inference%202017"
        },
        {
            "id": "13",
            "entry": "[13] C. Villani. Topics in Optimal Transportation. Number 58. American Mathematical Society, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C.%20Topics%20in%20Optimal%20Transportation.%20Number%2058%202003"
        },
        {
            "id": "14",
            "entry": "[14] Cuturi M. Peyr\u00e9 G. Computational Optimal Transport. arXiv preprint arXiv:1803.00567, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.00567"
        },
        {
            "id": "15",
            "entry": "[15] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein GANs. Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "17",
            "entry": "[17] A. Genevay, G. Peyr\u00e9, and M. Cuturi. Learning generative models with Sinkhorn divergences. International Conference on Artificial Intelligence and Statistics, pages 1608\u20131617, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20Genevay%2C%20G.%20Peyr%C3%A9%20Cuturi%2C%20M.%20Learning%20generative%20models%20with%20Sinkhorn%20divergences%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20Genevay%2C%20G.%20Peyr%C3%A9%20Cuturi%2C%20M.%20Learning%20generative%20models%20with%20Sinkhorn%20divergences%202018"
        },
        {
            "id": "18",
            "entry": "[18] G. Montavon, K. M\u00fcller, and M. Cuturi. Wasserstein training of restricted Boltzmann machines. Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montavon%2C%20G.%20M%C3%BCller%2C%20K.%20Cuturi%2C%20M.%20Wasserstein%20training%20of%20restricted%20Boltzmann%20machines%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montavon%2C%20G.%20M%C3%BCller%2C%20K.%20Cuturi%2C%20M.%20Wasserstein%20training%20of%20restricted%20Boltzmann%20machines%202016"
        },
        {
            "id": "19",
            "entry": "[19] R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343\u2013348, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinkhorn%2C%20R.%20Knopp%2C%20P.%20Concerning%20nonnegative%20matrices%20and%20doubly%20stochastic%20matrices%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinkhorn%2C%20R.%20Knopp%2C%20P.%20Concerning%20nonnegative%20matrices%20and%20doubly%20stochastic%20matrices%201967"
        },
        {
            "id": "20",
            "entry": "[20] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20M.%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20M.%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013"
        },
        {
            "id": "21",
            "entry": "[21] F. Husz\u00e1r. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08235"
        },
        {
            "id": "22",
            "entry": "[22] D. Tran, R. Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free variational inference. arXiv preprint arXiv:1702.08896, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08896"
        },
        {
            "id": "23",
            "entry": "[23] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville. Adversarially learned inference. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20V.%20Belghazi%2C%20I.%20Poole%2C%20B.%20Mastropietro%2C%20O.%20Adversarially%20learned%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20V.%20Belghazi%2C%20I.%20Poole%2C%20B.%20Mastropietro%2C%20O.%20Adversarially%20learned%20inference%202017"
        },
        {
            "id": "24",
            "entry": "[24] L. Mescheder, S. Nowozin, and A. Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04722"
        },
        {
            "id": "25",
            "entry": "[25] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Bottou%2C%20L.%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Bottou%2C%20L.%20Towards%20principled%20methods%20for%20training%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "26",
            "entry": "[26] D. P. Kingma and M. Welling. Auto\u2013encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "27",
            "entry": "[27] D. Fouskakis and D. Draper. Stochastic optimization: A review. International Statistical Review, 70(3):315\u2013349, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fouskakis%2C%20D.%20Draper%2C%20D.%20Stochastic%20optimization%3A%20A%20review%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fouskakis%2C%20D.%20Draper%2C%20D.%20Stochastic%20optimization%3A%20A%20review%202002"
        },
        {
            "id": "28",
            "entry": "[28] J. Weed and F. Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00087"
        },
        {
            "id": "29",
            "entry": "[29] D. Burago, I. D. Burago, and S. Ivanov. A Course in Metric Geometry, volume 33. American Mathematical Society, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burago%2C%20D.%20Burago%2C%20I.D.%20Ivanov%2C%20S.%20A%20Course%20in%20Metric%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burago%2C%20D.%20Burago%2C%20I.D.%20Ivanov%2C%20S.%20A%20Course%20in%20Metric%202001"
        },
        {
            "id": "30",
            "entry": "[30] Y. Pu, Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens, and L. Carin. Variational autoencoder for deep learning of images, labels and captions. Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pu%2C%20Y.%20Gan%2C%20Z.%20Henao%2C%20R.%20Yuan%2C%20X.%20Variational%20autoencoder%20for%20deep%20learning%20of%20images%2C%20labels%20and%20captions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pu%2C%20Y.%20Gan%2C%20Z.%20Henao%2C%20R.%20Yuan%2C%20X.%20Variational%20autoencoder%20for%20deep%20learning%20of%20images%2C%20labels%20and%20captions%202016"
        },
        {
            "id": "31",
            "entry": "[31] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05644"
        },
        {
            "id": "32",
            "entry": "[32] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. International Conference on Learning Representations, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tolstikhin%2C%20I.%20Bousquet%2C%20O.%20Gelly%2C%20S.%20Schoelkopf%2C%20B.%20Wasserstein%20auto-encoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tolstikhin%2C%20I.%20Bousquet%2C%20O.%20Gelly%2C%20S.%20Schoelkopf%2C%20B.%20Wasserstein%20auto-encoders%202018"
        }
    ]
}
