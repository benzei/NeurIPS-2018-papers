{
    "filename": "8168-optimal-algorithms-for-continuous-non-monotone-submodular-and-dr-submodular-maximization.pdf",
    "metadata": {
        "title": "Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization",
        "date": 2018,
        "author": "Rad Niazadeh Department of Computer Science",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8168-optimal-algorithms-for-continuous-non-monotone-submodular-and-dr-submodular-maximization.pdf"
        },
        "abstract": "In this paper we study the fundamental problems of maximizing abcontinuous non-monotone submodular function over a hypercube, with and without coordinatewise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first"
    },
    "keywords": [
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "submodular maximization",
            "url": "https://en.wikipedia.org/wiki/submodular_maximization"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "Submodular optimization is a sweet spot between tractability and expressiveness, with numerous applications in machine learning (e.g., <a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\"><a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\">Krause and Golovin [2014</a></a>], and see below) and with many algorithms that are both practical and enjoy good rigorous guarantees (e.g., <a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\"><a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\">Buchbinder et al [2012</a></a>, 2015])",
        "For the special case of DR-submodular maximization, i.e., when the submodular functions is coordinate-wise concave along all coordinates, we provide a faster",
        "Submodular optimization is a sweet spot between tractability and expressiveness, with numerous applications in machine learning (e.g., <a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\">Krause and Golovin [2014</a>], and see below) and with many algorithms that are both practical and enjoy good rigorous guarantees (e.g., <a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\">Buchbinder et al [2012</a>, 2015])",
        "We proposed a tight approximation algorithm for continuous submodular maximization, and a qausilinear time tight approximation algorithm for the special case of DR-submodular maxmization"
    ],
    "key_statements": [
        "Submodular optimization is a sweet spot between tractability and expressiveness, with numerous applications in machine learning (e.g., <a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\"><a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\">Krause and Golovin [2014</a></a>], and see below) and with many algorithms that are both practical and enjoy good rigorous guarantees (e.g., <a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\"><a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\">Buchbinder et al [2012</a></a>, 2015])",
        "For the special case of DR-submodular maximization, i.e., when the submodular functions is coordinate-wise concave along all coordinates, we provide a faster",
        "Submodular optimization is a sweet spot between tractability and expressiveness, with numerous applications in machine learning (e.g., <a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\">Krause and Golovin [2014</a>], and see below) and with many algorithms that are both practical and enjoy good rigorous guarantees (e.g., <a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\">Buchbinder et al [2012</a>, 2015])",
        "We proposed a tight approximation algorithm for continuous submodular maximization, and a qausilinear time tight approximation algorithm for the special case of DR-submodular maxmization"
    ],
    "summary": [
        "Submodular optimization is a sweet spot between tractability and expressiveness, with numerous applications in machine learning (e.g., <a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\"><a class=\"ref-link\" id=\"cKrause_2014_a\" href=\"#rKrause_2014_a\">Krause and Golovin [2014</a></a>], and see below) and with many algorithms that are both practical and enjoy good rigorous guarantees (e.g., <a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\"><a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\">Buchbinder et al [2012</a></a>, 2015]).",
        "-approximation algorithm for continuous submodular function maximization; the approximation factor of",
        "The main result of this paper achieves this best-case scenario: There is an algorithm for maximizing a continuous submodular function over the hypercube that guarantees a",
        "-approximation deterministic bi-greedy algorithm of [<a class=\"ref-link\" id=\"cBuchbinder_et+al_2012_a\" href=\"#rBuchbinder_et+al_2012_a\">Buchbinder et al, 2012</a>, 2015] for set functions to continuous domains.",
        "To get the optimal approximation factor and systematically passing the barrier of pure continuous submodularity, our algorithm requires a number of new ideas, including a reduction to the analysis of a zero-sum game for each coordinate, and the use of the special geometry of this game to bound the value of the game at its equilibrium.",
        "Our first main result is a for maximizing a continuous submodular function F , a.k.a. weak DR-SM, which is information-theoretically optimal [<a class=\"ref-link\" id=\"cFeige_et+al_2007_a\" href=\"#rFeige_et+al_2007_a\">Feige et al, 2007</a>, 2011].",
        "This result assumes that F is coordinate-wise Lipschitz continuous.8 Before describing our algorithm, we introduce the notion of the positive-orthant concave envelope of a two-dimensional curve, which is useful for understanding our algorithm.",
        "In the rest of the proof, we geometrically characterize the ADV\u2019s positive region against a mixed strategy of ALG over any 2-point support, and we show for the particular choice of P1, P2 and in Algorithm 1 the positive region covers the entire curve {g(z), h(z)}z2[0,1].",
        "If ALG plays the two point mixed strategy described in Algorithm 1, for every x\u21e4i 2 [0, 1] the point (g0, h0) = (g(x\u21e4i ), h(x\u21e4i )) is in the ADV\u2019s positive region.",
        "If F is coordinate-wise Lipschitz continuous with constant C > 0, Algorithm 1 can be implemented with O(n2/\u270f) calls to F and returning a pointz s.t. 2E [F (\u02c6z)] F (x\u21e4) 2C\u270f, where x\u21e4 2 argmax F(x) is the optimal solution.",
        "Our second result is a fast binary search algorithm, achieving the tight in quasi-linear time in n, but only for the special case of strong DR-SM functions",
        "If F (.) is non-negative and DR-submodular (a.k.a Strong DR-SM) and is coordinatewise Lipschitz continuous with constant C > 0, Algorithm 2 runs in time O",
        "We consider two application domains, namely Non-concave Quadratic Programming (NQP) [Bian et al, 2017b, <a class=\"ref-link\" id=\"cKim_2003_a\" href=\"#rKim_2003_a\">Kim and Kojima, 2003</a>, <a class=\"ref-link\" id=\"cLuo_et+al_2010_a\" href=\"#rLuo_et+al_2010_a\">Luo et al, 2010</a>], under both strong-DR and weak-DR, and maximization of softmax extension for MAP inference of determinantal point process[Kulesza et al, 2012, <a class=\"ref-link\" id=\"cGillenwater_et+al_2012_a\" href=\"#rGillenwater_et+al_2012_a\">Gillenwater et al, 2012</a>].",
        "One interesting avenue for future research is to generalize our techniques to the maximization over any arbitrary separable convex set, which will result in a broader application domain"
    ],
    "headline": "In this paper we study the fundamental problems of maximizing abcontinuous non-monotone submodular function over a hypercube, with and without coordinatewise concavity",
    "reference_links": [
        {
            "id": "Antoniadis_et+al_2011_a",
            "entry": "Anestis Antoniadis, Ir\u00e8ne Gijbels, and Mila Nikolova. Penalized likelihood regression for generalized linear models with non-quadratic penalties. Annals of the Institute of Statistical Mathematics, 63 (3):585\u2013615, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antoniadis%2C%20Anestis%20Gijbels%2C%20Ir%C3%A8ne%20Nikolova%2C%20Mila%20Penalized%20likelihood%20regression%20for%20generalized%20linear%20models%20with%20non-quadratic%20penalties%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antoniadis%2C%20Anestis%20Gijbels%2C%20Ir%C3%A8ne%20Nikolova%2C%20Mila%20Penalized%20likelihood%20regression%20for%20generalized%20linear%20models%20with%20non-quadratic%20penalties%202011"
        },
        {
            "id": "Bach_2013_a",
            "entry": "Francis Bach et al. Learning with submodular functions: A convex optimization perspective. Foundations and Trends R in Machine Learning, 6(2-3):145\u2013373, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013"
        },
        {
            "id": "Bian_et+al_0000_a",
            "entry": "An Bian, Kfir Levy, Andreas Krause, and Joachim M Buhmann. Continuous DR-submodular maximization: Structure and algorithms. In Advances in Neural Information Processing Systems, pages 486\u2013496, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bian%2C%20An%20Levy%2C%20Kfir%20Krause%2C%20Andreas%20Buhmann%2C%20Joachim%20M.%20Continuous%20DR-submodular%20maximization%3A%20Structure%20and%20algorithms",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bian%2C%20An%20Levy%2C%20Kfir%20Krause%2C%20Andreas%20Buhmann%2C%20Joachim%20M.%20Continuous%20DR-submodular%20maximization%3A%20Structure%20and%20algorithms"
        },
        {
            "id": "Bian_et+al_0000_b",
            "entry": "Andrew An Bian, Baharan Mirzasoleiman, Joachim Buhmann, and Andreas Krause. Guaranteed nonconvex optimization: Submodular maximization over continuous domains. In Artificial Intelligence and Statistics, pages 111\u2013120, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bian%2C%20Andrew%20An%20Mirzasoleiman%2C%20Baharan%20Buhmann%2C%20Joachim%20Krause%2C%20Andreas%20Guaranteed%20nonconvex%20optimization%3A%20Submodular%20maximization%20over%20continuous%20domains",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bian%2C%20Andrew%20An%20Mirzasoleiman%2C%20Baharan%20Buhmann%2C%20Joachim%20Krause%2C%20Andreas%20Guaranteed%20nonconvex%20optimization%3A%20Submodular%20maximization%20over%20continuous%20domains"
        },
        {
            "id": "Buchbinder_2016_a",
            "entry": "Niv Buchbinder and Moran Feldman. Deterministic algorithms for submodular maximization problems. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms, pages 392\u2013403. SIAM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buchbinder%2C%20Niv%20Feldman%2C%20Moran%20Deterministic%20algorithms%20for%20submodular%20maximization%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buchbinder%2C%20Niv%20Feldman%2C%20Moran%20Deterministic%20algorithms%20for%20submodular%20maximization%20problems%202016"
        },
        {
            "id": "Buchbinder_et+al_2012_a",
            "entry": "Niv Buchbinder, Moran Feldman, Joseph Seffi Naor, and Roy Schwartz. A tight linear time (1/2)approximation for unconstrained submodular maximization. In Proceedings of the 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science, pages 649\u2013658. IEEE Computer Society, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buchbinder%2C%20Niv%20Feldman%2C%20Moran%20Naor%2C%20Joseph%20Seffi%20Schwartz%2C%20Roy%20A%20tight%20linear%20time%20%281/2%29approximation%20for%20unconstrained%20submodular%20maximization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buchbinder%2C%20Niv%20Feldman%2C%20Moran%20Naor%2C%20Joseph%20Seffi%20Schwartz%2C%20Roy%20A%20tight%20linear%20time%20%281/2%29approximation%20for%20unconstrained%20submodular%20maximization%202012"
        },
        {
            "id": "Buchbinder_et+al_2015_a",
            "entry": "Niv Buchbinder, Moran Feldman, Joseph Seffi, and Roy Schwartz. A tight linear time (1/2)approximation for unconstrained submodular maximization. SIAM Journal on Computing, 44(5): 1384\u20131402, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buchbinder%2C%20Niv%20Feldman%2C%20Moran%20Seffi%2C%20Joseph%20Schwartz%2C%20Roy%20A%20tight%20linear%20time%20%281/2%29approximation%20for%20unconstrained%20submodular%20maximization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buchbinder%2C%20Niv%20Feldman%2C%20Moran%20Seffi%2C%20Joseph%20Schwartz%2C%20Roy%20A%20tight%20linear%20time%20%281/2%29approximation%20for%20unconstrained%20submodular%20maximization%202015"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Lin Chen, Hamed Hassani, and Amin Karbasi. Online continuous submodular maximization. In International Conference on Artificial Intelligence and Statistics, pages 1896\u20131905, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Lin%20Hassani%2C%20Hamed%20Karbasi%2C%20Amin%20Online%20continuous%20submodular%20maximization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Lin%20Hassani%2C%20Hamed%20Karbasi%2C%20Amin%20Online%20continuous%20submodular%20maximization%202018"
        },
        {
            "id": "Djolonga_2014_a",
            "entry": "Josip Djolonga and Andreas Krause. From map to marginals: Variational inference in bayesian submodular models. In Advances in Neural Information Processing Systems, pages 244\u2013252, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Djolonga%2C%20Josip%20Krause%2C%20Andreas%20From%20map%20to%20marginals%3A%20Variational%20inference%20in%20bayesian%20submodular%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Djolonga%2C%20Josip%20Krause%2C%20Andreas%20From%20map%20to%20marginals%3A%20Variational%20inference%20in%20bayesian%20submodular%20models%202014"
        },
        {
            "id": "Feige_et+al_2007_a",
            "entry": "Uriel Feige, Vahab S Mirrokni, and Jan Vondrak. Maximizing non-monotone submodular functions. In Foundations of Computer Science, 2007. FOCS\u201907. 48th Annual IEEE Symposium on, pages 461\u2013471. IEEE, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feige%2C%20Uriel%20Mirrokni%2C%20Vahab%20S.%20Vondrak%2C%20Jan%20Maximizing%20non-monotone%20submodular%20functions.%20In%20Foundations%20of%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feige%2C%20Uriel%20Mirrokni%2C%20Vahab%20S.%20Vondrak%2C%20Jan%20Maximizing%20non-monotone%20submodular%20functions.%20In%20Foundations%20of%202007"
        },
        {
            "id": "Feige_et+al_2011_a",
            "entry": "Uriel Feige, Vahab S Mirrokni, and Jan Vondrak. Maximizing non-monotone submodular functions. SIAM Journal on Computing, 40(4):1133\u20131153, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feige%2C%20Uriel%20Mirrokni%2C%20Vahab%20S.%20Vondrak%2C%20Jan%20Maximizing%20non-monotone%20submodular%20functions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feige%2C%20Uriel%20Mirrokni%2C%20Vahab%20S.%20Vondrak%2C%20Jan%20Maximizing%20non-monotone%20submodular%20functions%202011"
        },
        {
            "id": "Gillenwater_et+al_2012_a",
            "entry": "Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Near-optimal map inference for determinantal point processes. In Advances in Neural Information Processing Systems, pages 2735\u20132743, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gillenwater%2C%20Jennifer%20Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Near-optimal%20map%20inference%20for%20determinantal%20point%20processes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gillenwater%2C%20Jennifer%20Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Near-optimal%20map%20inference%20for%20determinantal%20point%20processes%202012"
        },
        {
            "id": "Gotovos_et+al_2015_a",
            "entry": "Alkis Gotovos, Amin Karbasi, and Andreas Krause. Non-monotone adaptive submodular maximization. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gotovos%2C%20Alkis%20Karbasi%2C%20Amin%20Krause%2C%20Andreas%20Non-monotone%20adaptive%20submodular%20maximization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gotovos%2C%20Alkis%20Karbasi%2C%20Amin%20Krause%2C%20Andreas%20Non-monotone%20adaptive%20submodular%20maximization%202015"
        },
        {
            "id": "Graham_1972_a",
            "entry": "Ronald L Graham. An efficient algorith for determining the convex hull of a finite planar set. Information processing letters, 1(4):132\u2013133, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graham%2C%20Ronald%20L.%20An%20efficient%20algorith%20for%20determining%20the%20convex%20hull%20of%20a%20finite%20planar%20set%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graham%2C%20Ronald%20L.%20An%20efficient%20algorith%20for%20determining%20the%20convex%20hull%20of%20a%20finite%20planar%20set%201972"
        },
        {
            "id": "Hartline_et+al_2008_a",
            "entry": "Jason Hartline, Vahab Mirrokni, and Mukund Sundararajan. Optimal marketing strategies over social networks. In Proceedings of the 17th international conference on World Wide Web, pages 189\u2013198. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartline%2C%20Jason%20Mirrokni%2C%20Vahab%20Sundararajan%2C%20Mukund%20Optimal%20marketing%20strategies%20over%20social%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hartline%2C%20Jason%20Mirrokni%2C%20Vahab%20Sundararajan%2C%20Mukund%20Optimal%20marketing%20strategies%20over%20social%20networks%202008"
        },
        {
            "id": "Hassani_et+al_2017_a",
            "entry": "Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi. Gradient methods for submodular maximization. In Advances in Neural Information Processing Systems, pages 5843\u20135853, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassani%2C%20Hamed%20Soltanolkotabi%2C%20Mahdi%20Karbasi%2C%20Amin%20Gradient%20methods%20for%20submodular%20maximization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassani%2C%20Hamed%20Soltanolkotabi%2C%20Mahdi%20Karbasi%2C%20Amin%20Gradient%20methods%20for%20submodular%20maximization%202017"
        },
        {
            "id": "Ito_2016_a",
            "entry": "Shinji Ito and Ryohei Fujimaki. Large-scale price optimization via network flow. In Advances in Neural Information Processing Systems, pages 3855\u20133863, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ito%2C%20Shinji%20Fujimaki%2C%20Ryohei%20Large-scale%20price%20optimization%20via%20network%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ito%2C%20Shinji%20Fujimaki%2C%20Ryohei%20Large-scale%20price%20optimization%20via%20network%20flow%202016"
        },
        {
            "id": "Iwata_2001_a",
            "entry": "Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. A combinatorial strongly polynomial algorithm for minimizing submodular functions. Journal of the ACM (JACM), 48(4):761\u2013777, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iwata%2C%20Satoru%20Fleischer%2C%20Lisa%20and%20Satoru%20Fujishige.%20A%20combinatorial%20strongly%20polynomial%20algorithm%20for%20minimizing%20submodular%20functions%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iwata%2C%20Satoru%20Fleischer%2C%20Lisa%20and%20Satoru%20Fujishige.%20A%20combinatorial%20strongly%20polynomial%20algorithm%20for%20minimizing%20submodular%20functions%202001"
        },
        {
            "id": "Kapralov_2013_a",
            "entry": "Michael Kapralov, Ian Post, and Jan Vondr\u00e1k. Online submodular welfare maximization: Greedy is optimal. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1216\u20131225. SIAM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Vondr%C3%A1k.%20Online%20submodular%20welfare%20maximization%3A%20Greedy%20is%20optimal%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Vondr%C3%A1k.%20Online%20submodular%20welfare%20maximization%3A%20Greedy%20is%20optimal%202013"
        },
        {
            "id": "Kim_2003_a",
            "entry": "Sunyoung Kim and Masakazu Kojima. Exact solutions of some nonconvex quadratic optimization problems via sdp and socp relaxations. Computational Optimization and Applications, 26(2): 143\u2013154, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Sunyoung%20Kojima%2C%20Masakazu%20Exact%20solutions%20of%20some%20nonconvex%20quadratic%20optimization%20problems%20via%20sdp%20and%20socp%20relaxations%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Sunyoung%20Kojima%2C%20Masakazu%20Exact%20solutions%20of%20some%20nonconvex%20quadratic%20optimization%20problems%20via%20sdp%20and%20socp%20relaxations%202003"
        },
        {
            "id": "Krause_2014_a",
            "entry": "Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability: Practical Approaches to Hard Problems, pages 71\u2013104. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20Andreas%20Golovin%2C%20Daniel%20Submodular%20function%20maximization.%20In%20Tractability%3A%20Practical%20Approaches%20to%20Hard%20Problems%202014"
        },
        {
            "id": "Kulesza_2012_a",
            "entry": "Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Foundations and Trends R in Machine Learning, 5(2\u20133):123\u2013286, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Determinantal%20point%20processes%20for%20machine%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Determinantal%20point%20processes%20for%20machine%20learning%202012"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Chengtao Li, Suvrit Sra, and Stefanie Jegelka. Fast mixing markov chains for strongly rayleigh measures, dpps, and constrained sampling. In Advances in Neural Information Processing Systems, pages 4188\u20134196, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chengtao%20Sra%2C%20Suvrit%20Jegelka%2C%20Stefanie%20Fast%20mixing%20markov%20chains%20for%20strongly%20rayleigh%20measures%2C%20dpps%2C%20and%20constrained%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chengtao%20Sra%2C%20Suvrit%20Jegelka%2C%20Stefanie%20Fast%20mixing%20markov%20chains%20for%20strongly%20rayleigh%20measures%2C%20dpps%2C%20and%20constrained%20sampling%202016"
        },
        {
            "id": "Luo_et+al_2010_a",
            "entry": "Zhi-Quan Luo, Wing-Kin Ma, Anthony Man-Cho So, Yinyu Ye, and Shuzhong Zhang. Semidefinite relaxation of quadratic optimization problems. IEEE Signal Processing Magazine, 27(3):20\u201334, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Zhi-Quan%20Ma%2C%20Wing-Kin%20So%2C%20Anthony%20Man-Cho%20Ye%2C%20Yinyu%20Semidefinite%20relaxation%20of%20quadratic%20optimization%20problems%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Zhi-Quan%20Ma%2C%20Wing-Kin%20So%2C%20Anthony%20Man-Cho%20Ye%2C%20Yinyu%20Semidefinite%20relaxation%20of%20quadratic%20optimization%20problems%202010"
        },
        {
            "id": "Mirzasoleiman_et+al_2013_a",
            "entry": "Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed submodular maximization: Identifying representative elements in massive data. In Advances in Neural Information Processing Systems, pages 2049\u20132057, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirzasoleiman%2C%20Baharan%20Karbasi%2C%20Amin%20Sarkar%2C%20Rik%20Krause%2C%20Andreas%20Distributed%20submodular%20maximization%3A%20Identifying%20representative%20elements%20in%20massive%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirzasoleiman%2C%20Baharan%20Karbasi%2C%20Amin%20Sarkar%2C%20Rik%20Krause%2C%20Andreas%20Distributed%20submodular%20maximization%3A%20Identifying%20representative%20elements%20in%20massive%20data%202013"
        },
        {
            "id": "Mokhtari_et+al_2018_a",
            "entry": "Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Conditional gradient method for stochastic submodular maximization: Closing the gap. In International Conference on Artificial Intelligence and Statistics, pages 1886\u20131895, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mokhtari%2C%20Aryan%20Hassani%2C%20Hamed%20Karbasi%2C%20Amin%20Conditional%20gradient%20method%20for%20stochastic%20submodular%20maximization%3A%20Closing%20the%20gap%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mokhtari%2C%20Aryan%20Hassani%2C%20Hamed%20Karbasi%2C%20Amin%20Conditional%20gradient%20method%20for%20stochastic%20submodular%20maximization%3A%20Closing%20the%20gap%202018"
        },
        {
            "id": "Roughgarden_2018_a",
            "entry": "Tim Roughgarden and Joshua R Wang. An optimal learning algorithm for online unconstrained submodular maximization. In To Appear in Proceedings of the 31st Conference on Learning Theory (COLT), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roughgarden%2C%20Tim%20Wang%2C%20Joshua%20R.%20An%20optimal%20learning%20algorithm%20for%20online%20unconstrained%20submodular%20maximization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roughgarden%2C%20Tim%20Wang%2C%20Joshua%20R.%20An%20optimal%20learning%20algorithm%20for%20online%20unconstrained%20submodular%20maximization%202018"
        },
        {
            "id": "Schrijver_2000_a",
            "entry": "Alexander Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polynomial time. Journal of Combinatorial Theory, Series B, 80(2):346\u2013355, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schrijver%2C%20Alexander%20A%20combinatorial%20algorithm%20minimizing%20submodular%20functions%20in%20strongly%20polynomial%20time%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schrijver%2C%20Alexander%20A%20combinatorial%20algorithm%20minimizing%20submodular%20functions%20in%20strongly%20polynomial%20time%202000"
        },
        {
            "id": "Soma_2015_a",
            "entry": "Tasuku Soma and Yuichi Yoshida. A generalization of submodular cover via the diminishing return property on the integer lattice. In Advances in Neural Information Processing Systems, pages 847\u2013855, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soma%2C%20Tasuku%20Yoshida%2C%20Yuichi%20A%20generalization%20of%20submodular%20cover%20via%20the%20diminishing%20return%20property%20on%20the%20integer%20lattice%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soma%2C%20Tasuku%20Yoshida%2C%20Yuichi%20A%20generalization%20of%20submodular%20cover%20via%20the%20diminishing%20return%20property%20on%20the%20integer%20lattice%202015"
        },
        {
            "id": "Soma_2017_a",
            "entry": "Tasuku Soma and Yuichi Yoshida. Non-monotone dr-submodular function maximization. In AAAI, volume 17, pages 898\u2013904, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soma%2C%20Tasuku%20Yoshida%2C%20Yuichi%20Non-monotone%20dr-submodular%20function%20maximization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soma%2C%20Tasuku%20Yoshida%2C%20Yuichi%20Non-monotone%20dr-submodular%20function%20maximization%202017"
        },
        {
            "id": "Staib_2017_a",
            "entry": "Matthew Staib and Stefanie Jegelka. Robust budget allocation via continuous submodular functions. arXiv preprint arXiv:1702.08791, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08791"
        },
        {
            "id": "Zhang_et+al_2015_a",
            "entry": "Jian Zhang, Josip Djolonga, and Andreas Krause. Higher-order inference for multi-class logsupermodular models. In Proceedings of the IEEE International Conference on Computer Vision, pages 1859\u20131867, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jian%20Djolonga%2C%20Josip%20Krause%2C%20Andreas%20Higher-order%20inference%20for%20multi-class%20logsupermodular%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jian%20Djolonga%2C%20Josip%20Krause%2C%20Andreas%20Higher-order%20inference%20for%20multi-class%20logsupermodular%20models%202015"
        }
    ]
}
