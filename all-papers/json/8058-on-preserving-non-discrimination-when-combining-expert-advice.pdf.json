{
    "filename": "8058-on-preserving-non-discrimination-when-combining-expert-advice.pdf",
    "metadata": {
        "title": "On preserving non-discrimination when combining expert advice",
        "author": "Avrim Blum, Suriya Gunasekar, Thodoris Lykouris, Nati Srebro",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8058-on-preserving-non-discrimination-when-combining-expert-advice.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions. We consider the most basic extension of classical online learning: Given a class of predictors that are individually non-discriminatory with respect to a particular metric, how can we combine them to perform as well as the best predictor, while preserving non-discrimination? Surprisingly we show that this task is unachievable for the prevalent notion of equalized odds that requires equal false negative rates and equal false positive rates across groups. On the positive side, for another notion of non-discrimination, equalized error rates, we show that running separate instances of the classical multiplicative weights algorithm for each group achieves this guarantee. Interestingly, even for this notion, we show that algorithms with stronger performance guarantees than multiplicative weights cannot preserve non-discrimination."
    },
    "keywords": [
        {
            "term": "error rate",
            "url": "https://en.wikipedia.org/wiki/error_rate"
        },
        {
            "term": "recommender system",
            "url": "https://en.wikipedia.org/wiki/recommender_system"
        },
        {
            "term": "disparate impact",
            "url": "https://en.wikipedia.org/wiki/disparate_impact"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "The emergence of machine learning in the last decade has given rise to an important debate regarding the ethical and societal responsibility of its offspring",
        "We show that, if all predictors are individually non-discriminatory with respect to equalized error rates, running separate multiplicative weights algorithms, one for each subpopulation, preserves this non-discrimination without decay in the efficiency (Theorem 3)",
        "We show that any algorithm that ignores the group identity can be unboundedly unfair with respect to equalized error rates (Theorem 4)",
        "We introduce the study of avoiding discrimination towards protected groups in online settings with non-i.i.d. examples",
        "We introduce the notion of equalized error rates that can be a useful metric for non-discrimination in settings where all examples that contribute towards the performance contribute towards fairness",
        "A racist program organizer can select to admit all students of the one group and decline the students of the other, while satisfying equalized error rates \u2013 this does not satisfy equalized odds"
    ],
    "key_statements": [
        "The emergence of machine learning in the last decade has given rise to an important debate regarding the ethical and societal responsibility of its offspring",
        "We show that, if all predictors are individually non-discriminatory with respect to equalized error rates, running separate multiplicative weights algorithms, one for each subpopulation, preserves this non-discrimination without decay in the efficiency (Theorem 3)",
        "We show that any algorithm that ignores the group identity can be unboundedly unfair with respect to equalized error rates (Theorem 4)",
        "We introduce the study of avoiding discrimination towards protected groups in online settings with non-i.i.d. examples",
        "We introduce the notion of equalized error rates that can be a useful metric for non-discrimination in settings where all examples that contribute towards the performance contribute towards fairness",
        "A racist program organizer can select to admit all students of the one group and decline the students of the other, while satisfying equalized error rates \u2013 this does not satisfy equalized odds"
    ],
    "summary": [
        "The emergence of machine learning in the last decade has given rise to an important debate regarding the ethical and societal responsibility of its offspring.",
        "Our goal is to develop online learning algorithms that combine fair in isolation experts in order to achieve both vanishing average expected -approximate regret, i.e. for any fixed > 0 and f \u2208 F, E\u03c3[ApxReg ,T (f )] = o(T ), and non-discrimination with respect to fairness metrics of interest.",
        "Our first construction (Theorem 1) shows that any no-regret learning algorithm that is group-unaware cannot guarantee fairness in composition, even in instances that are perfectly balanced \u2013 the only adversarial component is the order in which these examples arrive.",
        "2. In Phase II, there are two plausible worlds: (a) the adversary assigns negative labels to both groups if the expected number of times that the algorithm selected the negative expert with probability higher than \u03b3( ) on members of group A is less than c\u00b7b\u00b7T , i.e. E\u03c3 1 t \u2264 \u0398 \u00b7 T : g(t) = A, pht n \u2265 \u03b3( ) < c\u00b7b\u00b7T .",
        "3. In Phase II, any -approximate regret algorithm assigns large enough probability to expert hp for group B, implying an upper bound on the false negative rate on B, i.e. F N R(B) \u2264 1/2(1\u2212b).",
        "We again assume that experts are fair in isolation with respect to equalized error rate and show that a simple scheme where we run separately one instance of multiplicative weights for each group achieves fairness in composition (Theorem 3).",
        "This suggests that the algorithm needs to actively discriminate based on the groups to achieve fairness with respect to equalized error rates.",
        "For any \u03b1 > 0 and any < \u03b1 such that running separate instances of multiplicative weights for each group with learning rate \u03b7 = min( , \u03b1/6) guarantees \u03b1-fairness in composition and -approximate regret of at most O(|G| log(d)/ ).",
        "This impossibility result holds for any algorithm with vanishing -approximate regret where the learning dynamic is a deterministic function of the difference between the cumumative losses of the experts.",
        "For any \u03b1 < 1/2 and > 0, any algorithm that can achieve the vanishing approximate regret property against shifting comparators f of length K(f ) = 2, running separate instances of the algorithm for each group is \u03b1-unfair in composition with respect to equalized error rate.",
        "We introduce the notion of equalized error rates that can be a useful metric for non-discrimination in settings where all examples that contribute towards the performance contribute towards fairness."
    ],
    "headline": "We study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias: There\u2019s software used across the country to predict future criminals. And it\u2019s biased against blacks. ProPublica, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angwin%2C%20Julia%20Larson%2C%20Jeff%20Mattu%2C%20Surya%20Kirchner%2C%20Lauren%20Machine%20bias%3A%20There%E2%80%99s%20software%20used%20across%20the%20country%20to%20predict%20future%20criminals.%20And%20it%E2%80%99s%20biased%20against%20blacks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angwin%2C%20Julia%20Larson%2C%20Jeff%20Mattu%2C%20Surya%20Kirchner%2C%20Lauren%20Machine%20bias%3A%20There%E2%80%99s%20software%20used%20across%20the%20country%20to%20predict%20future%20criminals.%20And%20it%E2%80%99s%20biased%20against%20blacks%202016"
        },
        {
            "id": "2",
            "entry": "[2] Julia Angwin and Terry Parris Jr. Facebook lets advertisers exclude users by race. ProPublica blog, 28, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angwin%2C%20Julia%20Parris%2C%20Jr%2C%20Terry%20Facebook%20lets%20advertisers%20exclude%20users%20by%20race%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angwin%2C%20Julia%20Parris%2C%20Jr%2C%20Terry%20Facebook%20lets%20advertisers%20exclude%20users%20by%20race%202016"
        },
        {
            "id": "3",
            "entry": "[3] Maria-Florina Balcan, Travis Dick, Ritesh Noothigattu, and Ariel Procaccia. Envy-free classification, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MariaFlorina%20Balcan%20Travis%20Dick%20Ritesh%20Noothigattu%20and%20Ariel%20Procaccia%20Envyfree%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MariaFlorina%20Balcan%20Travis%20Dick%20Ritesh%20Noothigattu%20and%20Ariel%20Procaccia%20Envyfree%20classification%202018"
        },
        {
            "id": "4",
            "entry": "[4] Solon Barocas and Andrew D. Selbst. Big Data\u2019s Disparate Impact. California Law Review, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barocas%2C%20Solon%20Selbst%2C%20Andrew%20D.%20Big%20Data%E2%80%99s%20Disparate%20Impact%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barocas%2C%20Solon%20Selbst%2C%20Andrew%20D.%20Big%20Data%E2%80%99s%20Disparate%20Impact%202016"
        },
        {
            "id": "5",
            "entry": "[5] Sarah Bird, Solon Barocas, Kate Crawford, Fernando Diaz, and Hanna Wallach. Exploring or Exploiting? Social and Ethical Implications of Autonomous Experimentation in AI. In Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT-ML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bird%2C%20Sarah%20Barocas%2C%20Solon%20Crawford%2C%20Kate%20Diaz%2C%20Fernando%20Exploring%20or%20Exploiting%3F%20Social%20and%20Ethical%20Implications%20of%20Autonomous%20Experimentation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bird%2C%20Sarah%20Barocas%2C%20Solon%20Crawford%2C%20Kate%20Diaz%2C%20Fernando%20Exploring%20or%20Exploiting%3F%20Social%20and%20Ethical%20Implications%20of%20Autonomous%20Experimentation%202016"
        },
        {
            "id": "6",
            "entry": "[6] Avrim Blum and Yishay Mansour. From external to internal regret. In Proceedings of the 18th Annual Conference on Learning Theory (COLT), 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Avrim%20Blum%20and%20Yishay%20Mansour.%20From%20external%20to%20internal%20regret%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Avrim%20Blum%20and%20Yishay%20Mansour.%20From%20external%20to%20internal%20regret%202005"
        },
        {
            "id": "7",
            "entry": "[7] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buolamwini%2C%20Joy%20Gebru%2C%20Timnit%20Gender%20shades%3A%20Intersectional%20accuracy%20disparities%20in%20commercial%20gender%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buolamwini%2C%20Joy%20Gebru%2C%20Timnit%20Gender%20shades%3A%20Intersectional%20accuracy%20disparities%20in%20commercial%20gender%20classification%202018"
        },
        {
            "id": "8",
            "entry": "[8] Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency constraints. In IEEE International Conference on Data Mining (ICDM), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calders%2C%20Toon%20Kamiran%2C%20Faisal%20Pechenizkiy%2C%20Mykola%20Building%20classifiers%20with%20independency%20constraints%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calders%2C%20Toon%20Kamiran%2C%20Faisal%20Pechenizkiy%2C%20Mykola%20Building%20classifiers%20with%20independency%20constraints%202009"
        },
        {
            "id": "9",
            "entry": "[9] L. Elisa Celis and Nisheeth K. Vishnoi. Fair personalization. In Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT-ML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Celis%2C%20L.Elisa%20Vishnoi%2C%20Nisheeth%20K.%20Fair%20personalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Celis%2C%20L.Elisa%20Vishnoi%2C%20Nisheeth%20K.%20Fair%20personalization%202017"
        },
        {
            "id": "10",
            "entry": "[10] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2):153\u2013163, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017"
        },
        {
            "id": "11",
            "entry": "[11] Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.00023"
        },
        {
            "id": "12",
            "entry": "[12] Amit Datta, Michael Carl Tschantz, and Anupam Datta. Automated experiments on ad privacy settings. Proceedings on Privacy Enhancing Technologies, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Datta%2C%20Amit%20Tschantz%2C%20Michael%20Carl%20Datta%2C%20Anupam%20Automated%20experiments%20on%20ad%20privacy%20settings.%20Proceedings%20on%20Privacy%20Enhancing%20Technologies%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Datta%2C%20Amit%20Tschantz%2C%20Michael%20Carl%20Datta%2C%20Anupam%20Automated%20experiments%20on%20ad%20privacy%20settings.%20Proceedings%20on%20Privacy%20Enhancing%20Technologies%202015"
        },
        {
            "id": "13",
            "entry": "[13] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "14",
            "entry": "[14] Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman. Regret to the best vs. regret to the average. Journal of Machine Learning (JMLR), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20Eyal%20Kearns%2C%20Michael%20Mansour%2C%20Yishay%20Wortman%2C%20Jennifer%20Regret%20to%20the%20best%20vs.%20regret%20to%20the%20average%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20Eyal%20Kearns%2C%20Michael%20Mansour%2C%20Yishay%20Wortman%2C%20Jennifer%20Regret%20to%20the%20best%20vs.%20regret%20to%20the%20average%202008"
        },
        {
            "id": "15",
            "entry": "[15] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20A.%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20A.%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015"
        },
        {
            "id": "16",
            "entry": "[16] Avi Feller, Emma Pierson, Sam Corbett-Davies, and Sharad Goel. A computer program used for bail and sentencing decisions was labeled biased against blacks. it\u2019s actually not that clear. The Washington Post, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feller%2C%20Avi%20Pierson%2C%20Emma%20Corbett-Davies%2C%20Sam%20Goel%2C%20Sharad%20A%20computer%20program%20used%20for%20bail%20and%20sentencing%20decisions%20was%20labeled%20biased%20against%20blacks.%20it%E2%80%99s%20actually%20not%20that%20clear%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feller%2C%20Avi%20Pierson%2C%20Emma%20Corbett-Davies%2C%20Sam%20Goel%2C%20Sharad%20A%20computer%20program%20used%20for%20bail%20and%20sentencing%20decisions%20was%20labeled%20biased%20against%20blacks.%20it%E2%80%99s%20actually%20not%20that%20clear%202016"
        },
        {
            "id": "17",
            "entry": "[17] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997"
        },
        {
            "id": "18",
            "entry": "[18] Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online learning with an unknown fairness metric. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gillen%2C%20Stephen%20Jung%2C%20Christopher%20Kearns%2C%20Michael%20Roth%2C%20Aaron%20Online%20learning%20with%20an%20unknown%20fairness%20metric%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gillen%2C%20Stephen%20Jung%2C%20Christopher%20Kearns%2C%20Michael%20Roth%2C%20Aaron%20Online%20learning%20with%20an%20unknown%20fairness%20metric%202018"
        },
        {
            "id": "19",
            "entry": "[19] Gabriel Goh, Andrew Cotter, Maya Gupta, and Michael P Friedlander. Satisfying real-world goals with dataset constraints. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goh%2C%20Gabriel%20Cotter%2C%20Andrew%20Gupta%2C%20Maya%20Friedlander%2C%20Michael%20P.%20Satisfying%20real-world%20goals%20with%20dataset%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goh%2C%20Gabriel%20Cotter%2C%20Andrew%20Gupta%2C%20Maya%20Friedlander%2C%20Michael%20P.%20Satisfying%20real-world%20goals%20with%20dataset%20constraints%202016"
        },
        {
            "id": "20",
            "entry": "[20] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances in neural information processing systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016"
        },
        {
            "id": "21",
            "entry": "[21] Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. Journal of Machine Learning Research, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herbster%2C%20Mark%20Warmuth%2C%20Manfred%20K.%20Tracking%20the%20best%20linear%20predictor%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herbster%2C%20Mark%20Warmuth%2C%20Manfred%20K.%20Tracking%20the%20best%20linear%20predictor%202001"
        },
        {
            "id": "22",
            "entry": "[22] Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016"
        },
        {
            "id": "23",
            "entry": "[23] Sampath Kannan, Michael Kearns, Jamie Morgenstern, Mallesh Pai, Aaron Roth, Rakesh Vohra, and Zhiwei Steven Wu. Fairness incentives for myopic agents. In Proceedings of the 2017 ACM Conference on Economics and Computation (EC), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kannan%2C%20Sampath%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20Pai%2C%20Mallesh%20Fairness%20incentives%20for%20myopic%20agents%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kannan%2C%20Sampath%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20Pai%2C%20Mallesh%20Fairness%20incentives%20for%20myopic%20agents%202017"
        },
        {
            "id": "24",
            "entry": "[24] Sampath Kannan, Jamie Morgenstern, Aaron Roth, Bo Waggoner, and Zhiwei Steven Wu. A smoothed analysis of the greedy algorithm for the linear contextual bandit problem. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kannan%2C%20Sampath%20Morgenstern%2C%20Jamie%20Roth%2C%20Aaron%20Waggoner%2C%20Bo%20A%20smoothed%20analysis%20of%20the%20greedy%20algorithm%20for%20the%20linear%20contextual%20bandit%20problem%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kannan%2C%20Sampath%20Morgenstern%2C%20Jamie%20Roth%2C%20Aaron%20Waggoner%2C%20Bo%20A%20smoothed%20analysis%20of%20the%20greedy%20algorithm%20for%20the%20linear%20contextual%20bandit%20problem%202018"
        },
        {
            "id": "25",
            "entry": "[25] Matthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereotypes in image search results for occupations. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kay%2C%20Matthew%20Matuszek%2C%20Cynthia%20Munson%2C%20Sean%20A.%20Unequal%20representation%20and%20gender%20stereotypes%20in%20image%20search%20results%20for%20occupations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kay%2C%20Matthew%20Matuszek%2C%20Cynthia%20Munson%2C%20Sean%20A.%20Unequal%20representation%20and%20gender%20stereotypes%20in%20image%20search%20results%20for%20occupations%202015"
        },
        {
            "id": "26",
            "entry": "[26] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20Michael%20Neel%2C%20Seth%20Roth%2C%20Aaron%20Wu%2C%20Zhiwei%20Steven%20Preventing%20fairness%20gerrymandering%3A%20Auditing%20and%20learning%20for%20subgroup%20fairness%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20Michael%20Neel%2C%20Seth%20Roth%2C%20Aaron%20Wu%2C%20Zhiwei%20Steven%20Preventing%20fairness%20gerrymandering%3A%20Auditing%20and%20learning%20for%20subgroup%20fairness%202018"
        },
        {
            "id": "27",
            "entry": "[27] Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kilbertus%2C%20Niki%20Carulla%2C%20Mateo%20Rojas%20Parascandolo%2C%20Giambattista%20Hardt%2C%20Moritz%20Avoiding%20discrimination%20through%20causal%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kilbertus%2C%20Niki%20Carulla%2C%20Mateo%20Rojas%20Parascandolo%2C%20Giambattista%20Hardt%2C%20Moritz%20Avoiding%20discrimination%20through%20causal%20reasoning%202017"
        },
        {
            "id": "28",
            "entry": "[28] Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In Innovations of Theoretical Computer Science (ITCS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20Jon%20M.%20Mullainathan%2C%20Sendhil%20Raghavan%2C%20Manish%20Inherent%20trade-offs%20in%20the%20fair%20determination%20of%20risk%20scores%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20Jon%20M.%20Mullainathan%2C%20Sendhil%20Raghavan%2C%20Manish%20Inherent%20trade-offs%20in%20the%20fair%20determination%20of%20risk%20scores%202017"
        },
        {
            "id": "29",
            "entry": "[29] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matt%20J%20Kusner%20Joshua%20Loftus%20Chris%20Russell%20and%20Ricardo%20Silva%20Counterfactual%20fairness%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matt%20J%20Kusner%20Joshua%20Loftus%20Chris%20Russell%20and%20Ricardo%20Silva%20Counterfactual%20fairness%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%202017"
        },
        {
            "id": "30",
            "entry": "[30] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108(2):212\u2013261, February 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littlestone%2C%20Nick%20Warmuth%2C%20Manfred%20K.%20The%20weighted%20majority%20algorithm%201994-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littlestone%2C%20Nick%20Warmuth%2C%20Manfred%20K.%20The%20weighted%20majority%20algorithm%201994-02"
        },
        {
            "id": "31",
            "entry": "[31] Katherine A Liu and Natalie A Dipietro Mager. Women\u2019s involvement in clinical trials: historical perspective and future implications. Pharmacy Practice, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Katherine%20A%20Liu%20and%20Natalie%20A%20Dipietro%20Mager.%20Women%E2%80%99s%20involvement%20in%20clinical%20trials%3A%20historical%20perspective%20and%20future%20implications%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Katherine%20A%20Liu%20and%20Natalie%20A%20Dipietro%20Mager.%20Women%E2%80%99s%20involvement%20in%20clinical%20trials%3A%20historical%20perspective%20and%20future%20implications%202016"
        },
        {
            "id": "32",
            "entry": "[32] Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. 35th International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Lydia%20T.%20Dean%2C%20Sarah%20Rolf%2C%20Esther%20Simchowitz%2C%20Max%20Delayed%20impact%20of%20fair%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Lydia%20T.%20Dean%2C%20Sarah%20Rolf%2C%20Esther%20Simchowitz%2C%20Max%20Delayed%20impact%20of%20fair%20machine%20learning%202018"
        },
        {
            "id": "33",
            "entry": "[33] Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C. Parkes. Calibrated fairness in bandits. Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT-ML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%20Liu%20Goran%20Radanovic%20Christos%20Dimitrakakis%20Debmalya%20Mandal%20and%20David%20C%20Parkes%20Calibrated%20fairness%20in%20bandits%20Workshop%20on%20Fairness%20Accountability%20and%20Transparency%20in%20Machine%20Learning%20FATML%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%20Liu%20Goran%20Radanovic%20Christos%20Dimitrakakis%20Debmalya%20Mandal%20and%20David%20C%20Parkes%20Calibrated%20fairness%20in%20bandits%20Workshop%20on%20Fairness%20Accountability%20and%20Transparency%20in%20Machine%20Learning%20FATML%202017"
        },
        {
            "id": "34",
            "entry": "[34] Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adanormalhedge. In Proceedings of The 28th Conference on Learning Theory (COLT), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Haipeng%20Schapire%2C%20Robert%20E.%20Achieving%20all%20with%20no%20parameters%3A%20Adanormalhedge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Haipeng%20Schapire%2C%20Robert%20E.%20Achieving%20all%20with%20no%20parameters%3A%20Adanormalhedge%202015"
        },
        {
            "id": "35",
            "entry": "[35] Thodoris Lykouris, Vasilis Syrgkanis, and \u00c9va Tardos. Learning and efficiency in games with dynamic population. In Proceedings of the Twenty-seventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lykouris%2C%20Thodoris%20Syrgkanis%2C%20Vasilis%20Tardos%2C%20%C3%89va%20Learning%20and%20efficiency%20in%20games%20with%20dynamic%20population%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lykouris%2C%20Thodoris%20Syrgkanis%2C%20Vasilis%20Tardos%2C%20%C3%89va%20Learning%20and%20efficiency%20in%20games%20with%20dynamic%20population%202016"
        },
        {
            "id": "36",
            "entry": "[36] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedreshi%2C%20Dino%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20Discrimination-aware%20data%20mining%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedreshi%2C%20Dino%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20Discrimination-aware%20data%20mining%202008"
        },
        {
            "id": "37",
            "entry": "[37] Manish Raghavan, Aleksandrs Slivkins, Jennifer Vaughan Wortman, and Zhiwei Steven Wu. The externalities of exploration and how data diversity helps exploitation. In Proceedings of the 31st Conference On Learning Theory (COLT), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghavan%2C%20Manish%20Slivkins%2C%20Aleksandrs%20Wortman%2C%20Jennifer%20Vaughan%20Wu%2C%20Zhiwei%20Steven%20The%20externalities%20of%20exploration%20and%20how%20data%20diversity%20helps%20exploitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghavan%2C%20Manish%20Slivkins%2C%20Aleksandrs%20Wortman%2C%20Jennifer%20Vaughan%20Wu%2C%20Zhiwei%20Steven%20The%20externalities%20of%20exploration%20and%20how%20data%20diversity%20helps%20exploitation%202018"
        },
        {
            "id": "38",
            "entry": "[38] Latanya Sweeney. Discrimination in online ad delivery. Commun. ACM, 56(5):44\u201354, May 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sweeney%2C%20Latanya%20Discrimination%20in%20online%20ad%20delivery%202013-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sweeney%2C%20Latanya%20Discrimination%20in%20online%20ad%20delivery%202013-05"
        },
        {
            "id": "39",
            "entry": "[39] Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-discriminatory predictors. In Conference on Learning Theory (COLT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodworth%2C%20Blake%20Gunasekar%2C%20Suriya%20Ohannessian%2C%20Mesrob%20I.%20Srebro%2C%20Nathan%20Learning%20non-discriminatory%20predictors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodworth%2C%20Blake%20Gunasekar%2C%20Suriya%20Ohannessian%2C%20Mesrob%20I.%20Srebro%2C%20Nathan%20Learning%20non-discriminatory%20predictors%202017"
        },
        {
            "id": "40",
            "entry": "[40] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Learning fair classifiers. Proceedings of 30th Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Learning%20fair%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Learning%20fair%20classifiers%202017"
        },
        {
            "id": "41",
            "entry": "[41] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning (ICML), 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zemel%2C%20Rich%20Wu%2C%20Yu%20Swersky%2C%20Kevin%20Pitassi%2C%20Toni%20Learning%20fair%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20Rich%20Wu%2C%20Yu%20Swersky%2C%20Kevin%20Pitassi%2C%20Toni%20Learning%20fair%20representations%202013"
        }
    ]
}
