{
    "filename": "7434-accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization.pdf",
    "metadata": {
        "title": "Accelerated Stochastic Matrix Inversion:  General Theory and  Speeding up BFGS Rules for Faster Second-Order Optimization",
        "author": "Robert Gower, Filip Hanzely, Peter Richtarik, Sebastian U. Stich",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7434-accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization.pdf"
        },
        "abstract": "We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning. As an application of our general theory, we develop the first accelerated (deterministic and stochastic) quasi-Newton updates. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models."
    },
    "keywords": [
        {
            "term": "speed up",
            "url": "https://en.wikipedia.org/wiki/speed_up"
        },
        {
            "term": "newton method",
            "url": "https://en.wikipedia.org/wiki/newton_method"
        },
        {
            "term": "euclidean space",
            "url": "https://en.wikipedia.org/wiki/euclidean_space"
        },
        {
            "term": "general theory",
            "url": "https://en.wikipedia.org/wiki/general_theory"
        },
        {
            "term": "positive definite matrix",
            "url": "https://en.wikipedia.org/wiki/positive_definite_matrix"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "BFGS",
            "url": "https://en.wikipedia.org/wiki/BFGS"
        },
        {
            "term": "second order",
            "url": "https://en.wikipedia.org/wiki/second_order"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "linear system",
            "url": "https://en.wikipedia.org/wiki/linear_system"
        },
        {
            "term": "matrix inversion",
            "url": "https://en.wikipedia.org/wiki/matrix_inversion"
        }
    ],
    "highlights": [
        "Consider the optimization problem min f (w), (1)<br/><br/>w2Rn and assume f is sufficiently smooth",
        "A new wave of second order stochastic methods are being developed with the aim of solving large scale optimization problems",
        "We develop a new stochastic accelerated BFGS update that can form the basis of new stochastic quasi-Newton methods",
        "We developed an accelerated sketch-and-project method for solving linear systems in Euclidean spaces",
        "We show that under a careful choice of the parameters of the method\u2014depending on the problem structure and conditioning\u2014acceleration might result into significant speedups both for the matrix inversion problem and for the stochastic BFGS algorithm",
        "We confirm experimentally that our accelerated methods can lead to speed-ups when compared to the classical BFGS algorithm"
    ],
    "key_statements": [
        "Consider the optimization problem min f (w), (1)<br/><br/>w2Rn and assume f is sufficiently smooth",
        "A new wave of second order stochastic methods are being developed with the aim of solving large scale optimization problems",
        "We develop a new stochastic accelerated BFGS update that can form the basis of new stochastic quasi-Newton methods",
        "The above is a sketch-and-project update for a linear system in Rn2 , which allows to obtain an alternative proof of Theorem 5, without using our results from Euclidean spaces",
        "As a tweak in the stochastic BFGS allows for a faster estimation of Hessian inverse and more accurate steps of the method, one might wonder if a equivalent tweak might speed up the standard, deterministic BFGS algorithm for solving (1)",
        "We developed an accelerated sketch-and-project method for solving linear systems in Euclidean spaces",
        "We show that under a careful choice of the parameters of the method\u2014depending on the problem structure and conditioning\u2014acceleration might result into significant speedups both for the matrix inversion problem and for the stochastic BFGS algorithm",
        "We confirm experimentally that our accelerated methods can lead to speed-ups when compared to the classical BFGS algorithm"
    ],
    "summary": [
        "Consider the optimization problem min f (w), (1)<br/><br/>w2Rn and assume f is sufficiently smooth.",
        "In its most general form, it can be seen as an accelerated version of a sketch-and-project method in Euclidean spaces which we present .",
        "We generalize the analysis of an accelerated version of the sketch-and-project algorithm [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] to linear operator systems in Euclidean spaces.",
        "This algorithm can be seen as a special case of the accelerated sketch-and-project in Euclidean space, its convergence follows from the main theorem.",
        "Whilst for the non-accelerated sketch-and-project algorithm for matrix inversion [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] the knowledge of these parameters is not necessary, they need to be given as input to the accelerated scheme.",
        "The accelerated sketch-and-project algorithm for matrix inversion is used to accelerate the BFGS update, which in turn leads to the development of an accelerated BFGS optimization method.",
        "We propose an accelerated randomized algorithm to solve linear systems in Euclidean spaces.",
        "We will use the result of this section later to analyze our newly proposed matrix inversion algorithm, which we use to estimate the inverse of the Hessian within a quasi-Newton method.2",
        "The update of the inverse Hessian used in quasi-Newton methods can be seen as a sketch-and-project update applied to the linear system AX = I, while X = X> is enforced, and where A denotes and approximation of the Hessian.",
        "By observing that (20) is the sketch-and-project algorithm applied to a linear operator equation, we have constructed an accelerated version in Algorithm 2.",
        "The above is a sketch-and-project update for a linear system in Rn2 , which allows to obtain an alternative proof of Theorem 5, without using our results from Euclidean spaces.",
        "As a tweak in the stochastic BFGS allows for a faster estimation of Hessian inverse and more accurate steps of the method, one might wonder if a equivalent tweak might speed up the standard, deterministic BFGS algorithm for solving (1).",
        "We developed an accelerated sketch-and-project method for solving linear systems in Euclidean spaces.",
        "Our accelerated matrix inversion algorithm was incorporated into an optimization framework to develop both accelerated stochastic and accelerated deterministic BFGS, which to the best of our knowledge, are the first accelerated quasi-Newton updates.",
        "We show that under a careful choice of the parameters of the method\u2014depending on the problem structure and conditioning\u2014acceleration might result into significant speedups both for the matrix inversion problem and for the stochastic BFGS algorithm.",
        "We confirm experimentally that our accelerated methods can lead to speed-ups when compared to the classical BFGS algorithm"
    ],
    "headline": "We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine learning in linear time. The Journal of Machine Learning Research, 18(1):4148\u20134187, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Naman%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Second-order%20stochastic%20optimization%20for%20machine%20learning%20in%20linear%20time%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Naman%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Second-order%20stochastic%20optimization%20for%20machine%20learning%20in%20linear%20time%202017"
        },
        {
            "id": "2",
            "entry": "[2] Albert S Berahas, Raghu Bollapragada, and Jorge Nocedal. An investigation of Newton-sketch and subsampled Newton methods. CoRR, abs/1705.06211, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06211"
        },
        {
            "id": "3",
            "entry": "[3] Albert S Berahas, Jorge Nocedal, and pages=1055\u20131063 year=2016 Tak\u00e1c, Martin, booktitle=Advances in Neural Information Processing Systems. A multi-batch l-bfgs method for machine learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berahas%2C%20Albert%20S.%20Nocedal%2C%20Jorge%20and%20pages%3D1055%E2%80%931063%20year%3D2016%20Tak%C3%A1c%2C%20Martin%2C%20booktitle%3DAdvances%20in%20Neural%20Information%20Processing%20Systems.%20A%20multi-batch%20l-bfgs%20method%20for%20machine%20learning"
        },
        {
            "id": "4",
            "entry": "[4] Charles G Broyden. Quasi-Newton methods and their application to function minimisation. Mathematics of Computation, 21(99):368\u2013381, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Broyden%2C%20Charles%20G.%20Quasi-Newton%20methods%20and%20their%20application%20to%20function%20minimisation%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Broyden%2C%20Charles%20G.%20Quasi-Newton%20methods%20and%20their%20application%20to%20function%20minimisation%201967"
        },
        {
            "id": "5",
            "entry": "[5] Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):977\u2013995, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20Richard%20H.%20Chin%2C%20Gillian%20M.%20Neveitt%2C%20Will%20Nocedal%2C%20Jorge%20On%20the%20use%20of%20stochastic%20hessian%20information%20in%20optimization%20methods%20for%20machine%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20Richard%20H.%20Chin%2C%20Gillian%20M.%20Neveitt%2C%20Will%20Nocedal%2C%20Jorge%20On%20the%20use%20of%20stochastic%20hessian%20information%20in%20optimization%20methods%20for%20machine%20learning%202011"
        },
        {
            "id": "6",
            "entry": "[6] Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasiNewton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008\u20131031, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20Richard%20H.%20Hansen%2C%20Samantha%20L.%20Nocedal%2C%20Jorge%20Singer%2C%20Yoram%20A%20stochastic%20quasiNewton%20method%20for%20large-scale%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20Richard%20H.%20Hansen%2C%20Samantha%20L.%20Nocedal%2C%20Jorge%20Singer%2C%20Yoram%20A%20stochastic%20quasiNewton%20method%20for%20large-scale%20optimization%202016"
        },
        {
            "id": "7",
            "entry": "[7] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1\u201327:27, May 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20Libsvm%3A%20A%20library%20for%20support%20vector%20machines%202011-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20Libsvm%3A%20A%20library%20for%20support%20vector%20machines%202011-05"
        },
        {
            "id": "8",
            "entry": "[8] Frank Curtis. A self-correcting variable-metric algorithm for stochastic optimization. In International Conference on Machine Learning, pages 632\u2013641, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Curtis%2C%20Frank%20A%20self-correcting%20variable-metric%20algorithm%20for%20stochastic%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Curtis%2C%20Frank%20A%20self-correcting%20variable-metric%20algorithm%20for%20stochastic%20optimization%202016"
        },
        {
            "id": "9",
            "entry": "[9] Charles A Desoer and Barry H Whalen. A note on pseudoinverses. Journal of the Society of Industrial and Applied Mathematics, 11(2):442\u2013447, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Desoer%2C%20Charles%20A.%20Whalen%2C%20Barry%20H.%20A%20note%20on%20pseudoinverses%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Desoer%2C%20Charles%20A.%20Whalen%2C%20Barry%20H.%20A%20note%20on%20pseudoinverses%201963"
        },
        {
            "id": "10",
            "entry": "[10] Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):317\u2013 322, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fletcher%2C%20Roger%20A%20new%20approach%20to%20variable%20metric%20algorithms%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fletcher%2C%20Roger%20A%20new%20approach%20to%20variable%20metric%20algorithms%201970"
        },
        {
            "id": "11",
            "entry": "[11] Donald Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23\u201326, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldfarb%2C%20Donald%20A%20family%20of%20variable-metric%20methods%20derived%20by%20variational%20means%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldfarb%2C%20Donald%20A%20family%20of%20variable-metric%20methods%20derived%20by%20variational%20means%201970"
        },
        {
            "id": "12",
            "entry": "[12] Robert M Gower, Donald Goldfarb, and Peter Richt\u00e1rik. Stochastic block BFGS: Squeezing more curvature out of data. In International Conference on Machine Learning, pages 1869\u20131878, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gower%2C%20Robert%20M.%20Goldfarb%2C%20Donald%20Richt%C3%A1rik%2C%20Peter%20Stochastic%20block%20BFGS%3A%20Squeezing%20more%20curvature%20out%20of%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gower%2C%20Robert%20M.%20Goldfarb%2C%20Donald%20Richt%C3%A1rik%2C%20Peter%20Stochastic%20block%20BFGS%3A%20Squeezing%20more%20curvature%20out%20of%20data%202016"
        },
        {
            "id": "13",
            "entry": "[13] Robert M Gower and Peter Richt\u00e1rik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660\u20131690, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gower%2C%20Robert%20M.%20Richt%C3%A1rik%2C%20Peter%20Randomized%20iterative%20methods%20for%20linear%20systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gower%2C%20Robert%20M.%20Richt%C3%A1rik%2C%20Peter%20Randomized%20iterative%20methods%20for%20linear%20systems%202015"
        },
        {
            "id": "14",
            "entry": "[14] Robert M Gower and Peter Richt\u00e1rik. Stochastic dual ascent for solving linear systems. arXiv:1512.06890, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.06890"
        },
        {
            "id": "15",
            "entry": "[15] Robert M Gower and Peter Richt\u00e1rik. Randomized quasi-Newton updates are linearly convergent matrix inversion algorithms. SIAM Journal on Matrix Analysis and Applications, 38(4):1380\u2013 1409, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gower%2C%20Robert%20M.%20Richt%C3%A1rik%2C%20Peter%20Randomized%20quasi-Newton%20updates%20are%20linearly%20convergent%20matrix%20inversion%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gower%2C%20Robert%20M.%20Richt%C3%A1rik%2C%20Peter%20Randomized%20quasi-Newton%20updates%20are%20linearly%20convergent%20matrix%20inversion%20algorithms%202017"
        },
        {
            "id": "16",
            "entry": "[16] Stefan Kaczmarz. Angen\u00e4herte Aufl\u00f6sung von Systemen linearer Gleichungen. Bulletin International de l\u2019Acad\u00e9mie Polonaise des Sciences et des Lettres, 35:355\u2013357, 1937.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaczmarz%2C%20Stefan%20Angen%C3%A4herte%20Aufl%C3%B6sung%20von%20Systemen%20linearer%20Gleichungen%201937",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaczmarz%2C%20Stefan%20Angen%C3%A4herte%20Aufl%C3%B6sung%20von%20Systemen%20linearer%20Gleichungen%201937"
        },
        {
            "id": "17",
            "entry": "[17] Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503\u2013528, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Dong%20C.%20Nocedal%2C%20Jorge%20On%20the%20limited%20memory%20BFGS%20method%20for%20large%20scale%20optimization%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Dong%20C.%20Nocedal%2C%20Jorge%20On%20the%20limited%20memory%20BFGS%20method%20for%20large%20scale%20optimization%201989"
        },
        {
            "id": "18",
            "entry": "[18] Ji Liu and Stephen J Wright. An accelerated randomized Kaczmarz algorithm. Math. Comput., 85(297):153\u2013178, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20An%20accelerated%20randomized%20Kaczmarz%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20An%20accelerated%20randomized%20Kaczmarz%20algorithm%202016"
        },
        {
            "id": "19",
            "entry": "[19] Nicolas Loizou and Peter Richt\u00e1rik. Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods. arXiv preprint arXiv:1712.09677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09677"
        },
        {
            "id": "20",
            "entry": "[20] Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory BFGS. The Journal of Machine Learning Research, 16:3151\u20133181, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mokhtari%2C%20Aryan%20Ribeiro%2C%20Alejandro%20Global%20convergence%20of%20online%20limited%20memory%20BFGS%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mokhtari%2C%20Aryan%20Ribeiro%2C%20Alejandro%20Global%20convergence%20of%20online%20limited%20memory%20BFGS%202015"
        },
        {
            "id": "21",
            "entry": "[21] Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic L-BFGS algorithm. In Artificial Intelligence and Statistics, pages 249\u2013258, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moritz%2C%20Philipp%20Nishihara%2C%20Robert%20Jordan%2C%20Michael%20A%20linearly-convergent%20stochastic%20L-BFGS%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moritz%2C%20Philipp%20Nishihara%2C%20Robert%20Jordan%2C%20Michael%20A%20linearly-convergent%20stochastic%20L-BFGS%20algorithm%202016"
        },
        {
            "id": "22",
            "entry": "[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). Soviet Mathematics Doklady, 27(2):372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983"
        },
        {
            "id": "23",
            "entry": "[23] Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Efficiency%20of%20coordinate%20descent%20methods%20on%20huge-scale%20optimization%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Efficiency%20of%20coordinate%20descent%20methods%20on%20huge-scale%20optimization%20problems%202012"
        },
        {
            "id": "24",
            "entry": "[24] Yurii Nesterov and Sebastian U Stich. Efficiency of the accelerated coordinate descent method on structured optimization problems. SIAM Journal on Optimization, 27(1):110\u2013123, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Stich%2C%20Sebastian%20U.%20Efficiency%20of%20the%20accelerated%20coordinate%20descent%20method%20on%20structured%20optimization%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Stich%2C%20Sebastian%20U.%20Efficiency%20of%20the%20accelerated%20coordinate%20descent%20method%20on%20structured%20optimization%20problems%202017"
        },
        {
            "id": "25",
            "entry": "[25] Gert K Pedersen. Analysis Now. Graduate Texts in Mathematics. Springer New York, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedersen%2C%20Gert%20K.%20Analysis%20Now.%20Graduate%20Texts%20in%20Mathematics%201996"
        },
        {
            "id": "26",
            "entry": "[26] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205\u2013245, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilanci%2C%20Mert%20Wainwright%2C%20Martin%20J.%20Newton%20sketch%3A%20A%20near%20linear-time%20optimization%20algorithm%20with%20linear-quadratic%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilanci%2C%20Mert%20Wainwright%2C%20Martin%20J.%20Newton%20sketch%3A%20A%20near%20linear-time%20optimization%20algorithm%20with%20linear-quadratic%20convergence%202017"
        },
        {
            "id": "27",
            "entry": "[27] Peter Richt\u00e1rik and Martin Tak\u00e1c. Stochastic reformulations of linear systems: accelerated method. Manuscript, October 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20Stochastic%20reformulations%20of%20linear%20systems%3A%20accelerated%20method%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20Stochastic%20reformulations%20of%20linear%20systems%3A%20accelerated%20method%202017-10"
        },
        {
            "id": "28",
            "entry": "[28] Peter Richt\u00e1rik and Martin Tak\u00e1c. Stochastic reformulations of linear systems: algorithms and convergence theory. arXiv:1706.01108, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01108"
        },
        {
            "id": "29",
            "entry": "[29] Nicol N Schraudolph, Jin Yu, and Simon G\u00fcnter. A stochastic quasi-Newton method for online convex optimization. In Artificial Intelligence and Statistics, pages 436\u2013443, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schraudolph%2C%20Nicol%20N.%20Yu%2C%20Jin%20G%C3%BCnter%2C%20Simon%20A%20stochastic%20quasi-Newton%20method%20for%20online%20convex%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schraudolph%2C%20Nicol%20N.%20Yu%2C%20Jin%20G%C3%BCnter%2C%20Simon%20A%20stochastic%20quasi-Newton%20method%20for%20online%20convex%20optimization%202007"
        },
        {
            "id": "30",
            "entry": "[30] David F Shanno. Conditioning of quasi-Newton methods for function minimization. Mathematics of computation, 24(111):647\u2013656, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shanno%2C%20David%20F.%20Conditioning%20of%20quasi-Newton%20methods%20for%20function%20minimization%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shanno%2C%20David%20F.%20Conditioning%20of%20quasi-Newton%20methods%20for%20function%20minimization%201970"
        },
        {
            "id": "31",
            "entry": "[31] Sebastian U Stich. Convex Optimization with Random Pursuit. PhD thesis, ETH Zurich, 2014. Diss., Eidgen\u00f6ssische Technische Hochschule ETH Z\u00fcrich, Nr. 22111.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stich%2C%20Sebastian%20U.%20Convex%20Optimization%20with%20Random%20Pursuit%202014"
        },
        {
            "id": "32",
            "entry": "[32] Sebastian U Stich, Christian L M\u00fcller, and Bernd G\u00e4rtner. Variable metric random pursuit. Mathematical Programming, 156(1):549\u2013579, Mar 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stich%2C%20Sebastian%20U.%20M%C3%BCller%2C%20Christian%20L.%20G%C3%A4rtner%2C%20Bernd%20Variable%20metric%20random%20pursuit%202016-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stich%2C%20Sebastian%20U.%20M%C3%BCller%2C%20Christian%20L.%20G%C3%A4rtner%2C%20Bernd%20Variable%20metric%20random%20pursuit%202016-03"
        },
        {
            "id": "33",
            "entry": "[33] Thomas Strohmer and Roman Vershynin. A randomized Kaczmarz algorithm with exponential convergence. Journal of Fourier Analysis and Applications, 15(2):262, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strohmer%2C%20Thomas%20Vershynin%2C%20Roman%20A%20randomized%20Kaczmarz%20algorithm%20with%20exponential%20convergence%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strohmer%2C%20Thomas%20Vershynin%2C%20Roman%20A%20randomized%20Kaczmarz%20algorithm%20with%20exponential%20convergence%202009"
        },
        {
            "id": "34",
            "entry": "[34] Stephen Tu, Shivaram Venkataraman, Ashia C Wilson, Alex Gittens, Michael I Jordan, and Benjamin Recht. Breaking locality accelerates block Gauss-Seidel. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 3482\u20133491, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tu%2C%20Stephen%20Venkataraman%2C%20Shivaram%20Wilson%2C%20Ashia%20C.%20Gittens%2C%20Alex%20Breaking%20locality%20accelerates%20block%20Gauss-Seidel%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tu%2C%20Stephen%20Venkataraman%2C%20Shivaram%20Wilson%2C%20Ashia%20C.%20Gittens%2C%20Alex%20Breaking%20locality%20accelerates%20block%20Gauss-Seidel%202017-08"
        },
        {
            "id": "35",
            "entry": "[35] Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods for nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927\u2013956, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiao%20Ma%2C%20Shiqian%20Goldfarb%2C%20Donald%20Liu%2C%20Wei%20Stochastic%20quasi-Newton%20methods%20for%20nonconvex%20stochastic%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xiao%20Ma%2C%20Shiqian%20Goldfarb%2C%20Donald%20Liu%2C%20Wei%20Stochastic%20quasi-Newton%20methods%20for%20nonconvex%20stochastic%20optimization%202017"
        },
        {
            "id": "36",
            "entry": "[36] Stephen J Wright. Coordinate descent algorithms. Math. Program., 151(1):3\u201334, June 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wright%2C%20Stephen%20J.%20Coordinate%20descent%20algorithms%202015-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wright%2C%20Stephen%20J.%20Coordinate%20descent%20algorithms%202015-06"
        },
        {
            "id": "37",
            "entry": "[37] Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Newton-type methods for non-convex optimization under inexact hessian information. arXiv preprint arXiv:1708.07164, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07164"
        },
        {
            "id": "38",
            "entry": "[38] Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher R\u00e9, and Michael W Mahoney. Sub-sampled newton methods with non-uniform sampling. In Advances in Neural Information Processing Systems, pages 3000\u20133008, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Peng%20Yang%2C%20Jiyan%20Roosta-Khorasani%2C%20Farbod%20R%C3%A9%2C%20Christopher%20Sub-sampled%20newton%20methods%20with%20non-uniform%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Peng%20Yang%2C%20Jiyan%20Roosta-Khorasani%2C%20Farbod%20R%C3%A9%2C%20Christopher%20Sub-sampled%20newton%20methods%20with%20non-uniform%20sampling%202016"
        }
    ]
}
