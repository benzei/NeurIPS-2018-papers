{
    "filename": "8269-analysis-of-krylov-subspace-solutions-of-regularized-non-convex-quadratic-problems.pdf",
    "metadata": {
        "title": "Analysis of Krylov Subspace Solutions of  Regularized Non-Convex Quadratic Problems",
        "date": 2018,
        "author": "Yair Carmon Department of Electrical Engineering",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8269-analysis-of-krylov-subspace-solutions-of-regularized-non-convex-quadratic-problems.pdf"
        },
        "abstract": "We provide convergence rates for Krylov subspace solutions to the trust-region and cubic-regularized (nonconvex) quadratic problems. Such solutions may be efficiently computed by the Lanczos method and haveplong been used in practice. We prove error bounds of the form 1/t2 and e 4t/"
    },
    "keywords": [
        {
            "term": "lanczos algorithm",
            "url": "https://en.wikipedia.org/wiki/lanczos_algorithm"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "krylov subspace",
            "url": "https://en.wikipedia.org/wiki/krylov_subspace"
        },
        {
            "term": "lanczos method",
            "url": "https://en.wikipedia.org/wiki/lanczos_method"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "block lanczos",
            "url": "https://en.wikipedia.org/wiki/Block_Lanczos"
        },
        {
            "term": "quadratic function",
            "url": "https://en.wikipedia.org/wiki/quadratic_function"
        },
        {
            "term": "newton method",
            "url": "https://en.wikipedia.org/wiki/newton_method"
        }
    ],
    "highlights": [
        "Related work Zhang et al [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] show that the error of certain polynomial approximation problems bounds the suboptimality of Krylov subspace solutipons to the trust region-variant of the problems (1), implying convergence at a rate exponential in t/",
        "There, we show a rate of convergence roughly t 1, reflecting the well-known complexity gap between gradient descent and conjugate gradient methods [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]"
    ],
    "key_statements": [
        "Related work Zhang et al [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] show that the error of certain polynomial approximation problems bounds the suboptimality of Krylov subspace solutipons to the trust region-variant of the problems (1), implying convergence at a rate exponential in t/",
        "There, we show a rate of convergence roughly t 1, reflecting the well-known complexity gap between gradient descent and conjugate gradient methods [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]"
    ],
    "summary": [
        "Related work Zhang et al [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] show that the error of certain polynomial approximation problems bounds the suboptimality of Krylov subspace solutipons to the trust region-variant of the problems (1), implying convergence at a rate exponential in t/",
        "Bx denote a minimizer of the trust-region problem in the Krylov subspace of order t .",
        "As with eigenvector methods [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], simple randomization approaches allow us to handle the hard case with high probability, at the modest cost of introducing to the error bounds a logarithmic dependence on d.",
        "The trust-region and cubic-regularized problems (1) can be solved efficiently in K2t(A, {b, v}) using the block Lanczos method [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]; we survey this technique in Section A.1 in the supplement.",
        "Corollary 2 implies we can solve the trust-region problem to \u270f accuracy in roughly \u270f 1/2 log d matrix-vector products, even in the hard case.",
        "The main drawback of this randomization approach is that half the matrix-vector products are expended on the random vector; when the problem is well-conditioned or when |uTminb|/kbk is not extremely small, using the standard subspace solution is nearly twice as fast.",
        "The second approach follows the proposal [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] to construct a perturbed version of the linear term b, denotedb, and solve the problem instance (A, \u0303b, R) in the Krylov subspace Kt(A, \u0303b).",
        "We believe it is possible to show that, with randomization, Krylov subspace methods exhibit linear convergence even in the hard case, where the condition number is replaced by the normalized eigen-gap ( max min)/( 2 min), with 2 the smallest eigenvalue of A larger than min.",
        "We state the result for the cubic-regularization problem; corresponding lower bounds for the trust-region problem are immediate from the optimality gap relation (16).1",
        "The lower bounds (19) matches the linear convergence guarantee (17) to within a numerical constant, as we may choose max, min and ?",
        "Running Krylov subspace methods for d iterations with inexact arithmetic often results in solutions that are very far from exact, while guarantees of the form (17) are more robust to roundoff errors [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>].",
        "The randomization we employ in Corollaries 2 and 3 breaks the lower bound (20) when min < 0 and kbk /|uTminb| is very large, so there is some substantial power from randomization in this case.",
        "To see whether our analysis applies to non-worst case problem instances, we generate 5,000 random cubic-regularization problems with d = 106 and controlled condition number"
    ],
    "headline": "We provide convergence rates for Krylov subspace solutions to the trust-region and cubic-regularized  quadratic problems",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and T. Ma. Finding approximate local minima faster than gradient descent. In Proceedings of the Forty-Ninth Annual ACM Symposium on the Theory of Computing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017"
        },
        {
            "id": "2",
            "entry": "[2] Z. Allen-Zhu and L. Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. In Proceedings of the 8th Innovations in Theoretical Computer Science, ITCS \u201917, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Orecchia%2C%20L.%20Linear%20coupling%3A%20An%20ultimate%20unification%20of%20gradient%20and%20mirror%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Orecchia%2C%20L.%20Linear%20coupling%3A%20An%20ultimate%20unification%20of%20gradient%20and%20mirror%20descent%202017"
        },
        {
            "id": "3",
            "entry": "[3] J. Blanchet, C. Cartis, M. Menickelly, and K. Scheinberg. Convergence rate analysis of a stochastic trust region method for nonconvex optimization. arXiv:1609.07428 [math.OC], 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07428"
        },
        {
            "id": "4",
            "entry": "[4] A. S. Cameron Musco, Christopher Musco. Stability of the Lanczos method for matrix function approximation. arXiv:1708.07788 [cs.DS], 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07788"
        },
        {
            "id": "5",
            "entry": "[5] Y. Carmon and J. C. Duchi. Gradient descent efficiently finds the cubic-regularized non-convex Newton step. arXiv:1612.00547 [math.OC], 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00547"
        },
        {
            "id": "6",
            "entry": "[6] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Convex until proven guilty: dimensionfree acceleration of gradient descent on non-convex functions. In Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20Convex%20until%20proven%20guilty%3A%20dimensionfree%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20Convex%20until%20proven%20guilty%3A%20dimensionfree%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017"
        },
        {
            "id": "7",
            "entry": "[7] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points II: First order methods. arXiv:1711.00841 [math.OC], 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00841"
        },
        {
            "id": "8",
            "entry": "[8] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Accelerated methods for non-convex optimization. SIAM Journal on Optimization, 28(2):1751\u20131772, 2018. URL https://arXiv.org/abs/1611.00756.",
            "url": "https://arXiv.org/abs/1611.00756",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "9",
            "entry": "[9] C. Cartis, N. I. M. Gould, and P. L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. Part I: motivation, convergence and numerical results. Mathematical Programming, Series A, 127:245\u2013295, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20Part%20I%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20Part%20I%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011"
        },
        {
            "id": "10",
            "entry": "[10] E. S. Coakley and V. Rokhlin. A fast divide-and-conquer algorithm for computing the spectra of real symmetric tridiagonal matrices. Applied and Computational Harmonic Analysis, 34(3): 379\u2013414, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coakley%2C%20E.S.%20Rokhlin%2C%20V.%20A%20fast%20divide-and-conquer%20algorithm%20for%20computing%20the%20spectra%20of%20real%20symmetric%20tridiagonal%20matrices%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coakley%2C%20E.S.%20Rokhlin%2C%20V.%20A%20fast%20divide-and-conquer%20algorithm%20for%20computing%20the%20spectra%20of%20real%20symmetric%20tridiagonal%20matrices%202013"
        },
        {
            "id": "11",
            "entry": "[11] A. R. Conn, N. I. M. Gould, and P. L. Toint. Trust Region Methods. MPS-SIAM Series on Optimization. SIAM, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conn%2C%20A.R.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Trust%20Region%20Methods%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conn%2C%20A.R.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Trust%20Region%20Methods%202000"
        },
        {
            "id": "12",
            "entry": "[12] J. Cullum and W. E. Donath. A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace of large, sparse, real symmetric matrices. In Decision and Control including the 13th Symposium on Adaptive Processes, 1974 IEEE Conference on, volume 13, pages 505\u2013509. IEEE, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cullum%2C%20J.%20Donath%2C%20W.E.%20A%20block%20Lanczos%20algorithm%20for%20computing%20the%20q%20algebraically%20largest%20eigenvalues%20and%20a%20corresponding%20eigenspace%20of%20large%2C%20sparse%2C%20real%20symmetric%20matrices%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cullum%2C%20J.%20Donath%2C%20W.E.%20A%20block%20Lanczos%20algorithm%20for%20computing%20the%20q%20algebraically%20largest%20eigenvalues%20and%20a%20corresponding%20eigenspace%20of%20large%2C%20sparse%2C%20real%20symmetric%20matrices%201974"
        },
        {
            "id": "13",
            "entry": "[13] V. Druskin and L. Knizhnerman. Error bounds in the simple Lanczos procedure for computing functions of symmetric matrices and eigenvalues. U.S.S.R. Computational Mathematics and Mathematical Physics, 31(7):970\u2013983, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Druskin%2C%20V.%20Knizhnerman%2C%20L.%20Error%20bounds%20in%20the%20simple%20Lanczos%20procedure%20for%20computing%20functions%20of%20symmetric%20matrices%20and%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Druskin%2C%20V.%20Knizhnerman%2C%20L.%20Error%20bounds%20in%20the%20simple%20Lanczos%20procedure%20for%20computing%20functions%20of%20symmetric%20matrices%20and%201991"
        },
        {
            "id": "14",
            "entry": "[14] G. Golub and C. V. Loan. Matrix computations. John Hopkins University Press, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20G.%20Loan%2C%20C.V.%20Matrix%20computations%201989"
        },
        {
            "id": "15",
            "entry": "[15] G. H. Golub and R. Underwood. The block Lanczos method for computing eigenvalues. In Mathematical software, pages 361\u2013377.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20G.H.%20Underwood%2C%20R.%20The%20block%20Lanczos%20method%20for%20computing%20eigenvalues",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golub%2C%20G.H.%20Underwood%2C%20R.%20The%20block%20Lanczos%20method%20for%20computing%20eigenvalues"
        },
        {
            "id": "16",
            "entry": "[16] N. I. Gould, D. Orban, and P. L. Toint. GALAHAD, a library of thread-safe Fortran 90 packages for large-scale nonlinear optimization. ACM Transactions on Mathematical Software (TOMS), 29(4):353\u2013372, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gould%2C%20N.I.%20Orban%2C%20D.%20Toint%2C%20P.L.%20GALAHAD%2C%20a%20library%20of%20thread-safe%20Fortran%2090%20packages%20for%20large-scale%20nonlinear%20optimization%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gould%2C%20N.I.%20Orban%2C%20D.%20Toint%2C%20P.L.%20GALAHAD%2C%20a%20library%20of%20thread-safe%20Fortran%2090%20packages%20for%20large-scale%20nonlinear%20optimization%202003"
        },
        {
            "id": "17",
            "entry": "[17] N. I. M. Gould, S. Lucidi, M. Roma, and P. L. Toint. Solving the trust-region subproblem using the Lanczos method. SIAM Journal on Optimization, 9(2):504\u2013525, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gould%2C%20N.I.M.%20Lucidi%2C%20S.%20Roma%2C%20M.%20Toint%2C%20P.L.%20Solving%20the%20trust-region%20subproblem%20using%20the%20Lanczos%20method%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gould%2C%20N.I.M.%20Lucidi%2C%20S.%20Roma%2C%20M.%20Toint%2C%20P.L.%20Solving%20the%20trust-region%20subproblem%20using%20the%20Lanczos%20method%201999"
        },
        {
            "id": "18",
            "entry": "[18] A. Griewank. The modification of Newton\u2019s method for unconstrained optimization by bounding cubic terms. Technical report, Technical report NA/12, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20A.%20The%20modification%20of%20Newton%E2%80%99s%20method%20for%20unconstrained%20optimization%20by%20bounding%20cubic%20terms%201981"
        },
        {
            "id": "19",
            "entry": "[19] E. Hazan and T. Koren. A linear-time algorithm for trust region problems. Mathematical Programming, Series A, 158(1):363\u2013381, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Koren%2C%20T.%20A%20linear-time%20algorithm%20for%20trust%20region%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Koren%2C%20T.%20A%20linear-time%20algorithm%20for%20trust%20region%20problems%202016"
        },
        {
            "id": "20",
            "entry": "[20] M. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems. Journal of Research of the National Bureau of Standards, 49(6), 1952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hestenes%2C%20M.%20Stiefel%2C%20E.%20Methods%20of%20conjugate%20gradients%20for%20solving%20linear%20systems%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hestenes%2C%20M.%20Stiefel%2C%20E.%20Methods%20of%20conjugate%20gradients%20for%20solving%20linear%20systems%201952"
        },
        {
            "id": "21",
            "entry": "[21] N. Ho-Nguyen and F. K\u0131l\u0131nc-Karzan. A second-order cone based approach for solving the trust-region subproblem and its variants. arXiv:1603.03366 [math.OC], 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.03366"
        },
        {
            "id": "22",
            "entry": "[22] C. Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv:1711.10456 [cs.LG], 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10456"
        },
        {
            "id": "23",
            "entry": "[23] J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization. In Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017"
        },
        {
            "id": "24",
            "entry": "[24] J. Kuczynski and H. Wozniakowski. Estimating the largest eigenvalue by the power and Lanczos algorithms with a random start. SIAM Journal on Matrix Analysis and Applications, 13(4): 1094\u20131122, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuczynski%2C%20J.%20Wozniakowski%2C%20H.%20Estimating%20the%20largest%20eigenvalue%20by%20the%20power%20and%20Lanczos%20algorithms%20with%20a%20random%20start%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuczynski%2C%20J.%20Wozniakowski%2C%20H.%20Estimating%20the%20largest%20eigenvalue%20by%20the%20power%20and%20Lanczos%20algorithms%20with%20a%20random%20start%201992"
        },
        {
            "id": "25",
            "entry": "[25] F. Lenders, C. Kirches, and A. Potschka. trlib: A vector-free implementation of the GLTR method for iterative solution of the trust region problem. Optimization Methods and Software, 33(3):420\u2013449, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lenders%2C%20F.%20Kirches%2C%20C.%20A.%20Potschka.%20trlib%3A%20A%20vector-free%20implementation%20of%20the%20GLTR%20method%20for%20iterative%20solution%20of%20the%20trust%20region%20problem%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lenders%2C%20F.%20Kirches%2C%20C.%20A.%20Potschka.%20trlib%3A%20A%20vector-free%20implementation%20of%20the%20GLTR%20method%20for%20iterative%20solution%20of%20the%20trust%20region%20problem%202018"
        },
        {
            "id": "26",
            "entry": "[26] A. Nemirovski. Efficient methods in convex programming. Technion: The Israel Institute of Technology, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Efficient%20methods%20in%20convex%20programming%201994"
        },
        {
            "id": "27",
            "entry": "[27] A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Yudin%2C%20D.%20Problem%20Complexity%20and%20Method%20Efficiency%20in%20Optimization%201983"
        },
        {
            "id": "28",
            "entry": "[28] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20Lectures%20on%20Convex%20Optimization%202004"
        },
        {
            "id": "29",
            "entry": "[29] Y. Nesterov and B. Polyak. Cubic regularization of Newton method and its global performance. Mathematical Programming, Series A, 108:177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Polyak%2C%20B.%20Cubic%20regularization%20of%20Newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Polyak%2C%20B.%20Cubic%20regularization%20of%20Newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "30",
            "entry": "[30] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Nocedal%20and%20S%20J%20Wright%20Numerical%20Optimization%20Springer%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Nocedal%20and%20S%20J%20Wright%20Numerical%20Optimization%20Springer%202006"
        },
        {
            "id": "31",
            "entry": "[31] B. A. Pearlmutter. Fast exact multiplication by the Hessian. Neural computation, 6(1):147\u2013 160, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearlmutter%2C%20B.A.%20Fast%20exact%20multiplication%20by%20the%20Hessian%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pearlmutter%2C%20B.A.%20Fast%20exact%20multiplication%20by%20the%20Hessian%201994"
        },
        {
            "id": "32",
            "entry": "[32] J. Regier, M. I. Jordan, and J. McAuliffe. Fast black-box variational inference through stochastic trust-region optimization. In Advances in Neural Information Processing Systems 30, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Regier%2C%20J.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.%20Fast%20black-box%20variational%20inference%20through%20stochastic%20trust-region%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Regier%2C%20J.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.%20Fast%20black-box%20variational%20inference%20through%20stochastic%20trust-region%20optimization%202017"
        },
        {
            "id": "33",
            "entry": "[33] N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural computation, 14(7):1723\u20131738, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schraudolph%2C%20N.N.%20Fast%20curvature%20matrix-vector%20products%20for%20second-order%20gradient%20descent%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schraudolph%2C%20N.N.%20Fast%20curvature%20matrix-vector%20products%20for%20second-order%20gradient%20descent%202002"
        },
        {
            "id": "34",
            "entry": "[34] M. Simchowitz. On the randomized complexity of minimizing a convex quadratic function. arXiv:1807.09386 [cs.LG], 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.09386"
        },
        {
            "id": "35",
            "entry": "[35] L. N. Trefethen and D. Bau III. Numerical Linear Algebra. SIAM, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L%20N%20Trefethen%20and%20D%20Bau%20III%20Numerical%20Linear%20Algebra%20SIAM%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L%20N%20Trefethen%20and%20D%20Bau%20III%20Numerical%20Linear%20Algebra%20SIAM%201997"
        },
        {
            "id": "36",
            "entry": "[36] N. Tripuraneni, M. Stern, C. Jin, J. Regier, and M. I. Jordan. Stochastic cubic regularization for fast nonconvex optimization. arXiv:1711.02838 [cs.LG], 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02838"
        },
        {
            "id": "37",
            "entry": "[37] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. 2008. URL http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf.",
            "url": "http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf"
        },
        {
            "id": "38",
            "entry": "[38] Z. Yao, P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Inexact non-convex newton-type methods. arXiv:1802.06925 [math.OC], 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06925"
        },
        {
            "id": "39",
            "entry": "[39] L.-H. Zhang, C. Shen, and R.-C. Li. On the generalized Lanczos trust-region method. SIAM Journal on Optimization, 27(3):2110\u20132142, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20L.-H.%20Shen%2C%20C.%20Li%2C%20R.-C.%20On%20the%20generalized%20Lanczos%20trust-region%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20L.-H.%20Shen%2C%20C.%20Li%2C%20R.-C.%20On%20the%20generalized%20Lanczos%20trust-region%20method%202017"
        }
    ]
}
