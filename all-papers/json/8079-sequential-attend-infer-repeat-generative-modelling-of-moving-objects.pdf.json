{
    "filename": "8079-sequential-attend-infer-repeat-generative-modelling-of-moving-objects.pdf",
    "metadata": {
        "title": "Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects",
        "author": "Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, Ingmar Posner",
        "date": 2016,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8079-sequential-attend-infer-repeat-generative-modelling-of-moving-objects.pdf",
            "doi": "10.1038/nbt.3343.arXiv"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for videos of moving objects. It can reliably discover and track objects throughout the sequence of frames, and can also generate future frames conditioning on the current frame, thereby simulating expected motion of objects. This is achieved by explicitly encoding object presence, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et al., 2016), including learning in an unsupervised manner, and addresses its shortcomings. We use a moving multi-MNIST dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how SQAIR overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision."
    },
    "keywords": [
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        }
    ],
    "highlights": [
        "The ability to identify objects in their environments and to understand relations between them is a cornerstone of human intelligence (<a class=\"ref-link\" id=\"cKemp_2008_a\" href=\"#rKemp_2008_a\"><a class=\"ref-link\" id=\"cKemp_2008_a\" href=\"#rKemp_2008_a\">Kemp and Tenenbaum, 2008</a></a>)",
        "In Section 4, we demonstrate the model on a dataset of multiple moving MNIST digits (Section 4.1) and compare it against AIR trained on each frame and Variational Recurrent Neural Network (VRNN) of <a class=\"ref-link\" id=\"cChung_et+al_2015_a\" href=\"#rChung_et+al_2015_a\">Chung et al, 2015</a> with convolutional architectures, and show the superior performance of SQAIR in terms of log marginal likelihood and interpretability of latent variables",
        "We examine conditional generation, where we look at samples from the generative model of SQAIR conditioned on three images from a real sequence",
        "In this paper we proposed SQAIR, a probabilistic model that extends AIR to image sequences, and thereby achieves temporally consistent reconstructions and samples",
        "This work continues the thread of Greff, Steenkiste, et al, 2017, <a class=\"ref-link\" id=\"cSteenkiste_et+al_2018_a\" href=\"#rSteenkiste_et+al_2018_a\">Steenkiste et al, 2018</a> and, together with <a class=\"ref-link\" id=\"cHsieh_et+al_2018_a\" href=\"#rHsieh_et+al_2018_a\">Hsieh et al, 2018</a>, presents unsupervised object detection & tracking with learnable likelihoods by the means of generative modelling of objects",
        "SQAIR can be used for conditional generation, where it can extrapolate sequences into the future"
    ],
    "key_statements": [
        "The ability to identify objects in their environments and to understand relations between them is a cornerstone of human intelligence (<a class=\"ref-link\" id=\"cKemp_2008_a\" href=\"#rKemp_2008_a\"><a class=\"ref-link\" id=\"cKemp_2008_a\" href=\"#rKemp_2008_a\">Kemp and Tenenbaum, 2008</a></a>)",
        "We argue that this notion of consistency can be seen as an inductive bias that improves the efficiency of our learning",
        "In Section 4, we demonstrate the model on a dataset of multiple moving MNIST digits (Section 4.1) and compare it against AIR trained on each frame and Variational Recurrent Neural Network (VRNN) of <a class=\"ref-link\" id=\"cChung_et+al_2015_a\" href=\"#rChung_et+al_2015_a\">Chung et al, 2015</a> with convolutional architectures, and show the superior performance of SQAIR in terms of log marginal likelihood and interpretability of latent variables",
        "We investigate the utility of inferred latent variables of SQAIR in downstream tasks",
        "The resulting probabilistic model is composed of two parts: Discovery (DISC), which is responsible for detecting new objects at every time-step, and Propagation (PROP), responsible for updating latent variables from the previous time-step given the new observation, effectively implementing the temporal state-space model",
        "Inference to AIR, inference in SQAIR can capture the number of objects and the representation describing the location and appearance of each object that is necessary to explain every image in a sequence",
        "Variational Recurrent Neural Network under-performs due to the inability of disentangling overlapping objects, while both Variational Recurrent Neural Network and AIR suffer from low temporal consistency of learned representations, see Appendix H",
        "We evaluate SQAIR qualitatively by analyzing reconstructions and samples produced by the model against reconstructions and samples from Variational Recurrent Neural Network",
        "We examine conditional generation, where we look at samples from the generative model of SQAIR conditioned on three images from a real sequence",
        "Unlike the other unsupervised approaches, temporal consistency is baked into the model structure of SQAIR and further enforced by lower KL divergence when an object is tracked",
        "In this paper we proposed SQAIR, a probabilistic model that extends AIR to image sequences, and thereby achieves temporally consistent reconstructions and samples",
        "This work continues the thread of Greff, Steenkiste, et al, 2017, <a class=\"ref-link\" id=\"cSteenkiste_et+al_2018_a\" href=\"#rSteenkiste_et+al_2018_a\">Steenkiste et al, 2018</a> and, together with <a class=\"ref-link\" id=\"cHsieh_et+al_2018_a\" href=\"#rHsieh_et+al_2018_a\">Hsieh et al, 2018</a>, presents unsupervised object detection & tracking with learnable likelihoods by the means of generative modelling of objects",
        "SQAIR can be used for conditional generation, where it can extrapolate sequences into the future"
    ],
    "summary": [
        "The ability to identify objects in their environments and to understand relations between them is a cornerstone of human intelligence (<a class=\"ref-link\" id=\"cKemp_2008_a\" href=\"#rKemp_2008_a\"><a class=\"ref-link\" id=\"cKemp_2008_a\" href=\"#rKemp_2008_a\">Kemp and Tenenbaum, 2008</a></a>).",
        "AIR is able to decompose a visual scene into its constituent components and to generate a number of latent variables that explicitly encode the location and appearance of each object.",
        "We extend AIR into a spatio-temporal state-space model and train it on unlabelled image sequences of dynamic objects.",
        "In Section 3, we discuss its limitations and how it can be improved, thereby introducing Sequential Attend, Infer, Repeat (SQAIR), our extension of AIR to image sequences.",
        "In Section 4, we demonstrate the model on a dataset of multiple moving MNIST digits (Section 4.1) and compare it against AIR trained on each frame and Variational Recurrent Neural Network (VRNN) of <a class=\"ref-link\" id=\"cChung_et+al_2015_a\" href=\"#rChung_et+al_2015_a\">Chung et al, 2015</a> with convolutional architectures, and show the superior performance of SQAIR in terms of log marginal likelihood and interpretability of latent variables.",
        "Each triplet of latent variables explicitly encodes position, appearance and presence of the respective object, and the model is able to infer the number of objects present in the scene.",
        "We introduce Sequential Attend, Infer, Repeat (SQAIR), whereby AIR is augmented with a state-space model (SSM) to achieve temporal consistency in the generated images of the sequence.",
        "The resulting probabilistic model is composed of two parts: Discovery (DISC), which is responsible for detecting new objects at every time-step, and Propagation (PROP), responsible for updating latent variables from the previous time-step given the new observation, effectively implementing the temporal SSM.",
        "Inference to AIR, inference in SQAIR can capture the number of objects and the representation describing the location and appearance of each object that is necessary to explain every image in a sequence.",
        "The latent variables from the previous time step are used to infer the current ones, modelling the change in location and appearance of the corresponding objects, thereby attaining temporal consistency.",
        "VRNN under-performs due to the inability of disentangling overlapping objects, while both VRNN and AIR suffer from low temporal consistency of learned representations, see Appendix H.",
        "Unlike the other unsupervised approaches, temporal consistency is baked into the model structure of SQAIR and further enforced by lower KL divergence when an object is tracked.",
        "This is addressed in stochastic latent variable models trained using variational inference to generate multiple plausible videos given a sequence of images (<a class=\"ref-link\" id=\"cBabaeizadeh_et+al_2017_a\" href=\"#rBabaeizadeh_et+al_2017_a\">Babaeizadeh et al, 2017</a>; <a class=\"ref-link\" id=\"cDenton_2018_a\" href=\"#rDenton_2018_a\">Denton and Fergus, 2018</a>).",
        "In this paper we proposed SQAIR, a probabilistic model that extends AIR to image sequences, and thereby achieves temporally consistent reconstructions and samples."
    ],
    "headline": "We present Sequential Attend, Infer, Repeat , an interpretable deep generative model for videos of moving objects",
    "reference_links": [
        {
            "id": "Babaeizadeh_et+al_2017_a",
            "entry": "Babaeizadeh, M., C. Finn, D. Erhan, R. H. Campbell, and S. Levine (2017). \u201cStochastic Variational Video Prediction\u201d. In: CoRR. arXiv: 1710.11252.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11252"
        },
        {
            "id": "Bewley_et+al_2016_a",
            "entry": "Bewley, A., Z. Ge, L. Ott, F. T. Ramos, and B. Upcroft (2016). \u201cSimple online and realtime tracking\u201d. In: ICIP.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bewley%2C%20A.%20Ge%2C%20Z.%20Ott%2C%20L.%20Ramos%2C%20F.T.%20Simple%20online%20and%20realtime%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bewley%2C%20A.%20Ge%2C%20Z.%20Ott%2C%20L.%20Ramos%2C%20F.T.%20Simple%20online%20and%20realtime%20tracking%202016"
        },
        {
            "id": "Burda_et+al_2016_a",
            "entry": "Burda, Y., R. Grosse, and R. Salakhutdinov (2016). \u201cImportance Weighted Autoencoders\u201d. In: ICLR. arXiv: 1509.00519.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00519"
        },
        {
            "id": "Cho_et+al_2015_a",
            "entry": "Cho, M., S. Kwak, C. Schmid, and J. Ponce (2015). \u201cUnsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals\u201d. In: CoRR. arXiv: 1501.06170.",
            "arxiv_url": "https://arxiv.org/pdf/1501.06170"
        },
        {
            "id": "Chung_et+al_2015_a",
            "entry": "Chung, J., K. Kastner, L. Dinh, K. Goel, A. Courville, and Y. Bengio (2015). \u201cA Recurrent Latent Variable Model for Sequential Data\u201d. In: NIPS. arXiv: 1506.02216.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02216"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Clevert, D.-A., T. Unterthiner, and S. Hochreiter (2015). \u201cFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\u201d. In: CoRR. arXiv: 1511.07289.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Denton_2017_a",
            "entry": "Denton, E. and V. Birodkar (2017). \u201cUnsupervised learning of disentangled representations from video\u201d. In: NIPS.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017"
        },
        {
            "id": "Denton_2018_a",
            "entry": "Denton, E. and R. Fergus (2018). \u201cStochastic Video Generation with a Learned Prior\u201d. In: ICML. Eslami, S. M. A., N. Heess, T. Weber, Y. Tassa, D. Szepesvari, K. Kavukcuoglu, and G. E. Hinton",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.%20R.%20Stochastic%20Video%20Generation%20with%20a%20Learned%20Prior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.%20R.%20Stochastic%20Video%20Generation%20with%20a%20Learned%20Prior%202018"
        },
        {
            "id": "Attend_0000_a",
            "entry": "(2016). \u201cAttend, Infer, Repeat: Fast Scene Understanding with Generative Models\u201d. In: NIPS. arXiv: 1603.08575.",
            "arxiv_url": "https://arxiv.org/pdf/1603.08575"
        },
        {
            "id": "Gael_et+al_2009_a",
            "entry": "Gael, J. V., Y. W. Teh, and Z. Ghahramani (2009). \u201cThe Infinite Factorial Hidden Markov Model\u201d. In: NIPS. Graves, A., G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwinska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou, A. P. Badia, K. M. Hermann, Y. Zwols, G. Ostrovski, A. Cain, H. King, C. Summerfield, P. Blunsom, K. Kavukcuoglu, and D. Hassabis (2016). \u201cHybrid computing using a neural network with dynamic external memory\u201d. In: Nature 538.7626.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gael%2C%20J.V.%20Teh%2C%20Y.W.%20Z.%20The%20Infinite%20Factorial%20Hidden%20Markov%20Model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gael%2C%20J.V.%20Teh%2C%20Y.W.%20Z.%20The%20Infinite%20Factorial%20Hidden%20Markov%20Model%202009"
        },
        {
            "id": "Greff_et+al_2016_a",
            "entry": "Greff, K., A. Rasmus, M. Berglund, T. H. Hao, H. Valpola, and J. Schmidhuber (2016). \u201cTagger: Deep Unsupervised Perceptual Grouping\u201d. In: NIPS. Greff, K., S. van Steenkiste, and J. Schmidhuber (2017). \u201cNeural Expectation Maximization\u201d. In: NIPS. Gulrajani, I., K. Kumar, F. Ahmed, A. A. Taiga, F. Visin, D. Vazquez, and A. Courville (2016). \u201cPixelvae: A latent variable model for natural images\u201d. In: CoRR. arXiv: 1611.05013.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05013"
        },
        {
            "id": "Ha_2018_a",
            "entry": "Ha, D. and J. Schmidhuber (2018). \u201cWorld Models\u201d. In: CoRR. arXiv: 1603.10122.",
            "arxiv_url": "https://arxiv.org/pdf/1603.10122"
        },
        {
            "id": "Hsieh_et+al_2018_a",
            "entry": "Hsieh, J.-T., B. Liu, D.-A. Huang, L. Fei-Fei, and J. C. Niebles (2018). \u201cLearning to Decompose and Disentangle Representations for Video Prediction\u201d. In: NIPS. Ilin, A., I. Pr\u00e9mont-Schwarz, T. H. Hao, A. Rasmus, R. Boney, and H. Valpola (2017). \u201cRecurrent Ladder Networks\u201d. In: NIPS. Itseez (2015). Open Source Computer Vision Library. https://github.com/itseez/opencv.",
            "url": "https://github.com/itseez/opencv",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsieh%2C%20J.-T.%20Liu%2C%20B.%20Huang%2C%20D.-A.%20Fei-Fei%2C%20L.%20Learning%20to%20Decompose%20and%20Disentangle%20Representations%20for%20Video%20Prediction%202018"
        },
        {
            "id": "Jacobsen_et+al_2016_a",
            "entry": "Jacobsen, J.-H., J. Van Gemert, Z. Lou, and A. W. M. Smeulders (2016). \u201cStructured Receptive Fields in CNNs\u201d. In: CVPR. Jaderberg, M., K. Simonyan, A. Zisserman, and K. Kavukcuoglu (2015). \u201cSpatial Transformer Networks\u201d. In: NIPS. DOI: 10.1038/nbt.3343. arXiv: 1506.02025v1.",
            "crossref": "https://dx.doi.org/10.1038/nbt.3343.arXiv",
            "arxiv_url": "https://arxiv.org/pdf/1506.02025v1"
        },
        {
            "id": "Kemp_2008_a",
            "entry": "Kemp, C. and J. B. Tenenbaum (2008). \u201cThe discovery of structural form\u201d. In: Proceedings of the National Academy of Sciences 105.31. Kim, H. and A. Mnih (2018). \u201cDisentangling by factorising\u201d. In: ICML. arXiv: 1802.05983.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05983"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Kingma, D. P. and J. Ba (2015). \u201cAdam: A Method for Stochastic Optimization\u201d. In: ICLR. arXiv: 1412.6980.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Kingma, D. P. and M. Welling (2013). \u201cAuto-encoding variational bayes\u201d. In: arXiv preprint arXiv:1312.6114.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kosiorek_et+al_2017_a",
            "entry": "Kosiorek, A. R., A. Bewley, and I. Posner (2017). \u201cHierarchical Attentive Recurrent Tracking\u201d. In: NIPS. arXiv: 1706.09262.",
            "arxiv_url": "https://arxiv.org/pdf/1706.09262"
        },
        {
            "id": "Kwak_et+al_2015_a",
            "entry": "Kwak, S., M. Cho, I. Laptev, J. Ponce, and C. Schmid (2015). \u201cUnsupervised object discovery and tracking in video collections\u201d. In: ICCV. IEEE. LeCun, Y., Y. Bengio, and G. Hinton (2015). \u201cDeep learning\u201d. In: Nature 521.7553. LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel (1989). \u201cBackpropagation applied to handwritten zip code recognition\u201d. In: Neural computation 1.4. Maddison, C. J., J. Lawson, G. Tucker, N. Heess, M. Norouzi, A. Mnih, A. Doucet, and Y. Teh (2017). \u201cFiltering Variational Objectives\u201d. In: Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kwak%2C%20S.%20Cho%2C%20M.%20Laptev%2C%20I.%20Ponce%2C%20J.%20Unsupervised%20object%20discovery%20and%20tracking%20in%20video%20collections%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kwak%2C%20S.%20Cho%2C%20M.%20Laptev%2C%20I.%20Ponce%2C%20J.%20Unsupervised%20object%20discovery%20and%20tracking%20in%20video%20collections%202015"
        },
        {
            "id": "Mnih_2014_a",
            "entry": "Mnih, A. and K. Gregor (2014). \u201cNeural Variational Inference and Learning in Belief Networks\u201d. In: ICML. arXiv: 1402.0030v2.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0030v2"
        },
        {
            "id": "Mnih_2016_a",
            "entry": "Mnih, A. and D. J. Rezende (2016). \u201cVariational inference for Monte Carlo objectives\u201d. In: ICML. arXiv: 1602.06725.",
            "arxiv_url": "https://arxiv.org/pdf/1602.06725"
        },
        {
            "id": "Neiswanger_2012_a",
            "entry": "Neiswanger, W. and F. Wood (2012). \u201cUnsupervised Detection and Tracking of Arbitrary Objects with Dependent Dirichlet Process Mixtures\u201d. In: CoRR. arXiv: 1210.3288.",
            "arxiv_url": "https://arxiv.org/pdf/1210.3288"
        },
        {
            "id": "Oord_et+al_2016_a",
            "entry": "Oord, A. van den, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu (2016). \u201cConditional Image Generation with PixelCNN Decoders\u201d. In: NIPS. arXiv: 1606.05328.",
            "arxiv_url": "https://arxiv.org/pdf/1606.05328"
        },
        {
            "id": "Ranzato_et+al_2014_a",
            "entry": "Ranzato, M., A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and S. Chopra (2014). \u201cVideo (language) modeling: a baseline for generative models of natural videos\u201d. In: CoRR. arXiv: 1412.6604.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6604"
        },
        {
            "id": "Ristani_et+al_2016_a",
            "entry": "Ristani, E., F. Solera, R. Zou, R. Cucchiara, and C. Tomasi (2016). \u201cPerformance measures and a data set for multi-target, multi-camera tracking\u201d. In: ECCV. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ristani%2C%20E.%20Solera%2C%20F.%20Zou%2C%20R.%20Cucchiara%2C%20R.%20Performance%20measures%20and%20a%20data%20set%20for%20multi-target%2C%20multi-camera%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ristani%2C%20E.%20Solera%2C%20F.%20Zou%2C%20R.%20Cucchiara%2C%20R.%20Performance%20measures%20and%20a%20data%20set%20for%20multi-target%2C%20multi-camera%20tracking%202016"
        },
        {
            "id": "Santoro_et+al_2017_a",
            "entry": "Santoro, A., D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap (2017). \u201cA simple neural network module for relational reasoning\u201d. In: NIPS. arXiv: 1706.01427.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01427"
        },
        {
            "id": "Schulter_et+al_2017_a",
            "entry": "Schulter, S., P. Vernaza, W. Choi, and M. K. Chandraker (2017). \u201cDeep Network Flow for Multi-object Tracking\u201d. In: CVPR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulter%2C%20S.%20Vernaza%2C%20P.%20Choi%2C%20W.%20K%2C%20M.%20Deep%20Network%20Flow%20for%20Multi-object%20Tracking%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulter%2C%20S.%20Vernaza%2C%20P.%20Choi%2C%20W.%20K%2C%20M.%20Deep%20Network%20Flow%20for%20Multi-object%20Tracking%202017"
        },
        {
            "id": "Shi_et+al_2016_a",
            "entry": "Shi, W., J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang (2016). \u201cReal-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\u201d. In: CVPR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20W.%20Caballero%2C%20J.%20Huszar%2C%20F.%20Totz%2C%20J.%20Real-Time%20Single%20Image%20and%20Video%20Super-Resolution%20Using%20an%20Efficient%20Sub-Pixel%20Convolutional%20Neural%20Network%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20W.%20Caballero%2C%20J.%20Huszar%2C%20F.%20Totz%2C%20J.%20Real-Time%20Single%20Image%20and%20Video%20Super-Resolution%20Using%20an%20Efficient%20Sub-Pixel%20Convolutional%20Neural%20Network%202016"
        },
        {
            "id": "Srivastava_et+al_2015_a",
            "entry": "Srivastava, N., E. Mansimov, and R. Salakhudinov (2015). \u201cUnsupervised learning of video representations using lstms\u201d. In: ICML.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Mansimov%2C%20E.%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Mansimov%2C%20E.%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015"
        },
        {
            "id": "Steenkiste_et+al_2018_a",
            "entry": "Steenkiste, S. van, M. Chang, K. Greff, and J. Schmidhuber (2018). \u201cRelational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions\u201d. In: ICLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steenkiste%2C%20S.van%20Chang%2C%20M.%20Greff%2C%20K.%20J.%20Relational%20Neural%20Expectation%20Maximization%3A%20Unsupervised%20Discovery%20of%20Objects%20and%20their%20Interactions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steenkiste%2C%20S.van%20Chang%2C%20M.%20Greff%2C%20K.%20J.%20Relational%20Neural%20Expectation%20Maximization%3A%20Unsupervised%20Discovery%20of%20Objects%20and%20their%20Interactions%202018"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tieleman, T. and G. Hinton (2012). Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20G.%20Lecture%206.5%E2%80%94RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude.%20COURSERA%3A%20Neural%20Networks%20for%20Machine%20Learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20G.%20Lecture%206.5%E2%80%94RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude.%20COURSERA%3A%20Neural%20Networks%20for%20Machine%20Learning%202012"
        },
        {
            "id": "Tulyakov_et+al_2018_a",
            "entry": "Tulyakov, S., M.-Y. Liu, X. Yang, and J. Kautz (2018). \u201cMocogan: Decomposing motion and content for video generation\u201d. In: CVPR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tulyakov%2C%20S.%20Liu%2C%20M.-Y.%20Yang%2C%20X.%20J.%20Mocogan%3A%20Decomposing%20motion%20and%20content%20for%20video%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tulyakov%2C%20S.%20Liu%2C%20M.-Y.%20Yang%2C%20X.%20J.%20Mocogan%3A%20Decomposing%20motion%20and%20content%20for%20video%20generation%202018"
        },
        {
            "id": "Valmadre_et+al_2017_a",
            "entry": "Valmadre, J., L. Bertinetto, J. F. Henriques, A. Vedaldi, and P. H. S. Torr (2017). \u201cEnd-to-end representation learning for Correlation Filter based tracking\u201d. In: CVPR. arXiv: 1704.06036.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06036"
        },
        {
            "id": "Weber_et+al_2017_a",
            "entry": "Weber, T., S. Racani\u00e8re, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li, et al. (2017). \u201cImagination-augmented agents for deep reinforcement learning\u201d. In: NIPS.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weber%2C%20T.%20Racani%C3%A8re%2C%20S.%20Reichert%2C%20D.P.%20Buesing%2C%20L.%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Xiao_2016_a",
            "entry": "Xiao, F. and Y. Jae Lee (2016). \u201cTrack and segment: An iterative unsupervised approach for video object proposals\u201d. In: CVPR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20F.%20Lee%2C%20Y.Jae%20Track%20and%20segment%3A%20An%20iterative%20unsupervised%20approach%20for%20video%20object%20proposals%202016"
        },
        {
            "id": "Zaheer_et+al_2017_a",
            "entry": "Zaheer, M., S. Kottur, S. Ravanbakhsh, B. P\u00f3czos, R. R. Salakhutdinov, and A. J. Smola (2017). \u201cDeep Sets\u201d. In: NIPS.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20P%C3%B3czos%2C%20B.%20Deep%20Sets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaheer%2C%20M.%20Kottur%2C%20S.%20Ravanbakhsh%2C%20S.%20P%C3%B3czos%2C%20B.%20Deep%20Sets%202017"
        }
    ]
}
