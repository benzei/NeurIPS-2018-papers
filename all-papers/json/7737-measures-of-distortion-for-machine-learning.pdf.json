{
    "filename": "7737-measures-of-distortion-for-machine-learning.pdf",
    "metadata": {
        "title": "Measures of distortion for machine learning",
        "author": "Leena Chennuru Vankadara, Ulrike von Luxburg",
        "date": 2011,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7737-measures-of-distortion-for-machine-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply machine learning algorithms to analyze the data. The quality of such an embedding is typically described in terms of a distortion measure. In this paper, we show that many of the existing distortion measures behave in an undesired way, when considered from a machine learning point of view. We investigate desirable properties of distortion measures and formally prove that most of the existing measures fail to satisfy these properties. These theoretical findings are supported by simulations, which for example demonstrate that existing distortion measures are not robust to noise or outliers and cannot serve as good indicators for classification accuracy. As an alternative, we suggest a new measure of distortion, called \u03c3-distortion. We can show both in theory and in experiments that it satisfies all desirable properties and is a better candidate to evaluate distortion in the context of machine learning."
    },
    "keywords": [
        {
            "term": "multidimensional scaling",
            "url": "https://en.wikipedia.org/wiki/multidimensional_scaling"
        },
        {
            "term": "metric space",
            "url": "https://en.wikipedia.org/wiki/metric_space"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "euclidean space",
            "url": "https://en.wikipedia.org/wiki/euclidean_space"
        }
    ],
    "highlights": [
        "Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply out of the box machine learning algorithms to analyze the data",
        "The quality of such an embedding is described in terms of a distortion measure that summarizes how the distances between the embedded points deviate from the original distances",
        "Such distortion measures are sometimes evaluated in hindsight to evaluate the quality of an embedding, and sometimes used directly as objective functions in embedding algorithms, for example the stress functions that are commonly used in different variants of Multidimensional scaling (Cox and Cox, 2000)",
        "We identify a set of properties that are essential for any distortion measure",
        "In light of these properties, we propose a new measure of distortion that is designed towards machine learning applications: the \u03c3-distortion",
        "We prove in theory and through simulations that our new measure of distortion satisfies many of the properties that are important for machine learning, while all the other measures of distortion have serious drawbacks and fail to satisfy all of the properties"
    ],
    "key_statements": [
        "Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply out of the box machine learning algorithms to analyze the data",
        "The quality of such an embedding is described in terms of a distortion measure that summarizes how the distances between the embedded points deviate from the original distances",
        "Such distortion measures are sometimes evaluated in hindsight to evaluate the quality of an embedding, and sometimes used directly as objective functions in embedding algorithms, for example the stress functions that are commonly used in different variants of Multidimensional scaling (Cox and Cox, 2000)",
        "We identify a set of properties that are essential for any distortion measure",
        "In light of these properties, we propose a new measure of distortion that is designed towards machine learning applications: the \u03c3-distortion",
        "We prove in theory and through simulations that our new measure of distortion satisfies many of the properties that are important for machine learning, while all the other measures of distortion have serious drawbacks and fail to satisfy all of the properties",
        "In Section 3.3 we introduce a new measure of distortion that does not suffer from these issues, and demonstrate its practical behavior in Section 4.\n3 Properties of distortion measures",
        "We evaluate the behavior of various distortion measures by conducting experiments in two different settings: 1) Dimensionality reduction 2) A pipeline consisting of dimensionality reduction followed by classification"
    ],
    "summary": [
        "Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply out of the box machine learning algorithms to analyze the data.",
        "In machine learning, we are interested in consistency statements: given a sample of n points from some underlying space, we would like to measure the distortion of an embedding algorithm as n \u2192 \u221e.",
        "To study these issues more systematically, we will identify a set of properties that any measure of distortion should satisfy in the context of machine learning applications.",
        "We stipulate that a measure of distortion that is nice for machine learning should satisfy that if the underlying geometry of the metric space is \u201cnice\u201d, we can guarantee that there exists an embedding in constant dimension with constant distortion.",
        "(d) The distortion measures \u03a6wc, \u03a6avg, \u03a6navg, \u03a6lq , \u03a6 , \u03a6klocal fail to incorporate a probability distribution defined on the data space.",
        "In the case of average distortion, -distortion, and k-local distortion for fixed values of k, , it has been shown that any finite subset of a doubling metric space can be embedded into a constant dimensional Euclidean space with O(1) distortion (Abraham, Bartal, and Neiman, 2011; Abraham, Bartal, and Neiman, 2009).",
        "The \u03c3distortion (a) is invariant to scale and translations, (b) satisfies the property of monotonicity, (c) is robust to outliers in data and outliers in distances, and (d) incorporates a probability distribution into its evaluation.",
        "In addition to satisfying all of the aforementioned properties, the proofs of Abraham, Bartal, and Neiman, 2011 can be extended to show that one can embed any finite subset of a doubling metric space into constant dimensional Euclidean space with O(1) distortion.",
        "Given any finite sample Xn = {x1, x2, ..., xn} from an arbitrary metric space (X, dX ) and a probability distribution P on Xn, for any 1 \u2264 p < \u221e there exists an embedding f : Xn \u2192 lpD, where D = O with \u03c3-distortion = O(1).",
        "Given any finite sample Xn = {x1, x2, ..., xn} from a doubling metric space (X, dX ) and a probability distribution P on Xn, for any 1 \u2264 p < \u221e there exists an embedding f : Xn \u2192 lpD, where D = O(1) with \u03c3-distortion = O(1).",
        "Both in theory and experiments we can demonstrate that many of the existing measures of distortion behave in an undesired way: in simulations they show the wrong tradeoff with respect to the dimension of the original space, and they are not robust to noise or outliers, and cannot serve as a good indicator for classification accuracy."
    ],
    "headline": "We show that many of the existing distortion measures behave in an undesired way, when considered from a machine learning point of view",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] I. Abraham, Y. Bartal, J. Kleinberg, T-H. Chan, O. Neiman, K. Dhamdhere, A. Slivkins, and A. Gupta, \u201cMetric embeddings with relaxed guarantees\u201d, IEEE Symposium on Foundations of Computer Science, pp. 83\u2013100, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abraham%2C%20I.%20Bartal%2C%20Y.%20Kleinberg%2C%20J.%20Chan%2C%20T.-H.%20Metric%20embeddings%20with%20relaxed%20guarantees%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abraham%2C%20I.%20Bartal%2C%20Y.%20Kleinberg%2C%20J.%20Chan%2C%20T.-H.%20Metric%20embeddings%20with%20relaxed%20guarantees%202005"
        },
        {
            "id": "2",
            "entry": "[2] I. Abraham, Y. Bartal, and O. Neiman, \u201cAdvances in metric embedding theory\u201d, Advances in Mathematics, vol. 228, no. 6, pp. 3026\u20133126, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20Advances%20in%20metric%20embedding%20theory%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20Advances%20in%20metric%20embedding%20theory%202011"
        },
        {
            "id": "3",
            "entry": "[3] I. Abraham, Y. Bartal, and O. Neiman, \u201cEmbedding metric spaces in their intrinsic dimension\u201d, ACM-SIAM Symposium on Discrete Algorithms, pp. 363\u2013372, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20Embedding%20metric%20spaces%20in%20their%20intrinsic%20dimension%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20Embedding%20metric%20spaces%20in%20their%20intrinsic%20dimension%202008"
        },
        {
            "id": "4",
            "entry": "[4] I. Abraham, Y. Bartal, and O. Neiman, \u201cLocal embeddings of metric spaces\u201d, ACM Symposium on Theory of Computing, pp. 631\u2013640, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20Local%20embeddings%20of%20metric%20spaces%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20Local%20embeddings%20of%20metric%20spaces%202007"
        },
        {
            "id": "5",
            "entry": "[5] I. Abraham, Y. Bartal, and O. Neiman, \u201cOn low dimensional local embeddings\u201d, ACM-SIAM Symposium on Discrete Algorithms, pp. 875\u2013884, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20On%20low%20dimensional%20local%20embeddings%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abraham%2C%20I.%20Bartal%2C%20Y.%20Neiman%2C%20O.%20On%20low%20dimensional%20local%20embeddings%202009"
        },
        {
            "id": "6",
            "entry": "[6] Y. Bartal, L. Gottlieb, and O. Neiman, \u201cOn the impossibility of dimension reduction for doubling subsets of lp\u201d, SIAM Journal on Discrete Mathematics, vol. 29, no. 3, pp. 1207\u20131222, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartal%2C%20Y.%20Gottlieb%2C%20L.%20Neiman%2C%20O.%20On%20the%20impossibility%20of%20dimension%20reduction%20for%20doubling%20subsets%20of%20lp%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartal%2C%20Y.%20Gottlieb%2C%20L.%20Neiman%2C%20O.%20On%20the%20impossibility%20of%20dimension%20reduction%20for%20doubling%20subsets%20of%20lp%202015"
        },
        {
            "id": "7",
            "entry": "[7] J. Bourgain, \u201cOn Lipschitz embedding of finite metric spaces in Hilbert space\u201d, Israel Journal of Mathematics, vol. 52, no. 1, pp. 46\u201352, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bourgain%2C%20J.%20On%20Lipschitz%20embedding%20of%20finite%20metric%20spaces%20in%20Hilbert%20space%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bourgain%2C%20J.%20On%20Lipschitz%20embedding%20of%20finite%20metric%20spaces%20in%20Hilbert%20space%201985"
        },
        {
            "id": "8",
            "entry": "[8] T. Chan, A. Gupta, and K. Talwar, \u201cUltra-low-dimensional embeddings for doubling metrics\u201d, Journal of the ACM, vol. 57, no. 4 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20T.%20Gupta%2C%20A.%20Talwar%2C%20K.%20Ultra-low-dimensional%20embeddings%20for%20doubling%20metrics%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20T.%20Gupta%2C%20A.%20Talwar%2C%20K.%20Ultra-low-dimensional%20embeddings%20for%20doubling%20metrics%202010"
        },
        {
            "id": "9",
            "entry": "[9] T Cox and M. Cox, Multidimensional scaling, Chapman and hall/CRC, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cox%2C%20T.%20Cox%2C%20M.%20Multidimensional%20scaling%202000"
        },
        {
            "id": "10",
            "entry": "[10] A. Gupta, R. Krauthgamer, and J. Lee, \u201cBounded geometries, fractals, and low-distortion embeddings\u201d, Foundations of Computer Science, pp. 534\u2013543, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20A.%20Krauthgamer%2C%20R.%20Lee%2C%20J.%20Bounded%20geometries%2C%20fractals%2C%20and%20low-distortion%20embeddings%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20A.%20Krauthgamer%2C%20R.%20Lee%2C%20J.%20Bounded%20geometries%2C%20fractals%2C%20and%20low-distortion%20embeddings%202003"
        },
        {
            "id": "11",
            "entry": "[11] H. Hotelling, \u201cAnalysis of a complex of statistical variables into principal components.\u201d, Journal of Educational Psychology, vol. 24, no. 6, p. 417, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hotelling%2C%20H.%20Analysis%20of%20a%20complex%20of%20statistical%20variables%20into%20principal%20components.%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hotelling%2C%20H.%20Analysis%20of%20a%20complex%20of%20statistical%20variables%20into%20principal%20components.%201933"
        },
        {
            "id": "12",
            "entry": "[12] W. Johnson and J. Lindenstrauss, \u201cExtensions of Lipschitz mappings into a Hilbert space\u201d, Contemporary Mathematics, vol. 26, no. 189-206 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20W.%20Lindenstrauss%2C%20J.%20Extensions%20of%20Lipschitz%20mappings%20into%20a%20Hilbert%20space%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20W.%20Lindenstrauss%2C%20J.%20Extensions%20of%20Lipschitz%20mappings%20into%20a%20Hilbert%20space%201984"
        },
        {
            "id": "13",
            "entry": "[13] N. Lawrence, \u201cGaussian process latent variable models for visualisation of high dimensional data\u201d, Neural Information Processing Systems, pp. 329\u2013336, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lawrence%2C%20N.%20Gaussian%20process%20latent%20variable%20models%20for%20visualisation%20of%20high%20dimensional%20data%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lawrence%2C%20N.%20Gaussian%20process%20latent%20variable%20models%20for%20visualisation%20of%20high%20dimensional%20data%202004"
        },
        {
            "id": "14",
            "entry": "[14] L. Maaten and G. Hinton, \u201cVisualizing data using t-SNE\u201d, Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008"
        },
        {
            "id": "15",
            "entry": "[15] S. Semmes, \u201cBilipschitz embeddings of metric spaces into Euclidean spaces\u201d, Publicacions Matematicks, pp. 571\u2013653, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Semmes%2C%20S.%20Bilipschitz%20embeddings%20of%20metric%20spaces%20into%20Euclidean%20spaces%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Semmes%2C%20S.%20Bilipschitz%20embeddings%20of%20metric%20spaces%20into%20Euclidean%20spaces%201999"
        },
        {
            "id": "16",
            "entry": "[16] S. Semmes, \u201cOn the nonexistence of bilipschitz parameterizations and geometric problems about A-infinity weights\u201d, Revista Matematica Iberoamericana, vol. 12, no. 2, pp. 337\u2013410, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Semmes%2C%20S.%20On%20the%20nonexistence%20of%20bilipschitz%20parameterizations%20and%20geometric%20problems%20about%20A-infinity%20weights%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Semmes%2C%20S.%20On%20the%20nonexistence%20of%20bilipschitz%20parameterizations%20and%20geometric%20problems%20about%20A-infinity%20weights%201996"
        },
        {
            "id": "17",
            "entry": "[17] B. Shaw and T. Jebara, \u201cStructure preserving embedding\u201d, International Conference on Machine Learning, pp. 937\u2013944, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shaw%2C%20B.%20Jebara%2C%20T.%20Structure%20preserving%20embedding%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shaw%2C%20B.%20Jebara%2C%20T.%20Structure%20preserving%20embedding%202009"
        },
        {
            "id": "18",
            "entry": "[18] J. Tenenbaum, V. De Silva, and J. Langford, \u201cA global geometric framework for nonlinear dimensionality reduction\u201d, Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tenenbaum%2C%20J.%20Silva%2C%20V.De%20Langford%2C%20J.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tenenbaum%2C%20J.%20Silva%2C%20V.De%20Langford%2C%20J.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000"
        },
        {
            "id": "19",
            "entry": "[19] M. Tipping and C. Bishop, \u201cProbabilistic principal component analysis\u201d, Journal of the Royal Statistical Society: Series B, vol. 61, no. 3, pp. 611\u2013622, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tipping%2C%20M.%20Bishop%2C%20C.%20Probabilistic%20principal%20component%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tipping%2C%20M.%20Bishop%2C%20C.%20Probabilistic%20principal%20component%20analysis%201999"
        },
        {
            "id": "20",
            "entry": "[20] K. Weinberger and L. Saul, \u201cAn introduction to nonlinear dimensionality reduction by maximum variance unfolding\u201d, Association for the Advancement of Artificial Intelligence (AAAI), vol. 6, pp. 1683\u20131686, 2006. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20K.%20Saul%2C%20L.%20An%20introduction%20to%20nonlinear%20dimensionality%20reduction%20by%20maximum%20variance%20unfolding%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20K.%20Saul%2C%20L.%20An%20introduction%20to%20nonlinear%20dimensionality%20reduction%20by%20maximum%20variance%20unfolding%202006"
        }
    ]
}
