{
    "filename": "8121-iterative-value-aware-model-learning.pdf",
    "metadata": {
        "title": "Iterative Value-Aware Model Learning",
        "author": "Amir-massoud Farahmand",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8121-iterative-value-aware-model-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper introduces a model-based reinforcement learning (MBRL) framework that incorporates the underlying decision problem in learning the transition model of the environment. This is in contrast with conventional approaches to MBRL that learn the model of the environment, for example by finding the maximum likelihood estimate, without taking into account the decision problem. Value-Aware Model Learning (VAML) framework argues that this might not be a good idea, especially if the true model of the environment does not belong to the model class from which we are estimating the model. The original VAML framework, however, may result in an optimization problem that is difficult to solve. This paper introduces a new MBRL class of algorithms, called Iterative VAML, that benefits from the structure of how the planning is performed (i.e., through approximate value iteration) to devise a simpler optimization problem. The paper theoretically analyzes Iterative VAML and provides finite sample error upper bound guarantee for it."
    },
    "keywords": [
        {
            "term": "Mitsubishi Electric Research Laboratories",
            "url": "https://en.wikipedia.org/wiki/Mitsubishi_Electric_Research_Laboratories"
        },
        {
            "term": "Maximum Likelihood Estimator",
            "url": "https://en.wikipedia.org/wiki/Maximum_Likelihood_Estimator"
        },
        {
            "term": "decision problem",
            "url": "https://en.wikipedia.org/wiki/decision_problem"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "policy iteration",
            "url": "https://en.wikipedia.org/wiki/policy_iteration"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "error propagation",
            "url": "https://en.wikipedia.org/wiki/error_propagation"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        }
    ],
    "highlights": [
        "Aware Model Learning<br/><br/>To formalize the framework, let us consider a discounted Markov Decision Process (MDP) (X , A, R\u2217, P\u2217, \u03b3) [Szepesv\u00e1ri, 2010]",
        "We provide a finite-sample error upper bound guarantee for the model learning that shows the effect of the number of samples and complexity of the model on the error bound (Section 4.1)",
        "As Maximum Likelihood Estimator is the minimizer of the KL-divergence based on data, these upper bounds suggest that if we find a good Maximum Likelihood Estimator, we have an accurate Bellman operator",
        "It defines the loss function based on the actual sequence of value functions generated by an (Approximate) Value Iteration (AVI) type of Planner",
        "We have introduced IterVAML, a decision-aware model-based RL algorithm",
        "We proved finite sample error upper bound for the model learning procedure (Theorem 1) and a generic error propagation result for an approximate value iteration algorithm that uses an inaccurate model (Theorem 2)"
    ],
    "key_statements": [
        "Aware Model Learning<br/><br/>To formalize the framework, let us consider a discounted Markov Decision Process (MDP) (X , A, R\u2217, P\u2217, \u03b3) [Szepesv\u00e1ri, 2010]",
        "We provide a finite-sample error upper bound guarantee for the model learning that shows the effect of the number of samples and complexity of the model on the error bound (Section 4.1)",
        "As Maximum Likelihood Estimator is the minimizer of the KL-divergence based on data, these upper bounds suggest that if we find a good Maximum Likelihood Estimator, we have an accurate Bellman operator",
        "It defines the loss function based on the actual sequence of value functions generated by an (Approximate) Value Iteration (AVI) type of Planner",
        "We consider how errors at each iteration propagate throughout the iterations of IterVAML and affect the quality of the learned policy (Theorem 2 in Section 4.2)",
        "Assumption A4 (Value Function Space) The value function space F |A| is Vmax-bounded with\n1. This assumption requires that all the value functions Qk and Vk generated by performing a step of Approximate Value Iteration (12) and used in the model learning steps (11) are Vmax-bounded",
        "F + = maxa Q(\u00b7, a) : Q \u2208 F |A|. This result provides an upper bound on the quality of the learned policy \u03c0K, as a function of the number of samples and the properties of the model space M and the Markov Decision Process",
        "We have introduced IterVAML, a decision-aware model-based RL algorithm",
        "We proved finite sample error upper bound for the model learning procedure (Theorem 1) and a generic error propagation result for an approximate value iteration algorithm that uses an inaccurate model (Theorem 2)"
    ],
    "summary": [
        "Aware Model Learning<br/><br/>To formalize the framework, let us consider a discounted Markov Decision Process (MDP) (X , A, R\u2217, P\u2217, \u03b3) [Szepesv\u00e1ri, 2010].",
        "Any knowledge about the reward, value function, or policy is ignored in the conventional model learning approaches in MBRL.",
        "The formulation by <a class=\"ref-link\" id=\"cFarahmand_et+al_2017_a\" href=\"#rFarahmand_et+al_2017_a\">Farahmand et al [2017a</a>] incorporates the knowledge about the value function space in learning the model.",
        "It defines the loss function based on the actual sequence of value functions generated by an (Approximate) Value Iteration (AVI) type of Planner.",
        "We define the following \u201cidealized\u201d optimization problem: Given a model space M and the current approximation of the value function Qk, solve",
        "First we analyze one iteration of model learning) and provide an upper bound on the error in learning the model (Theorem 1 in Section 4.1).",
        "We consider how errors at each iteration propagate throughout the iterations of IterVAML and affect the quality of the learned policy (Theorem 2 in Section 4.2).",
        "This result upper bounds the error of Pin approximating the -state expectation of the value function.",
        "For the control case we consider the performance loss, which is defined as the difference between the value of following the greedy policy w.r.t. QK compared to the value of the optimal policy Q\u2217, weighted according to a user-defined probability distribution \u03c1 \u2208 M (X \u00d7 A), i.e., \u03c1(Q\u2217 \u2212 Q\u03c0K ).",
        "The definition of C(\u03c1, \u03bd) is similar to the second order discounted future state distribution concentration coefficient of <a class=\"ref-link\" id=\"cMunos_2007_a\" href=\"#rMunos_2007_a\">Munos [2007</a>], with the main difference being that it is defined for the expectation of the R-N derivative instead of its supremum.",
        "1. This assumption requires that all the value functions Qk and Vk generated by performing a step of AVI (12) and used in the model learning steps (11) are Vmax-bounded.",
        "This result provides an upper bound on the quality of the learned policy \u03c0K, as a function of the number of samples and the properties of the model space M and the MDP.",
        "The model approximation error term supV \u2208F+ infP\u2208M (Pz \u2212 Pz\u2217)V 2,\u03bd shows the interaction between the model M and the value function space F |A|.",
        "We proved finite sample error upper bound for the model learning procedure (Theorem 1) and a generic error propagation result for an approximate value iteration algorithm that uses an inaccurate model (Theorem 2).",
        "The consequence of these two results was Theorem 3, which provides an error upper bound guarantee on the quality of the outcome policy of IterVAML.",
        "Another direction is to investigate other approaches to decision-aware model-based RL algorithms"
    ],
    "headline": "This paper introduces a model-based reinforcement learning  framework that incorporates the underlying decision problem in learning the transition model of the environment",
    "reference_links": [
        {
            "id": "Andr_2008_a",
            "entry": "Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path. Machine Learning, 71:89\u2013129, 2008. 2, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andr%C3%A1s%20Antos%2C%20Csaba%20Szepesv%C3%A1ri%20Munos%2C%20R%C3%A9mi%20Learning%20near-optimal%20policies%20with%20Bellmanresidual%20minimization%20based%20fitted%20policy%20iteration%20and%20a%20single%20sample%20path%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andr%C3%A1s%20Antos%2C%20Csaba%20Szepesv%C3%A1ri%20Munos%2C%20R%C3%A9mi%20Learning%20near-optimal%20policies%20with%20Bellmanresidual%20minimization%20based%20fitted%20policy%20iteration%20and%20a%20single%20sample%20path%202008"
        },
        {
            "id": "Asadi_et+al_2018_a",
            "entry": "Kavosh Asadi, Evan Cater, Dipendra Misra, and Michael L. Littman. Equivalence between wasserstein and value-aware model-based reinforcement learning. In FAIM Workshop on Prediction and Generative Modeling in Reinforcement Learning, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asadi%2C%20Kavosh%20Cater%2C%20Evan%20Misra%2C%20Dipendra%20Littman%2C%20Michael%20L.%20Equivalence%20between%20wasserstein%20and%20value-aware%20model-based%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asadi%2C%20Kavosh%20Cater%2C%20Evan%20Misra%2C%20Dipendra%20Littman%2C%20Michael%20L.%20Equivalence%20between%20wasserstein%20and%20value-aware%20model-based%20reinforcement%20learning%202018"
        },
        {
            "id": "Bartlett_et+al_2005_a",
            "entry": "Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexities. The Annals of Statistics, 33(4):1497\u20131537, 2005. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20Rademacher%20complexities%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20Rademacher%20complexities%202005"
        },
        {
            "id": "Bertsekas_2011_a",
            "entry": "Dimitri P. Bertsekas. Approximate policy iteration: A survey and some new methods. Journal of Control Theory and Applications, 9(3):310\u2013335, 2011. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Approximate%20policy%20iteration%3A%20A%20survey%20and%20some%20new%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20Dimitri%20P.%20Approximate%20policy%20iteration%3A%20A%20survey%20and%20some%20new%20methods%202011"
        },
        {
            "id": "Doukhan_1994_a",
            "entry": "Paul Doukhan. Mixing: Properties and Examples, volume 85 of Lecture Notes in Statistics. SpringerVerlag, Berlin, 1994. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doukhan%2C%20Paul%20Mixing%3A%20Properties%20and%20Examples%2C%20volume%2085%20of%20Lecture%20Notes%20in%20Statistics%201994"
        },
        {
            "id": "Ernst_et+al_2005_a",
            "entry": "Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research (JMLR), 6:503\u2013556, 2005. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-based%20batch%20mode%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-based%20batch%20mode%20reinforcement%20learning%202005"
        },
        {
            "id": "Farahmand_2011_a",
            "entry": "Amir-massoud Farahmand. Regularization in Reinforcement Learning. PhD thesis, University of Alberta, 2011. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Regularization%20in%20Reinforcement%20Learning%202011"
        },
        {
            "id": "Farahmand_et+al_2012_a",
            "entry": "Amir-massoud Farahmand and Doina Precup. Value pursuit iteration. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems (NIPS - 25), pages 1349\u20131357. Curran Associates, Inc., 2012. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amirmassoud%20Farahmand%20and%20Doina%20Precup%20Value%20pursuit%20iteration%20In%20F%20Pereira%20CJC%20Burges%20L%20Bottou%20and%20KQ%20Weinberger%20editors%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%20%2025%20pages%2013491357%20Curran%20Associates%20Inc%202012%202%205",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amirmassoud%20Farahmand%20and%20Doina%20Precup%20Value%20pursuit%20iteration%20In%20F%20Pereira%20CJC%20Burges%20L%20Bottou%20and%20KQ%20Weinberger%20editors%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%20%2025%20pages%2013491357%20Curran%20Associates%20Inc%202012%202%205"
        },
        {
            "id": "Farahmand_2012_b",
            "entry": "Amir-massoud Farahmand and Csaba Szepesv\u00e1ri. Regularized least-squares regression: Learning from a \u03b2-mixing sequence. Journal of Statistical Planning and Inference, 142(2):493 \u2013 505, 2012. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Szepesv%C3%A1ri%2C%20Csaba%20Regularized%20least-squares%20regression%3A%20Learning%20from%20a%20%CE%B2-mixing%20sequence%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Szepesv%C3%A1ri%2C%20Csaba%20Regularized%20least-squares%20regression%3A%20Learning%20from%20a%20%CE%B2-mixing%20sequence%202012"
        },
        {
            "id": "Farahmand_et+al_2009_a",
            "entry": "Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv\u00e1ri, and Shie Mannor. Regularized fitted Q-iteration for planning in continuous-space Markovian Decision Problems. In Proceedings of American Control Conference (ACC), pages 725\u2013730, June 2009. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Ghavamzadeh%2C%20Mohammad%20Szepesv%C3%A1ri%2C%20Csaba%20Mannor%2C%20Shie%20Regularized%20fitted%20Q-iteration%20for%20planning%20in%20continuous-space%20Markovian%20Decision%20Problems%202009-06-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Ghavamzadeh%2C%20Mohammad%20Szepesv%C3%A1ri%2C%20Csaba%20Mannor%2C%20Shie%20Regularized%20fitted%20Q-iteration%20for%20planning%20in%20continuous-space%20Markovian%20Decision%20Problems%202009-06-02"
        },
        {
            "id": "Farahmand_et+al_2010_a",
            "entry": "Amir-massoud Farahmand, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Error propagation for approximate policy and value iteration. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems (NIPS - 23), pages 568\u2013576. 2010. 8, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Munos%2C%20R%C3%A9mi%20Szepesv%C3%A1ri%2C%20Csaba%20Error%20propagation%20for%20approximate%20policy%20and%20value%20iteration%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Munos%2C%20R%C3%A9mi%20Szepesv%C3%A1ri%2C%20Csaba%20Error%20propagation%20for%20approximate%20policy%20and%20value%20iteration%202010"
        },
        {
            "id": "Farahmand_et+al_2016_a",
            "entry": "Amir-massoud Farahmand, Andr\u00e9 M.S. Barreto, and Daniel N. Nikovski. Value-aware loss function for model learning in reinforcement learning. In 13th European Workshop on Reinforcement Learning (EWRL), December 2016a. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Barreto%2C%20Andr%C3%A9%20M.S.%20Nikovski%2C%20Daniel%20N.%20Value-aware%20loss%20function%20for%20model%20learning%20in%20reinforcement%20learning%202016-12-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Barreto%2C%20Andr%C3%A9%20M.S.%20Nikovski%2C%20Daniel%20N.%20Value-aware%20loss%20function%20for%20model%20learning%20in%20reinforcement%20learning%202016-12-01"
        },
        {
            "id": "Farahmand_et+al_2016_b",
            "entry": "Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv\u00e1ri, and Shie Mannor. Regularized policy iteration with nonparametric function spaces. Journal of Machine Learning Research (JMLR), 17(139):1\u201366, 2016b. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Ghavamzadeh%2C%20Mohammad%20Szepesv%C3%A1ri%2C%20Csaba%20Mannor%2C%20Shie%20Regularized%20policy%20iteration%20with%20nonparametric%20function%20spaces%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Ghavamzadeh%2C%20Mohammad%20Szepesv%C3%A1ri%2C%20Csaba%20Mannor%2C%20Shie%20Regularized%20policy%20iteration%20with%20nonparametric%20function%20spaces%202016"
        },
        {
            "id": "Farahmand_et+al_2016_c",
            "entry": "Amir-massoud Farahmand, Daniel N. Nikovski, Yuji Igarashi, and Hiroki Konaka. Truncated approximate dynamic programming with task-dependent terminal value. In AAAI Conference on Artificial Intelligence, February 2016c. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Nikovski%2C%20Daniel%20N.%20Igarashi%2C%20Yuji%20Konaka%2C%20Hiroki%20Truncated%20approximate%20dynamic%20programming%20with%20task-dependent%20terminal%20value%202016-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Nikovski%2C%20Daniel%20N.%20Igarashi%2C%20Yuji%20Konaka%2C%20Hiroki%20Truncated%20approximate%20dynamic%20programming%20with%20task-dependent%20terminal%20value%202016-02"
        },
        {
            "id": "Farahmand_et+al_2017_a",
            "entry": "Amir-massoud Farahmand, Andr\u00e9 M.S. Barreto, and Daniel N. Nikovski. Value-aware loss function for model-based reinforcement learning. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1486\u20131494, April 2017a. 1, 2, 3, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Barreto%2C%20Andr%C3%A9%20M.S.%20Nikovski%2C%20Daniel%20N.%20Value-aware%20loss%20function%20for%20model-based%20reinforcement%20learning%202017-04-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Barreto%2C%20Andr%C3%A9%20M.S.%20Nikovski%2C%20Daniel%20N.%20Value-aware%20loss%20function%20for%20model-based%20reinforcement%20learning%202017-04-01"
        },
        {
            "id": "Farahmand_et+al_2017_b",
            "entry": "Amir-massoud Farahmand, Saleh Nabi, and Daniel N. Nikovski. Deep reinforcement learning for partial differential equation control. In American Control Conference (ACC), 2017b. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-massoud%20Nabi%2C%20Saleh%20Nikovski%2C%20Daniel%20N.%20Deep%20reinforcement%20learning%20for%20partial%20differential%20equation%20control%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-massoud%20Nabi%2C%20Saleh%20Nikovski%2C%20Daniel%20N.%20Deep%20reinforcement%20learning%20for%20partial%20differential%20equation%20control%202017"
        },
        {
            "id": "Farquhar_et+al_2018_a",
            "entry": "Gregory Farquhar, Tim Rocktaeschel, Maximilian Igl, and Shimon Whiteson. TreeQN and ATreec: Differentiable tree planning for deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2018. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farquhar%2C%20Gregory%20Rocktaeschel%2C%20Tim%20Igl%2C%20Maximilian%20Whiteson%2C%20Shimon%20TreeQN%20and%20ATreec%3A%20Differentiable%20tree%20planning%20for%20deep%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farquhar%2C%20Gregory%20Rocktaeschel%2C%20Tim%20Igl%2C%20Maximilian%20Whiteson%2C%20Shimon%20TreeQN%20and%20ATreec%3A%20Differentiable%20tree%20planning%20for%20deep%20reinforcement%20learning%202018"
        },
        {
            "id": "Gin_2015_a",
            "entry": "Evarist Gin\u00e9 and Richard Nickl. Mathematical Foundations of Infinite-Dimensional Statistical Models. Cambridge University Press, 2015. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gin%C3%A9%2C%20Evarist%20Nickl%2C%20Richard%20Mathematical%20Foundations%20of%20Infinite-Dimensional%20Statistical%20Models%202015"
        },
        {
            "id": "Gordon_1995_a",
            "entry": "Geoffrey Gordon. Stable function approximation in dynamic programming. In International Conference on Machine Learning (ICML), 1995. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordon%2C%20Geoffrey%20Stable%20function%20approximation%20in%20dynamic%20programming%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordon%2C%20Geoffrey%20Stable%20function%20approximation%20in%20dynamic%20programming%201995"
        },
        {
            "id": "Gyoerfi_et+al_2002_a",
            "entry": "L\u00e1szl\u00f3 Gy\u00f6rfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of Nonparametric Regression. Springer Verlag, New York, 2002. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gy%C3%B6rfi%2C%20L%C3%A1szl%C3%B3%20Kohler%2C%20Michael%20Krzyzak%2C%20Adam%20Walk%2C%20Harro%20A%20Distribution-Free%20Theory%20of%20Nonparametric%20Regression%202002"
        },
        {
            "id": "Huang_et+al_2015_a",
            "entry": "De-An Huang, Amir-massoud Farahmand, Kris M Kitani, and J. Andrew Bagnell. Approximate MaxEnt inverse optimal control and its application for mental simulation of human interactions. In AAAI Conference on Artificial Intelligence, January 2015. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20De-An%20Farahmand%2C%20Amir-massoud%20Kitani%2C%20Kris%20M.%20Bagnell%2C%20J.Andrew%20Approximate%20MaxEnt%20inverse%20optimal%20control%20and%20its%20application%20for%20mental%20simulation%20of%20human%20interactions%202015-01-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20De-An%20Farahmand%2C%20Amir-massoud%20Kitani%2C%20Kris%20M.%20Bagnell%2C%20J.Andrew%20Approximate%20MaxEnt%20inverse%20optimal%20control%20and%20its%20application%20for%20mental%20simulation%20of%20human%20interactions%202015-01-08"
        },
        {
            "id": "Joseph_et+al_2013_a",
            "entry": "Joshua Joseph, Alborz Geramifard, John W Roberts, Jonathan P How, and Nicholas Roy. Reinforcement learning with misspecified model classes. In Proceedings of IEEE International Conference on Robotics and Automation (ICRA), pages 939\u2013946. IEEE, 2013. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20Joshua%20Geramifard%2C%20Alborz%20Roberts%2C%20John%20W.%20How%2C%20Jonathan%20P.%20Reinforcement%20learning%20with%20misspecified%20model%20classes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joseph%2C%20Joshua%20Geramifard%2C%20Alborz%20Roberts%2C%20John%20W.%20How%2C%20Jonathan%20P.%20Reinforcement%20learning%20with%20misspecified%20model%20classes%202013"
        },
        {
            "id": "Lagoudakis_2003_a",
            "entry": "Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research (JMLR), 4:1107\u20131149, 2003. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lagoudakis%2C%20Michail%20G.%20Parr%2C%20Ronald%20Least-squares%20policy%20iteration%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lagoudakis%2C%20Michail%20G.%20Parr%2C%20Ronald%20Least-squares%20policy%20iteration%202003"
        },
        {
            "id": "Alessandro_2012_a",
            "entry": "Alessandro Lazaric, Mohammad Ghavamzadeh, and R\u00e9mi Munos. Finite-sample analysis of leastsquares policy iteration. Journal of Machine Learning Research (JMLR), 13:3041\u20133074, October 2012. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alessandro%20Lazaric%2C%20Mohammad%20Ghavamzadeh%20Munos%2C%20R%C3%A9mi%20Finite-sample%20analysis%20of%20leastsquares%20policy%20iteration%202012-10-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alessandro%20Lazaric%2C%20Mohammad%20Ghavamzadeh%20Munos%2C%20R%C3%A9mi%20Finite-sample%20analysis%20of%20leastsquares%20policy%20iteration%202012-10-02"
        },
        {
            "id": "Lee_et+al_1998_a",
            "entry": "Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning with squared loss. IEEE Transactions on Information Theory, 44(5):1974\u20131980, 1998. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Wee%20Sun%20Bartlett%2C%20Peter%20L.%20Williamson%2C%20Robert%20C.%20The%20importance%20of%20convexity%20in%20learning%20with%20squared%20loss%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Wee%20Sun%20Bartlett%2C%20Peter%20L.%20Williamson%2C%20Robert%20C.%20The%20importance%20of%20convexity%20in%20learning%20with%20squared%20loss%201998"
        },
        {
            "id": "Lee_et+al_2008_a",
            "entry": "Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. Correction to the importance of convexity in learning with squared loss. IEEE Transactions on Information Theory, 54(9):4395, 2008. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Wee%20Sun%20Bartlett%2C%20Peter%20L.%20Williamson%2C%20Robert%20C.%20Correction%20to%20the%20importance%20of%20convexity%20in%20learning%20with%20squared%20loss%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Wee%20Sun%20Bartlett%2C%20Peter%20L.%20Williamson%2C%20Robert%20C.%20Correction%20to%20the%20importance%20of%20convexity%20in%20learning%20with%20squared%20loss%202008"
        },
        {
            "id": "Mann_et+al_2015_a",
            "entry": "Timothy A. Mann, Shie Mannor, and Doina Precup. Approximate value iteration with temporally extended actions. Journal of Artificial Intelligence Research (JAIR), 53:375\u2013438, 2015. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mann%2C%20Timothy%20A.%20Mannor%2C%20Shie%20Precup%2C%20Doina%20Approximate%20value%20iteration%20with%20temporally%20extended%20actions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mann%2C%20Timothy%20A.%20Mannor%2C%20Shie%20Precup%2C%20Doina%20Approximate%20value%20iteration%20with%20temporally%20extended%20actions%202015"
        },
        {
            "id": "Meir_2000_a",
            "entry": "Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning, 39(1):5\u201334, 2000. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meir%2C%20Ron%20Nonparametric%20time%20series%20prediction%20through%20adaptive%20model%20selection%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meir%2C%20Ron%20Nonparametric%20time%20series%20prediction%20through%20adaptive%20model%20selection%202000"
        },
        {
            "id": "Mendelson_2008_a",
            "entry": "Shahar Mendelson. Lower bounds for the empirical risk minimization algorithm. IEEE Transactions on Information Theory, 54(8):3797 \u2013 3803, August 2008. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mendelson%2C%20Shahar%20Lower%20bounds%20for%20the%20empirical%20risk%20minimization%20algorithm%202008-08-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mendelson%2C%20Shahar%20Lower%20bounds%20for%20the%20empirical%20risk%20minimization%20algorithm%202008-08-07"
        },
        {
            "id": "Mnih_et+al_0000_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 02 2015. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning"
        },
        {
            "id": "Mohri_2009_a",
            "entry": "Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In Advances in Neural Information Processing Systems 21, pages 1097\u20131104. Curran Associates, Inc., 2009. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Rademacher%20complexity%20bounds%20for%20non-i.i.d.%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Rademacher%20complexity%20bounds%20for%20non-i.i.d.%20processes%202009"
        },
        {
            "id": "Mohri_2010_a",
            "entry": "Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary \u03c6-mixing and \u03b2-mixing processes. Journal of Machine Learning Research (JMLR), 11:789\u2013814, 2010. ISSN 1532-4435. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Stability%20bounds%20for%20stationary%20%CF%86-mixing%20and%20%CE%B2-mixing%20processes%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Stability%20bounds%20for%20stationary%20%CF%86-mixing%20and%20%CE%B2-mixing%20processes%202010"
        },
        {
            "id": "Munos_2007_a",
            "entry": "R\u00e9mi Munos. Performance bounds in Lp norm for approximate value iteration. SIAM Journal on Control and Optimization, pages 541\u2013561, 2007. 8, 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R%C3%A9mi%20Performance%20bounds%20in%20Lp%20norm%20for%20approximate%20value%20iteration%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R%C3%A9mi%20Performance%20bounds%20in%20Lp%20norm%20for%20approximate%20value%20iteration%202007"
        },
        {
            "id": "Munos_2008_a",
            "entry": "R\u00e9mi Munos and Csaba Szepesv\u00e1ri. Finite-time bounds for fitted value iteration. Journal of Machine Learning Research (JMLR), 9:815\u2013857, 2008. 2, 5, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R%C3%A9mi%20Szepesv%C3%A1ri%2C%20Csaba%20Finite-time%20bounds%20for%20fitted%20value%20iteration%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R%C3%A9mi%20Szepesv%C3%A1ri%2C%20Csaba%20Finite-time%20bounds%20for%20fitted%20value%20iteration%202008"
        },
        {
            "id": "Oh_et+al_2017_a",
            "entry": "Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information Processing Systems (NIPS - 30), pages 6118\u20136128. Curran Associates, Inc., 2017. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Junhyuk%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Value%20prediction%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Junhyuk%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Value%20prediction%20network%202017"
        },
        {
            "id": "Racani_et+al_2017_a",
            "entry": "S\u00e9bastien Racani\u00e8re, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri\u00e0 Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems (NIPS - 30), pages 5690\u20135701. Curran Associates, Inc., 2017. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Racani%C3%A8re%2C%20S%C3%A9bastien%20Weber%2C%20Theophane%20Reichert%2C%20David%20Buesing%2C%20Lars%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Racani%C3%A8re%2C%20S%C3%A9bastien%20Weber%2C%20Theophane%20Reichert%2C%20David%20Buesing%2C%20Lars%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Scherrer_et+al_2012_a",
            "entry": "Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, and Matthieu Geist. Approximate modified policy iteration. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012. 2, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scherrer%2C%20Bruno%20Ghavamzadeh%2C%20Mohammad%20Gabillon%2C%20Victor%20Geist%2C%20Matthieu%20Approximate%20modified%20policy%20iteration%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scherrer%2C%20Bruno%20Ghavamzadeh%2C%20Mohammad%20Gabillon%2C%20Victor%20Geist%2C%20Matthieu%20Approximate%20modified%20policy%20iteration%202012"
        },
        {
            "id": "Silver_et+al_2017_a",
            "entry": "David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andr\u00e9 M.S. Barreto, and Thomas Degris. The predictron: End-to-end learning and planning. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 3191\u20133199, 2017. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20van%20Hasselt%2C%20Hado%20Hessel%2C%20Matteo%20Schaul%2C%20Tom%20The%20predictron%3A%20End-to-end%20learning%20and%20planning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20van%20Hasselt%2C%20Hado%20Hessel%2C%20Matteo%20Schaul%2C%20Tom%20The%20predictron%3A%20End-to-end%20learning%20and%20planning%202017"
        },
        {
            "id": "Steinwart_2008_a",
            "entry": "Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20Vector%20Machines%202008"
        },
        {
            "id": "Neural_2009_a",
            "entry": "Neural Information Processing Systems (NIPS - 22), pages 1768\u20131776. Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neural%20Information%20Processing%20Systems%20NIPS%20%2022%20pages%2017681776%20Curran%20Associates%20Inc%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neural%20Information%20Processing%20Systems%20NIPS%20%2022%20pages%2017681776%20Curran%20Associates%20Inc%202009"
        },
        {
            "id": "6",
            "entry": "6 Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the 7th International Conference on Machine Learning (ICML), 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Integrated%20architectures%20for%20learning%2C%20planning%2C%20and%20reacting%20based%20on%20approximating%20dynamic%20programming%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Integrated%20architectures%20for%20learning%2C%20planning%2C%20and%20reacting%20based%20on%20approximating%20dynamic%20programming%201990"
        },
        {
            "id": "5",
            "entry": "5 Csaba Szepesv\u00e1ri. Algorithms for Reinforcement Learning. Morgan Claypool Publishers, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesv%C3%A1ri%2C%20Csaba%20Algorithms%20for%20Reinforcement%20Learning%202010"
        },
        {
            "id": "2",
            "entry": "2 Csaba Szepesv\u00e1ri and William D. Smart. Interpolation-based Q-learning. In Proceedings of the twenty-first International Conference on Machine learning (ICML), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szepesv%C3%A1ri%2C%20Csaba%20Smart%2C%20William%20D.%20Interpolation-based%20Q-learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szepesv%C3%A1ri%2C%20Csaba%20Smart%2C%20William%20D.%20Interpolation-based%20Q-learning%202004"
        },
        {
            "id": "2",
            "entry": "2 Samuele Tosatto, Matteo Pirotta, Carlo D\u2019Eramo, and Marcello Restelli. Boosted fitted q-iteration. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 3434\u20133443, August 2017. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samuele%20Tosatto%2C%20Matteo%20Pirotta%2C%20Carlo%20D%E2%80%99Eramo%20Restelli%2C%20Marcello%20Boosted%20fitted%20q-iteration%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samuele%20Tosatto%2C%20Matteo%20Pirotta%2C%20Carlo%20D%E2%80%99Eramo%20Restelli%2C%20Marcello%20Boosted%20fitted%20q-iteration%202017-08"
        },
        {
            "id": "Van_2000_a",
            "entry": "Sara A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20de%20Geer%2C%20Sara%20A.%20Empirical%20Processes%20in%20M-Estimation%202000"
        },
        {
            "id": "7",
            "entry": "7 Yuhong Yang and Andrew R. Barron. Information-theoretic determination of minimax rates of convergence. The Annals of Statistics, 27(5):1564\u20131599, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yuhong%20Barron%2C%20Andrew%20R.%20Information-theoretic%20determination%20of%20minimax%20rates%20of%20convergence%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yuhong%20Barron%2C%20Andrew%20R.%20Information-theoretic%20determination%20of%20minimax%20rates%20of%20convergence%201999"
        },
        {
            "id": "7",
            "entry": "7 Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability, 22(1):94\u2013116, January 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Bin%20Rates%20of%20convergence%20for%20empirical%20processes%20of%20stationary%20mixing%20sequences%201994-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Bin%20Rates%20of%20convergence%20for%20empirical%20processes%20of%20stationary%20mixing%20sequences%201994-01"
        },
        {
            "id": "6",
            "entry": "6 Ding-Xuan Zhou. Capacity of reproducing kernel spaces in learning theory. IEEE Transactions on Information Theory, 49:1743\u20131752, 2003. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Ding-Xuan%20Capacity%20of%20reproducing%20kernel%20spaces%20in%20learning%20theory%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Ding-Xuan%20Capacity%20of%20reproducing%20kernel%20spaces%20in%20learning%20theory%202003"
        }
    ]
}
