{
    "filename": "8264-learning-beam-search-policies-via-imitation-learning.pdf",
    "metadata": {
        "date": 2018,
        "title": "Learning Beam Search Policies via Imitation Learning",
        "author": "Renato Negrinho1, Matthew R. Gormley1, Geoffrey J. Gordon1,2 1Machine Learning Department, Carnegie Mellon University; 2Microsoft Research",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8264-learning-beam-search-policies-via-imitation-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Beam search is widely used for approximate decoding in structured prediction problems. Models often use a beam at test time but ignore its existence at train time, and therefore do not explicitly learn how to use the beam. We develop an unifying meta-algorithm for learning beam search policies using imitation learning. In our setting, the beam is part of the model, and not just an artifact of approximate decoding. Our meta-algorithm captures existing learning algorithms and suggests new ones. It also lets us show novel no-regret guarantees for learning beam search policies."
    },
    "keywords": [
        {
            "term": "structured prediction",
            "url": "https://en.wikipedia.org/wiki/structured_prediction"
        }
    ],
    "highlights": [
        "Beam search is the dominant method for approximate decoding in structured prediction tasks such as machine translation [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], speech recognition [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], image captioning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and syntactic parsing [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "Most models that use beam search at test time ignore the beam at train time and instead are learned via methods like likelihood maximization",
        "In Theorem 1, we prove no-regret guarantees for the case where the no-regret online algorithm is presented with explicit expectations for the loss incurred by a beam search policy",
        "We provide regret guarantees for both new and existing algorithms for learning beam search policies",
        "Our work is the first to provide a beam-aware algorithm with no-regret guarantees"
    ],
    "key_statements": [
        "Beam search is the dominant method for approximate decoding in structured prediction tasks such as machine translation [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], speech recognition [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], image captioning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and syntactic parsing [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "Most models that use beam search at test time ignore the beam at train time and instead are learned via methods like likelihood maximization",
        "In Theorem 1, we prove no-regret guarantees for the case where the no-regret online algorithm is presented with explicit expectations for the loss incurred by a beam search policy",
        "We provide regret guarantees for both new and existing algorithms for learning beam search policies",
        "Our work is the first to provide a beam-aware algorithm with no-regret guarantees"
    ],
    "summary": [
        "Beam search is the dominant method for approximate decoding in structured prediction tasks such as machine translation [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], speech recognition [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], image captioning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and syntactic parsing [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>].",
        "We are the first to prove no-regret guarantees for an algorithm to learn beam search policies.",
        "Within our imitation learning framework, we can use this ability to compute oracle completion costs for the neighbors of the elements of a beam at train time to induce an oracle that allows us to continue collecting supervision after the best hypothesis falls out of the beam.",
        "Our meta-algorithm captures much of the existing literature on beam-aware methods (e.g., [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]), allowing a clearer understanding of and comparison to existing approaches, for example, by emphasizing that they arise from specific choices of training loss function and data collection strategy, and by proving novel regret guarantees for them.",
        "Instantiating our meta-algorithm requires choosing both a surrogate training loss function (Section 4.1) and a data collection strategy (Section 4.2).",
        "While certain losses do not appear beam-aware and Equation (12)), it is important to keep in mind that all losses are collected by executing a policy on the beam search space Gk. Given a beam b \u2208 Vk, the score vector s \u2208 Rn and cost vector c \u2208 Rn are defined for the elements of Ab. The losses incurred depend on the specific beams visited.",
        "Our meta-algorithm requires choosing a train time policy \u03c0 : Vk \u2192 \u2206(Vk) to traverse the beam search space Gk to collect supervision.",
        "We state regret guarantees for learning beam search policies using the continue, reset, or stop data collection strategies.",
        "In Theorem 1, we prove no-regret guarantees for the case where the no-regret online algorithm is presented with explicit expectations for the loss incurred by a beam search policy.",
        "In Theorem 2, we upper bound the expected cost incurred by a beam search policy as a function of its expected loss.",
        "Our regret guarantees for beam-aware algorithms with different data collection strategies are novel.",
        "Let the iterates be chosen by a no-regret online learning algorithm, based on the sequence of losses t = \u02c6(\u00b7, \u03b8t) : \u0398 \u2192 R, for t \u2208 [m], we have P",
        "If the probability of stopping or resetting does not go completely to zero, it is still possible to provide regret guarantees for the performance of this algorithm but with a term that does not vanish with increasing m.",
        "We provide regret guarantees for both new and existing algorithms for learning beam search policies.",
        "We recover existing algorithms in the literature through specific choices for the loss function and data collection strategy.",
        "Our work is the first to provide a beam-aware algorithm with no-regret guarantees"
    ],
    "headline": "We develop an unifying meta-algorithm for learning beam search policies using imitation learning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "2",
            "entry": "[2] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. ICASSP, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "3",
            "entry": "[3] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Toshev%2C%20Alexander%20Bengio%2C%20Samy%20Erhan%2C%20Dumitru%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Toshev%2C%20Alexander%20Bengio%2C%20Samy%20Erhan%2C%20Dumitru%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015"
        },
        {
            "id": "4",
            "entry": "[4] David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. Structured training for neural network transition-based parsing. ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weiss%2C%20David%20Alberti%2C%20Chris%20Collins%2C%20Michael%20Petrov%2C%20Slav%20Structured%20training%20for%20neural%20network%20transition-based%20parsing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weiss%2C%20David%20Alberti%2C%20Chris%20Collins%2C%20Michael%20Petrov%2C%20Slav%20Structured%20training%20for%20neural%20network%20transition-based%20parsing%202015"
        },
        {
            "id": "5",
            "entry": "[5] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "6",
            "entry": "[6] Michael Collins and Brian Roark. Incremental parsing with the perceptron algorithm. ACL, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collins%2C%20Michael%20Roark%2C%20Brian%20Incremental%20parsing%20with%20the%20perceptron%20algorithm%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collins%2C%20Michael%20Roark%2C%20Brian%20Incremental%20parsing%20with%20the%20perceptron%20algorithm%202004"
        },
        {
            "id": "7",
            "entry": "[7] Hal Daum\u00e9 and Daniel Marcu. Learning as search optimization: Approximate large margin methods for structured prediction. ICML, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daum%C3%A9%2C%20Hal%20Marcu%2C%20Daniel%20Learning%20as%20search%20optimization%3A%20Approximate%20large%20margin%20methods%20for%20structured%20prediction%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daum%C3%A9%2C%20Hal%20Marcu%2C%20Daniel%20Learning%20as%20search%20optimization%3A%20Approximate%20large%20margin%20methods%20for%20structured%20prediction%202005"
        },
        {
            "id": "8",
            "entry": "[8] Liang Huang, Suphan Fayong, and Yang Guo. Structured perceptron with inexact search. NAACL, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Liang%20Fayong%2C%20Suphan%20Guo%2C%20Yang%20Structured%20perceptron%20with%20inexact%20search%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Liang%20Fayong%2C%20Suphan%20Guo%2C%20Yang%20Structured%20perceptron%20with%20inexact%20search%202012"
        },
        {
            "id": "9",
            "entry": "[9] Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. Globally normalized transition-based neural networks. ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andor%2C%20Daniel%20Alberti%2C%20Chris%20Weiss%2C%20David%20Severyn%2C%20Aliaksei%20Globally%20normalized%20transition-based%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andor%2C%20Daniel%20Alberti%2C%20Chris%20Weiss%2C%20David%20Severyn%2C%20Aliaksei%20Globally%20normalized%20transition-based%20neural%20networks%202016"
        },
        {
            "id": "10",
            "entry": "[10] Yuehua Xu and Alan Fern. On learning linear ranking functions for beam search. ICML, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Yuehua%20Fern%2C%20Alan%20On%20learning%20linear%20ranking%20functions%20for%20beam%20search%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Yuehua%20Fern%2C%20Alan%20On%20learning%20linear%20ranking%20functions%20for%20beam%20search%202007"
        },
        {
            "id": "11",
            "entry": "[11] Sam Wiseman and Alexander Rush. Sequence-to-sequence learning as beam-search optimization. ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20Sequence-to-sequence%20learning%20as%20beam-search%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20Sequence-to-sequence%20learning%20as%20beam-search%20optimization%202016"
        },
        {
            "id": "12",
            "entry": "[12] Kartik Goyal, Graham Neubig, Chris Dyer, and Taylor Berg-Kirkpatrick. A continuous relaxation of beam search for end-to-end training of neural sequence models. AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Kartik%20Neubig%2C%20Graham%20Dyer%2C%20Chris%20Berg-Kirkpatrick%2C%20Taylor%20A%20continuous%20relaxation%20of%20beam%20search%20for%20end-to-end%20training%20of%20neural%20sequence%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Kartik%20Neubig%2C%20Graham%20Dyer%2C%20Chris%20Berg-Kirkpatrick%2C%20Taylor%20A%20continuous%20relaxation%20of%20beam%20search%20for%20end-to-end%20training%20of%20neural%20sequence%20models%202018"
        },
        {
            "id": "13",
            "entry": "[13] Hal Daum\u00e9, John Langford, and Daniel Marcu. Search-based structured prediction, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daum%C3%A9%2C%20Hal%20Langford%2C%20John%20Marcu%2C%20Daniel%20Search-based%20structured%20prediction%202009"
        },
        {
            "id": "14",
            "entry": "[14] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015"
        },
        {
            "id": "15",
            "entry": "[15] St\u00e9phane Ross and Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.5979"
        },
        {
            "id": "16",
            "entry": "[16] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum\u00e9, and John Langford. Learning to search better than your teacher. ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Kai-Wei%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Daum%C3%A9%2C%20Hal%20Learning%20to%20search%20better%20than%20your%20teacher%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Kai-Wei%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Daum%C3%A9%2C%20Hal%20Learning%20to%20search%20better%20than%20your%20teacher%202015"
        },
        {
            "id": "17",
            "entry": "[17] Alina Beygelzimer, John Langford, and Bianca Zadrozny. Machine learning techniques\u2014reductions between prediction quality metrics. Performance Modeling and Engineering, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beygelzimer%2C%20Alina%20Langford%2C%20John%20Zadrozny%2C%20Bianca%20Machine%20learning%20techniques%E2%80%94reductions%20between%20prediction%20quality%20metrics%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beygelzimer%2C%20Alina%20Langford%2C%20John%20Zadrozny%2C%20Bianca%20Machine%20learning%20techniques%E2%80%94reductions%20between%20prediction%20quality%20metrics%202008"
        },
        {
            "id": "18",
            "entry": "[18] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "19",
            "entry": "[19] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. ACL, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "20",
            "entry": "[20] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. ICML, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        },
        {
            "id": "21",
            "entry": "[21] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalai%2C%20Adam%20Vempala%2C%20Santosh%20Efficient%20algorithms%20for%20online%20decision%20problems%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalai%2C%20Adam%20Vempala%2C%20Santosh%20Efficient%20algorithms%20for%20online%20decision%20problems%202005"
        },
        {
            "id": "22",
            "entry": "[22] Elad Hazan. Introduction to online convex optimization. Foundations and Trends R in Optimization, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016"
        },
        {
            "id": "23",
            "entry": "[23] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicolo%20Conconi%2C%20Alex%20Gentile%2C%20Claudio%20On%20the%20generalization%20ability%20of%20on-line%20learning%20algorithms%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20Nicolo%20Conconi%2C%20Alex%20Gentile%2C%20Claudio%20On%20the%20generalization%20ability%20of%20on-line%20learning%20algorithms%202004"
        },
        {
            "id": "24",
            "entry": "[24] Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov networks. NIPS, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taskar%2C%20Ben%20Guestrin%2C%20Carlos%20Koller%2C%20Daphne%20Max-margin%20Markov%20networks%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taskar%2C%20Ben%20Guestrin%2C%20Carlos%20Koller%2C%20Daphne%20Max-margin%20Markov%20networks%202003"
        },
        {
            "id": "25",
            "entry": "[25] Kevin Gimpel and Noah Smith. Softmax-margin CRFs: Training log-linear models with cost functions. In ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gimpel%2C%20Kevin%20Smith%2C%20Noah%20Softmax-margin%20CRFs%3A%20Training%20log-linear%20models%20with%20cost%20functions",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gimpel%2C%20Kevin%20Smith%2C%20Noah%20Softmax-margin%20CRFs%3A%20Training%20log-linear%20models%20with%20cost%20functions"
        }
    ]
}
