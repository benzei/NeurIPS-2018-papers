{
    "filename": "7542-analytic-solution-and-stationary-phase-approximation-for-the-bayesian-lasso-and-elastic-net.pdf",
    "metadata": {
        "title": "Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net",
        "author": "Tom Michoel",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7542-analytic-solution-and-stationary-phase-approximation-for-the-bayesian-lasso-and-elastic-net.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve regression shrinkage and variable selection, allowing the inference of robust models from large data sets. However, there has been limited success in deriving estimates for the full posterior distribution of regression coefficients in these models, due to a need to evaluate analytically intractable partition function integrals. Here, the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over \u201cregression frequencies\u201d. This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients, and allows for Bayesian inference of much higher dimensional models than previously possible."
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "fourier transform",
            "url": "https://en.wikipedia.org/wiki/fourier_transform"
        },
        {
            "term": "regression coefficient",
            "url": "https://en.wikipedia.org/wiki/regression_coefficient"
        },
        {
            "term": "elastic net",
            "url": "https://en.wikipedia.org/wiki/elastic_net"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "variable selection",
            "url": "https://en.wikipedia.org/wiki/variable_selection"
        },
        {
            "term": "prior distribution",
            "url": "https://en.wikipedia.org/wiki/prior_distribution"
        },
        {
            "term": "stationary phase approximation",
            "url": "https://en.wikipedia.org/wiki/stationary_phase_approximation"
        }
    ],
    "highlights": [
        "Statistical modelling of high-dimensional data sets where the number of variables exceeds the number of experimental samples may result in over-fitted models that do not generalize well to unseen data",
        "Application of the stationary phase approximation resulted in marginal posterior distributions which were indistinguishable from those obtained by Gibbs sampling (Figure 2)",
        "The stationary phase approximation can be particularly advantageous in prediction problems, where the response value y \u2208 R for a newly measured predictor sample a \u2208 Rp is obtained using regression coefficients learned from training data",
        "The application of Bayesian methods to infer expected effect sizes and marginal posterior distributions in 1-penalized models has so far required the use of computationally expensive Gibbs sampling methods",
        "It was shown that highly accurate inference in these models is possible using an analytic stationary phase approximation to the partition function integrals. This approximation exploits the fact that the Fourier transform of the non-differentiable double-exponential prior distribution is a well-behaved exponential of a log-barrier function, which is intimately related to the Legendre-Fenchel transform of the 1-penalty term",
        "The Fourier transform plays the same role for Bayesian inference problems as convex duality plays for maximum-likelihood approaches"
    ],
    "key_statements": [
        "Statistical modelling of high-dimensional data sets where the number of variables exceeds the number of experimental samples may result in over-fitted models that do not generalize well to unseen data",
        "The lasso and elastic net are of particular interest, because in their maximum-likelihood solutions, a subset of regression coefficients are exactly zero",
        "Application of the stationary phase approximation resulted in marginal posterior distributions which were indistinguishable from those obtained by Gibbs sampling (Figure 2)",
        "The stationary phase approximation can be particularly advantageous in prediction problems, where the response value y \u2208 R for a newly measured predictor sample a \u2208 Rp is obtained using regression coefficients learned from training data",
        "The regression coefficients. This contrasts with ridge regression, where regression coefficients are normally distributed leading to over-estimation of small effects, and maximum-likelihood elastic net, where small effects become identically zero and don\u2019t contribute to the predicted value at all",
        "The application of Bayesian methods to infer expected effect sizes and marginal posterior distributions in 1-penalized models has so far required the use of computationally expensive Gibbs sampling methods",
        "It was shown that highly accurate inference in these models is possible using an analytic stationary phase approximation to the partition function integrals. This approximation exploits the fact that the Fourier transform of the non-differentiable double-exponential prior distribution is a well-behaved exponential of a log-barrier function, which is intimately related to the Legendre-Fenchel transform of the 1-penalty term",
        "The Fourier transform plays the same role for Bayesian inference problems as convex duality plays for maximum-likelihood approaches"
    ],
    "summary": [
        "Statistical modelling of high-dimensional data sets where the number of variables exceeds the number of experimental samples may result in over-fitted models that do not generalize well to unseen data.",
        "To test the accuracy of the stationary phase approximation, we implemented algorithms to solve the saddle point equations and compute the partition function and marginal posterior distribution, as well as an existing Gibbs sampler algorithm [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] in Matlab.",
        "Convergence of the posterior expectation values x\u03c4 to the maximum-likelihood coefficients x, as measured by the \u221e-norm difference x\u03c4 \u2212 x \u221e = maxj |x\u03c4,j \u2212 xj| is noticeably slower, particularly in the p n setting of the leukemia data (Figure 1d\u2013f).",
        "Application of the stationary phase approximation resulted in marginal posterior distributions which were indistinguishable from those obtained by Gibbs sampling (Figure 2).",
        "Accurate estimations of the marginal posterior distributions requires using the full stationary phase approximations [eq (18)] to the partition functions in eq (20).",
        "The stationary phase approximation can be particularly advantageous in prediction problems, where the response value y \u2208 R for a newly measured predictor sample a \u2208 Rp is obtained using regression coefficients learned from training data.",
        "Comparison with the unpenalized (\u03bc = 0) ridge regression coefficients shows that the Bayesian expectation values are strongly shrunk towards zero, except for the non-zero maximum-likelihood coefficients, which remain relatively unchanged (Figure 3d), resulting in a double-exponential distribution for",
        "This contrasts with ridge regression, where regression coefficients are normally distributed leading to over-estimation of small effects, and maximum-likelihood elastic net, where small effects become identically zero and don\u2019t contribute to the predicted value at all.",
        "The application of Bayesian methods to infer expected effect sizes and marginal posterior distributions in 1-penalized models has so far required the use of computationally expensive Gibbs sampling methods.",
        "This approximation exploits the fact that the Fourier transform of the non-differentiable double-exponential prior distribution is a well-behaved exponential of a log-barrier function, which is intimately related to the Legendre-Fenchel transform of the 1-penalty term.",
        "If the posterior expectation values of the regression coefficients are used instead of their maximum-likelihood values to predict unmeasured responses, the optimal inverse-temperature parameter can be determined by standard cross-validation on the training data, as in the drug response prediction experiments.",
        "Expressing intractable partition function integrals as complex-valued oscillatory integrals through the Fourier transform is a powerful approach for performing Bayesian inference in the lasso and elastic net regression models, and 1-penalized models more generally.",
        "Use of the stationary phase approximation to these integrals results in highly accurate estimates for the posterior expectation values and marginal distributions at a much reduced computational cost compared to Gibbs sampling.",
        "The Fourier transform plays the same role for Bayesian inference problems as convex duality plays for maximum-likelihood approaches"
    ],
    "headline": "We show that in such models, approximate Bayesian inference is possible using a Laplace-like approximation, more precisely the stationary phase or saddle point approximation for complex-valued oscillatory integrals ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Friedman J, Hastie T and Tibshirani R. The elements of statistical learning (Springer series in statistics Springer, Berlin2001).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20J.%20T%2C%20Hastie%20Tibshirani%2C%20R.%20The%20elements%20of%20statistical%20learning"
        },
        {
            "id": "2",
            "entry": "[2] Hoerl AE and Kennard RW. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics 12:55\u201367 (1970).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=AE%2C%20Hoerl%20RW%2C%20Kennard%20Ridge%20regression%3A%20Biased%20estimation%20for%20nonorthogonal%20problems%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=AE%2C%20Hoerl%20RW%2C%20Kennard%20Ridge%20regression%3A%20Biased%20estimation%20for%20nonorthogonal%20problems%201970"
        },
        {
            "id": "3",
            "entry": "[3] Tibshirani R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B (Methodological) 267\u2013288 (1996).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "4",
            "entry": "[4] Zou H and Hastie T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67:301\u2013320 (2005).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H%2C%20Zou%20T%2C%20Hastie%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H%2C%20Zou%20T%2C%20Hastie%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005"
        },
        {
            "id": "5",
            "entry": "[5] Park T and Casella G. The Bayesian lasso. Journal of the American Statistical Association 103:681\u2013686 (2008).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=T%2C%20Park%20G%2C%20Casella%20The%20Bayesian%20lasso%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=T%2C%20Park%20G%2C%20Casella%20The%20Bayesian%20lasso%202008"
        },
        {
            "id": "6",
            "entry": "[6] Hans C. Bayesian lasso regression. Biometrika 835\u2013845 (2009).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hans%2C%20C.%20Bayesian%20lasso%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hans%2C%20C.%20Bayesian%20lasso%20regression%202009"
        },
        {
            "id": "7",
            "entry": "[7] Li Q, Lin N et al. The Bayesian elastic net. Bayesian Analysis 5:151\u2013170 (2010).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Q.%20Lin%2C%20N.%20The%20Bayesian%20elastic%20net%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Q.%20Lin%2C%20N.%20The%20Bayesian%20elastic%20net%202010"
        },
        {
            "id": "8",
            "entry": "[8] Hans C. Elastic net regression modeling with the orthant normal prior. Journal of the American Statistical Association 106:1383\u20131393 (2011).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hans%2C%20C.%20Elastic%20net%20regression%20modeling%20with%20the%20orthant%20normal%20prior%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hans%2C%20C.%20Elastic%20net%20regression%20modeling%20with%20the%20orthant%20normal%20prior%202011"
        },
        {
            "id": "9",
            "entry": "[9] Liu JS. Monte Carlo strategies in scientific computing. (Springer2004).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20J.S.%20Monte%20Carlo%20strategies%20in%20scientific%20computing"
        },
        {
            "id": "10",
            "entry": "[10] Mallick H and Yi N. Bayesian methods for high dimensional linear models. Journal of Biometrics & Biostatistics 1:005 (2013).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H%2C%20Mallick%20N%2C%20Yi%20Bayesian%20methods%20for%20high%20dimensional%20linear%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H%2C%20Mallick%20N%2C%20Yi%20Bayesian%20methods%20for%20high%20dimensional%20linear%20models%202013"
        },
        {
            "id": "11",
            "entry": "[11] Rajaratnam B and Sparks D. Fast Bayesian lasso for high-dimensional regression. arXiv preprint arXiv:150903697 (2015).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%2C%20Rajaratnam%20D%2C%20Sparks%20Fast%20Bayesian%20lasso%20for%20high-dimensional%20regression%202015"
        },
        {
            "id": "12",
            "entry": "[12] Rajaratnam B and Sparks D. MCMC-based inference in the era of big data: A fundamental analysis of the convergence complexity of high-dimensional chains. arXiv preprint arXiv:150800947 (2015).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%2C%20Rajaratnam%20D%2C%20Sparks%20MCMC-based%20inference%20in%20the%20era%20of%20big%20data%3A%20A%20fundamental%20analysis%20of%20the%20convergence%20complexity%20of%20high-dimensional%20chains%202015"
        },
        {
            "id": "13",
            "entry": "[13] Kass RE and Steffey D. Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models). Journal of the American Statistical Association 84:717\u2013726 (1989).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RE%2C%20Kass%20D%2C%20Steffey%20Approximate%20Bayesian%20inference%20in%20conditionally%20independent%20hierarchical%20models%20%28parametric%20empirical%20Bayes%20models%29%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RE%2C%20Kass%20D%2C%20Steffey%20Approximate%20Bayesian%20inference%20in%20conditionally%20independent%20hierarchical%20models%20%28parametric%20empirical%20Bayes%20models%29%201989"
        },
        {
            "id": "14",
            "entry": "[14] Rue H, Martino S and Chopin N. Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 71:319\u2013392 (2009).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rue%2C%20H.%20S%2C%20Martino%20Chopin%2C%20N.%20Approximate%20Bayesian%20inference%20for%20latent%20Gaussian%20models%20by%20using%20integrated%20nested%20Laplace%20approximations%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rue%2C%20H.%20S%2C%20Martino%20Chopin%2C%20N.%20Approximate%20Bayesian%20inference%20for%20latent%20Gaussian%20models%20by%20using%20integrated%20nested%20Laplace%20approximations%202009"
        },
        {
            "id": "15",
            "entry": "[15] Watanabe S. A widely applicable Bayesian information criterion. Journal of Machine Learning Research 14:867\u2013897 (2013).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watanabe%2C%20S.%20A%20widely%20applicable%20Bayesian%20information%20criterion%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watanabe%2C%20S.%20A%20widely%20applicable%20Bayesian%20information%20criterion%202013"
        },
        {
            "id": "16",
            "entry": "[16] Drton M and Plummer M. A Bayesian information criterion for singular models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 79:323\u2013380 (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%2C%20Drton%20M%2C%20Plummer%20A%20Bayesian%20information%20criterion%20for%20singular%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%2C%20Drton%20M%2C%20Plummer%20A%20Bayesian%20information%20criterion%20for%20singular%20models%202017"
        },
        {
            "id": "17",
            "entry": "[17] Wong R. Asymptotic approximation of integrals, vol. 34 (SIAM2001).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wong%2C%20R.%20Asymptotic%20approximation%20of",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wong%2C%20R.%20Asymptotic%20approximation%20of"
        },
        {
            "id": "18",
            "entry": "[18] Daniels HE. Saddlepoint approximations in statistics. The Annals of Mathematical Statistics 631\u2013650 (1954).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniels%2C%20H.E.%20Saddlepoint%20approximations%20in%20statistics%201954",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniels%2C%20H.E.%20Saddlepoint%20approximations%20in%20statistics%201954"
        },
        {
            "id": "19",
            "entry": "[19] Litvinov GL. The Maslov dequantization, idempotent and tropical mathematics: a brief introduction. arXiv preprint math/0507014 (2005).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Litvinov%2C%20G.L.%20The%20Maslov%20dequantization%2C%20idempotent%20and%20tropical%20mathematics%3A%20a%20brief%20introduction%202005"
        },
        {
            "id": "20",
            "entry": "[20] Boyd S and Vandenberghe L. Convex optimization (Cambridge university press2004).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%2C%20Boyd%20L%2C%20Vandenberghe%20Convex%20optimization%202004"
        },
        {
            "id": "21",
            "entry": "[21] Rockafellar RT. Convex Analysis (Princeton University Press1970).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.T.%20Convex%20Analysis"
        },
        {
            "id": "22",
            "entry": "[22] Osborne M, Presnell B and Turlach B. On the lasso and its dual. Journal of Computational and Graphical Statistics 9:319\u2013337 (2000).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osborne%2C%20M.%20B%2C%20Presnell%20Turlach%2C%20B.%20On%20the%20lasso%20and%20its%20dual%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osborne%2C%20M.%20B%2C%20Presnell%20Turlach%2C%20B.%20On%20the%20lasso%20and%20its%20dual%202000"
        },
        {
            "id": "23",
            "entry": "[23] Osborne MR, Presnell B and Turlach BA. A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis 20:389\u2013403 (2000).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osborne%2C%20M.R.%20B%2C%20Presnell%20Turlach%2C%20B.A.%20A%20new%20approach%20to%20variable%20selection%20in%20least%20squares%20problems%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osborne%2C%20M.R.%20B%2C%20Presnell%20Turlach%2C%20B.A.%20A%20new%20approach%20to%20variable%20selection%20in%20least%20squares%20problems%202000"
        },
        {
            "id": "24",
            "entry": "[24] El Ghaoui L, Viallon V and Rabbani T. Safe feature elimination for the lasso and sparse supervised learning problems. Pacific Journal of Optimization 8:667\u2013698 (2012).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L%2C%20El%20Ghaoui%20V%2C%20Viallon%20T%2C%20Rabbani%20Safe%20feature%20elimination%20for%20the%20lasso%20and%20sparse%20supervised%20learning%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L%2C%20El%20Ghaoui%20V%2C%20Viallon%20T%2C%20Rabbani%20Safe%20feature%20elimination%20for%20the%20lasso%20and%20sparse%20supervised%20learning%20problems%202012"
        },
        {
            "id": "25",
            "entry": "[25] Tibshirani R et al. Strong rules for discarding predictors in lasso-type problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 74:245\u2013266 (2012).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20R.%20Strong%20rules%20for%20discarding%20predictors%20in%20lasso-type%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20R.%20Strong%20rules%20for%20discarding%20predictors%20in%20lasso-type%20problems%202012"
        },
        {
            "id": "26",
            "entry": "[26] Tibshirani R. The lasso problem and uniqueness. Electronic Journal of Statistics 7:1456\u20131490 (2013).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20R.%20The%20lasso%20problem%20and%20uniqueness%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20R.%20The%20lasso%20problem%20and%20uniqueness%202013"
        },
        {
            "id": "27",
            "entry": "[27] Michoel T. Natural coordinate descent algorithm for L1-penalised regression in generalised linear models. Computational Statistics & Data Analysis 97:60\u201370 (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Michoel%2C%20T.%20Natural%20coordinate%20descent%20algorithm%20for%20L1-penalised%20regression%20in%20generalised%20linear%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Michoel%2C%20T.%20Natural%20coordinate%20descent%20algorithm%20for%20L1-penalised%20regression%20in%20generalised%20linear%20models%202016"
        },
        {
            "id": "28",
            "entry": "[28] Friedman J, Hastie T and Tibshirani R. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software 33:1 (2010).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20J.%20T%2C%20Hastie%20Tibshirani%2C%20R.%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20J.%20T%2C%20Hastie%20Tibshirani%2C%20R.%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010"
        },
        {
            "id": "29",
            "entry": "[29] Rakitsch B et al. A lasso multi-marker mixed model for association mapping with population structure correction. Bioinformatics 29:206\u2013214 (2012).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakitsch%2C%20B.%20A%20lasso%20multi-marker%20mixed%20model%20for%20association%20mapping%20with%20population%20structure%20correction%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakitsch%2C%20B.%20A%20lasso%20multi-marker%20mixed%20model%20for%20association%20mapping%20with%20population%20structure%20correction%202012"
        },
        {
            "id": "30",
            "entry": "[30] Lang S. Complex Analysis, vol. 103 of Graduate Texts in Mathematics (Springrer2002).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lang%20S%20Complex%20Analysis%20vol%20103%20of%20Graduate%20Texts%20in%20Mathematics%20Springrer2002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lang%20S%20Complex%20Analysis%20vol%20103%20of%20Graduate%20Texts%20in%20Mathematics%20Springrer2002"
        },
        {
            "id": "31",
            "entry": "[31] Schneidemann V. Introduction to Complex Analysis in Several Variables (Birkh\u00e4user Verlag2005).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schneidemann%2C%20V.%20Introduction%20to%20Complex%20Analysis%20in%20Several%20Variables%202005"
        },
        {
            "id": "32",
            "entry": "[32] Van Zon R and Cohen E. Extended heat-fluctuation theorems for a system with deterministic and stochastic forces. Physical Review E 69:056121 (2004).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R%2C%20Van%20Zon%20E%2C%20Cohen%20Extended%20heat-fluctuation%20theorems%20for%20a%20system%20with%20deterministic%20and%20stochastic%20forces%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R%2C%20Van%20Zon%20E%2C%20Cohen%20Extended%20heat-fluctuation%20theorems%20for%20a%20system%20with%20deterministic%20and%20stochastic%20forces%202004"
        },
        {
            "id": "33",
            "entry": "[33] Lee JS, Kwon C and Park H. Modified saddle-point integral near a singularity for the large deviation function. Journal of Statistical Mechanics: Theory and Experiment 2013:P11002 (2013).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20J.S.%20C%2C%20Kwon%20Park%2C%20H.%20Modified%20saddle-point%20integral%20near%20a%20singularity%20for%20the%20large%20deviation%20function%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20J.S.%20C%2C%20Kwon%20Park%2C%20H.%20Modified%20saddle-point%20integral%20near%20a%20singularity%20for%20the%20large%20deviation%20function%202013"
        },
        {
            "id": "34",
            "entry": "[34] Efron B et al. Least angle regression. The Annals of Statistics 32:407\u2013499 (2004).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Efron%2C%20B.%20Least%20angle%20regression%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Efron%2C%20B.%20Least%20angle%20regression%202004"
        },
        {
            "id": "35",
            "entry": "[35] Makalic E and Schmidt DF. High-dimensional Bayesian regularised regression with the BayesReg package. arXiv:161106649 (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=E%2C%20Makalic%20DF%2C%20Schmidt%20High-dimensional%20Bayesian%20regularised%20regression%20with%20the%20BayesReg%20package.%20arXiv%3A%20161106649%202016"
        },
        {
            "id": "36",
            "entry": "[36] Barretina J et al. The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity. Nature 483:603\u2013607 (2012). ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barretina%2C%20J.%20The%20Cancer%20Cell%20Line%20Encyclopedia%20enables%20predictive%20modelling%20of%20anticancer%20drug%20sensitivity%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barretina%2C%20J.%20The%20Cancer%20Cell%20Line%20Encyclopedia%20enables%20predictive%20modelling%20of%20anticancer%20drug%20sensitivity%202012"
        }
    ]
}
