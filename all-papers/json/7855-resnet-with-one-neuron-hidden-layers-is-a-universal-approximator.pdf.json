{
    "filename": "7855-resnet-with-one-neuron-hidden-layers-is-a-universal-approximator.pdf",
    "metadata": {
        "title": "ResNet with one-neuron hidden layers is a Universal Approximator",
        "author": "Hongzhou Lin, Stefanie Jegelka",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7855-resnet-with-one-neuron-hidden-layers-is-a-universal-approximator.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. `1(Rd). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "residual block",
            "url": "https://en.wikipedia.org/wiki/residual_block"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "universal approximator",
            "url": "https://en.wikipedia.org/wiki/universal_approximator"
        },
        {
            "term": "identity mapping",
            "url": "https://en.wikipedia.org/wiki/identity_mapping"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        }
    ],
    "highlights": [
        "Deep neural networks are central to many recent successes of machine learning, including applications such as computer vision, natural language processing, or reinforcement learning",
        "A typical observation is that deeper networks offer better performance. This phenomenon, at least on the training set, supports the intuition that a deeper network should have more capacity to approximate the target function, and leads to a question that has received increasing interest in the theory of deep learning: can all functions that we may care about be approximated well by a sufficiently large and deep network? In this work, we address this important question for the popular ResNet architecture",
        "Neurons. These specific examples do not imply that all shallow networks can be represented by deep networks, leading to an important question: If the number of neurons in each layer is bounded, does universal approximation hold when the depth goes to infinity?. This question has recently been studied by [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] for fully connected networks with ReLU activation functions: if each hidden layer has at least d + 1 neurons, where d is the dimension of the input space, the universal approximation theorem holds as the depth goes to infinity",
        "The main contribution of this paper is to show that ResNet with one single neuron per hidden layer is enough to provide universal approximation as the depth goes to infinity",
        "We have shown a universal approximation theorem for the ResNet structure with one unit per hidden layer",
        "This result stands in contrast to recent results on fully connected networks, for which universal approximation fails with width d or less"
    ],
    "key_statements": [
        "Deep neural networks are central to many recent successes of machine learning, including applications such as computer vision, natural language processing, or reinforcement learning",
        "A typical observation is that deeper networks offer better performance. This phenomenon, at least on the training set, supports the intuition that a deeper network should have more capacity to approximate the target function, and leads to a question that has received increasing interest in the theory of deep learning: can all functions that we may care about be approximated well by a sufficiently large and deep network? In this work, we address this important question for the popular ResNet architecture",
        "Results in the late eighties showed that a network with a single hidden layer can approximate any continuous function with compact support to arbitrary accuracy, when the width goes to infinity [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "Neurons. These specific examples do not imply that all shallow networks can be represented by deep networks, leading to an important question: If the number of neurons in each layer is bounded, does universal approximation hold when the depth goes to infinity?",
        "Neurons. These specific examples do not imply that all shallow networks can be represented by deep networks, leading to an important question: If the number of neurons in each layer is bounded, does universal approximation hold when the depth goes to infinity?. This question has recently been studied by [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] for fully connected networks with ReLU activation functions: if each hidden layer has at least d + 1 neurons, where d is the dimension of the input space, the universal approximation theorem holds as the depth goes to infinity",
        "The main contribution of this paper is to show that ResNet with one single neuron per hidden layer is enough to provide universal approximation as the depth goes to infinity",
        "We present the universal approximation theorem for ResNet with one-neuron hidden layers",
        "A natural idea to approximate h is to construct a trapezoid function on each subdivision I and to k sum them up. This is the main strategy used in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] to show a universal approximation theorem for fully connected networks with width at least d + 1",
        "We have shown a universal approximation theorem for the ResNet structure with one unit per hidden layer",
        "This result stands in contrast to recent results on fully connected networks, for which universal approximation fails with width d or less",
        "We have shown a universal approximation theorem for ResNet with one-neuron hidden layers"
    ],
    "summary": [
        "Deep neural networks are central to many recent successes of machine learning, including applications such as computer vision, natural language processing, or reinforcement learning.",
        "Results in the late eighties showed that a network with a single hidden layer can approximate any continuous function with compact support to arbitrary accuracy, when the width goes to infinity [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "These specific examples do not imply that all shallow networks can be represented by deep networks, leading to an important question: If the number of neurons in each layer is bounded, does universal approximation hold when the depth goes to infinity?",
        "This question has recently been studied by [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] for fully connected networks with ReLU activation functions: if each hidden layer has at least d + 1 neurons, where d is the dimension of the input space, the universal approximation theorem holds as the depth goes to infinity.",
        "The main contribution of this paper is to show that ResNet with one single neuron per hidden layer is enough to provide universal approximation as the depth goes to infinity.",
        "We begin by empirically exploring the difference between narrow fully connected networks, with d neurons per hidden layer, and ResNet via a simple example: classifying the unit ball in the plane.",
        "The decision boundary seems to converge to the unit ball, implying that Proposition 2.1 cannot hold for ResNet. These observations motivate the universal approximation theorem that we will show .",
        "We present the universal approximation theorem for ResNet with one-neuron hidden layers.",
        "This is the main strategy used in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] to show a universal approximation theorem for fully connected networks with width at least d + 1.",
        "The deksired grid indicator function can be obtained by performing a max operator with the threshold T , i.e., cutting off the smaller values and setting them to zero.",
        "A straightforward consequence of our construction is that we can approximate any piecewise constant function to arbitrary accuracy with a ResNet of O hidden units/layers.",
        "We have shown a universal approximation theorem for the ResNet structure with one unit per hidden layer.",
        "While we achieve universal approximation with only one hidden neuron in each basic residual block, one may argue that the structure of ResNet still passes the identity to the layer.",
        "As shown in Section 2, a width d fully connected network can never approximate a compact decision boundary even if we allow infinite depth.",
        "We have shown a universal approximation theorem for ResNet with one-neuron hidden layers."
    ],
    "headline": "We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. `1",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv:1802.06509, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06509"
        },
        {
            "id": "2",
            "entry": "[2] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930\u2013945, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20A.R.%20Universal%20approximation%20bounds%20for%20superpositions%20of%20a%20sigmoidal%20function%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20A.R.%20Universal%20approximation%20bounds%20for%20superpositions%20of%20a%20sigmoidal%20function%201993"
        },
        {
            "id": "3",
            "entry": "[3] H. Beise, S. D. Da Cruz, and U. Schroder. On decision regions of narrow deep neural networks. arXiv:1807.01194, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01194"
        },
        {
            "id": "4",
            "entry": "[4] A. Brutzkus, A. Globerson, E. Malach, and S. Shalev-Shwartz. Sgd learns over-parameterized networks that provably generalize on linearly separable data. In The International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brutzkus%2C%20A.%20Globerson%2C%20A.%20Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Sgd%20learns%20over-parameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brutzkus%2C%20A.%20Globerson%2C%20A.%20Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Sgd%20learns%20over-parameterized%20networks%20that%20provably%20generalize%20on%20linearly%20separable%20data%202018"
        },
        {
            "id": "5",
            "entry": "[5] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In The International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "6",
            "entry": "[6] N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis. In Conference on Learning Theory (COLT), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20N.%20Sharir%2C%20O.%20Shashua%2C%20A.%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20N.%20Sharir%2C%20O.%20Shashua%2C%20A.%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016"
        },
        {
            "id": "7",
            "entry": "[7] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303\u2013314, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function.%20Mathematics%20of%20control%2C%20signals%20and%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function.%20Mathematics%20of%20control%2C%20signals%20and%201989"
        },
        {
            "id": "8",
            "entry": "[8] S. S. Du, J.D. Lee, Y. Tian, B. Poczos, and A. Singh. Gradient descent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00779"
        },
        {
            "id": "9",
            "entry": "[9] R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory (COLT), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eldan%2C%20R.%20Shamir%2C%20O.%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eldan%2C%20R.%20Shamir%2C%20O.%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016"
        },
        {
            "id": "10",
            "entry": "[10] K. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3):183\u2013192, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Funahashi%2C%20K.%20On%20the%20approximate%20realization%20of%20continuous%20mappings%20by%20neural%20networks%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Funahashi%2C%20K.%20On%20the%20approximate%20realization%20of%20continuous%20mappings%20by%20neural%20networks%201989"
        },
        {
            "id": "11",
            "entry": "[11] B. Hanin and M. Sellke. Approximating continuous functions by relu nets of minimal width. arXiv:1710.11278, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11278"
        },
        {
            "id": "12",
            "entry": "[12] M. Hardt and T. Ma. Identity matters in deep learning. In The International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20M.%20Ma%2C%20T.%20Identity%20matters%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20M.%20Ma%2C%20T.%20Identity%20matters%20in%20deep%20learning%202017"
        },
        {
            "id": "13",
            "entry": "[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "14",
            "entry": "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016"
        },
        {
            "id": "15",
            "entry": "[15] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359\u2013366, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20K.%20Stinchcombe%2C%20M.%20White%2C%20H.%20Multilayer%20feedforward%20networks%20are%20universal%20approximators%201989"
        },
        {
            "id": "16",
            "entry": "[16] K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "17",
            "entry": "[17] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 1097\u2013 1105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "18",
            "entry": "[18] V. Kurkov\u00e1. Kolmogorov\u2019s theorem and multilayer neural networks. Neural networks, 5(3): 501\u2013506, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurkov%C3%A1%2C%20V.%20Kolmogorov%E2%80%99s%20theorem%20and%20multilayer%20neural%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurkov%C3%A1%2C%20V.%20Kolmogorov%E2%80%99s%20theorem%20and%20multilayer%20neural%20networks%201992"
        },
        {
            "id": "19",
            "entry": "[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "20",
            "entry": "[20] S. Liang and R. Srikant. Why deep neural networks for function approximation? In The International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20S.%20Srikant%2C%20R.%20Why%20deep%20neural%20networks%20for%20function%20approximation%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20S.%20Srikant%2C%20R.%20Why%20deep%20neural%20networks%20for%20function%20approximation%3F%202017"
        },
        {
            "id": "21",
            "entry": "[21] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks: A view from the width. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Z.%20Pu%2C%20H.%20Wang%2C%20F.%20Hu%2C%20Z.%20The%20expressive%20power%20of%20neural%20networks%3A%20A%20view%20from%20the%20width%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Z.%20Pu%2C%20H.%20Wang%2C%20F.%20Hu%2C%20Z.%20The%20expressive%20power%20of%20neural%20networks%3A%20A%20view%20from%20the%20width%202017"
        },
        {
            "id": "22",
            "entry": "[22] H. N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural computation, 8(1):164\u2013177, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.N.%20Neural%20networks%20for%20optimal%20approximation%20of%20smooth%20and%20analytic%20functions%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.N.%20Neural%20networks%20for%20optimal%20approximation%20of%20smooth%20and%20analytic%20functions%201996"
        },
        {
            "id": "23",
            "entry": "[23] H. N. Mhaskar and T. Poggio. Deep vs. shallow networks: An approximation theory perspective. Analysis and Applications, 14(06):829\u2013848, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.N.%20Poggio%2C%20T.%20Deep%20vs.%20shallow%20networks%3A%20An%20approximation%20theory%20perspective%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.N.%20Poggio%2C%20T.%20Deep%20vs.%20shallow%20networks%3A%20An%20approximation%20theory%20perspective%202016"
        },
        {
            "id": "24",
            "entry": "[24] Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proceedings of the International Conferences on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Q.%20Hein%2C%20M.%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Q.%20Hein%2C%20M.%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017"
        },
        {
            "id": "25",
            "entry": "[25] D. Rolnick and M. Tegmark. The power of deeper networks for expressing natural functions. In The International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rolnick%2C%20D.%20M.%20Tegmark.%20The%20power%20of%20deeper%20networks%20for%20expressing%20natural%20functions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rolnick%2C%20D.%20M.%20Tegmark.%20The%20power%20of%20deeper%20networks%20for%20expressing%20natural%20functions%202018"
        },
        {
            "id": "26",
            "entry": "[26] S. Shalev-Shwartz, O. Shamir, and S. Shammah. Weight sharing is crucial to succesful optimization. arXiv:1706.00687, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00687"
        },
        {
            "id": "27",
            "entry": "[27] O. Shamir. Are resnets provably better than linear predictors? arXiv:1804.06739, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06739"
        },
        {
            "id": "28",
            "entry": "[28] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In The International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "29",
            "entry": "[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research (JMLR), 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "30",
            "entry": "[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, J. Rick Chang, et al. Going deeper with convolutions. In IEEE conference on computer vision and pattern recognition (CVPR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "31",
            "entry": "[31] L. Szymanski and B. McCane. Deep networks are effective encoders of periodicity. IEEE transactions on neural networks and learning systems, 25(10):1816\u20131827, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szymanski%2C%20L.%20McCane%2C%20B.%20Deep%20networks%20are%20effective%20encoders%20of%20periodicity%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szymanski%2C%20L.%20McCane%2C%20B.%20Deep%20networks%20are%20effective%20encoders%20of%20periodicity%202014"
        },
        {
            "id": "32",
            "entry": "[32] M. Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory (COLT), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Telgarsky%2C%20M.%20Benefits%20of%20depth%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Telgarsky%2C%20M.%20Benefits%20of%20depth%20in%20neural%20networks%202016"
        },
        {
            "id": "33",
            "entry": "[33] D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94: 103\u2013114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017"
        },
        {
            "id": "34",
            "entry": "[34] D. Yarotsky. Optimal approximation of continuous functions by very deep relu networks. arXiv preprint arXiv:1802.03620, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03620"
        },
        {
            "id": "35",
            "entry": "[35] C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. In The International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yun%2C%20C.%20Sra%2C%20S.%20Jadbabaie%2C%20A.%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yun%2C%20C.%20Sra%2C%20S.%20Jadbabaie%2C%20A.%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202018"
        },
        {
            "id": "36",
            "entry": "[36] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In The International Conference on Learning Representations (ICLR), 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202016"
        }
    ]
}
