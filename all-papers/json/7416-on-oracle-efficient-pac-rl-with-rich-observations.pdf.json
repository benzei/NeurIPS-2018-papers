{
    "filename": "7416-on-oracle-efficient-pac-rl-with-rich-observations.pdf",
    "metadata": {
        "title": "On Oracle-Efficient PAC RL with Rich Observations",
        "author": "Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7416-on-oracle-efficient-pac-rl-with-rich-observations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the computational tractability of PAC reinforcement learning with rich observations. We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation\u2014accessing policy and value function classes exclusively through standard optimization primitives\u2014and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. With stochastic hidden state dynamics, we prove that the only known sample-efficient algorithm, OLIVE [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings."
    },
    "keywords": [
        {
            "term": "decision process",
            "url": "https://en.wikipedia.org/wiki/decision_process"
        },
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        },
        {
            "term": "function approximation",
            "url": "https://en.wikipedia.org/wiki/function_approximation"
        },
        {
            "term": "linear program",
            "url": "https://en.wikipedia.org/wiki/linear_program"
        },
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        }
    ],
    "highlights": [
        "We study episodic reinforcement learning (RL) in environments with realistically rich observations such as images or text, which we refer to broadly as contextual decision processes",
        "We emphasize that our main goal is to understand the interplay between statistical and computational efficiency to discover new algorithmic ideas that may lead to practical methods, rather than improve sample complexity bounds.\n5 Toward Oracle-Efficient PAC-reinforcement learning with Stochastic Hidden State Dynamics",
        "This paper describes new reinforcement learning algorithms for environments with rich stochastic observations and deterministic hidden state dynamics",
        "These algorithms are computationally efficient in an oracle model, and we emphasize that the oracle-based approach has led to practical algorithms for many other settings",
        "We show that the only known approach for this setting is not implementable with standard oracles, and we provide several constructions demonstrating other concrete challenges of reinforcement learning with stochastic state dynamics"
    ],
    "key_statements": [
        "We study episodic reinforcement learning (RL) in environments with realistically rich observations such as images or text, which we refer to broadly as contextual decision processes",
        "We aim for methods that use function approximation in a provably effective manner to find the best possible policy through strategic exploration. While such problems are central to empirical reinforcement learning research [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], most theoretical results on strategic exploration focus on tabular MDPs with small state spaces [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>\u2013<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "Little work exists on provably effective exploration with large observation spaces that require generalization through function approximation",
        "The few algorithms that do exist either have poor sample complexity guarantees [e.g., 11\u201314] or require fully deterministic environments [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and are inapplicable to most real-world applications and modern empirical reinforcement learning benchmarks. This scarcity of positive results on efficient exploration with function approximation can likely be attributed to the challenging nature of this problem rather than a lack of interest by the research community",
        "We study the computational aspects of the OLIVE algorithm [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], which applies to a wide range of environments",
        "We describe several other barriers, and two other oracle-based algorithms for the deterministic-dynamics setting that are considerably different from VALOR",
        "We propose and analyze a new algorithm, VALOR (Values stored Locally for reinforcement learning) shown in Algorithm 1. This algorithm is oracle-efficient and enjoys a polynomial sample-complexity guarantee in the deterministic hidden-state dynamics setting described earlier, which was originally introduced by Krishnamurthy et al [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "By a careful design of the algorithm and through an intricate and novel analysis, we show that this bias only accumulates linearly, which leads to a polynomial sample complexity guarantee.\n4This is not rare in reinforcement learning; see e.g., Chapter 3.4 of Ross [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>].\n4.2",
        "We show that they can be reduced to CSC on \u03a0 and linear program on G, respectively, and VALOR is oracle-efficient",
        "In Appendix B.7 we show that a version of VALOR with alternative oracle assumptions enjoys a better PAC bound than LSVEE",
        "We emphasize that our main goal is to understand the interplay between statistical and computational efficiency to discover new algorithmic ideas that may lead to practical methods, rather than improve sample complexity bounds.\n5 Toward Oracle-Efficient PAC-reinforcement learning with Stochastic Hidden State Dynamics",
        "While we only show this for CSC, linear program, and least-squares regression oracles explicitly, we expect other practically relevant oracles to be efficient in the tabular setting, and they could not help to implement OLIVE efficiently",
        "For tabular value functions G = (X \u2192 [0, 1]) and policies \u03a0 = (X \u2192 A), the CSC, linear program, and least-squares regression oracles can be implemented in time polynomial in |X |, K = |A| and the input size",
        "Proposition 6 implies that if OLIVE could be implemented with polynomially many CSC/linear program/least-squares regression oracle calls, its total runtime would be polynomial for tabular MDPs",
        "In Appendix E, we provide a series of examples that illustrate some limitations of such algorithms",
        "This paper describes new reinforcement learning algorithms for environments with rich stochastic observations and deterministic hidden state dynamics",
        "These algorithms are computationally efficient in an oracle model, and we emphasize that the oracle-based approach has led to practical algorithms for many other settings",
        "We show that the only known approach for this setting is not implementable with standard oracles, and we provide several constructions demonstrating other concrete challenges of reinforcement learning with stochastic state dynamics"
    ],
    "summary": [
        "We study episodic reinforcement learning (RL) in environments with realistically rich observations such as images or text, which we refer to broadly as contextual decision processes.",
        "This algorithm is oracle-efficient and enjoys a polynomial sample-complexity guarantee in the deterministic hidden-state dynamics setting described earlier, which was originally introduced by Krishnamurthy et al [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "After dfslearn returns, we first use the stored data and values (Line 5) to compute a policy that is near optimal on all explored states.",
        "LSVEE uses a Q-value function class F \u2282 (X \u00d7 A \u2192 [0, 1]) and a state identity test based on Bellman errors on data sets D consisting of (x, a, r, x ) tuples: E D f (x, a) \u2212 r \u2212 Ex \u223ca maxa \u2208A f (x , a ) 2 .",
        "We emphasize that our main goal is to understand the interplay between statistical and computational efficiency to discover new algorithmic ideas that may lead to practical methods, rather than improve sample complexity bounds.",
        "For this more general setting with stochastic hidden state dynamics, OLIVE [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] is the only known algorithm with polynomial sample complexity, but its computational properties remain underexplored.",
        "We show this result by proving that even in tabular MDPs, OLIVE solves an NP-hard problem to determine its exploration policy, while all oracles we consider have polynomial runtime in the tabular setting.",
        "This theorem shows that there are no known oracle-efficient PAC-RL methods for this general setting and that applying clever optimization tricks to implement OLIVE is not enough to achieve a practical algorithm.",
        "For tabular value functions G = (X \u2192 [0, 1]) and policies \u03a0 = (X \u2192 A), the CSC, LP, and LS oracles can be implemented in time polynomial in |X |, K = |A| and the input size.",
        "Proposition 6 implies that if OLIVE could be implemented with polynomially many CSC/LP/LS oracle calls, its total runtime would be polynomial for tabular MDPs. Assuming P = NP, this contradicts Theorem 5 which states that determining the exploration policy of OLIVE in tabular MDPs is NP-hard.",
        "An important element of VALOR is that it explicitly stores value estimates of the hidden states, which we call \u201clocal values.\u201d Local values lead to statistical and computational efficiency under weak realizability conditions, but this approach is unlikely to generalize to the stochastic setting where the agent may not be able to consistently visit a particular hidden state.",
        "These algorithms are oracle-efficient and while we only establish statistical efficiency with deterministic hidden state dynamics, we believe that they considerably expand the space of plausible algorithms for the general setting."
    ],
    "headline": "We study the computational tractability of PAC reinforcement learning with rich observations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Nan%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Langford%2C%20John%20Contextual%20decision%20processes%20with%20low%20Bellman%20rank%20are%20PAC-learnable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Nan%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Langford%2C%20John%20Contextual%20decision%20processes%20with%20low%20Bellman%20rank%20are%20PAC-learnable%202017"
        },
        {
            "id": "2",
            "entry": "[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "3",
            "entry": "[3] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20Michael%20Singh%2C%20Satinder%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20Michael%20Singh%2C%20Satinder%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002"
        },
        {
            "id": "4",
            "entry": "[4] Ronen I. Brafman and Moshe Tennenholtz. R-max \u2013 a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brafman%2C%20Ronen%20I.%20Tennenholtz%2C%20Moshe%20R-max%20%E2%80%93%20a%20general%20polynomial%20time%20algorithm%20for%20near-optimal%20reinforcement%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brafman%2C%20Ronen%20I.%20Tennenholtz%2C%20Moshe%20R-max%20%E2%80%93%20a%20general%20polynomial%20time%20algorithm%20for%20near-optimal%20reinforcement%20learning%202003"
        },
        {
            "id": "5",
            "entry": "[5] Alexander L. Strehl and Michael L. Littman. A theoretical analysis of model-based interval estimation. In International Conference on Machine learning, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20Alexander%20L.%20Littman%2C%20Michael%20L.%20A%20theoretical%20analysis%20of%20model-based%20interval%20estimation%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20Alexander%20L.%20Littman%2C%20Michael%20L.%20A%20theoretical%20analysis%20of%20model-based%20interval%20estimation%202005"
        },
        {
            "id": "6",
            "entry": "[6] Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC model-free reinforcement learning. In International Conference on Machine Learning, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Wiewiora%2C%20Eric%20Langford%2C%20John%20PAC%20model-free%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Wiewiora%2C%20Eric%20Langford%2C%20John%20PAC%20model-free%20reinforcement%20learning%202006"
        },
        {
            "id": "7",
            "entry": "[7] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In Advances in Neural Information Processing Systems, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202009"
        },
        {
            "id": "8",
            "entry": "[8] Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. In Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dann%2C%20Christoph%20Brunskill%2C%20Emma%20Sample%20complexity%20of%20episodic%20fixed-horizon%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dann%2C%20Christoph%20Brunskill%2C%20Emma%20Sample%20complexity%20of%20episodic%20fixed-horizon%20reinforcement%20learning%202015"
        },
        {
            "id": "9",
            "entry": "[9] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20Mohammad%20Gheshlaghi%20Osband%2C%20Ian%20Munos%2C%20Remi%20Minimax%20regret%20bounds%20for%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azar%2C%20Mohammad%20Gheshlaghi%20Osband%2C%20Ian%20Munos%2C%20Remi%20Minimax%20regret%20bounds%20for%20reinforcement%20learning%202017"
        },
        {
            "id": "10",
            "entry": "[10] Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dann%2C%20Christoph%20Lattimore%2C%20Tor%20Brunskill%2C%20Emma%20Unifying%20PAC%20and%20regret%3A%20Uniform%20PAC%20bounds%20for%20episodic%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dann%2C%20Christoph%20Lattimore%2C%20Tor%20Brunskill%2C%20Emma%20Unifying%20PAC%20and%20regret%3A%20Uniform%20PAC%20bounds%20for%20episodic%20reinforcement%20learning%202017"
        },
        {
            "id": "11",
            "entry": "[11] Sham M. Kakade, Michael Kearns, and John Langford. Exploration in metric state spaces. In International Conference on Machine Learning, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Kearns%2C%20Michael%20Langford%2C%20John%20Exploration%20in%20metric%20state%20spaces%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Kearns%2C%20Michael%20Langford%2C%20John%20Exploration%20in%20metric%20state%20spaces%202003"
        },
        {
            "id": "12",
            "entry": "[12] Jason Pazis and Ronald Parr. PAC optimal exploration in continuous space Markov decision processes. In AAAI Conference on Artificial Intelligence, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pazis%2C%20Jason%20Parr%2C%20Ronald%20PAC%20optimal%20exploration%20in%20continuous%20space%20Markov%20decision%20processes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pazis%2C%20Jason%20Parr%2C%20Ronald%20PAC%20optimal%20exploration%20in%20continuous%20space%20Markov%20decision%20processes%202013"
        },
        {
            "id": "13",
            "entry": "[13] Robert Grande, Thomas Walsh, and Jonathan How. Sample efficient reinforcement learning with gaussian processes. In International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grande%2C%20Robert%20Walsh%2C%20Thomas%20How%2C%20Jonathan%20Sample%20efficient%20reinforcement%20learning%20with%20gaussian%20processes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grande%2C%20Robert%20Walsh%2C%20Thomas%20How%2C%20Jonathan%20Sample%20efficient%20reinforcement%20learning%20with%20gaussian%20processes%202014"
        },
        {
            "id": "14",
            "entry": "[14] Jason Pazis and Ronald Parr. Efficient PAC-optimal exploration in concurrent, continuous state MDPs with delayed updates. In AAAI Conference on Artificial Intelligence, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pazis%2C%20Jason%20Parr%2C%20Ronald%20Efficient%20PAC-optimal%20exploration%20in%20concurrent%2C%20continuous%20state%20MDPs%20with%20delayed%20updates%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pazis%2C%20Jason%20Parr%2C%20Ronald%20Efficient%20PAC-optimal%20exploration%20in%20concurrent%2C%20continuous%20state%20MDPs%20with%20delayed%20updates%202016"
        },
        {
            "id": "15",
            "entry": "[15] Zheng Wen and Benjamin Van Roy. Efficient exploration and value function generalization in deterministic systems. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%20Wen%20and%20Benjamin%20Van%20Roy%20Efficient%20exploration%20and%20value%20function%20generalization%20in%20deterministic%20systems%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%20Wen%20and%20Benjamin%20Van%20Roy%20Efficient%20exploration%20and%20value%20function%20generalization%20in%20deterministic%20systems%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202013"
        },
        {
            "id": "16",
            "entry": "[16] Zheng Wen and Benjamin Van Roy. Efficient reinforcement learning in deterministic systems with value function generalization. Mathematics of Operations Research, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Zheng%20Roy%2C%20Benjamin%20Van%20Efficient%20reinforcement%20learning%20in%20deterministic%20systems%20with%20value%20function%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Zheng%20Roy%2C%20Benjamin%20Van%20Efficient%20reinforcement%20learning%20in%20deterministic%20systems%20with%20value%20function%20generalization%202017"
        },
        {
            "id": "17",
            "entry": "[17] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Langford%2C%20John%20PAC%20reinforcement%20learning%20with%20rich%20observations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Langford%2C%20John%20PAC%20reinforcement%20learning%20with%20rich%20observations%202016"
        },
        {
            "id": "18",
            "entry": "[18] Daniel Joseph Hsu. Algorithms for active learning. PhD thesis, UC San Diego, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Daniel%20Joseph%20Algorithms%20for%20active%20learning%202010"
        },
        {
            "id": "19",
            "entry": "[19] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Hsu%2C%20Daniel%20Kale%2C%20Satyen%20Langford%2C%20John%20Taming%20the%20monster%3A%20A%20fast%20and%20simple%20algorithm%20for%20contextual%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Hsu%2C%20Daniel%20Kale%2C%20Satyen%20Langford%2C%20John%20Taming%20the%20monster%3A%20A%20fast%20and%20simple%20algorithm%20for%20contextual%20bandits%202014"
        },
        {
            "id": "20",
            "entry": "[20] Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv:1406.5979, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.5979"
        },
        {
            "id": "21",
            "entry": "[21] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume III, and John Langford. Learning to search better than your teacher. In International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Kai-Wei%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Daume%2C%20III%2C%20Hal%20Learning%20to%20search%20better%20than%20your%20teacher%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Kai-Wei%20Krishnamurthy%2C%20Akshay%20Agarwal%2C%20Alekh%20Daume%2C%20III%2C%20Hal%20Learning%20to%20search%20better%20than%20your%20teacher%202015"
        },
        {
            "id": "22",
            "entry": "[22] Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of Machine Learning Research, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allwein%2C%20Erin%20L.%20Schapire%2C%20Robert%20E.%20Singer%2C%20Yoram%20Reducing%20multiclass%20to%20binary%3A%20A%20unifying%20approach%20for%20margin%20classifiers%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allwein%2C%20Erin%20L.%20Schapire%2C%20Robert%20E.%20Singer%2C%20Yoram%20Reducing%20multiclass%20to%20binary%3A%20A%20unifying%20approach%20for%20margin%20classifiers%202000"
        },
        {
            "id": "23",
            "entry": "[23] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The Malmo Platform for artificial intelligence experimentation. In International Joint Conference on Artificial Intelligence, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Matthew%20Hofmann%2C%20Katja%20Hutton%2C%20Tim%20Bignell%2C%20David%20The%20Malmo%20Platform%20for%20artificial%20intelligence%20experimentation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Matthew%20Hofmann%2C%20Katja%20Hutton%2C%20Tim%20Bignell%2C%20David%20The%20Malmo%20Platform%20for%20artificial%20intelligence%20experimentation%202016"
        },
        {
            "id": "24",
            "entry": "[24] Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored MDPs. In International Joint Conference on Artificial Intelligence, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20Michael%20Koller%2C%20Daphne%20Efficient%20reinforcement%20learning%20in%20factored%20MDPs%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20Michael%20Koller%2C%20Daphne%20Efficient%20reinforcement%20learning%20in%20factored%20MDPs%201999"
        },
        {
            "id": "25",
            "entry": "[25] Lihong Li, Thomas J. Walsh, and Michael L. Littman. Towards a unified theory of state abstraction for MDPs. In International Symposium on Artificial Intelligence and Mathematics, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Walsh%2C%20Thomas%20J.%20Littman%2C%20Michael%20L.%20Towards%20a%20unified%20theory%20of%20state%20abstraction%20for%20MDPs%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Walsh%2C%20Thomas%20J.%20Littman%2C%20Michael%20L.%20Towards%20a%20unified%20theory%20of%20state%20abstraction%20for%20MDPs%202006"
        },
        {
            "id": "26",
            "entry": "[26] Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning in rich-observation MDPs using spectral methods. arXiv:1611.03907, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03907"
        },
        {
            "id": "27",
            "entry": "[27] Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of POMDPs using spectral methods. In Conference on Learning Theory, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azizzadenesheli%2C%20Kamyar%20Lazaric%2C%20Alessandro%20Anandkumar%2C%20Animashree%20Reinforcement%20learning%20of%20POMDPs%20using%20spectral%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azizzadenesheli%2C%20Kamyar%20Lazaric%2C%20Alessandro%20Anandkumar%2C%20Animashree%20Reinforcement%20learning%20of%20POMDPs%20using%20spectral%20methods%202016"
        },
        {
            "id": "28",
            "entry": "[28] Zhaohan Daniel Guo, Shayan Doroudi, and Emma Brunskill. A PAC RL algorithm for episodic POMDPs. In Artificial Intelligence and Statistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Zhaohan%20Daniel%20Doroudi%2C%20Shayan%20Brunskill%2C%20Emma%20A%20PAC%20RL%20algorithm%20for%20episodic%20POMDPs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Zhaohan%20Daniel%20Doroudi%2C%20Shayan%20Brunskill%2C%20Emma%20A%20PAC%20RL%20algorithm%20for%20episodic%20POMDPs%202016"
        },
        {
            "id": "29",
            "entry": "[29] Dan Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russo%2C%20Dan%20Roy%2C%20Benjamin%20Van%20Eluder%20dimension%20and%20the%20sample%20complexity%20of%20optimistic%20exploration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russo%2C%20Dan%20Roy%2C%20Benjamin%20Van%20Eluder%20dimension%20and%20the%20sample%20complexity%20of%20optimistic%20exploration%202013"
        },
        {
            "id": "30",
            "entry": "[30] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder dimension. In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Model-based%20reinforcement%20learning%20and%20the%20Eluder%20dimension%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Model-based%20reinforcement%20learning%20and%20the%20Eluder%20dimension%202014"
        },
        {
            "id": "31",
            "entry": "[31] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20network%20learning%3A%20Theoretical%20foundations%202009"
        },
        {
            "id": "32",
            "entry": "[32] Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed: Convergent reinforcement learning with nonlinear function approximation. In International Conference on Machine Learning, pages 1133\u20131142, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Bo%20Shaw%2C%20Albert%20Li%2C%20Lihong%20Xiao%2C%20Lin%20Sbeed%3A%20Convergent%20reinforcement%20learning%20with%20nonlinear%20function%20approximation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Bo%20Shaw%2C%20Albert%20Li%2C%20Lihong%20Xiao%2C%20Lin%20Sbeed%3A%20Convergent%20reinforcement%20learning%20with%20nonlinear%20function%20approximation%202018"
        },
        {
            "id": "33",
            "entry": "[33] Alina Beygelzimer, John Langford, and Pradeep Ravikumar. Error-correcting tournaments. In International Conference on Algorithmic Learning Theory, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beygelzimer%2C%20Alina%20Langford%2C%20John%20Ravikumar%2C%20Pradeep%20Error-correcting%20tournaments%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beygelzimer%2C%20Alina%20Langford%2C%20John%20Ravikumar%2C%20Pradeep%20Error-correcting%20tournaments%202009"
        },
        {
            "id": "34",
            "entry": "[34] John Langford and Alina Beygelzimer. Sensitive error correcting output codes. In International Conference on Computational Learning Theory, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Beygelzimer%2C%20Alina%20Sensitive%20error%20correcting%20output%20codes%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Beygelzimer%2C%20Alina%20Sensitive%20error%20correcting%20output%20codes%202005"
        },
        {
            "id": "35",
            "entry": "[35] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in Neural Information Processing Systems, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Zhang%2C%20Tong%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Zhang%2C%20Tong%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008"
        },
        {
            "id": "36",
            "entry": "[36] Stephane Ross. Interactive learning for sequential decisions and predictions. PhD thesis, Carnegie Mellon University, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20Stephane%20Interactive%20learning%20for%20sequential%20decisions%20and%20predictions%202013"
        },
        {
            "id": "37",
            "entry": "[37] Leonid G Khachiyan. Polynomial algorithms in linear programming. USSR Computational Mathematics and Mathematical Physics, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khachiyan%2C%20Leonid%20G.%20Polynomial%20algorithms%20in%20linear%20programming%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khachiyan%2C%20Leonid%20G.%20Polynomial%20algorithms%20in%20linear%20programming%201980"
        },
        {
            "id": "38",
            "entry": "[38] Geoffrey J Gordon. Stable function approximation in dynamic programming. In International Conference on Machine Learning, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordon%2C%20Geoffrey%20J.%20Stable%20function%20approximation%20in%20dynamic%20programming%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordon%2C%20Geoffrey%20J.%20Stable%20function%20approximation%20in%20dynamic%20programming%201995"
        },
        {
            "id": "39",
            "entry": "[39] J Andrew Bagnell, Sham M Kakade, Jeff G Schneider, and Andrew Y Ng. Policy search by dynamic programming. In Advances in Neural Information Processing Systems, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bagnell%2C%20J.Andrew%20Kakade%2C%20Sham%20M.%20Schneider%2C%20Jeff%20G.%20and%20Andrew%20Y%20Ng.%20Policy%20search%20by%20dynamic%20programming%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bagnell%2C%20J.Andrew%20Kakade%2C%20Sham%20M.%20Schneider%2C%20Jeff%20G.%20and%20Andrew%20Y%20Ng.%20Policy%20search%20by%20dynamic%20programming%202004"
        },
        {
            "id": "40",
            "entry": "[40] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm and applications. Theory of Computing, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20The%20multiplicative%20weights%20update%20method%3A%20a%20meta-algorithm%20and%20applications%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20The%20multiplicative%20weights%20update%20method%3A%20a%20meta-algorithm%20and%20applications%202012"
        },
        {
            "id": "41",
            "entry": "[41] Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal of Machine Learning Research, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20Remi%20Szepesvari%2C%20Csaba%20Finite-time%20bounds%20for%20fitted%20value%20iteration%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20Remi%20Szepesvari%2C%20Csaba%20Finite-time%20bounds%20for%20fitted%20value%20iteration%202008"
        },
        {
            "id": "42",
            "entry": "[42] Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antos%2C%20Andras%20Szepesvari%2C%20Csaba%20Munos%2C%20Remi%20Learning%20near-optimal%20policies%20with%20Bellman-residual%20minimization%20based%20fitted%20policy%20iteration%20and%20a%20single%20sample%20path%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antos%2C%20Andras%20Szepesvari%2C%20Csaba%20Munos%2C%20Remi%20Learning%20near-optimal%20policies%20with%20Bellman-residual%20minimization%20based%20fitted%20policy%20iteration%20and%20a%20single%20sample%20path%202008"
        },
        {
            "id": "43",
            "entry": "[43] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander J. Smola. A kernel two-sample test. Journal of Machine Learning Research, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20Arthur%20Borgwardt%2C%20Karsten%20M.%20Rasch%2C%20Malte%20J.%20Scholkopf%2C%20Bernhard%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "44",
            "entry": "[44] Bernhard Scholkopf and Alexander J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT Press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholkopf%2C%20Bernhard%20Smola%2C%20Alexander%20J.%20Learning%20with%20kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202002"
        },
        {
            "id": "45",
            "entry": "[45] Martin Grotschel, Laszlo Lovasz, and Alexander Schrijver. The ellipsoid method and its consequences in combinatorial optimization. Combinatorica, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grotschel%2C%20Martin%20Lovasz%2C%20Laszlo%20Schrijver%2C%20Alexander%20The%20ellipsoid%20method%20and%20its%20consequences%20in%20combinatorial%20optimization%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grotschel%2C%20Martin%20Lovasz%2C%20Laszlo%20Schrijver%2C%20Alexander%20The%20ellipsoid%20method%20and%20its%20consequences%20in%20combinatorial%20optimization%201981"
        },
        {
            "id": "46",
            "entry": "[46] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-based%20batch%20mode%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-based%20batch%20mode%20reinforcement%20learning%202005"
        },
        {
            "id": "47",
            "entry": "[47] Amir-Massoud Farahmand, Csaba Szepesvari, and Remi Munos. Error propagation for approximate policy and value iteration. In Advances in Neural Information Processing Systems, 2010. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20Amir-Massoud%20Szepesvari%2C%20Csaba%20Munos%2C%20Remi%20Error%20propagation%20for%20approximate%20policy%20and%20value%20iteration%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20Amir-Massoud%20Szepesvari%2C%20Csaba%20Munos%2C%20Remi%20Error%20propagation%20for%20approximate%20policy%20and%20value%20iteration%202010"
        }
    ]
}
