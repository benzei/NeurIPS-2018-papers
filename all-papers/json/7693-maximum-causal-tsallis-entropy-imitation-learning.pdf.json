{
    "filename": "7693-maximum-causal-tsallis-entropy-imitation-learning.pdf",
    "metadata": {
        "title": "Maximum Causal Tsallis Entropy Imitation Learning",
        "author": "Kyungjae Lee, Sungjoon Choi, Songhwai Oh",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7693-maximum-causal-tsallis-entropy-imitation-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Shannon entropy is less effective."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "Markov decision processes",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_processes"
        },
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        }
    ],
    "highlights": [
        "Markov Decision Processes Markov decision processes (MDPs) are a well-known mathematical framework for a sequential decision making problem",
        "We show that the MCTEIL algorithm can be obtained by extending the maximum causal Tsallis entropy framework to the generative adversarial setting, to generative adversarial imitation learning (GAIL) by Ho and Ermon [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], which is based on the maximum causal entropy framework",
        "Since the maximum Tsallis entropy makes each mixture mean explore in different directions and a sparsemax distribution assigns zero weight to unnecessary mixture components, MCTEIL efficiently explores and shows better performance compared to soft generative adversarial imitation learning with a soft mixture density network",
        "We have proposed a novel maximum causal Tsallis entropy (MCTE) framework, which induces a sparsemax distribution as the optimal solution",
        "We have developed the maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm, which can efficiently solve a maximum causal Tsallis entropy problem in a continuous action space since the Tsallis entropy of a mixture of Gaussians encourages exploration and efficient mixture utilization",
        "From the analysis and experiments, we have shown that the proposed MCTEIL method is an efficient and principled way to learn the multi-modal behavior of an expert"
    ],
    "key_statements": [
        "Markov Decision Processes Markov decision processes (MDPs) are a well-known mathematical framework for a sequential decision making problem",
        "We propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning, which can learn from a uni-modal to multi-modal policy distribution by adjusting its supporting set",
        "To apply the maximum causal Tsallis entropy framework to a complex and model-free problem, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) with a sparse mixture density network whose mixture weights are modeled as a sparsemax distribution",
        "We show that the MCTEIL algorithm can be obtained by extending the maximum causal Tsallis entropy framework to the generative adversarial setting, to generative adversarial imitation learning (GAIL) by Ho and Ermon [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], which is based on the maximum causal entropy framework",
        "We show that the proposed maximum causal Tsallis entropy problem is equivalent to a minimax game with the Brier score [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "Since the maximum Tsallis entropy makes each mixture mean explore in different directions and a sparsemax distribution assigns zero weight to unnecessary mixture components, MCTEIL efficiently explores and shows better performance compared to soft generative adversarial imitation learning with a soft mixture density network",
        "We have proposed a novel maximum causal Tsallis entropy (MCTE) framework, which induces a sparsemax distribution as the optimal solution",
        "We have developed the maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm, which can efficiently solve a maximum causal Tsallis entropy problem in a continuous action space since the Tsallis entropy of a mixture of Gaussians encourages exploration and efficient mixture utilization",
        "From the analysis and experiments, we have shown that the proposed MCTEIL method is an efficient and principled way to learn the multi-modal behavior of an expert"
    ],
    "summary": [
        "Markov Decision Processes Markov decision processes (MDPs) are a well-known mathematical framework for a sequential decision making problem.",
        "We propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning, which can learn from a uni-modal to multi-modal policy distribution by adjusting its supporting set.",
        "The maximum causal entropy (MCE) framework [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] has been proposed to model stochastic behavior in demonstrations, where the optimal policy follows a softmax distribution.",
        "To apply the MCTE framework to a complex and model-free problem, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) with a sparse mixture density network whose mixture weights are modeled as a sparsemax distribution.",
        "Since existing IRL methods, including GAIL, are often based on the maximum causal entropy, they model the expert\u2019s policy using a softmax distribution, which can assign non-zero probability to non-expert actions in a discrete action space.",
        "We formulate maximum causal Tsallis entropy imitation learning (MCTEIL) and show that MCTE induces a sparse and multi-modal distribution which has an adaptable supporting set.",
        "The optimality conditions of the problem (6) tell us that the optimal policy is a sparsemax distribution which assigns zero probability to an action whose auxiliary variable qsa is below the threshold \u03c4 , Algorithm 1 Maximum Causal Tsallis Entropy Imitation Learning",
        "We propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm to solve a model-free IL problem in a continuous action space.",
        "To apply the MCTE framework for a continuous space and model-free case, we follow the extension of GAIL [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], which trains a policy and reward alternatively, instead of solving RL at every iteration.",
        "We test MCTEIL with a sparse MDN on MuJoCo [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], which is a physics-based simulator, using Halfcheetah, Walker2d, Reacher, and Ant. We train the expert policy distribution using trust region policy optimization (TRPO) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] under the true reward function and generate 50 demonstrations from the expert policy.",
        "Since the maximum Tsallis entropy makes each mixture mean explore in different directions and a sparsemax distribution assigns zero weight to unnecessary mixture components, MCTEIL efficiently explores and shows better performance compared to soft GAIL with a soft MDN.",
        "We have proposed a novel maximum causal Tsallis entropy (MCTE) framework, which induces a sparsemax distribution as the optimal solution.",
        "We have developed the maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm, which can efficiently solve a MCTE problem in a continuous action space since the Tsallis entropy of a mixture of Gaussians encourages exploration and efficient mixture utilization.",
        "From the analysis and experiments, we have shown that the proposed MCTEIL method is an efficient and principled way to learn the multi-modal behavior of an expert"
    ],
    "headline": "We propose a novel maximum causal Tsallis entropy  framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, \u201cMaximum entropy inverse reinforcement learning,\u201d in Proceedings of the 23rd International Conference on Artificial Intelligence, July 2008, pp. 1433\u20131438.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%2C%202008-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%2C%202008-07"
        },
        {
            "id": "2",
            "entry": "[2] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, \u201cReinforcement learning with deep energybased policies,\u201d in Proceedings of the 34th International Conference on Machine Learning, August 2017, pp. 1352\u20131361.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20T.%20Tang%2C%20H.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Reinforcement%20learning%20with%20deep%20energybased%20policies%2C%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20T.%20Tang%2C%20H.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Reinforcement%20learning%20with%20deep%20energybased%20policies%2C%202017-08"
        },
        {
            "id": "3",
            "entry": "[3] K. Lee, S. Choi, and S. Oh, \u201cSparse Markov decision processes with causal sparse Tsallis entropy regularization for reinforcement learning,\u201d IEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1466\u20131473, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20K.%20Choi%2C%20S.%20Oh%2C%20S.%20Sparse%20Markov%20decision%20processes%20with%20causal%20sparse%20Tsallis%20entropy%20regularization%20for%20reinforcement%20learning%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20K.%20Choi%2C%20S.%20Oh%2C%20S.%20Sparse%20Markov%20decision%20processes%20with%20causal%20sparse%20Tsallis%20entropy%20regularization%20for%20reinforcement%20learning%2C%202018"
        },
        {
            "id": "4",
            "entry": "[4] N. Heess, D. Silver, and Y. W. Teh, \u201cActor-critic reinforcement learning with energy-based policies,\u201d in Proceedings of the 10th European Workshop on Reinforcement Learning, June 2012, pp. 43\u201358.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heess%2C%20N.%20Silver%2C%20D.%20Teh%2C%20Y.W.%20Actor-critic%20reinforcement%20learning%20with%20energy-based%20policies%2C%202012-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heess%2C%20N.%20Silver%2C%20D.%20Teh%2C%20Y.W.%20Actor-critic%20reinforcement%20learning%20with%20energy-based%20policies%2C%202012-06"
        },
        {
            "id": "5",
            "entry": "[5] P. Vamplew, R. Dazeley, and C. Foale, \u201cSoftmax exploration strategies for multiobjective reinforcement learning,\u201d Neurocomputing, vol. 263, pp. 74\u201386, Jun 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vamplew%2C%20P.%20Dazeley%2C%20R.%20Foale%2C%20C.%20Softmax%20exploration%20strategies%20for%20multiobjective%20reinforcement%20learning%2C%202017-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vamplew%2C%20P.%20Dazeley%2C%20R.%20Foale%2C%20C.%20Softmax%20exploration%20strategies%20for%20multiobjective%20reinforcement%20learning%2C%202017-06"
        },
        {
            "id": "6",
            "entry": "[6] A. F. T. Martins and R. F. Astudillo, \u201cFrom softmax to sparsemax: A sparse model of attention and multi-label classification,\u201d in Proceedings of the 33nd International Conference on Machine Learning, June 2016, pp. 1614\u20131623.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martins%2C%20A.F.T.%20Astudillo%2C%20R.F.%20From%20softmax%20to%20sparsemax%3A%20A%20sparse%20model%20of%20attention%20and%20multi-label%20classification%2C%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martins%2C%20A.F.T.%20Astudillo%2C%20R.F.%20From%20softmax%20to%20sparsemax%3A%20A%20sparse%20model%20of%20attention%20and%20multi-label%20classification%2C%202016-06"
        },
        {
            "id": "7",
            "entry": "[7] M. Bloem and N. Bambos, \u201cInfinite time horizon maximum causal entropy inverse reinforcement learning,\u201d in Proceedings of the 53rd International Conference on Decision and Control, December 2014, pp. 4911\u20134916.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bloem%2C%20M.%20Bambos%2C%20N.%20Infinite%20time%20horizon%20maximum%20causal%20entropy%20inverse%20reinforcement%20learning%2C%202014-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bloem%2C%20M.%20Bambos%2C%20N.%20Infinite%20time%20horizon%20maximum%20causal%20entropy%20inverse%20reinforcement%20learning%2C%202014-12"
        },
        {
            "id": "8",
            "entry": "[8] Y. Chow, O. Nachum, and M. Ghavamzadeh, \u201cPath consistency learning in tsallis entropy regularized mdps,\u201d in Proceedings of the International Conference on Machine Learning, July 2018, pp. 978\u2013987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20Y.%20Nachum%2C%20O.%20Ghavamzadeh%2C%20M.%20Path%20consistency%20learning%20in%20tsallis%20entropy%20regularized%20mdps%2C%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20Y.%20Nachum%2C%20O.%20Ghavamzadeh%2C%20M.%20Path%20consistency%20learning%20in%20tsallis%20entropy%20regularized%20mdps%2C%202018-07"
        },
        {
            "id": "9",
            "entry": "[9] J. Ho and S. Ermon, \u201cGenerative adversarial imitation learning,\u201d in Advances in Neural Information Processing Systems, December 2016, pp. 4565\u20134573.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%2C%202016-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%2C%202016-12"
        },
        {
            "id": "10",
            "entry": "[10] E. Todorov, T. Erez, and Y. Tassa, \u201cMuJoCo: A physics engine for model-based control,\u201d in Proceedings of the International Conference on Intelligent Robots and Systems, October 2012, pp. 5026\u20135033.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20MuJoCo%3A%20A%20physics%20engine%20for%20model-based%20control%2C%202012-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20MuJoCo%3A%20A%20physics%20engine%20for%20model-based%20control%2C%202012-10"
        },
        {
            "id": "11",
            "entry": "[11] P. Abbeel and A. Y. Ng, \u201cApprenticeship learning via inverse reinforcement learning,\u201d in Proceedings of the 21st International Conference of Machine Learning, July 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%2C%202004-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%2C%202004-07"
        },
        {
            "id": "12",
            "entry": "[12] N. D. Ratliff, J. A. Bagnell, and M. Zinkevich, \u201cMaximum margin planning,\u201d in Proc. of the 23rd International Conference on Machine learning, June 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ratliff%2C%20N.D.%20Bagnell%2C%20J.A.%20Zinkevich%2C%20M.%20Maximum%20margin%20planning%2C%202006-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ratliff%2C%20N.D.%20Bagnell%2C%20J.A.%20Zinkevich%2C%20M.%20Maximum%20margin%20planning%2C%202006-06"
        },
        {
            "id": "13",
            "entry": "[13] D. Ramachandran and E. Amir, \u201cBayesian inverse reinforcement learning,\u201d in Proceedings of the 20th International Joint Conference on Artificial Intelligence, January 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramachandran%2C%20D.%20Amir%2C%20E.%20Bayesian%20inverse%20reinforcement%20learning%2C%202007-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramachandran%2C%20D.%20Amir%2C%20E.%20Bayesian%20inverse%20reinforcement%20learning%2C%202007-01"
        },
        {
            "id": "14",
            "entry": "[14] S. Levine, Z. Popovic, and V. Koltun, \u201cNonlinear inverse reinforcement learning with gaussian processes,\u201d in Advances in Neural Information Processing Systems, 2011, pp. 19\u201327.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20S.%20Popovic%2C%20Z.%20Koltun%2C%20V.%20Nonlinear%20inverse%20reinforcement%20learning%20with%20gaussian%20processes%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20S.%20Popovic%2C%20Z.%20Koltun%2C%20V.%20Nonlinear%20inverse%20reinforcement%20learning%20with%20gaussian%20processes%2C%202011"
        },
        {
            "id": "15",
            "entry": "[15] J. Zheng, S. Liu, and L. M. Ni, \u201cRobust bayesian inverse reinforcement learning with sparse behavior noise,\u201d in Proc. of the 28th AAAI Conference on Artificial Intelligence. AAAI Press, July 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20J.%20Liu%2C%20S.%20Ni%2C%20L.M.%20Robust%20bayesian%20inverse%20reinforcement%20learning%20with%20sparse%20behavior%20noise%2C%202014-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20J.%20Liu%2C%20S.%20Ni%2C%20L.M.%20Robust%20bayesian%20inverse%20reinforcement%20learning%20with%20sparse%20behavior%20noise%2C%202014-07"
        },
        {
            "id": "16",
            "entry": "[16] J. Choi and K.-E. Kim, \u201cHierarchical bayesian inverse reinforcement learning,\u201d Cybernetics, IEEE Transactions on, vol. 45, no. 4, pp. 793\u2013805, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20J.%20Kim%2C%20K.-E.%20Hierarchical%20bayesian%20inverse%20reinforcement%20learning%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20J.%20Kim%2C%20K.-E.%20Hierarchical%20bayesian%20inverse%20reinforcement%20learning%2C%202015"
        },
        {
            "id": "17",
            "entry": "[17] J. Choi and K. Kim, \u201cBayesian nonparametric feature construction for inverse reinforcement learning,\u201d in Proceedings of the 23rd International Joint Conference on Artificial Intelligence. IJCAI/AAAI, August 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20J.%20Kim%2C%20K.%20Bayesian%20nonparametric%20feature%20construction%20for%20inverse%20reinforcement%20learning%2C%202013-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20J.%20Kim%2C%20K.%20Bayesian%20nonparametric%20feature%20construction%20for%20inverse%20reinforcement%20learning%2C%202013-08"
        },
        {
            "id": "18",
            "entry": "[18] M. Wulfmeier, P. Ondruska, and I. Posner, \u201cMaximum entropy deep inverse reinforcement learning,\u201d arXiv preprint arXiv:1507.04888, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.04888"
        },
        {
            "id": "19",
            "entry": "[19] K. Hausman, Y. Chebotar, S. Schaal, G. S. Sukhatme, and J. J. Lim, \u201cMulti-modal imitation learning from unstructured demonstrations using generative adversarial nets,\u201d in Advances in Neural Information Processing Systems, December 2017, pp. 1235\u20131245.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausman%2C%20K.%20Chebotar%2C%20Y.%20Schaal%2C%20S.%20Sukhatme%2C%20G.S.%20Multi-modal%20imitation%20learning%20from%20unstructured%20demonstrations%20using%20generative%20adversarial%20nets%2C%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausman%2C%20K.%20Chebotar%2C%20Y.%20Schaal%2C%20S.%20Sukhatme%2C%20G.S.%20Multi-modal%20imitation%20learning%20from%20unstructured%20demonstrations%20using%20generative%20adversarial%20nets%2C%202017-12"
        },
        {
            "id": "20",
            "entry": "[20] Z. Wang, J. S. Merel, S. E. Reed, N. de Freitas, G. Wayne, and N. Heess, \u201cRobust imitation of diverse behaviors,\u201d in Advances in Neural Information Processing Systems, December 2017, pp. 5326\u20135335.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Merel%2C%20J.S.%20Reed%2C%20S.E.%20de%20Freitas%2C%20N.%20Robust%20imitation%20of%20diverse%20behaviors%2C%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Merel%2C%20J.S.%20Reed%2C%20S.E.%20de%20Freitas%2C%20N.%20Robust%20imitation%20of%20diverse%20behaviors%2C%202017-12"
        },
        {
            "id": "21",
            "entry": "[21] Y. Li, J. Song, and S. Ermon, \u201cInfogail: Interpretable imitation learning from visual demonstrations,\u201d in Advances in Neural Information Processing Systems, December 2017, pp. 3815\u20133825.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Song%2C%20J.%20Ermon%2C%20S.%20Infogail%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%2C%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Song%2C%20J.%20Ermon%2C%20S.%20Infogail%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%2C%202017-12"
        },
        {
            "id": "22",
            "entry": "[22] B. D. Ziebart, \u201cModeling purposeful adaptive behavior with the principle of maximum causal entropy,\u201d Ph.D. dissertation, Carnegie Mellon University, Pittsburgh, PA, USA, 2010, aAI3438449.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20B.D.%20Modeling%20purposeful%20adaptive%20behavior%20with%20the%20principle%20of%20maximum%20causal%20entropy%2C%202010"
        },
        {
            "id": "23",
            "entry": "[23] U. Syed and R. E. Schapire, \u201cA game-theoretic approach to apprenticeship learning,\u201d in Advances in neural information processing systems, December 2007, pp. 1449\u20131456.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syed%2C%20U.%20Schapire%2C%20R.E.%20%E2%80%9CA%20game-theoretic%20approach%20to%20apprenticeship%20learning%2C%E2%80%9D%20in%20Advances%20in%20neural%20information%20processing%20systems%202007-12"
        },
        {
            "id": "24",
            "entry": "[24] G. W. Brier, \u201cVerification of forecasts expressed in terms of probability,\u201d Monthey Weather Review, vol. 78, no. 1, pp. 1\u20133, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brier%2C%20G.W.%20Verification%20of%20forecasts%20expressed%20in%20terms%20of%20probability%2C%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brier%2C%20G.W.%20Verification%20of%20forecasts%20expressed%20in%20terms%20of%20probability%2C%201950"
        },
        {
            "id": "25",
            "entry": "[25] U. Syed, M. Bowling, and R. E. Schapire, \u201cApprenticeship learning using linear programming,\u201d in Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 1032\u20131039.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syed%2C%20U.%20Bowling%2C%20M.%20Schapire%2C%20R.E.%20Apprenticeship%20learning%20using%20linear%20programming%2C%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Syed%2C%20U.%20Bowling%2C%20M.%20Schapire%2C%20R.E.%20Apprenticeship%20learning%20using%20linear%20programming%2C%202008"
        },
        {
            "id": "26",
            "entry": "[26] M. L. Puterman, Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20M.L.%20Markov%20decision%20processes%3A%20discrete%20stochastic%20dynamic%20programming%202014"
        },
        {
            "id": "27",
            "entry": "[27] P. D. Gr\u00fcnwald and A. P. Dawid, \u201cGame theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory,\u201d Annals of Statistics, pp. 1367\u20131433, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%BCnwald%2C%20P.D.%20Dawid%2C%20A.P.%20Game%20theory%2C%20maximum%20entropy%2C%20minimum%20discrepancy%20and%20robust%20Bayesian%20decision%20theory%2C%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gr%C3%BCnwald%2C%20P.D.%20Dawid%2C%20A.P.%20Game%20theory%2C%20maximum%20entropy%2C%20minimum%20discrepancy%20and%20robust%20Bayesian%20decision%20theory%2C%202004"
        },
        {
            "id": "28",
            "entry": "[28] P. W. Millar, \u201cThe minimax principle in asymptotic statistical theory,\u201d in Ecole d\u2019Et\u00e9 de Probabilit\u00e9s de Saint-Flour XI\u20141981. Springer, 1983, pp. 75\u2013265.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Millar%2C%20P.W.%20%E2%80%9CThe%20minimax%20principle%20in%20asymptotic%20statistical%20theory%2C%E2%80%9D%20in%20Ecole%20d%E2%80%99Et%C3%A9%20de%20Probabilit%C3%A9s%20de%20Saint-Flour%20XI%E2%80%941981%201983"
        },
        {
            "id": "29",
            "entry": "[29] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz, \u201cTrust region policy optimization,\u201d in Proceedings of the 32nd International Conference on Machine Learning, July 2015, pp. 1889\u2013 1897. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.I.%20Trust%20region%20policy%20optimization%2C%202015-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.I.%20Trust%20region%20policy%20optimization%2C%202015-07"
        }
    ]
}
