{
    "filename": "7840-dirichlet-based-gaussian-processes-for-large-scale-calibrated-classification.pdf",
    "metadata": {
        "title": "Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification",
        "author": "Dimitrios Milios, Raffaello Camoriano, Pietro Michiardi, Lorenzo Rosasco, Maurizio Filippone",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7840-dirichlet-based-gaussian-processes-for-large-scale-calibrated-classification.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper studies the problem of deriving fast and accurate classification algorithms with uncertainty quantification. Gaussian process classification provides a principled approach, but the corresponding computational burden is hardly sustainable in large-scale problems and devising efficient alternatives is a challenge. In this work, we investigate if and how Gaussian process regression directly applied to classification labels can be used to tackle this question. While in this case training is remarkably faster, predictions need to be calibrated for classification and uncertainty estimation. To this aim, we propose a novel regression approach where the labels are obtained through the interpretation of classification labels as the coefficients of a degenerate Dirichlet distribution. Extensive experimental results show that the proposed approach provides essentially the same accuracy and uncertainty quantification as Gaussian process classification while requiring only a fraction of computational resources."
    },
    "keywords": [
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "gaussian processes",
            "url": "https://en.wikipedia.org/wiki/gaussian_processes"
        },
        {
            "term": "uncertainty quantification",
            "url": "https://en.wikipedia.org/wiki/uncertainty_quantification"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        }
    ],
    "highlights": [
        "Gaussian Processes classification is usually approached assuming a latent process, which is given a Gaussian Processes prior, that is transformed into a probability of class labels through a suitable squashing function [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "Calibration is assessed through the Expected Calibration Error (ECE) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], which is the average of the absolute difference between accuracy and confidence: Expected Calibration Error",
        "We consider an isotropic RBF kernel; the kernel hyper-parameters are selected by maximizing the marginal likelihood for the Gaussian Processes-based approaches, and by k-fold cross validation for Nystr\u00f6m KRR",
        "Most Gaussian Processes-based approaches to classification in the literature are characterized by a meticulous approximation of the likelihood",
        "We experimentally show that such Gaussian Processes classifiers tend to be well-calibrated, meaning that they correctly estimate classification uncertainty, as this is expressed through class probabilities",
        "Considering the strengths and practical limitations of Gaussian Processes, we proposed a classification approach that is essentially an heteroskedastic Gaussian Processes regression on a latent space induced by a transformation of the labels, which are viewed as Dirichlet-distributed random variables"
    ],
    "key_statements": [
        "Gaussian Processes classification is usually approached assuming a latent process, which is given a Gaussian Processes prior, that is transformed into a probability of class labels through a suitable squashing function [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "Calibration is assessed through the Expected Calibration Error (ECE) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], which is the average of the absolute difference between accuracy and confidence: Expected Calibration Error",
        "Other metrics used in this work to characterize the quality of a classifier are the error rate on the test set, and the mean negative log-likelihood (MNLL) of the test set under the classification model: mean negative log-likelihood",
        "It is straightforward to sample from the posterior log-Normal marginals, which should behave approximately as Gamma-distributed samples to generate posterior Dirichlet samples as in Equation (5), which corresponds to a simple application of the softmax function on the samples from the Gaussian Processes posterior",
        "We consider an isotropic RBF kernel; the kernel hyper-parameters are selected by maximizing the marginal likelihood for the Gaussian Processes-based approaches, and by k-fold cross validation for Nystr\u00f6m KRR",
        "Gaussian Processes-based machine learning approaches are known to be computationally expensive; their practical application on large datasets demands the use of scalable methods to perform approximate inference",
        "Most Gaussian Processes-based approaches to classification in the literature are characterized by a meticulous approximation of the likelihood",
        "We experimentally show that such Gaussian Processes classifiers tend to be well-calibrated, meaning that they correctly estimate classification uncertainty, as this is expressed through class probabilities",
        "Considering the strengths and practical limitations of Gaussian Processes, we proposed a classification approach that is essentially an heteroskedastic Gaussian Processes regression on a latent space induced by a transformation of the labels, which are viewed as Dirichlet-distributed random variables"
    ],
    "summary": [
        "GPC is usually approached assuming a latent process, which is given a GP prior, that is transformed into a probability of class labels through a suitable squashing function [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "It is well-known that if the observed labels are 0 and 1, the function f that minimizes the mean squared error converges to the true class probabilities in the limit of infinite data [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "Given a decision function f , which is the result of a trained binary classifier, the class probabilities are given by the sigmoid transformation \u03c0(x) = \u03c3 + b), where a and b are optimised over a separate validation set, so that the resulting model best explains the data.",
        "We propose to perform GP regression on labels transformed in such a way that a less crude approximation of the categorical likelihood is achieved.",
        "There is an obvious defect in GP-based least-squares classification: each point is associated with a Gaussian likelihood, which is not the appropriate noise model for Bernoulli-distributed variables.",
        "It is straightforward to sample from the posterior log-Normal marginals, which should behave approximately as Gamma-distributed samples to generate posterior Dirichlet samples as in Equation (5), which corresponds to a simple application of the softmax function on the samples from the GP posterior.",
        "We consider an isotropic RBF kernel; the kernel hyper-parameters are selected by maximizing the marginal likelihood for the GP-based approaches, and by k-fold cross validation for NKRR.",
        "GP-based machine learning approaches are known to be computationally expensive; their practical application on large datasets demands the use of scalable methods to perform approximate inference.",
        "We experimentally show that such GP classifiers tend to be well-calibrated, meaning that they correctly estimate classification uncertainty, as this is expressed through class probabilities.",
        "The crude approximation of a non-Gaussian likelihood with a Gaussian one has a negative impact on classification quality, especially as this is reflected by the calibration properties of the classifier.",
        "Considering the strengths and practical limitations of GPs, we proposed a classification approach that is essentially an heteroskedastic GP regression on a latent space induced by a transformation of the labels, which are viewed as Dirichlet-distributed random variables.",
        "This allowed us to convert C-class classification to a problem of regression involving C latent processes with Gamma likelihoods.",
        "We note that the predictive distribution of the GPD approach is different from that obtained by GPC, as can be seen in the extended results in the supplementary material."
    ],
    "headline": "This paper studies the problem of deriving fast and accurate classification algorithms with uncertainty quantification",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Asuncion and D. J. Newman. UCI machine learning repository, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asuncion%2C%20A.%20Newman%2C%20D.J.%20UCI%20machine%20learning%20repository%202007"
        },
        {
            "id": "2",
            "entry": "[2] L. Baldassarre, L. Rosasco, A. Barla, and A. Verri. Multi-output learning via spectral filtering. Machine learning, 87(3):259\u2013301, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldassarre%2C%20L.%20Rosasco%2C%20L.%20Barla%2C%20A.%20Verri%2C%20A.%20Multi-output%20learning%20via%20spectral%20filtering%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldassarre%2C%20L.%20Rosasco%2C%20L.%20Barla%2C%20A.%20Verri%2C%20A.%20Multi-output%20learning%20via%20spectral%20filtering%202012"
        },
        {
            "id": "3",
            "entry": "[3] P. Bartlett, M. Jordan, and J. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.%20Jordan%2C%20M.%20McAuliffe%2C%20J.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.%20Jordan%2C%20M.%20McAuliffe%2C%20J.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006"
        },
        {
            "id": "4",
            "entry": "[4] R. Camoriano, T. Angles, A. Rudi, and L. Rosasco. NYTRO: When Subsampling Meets Early Stopping. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 1403\u20131411, 2016. Springer US, Boston, MA, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camoriano%2C%20R.%20Angles%2C%20T.%20Rudi%2C%20A.%20Rosasco%2C%20L.%20NYTRO%3A%20When%20Subsampling%20Meets%20Early%20Stopping%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Camoriano%2C%20R.%20Angles%2C%20T.%20Rudi%2C%20A.%20Rosasco%2C%20L.%20NYTRO%3A%20When%20Subsampling%20Meets%20Early%20Stopping%202016"
        },
        {
            "id": "6",
            "entry": "[6] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On Calibration of Modern Neural Networks. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321\u20131330, International Convention Centre, Sydney, Australia, Aug. 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20C.%20Pleiss%2C%20G.%20Sun%2C%20Y.%20Weinberger%2C%20K.Q.%20On%20Calibration%20of%20Modern%20Neural%20Networks%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20C.%20Pleiss%2C%20G.%20Sun%2C%20Y.%20Weinberger%2C%20K.Q.%20On%20Calibration%20of%20Modern%20Neural%20Networks%202017-08"
        },
        {
            "id": "7",
            "entry": "[7] T. Hastie, R. Tibshirani, J. Friedman, and J. Franklin. The elements of statistical learning: data mining, inference and prediction. The Mathematical Intelligencer, 27(2):83\u201385, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20T.%20Tibshirani%2C%20R.%20Friedman%2C%20J.%20Franklin%2C%20J.%20The%20elements%20of%20statistical%20learning%3A%20data%20mining%2C%20inference%20and%20prediction%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastie%2C%20T.%20Tibshirani%2C%20R.%20Friedman%2C%20J.%20Franklin%2C%20J.%20The%20elements%20of%20statistical%20learning%3A%20data%20mining%2C%20inference%20and%20prediction%202001"
        },
        {
            "id": "8",
            "entry": "[8] J. Hensman, A. Matthews, and Z. Ghahramani. Scalable Variational Gaussian Process Classification. In G. Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages 351\u2013360, San Diego, California, USA, May 2015. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20PMLR%202015-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20PMLR%202015-05"
        },
        {
            "id": "9",
            "entry": "[9] J. Hensman, A. G. Matthews, M. Filippone, and Z. Ghahramani. MCMC for Variationally Sparse Gaussian Processes. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1648\u20131656. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Filippone%2C%20M.%20Ghahramani%2C%20Z.%20MCMC%20for%20Variationally%20Sparse%20Gaussian%20Processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Matthews%2C%20A.G.%20Filippone%2C%20M.%20Ghahramani%2C%20Z.%20MCMC%20for%20Variationally%20Sparse%20Gaussian%20Processes%202015"
        },
        {
            "id": "10",
            "entry": "[10] D. Hern\u00e1ndez-Lobato and J. M. Hern\u00e1ndez-Lobato. Scalable gaussian process classification via expectation propagation. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 168\u2013176. PMLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Scalable%20gaussian%20process%20classification%20via%20expectation%20propagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Scalable%20gaussian%20process%20classification%20via%20expectation%20propagation%202016"
        },
        {
            "id": "11",
            "entry": "[11] P. Huang, H. Avron, T. N. Sainath, V. Sindhwani, and B. Ramabhadran. Kernel methods match deep neural networks on TIMIT. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20P.%20Avron%2C%20H.%20Sainath%2C%20T.N.%20Sindhwani%2C%20V.%20Kernel%20methods%20match%20deep%20neural%20networks%20on%20TIMIT%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20P.%20Avron%2C%20H.%20Sainath%2C%20T.N.%20Sindhwani%2C%20V.%20Kernel%20methods%20match%20deep%20neural%20networks%20on%20TIMIT%202014"
        },
        {
            "id": "12",
            "entry": "[12] A. Kendall and Y. Gal. What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?, Mar. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20Deep%20Learning%20for%202017-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20Deep%20Learning%20for%202017-03"
        },
        {
            "id": "13",
            "entry": "[13] M. Kull, T. S. Filho, and P. Flach. Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers. In A. Singh and J. Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 623\u2013631, Fort Lauderdale, FL, USA, Apr. 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kull%2C%20M.%20Filho%2C%20T.S.%20Flach%2C%20P.%20Beta%20calibration%3A%20a%20well-founded%20and%20easily%20implemented%20improvement%20on%20logistic%20calibration%20for%20binary%20classifiers%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kull%2C%20M.%20Filho%2C%20T.S.%20Flach%2C%20P.%20Beta%20calibration%3A%20a%20well-founded%20and%20easily%20implemented%20improvement%20on%20logistic%20calibration%20for%20binary%20classifiers%202017-04"
        },
        {
            "id": "14",
            "entry": "[14] S. Kumar, M. Mohri, and A. Talwalkar. Ensemble Nystrom Method. In NIPS, pages 1060\u20131068. Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20S.%20Mohri%2C%20M.%20Talwalkar%2C%20A.%20Ensemble%20Nystrom%20Method%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20S.%20Mohri%2C%20M.%20Talwalkar%2C%20A.%20Ensemble%20Nystrom%20Method%202009"
        },
        {
            "id": "15",
            "entry": "[15] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world, Feb. 2017. arXiv:1607.02533.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "16",
            "entry": "[16] M. Kuss and C. E. Rasmussen. Assessing Approximate Inference for Binary Gaussian Process Classification. Journal of Machine Learning Research, 6:1679\u20131704, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuss%2C%20M.%20Rasmussen%2C%20C.E.%20Assessing%20Approximate%20Inference%20for%20Binary%20Gaussian%20Process%20Classification%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuss%2C%20M.%20Rasmussen%2C%20C.E.%20Assessing%20Approximate%20Inference%20for%20Binary%20Gaussian%20Process%20Classification%202005"
        },
        {
            "id": "17",
            "entry": "[17] Z. Lu, A. May, K. Liu, A. B. Garakani, D. Guo, A. Bellet, L. Fan, M. Collins, B. Kingsbury, M. Picheny, and F. Sha. How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets. CoRR, abs/1411.4000, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.4000"
        },
        {
            "id": "18",
            "entry": "[18] A. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. Le\u00f3n-Villagr\u00e1, Z. Ghahramani, and J. Hensman. GPflow: A Gaussian process library using TensorFlow. Journal of Machine Learning Research, 18(40):1\u20136, Apr. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthews%2C%20A.G.%20van%20der%20Wilk%2C%20M.%20Nickson%2C%20T.%20Fujii%2C%20K.%20GPflow%3A%20A%20Gaussian%20process%20library%20using%20TensorFlow%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matthews%2C%20A.G.%20van%20der%20Wilk%2C%20M.%20Nickson%2C%20T.%20Fujii%2C%20K.%20GPflow%3A%20A%20Gaussian%20process%20library%20using%20TensorFlow%202017-04"
        },
        {
            "id": "19",
            "entry": "[19] T. P. Minka. Expectation Propagation for approximate Bayesian inference. In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, UAI \u201901, pages 362\u2013369, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20T.P.%20Expectation%20Propagation%20for%20approximate%20Bayesian%20inference%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minka%2C%20T.P.%20Expectation%20Propagation%20for%20approximate%20Bayesian%20inference%202001"
        },
        {
            "id": "20",
            "entry": "[20] Y. Mroueh, T. Poggio, L. Rosasco, and J.-J. Slotine. Multiclass learning with simplex coding. In Advances in Neural Information Processing Systems, pages 2789\u20132797, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mroueh%2C%20Y.%20Poggio%2C%20T.%20Rosasco%2C%20L.%20Slotine%2C%20J.-J.%20Multiclass%20learning%20with%20simplex%20coding%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mroueh%2C%20Y.%20Poggio%2C%20T.%20Rosasco%2C%20L.%20Slotine%2C%20J.-J.%20Multiclass%20learning%20with%20simplex%20coding%202012"
        },
        {
            "id": "21",
            "entry": "[21] H. Nickisch and C. E. Rasmussen. Approximations for Binary Gaussian Process Classification. Journal of Machine Learning Research, 9:2035\u20132078, Oct. 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickisch%2C%20H.%20Rasmussen%2C%20C.E.%20Approximations%20for%20Binary%20Gaussian%20Process%20Classification%202008-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickisch%2C%20H.%20Rasmussen%2C%20C.E.%20Approximations%20for%20Binary%20Gaussian%20Process%20Classification%202008-10"
        },
        {
            "id": "22",
            "entry": "[22] A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22Nd International Conference on Machine Learning, ICML \u201905, pages 625\u2013632. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niculescu-Mizil%2C%20A.%20Caruana%2C%20R.%20Predicting%20good%20probabilities%20with%20supervised%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niculescu-Mizil%2C%20A.%20Caruana%2C%20R.%20Predicting%20good%20probabilities%20with%20supervised%20learning%202005"
        },
        {
            "id": "23",
            "entry": "[23] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural Comput., 21(3):786\u2013792, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opper%2C%20M.%20Archambeau%2C%20C.%20The%20variational%20gaussian%20approximation%20revisited%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Opper%2C%20M.%20Archambeau%2C%20C.%20The%20variational%20gaussian%20approximation%20revisited%202009"
        },
        {
            "id": "24",
            "entry": "[24] J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10(3), 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Platt%2C%20J.%20Probabilistic%20outputs%20for%20support%20vector%20machines%20and%20comparisons%20to%20regularized%20likelihood%20methods%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Platt%2C%20J.%20Probabilistic%20outputs%20for%20support%20vector%20machines%20and%20comparisons%20to%20regularized%20likelihood%20methods%201999"
        },
        {
            "id": "25",
            "entry": "[25] C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "26",
            "entry": "[26] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "27",
            "entry": "[27] R. Rifkin, G. Yeo, T. Poggio, and Others. Regularized least-squares classification. Nato Science Series Sub Series III Computer and Systems Sciences, 190:131\u2013154, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rifkin%2C%20R.%20Yeo%2C%20G.%20Poggio%2C%20T.%20Others%20Regularized%20least-squares%20classification%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rifkin%2C%20R.%20Yeo%2C%20G.%20Poggio%2C%20T.%20Others%20Regularized%20least-squares%20classification%202003"
        },
        {
            "id": "28",
            "entry": "[28] A. Rudi, R. Camoriano, and L. Rosasco. Less is More: Nystr\u00f6m Computational Regularization. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1657\u20131665. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20A.%20Camoriano%2C%20R.%20Rosasco%2C%20L.%20Less%20is%20More%3A%20Nystr%C3%B6m%20Computational%20Regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20A.%20Camoriano%2C%20R.%20Rosasco%2C%20L.%20Less%20is%20More%3A%20Nystr%C3%B6m%20Computational%20Regularization%202015"
        },
        {
            "id": "29",
            "entry": "[29] A. Rudi, L. Carratino, and L. Rosasco. FALKON: An Optimal Large Scale Kernel Method. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3888\u20133898. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20A.%20Carratino%2C%20L.%20Rosasco%2C%20L.%20FALKON%3A%20An%20Optimal%20Large%20Scale%20Kernel%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20A.%20Carratino%2C%20L.%20Rosasco%2C%20L.%20FALKON%3A%20An%20Optimal%20Large%20Scale%20Kernel%202017"
        },
        {
            "id": "30",
            "entry": "[30] A. Rudi, L. Carratino, and L. Rosasco. Falkon: An optimal large scale kernel method. In Advances in Neural Information Processing Systems, pages 3891\u20133901, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20A.%20Carratino%2C%20L.%20Rosasco%2C%20L.%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20A.%20Carratino%2C%20L.%20Rosasco%2C%20L.%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017"
        },
        {
            "id": "31",
            "entry": "[31] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shawe-Taylor%2C%20J.%20Cristianini%2C%20N.%20Kernel%20Methods%20for%20Pattern%20Analysis%202004"
        },
        {
            "id": "32",
            "entry": "[32] S. Si, C.-J. Hsieh, and I. S. Dhillon. Memory Efficient Kernel Approximation. In ICML, volume 32 of JMLR Proceedings, pages 701\u2013709. JMLR.org, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Si%2C%20S.%20Hsieh%2C%20C.-J.%20Dhillon%2C%20I.S.%20Memory%20Efficient%20Kernel%20Approximation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Si%2C%20S.%20Hsieh%2C%20C.-J.%20Dhillon%2C%20I.S.%20Memory%20Efficient%20Kernel%20Approximation%202014"
        },
        {
            "id": "33",
            "entry": "[33] A. J. Smola and B. Sch\u00f6lkopf. Sparse Greedy Matrix Approximation for Machine Learning. In ICML, pages 911\u2013918. Morgan Kaufmann, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20A.J.%20Sch%C3%B6lkopf%2C%20B.%20Sparse%20Greedy%20Matrix%20Approximation%20for%20Machine%20Learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smola%2C%20A.J.%20Sch%C3%B6lkopf%2C%20B.%20Sparse%20Greedy%20Matrix%20Approximation%20for%20Machine%20Learning%202000"
        },
        {
            "id": "34",
            "entry": "[34] J. A. K. Suykens and J. Vandewalle. Least Squares Support Vector Machine Classifiers. Neural Process. Lett., 9(3):293\u2013300, June 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suykens%2C%20J.A.K.%20Vandewalle%2C%20J.%20Least%20Squares%20Support%20Vector%20Machine%20Classifiers%201999-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suykens%2C%20J.A.K.%20Vandewalle%2C%20J.%20Least%20Squares%20Support%20Vector%20Machine%20Classifiers%201999-06"
        },
        {
            "id": "35",
            "entry": "[35] M. K. Titsias. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In D. A. Dyk and M. Welling, editors, Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, AISTATS 2009, Clearwater Beach, Florida, USA, April 16-18, 2009, volume 5 of JMLR Proceedings, pages 567\u2013574. JMLR.org, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20M.K.%20Variational%20Learning%20of%20Inducing%20Variables%20in%20Sparse%20Gaussian%20Processes%202009-04-16",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20M.K.%20Variational%20Learning%20of%20Inducing%20Variables%20in%20Sparse%20Gaussian%20Processes%202009-04-16"
        },
        {
            "id": "36",
            "entry": "[36] C. K. I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20:1342\u20131351, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20C.K.I.%20Barber%2C%20D.%20Bayesian%20classification%20with%20Gaussian%20processes%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20C.K.I.%20Barber%2C%20D.%20Bayesian%20classification%20with%20Gaussian%20processes%201998"
        },
        {
            "id": "37",
            "entry": "[37] C. K. I. Williams and M. Seeger. Using the Nystr\u00f6m method to speed up kernel machines. In T. K. Leen, T. G. Dietterich, V. Tresp, T. K. Leen, T. G. Dietterich, and V. Tresp, editors, NIPS, pages 682\u2013688. MIT Press, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20C.K.I.%20Seeger%2C%20M.%20Using%20the%20Nystr%C3%B6m%20method%20to%20speed%20up%20kernel%20machines%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20C.K.I.%20Seeger%2C%20M.%20Using%20the%20Nystr%C3%B6m%20method%20to%20speed%20up%20kernel%20machines%202000"
        },
        {
            "id": "38",
            "entry": "[38] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing. Stochastic variational deep kernel learning. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, pages 2594\u20132602. Curran Associates Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.G.%20Hu%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20Stochastic%20variational%20deep%20kernel%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.G.%20Hu%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20Stochastic%20variational%20deep%20kernel%20learning%202016"
        },
        {
            "id": "39",
            "entry": "[39] B. Zadrozny and C. Elkan. Transforming Classifier Scores into Accurate Multiclass Probability Estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201902, pages 694\u2013699, New York, NY, USA, 2002. ACM. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zadrozny%2C%20B.%20Elkan%2C%20C.%20Transforming%20Classifier%20Scores%20into%20Accurate%20Multiclass%20Probability%20Estimates%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zadrozny%2C%20B.%20Elkan%2C%20C.%20Transforming%20Classifier%20Scores%20into%20Accurate%20Multiclass%20Probability%20Estimates%202002"
        }
    ]
}
