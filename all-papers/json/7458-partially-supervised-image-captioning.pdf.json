{
    "filename": "7458-partially-supervised-image-captioning.pdf",
    "metadata": {
        "title": "Partially-Supervised Image Captioning",
        "author": "Peter Anderson, Stephen Gould, Mark Johnson",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7458-partially-supervised-image-captioning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild \u2014 for example, as assistants for people with impaired vision \u2014 a much larger number and variety of visual concepts must be understood. To address this problem, we teach image captioning models new visual concepts from labeled images and object detection datasets. Since image labels and object classes can be interpreted as partial captions, we formulate this problem as learning from partiallyspecified sequence data. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores."
    },
    "keywords": [
        {
            "term": "Convolutional Neural Network",
            "url": "https://en.wikipedia.org/wiki/Convolutional_Neural_Network"
        },
        {
            "term": "expectation maximization",
            "url": "https://en.wikipedia.org/wiki/expectation_maximization"
        },
        {
            "term": "finite state automaton",
            "url": "https://en.wikipedia.org/wiki/finite_state_automaton"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "COCO",
            "url": "https://en.wikipedia.org/wiki/COCO"
        }
    ],
    "highlights": [
        "The task of automatically generating image descriptions, i.e., image captioning [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], is a longstanding and challenging problem in artificial intelligence that demands both visual and linguistic understanding",
        "Despite continual improvements to image captioning models and ever-improving COCO caption evaluation scores [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>\u2013<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], captioning models trained on these datasets fail to generalize to images in the wild [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "Given training data where the captions are either complete sequences or finite state automaton representing partiallyspecified sequences, we propose a novel two-step algorithm inspired by expectation maximization (EM) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] to learn the parameters of a sequence model such as a recurrent neural network (RNN) which we will use to generate complete sequences at test time",
        "Consistent with previous work [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>\u2013<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], we evaluate our approach on the COCO novel object captioning splits in which all mentions of eight selected object classes have been eliminated from the caption training data",
        "Applying this approach to image captioning, we demonstrate that a generic image captioning model can learn new visual concepts from labeled images, achieving state of the art results on the COCO novel object captioning splits",
        "We further show that we can train the model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores"
    ],
    "key_statements": [
        "The task of automatically generating image descriptions, i.e., image captioning [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], is a longstanding and challenging problem in artificial intelligence that demands both visual and linguistic understanding",
        "Despite continual improvements to image captioning models and ever-improving COCO caption evaluation scores [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>\u2013<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], captioning models trained on these datasets fail to generalize to images in the wild [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "Given training data where the captions are either complete sequences or finite state automaton representing partiallyspecified sequences, we propose a novel two-step algorithm inspired by expectation maximization (EM) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] to learn the parameters of a sequence model such as a recurrent neural network (RNN) which we will use to generate complete sequences at test time",
        "Consistent with previous work [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>\u2013<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], we evaluate our approach on the COCO novel object captioning splits in which all mentions of eight selected object classes have been eliminated from the caption training data",
        "We propose PS3, a novel algorithm for training sequence models such as Recurrent Neural Network on partially-specified sequences represented by finite state automaton",
        "In contrast to the specialized architectures previously proposed for handling novel objects [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>\u2013<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], we present a general approach to training sequence models on partially-specified data that uses constrained beam search [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] as a subroutine",
        "For constrained beam search decoding, image label predictions are generated by a linear mapping from the mean-pooled image feature\n1 k k i=1 vi to image label scores which is trained on the entire training set",
        "Applying this approach to image captioning, we demonstrate that a generic image captioning model can learn new visual concepts from labeled images, achieving state of the art results on the COCO novel object captioning splits",
        "We further show that we can train the model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores",
        "Future work could investigate training captioning models on finite state automata constructed from scene graph and visual relationship annotations, which are available at large scale [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>]"
    ],
    "summary": [
        "The task of automatically generating image descriptions, i.e., image captioning [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], is a longstanding and challenging problem in artificial intelligence that demands both visual and linguistic understanding.",
        "To train image captioning models on object detections and labeled images, we formulate the problem as learning from partially-specified sequence data.",
        "Given training data where the captions are either complete sequences or FSA representing partiallyspecified sequences, we propose a novel two-step algorithm inspired by expectation maximization (EM) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] to learn the parameters of a sequence model such as a recurrent neural network (RNN) which we will use to generate complete sequences at test time.",
        "2www.panderson.me/constrained-beam-search neural slot-filling [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] have been proposed to incorporate novel word predictions from an image classifier into the output of a captioning model.",
        "In contrast to the specialized architectures previously proposed for handling novel objects [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>\u2013<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], we present a general approach to training sequence models on partially-specified data that uses constrained beam search [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] as a subroutine.",
        "Dataset splits To evaluate our proposed approach, we use the COCO 2014 captions dataset [<a class=\"ref-link\" id=\"c52\" href=\"#r52\">52</a>] containing 83K training images and 41K validation images, each labeled with five human-annotated",
        "In the experimental procedure proposed by Hendricks et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] and followed by others [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], this auxiliary information is provided in the form of image labels corresponding to the 471 most common adjective, verb and noun base word forms extracted from the held-out training captions.",
        "Results In Table 1 we show validation set results for the Up-Down model with various combinations of PS3 training and constrained beam search decoding, as well as performance upper bounds using ground-truth data.",
        "The model trained with PS3 has assimilated all the information available from the external image labeler, such that using constrained beam search during decoding provides no additional benefit.",
        "Evaluating our model on the test set, we achieve state of the art results on the COCO novel object captioning task, as illustrated in Table 2.",
        "Our primary motivation in this work is to extend the visual vocabulary of existing captioning models by making large object detection datasets available for training.",
        "We train a captioning model simultaneously on COCO Captions [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] and object annotation labels for 25 additional animal classes from the Open Images V4 dataset [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "We propose a novel algorithm for training sequence models on partially-specified data represented by finite state automata.",
        "Applying this approach to image captioning, we demonstrate that a generic image captioning model can learn new visual concepts from labeled images, achieving state of the art results on the COCO novel object captioning splits.",
        "Future work could investigate training captioning models on finite state automata constructed from scene graph and visual relationship annotations, which are available at large scale [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>]"
    ],
    "headline": "We propose a novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Toshev%2C%20Alexander%20Bengio%2C%20Samy%20Erhan%2C%20Dumitru%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Toshev%2C%20Alexander%20Bengio%2C%20Samy%20Erhan%2C%20Dumitru%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015"
        },
        {
            "id": "2",
            "entry": "[2] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelvin%20Xu%20Jimmy%20Ba%20Ryan%20Kiros%20Kyunghyun%20Cho%20Aaron%20C%20Courville%20Ruslan%20Salakhutdinov%20Richard%20S%20Zemel%20and%20Yoshua%20Bengio%20Show%20attend%20and%20tell%20Neural%20image%20caption%20generation%20with%20visual%20attention%20In%20ICML%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kelvin%20Xu%20Jimmy%20Ba%20Ryan%20Kiros%20Kyunghyun%20Cho%20Aaron%20C%20Courville%20Ruslan%20Salakhutdinov%20Richard%20S%20Zemel%20and%20Yoshua%20Bengio%20Show%20attend%20and%20tell%20Neural%20image%20caption%20generation%20with%20visual%20attention%20In%20ICML%202015"
        },
        {
            "id": "3",
            "entry": "[3] Hao Fang, Saurabh Gupta, Forrest N. Iandola, Rupesh Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, and Geoffrey Zweig. From captions to visual concepts and back. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fang%2C%20Hao%20Gupta%2C%20Saurabh%20Iandola%2C%20Forrest%20N.%20Srivastava%2C%20Rupesh%20From%20captions%20to%20visual%20concepts%20and%20back%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fang%2C%20Hao%20Gupta%2C%20Saurabh%20Iandola%2C%20Forrest%20N.%20Srivastava%2C%20Rupesh%20From%20captions%20to%20visual%20concepts%20and%20back%202015"
        },
        {
            "id": "4",
            "entry": "[4] Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47: 853\u2013899, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hodosh%2C%20Micah%20Young%2C%20Peter%20Hockenmaier%2C%20Julia%20Framing%20image%20description%20as%20a%20ranking%20task%3A%20Data%2C%20models%20and%20evaluation%20metrics%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hodosh%2C%20Micah%20Young%2C%20Peter%20Hockenmaier%2C%20Julia%20Framing%20image%20description%20as%20a%20ranking%20task%3A%20Data%2C%20models%20and%20evaluation%20metrics%202013"
        },
        {
            "id": "5",
            "entry": "[5] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Young%2C%20Peter%20Lai%2C%20Alice%20Hodosh%2C%20Micah%20Hockenmaier%2C%20Julia%20From%20image%20descriptions%20to%20visual%20denotations%3A%20New%20similarity%20metrics%20for%20semantic%20inference%20over%20event%20descriptions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Young%2C%20Peter%20Lai%2C%20Alice%20Hodosh%2C%20Micah%20Hockenmaier%2C%20Julia%20From%20image%20descriptions%20to%20visual%20denotations%3A%20New%20similarity%20metrics%20for%20semantic%20inference%20over%20event%20descriptions%202014"
        },
        {
            "id": "6",
            "entry": "[6] Xinlei Chen, Tsung-Yi Lin Hao Fang, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv preprint arXiv:1504.00325, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.00325"
        },
        {
            "id": "7",
            "entry": "[7] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Selfcritical sequence training for image captioning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rennie%2C%20Steven%20J.%20Marcheret%2C%20Etienne%20Mroueh%2C%20Youssef%20Ross%2C%20Jarret%20Selfcritical%20sequence%20training%20for%20image%20captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rennie%2C%20Steven%20J.%20Marcheret%2C%20Etienne%20Mroueh%2C%20Youssef%20Ross%2C%20Jarret%20Selfcritical%20sequence%20training%20for%20image%20captioning%202017"
        },
        {
            "id": "8",
            "entry": "[8] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Jiasen%20Xiong%2C%20Caiming%20Parikh%2C%20Devi%20Socher%2C%20Richard%20Knowing%20when%20to%20look%3A%20Adaptive%20attention%20via%20a%20visual%20sentinel%20for%20image%20captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Jiasen%20Xiong%2C%20Caiming%20Parikh%2C%20Devi%20Socher%2C%20Richard%20Knowing%20when%20to%20look%3A%20Adaptive%20attention%20via%20a%20visual%20sentinel%20for%20image%20captioning%202017"
        },
        {
            "id": "9",
            "entry": "[9] Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, and William W. Cohen. Review networks for caption generation. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zhilin%20Yuan%2C%20Ye%20Wu%2C%20Yuexin%20Salakhutdinov%2C%20Ruslan%20Review%20networks%20for%20caption%20generation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zhilin%20Yuan%2C%20Ye%20Wu%2C%20Yuexin%20Salakhutdinov%2C%20Ruslan%20Review%20networks%20for%20caption%20generation%202016"
        },
        {
            "id": "10",
            "entry": "[10] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018"
        },
        {
            "id": "11",
            "entry": "[11] Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea, Chris Thrasher, Chris Buehler, and Chris Sienkiewicz. Rich Image Captioning in the Wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kenneth%20Tran%20Xiaodong%20He%20Lei%20Zhang%20Jian%20Sun%20Cornelia%20Carapcea%20Chris%20Thrasher%20Chris%20Buehler%20and%20Chris%20Sienkiewicz%20Rich%20Image%20Captioning%20in%20the%20Wild%20In%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%20Workshops%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kenneth%20Tran%20Xiaodong%20He%20Lei%20Zhang%20Jian%20Sun%20Cornelia%20Carapcea%20Chris%20Thrasher%20Chris%20Buehler%20and%20Chris%20Sienkiewicz%20Rich%20Image%20Captioning%20in%20the%20Wild%20In%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%20Workshops%202016"
        },
        {
            "id": "12",
            "entry": "[12] Haley MacLeod, Cynthia L Bennett, Meredith Ringel Morris, and Edward Cutrell. Understanding blind people\u2019s experiences with computer-generated captions of social media images. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacLeod%2C%20Haley%20Bennett%2C%20Cynthia%20L.%20Morris%2C%20Meredith%20Ringel%20Cutrell%2C%20Edward%20Understanding%20blind%20people%E2%80%99s%20experiences%20with%20computer-generated%20captions%20of%20social%20media%20images%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacLeod%2C%20Haley%20Bennett%2C%20Cynthia%20L.%20Morris%2C%20Meredith%20Ringel%20Cutrell%2C%20Edward%20Understanding%20blind%20people%E2%80%99s%20experiences%20with%20computer-generated%20captions%20of%20social%20media%20images%202017"
        },
        {
            "id": "13",
            "entry": "[13] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided open vocabulary image captioning with constrained beam search. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Peter%20Fernando%2C%20Basura%20Johnson%2C%20Mark%20Gould%2C%20Stephen%20Guided%20open%20vocabulary%20image%20captioning%20with%20constrained%20beam%20search%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Peter%20Fernando%2C%20Basura%20Johnson%2C%20Mark%20Gould%2C%20Stephen%20Guided%20open%20vocabulary%20image%20captioning%20with%20constrained%20beam%20search%202017"
        },
        {
            "id": "14",
            "entry": "[14] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2017.",
            "url": "https://github.com/openimages",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krasin%2C%20Ivan%20Duerig%2C%20Tom%20Alldrin%2C%20Neil%20Ferrari%2C%20Vittorio%20Openimages%3A%20A%20public%20dataset%20for%20large-scale%20multi-label%20and%20multi-class%20image%20classification%202017"
        },
        {
            "id": "15",
            "entry": "[15] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "16",
            "entry": "[16] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: the new data in multimedia research. Communications of the ACM, 59(2):64\u201373, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomee%2C%20Bart%20Shamma%2C%20David%20A.%20Friedland%2C%20Gerald%20Elizalde%2C%20Benjamin%20YFCC100M%3A%20the%20new%20data%20in%20multimedia%20research%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomee%2C%20Bart%20Shamma%2C%20David%20A.%20Friedland%2C%20Gerald%20Elizalde%2C%20Benjamin%20YFCC100M%3A%20the%20new%20data%20in%20multimedia%20research%202016"
        },
        {
            "id": "17",
            "entry": "[17] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio Ferrari. Extreme clicking for efficient object annotation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papadopoulos%2C%20Dim%20P.%20Uijlings%2C%20Jasper%20R.R.%20Keller%2C%20Frank%20Ferrari%2C%20Vittorio%20Extreme%20clicking%20for%20efficient%20object%20annotation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papadopoulos%2C%20Dim%20P.%20Uijlings%2C%20Jasper%20R.R.%20Keller%2C%20Frank%20Ferrari%2C%20Vittorio%20Extreme%20clicking%20for%20efficient%20object%20annotation%202017"
        },
        {
            "id": "18",
            "entry": "[18] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio Ferrari. We don\u2019t need no bounding-boxes: Training object class detectors using only human verification. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papadopoulos%2C%20Dim%20P.%20Uijlings%2C%20Jasper%20R.R.%20Keller%2C%20Frank%20Ferrari%2C%20Vittorio%20We%20don%E2%80%99t%20need%20no%20bounding-boxes%3A%20Training%20object%20class%20detectors%20using%20only%20human%20verification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papadopoulos%2C%20Dim%20P.%20Uijlings%2C%20Jasper%20R.R.%20Keller%2C%20Frank%20Ferrari%2C%20Vittorio%20We%20don%E2%80%99t%20need%20no%20bounding-boxes%3A%20Training%20object%20class%20detectors%20using%20only%20human%20verification%202016"
        },
        {
            "id": "19",
            "entry": "[19] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977"
        },
        {
            "id": "20",
            "entry": "[20] Geoffrey J. McLachlan and Thriyambakam Krishnan. The EM algorithm and extensions. Wiley series in probability and statistics. Wiley, Hoboken, NJ, 2. ed edition, 2008. ISBN 978-0-471-20170-0.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%2C%20Geoffrey%20J.%20McLachlan%20Thriyambakam%20Krishnan.%20The%20EM%20algorithm%20and%20extensions.%20Wiley%20series%20in%20probability%20and%20statistics%202008"
        },
        {
            "id": "21",
            "entry": "[21] Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, and Trevor Darrell. Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hendricks%2C%20Lisa%20Anne%20Venugopalan%2C%20Subhashini%20Rohrbach%2C%20Marcus%20Mooney%2C%20Raymond%20Deep%20Compositional%20Captioning%3A%20Describing%20Novel%20Object%20Categories%20without%20Paired%20Training%20Data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hendricks%2C%20Lisa%20Anne%20Venugopalan%2C%20Subhashini%20Rohrbach%2C%20Marcus%20Mooney%2C%20Raymond%20Deep%20Compositional%20Captioning%3A%20Describing%20Novel%20Object%20Categories%20without%20Paired%20Training%20Data%202016"
        },
        {
            "id": "22",
            "entry": "[22] Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond J. Mooney, Trevor Darrell, and Kate Saenko. Captioning Images with Diverse Objects. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Venugopalan%2C%20Subhashini%20Hendricks%2C%20Lisa%20Anne%20Rohrbach%2C%20Marcus%20Mooney%2C%20Raymond%20J.%20Captioning%20Images%20with%20Diverse%20Objects%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Venugopalan%2C%20Subhashini%20Hendricks%2C%20Lisa%20Anne%20Rohrbach%2C%20Marcus%20Mooney%2C%20Raymond%20J.%20Captioning%20Images%20with%20Diverse%20Objects%202017"
        },
        {
            "id": "23",
            "entry": "[23] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Incorporating copying mechanism in image captioning for learning novel objects. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Ting%20Pan%2C%20Yingwei%20Li%2C%20Yehao%20Mei%2C%20Tao%20Incorporating%20copying%20mechanism%20in%20image%20captioning%20for%20learning%20novel%20objects%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Ting%20Pan%2C%20Yingwei%20Li%2C%20Yehao%20Mei%2C%20Tao%20Incorporating%20copying%20mechanism%20in%20image%20captioning%20for%20learning%20novel%20objects%202017"
        },
        {
            "id": "24",
            "entry": "[24] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiasen%20Lu%20Jianwei%20Yang%20Dhruv%20Batra%20and%20Devi%20Parikh%20Neural%20baby%20talk%20In%20CVPR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiasen%20Lu%20Jianwei%20Yang%20Dhruv%20Batra%20and%20Devi%20Parikh%20Neural%20baby%20talk%20In%20CVPR%202018"
        },
        {
            "id": "25",
            "entry": "[25] Jeffrey Donahue, Lisa A. Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20A.%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description.%20In%20CVPR%202015"
        },
        {
            "id": "26",
            "entry": "[26] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Junhua%20Xu%2C%20Wei%20Yang%2C%20Yi%20Wang%2C%20Jiang%20Deep%20captioning%20with%20multimodal%20recurrent%20neural%20networks%20%28m-rnn%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Junhua%20Xu%2C%20Wei%20Yang%2C%20Yi%20Wang%2C%20Jiang%20Deep%20captioning%20with%20multimodal%20recurrent%20neural%20networks%20%28m-rnn%29%202015"
        },
        {
            "id": "27",
            "entry": "[27] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Fei-Fei%2C%20Li%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Fei-Fei%2C%20Li%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015"
        },
        {
            "id": "28",
            "entry": "[28] Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. Language models for image captioning: The quirks and what works. arXiv preprint arXiv:1505.01809, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.01809"
        },
        {
            "id": "29",
            "entry": "[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017"
        },
        {
            "id": "30",
            "entry": "[30] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.0850"
        },
        {
            "id": "31",
            "entry": "[31] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "32",
            "entry": "[32] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "33",
            "entry": "[33] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Incorporating copying mechanism in image captioning for learning novel objects. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Ting%20Pan%2C%20Yingwei%20Li%2C%20Yehao%20Mei%2C%20Tao%20Incorporating%20copying%20mechanism%20in%20image%20captioning%20for%20learning%20novel%20objects%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Ting%20Pan%2C%20Yingwei%20Li%2C%20Yehao%20Mei%2C%20Tao%20Incorporating%20copying%20mechanism%20in%20image%20captioning%20for%20learning%20novel%20objects%202017"
        },
        {
            "id": "34",
            "entry": "[34] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015"
        },
        {
            "id": "35",
            "entry": "[35] Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.03483"
        },
        {
            "id": "36",
            "entry": "[36] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1301.3781"
        },
        {
            "id": "37",
            "entry": "[37] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global Vectors for Word Representation. In EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20GloVe%3A%20Global%20Vectors%20for%20Word%20Representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20GloVe%3A%20Global%20Vectors%20for%20Word%20Representation%202014"
        },
        {
            "id": "38",
            "entry": "[38] Shahla Parveen and Phil Green. Speech recognition with missing data using recurrent neural nets. In NIPS, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parveen%2C%20Shahla%20Green%2C%20Phil%20Speech%20recognition%20with%20missing%20data%20using%20recurrent%20neural%20nets%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parveen%2C%20Shahla%20Green%2C%20Phil%20Speech%20recognition%20with%20missing%20data%20using%20recurrent%20neural%20nets%202002"
        },
        {
            "id": "39",
            "entry": "[39] Zachary C. Lipton, David C. Kale, and Randall Wetzel. Modeling missing data in clinical time series with RNNs. In Machine Learning for Healthcare, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lipton%2C%20Zachary%20C.%20Kale%2C%20David%20C.%20Wetzel%2C%20Randall%20Modeling%20missing%20data%20in%20clinical%20time%20series%20with%20RNNs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lipton%2C%20Zachary%20C.%20Kale%2C%20David%20C.%20Wetzel%2C%20Randall%20Modeling%20missing%20data%20in%20clinical%20time%20series%20with%20RNNs%202016"
        },
        {
            "id": "40",
            "entry": "[40] Zoubin Ghahramani and Michael I Jordan. Supervised learning from incomplete data via an em approach. In NIPS, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Supervised%20learning%20from%20incomplete%20data%20via%20an%20em%20approach%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghahramani%2C%20Zoubin%20Jordan%2C%20Michael%20I.%20Supervised%20learning%20from%20incomplete%20data%20via%20an%20em%20approach%201994"
        },
        {
            "id": "41",
            "entry": "[41] Michael Sipser. Introduction to the Theory of Computation. Cengage Learning, 3rd edition, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sipser%2C%20Michael%20Introduction%20to%20the%20Theory%20of%20Computation.%20Cengage%20Learning%202012"
        },
        {
            "id": "42",
            "entry": "[42] Philipp Koehn. Statistical Machine Translation. Cambridge University Press, New York, NY, USA, 1st edition, 2010. ISBN 0521874157, 9780521874151.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koehn%2C%20Philipp%20Statistical%20Machine%20Translation%202010"
        },
        {
            "id": "43",
            "entry": "[43] Chris Hokamp and Qun Liu. Lexically constrained decoding for sequence generation using grid beam search. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hokamp%2C%20Chris%20Liu%2C%20Qun%20Lexically%20constrained%20decoding%20for%20sequence%20generation%20using%20grid%20beam%20search%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hokamp%2C%20Chris%20Liu%2C%20Qun%20Lexically%20constrained%20decoding%20for%20sequence%20generation%20using%20grid%20beam%20search%202017"
        },
        {
            "id": "44",
            "entry": "[44] Matt Post and David Vilar. Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. arXiv preprint arXiv:1804.06609, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06609"
        },
        {
            "id": "45",
            "entry": "[45] Kyle Richardson, Jonathan Berant, and Jonas Kuhn. Polyglot semantic parsing in APIs. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richardson%2C%20Kyle%20Berant%2C%20Jonathan%20Kuhn%2C%20Jonas%20Polyglot%20semantic%20parsing%20in%20APIs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richardson%2C%20Kyle%20Berant%2C%20Jonathan%20Kuhn%2C%20Jonas%20Polyglot%20semantic%20parsing%20in%20APIs%202018"
        },
        {
            "id": "46",
            "entry": "[46] Sam Wiseman and Alexander M Rush. Sequence-to-sequence learning as beam-search optimization. In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20M.%20Sequence-to-sequence%20learning%20as%20beam-search%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20M.%20Sequence-to-sequence%20learning%20as%20beam-search%20optimization%202016"
        },
        {
            "id": "47",
            "entry": "[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015"
        },
        {
            "id": "48",
            "entry": "[48] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "49",
            "entry": "[49] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07332"
        },
        {
            "id": "50",
            "entry": "[50] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long Short-Term Memory. Neural Computation, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20Short-Term%20Memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20Short-Term%20Memory%201997"
        },
        {
            "id": "51",
            "entry": "[51] Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In ACL, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Dependency-based%20word%20embeddings%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Omer%20Goldberg%2C%20Yoav%20Dependency-based%20word%20embeddings%202014"
        },
        {
            "id": "52",
            "entry": "[52] T.Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft COCO: Common Objects in Context. In ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Dollar%20and%20C%20L%20Zitnick%20Microsoft%20COCO%20Common%20Objects%20in%20Context%20In%20ECCV%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Dollar%20and%20C%20L%20Zitnick%20Microsoft%20COCO%20Common%20Objects%20in%20Context%20In%20ECCV%202014"
        },
        {
            "id": "53",
            "entry": "[53] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xian%2C%20Yongqin%20Schiele%2C%20Bernt%20Akata%2C%20Zeynep%20Zero-shot%20learning-the%20good%2C%20the%20bad%20and%20the%20ugly%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xian%2C%20Yongqin%20Schiele%2C%20Bernt%20Akata%2C%20Zeynep%20Zero-shot%20learning-the%20good%2C%20the%20bad%20and%20the%20ugly%202017"
        },
        {
            "id": "54",
            "entry": "[54] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: Semantic Propositional Image Caption Evaluation. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peter%20Anderson%20Basura%20Fernando%20Mark%20Johnson%20and%20Stephen%20Gould%20SPICE%20Semantic%20Propositional%20Image%20Caption%20Evaluation%20In%20ECCV%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peter%20Anderson%20Basura%20Fernando%20Mark%20Johnson%20and%20Stephen%20Gould%20SPICE%20Semantic%20Propositional%20Image%20Caption%20Evaluation%20In%20ECCV%202016"
        },
        {
            "id": "55",
            "entry": "[55] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image description evaluation. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramakrishna%20Vedantam%2C%20C.Lawrence%20Zitnick%20Parikh%2C%20Devi%20CIDEr%3A%20Consensus-based%20image%20description%20evaluation.%20In%20CVPR%202015"
        },
        {
            "id": "56",
            "entry": "[56] Alon Lavie and Abhaya Agarwal. Meteor: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL): Second Workshop on Statistical Machine Translation, 2007. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lavie%2C%20Alon%20Agarwal%2C%20Abhaya%20Meteor%3A%20An%20automatic%20metric%20for%20MT%20evaluation%20with%20high%20levels%20of%20correlation%20with%20human%20judgments%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lavie%2C%20Alon%20Agarwal%2C%20Abhaya%20Meteor%3A%20An%20automatic%20metric%20for%20MT%20evaluation%20with%20high%20levels%20of%20correlation%20with%20human%20judgments%202007"
        }
    ]
}
