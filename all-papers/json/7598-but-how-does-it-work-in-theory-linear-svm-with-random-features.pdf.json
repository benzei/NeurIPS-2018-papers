{
    "filename": "7598-but-how-does-it-work-in-theory-linear-svm-with-random-features.pdf",
    "metadata": {
        "title": "But How Does It Work in Theory? Linear SVM with Random Features",
        "author": "Yitong Sun, Anna Gilbert, Ambuj Tewari",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7598-but-how-does-it-work-in-theory-linear-svm-with-random-features.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We prove that, under low noise assumptions, the support vector machine with N \u221am random features (RFSVM) can achieve the learning rate faster than O(1/ m) on a training set with m samples when an optimized feature map is used. Our work extends the previous fast rate analysis of random features method from least square loss to 0-1 loss. We also show that the reweighted feature selection method, which approximates the optimized feature map, helps improve the performance of RFSVM in experiments on a synthetic data set."
    },
    "keywords": [
        {
            "term": "kernel method",
            "url": "https://en.wikipedia.org/wiki/kernel_method"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        }
    ],
    "highlights": [
        "Kernel methods such as kernel support vector machines (KSVMs) have been widely and successfully used in classification tasks (<a class=\"ref-link\" id=\"cSteinwart_2008_a\" href=\"#rSteinwart_2008_a\"><a class=\"ref-link\" id=\"cSteinwart_2008_a\" href=\"#rSteinwart_2008_a\">Steinwart and Christmann [2008</a></a>])",
        "We mainly considered two learning scenarios: the realizable case, and unrealizable case, where the Bayes classifier does not belong to the reproducing kernel Hilbert space of the feature map",
        "When the Bayes classifier satisfies the separation condition; that is, when the two classes of points are apart by a positive distance, we prove that the RFSVM using an optimized feature map corresponding to Gaussian kernel can achieve a learning rate of O(1/m) with O) number of features.\n3",
        "If we drop the assumption of optimized feature map, only weak results can be obtained for the learning rate and the number of features required",
        "According to Theorem 2, the benefit brought by optimized feature map, that is, the fast learning rate, will show up when the sample size is greater than O)",
        "Our study proves that the fast learning rate is possible for RFSVM in both realizable and unrealizable scenarios when the optimized feature map is available"
    ],
    "key_statements": [
        "Kernel methods such as kernel support vector machines (KSVMs) have been widely and successfully used in classification tasks (<a class=\"ref-link\" id=\"cSteinwart_2008_a\" href=\"#rSteinwart_2008_a\"><a class=\"ref-link\" id=\"cSteinwart_2008_a\" href=\"#rSteinwart_2008_a\">Steinwart and Christmann [2008</a></a>])",
        "We mainly considered two learning scenarios: the realizable case, and unrealizable case, where the Bayes classifier does not belong to the reproducing kernel Hilbert space of the feature map",
        "When the Bayes classifier satisfies the separation condition; that is, when the two classes of points are apart by a positive distance, we prove that the RFSVM using an optimized feature map corresponding to Gaussian kernel can achieve a learning rate of O(1/m) with O) number of features.\n3",
        "If we drop the assumption of optimized feature map, only weak results can be obtained for the learning rate and the number of features required",
        "According to Theorem 2, the benefit brought by optimized feature map, that is, the fast learning rate, will show up when the sample size is greater than O)",
        "Our study proves that the fast learning rate is possible for RFSVM in both realizable and unrealizable scenarios when the optimized feature map is available"
    ],
    "summary": [
        "Kernel methods such as kernel support vector machines (KSVMs) have been widely and successfully used in classification tasks (<a class=\"ref-link\" id=\"cSteinwart_2008_a\" href=\"#rSteinwart_2008_a\"><a class=\"ref-link\" id=\"cSteinwart_2008_a\" href=\"#rSteinwart_2008_a\">Steinwart and Christmann [2008</a></a>]).",
        "2. When the Bayes classifier satisfies the separation condition; that is, when the two classes of points are apart by a positive distance, we prove that the RFSVM using an optimized feature map corresponding to Gaussian kernel can achieve a learning rate of O(1/m) with O) number of features.",
        "<a class=\"ref-link\" id=\"cRudi_2017_a\" href=\"#rRudi_2017_a\"><a class=\"ref-link\" id=\"cRudi_2017_a\" href=\"#rRudi_2017_a\">Rudi and Rosasco [2017</a></a>] obtained a similar fast learning rate for kernel ridge regression with random features (RFKRR), assuming polynomial decay of the spectrum of \u03a3\u03c6 and the existence of a minimizer of the risk in F\u03c6.",
        "Massart\u2019s low noise condition guarantees that the random samples represent the distribution behind them accurately, while the separation condition guarantees the existence of a smooth, in the sense of small derivatives, function achieving the same risk with the Bayes classifier.",
        "\u221a \u03bb = 1/m \u03b3 = \u03c4 / ln m N = C\u03c4,d,\u03c1 ln2d m) , the RFSVM using an optimized feature map corresponding to the Gaussian kernel with bandwidth \u03b3 achieves the learning rate m",
        "The assumption of a bounded data set and a bounded distribution density function can be dropped if we assume that the probability density function is upper bounded by C exp(\u2212\u03b32 x 2/2), which suffices to provide the sub-exponential decay of spectrum of \u03a3\u03c6.",
        "If we drop the assumption of optimized feature map, only weak results can be obtained for the learning rate and the number of features required.",
        "<a class=\"ref-link\" id=\"cRudi_2017_a\" href=\"#rRudi_2017_a\"><a class=\"ref-link\" id=\"cRudi_2017_a\" href=\"#rRudi_2017_a\">Rudi and Rosasco [2017</a></a>] compared the performance of RFKRR with Nystrom method, which is the other popular method to scale kernel ridge regression to large data sets.We do not find any theoretical guarantees on the fast learning rate of SVM with Nystrom method on classification problems in the literature, though there are several works on its approximation quality to the accurate model and its empirical performance.",
        "According to Theorem 2, the benefit brought by optimized feature map, that is, the fast learning rate, will show up when the sample size is greater than O).",
        "Our study proves that the fast learning rate is possible for RFSVM in both realizable and unrealizable scenarios when the optimized feature map is available.",
        "Considering that such a reweighted method does not rely on the label distribution at all, it will be useful in learning scenarios where multiple classification problems share the same features but differ in the class labels.",
        "We believe that a theoretical guarantee of the performance of the reweighted feature selection method and properly understanding the dependence on the dimensionality of data are interesting directions for future work"
    ],
    "headline": "Under low noise assumptions, the support vector machine with N \u221am random features  can achieve the learning rate faster than O on a training set with m samples when an optimized feature map is used",
    "reference_links": [
        {
            "id": "Bach_2017_a",
            "entry": "Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions. Journal of Machine Learning Research, 18(21):1\u201338, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017"
        },
        {
            "id": "Cortes_et+al_2010_a",
            "entry": "Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the impact of kernel approximation on learning accuracy. Journal of Machine Learning Research, 9:113\u2013120, 2010. ISSN 1532-4435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Talwalkar%2C%20Ameet%20On%20the%20impact%20of%20kernel%20approximation%20on%20learning%20accuracy%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Talwalkar%2C%20Ameet%20On%20the%20impact%20of%20kernel%20approximation%20on%20learning%20accuracy%202010"
        },
        {
            "id": "Cucker_2002_a",
            "entry": "Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39:1\u201349, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cucker%2C%20Felipe%20Smale%2C%20Steve%20On%20the%20mathematical%20foundations%20of%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cucker%2C%20Felipe%20Smale%2C%20Steve%20On%20the%20mathematical%20foundations%20of%20learning%202002"
        },
        {
            "id": "Dai_et+al_2014_a",
            "entry": "Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3041\u20133049. Curran Associates, Inc., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Bo%20Xie%2C%20Bo%20He%2C%20Niao%20Liang%2C%20Yingyu%20Scalable%20kernel%20methods%20via%20doubly%20stochastic%20gradients%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Bo%20Xie%2C%20Bo%20He%2C%20Niao%20Liang%2C%20Yingyu%20Scalable%20kernel%20methods%20via%20doubly%20stochastic%20gradients%202014"
        },
        {
            "id": "Eric_et+al_2008_a",
            "entry": "Moulines Eric, Francis R Bach, and Za\u00efd Harchaoui. Testing for homogeneity with kernel Fisher discriminant analysis. In Advances in Neural Information Processing Systems, pages 609\u2013616, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eric%2C%20Moulines%20Bach%2C%20Francis%20R.%20Harchaoui%2C%20Za%C3%AFd%20Testing%20for%20homogeneity%20with%20kernel%20Fisher%20discriminant%20analysis%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eric%2C%20Moulines%20Bach%2C%20Francis%20R.%20Harchaoui%2C%20Za%C3%AFd%20Testing%20for%20homogeneity%20with%20kernel%20Fisher%20discriminant%20analysis%202008"
        },
        {
            "id": "Hsieh_et+al_2008_a",
            "entry": "Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 408\u2013415, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390208.",
            "crossref": "https://dx.doi.org/10.1145/1390156.1390208",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1390156.1390208"
        },
        {
            "id": "Huang_et+al_2014_a",
            "entry": "P. S. Huang, H. Avron, T. N. Sainath, V. Sindhwani, and B. Ramabhadran. Kernel methods match deep neural networks on timit. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 205\u2013209, May 2014. doi: 10.1109/ICASSP.2014.6853587.",
            "crossref": "https://dx.doi.org/10.1109/ICASSP.2014.6853587",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/ICASSP.2014.6853587"
        },
        {
            "id": "Koltchinskii_2011_a",
            "entry": "Vladimir. Koltchinskii, SpringerLink (Online service), and \u00c9cole d\u2019\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems \u00c9cole d\u2019\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVIII-2008. Lecture Notes in Mathematics,0075-8434 ;2033. Springer-Verlag Berlin Heidelberg, Berlin, Heidelberg, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20Vladimir%20SpringerLink%20%28Online%20service%29%2C%20and%20%C3%89cole%20d%E2%80%99%C3%89t%C3%A9%20de%20Probabilit%C3%A9s%20de%20Saint-Flour.%20Oracle%20Inequalities%20in%20Empirical%20Risk%20Minimization%20and%20Sparse%20Recovery%20Problems%20%C3%89cole%20d%E2%80%99%C3%89t%C3%A9%20de%20Probabilit%C3%A9s%20de%20Saint-Flour%20XXXVIII-2008.%20Lecture%20Notes%20in%20Mathematics%2C0075-8434%20%3B%202033%202011"
        },
        {
            "id": "Lax_2002_a",
            "entry": "P.D. Lax. Functional analysis. Pure and applied mathematics. Wiley, 2002. ISBN 9780471556046. URL https://books.google.com/books?id=-jbvAAAAMAAJ.",
            "url": "https://books.google.com/books?id=-jbvAAAAMAAJ"
        },
        {
            "id": "Rahimi_2008_a",
            "entry": "Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1177\u20131184. Curran Associates, Inc., 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "Rahimi_2009_a",
            "entry": "Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1313\u20131320. Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Weighted%20sums%20of%20random%20kitchen%20sinks%3A%20Replacing%20minimization%20with%20randomization%20in%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Weighted%20sums%20of%20random%20kitchen%20sinks%3A%20Replacing%20minimization%20with%20randomization%20in%20learning%202009"
        },
        {
            "id": "Rudi_2017_a",
            "entry": "Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pages 3218\u20133228, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "Scovel_et+al_2010_a",
            "entry": "Clint Scovel, Don Hush, Ingo Steinwart, and James Theiler. Radial kernels and their reproducing kernel hilbert spaces. Journal of Complexity, 26(6):641\u2013660, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scovel%2C%20Clint%20Hush%2C%20Don%20Steinwart%2C%20Ingo%20Theiler%2C%20James%20Radial%20kernels%20and%20their%20reproducing%20kernel%20hilbert%20spaces%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scovel%2C%20Clint%20Hush%2C%20Don%20Steinwart%2C%20Ingo%20Theiler%2C%20James%20Radial%20kernels%20and%20their%20reproducing%20kernel%20hilbert%20spaces%202010"
        },
        {
            "id": "Shalev-Shwartz_et+al_2011_a",
            "entry": "Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: primal estimated sub-gradient solver for svm. Mathematical Programming, 127(1):3\u201330, 2011. ISSN 1436-4646. doi: 10.1007/s10107-010-0420-4.",
            "crossref": "https://dx.doi.org/10.1007/s10107-010-0420-4",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10107-010-0420-4"
        },
        {
            "id": "Sriperumbudur_2015_a",
            "entry": "Bharath Sriperumbudur and Zoltan Szabo. Optimal rates for random fourier features. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1144\u2013 1152. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5740-optimal-rates-for-random-fourier-features.pdf.",
            "url": "http://papers.nips.cc/paper/5740-optimal-rates-for-random-fourier-features.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sriperumbudur%2C%20Bharath%20Szabo%2C%20Zoltan%20Optimal%20rates%20for%20random%20fourier%20features%202015"
        },
        {
            "id": "Steinwart_2008_a",
            "entry": "I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics. Springer New York, 2008. ISBN 9780387772424.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20I.%20Christmann%2C%20A.%20Support%20Vector%20Machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20I.%20Christmann%2C%20A.%20Support%20Vector%20Machines%202008"
        },
        {
            "id": "Sutherland_2015_a",
            "entry": "Dougal J. Sutherland and Jeff G. Schneider. On the error of random fourier features. CoRR, abs/1506.02785, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02785"
        },
        {
            "id": "Widom_1963_a",
            "entry": "Harold Widom. Asymptotic behavior of the eigenvalues of certain integral equations. Transactions of the American Mathematical Society, 109(2):278\u2013295, 1963. ISSN 00029947. URL http://www.jstor.org/stable/1993907.",
            "url": "http://www.jstor.org/stable/1993907",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Widom%2C%20Harold%20Asymptotic%20behavior%20of%20the%20eigenvalues%20of%20certain%20integral%20equations%201963"
        },
        {
            "id": "Yang_et+al_2012_a",
            "entry": "Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 476\u2013484. Curran Associates, Inc., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Tianbao%20Li%2C%20Yu-feng%20Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Nystr%C3%B6m%20method%20vs%20random%20fourier%20features%3A%20A%20theoretical%20and%20empirical%20comparison%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Tianbao%20Li%2C%20Yu-feng%20Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Nystr%C3%B6m%20method%20vs%20random%20fourier%20features%3A%20A%20theoretical%20and%20empirical%20comparison%202012"
        },
        {
            "id": "Zhang_et+al_2012_a",
            "entry": "Kai Zhang, Liang Lan, Zhuang Wang, and Fabian Moerchen. Scaling up kernel svm on limited resources: A low-rank linearization approach. In Neil D. Lawrence and Mark Girolami, editors, Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, volume 22 of Proceedings of Machine Learning Research, pages 1425\u20131434, La Palma, Canary Islands, 21\u201323 Apr 2012. PMLR. URL http://proceedings.mlr.press/v22/zhang12d.html.",
            "url": "http://proceedings.mlr.press/v22/zhang12d.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Kai%20Lan%2C%20Liang%20Wang%2C%20Zhuang%20Moerchen%2C%20Fabian%20Scaling%20up%20kernel%20svm%20on%20limited%20resources%3A%20A%20low-rank%20linearization%20approach%202012-04"
        }
    ]
}
