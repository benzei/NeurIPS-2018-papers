{
    "filename": "8101-stein-variational-gradient-descent-as-moment-matching.pdf",
    "metadata": {
        "date": 2018,
        "title": "Stein Variational Gradient Descent as Moment Matching",
        "author": "Qiang Liu, Dilin Wang Department of Computer Science The University of Texas at Austin",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8101-stein-variational-gradient-descent-as-moment-matching.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set, whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing properties of SVGD with different kernels, shedding insight into optimal kernel choice. In particular, we show that SVGD with linear kernels yields exact estimation of means and variances on Gaussian distributions, while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein\u2019s identity or solving the Stein equation, which may motivate more efficient algorithms."
    },
    "keywords": [
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "Markov chain Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"
        },
        {
            "term": "approximate inference",
            "url": "https://en.wikipedia.org/wiki/approximate_inference"
        }
    ],
    "highlights": [
        "We introduce the basic background of the Stein variational method, a framework of approximate inference that integrates ideas from Stein\u2019s method, kernel methods, and variational inference",
        "In contrast to the dynamical perspective of <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al (2017</a>), we directly study what properties a set of particles would have if it satisfies the fixed point equation of Stein Variational Gradient Descent, regardless of how we obtain them algorithmically, or whether the fixed point is unique",
        "Our analysis indicates that the fixed point equation of Stein Variational Gradient Descent is essentially a moment matching condition which ensures that the fixed point particles {xi\u2217}ni=1 exactly estimate the expectations of all the functions in a special function set F\u2217, 1 n n f (x\u2217i ) = Epf, i=1",
        "\u2200f \u2208 F \u2217. This set F\u2217, which we call the Stein matching set, consists of functions obtained by applying Stein operator on the linear span of feature maps of the kernel used by Stein Variational Gradient Descent",
        "This set F\u2217, which we call the Stein matching set, consists of functions obtained by applying Stein operator on the linear span of feature maps of the kernel used by Stein Variational Gradient Descent. This framework allows us to understand properties of different kernels by studying their Stein matching sets F\u2217, which should ideally either match the test functions that we are interested in estimating, or is as large as possible to approximate the overall distribution. This process is difficult in general, but we make two observations in this work: i) We show that, by using linear kernels, Stein Variational Gradient Descent can exactly estimate the mean and variance of Gaussian distributions when the number of particles is larger than the dimension",
        "Our results are non-asymptotic in nature and provide an insightful framework for understanding the influence of kernels in the behavior of Stein Variational Gradient Descent fixed points"
    ],
    "key_statements": [
        "We introduce the basic background of the Stein variational method, a framework of approximate inference that integrates ideas from Stein\u2019s method, kernel methods, and variational inference",
        "In contrast to the dynamical perspective of <a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al (2017</a>), we directly study what properties a set of particles would have if it satisfies the fixed point equation of Stein Variational Gradient Descent, regardless of how we obtain them algorithmically, or whether the fixed point is unique",
        "Our analysis indicates that the fixed point equation of Stein Variational Gradient Descent is essentially a moment matching condition which ensures that the fixed point particles {xi\u2217}ni=1 exactly estimate the expectations of all the functions in a special function set F\u2217, 1 n n f (x\u2217i ) = Epf, i=1",
        "\u2200f \u2208 F \u2217. This set F\u2217, which we call the Stein matching set, consists of functions obtained by applying Stein operator on the linear span of feature maps of the kernel used by Stein Variational Gradient Descent",
        "This set F\u2217, which we call the Stein matching set, consists of functions obtained by applying Stein operator on the linear span of feature maps of the kernel used by Stein Variational Gradient Descent. This framework allows us to understand properties of different kernels by studying their Stein matching sets F\u2217, which should ideally either match the test functions that we are interested in estimating, or is as large as possible to approximate the overall distribution. This process is difficult in general, but we make two observations in this work: i) We show that, by using linear kernels, Stein Variational Gradient Descent can exactly estimate the mean and variance of Gaussian distributions when the number of particles is larger than the dimension",
        "By using random features of strictly\u221apositive definite kernels, the fixed points of Stein Variational Gradient Descent approximate the whole distribution with an O(1/ n) rate in kernelized Stein discrepancy",
        "We introduce the basic background of the Stein variational method, a framework of approximate inference that integrates ideas from Stein\u2019s method, kernel methods, and variational inference",
        "We address this problem by directly analyzing the properties of the fixed point equation of Stein Variational Gradient Descent, yielding results that work for finite sample size n, independent of the update rule used to arrive the fixed points.\n3 Stein Variational Gradient Descent as Moment Matching",
        "Similar idea has been investigated in <a class=\"ref-link\" id=\"cOates_et+al_2017_a\" href=\"#rOates_et+al_2017_a\">Oates et al (2017</a>), which developed a kernel approximation of Stein equation in the case based on a given set of points",
        "Our results are non-asymptotic in nature and provide an insightful framework for understanding the influence of kernels in the behavior of Stein Variational Gradient Descent fixed points"
    ],
    "summary": [
        "We introduce the basic background of the Stein variational method, a framework of approximate inference that integrates ideas from Stein\u2019s method, kernel methods, and variational inference.",
        "Our analysis indicates that the fixed point equation of SVGD is essentially a moment matching condition which ensures that the fixed point particles {xi\u2217}ni=1 exactly estimate the expectations of all the functions in a special function set F\u2217, 1 n n f (x\u2217i ) = Epf, i=1",
        "This framework allows us to understand properties of different kernels by studying their Stein matching sets F\u2217, which should ideally either match the test functions that we are interested in estimating, or is as large as possible to approximate the overall distribution.",
        "By using random features of strictly\u221apositive definite kernels, the fixed points of SVGD approximate the whole distribution with an O(1/ n) rate in kernelized Stein discrepancy.",
        "The power of Stein\u2019s identity is that, for a given distribution p, it defines an infinite number of functions of form Px \u03c6 that has zero expectation under p, all of which only depend on p through the Stein operator Px, or the score function \u2207x log p(x)",
        "We analyze in Section 3.2 the special case when the rank of the kernel is less than the particle size, in which case the Stein matching set is independent of the fixed points themselves.",
        "We shall design the algorithm, by engineering the kernels or feature maps, to make F\u2217 as large as possible in order to approximate the distribution well, or include the test functions of actual interest, such as mean and variance.",
        "Similar idea has been investigated in <a class=\"ref-link\" id=\"cOates_et+al_2017_a\" href=\"#rOates_et+al_2017_a\">Oates et al (2017</a>), which developed a kernel approximation of Stein equation in the case based on a given set of points.",
        "We show that when p is a multivariate Gaussian distribution, we can use linear features, relating to a linear kernel k(x, x ) = x x + 1, to ensure that SVGD exactly estimates all the first and second order moments of p.",
        "Non-degenerate kernels are required to obtain bounds on the whole distributions, but they complicate the analysis because their Stein matching set depends on the solution X\u2217 as shown in Lemma C.1.",
        "By running SVGD with k(x, x ), we acheive gX\u2217 (w ) = 0 for all at the fixed point, implying a zero empirical loss Dk = 0 assuming the rank condition holds.",
        "1) Assume {\u03c6(x, w )}m=1 is a set of random features with w i.i.d. drawn from pw on domain W, and X\u2217 = {xi\u2217}in=1 is an approximate fixed point of SVGD with random feature \u03c6(x, w ) in the sense that",
        "We expect that the connection between approximation inference and Stein\u2019s identity and Stein equation will provide further opportunities for deriving new generations of approximate inference algorithms"
    ],
    "headline": "Our results provide a theoretical framework for analyzing properties of Stein Variational Gradient Descent with different kernels, shedding insight into optimal kernel choice",
    "reference_links": [
        {
            "id": "Anderes_2002_a",
            "entry": "Anderes, Ethan and Coram, Marc. A general spline representation for nonparametric and semiparametric density estimates using diffeomorphisms. In arXiv preprint arXiv:1205.5314, 2002.",
            "arxiv_url": "https://arxiv.org/pdf/1205.5314"
        },
        {
            "id": "Barbour_2005_a",
            "entry": "Barbour, Andrew D and Chen, Louis Hsiao Yun. An introduction to Stein\u2019s method, volume 4. World Scientific, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barbour%2C%20Andrew%20D.%20Chen%2C%20Louis%20Hsiao%20Yun%20An%20introduction%20to%20Stein%E2%80%99s%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barbour%2C%20Andrew%20D.%20Chen%2C%20Louis%20Hsiao%20Yun%20An%20introduction%20to%20Stein%E2%80%99s%202005"
        },
        {
            "id": "Bartlett_2002_a",
            "entry": "Bartlett, Peter L and Mendelson, Shahar. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "Chwialkowski_et+al_2016_a",
            "entry": "Chwialkowski, Kacper, Strathmann, Heiko, and Gretton, Arthur. A kernel test of goodness-of-fit. International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chwialkowski%2C%20Kacper%20Strathmann%2C%20Heiko%20Gretton%2C%20Arthur%20A%20kernel%20test%20of%20goodness-of-fit%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chwialkowski%2C%20Kacper%20Strathmann%2C%20Heiko%20Gretton%2C%20Arthur%20A%20kernel%20test%20of%20goodness-of-fit%202016"
        },
        {
            "id": "Feng_et+al_2017_a",
            "entry": "Feng, Yihao, Wang, Dilin, and Liu, Qiang. Learning to draw samples with amortized Stein variational gradient descent. Conference on Uncertainty in Artificial Intelligence (UAI), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feng%2C%20Yihao%20Wang%2C%20Dilin%20Liu%2C%20Qiang%20Learning%20to%20draw%20samples%20with%20amortized%20Stein%20variational%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feng%2C%20Yihao%20Wang%2C%20Dilin%20Liu%2C%20Qiang%20Learning%20to%20draw%20samples%20with%20amortized%20Stein%20variational%20gradient%20descent%202017"
        },
        {
            "id": "Gorham_2017_a",
            "entry": "Gorham, Jackson and Mackey, Lester. Measuring sample quality with kernels. International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gorham%2C%20Jackson%20Mackey%2C%20Lester%20Measuring%20sample%20quality%20with%20kernels%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gorham%2C%20Jackson%20Mackey%2C%20Lester%20Measuring%20sample%20quality%20with%20kernels%202017"
        },
        {
            "id": "Haarnoja_et+al_2017_a",
            "entry": "Haarnoja, Tuomas, Tang, Haoran, Abbeel, Pieter, and Levine, Sergey. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning (ICML), pp. 1352\u20131361, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20Tuomas%20Tang%2C%20Haoran%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Reinforcement%20learning%20with%20deep%20energy-based%20policies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20Tuomas%20Tang%2C%20Haoran%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Reinforcement%20learning%20with%20deep%20energy-based%20policies%202017"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Kim, Taesup, Yoon, Jaesik, Dia, Ousmane, Kim, Sungwoong, Bengio, Yoshua, and Ahn, Sungjin. Bayesian model-agnostic meta-learning. In Advances In Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Taesup%20Yoon%2C%20Jaesik%20Dia%2C%20Ousmane%20Kim%2C%20Sungwoong%20Bayesian%20model-agnostic%20meta-learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Taesup%20Yoon%2C%20Jaesik%20Dia%2C%20Ousmane%20Kim%2C%20Sungwoong%20Bayesian%20model-agnostic%20meta-learning%202018"
        },
        {
            "id": "Koller_2009_a",
            "entry": "Koller, Daphne and Friedman, Nir. Probabilistic graphical models: principles and techniques. MIT press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koller%2C%20Daphne%20Friedman%2C%20Nir%20Probabilistic%20graphical%20models%3A%20principles%20and%20techniques%202009"
        },
        {
            "id": "Liu_2016_a",
            "entry": "Liu, Qiang and Wang, Dilin. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In Advances In Neural Information Processing Systems (NIPS), pp. 2378\u20132386, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20Bayesian%20inference%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20Bayesian%20inference%20algorithm%202016"
        },
        {
            "id": "Liu_et+al_2016_b",
            "entry": "Liu, Qiang, Lee, Jason, and Jordan, Michael. A kernelized Stein discrepancy for goodness-of-fit tests. In International Conference on Machine Learning, pp. 276\u2013284, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Lee%2C%20Jason%20Jordan%2C%20Michael%20A%20kernelized%20Stein%20discrepancy%20for%20goodness-of-fit%20tests%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Lee%2C%20Jason%20Jordan%2C%20Michael%20A%20kernelized%20Stein%20discrepancy%20for%20goodness-of-fit%20tests%202016"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Liu, Yang, Ramachandran, Prajit, Liu, Qiang, and Peng, Jian. Stein variational policy gradient. Conference on Uncertainty in Artificial Intelligence (UAI), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yang%20Ramachandran%2C%20Prajit%20Liu%2C%20Qiang%20Peng%2C%20Jian%20Stein%20variational%20policy%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Yang%20Ramachandran%2C%20Prajit%20Liu%2C%20Qiang%20Peng%2C%20Jian%20Stein%20variational%20policy%20gradient%202017"
        },
        {
            "id": "Lu_et+al_2018_a",
            "entry": "Lu, Jianfeng, Lu, Yulong, and Nolen, James. Scaling limit of the stein variational gradient descent part i: the mean field regime. arXiv preprint arXiv:1805.04035, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.04035"
        },
        {
            "id": "Oates_et+al_2017_a",
            "entry": "Oates, Chris J, Girolami, Mark, and Chopin, Nicolas. Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society, Series B, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oates%2C%20Chris%20J.%20Girolami%2C%20Mark%20Chopin%2C%20Nicolas%20Control%20functionals%20for%20Monte%20Carlo%20integration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oates%2C%20Chris%20J.%20Girolami%2C%20Mark%20Chopin%2C%20Nicolas%20Control%20functionals%20for%20Monte%20Carlo%20integration%202017"
        },
        {
            "id": "Ollivier_et+al_2014_a",
            "entry": "Ollivier, Yann, Pajot, Herv\u00e9, and Villani, C\u00e9dric. Optimal Transport: Theory and Applications, volume 413. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ollivier%2C%20Yann%20Pajot%2C%20Herv%C3%A9%20Villani%2C%20C%C3%A9dric%20Optimal%20Transport%3A%20Theory%20and%20Applications%2C%20volume%20413%202014"
        },
        {
            "id": "Pu_et+al_2017_a",
            "entry": "Pu, Yuchen, Gan, Zhe, Henao, Ricardo, Li, Chunyuan, Han, Shaobo, and Carin, Lawrence. VAE learning via Stein variational gradient descent. In Advances in Neural Information Processing Systems (NIPS), pp. 4239\u20134248, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pu%2C%20Yuchen%20Gan%2C%20Zhe%20Henao%2C%20Ricardo%20Li%2C%20Chunyuan%20VAE%20learning%20via%20Stein%20variational%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pu%2C%20Yuchen%20Gan%2C%20Zhe%20Henao%2C%20Ricardo%20Li%2C%20Chunyuan%20VAE%20learning%20via%20Stein%20variational%20gradient%20descent%202017"
        },
        {
            "id": "Rahimi_2007_a",
            "entry": "Rahimi, Ali and Recht, Benjamin. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NIPS), pp. 1177\u20131184, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "Rahimi_2008_a",
            "entry": "Rahimi, Ali and Recht, Benjamin. Uniform approximation of functions with random bases. In Communication, Control, and Computing, 46th Annual Allerton Conference on, pp. 555\u2013561. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Uniform%20approximation%20of%20functions%20with%20random%20bases%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Uniform%20approximation%20of%20functions%20with%20random%20bases%202008"
        },
        {
            "id": "Srebro_et+al_2010_a",
            "entry": "Srebro, Nathan, Sridharan, Karthik, and Tewari, Ambuj. Smoothness, low noise and fast rates. In Advances in neural information processing systems (NIPS), pp. 2199\u20132207, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srebro%2C%20Nathan%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20Smoothness%2C%20low%20noise%20and%20fast%20rates%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srebro%2C%20Nathan%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20Smoothness%2C%20low%20noise%20and%20fast%20rates%202010"
        },
        {
            "id": "Stein_1972_a",
            "entry": "Stein, Charles. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, pp. 583\u2013602, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stein%2C%20Charles%20A%20bound%20for%20the%20error%20in%20the%20normal%20approximation%20to%20the%20distribution%20of%20a%20sum%20of%20dependent%20random%20variables%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stein%2C%20Charles%20A%20bound%20for%20the%20error%20in%20the%20normal%20approximation%20to%20the%20distribution%20of%20a%20sum%20of%20dependent%20random%20variables%201972"
        },
        {
            "id": "Wainwright_2008_a",
            "entry": "Wainwright, Martin J, Jordan, Michael I, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1\u20132):1\u2013305, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008"
        },
        {
            "id": "Wang_2016_a",
            "entry": "Wang, Dilin and Liu, Qiang. Learning to draw samples: With application to amortized MLE for generative adversarial learning. arXiv preprint arXiv:1611.01722, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01722"
        },
        {
            "id": "Zhu_2018_a",
            "entry": "Zhu, Yinhao and Zabaras, Nicholas. Bayesian deep convolutional encoder\u2013decoder networks for surrogate modeling and uncertainty quantification. Journal of Computational Physics, 366:415\u2013 447, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Yinhao%20Zabaras%2C%20Nicholas%20Bayesian%20deep%20convolutional%20encoder%E2%80%93decoder%20networks%20for%20surrogate%20modeling%20and%20uncertainty%20quantification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Yinhao%20Zabaras%2C%20Nicholas%20Bayesian%20deep%20convolutional%20encoder%E2%80%93decoder%20networks%20for%20surrogate%20modeling%20and%20uncertainty%20quantification%202018"
        }
    ]
}
