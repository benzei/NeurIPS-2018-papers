{
    "filename": "7671-flexible-and-accurate-inference-and-learning-for-deep-generative-models.pdf",
    "metadata": {
        "title": "Flexible and accurate inference and learning for deep generative models",
        "author": "Eszter V\u00e9rtes Maneesh Sahani Gatsby Computational Neuroscience Unit",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7671-flexible-and-accurate-inference-and-learning-for-deep-generative-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce a new approach to learning in hierarchical latent-variable generative models called the \u201cdistributed distributional code Helmholtz machine\u201d, which emphasises flexibility and accuracy in the inferential process. Like the original Helmholtz machine and later variational autoencoder algorithms (but unlike adversarial methods) our approach learns an explicit inference or \u201crecognition\u201d model to approximate the posterior distribution over the latent variables. Unlike these earlier methods, it employs a posterior representation that is not limited to a narrow tractable parametrised form (nor is it represented by samples). To train the generative and recognition models we develop an extended wake-sleep algorithm inspired by the original Helmholtz machine. This makes it possible to learn hierarchical latent models with both discrete and continuous variables, where an accurate posterior representation is essential. We demonstrate that the new algorithm outperforms current state-of-the-art methods on synthetic, natural image patch and the MNIST data sets."
    },
    "keywords": [
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "variational method",
            "url": "https://en.wikipedia.org/wiki/variational_method"
        },
        {
            "term": "helmholtz machine",
            "url": "https://en.wikipedia.org/wiki/helmholtz_machine"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "log likelihood",
            "url": "https://en.wikipedia.org/wiki/log_likelihood"
        }
    ],
    "highlights": [
        "There is substantial interest in applying variational methods to learn complex latent-variable generative models, for which the full likelihood function and its gradients are intractable",
        "Variational methods rely on optimising a lower bound to the log-likelihood, which depends on an approximation to the posterior distribution over the latents [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We introduce a new approach to learning hierarchical generative models, the Distributed Distributional Code (DDC) Helmholtz Machine, which combines two ideas that originate in theoretical neuroscience: the Helmholtz Machine with wake-sleep learning [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]; and distributed or population codes for distributions [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "We found that there was close agreement between the posterior distributions of the true generative model and the one learned by the Distributed Distributional Code-Helmholtz Machine",
        "Sigmoid Belief Network trained on MNIST we evaluated the capacity of our model to learn hierarchical generative models with discrete latent variables by training a sigmoid belief network (SBN)",
        "The Distributed Distributional Code Helmholtz Machine offers a novel approach to learning hierarchical generative models, which combines the basic idea of the wake-sleep algorithm with a flexible posterior representation.\n3The model achieved an estimated negative log-likelihood of 90.97 nats, similar to the one reported by [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] (90.9 nats)"
    ],
    "key_statements": [
        "There is substantial interest in applying variational methods to learn complex latent-variable generative models, for which the full likelihood function and its gradients are intractable",
        "Variational methods rely on optimising a lower bound to the log-likelihood, which depends on an approximation to the posterior distribution over the latents [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We introduce a new approach to learning hierarchical generative models, the Distributed Distributional Code (DDC) Helmholtz Machine, which combines two ideas that originate in theoretical neuroscience: the Helmholtz Machine with wake-sleep learning [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]; and distributed or population codes for distributions [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "We found that there was close agreement between the posterior distributions of the true generative model and the one learned by the Distributed Distributional Code-Helmholtz Machine",
        "Sigmoid Belief Network trained on MNIST we evaluated the capacity of our model to learn hierarchical generative models with discrete latent variables by training a sigmoid belief network (SBN)",
        "The Distributed Distributional Code Helmholtz Machine offers a novel approach to learning hierarchical generative models, which combines the basic idea of the wake-sleep algorithm with a flexible posterior representation.\n3The model achieved an estimated negative log-likelihood of 90.97 nats, similar to the one reported by [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] (90.9 nats)",
        "The modified wake-sleep algorithm presented here still does not directly optimise a variational lower bound on the log-likelihood",
        "It can be viewed as following an approximation to the gradient of the log-likelihood, where the quality of the approximation depends on the richness of the Distributed Distributional Code representation used",
        "Precise conditions for convergence are yet to be established, but the expectation is that when the approximation is rich enough for the error in the resulting gradient estimate to be bounded, the algorithm will always reach a region around a local mode in which the true gradient does not exceed that error bound"
    ],
    "summary": [
        "There is substantial interest in applying variational methods to learn complex latent-variable generative models, for which the full likelihood function and its gradients are intractable.",
        "The current generative model is used to provide joint samples of the latent variables and fictitious observations and these are used as supervised training data to adapt the recognition network.",
        "Points x as input and produces expectations of the non-linear encoding functions {T (i)} as given by Eq (2); and learning how to use these expectations to update the generative model parameters using approximations of the form of Eq (5).",
        "The wake phase updates the generative parameters by computing the approximate gradient of the free energy, using the posterior expectations learned in the sleep phase.",
        "Sleep phase One aim of the sleep phase, given a current generative model p\u03b8(x, z), is to update the recognition network so that the Kullback-Leibler divergence between the true and the approximate posterior is minimised: \u03c6 = argmin DKL[p\u03b8(z|x)||q\u03c6(z|x)]",
        "The second aim of the DDC-HM sleep phase is quite different: a further set of weights must be learnt to approximate the gradients of the generative model joint likelihood.",
        "As shown in the appendix, using the function approximations trained using the sleep samples and the posterior representation produced by the recognition network, we can learn the generative model parameters without needing any explicit independence assumptions about the posterior distribution.",
        "The Importance Weighted Autoencoder (IWAE; [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]) optimises a tighter lower bound constructed by an importance sampled estimator of the log-likelihood using the recognition model as a proposal distribution.",
        "We used a recognition model with a hidden layer of size 100, and K1 = K2 = 100 encoding functions for each latent layer, with 200 sleep samples, and learned the parameters of the conditional distributions p(x|z1) and p(z1|z2) while keeping the prior on z2 fixed (m=3, \u03c3=0.1).",
        "We have fitted both a Variational Autoencoder (VAE) and an Importance Weighted Autoencoder (IWAE), using 2-layer recognition networks with 100 hidden units each, producing a factorised Gaussian posterior approximation.",
        "The DDC Helmholtz Machine offers a novel approach to learning hierarchical generative models, which combines the basic idea of the wake-sleep algorithm with a flexible posterior representation.",
        "The lack of strong parametric assumptions in the DDC representation allows the algorithm to learn generative models with complex posterior distributions accurately."
    ],
    "headline": "We introduce a new approach to learning in hierarchical latent-variable generative models called the \u201cdistributed distributional code Helmholtz machine\u201d, which emphasises flexibility and accuracy in the inferential process",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] MJ Wainwright and MI Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1\u20132):1\u2013305, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008"
        },
        {
            "id": "2",
            "entry": "[2] RE Turner and M Sahani. Two problems with variational expectation maximisation for timeseries models. In Bayesian Time series models, pp. 109\u2013130. Cambridge University Press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Turner%2C%20R.E.%20Sahani%2C%20M.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20timeseries%20models.%20In%20Bayesian%20Time%20series%20models%202011"
        },
        {
            "id": "3",
            "entry": "[3] DJ Rezende, S Mohamed, and D Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pp. 1278\u20131286, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "4",
            "entry": "[4] D Kingma and M Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations (ICLR2014). arXiv.org, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.%20Welling%2C%20M.%20Auto-Encoding%20Variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.%20Welling%2C%20M.%20Auto-Encoding%20Variational%20Bayes%202014"
        },
        {
            "id": "5",
            "entry": "[5] CK Sonderby, T Raiko, L Maaloe, SK Sonderby, and O Winther. Ladder variational autoencoders. In Advances in Neural Information Processing Systems 29, pp. 3738\u20133746. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sonderby%2C%20C.K.%20Raiko%2C%20T.%20Maaloe%2C%20L.%20Sonderby%2C%20S.K.%20Ladder%20variational%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sonderby%2C%20C.K.%20Raiko%2C%20T.%20Maaloe%2C%20L.%20Sonderby%2C%20S.K.%20Ladder%20variational%20autoencoders%202016"
        },
        {
            "id": "6",
            "entry": "[6] P Dayan, GE Hinton, RM Neal, and RS Zemel. The Helmholtz machine. Neural Computation, 7(5):889\u2013904, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20P.%20Hinton%2C%20G.E.%20Neal%2C%20R.M.%20Zemel%2C%20R.S.%20The%20Helmholtz%20machine%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20P.%20Hinton%2C%20G.E.%20Neal%2C%20R.M.%20Zemel%2C%20R.S.%20The%20Helmholtz%20machine%201995"
        },
        {
            "id": "7",
            "entry": "[7] RS Zemel, P Dayan, and A Pouget. Probabilistic interpretation of population codes. Neural Computation, 10(2):403\u2013430, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RS%20Zemel%2C%20P%20Dayan%2C%20and%20A%20Pouget.%20Probabilistic%20interpretation%20of%20population%20codes%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RS%20Zemel%2C%20P%20Dayan%2C%20and%20A%20Pouget.%20Probabilistic%20interpretation%20of%20population%20codes%201998"
        },
        {
            "id": "8",
            "entry": "[8] M Sahani and P Dayan. Doubly distributional population codes: Simultaneous representation of uncertainty and multiplicity. Neural Computation, 15(10):2255\u20132279, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sahani%2C%20M.%20Dayan%2C%20P.%20Doubly%20distributional%20population%20codes%3A%20Simultaneous%20representation%20of%20uncertainty%20and%20multiplicity%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sahani%2C%20M.%20Dayan%2C%20P.%20Doubly%20distributional%20population%20codes%3A%20Simultaneous%20representation%20of%20uncertainty%20and%20multiplicity%202003"
        },
        {
            "id": "9",
            "entry": "[9] S Gershman and N Goodman. Amortized inference in probabilistic reasoning. In Proceedings of the Annual Meeting of the Cognitive Science Society, vol. 36, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gershman%2C%20S.%20Goodman%2C%20N.%20Amortized%20inference%20in%20probabilistic%20reasoning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gershman%2C%20S.%20Goodman%2C%20N.%20Amortized%20inference%20in%20probabilistic%20reasoning%202014"
        },
        {
            "id": "10",
            "entry": "[10] T Minka. Divergence measures and message passing. Microsoft Research, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20T.%20Divergence%20measures%20and%20message%20passing%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minka%2C%20T.%20Divergence%20measures%20and%20message%20passing%202005"
        },
        {
            "id": "11",
            "entry": "[11] Y Burda, R Grosse, and R Salakhutdinov. Importance weighted autoencoders. arXiv:1509.00519, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00519"
        },
        {
            "id": "12",
            "entry": "[12] A Mnih and D Rezende. Variational inference for Monte Carlo objectives. In Proceedings of the 33rd International Conference on Machine Learning, pp. 2188\u20132196, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20A%20Mnih%20and%20D%20Rezende.%20Variational%20inference%20for%20Monte%20Carlo%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20A%20Mnih%20and%20D%20Rezende.%20Variational%20inference%20for%20Monte%20Carlo%20objectives%202016"
        },
        {
            "id": "13",
            "entry": "[13] R Ranganath, L Tang, L Charlin, and D Blei. Deep exponential families. In Artificial Intelligence and Statistics, pp. 762\u2013771, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tang%2C%20L.%20Charlin%2C%20L.%20Blei%2C%20D.%20Deep%20exponential%20families%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Tang%2C%20L.%20Charlin%2C%20L.%20Blei%2C%20D.%20Deep%20exponential%20families%202015"
        },
        {
            "id": "14",
            "entry": "[14] RM Neal. Connectionist learning of belief networks. Artificial Intelligence, 56(1):71\u2013113, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20R.M.%20Connectionist%20learning%20of%20belief%20networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20R.M.%20Connectionist%20learning%20of%20belief%20networks%201992"
        },
        {
            "id": "15",
            "entry": "[15] GE Hinton, P Dayan, BJ Frey, and RM Neal. The \"wake-sleep\" algorithm for unsupervised neural networks. Science, 268(5214):1158\u20131161, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Dayan%2C%20P.%20Frey%2C%20B.J.%20Neal%2C%20R.M.%20The%20%22wake-sleep%22%20algorithm%20for%20unsupervised%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Dayan%2C%20P.%20Frey%2C%20B.J.%20Neal%2C%20R.M.%20The%20%22wake-sleep%22%20algorithm%20for%20unsupervised%20neural%20networks%201995"
        },
        {
            "id": "16",
            "entry": "[16] A Rahimi and B Recht. Uniform approximation of functions with random bases. In 46th Annual Allerton Conference on Communication, Control, and Computing, pp. 555\u2013561, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20A.%20Recht%2C%20B.%20Uniform%20approximation%20of%20functions%20with%20random%20bases%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20A.%20Recht%2C%20B.%20Uniform%20approximation%20of%20functions%20with%20random%20bases%202008"
        },
        {
            "id": "17",
            "entry": "[17] A Gretton, KM Borgwardt, MJ Rasch, B Scholkopf, and A Smola. A kernel two-sample test. Journal of Machine Learning Research, 13:723\u2013773, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Scholkopf%2C%20B.%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Scholkopf%2C%20B.%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "18",
            "entry": "[18] D Rezende and S Mohamed. Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1530\u20131538, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "19",
            "entry": "[19] DP Kingma and J Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "20",
            "entry": "[20] W Bounliphone, E Belilovsky, MB Blaschko, I Antonoglou, and A Gretton. A test of relative similarity for model selection in generative models. arXiv:1511.04581, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.04581"
        },
        {
            "id": "21",
            "entry": "[21] W Jitkrittum, Z Szabo, K Chwialkowski, and A Gretton. Interpretable distribution features with maximum testing power. arXiv:1605.06796, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.06796"
        },
        {
            "id": "22",
            "entry": "[22] JH van Hateren and A van der Schaaf. Independent component filters of natural images compared with simple cells in primary visual cortex. Proceedings of the Royal Society B: Biological Sciences, 265(1394):359\u2013366, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Hateren%2C%20J.H.%20van%20der%20Schaaf%2C%20A.%20Independent%20component%20filters%20of%20natural%20images%20compared%20with%20simple%20cells%20in%20primary%20visual%20cortex%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Hateren%2C%20J.H.%20van%20der%20Schaaf%2C%20A.%20Independent%20component%20filters%20of%20natural%20images%20compared%20with%20simple%20cells%20in%20primary%20visual%20cortex%201998"
        },
        {
            "id": "23",
            "entry": "[23] R Salakhutdinov and I Murray. On the quantitative analysis of deep belief networks. In Proceedings of the 25th International Conference on Machine Learning, pp. 872\u2013879. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R.%20Salakhutdinov%20and%20I%20Murray.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R.%20Salakhutdinov%20and%20I%20Murray.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008"
        },
        {
            "id": "24",
            "entry": "[24] A Smola, A Gretton, L Song, and B Sch\u00f6lkopf. A Hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory, pp. 13\u201331.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20A.%20Gretton%2C%20A.%20Song%2C%20L.%20Sch%C3%B6lkopf%2C%20B.%20A%20Hilbert%20space%20embedding%20for%20distributions",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smola%2C%20A.%20Gretton%2C%20A.%20Song%2C%20L.%20Sch%C3%B6lkopf%2C%20B.%20A%20Hilbert%20space%20embedding%20for%20distributions"
        },
        {
            "id": "25",
            "entry": "[25] S Grunewalder, G Lever, L Baldassarre, S Patterson, A Gretton, and M Pontil. Conditional mean embeddings as regressors. In Proceedings of the 29th International Conference on Machine Learning, vol. 2, pp. 1823\u20131830, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grunewalder%2C%20S.%20Lever%2C%20G.%20Baldassarre%2C%20L.%20Patterson%2C%20S.%20Conditional%20mean%20embeddings%20as%20regressors%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grunewalder%2C%20S.%20Lever%2C%20G.%20Baldassarre%2C%20L.%20Patterson%2C%20S.%20Conditional%20mean%20embeddings%20as%20regressors%202012"
        }
    ]
}
