{
    "filename": "7491-a-smoothed-analysis-of-the-greedy-algorithm-for-the-linear-contextual-bandit-problem.pdf",
    "metadata": {
        "title": "A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem",
        "author": "Sampath Kannan and Jamie Morgenstern and Aaron Roth and Bo Waggoner and Zhiwei Steven Wu",
        "date": 2018,
        "identifiers": {
            "arxiv": "1801.03423",
            "doi": null,
            "isbn": null,
            "doc_id": null,
            "url": "https://papers.nips.cc/paper/7491-a-smoothed-analysis-of-the-greedy-algorithm-for-the-linear-contextual-bandit-problem.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Bandit learning is characterized by the tension between long-term exploration and short-term exploitation. However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. This raises a fairness concern. In such settings, one might like to run a \"greedy\" algorithm, which always makes the (myopically) optimal decision for the individuals at hand - but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm. We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve \"no regret\", perhaps (depending on the specifics of the setting) with a constant amount of initial training data. This suggests that \"generically\" (i.e. in slightly perturbed environments), exploration and exploitation need not be in conflict in the linear setting."
    },
    "keywords": [
        {
            "term": "greedy algorithm",
            "url": "https://en.wikipedia.org/wiki/greedy_algorithm"
        },
        {
            "term": "smoothed analysis",
            "url": "https://en.wikipedia.org/wiki/smoothed_analysis"
        }
    ],
    "highlights": [
        "Learning algorithms often need to operate in partial feedback settings, in which the decisions of the algorithm determine the data that it observes",
        "We study the linear contextual bandits problem, which informally, represents the following learning scenario which takes place over a sequence of rounds t",
        "We study the greedy algorithm, which trains least-squares estimates \u03b2it on the current set of observations, and at each round, picks the arm with the highest predicted reward: it = arg maxi \u03b2it \u00b7 xti",
        "We show that under smoothed analysis, there is a qualitative distinction between the single parameter and multiple parameter settings: 1",
        "They show that in the single parameter setting, when one further assumes that 1) the linear parameter is drawn from a Bayesian prior that is not too concentrated, 2) the contexts are drawn i.i.d. from a fixed distribution and perturbed, and 3) that the algorithm is allowed to make its decisions in \u201cbatches\u201d of polylog(d, t)/\u03c32 many rounds, the greedy algorithm is essentially instance optimal in terms of Bayesian regret, and that its regret grows at a rate of O(T 1/3) in the worst case",
        "Our goal will be to show that the greedy algorithm achieves no regret against any perturbed adversary in both the single-parameter and multiple-parameter settings"
    ],
    "key_statements": [
        "Learning algorithms often need to operate in partial feedback settings, in which the decisions of the algorithm determine the data that it observes",
        "We study the linear contextual bandits problem, which informally, represents the following learning scenario which takes place over a sequence of rounds t",
        "We study the greedy algorithm, which trains least-squares estimates \u03b2it on the current set of observations, and at each round, picks the arm with the highest predicted reward: it = arg maxi \u03b2it \u00b7 xti",
        "We show that under smoothed analysis, there is a qualitative distinction between the single parameter and multiple parameter settings: 1",
        "They show that in the single parameter setting, when one further assumes that 1) the linear parameter is drawn from a Bayesian prior that is not too concentrated, 2) the contexts are drawn i.i.d. from a fixed distribution and perturbed, and 3) that the algorithm is allowed to make its decisions in \u201cbatches\u201d of polylog(d, t)/\u03c32 many rounds, the greedy algorithm is essentially instance optimal in terms of Bayesian regret, and that its regret grows at a rate of O(T 1/3) in the worst case",
        "Our goal will be to show that the greedy algorithm achieves no regret against any perturbed adversary in both the single-parameter and multiple-parameter settings"
    ],
    "summary": [
        "Learning algorithms often need to operate in partial feedback settings, in which the decisions of the algorithm determine the data that it observes.",
        "Our results extend beyond this particular perturbed adversary: we give general conditions on the distribution over contexts which imply our regret bounds.",
        "The most closely related piece of work is <a class=\"ref-link\" id=\"cBastani_et+al_2017_a\" href=\"#rBastani_et+al_2017_a\">Bastani et al [2017</a>], who, in a stochastic setting, give conditions on the sampling distribution over contexts that causes the greedy algorithm to have diminishing regret in a closely related but incomparable version of the two-armed linear contextual bandits problem3.",
        "They show that in the single parameter setting, when one further assumes that 1) the linear parameter is drawn from a Bayesian prior that is not too concentrated, 2) the contexts are drawn i.i.d. from a fixed distribution and perturbed, and 3) that the algorithm is allowed to make its decisions in \u201cbatches\u201d of polylog(d, t)/\u03c32 many rounds, the greedy algorithm is essentially instance optimal in terms of Bayesian regret, and that its regret grows at a rate of O(T 1/3) in the worst case.",
        "Our goal will be to show that the greedy algorithm achieves no regret against any perturbed adversary in both the single-parameter and multiple-parameter settings.",
        "In the single parameter setting, diversity will imply a regret guarantee: when any arm is pulled, the context-reward pair gives useful information about all components of the parameter \u03b2.",
        "A small initial training set can guarantee that initial estimates achieve constant error, and so the margin condition implies that Greedy will continue to explore arms with a frequency that is proportional to the number of rounds for which they are optimal; diversity implies that estimates of those arms\u2019 parameters will improve quickly.",
        "In the multi-parameter setting, we cannot hope for the greedy algorithm to achieve vanishing regret without any initial information, as it never learns about parameters of arms it does not pull.",
        "We show that the diversity and margin conditions together on a generic bounded adversary imply low regret.",
        "The margin condition implies that for sufficiently accurate estimates, when an arm is optimal, the perturbations have a good chance of causing Greedy to pull that arm.",
        "The following key result leverages the margin condition to argue that, if i is optimal for a significant number of rounds, it is pulled by Greedy.",
        "This is vital to a good regret bound because it shows that ni(t), the number of samples from arm i, is steadily increasing in t if i is often optimal, which we know from the diversity condition implies that the estimate \u03b2it is converging."
    ],
    "headline": "Our work provides a theoretical explanation for this phenomenon. 1.1",
    "reference_links": [
        {
            "id": "Abbasi-Yadkori_et+al_2011_a",
            "entry": "Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312\u20132320, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Yasin%20P%C3%A1l%2C%20D%C3%A1vid%20Szepesv%C3%A1ri%2C%20Csaba%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Yasin%20P%C3%A1l%2C%20D%C3%A1vid%20Szepesv%C3%A1ri%2C%20Csaba%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011"
        },
        {
            "id": "Agarwal_et+al_2014_a",
            "entry": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pages 1638\u20131646, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Hsu%2C%20Daniel%20Kale%2C%20Satyen%20Langford%2C%20John%20Taming%20the%20monster%3A%20A%20fast%20and%20simple%20algorithm%20for%20contextual%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Hsu%2C%20Daniel%20Kale%2C%20Satyen%20Langford%2C%20John%20Taming%20the%20monster%3A%20A%20fast%20and%20simple%20algorithm%20for%20contextual%20bandits%202014"
        },
        {
            "id": "Arthur_et+al_2011_a",
            "entry": "David Arthur, Bodo Manthey, and Heiko R\u00f6glin. Smoothed analysis of the k-means method. Journal of the ACM (JACM), 58(5):19, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arthur%2C%20David%20Manthey%2C%20Bodo%20R%C3%B6glin%2C%20Heiko%20Smoothed%20analysis%20of%20the%20k-means%20method%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arthur%2C%20David%20Manthey%2C%20Bodo%20R%C3%B6glin%2C%20Heiko%20Smoothed%20analysis%20of%20the%20k-means%20method%202011"
        },
        {
            "id": "Barry-Jester_et+al_2015_a",
            "entry": "Anna Maria Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing. The Marshall Project, August 8 2015. URL https://www.themarshallproject.org/2015/08/04/the-new-science-of-sentencing. Retrieved 4/28/2016.",
            "url": "https://www.themarshallproject.org/2015/08/04/the-new-science-of-sentencing"
        },
        {
            "id": "Bastani_et+al_2017_a",
            "entry": "H. Bastani, M. Bayati, and K. Khosravi. Exploiting the Natural Exploration In Contextual Bandits. ArXiv e-prints, April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bastani%2C%20H.%20Bayati%2C%20M.%20Khosravi%2C%20K.%20Exploiting%20the%20Natural%20Exploration%20In%20Contextual%20Bandits.%20ArXiv%20e-prints%202017-04"
        },
        {
            "id": "Bhaskara_et+al_2014_a",
            "entry": "Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analysis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 594\u2013603. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhaskara%2C%20Aditya%20Charikar%2C%20Moses%20Moitra%2C%20Ankur%20Vijayaraghavan%2C%20Aravindan%20Smoothed%20analysis%20of%20tensor%20decompositions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhaskara%2C%20Aditya%20Charikar%2C%20Moses%20Moitra%2C%20Ankur%20Vijayaraghavan%2C%20Aravindan%20Smoothed%20analysis%20of%20tensor%20decompositions%202014"
        },
        {
            "id": "Bietti_et+al_2018_a",
            "entry": "A. Bietti, A. Agarwal, and J. Langford. Practical Evaluation and Optimization of Contextual Bandit Algorithms. ArXiv e-prints, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bietti%2C%20A.%20Agarwal%2C%20A.%20Langford%2C%20J.%20Practical%20Evaluation%20and%20Optimization%20of%20Contextual%20Bandit%20Algorithms.%20ArXiv%20e-prints%202018-02"
        },
        {
            "id": "Bird_et+al_2016_a",
            "entry": "Sarah Bird, Solon Barocas, Kate Crawford, Fernando Diaz, and Hanna Wallach. Exploring or exploiting? social and ethical implications of autonomous experimentation. Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bird%2C%20Sarah%20Barocas%2C%20Solon%20Crawford%2C%20Kate%20Diaz%2C%20Fernando%20Exploring%20or%20exploiting%3F%20social%20and%20ethical%20implications%20of%20autonomous%20experimentation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bird%2C%20Sarah%20Barocas%2C%20Solon%20Crawford%2C%20Kate%20Diaz%2C%20Fernando%20Exploring%20or%20exploiting%3F%20social%20and%20ethical%20implications%20of%20autonomous%20experimentation%202016"
        },
        {
            "id": "Blum_2002_a",
            "entry": "Avrim Blum and John Dunagan. Smoothed analysis of the perceptron algorithm for linear programming. In Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms, pages 905\u2013914. Society for Industrial and Applied Mathematics, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Dunagan%2C%20John%20Smoothed%20analysis%20of%20the%20perceptron%20algorithm%20for%20linear%20programming%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Dunagan%2C%20John%20Smoothed%20analysis%20of%20the%20perceptron%20algorithm%20for%20linear%20programming%202002"
        },
        {
            "id": "Byrnes_2016_a",
            "entry": "Nanette Byrnes. Artificial intolerance. MIT Technology Review, March 28 2016. URL https://www.technologyreview.com/s/600996/artificial-intolerance/. Retrieved 4/28/2016.",
            "url": "https://www.technologyreview.com/s/600996/artificial-intolerance/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrnes%2C%20Nanette%20Artificial%20intolerance%202016-03"
        },
        {
            "id": "Chu_et+al_2011_a",
            "entry": "Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff functions. In International Conference on Artificial Intelligence and Statistics, pages 208\u2013214, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chu%2C%20Wei%20Li%2C%20Lihong%20Reyzin%2C%20Lev%20Schapire%2C%20Robert%20E.%20Contextual%20bandits%20with%20linear%20payoff%20functions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chu%2C%20Wei%20Li%2C%20Lihong%20Reyzin%2C%20Lev%20Schapire%2C%20Robert%20E.%20Contextual%20bandits%20with%20linear%20payoff%20functions%202011"
        },
        {
            "id": "Ensign_et+al_2017_a",
            "entry": "Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Eduardo Scheidegger, and Suresh Venkatasubramanian. Runaway feedback loops in predictive policing. Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ensign%2C%20Danielle%20Friedler%2C%20Sorelle%20A.%20Neville%2C%20Scott%20Scheidegger%2C%20Carlos%20Eduardo%20Runaway%20feedback%20loops%20in%20predictive%20policing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ensign%2C%20Danielle%20Friedler%2C%20Sorelle%20A.%20Neville%2C%20Scott%20Scheidegger%2C%20Carlos%20Eduardo%20Runaway%20feedback%20loops%20in%20predictive%20policing%202017"
        },
        {
            "id": "Jabbari_et+al_2017_a",
            "entry": "Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1617\u20131626, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jabbari%2C%20Shahin%20Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20Fairness%20in%20reinforcement%20learning%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jabbari%2C%20Shahin%20Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20Fairness%20in%20reinforcement%20learning%202017-08"
        },
        {
            "id": "Joseph_et+al_0000_a",
            "entry": "Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Fair algorithms for infinite and contextual bandits. arXiv preprint arXiv:1610.09559, 2016a.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09559"
        },
        {
            "id": "Joseph_et+al_2016_a",
            "entry": "Matthew Joseph, Michael Kearns, Jamie H. Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 325\u2013333, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016-12-05"
        },
        {
            "id": "Kalai_et+al_2009_a",
            "entry": "Adam Tauman Kalai, Alex Samorodnitsky, and Shang-Hua Teng. Learning and smoothed analysis. In Foundations of Computer Science, 2009. FOCS\u201909. 50th Annual IEEE Symposium on, pages 395\u2013404. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalai%2C%20Adam%20Tauman%20Samorodnitsky%2C%20Alex%20Teng%2C%20Shang-Hua%20Learning%20and%20smoothed%20analysis.%20In%20Foundations%20of%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalai%2C%20Adam%20Tauman%20Samorodnitsky%2C%20Alex%20Teng%2C%20Shang-Hua%20Learning%20and%20smoothed%20analysis.%20In%20Foundations%20of%202009"
        },
        {
            "id": "Kannan_et+al_2017_a",
            "entry": "Sampath Kannan, Michael Kearns, Jamie Morgenstern, Mallesh M. Pai, Aaron Roth, Rakesh V. Vohra, and Zhiwei Steven Wu. Fairness incentives for myopic agents. In Constantinos Daskalakis, Moshe Babaioff, and Herv\u00e9 Moulin, editors, Proceedings of the 2017 ACM Conference on Economics and Computation, EC \u201917, Cambridge, MA, USA, June 26-30, 2017, pages 369\u2013386. ACM, 2017. doi: 10.1145/3033274.3085154. URL http://doi.acm.org/10.1145/3033274.3085154.",
            "crossref": "https://dx.doi.org/10.1145/3033274.3085154",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/3033274.3085154"
        },
        {
            "id": "Laurent_2000_a",
            "entry": "B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann. Statist., 28(5):1302\u20131338, 10 2000. doi: 10.1214/aos/1015957395. URL http://dx.doi.org/10.1214/aos/1015957395.",
            "crossref": "https://dx.doi.org/10.1214/aos/1015957395",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/aos/1015957395"
        },
        {
            "id": "Li_et+al_2010_a",
            "entry": "Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Schapire%2C%20Robert%20E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Schapire%2C%20Robert%20E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010"
        },
        {
            "id": "Li_et+al_2011_a",
            "entry": "Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 297\u2013306. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Wang%2C%20Xuanhui%20Unbiased%20offline%20evaluation%20of%20contextualbandit-based%20news%20article%20recommendation%20algorithms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Wang%2C%20Xuanhui%20Unbiased%20offline%20evaluation%20of%20contextualbandit-based%20news%20article%20recommendation%20algorithms%202011"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C. Parkes. Calibrated fairness in bandits. Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%20Liu%20Goran%20Radanovic%20Christos%20Dimitrakakis%20Debmalya%20Mandal%20and%20David%20C%20Parkes%20Calibrated%20fairness%20in%20bandits%20Workshop%20on%20Fairness%20Accountability%20and%20Transparency%20in%20Machine%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%20Liu%20Goran%20Radanovic%20Christos%20Dimitrakakis%20Debmalya%20Mandal%20and%20David%20C%20Parkes%20Calibrated%20fairness%20in%20bandits%20Workshop%20on%20Fairness%20Accountability%20and%20Transparency%20in%20Machine%20Learning%202017"
        },
        {
            "id": "Raghavan_et+al_2018_a",
            "entry": "Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, and Zhiwei Steven Wu. The externalities of exploration and how data diversity helps exploitation. In Proceedings of the 31st Conference on Learning Theory, COLT 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghavan%2C%20Manish%20Slivkins%2C%20Aleksandrs%20Vaughan%2C%20Jennifer%20Wortman%20Wu%2C%20Zhiwei%20Steven%20The%20externalities%20of%20exploration%20and%20how%20data%20diversity%20helps%20exploitation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghavan%2C%20Manish%20Slivkins%2C%20Aleksandrs%20Vaughan%2C%20Jennifer%20Wortman%20Wu%2C%20Zhiwei%20Steven%20The%20externalities%20of%20exploration%20and%20how%20data%20diversity%20helps%20exploitation%202018"
        },
        {
            "id": "Rudin_2013_a",
            "entry": "Cynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired Magazine, August 2013. URL http://www.wired.com/insights/2013/08/predictive-policing-using-machine-learning-to-detect-\\patterns-of-crime/. Retrieved 4/28/2016.",
            "url": "http://www.wired.com/insights/2013/08/predictive-policing-using-machine-learning-to-detect-\\patterns-of-crime/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudin%2C%20Cynthia%20Predictive%20policing%20using%20machine%20learning%20to%20detect%20patterns%20of%20crime%202013-08"
        },
        {
            "id": "Daniel_2004_a",
            "entry": "Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time. Journal of the ACM (JACM), 51(3):385\u2013463, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Daniel%20A%20Spielman%20and%20Shang-Hua%20Teng.%20Smoothed%20analysis%20of%20algorithms%3A%20Why%20the%20simplex%20algorithm%20usually%20takes%20polynomial%20time%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Daniel%20A%20Spielman%20and%20Shang-Hua%20Teng.%20Smoothed%20analysis%20of%20algorithms%3A%20Why%20the%20simplex%20algorithm%20usually%20takes%20polynomial%20time%202004"
        },
        {
            "id": "Syrgkanis_et+al_2016_a",
            "entry": "Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert Schapire. Efficient algorithms for adversarial contextual learning. In International Conference on Machine Learning, pages 2159\u20132168, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syrgkanis%2C%20Vasilis%20Krishnamurthy%2C%20Akshay%20Schapire%2C%20Robert%20Efficient%20algorithms%20for%20adversarial%20contextual%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Syrgkanis%2C%20Vasilis%20Krishnamurthy%2C%20Akshay%20Schapire%2C%20Robert%20Efficient%20algorithms%20for%20adversarial%20contextual%20learning%202016"
        },
        {
            "id": "Tropp_2012_a",
            "entry": "Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, Aug 2012. ISSN 1615-3383. doi: 10.1007/s10208-011-9099-z. URL https://doi.org/10.1007/s10208-011-9099-z.",
            "crossref": "https://dx.doi.org/10.1007/s10208-011-9099-z",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s10208-011-9099-z"
        }
    ]
}
