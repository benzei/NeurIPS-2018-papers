{
    "filename": "7533-natasha-2-faster-non-convex-optimization-than-sgd.pdf",
    "metadata": {
        "title": "Natasha 2: Faster Non-Convex Optimization Than SGD",
        "author": "Zeyuan Allen-Zhu",
        "identifiers": {
            "arxiv": "1708.08694",
            "doi": null,
            "isbn": null,
            "doc_id": null,
            "url": "https://papers.nips.cc/paper/7533-natasha-2-faster-non-convex-optimization-than-sgd.pdf"
        },
        "abstract": "We design a stochastic algorithm to find \u03b5-approximate local minima of any smooth nonconvex function in rate O(\u03b5\u22123.25), with only oracle access to stochastic gradients. The best result before this work was O(\u03b5\u22124) by stochastic gradient descent (SGD).2",
        "date": 2017
    },
    "keywords": [
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "positive semidefinite",
            "url": "https://en.wikipedia.org/wiki/positive_semidefinite"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "online learning",
            "url": "https://en.wikipedia.org/wiki/online_learning"
        },
        {
            "term": "linear time",
            "url": "https://en.wikipedia.org/wiki/linear_time"
        },
        {
            "term": "first order",
            "url": "https://en.wikipedia.org/wiki/first_order"
        },
        {
            "term": "Stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "In diverse world of deep learning research has given rise to numerous architectures for neural networks",
        "We address the problem of designing a new algorithm that has provably faster running time than the best known result for Stochastic gradient descent",
        "[<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] showed that, hiding factors that depend on L, L2 and V, Stochastic gradient descent finds an \u03b5-approximate local minimum of f (x) in gradient complexity T = O\u03b5\u22124)"
    ],
    "key_statements": [
        "In diverse world of deep learning research has given rise to numerous architectures for neural networks",
        "We address the problem of designing a new algorithm that has provably faster running time than the best known result for Stochastic gradient descent",
        "[<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] showed that, hiding factors that depend on L, L2 and V, Stochastic gradient descent finds an \u03b5-approximate local minimum of f (x) in gradient complexity T = O\u03b5\u22124)"
    ],
    "summary": [
        "In diverse world of deep learning research has given rise to numerous architectures for neural networks.",
        "It is a known fact that computing Hessian-vector products is as cheap as computing stochastic gradients, and we can use Oja\u2019s algorithm to escape from saddle points.",
        "As long as we are close to saddle points, we can already use Oja\u2019s algorithm to find such negative curvature, and move in its direction to decrease the objective, see Figure 2(a).",
        "If one swings by saddle points using Oja\u2019s algorithm and SGD variants, the convergence rate is T = O(\u03b5\u22123.5) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "If one applies SGD and only escapes from saddle points using Oja\u2019s algorithm, the convergence rate is T = O(\u03b5\u22124) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>].",
        "If one applies SCSG and only escapes from saddle points using Oja\u2019s algorithm, the convergence rate is T = O(\u03b5\u22123.333) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>].",
        "In each iteration t at point xt, SVRG defines gradient estimator \u2207f := \u2207fi \u2212 \u2207fi(x) + \u2207f (x) which satisfies Ei[\u2207f] = \u2207f, and performs proximal update xt+1 \u2190 xt \u2212 \u03b1\u2207f for learning rate \u03b1.",
        "Stochastic gradient descent (SGD) find approximate local minima [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], under (A1), (A2) and an additional assumption (A4): f (x) is second-order L2-Lipschitz smooth, meaning \u22072f (x) \u2212 \u22072f (y) 2 \u2264 L2 \u00b7 x \u2212 y .",
        "Ge et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] is the only result that gives provable online complexity for finding approximate local minima.",
        "Natasha1.5, do not find approximate local minima and may be stuck at saddle points.11 Ge et al.",
        "[<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] showed that, hiding factors that depend on L, L2 and V, SGD finds an \u03b5-approximate local minimum of f (x) in gradient complexity T = O\u03b5\u22124).",
        "Finds approximate stationary points of f (x) using Natasha1.5, or finds negative curvature of the Hessian \u22072f (x), using Oja\u2019s online eigenvector algorithm.",
        "The follow-up work [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] replaced Hessian-vector products in Natasha2 with only stochastic gradient computations, turning Natasha2 into a pure first-order method.",
        "We apply efficient online algorithms for the two tasks: namely, Oja\u2019s algorithm for finding minimum eigenvectors, and our new Natasha1.5 algorithm for finding stationary points.",
        "The above process finishes only if Natasha1.5 finds an approximate stationary point x of F k(x) that is inside the safe zone x: x \u2212 yk",
        "13repeatSVRG is an offline algorithm, and finds an \u03b5-approximate stationary point for a function f (x) that is \u03c3-nonconvex."
    ],
    "headline": "We address the problem of designing a new algorithm that has provably faster running time than the best known result for Stochastic gradient descent",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding Approximate Local Minima for Nonconvex Optimization in Linear Time. In STOC, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Naman%20Allen-Zhu%2C%20Zeyuan%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Finding%20Approximate%20Local%20Minima%20for%20Nonconvex%20Optimization%20in%20Linear%20Time%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Naman%20Allen-Zhu%2C%20Zeyuan%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Finding%20Approximate%20Local%20Minima%20for%20Nonconvex%20Optimization%20in%20Linear%20Time%202017"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20First%20Direct%20Acceleration%20of%20Stochastic%20Gradient%20Methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20First%20Direct%20Acceleration%20of%20Stochastic%20Gradient%20Methods%202017"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu. Natasha: Faster Non-Convex Stochastic Optimization via Strongly NonConvex Parameter. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Natasha%3A%20Faster%20Non-Convex%20Stochastic%20Optimization%20via%20Strongly%20NonConvex%20Parameter%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Natasha%3A%20Faster%20Non-Convex%20Stochastic%20Optimization%20via%20Strongly%20NonConvex%20Parameter%202017"
        },
        {
            "id": "4",
            "entry": "[4] Zeyuan Allen-Zhu. Katyusha X: Practical Momentum Method for Stochastic Sum-ofNonconvex Optimization. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Katyusha%20X%3A%20Practical%20Momentum%20Method%20for%20Stochastic%20Sum-ofNonconvex%20Optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%20X%3A%20Practical%20Momentum%20Method%20for%20Stochastic%20Sum-ofNonconvex%20Optimization%202018"
        },
        {
            "id": "5",
            "entry": "[5] Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20How%20To%20Make%20the%20Gradients%20Small%20Stochastically%3A%20Even%20Faster%20Convex%20and%20Nonconvex%20SGD%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20How%20To%20Make%20the%20Gradients%20Small%20Stochastically%3A%20Even%20Faster%20Convex%20and%20Nonconvex%20SGD%202018"
        },
        {
            "id": "6",
            "entry": "[6] Zeyuan Allen-Zhu and Elad Hazan. Variance Reduction for Faster Non-Convex Optimization. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20Reduction%20for%20Faster%20Non-Convex%20Optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20Reduction%20for%20Faster%20Non-Convex%20Optimization%202016"
        },
        {
            "id": "7",
            "entry": "[7] Zeyuan Allen-Zhu and Yuanzhi Li. LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20LazySVD%3A%20Even%20Faster%20SVD%20Decomposition%20Yet%20Without%20Agonizing%20Pain%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20LazySVD%3A%20Even%20Faster%20SVD%20Decomposition%20Yet%20Without%20Agonizing%20Pain%202016"
        },
        {
            "id": "8",
            "entry": "[8] Zeyuan Allen-Zhu and Yuanzhi Li. First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and Near-Optimal Rate. In FOCS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20First%20Efficient%20Convergence%20for%20Streaming%20k-PCA%3A%20a%20Global%2C%20Gap-Free%2C%20and%20Near-Optimal%20Rate%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20First%20Efficient%20Convergence%20for%20Streaming%20k-PCA%3A%20a%20Global%2C%20Gap-Free%2C%20and%20Near-Optimal%20Rate%202017"
        },
        {
            "id": "9",
            "entry": "[9] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Follow%20the%20Compressed%20Leader%3A%20Faster%20Online%20Learning%20of%20Eigenvectors%20and%20Faster%20MMWU%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Follow%20the%20Compressed%20Leader%3A%20Faster%20Online%20Learning%20of%20Eigenvectors%20and%20Faster%20MMWU%202017"
        },
        {
            "id": "10",
            "entry": "[10] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding Local Minima via First-Order Oracles. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Neon2%3A%20Finding%20Local%20Minima%20via%20First-Order%20Oracles%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Li%2C%20Yuanzhi%20Neon2%3A%20Finding%20Local%20Minima%20via%20First-Order%20Oracles%202018"
        },
        {
            "id": "11",
            "entry": "[11] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent. In Proceedings of the 8th Innovations in Theoretical Computer Science, ITCS \u201917, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Orecchia%2C%20Lorenzo%20Linear%20Coupling%3A%20An%20Ultimate%20Unification%20of%20Gradient%20and%20Mirror%20Descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Orecchia%2C%20Lorenzo%20Linear%20Coupling%3A%20An%20Ultimate%20Unification%20of%20Gradient%20and%20Mirror%20Descent%202017"
        },
        {
            "id": "12",
            "entry": "[12] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, Efficient, and Neural Algorithms for Sparse Coding. In COLT, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Simple%2C%20Ankur%20Moitra%20and%20Neural%20Algorithms%20for%20Sparse%20Coding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Simple%2C%20Ankur%20Moitra%20and%20Neural%20Algorithms%20for%20Sparse%20Coding%202015"
        },
        {
            "id": "13",
            "entry": "[13] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated Methods for Non-Convex Optimization. ArXiv e-prints, abs/1611.00756, November 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "14",
            "entry": "[14] Yair Carmon, Oliver Hinder, John C. Duchi, and Aaron Sidford. \u201dConvex Until Proven Guilty\u201d: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Yair%20Hinder%2C%20Oliver%20Duchi%2C%20John%20C.%20and%20Aaron%20Sidford.%E2%80%9DConvex%20Until%20Proven%20Guilty%E2%80%9D%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmon%2C%20Yair%20Hinder%2C%20Oliver%20Duchi%2C%20John%20C.%20and%20Aaron%20Sidford.%E2%80%9DConvex%20Until%20Proven%20Guilty%E2%80%9D%202017"
        },
        {
            "id": "15",
            "entry": "[15] Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages 739\u2013747, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Yuxin%20Candes%2C%20Emmanuel%20Solving%20random%20quadratic%20systems%20of%20equations%20is%20nearly%20as%20easy%20as%20solving%20linear%20systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Yuxin%20Candes%2C%20Emmanuel%20Solving%20random%20quadratic%20systems%20of%20equations%20is%20nearly%20as%20easy%20as%20solving%20linear%20systems%202015"
        },
        {
            "id": "16",
            "entry": "[16] Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "17",
            "entry": "[17] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. In NIPS, pages 2933\u20132941, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014"
        },
        {
            "id": "18",
            "entry": "[18] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20Fast%20Incremental%20Gradient%20Method%20With%20Support%20for%20Non-Strongly%20Convex%20Composite%20Objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20Fast%20Incremental%20Gradient%20Method%20With%20Support%20for%20Non-Strongly%20Convex%20Composite%20Objectives%202014"
        },
        {
            "id": "19",
            "entry": "[19] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "20",
            "entry": "[20] Roy Frostig, Rong Ge, Sham M. Kakade, and Aaron Sidford. Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frostig%2C%20Roy%20Ge%2C%20Rong%20Kakade%2C%20Sham%20M.%20Sidford%2C%20Aaron%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frostig%2C%20Roy%20Ge%2C%20Rong%20Kakade%2C%20Sham%20M.%20Sidford%2C%20Aaron%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "21",
            "entry": "[21] Dan Garber, Elad Hazan, Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Robust%20shift-and-invert%20preconditioning%3A%20Faster%20and%20more%20sample%20efficient%20algorithms%20for%20eigenvector%20computation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Robust%20shift-and-invert%20preconditioning%3A%20Faster%20and%20more%20sample%20efficient%20algorithms%20for%20eigenvector%20computation%202016"
        },
        {
            "id": "22",
            "entry": "[22] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, COLT 2015, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%94online%20stochastic%20gradient%20for%20tensor%20decomposition"
        },
        {
            "id": "23",
            "entry": "[23] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, pages 1\u201326, feb 2015. ISSN 00255610.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202015-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202015-02"
        },
        {
            "id": "24",
            "entry": "[24] I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization problems. ArXiv e-prints, December 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Vinyals%2C%20O.%20Saxe%2C%20A.M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems.%20ArXiv%20e-prints%202014-12"
        },
        {
            "id": "25",
            "entry": "[25] Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic non-convex problems. In International Conference on Machine Learning, pages 1833\u20131841, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Yehuda%20Shalev-Shwartz%2C%20Shai%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Yehuda%20Shalev-Shwartz%2C%20Shai%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016"
        },
        {
            "id": "26",
            "entry": "[26] Xi He, Dheevatsa Mudigere, Mikhail Smelyanskiy, and Martin Takac. Distributed HessianFree Optimization for Deep Neural Network. ArXiv e-prints, abs/1606.00511, June 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00511"
        },
        {
            "id": "27",
            "entry": "[27] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to Escape Saddle Points Efficiently. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20Escape%20Saddle%20Points%20Efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20Escape%20Saddle%20Points%20Efficiently%202017"
        },
        {
            "id": "28",
            "entry": "[28] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, NIPS 2013, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "29",
            "entry": "[29] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ArXiv e-prints, abs/1412.6980, 12 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "30",
            "entry": "[30] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Nonconvex Finite-Sum Optimization Via SCSG Methods. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Nonconvex%20Finite-Sum%20Optimization%20Via%20SCSG%20Methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Nonconvex%20Finite-Sum%20Optimization%20Via%20SCSG%20Methods%202017"
        },
        {
            "id": "31",
            "entry": "[31] Yuanzhi Li and Yang Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20Analysis%20of%20Two-layer%20Neural%20Networks%20with%20ReLU%20Activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20Analysis%20of%20Two-layer%20Neural%20Networks%20with%20ReLU%20Activation%202017"
        },
        {
            "id": "32",
            "entry": "[32] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A Universal Catalyst for First-Order Optimization. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20Universal%20Catalyst%20for%20First-Order%20Optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20Universal%20Catalyst%20for%20First-Order%20Optimization%202015"
        },
        {
            "id": "33",
            "entry": "[33] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course, volume I. Kluwer Academic Publishers, 2004. ISBN 1402075537.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20on%20Convex%20Programming%20Volume%3A%20A%20Basic%20course%2C%20volume%20I%202004"
        },
        {
            "id": "34",
            "entry": "[34] Yurii Nesterov. Accelerating the cubic regularization of newton\u2019s method on convex problems. Mathematical Programming, 112(1):159\u2013181, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Accelerating%20the%20cubic%20regularization%20of%20newton%E2%80%99s%20method%20on%20convex%20problems%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Accelerating%20the%20cubic%20regularization%20of%20newton%E2%80%99s%20method%20on%20convex%20problems%202008"
        },
        {
            "id": "35",
            "entry": "[35] Yurii Nesterov. How to make the gradients small. Optima, 88:10\u201311, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20How%20to%20make%20the%20gradients%20small%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20How%20to%20make%20the%20gradients%20small%202012"
        },
        {
            "id": "36",
            "entry": "[36] Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "37",
            "entry": "[37] Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical biology, 15(3):267\u2013273, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oja%2C%20Erkki%20Simplified%20neuron%20model%20as%20a%20principal%20component%20analyzer%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oja%2C%20Erkki%20Simplified%20neuron%20model%20as%20a%20principal%20component%20analyzer%201982"
        },
        {
            "id": "38",
            "entry": "[38] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "39",
            "entry": "[39] Sashank J Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov, and Alexander J Smola. A generic approach for escaping saddle points. ArXiv e-prints, abs/1709.01434, September 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01434"
        },
        {
            "id": "40",
            "entry": "[40] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. ArXiv e-prints, abs/1309.2388, September 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1309.2388"
        },
        {
            "id": "41",
            "entry": "[41] Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and Trends in Machine Learning, 4(2):107\u2013194, 2012. ISSN 1935-8237.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Online%20Learning%20and%20Online%20Convex%20Optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Online%20Learning%20and%20Online%20Convex%20Optimization%202012"
        },
        {
            "id": "42",
            "entry": "[42] Shai Shalev-Shwartz. SDCA without Duality, Regularization, and Individual Convexity. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20SDCA%20without%20Duality%2C%20Regularization%2C%20and%20Individual%20Convexity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20SDCA%20without%20Duality%2C%20Regularization%2C%20and%20Individual%20Convexity%202016"
        },
        {
            "id": "43",
            "entry": "[43] Ruoyu Sun and Zhi-Quan Luo. Guaranteed Matrix Completion via Nonconvex Factorization. In FOCS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ruoyu%20Luo%2C%20Zhi-Quan%20Guaranteed%20Matrix%20Completion%20via%20Nonconvex%20Factorization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ruoyu%20Luo%2C%20Zhi-Quan%20Guaranteed%20Matrix%20Completion%20via%20Nonconvex%20Factorization%202015"
        },
        {
            "id": "44",
            "entry": "[44] Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic Cubic Regularization for Fast Nonconvex Optimization. ArXiv e-prints, abs/1711.02838, November 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02838"
        },
        {
            "id": "45",
            "entry": "[45] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance Reduction. SIAM Journal on Optimization, 24(4):2057\u2014-2075, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20Proximal%20Stochastic%20Gradient%20Method%20with%20Progressive%20Variance%20Reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20Proximal%20Stochastic%20Gradient%20Method%20with%20Progressive%20Variance%20Reduction%202014"
        },
        {
            "id": "46",
            "entry": "[46] Yi Xu and Tianbao Yang. First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. ArXiv e-prints, abs/1711.01944, November 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01944"
        },
        {
            "id": "47",
            "entry": "[47] Matthew D Zeiler. ADADELTA: an adaptive learning rate method. ArXiv e-prints, abs/1212.5701, 12 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        },
        {
            "id": "48",
            "entry": "[48] Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number independent access of full gradients. In Advances in Neural Information Processing Systems, pages 980\u2013988, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Lijun%20Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Linear%20convergence%20with%20condition%20number%20independent%20access%20of%20full%20gradients%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Lijun%20Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Linear%20convergence%20with%20condition%20number%20independent%20access%20of%20full%20gradients%202013"
        }
    ]
}
