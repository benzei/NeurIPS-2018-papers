{
    "filename": "7969-revisiting-epsilon-gamma-tau-similarity-learning-for-domain-adaptation.pdf",
    "metadata": {
        "title": "Revisiting $(\\epsilon, \\gamma, \\tau)$-similarity learning for domain adaptation",
        "author": "Sofiane Dhouib, Ievgen Redko",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7969-revisiting-epsilon-gamma-tau-similarity-learning-for-domain-adaptation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Similarity learning is an active research area in machine learning that tackles the problem of finding a similarity function tailored to an observable data sample in order to achieve efficient classification. This learning scenario has been generally formalized by the means of a ( , \u03b3, \u03c4 )\u2212good similarity learning framework in the context of supervised classification and has been shown to have strong theoretical guarantees. In this paper, we propose to extend the theoretical analysis of similarity learning to the domain adaptation setting, a particular situation occurring when the similarity is learned and then deployed on samples following different probability distributions. We give a new definition of an ( , \u03b3)\u2212good similarity for domain adaptation and prove several results quantifying the performance of a similarity function on a target domain after it has been trained on a source domain. We particularly show that if the source distribution dominates the target one, then principally new domain adaptation learning bounds can be proved."
    },
    "keywords": [
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        },
        {
            "term": "metric learning",
            "url": "https://en.wikipedia.org/wiki/metric_learning"
        },
        {
            "term": "similarity function",
            "url": "https://en.wikipedia.org/wiki/similarity_function"
        },
        {
            "term": "linear classification",
            "url": "https://en.wikipedia.org/wiki/linear_classification"
        }
    ],
    "highlights": [
        "Many popular supervised learning algorithms rely on pairwise metrics calculated based on the instances of a given data set in order to learn a classifier",
        "Note that in the performed empirical evaluations, the divergence term remains constant for every considered rotation angle and is used only as a trade-off parameter",
        "We provided general theoretical guarantees for the similarity learning framework in the domain adaptation context",
        "Contrary to the previous generalization bounds established for domain adaptation problem, we showed that\u221awhen the source distribution dominates the target one, the bound can be improved via a factor.We further analyzed the worst margin term and showed that its convergence to the true value depends on the complexity of the search space of the similarity function, as well as on the regularity of the hinge loss\u2019s cumulative distribution function at a neighborhood of its maximum value",
        "In order to validate the usefulness of the proposed results, we showed empirically that the minimization of the terms in appearing in the obtained bounds allows to obtain an improved performance over the \u201dno adaptation\u201d baseline without explicitly minimizing the divergence term"
    ],
    "key_statements": [
        "Many popular supervised learning algorithms rely on pairwise metrics calculated based on the instances of a given data set in order to learn a classifier",
        "We present a theoretical study of the ( , \u03b3, \u03c4 )\u2212 framework in the domain adaptation context where only the marginal distributions across the source and the target domains are assumed to change while the labeling functions remain the same2",
        "Assuming that \u03bb is small while our result shows that source error given by the goodness of the similarity function can partially leverage the divergence between the two domains as it multiplies the latter",
        "Note that in the performed empirical evaluations, the divergence term remains constant for every considered rotation angle and is used only as a trade-off parameter",
        "We provided general theoretical guarantees for the similarity learning framework in the domain adaptation context",
        "Contrary to the previous generalization bounds established for domain adaptation problem, we showed that\u221awhen the source distribution dominates the target one, the bound can be improved via a factor.We further analyzed the worst margin term and showed that its convergence to the true value depends on the complexity of the search space of the similarity function, as well as on the regularity of the hinge loss\u2019s cumulative distribution function at a neighborhood of its maximum value",
        "In order to validate the usefulness of the proposed results, we showed empirically that the minimization of the terms in appearing in the obtained bounds allows to obtain an improved performance over the \u201dno adaptation\u201d baseline without explicitly minimizing the divergence term"
    ],
    "summary": [
        "Many popular supervised learning algorithms rely on pairwise metrics calculated based on the instances of a given data set in order to learn a classifier.",
        "Apart from the source goodness, the established inequality contains a term reflecting the distance between the distributions of two domains and a worst margin term measuring the worst error obtainable by the similarity function for some instance from the learning sample.",
        "We start by giving a definition of ( , \u03b3)-goodness with an arbitrary distribution of landmarks, and propose a generalization bound that relates the goodness of the same similarity function learned on the source and target domains.",
        "Assuming that \u03bb is small while our result shows that source error given by the goodness of the similarity function can partially leverage the divergence between the two domains as it multiplies the latter.",
        "We note that the bounds proposed in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] involve a non-estimable term that, similar to \u03bb in (4) is assumed to be small while the worst margin term presented in our result is subject to the analysis provided .",
        "Based on the obtained expression, we note from Lemma 1 that the target goodness can be bounded in terms of both values that characterize the similarity function in the source domain.",
        "Results In Figure 2, we plot the goodness of the similarity function on the target data set before and after adaptation, i.e after solving the minimization problems described above.",
        "The obtained results contain a divergence term between the two domains distributions that naturally appears when bounding the deviation between the same similarity\u2019s performance on them and a worst margin term measuring the worst error obtainable by the similarity function for some instance from the learning sample.",
        "Contrary to the previous generalization bounds established for domain adaptation problem, we showed that\u221awhen the source distribution dominates the target one, the bound can be improved via a factor.We further analyzed the worst margin term and showed that its convergence to the true value depends on the complexity of the search space of the similarity function, as well as on the regularity of the hinge loss\u2019s cumulative distribution function at a neighborhood of its maximum value.",
        "In our new definition of the ( , \u03b3)\u2212goodness, the landmark distribution is assumed to be different from that used to generate source and target data samples and a question about the existence of a landmark distribution that leads to tighter bounds naturally arises.",
        "One can expect to obtain a result showing that the goodness of the similarity learned with source landmarks only is worse than that learned on a mixture distribution.",
        "In order to validate the usefulness of the proposed results, we showed empirically that the minimization of the terms in appearing in the obtained bounds allows to obtain an improved performance over the \u201dno adaptation\u201d baseline without explicitly minimizing the divergence term"
    ],
    "headline": "We propose to extend the theoretical analysis of similarity learning to the domain adaptation setting, a particular situation occurring when the similarity is learned and deployed on samples following different probability distributions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions Information Theory, 13(1):21\u201327, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20T.%20Hart%2C%20P.%20Nearest%20neighbor%20pattern%20classification%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cover%2C%20T.%20Hart%2C%20P.%20Nearest%20neighbor%20pattern%20classification%202006"
        },
        {
            "id": "2",
            "entry": "[2] Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classifiers. In COLT, pages 144\u2013152, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boser%2C%20Bernhard%20E.%20Guyon%2C%20Isabelle%20M.%20Vapnik%2C%20Vladimir%20N.%20A%20training%20algorithm%20for%20optimal%20margin%20classifiers%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boser%2C%20Bernhard%20E.%20Guyon%2C%20Isabelle%20M.%20Vapnik%2C%20Vladimir%20N.%20A%20training%20algorithm%20for%20optimal%20margin%20classifiers%201992"
        },
        {
            "id": "3",
            "entry": "[3] Aur\u00e9lien Bellet, Amaury Habrard, and Marc Sebban. A survey on metric learning for feature vectors and structured data. arXiv preprint arXiv:1306.6709, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1306.6709"
        },
        {
            "id": "4",
            "entry": "[4] Brian Kulis. Metric learning: A survey. Foundations and Trends in Machine Learning, 5(4):287\u2013364, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulis%2C%20Brian%20Metric%20learning%3A%20A%20survey.%20Foundations%20and%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulis%2C%20Brian%20Metric%20learning%3A%20A%20survey.%20Foundations%20and%202013"
        },
        {
            "id": "5",
            "entry": "[5] Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learning with similarity functions. Machine Learning, 72(1-2):89\u2013112, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20Maria-Florina%20Blum%2C%20Avrim%20Srebro%2C%20Nathan%20A%20theory%20of%20learning%20with%20similarity%20functions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20Maria-Florina%20Blum%2C%20Avrim%20Srebro%2C%20Nathan%20A%20theory%20of%20learning%20with%20similarity%20functions%202008"
        },
        {
            "id": "6",
            "entry": "[6] Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. Improved guarantees for learning via similarity functions. In COLT, pages 287\u2013298, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20Maria-Florina%20Blum%2C%20Avrim%20Srebro%2C%20Nathan%20Improved%20guarantees%20for%20learning%20via%20similarity%20functions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20Maria-Florina%20Blum%2C%20Avrim%20Srebro%2C%20Nathan%20Improved%20guarantees%20for%20learning%20via%20similarity%20functions%202008"
        },
        {
            "id": "7",
            "entry": "[7] Aur\u00e9lien Bellet, Amaury Habrard, and Marc Sebban. Similarity learning for provably accurate sparse linear classification. In ICML, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellet%2C%20Aur%C3%A9lien%20Habrard%2C%20Amaury%20Sebban%2C%20Marc%20Similarity%20learning%20for%20provably%20accurate%20sparse%20linear%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellet%2C%20Aur%C3%A9lien%20Habrard%2C%20Amaury%20Sebban%2C%20Marc%20Similarity%20learning%20for%20provably%20accurate%20sparse%20linear%20classification%202012"
        },
        {
            "id": "8",
            "entry": "[8] Zheng-Chu Guo and Yiming Ying. Guaranteed classification via regularized similarity learning. Neural Computation, 26(3):497\u2013522, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Zheng-Chu%20Ying%2C%20Yiming%20Guaranteed%20classification%20via%20regularized%20similarity%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Zheng-Chu%20Ying%2C%20Yiming%20Guaranteed%20classification%20via%20regularized%20similarity%20learning%202014"
        },
        {
            "id": "9",
            "entry": "[9] Maria-Irina Nicolae, \u00c9ric Gaussier, Amaury Habrard, and Marc Sebban. Joint semi-supervised similarity learning for linear classification. In ECML/PKDD, pages 594\u2013609, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nicolae%2C%20Maria-Irina%20Gaussier%2C%20%C3%89ric%20Habrard%2C%20Amaury%20Sebban%2C%20Marc%20Joint%20semi-supervised%20similarity%20learning%20for%20linear%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nicolae%2C%20Maria-Irina%20Gaussier%2C%20%C3%89ric%20Habrard%2C%20Amaury%20Sebban%2C%20Marc%20Joint%20semi-supervised%20similarity%20learning%20for%20linear%20classification%202015"
        },
        {
            "id": "10",
            "entry": "[10] Nicolae Irina, Marc Sebban, Amaury Habrard, Eric Gaussier, and Massih-Reza Amini. Algorithmic Robustness for Semi-Supervised ( , \u03b3, \u03c4 )-Good Metric Learning. In ICONIP, page 10, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Irina%2C%20Nicolae%20Sebban%2C%20Marc%20Amaury%20Habrard%2C%20Eric%20Gaussier%2C%20and%20Massih-Reza%20Amini.%20Algorithmic%20Robustness%20for%20Semi-Supervised%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Irina%2C%20Nicolae%20Sebban%2C%20Marc%20Amaury%20Habrard%2C%20Eric%20Gaussier%2C%20and%20Massih-Reza%20Amini.%20Algorithmic%20Robustness%20for%20Semi-Supervised%202015"
        },
        {
            "id": "11",
            "entry": "[11] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Sinno%20Jialin%20Pan%20and%20Qiang%20Yang.%20A%20survey%20on%20transfer%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Sinno%20Jialin%20Pan%20and%20Qiang%20Yang.%20A%20survey%20on%20transfer%20learning%202010"
        },
        {
            "id": "12",
            "entry": "[12] Anna Margolis. A literature review on domain adaptation with unlabeled data, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Margolis%2C%20Anna%20A%20literature%20review%20on%20domain%20adaptation%20with%20unlabeled%20data%202011"
        },
        {
            "id": "13",
            "entry": "[13] Vishal M. Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE Signal Processing Magazine, 32(3):53\u201369, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patel%2C%20Vishal%20M.%20Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Visual%20domain%20adaptation%3A%20A%20survey%20of%20recent%20advances%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patel%2C%20Vishal%20M.%20Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Visual%20domain%20adaptation%3A%20A%20survey%20of%20recent%20advances%202015"
        },
        {
            "id": "14",
            "entry": "[14] Karl Weiss, Taghi M. Khoshgoftaar, and Ding Wang. A survey of transfer learning. Journal of Big Data, 3(1), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weiss%2C%20Karl%20M%2C%20Taghi%20Khoshgoftaar%2C%20and%20Ding%20Wang.%20A%20survey%20of%20transfer%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weiss%2C%20Karl%20M%2C%20Taghi%20Khoshgoftaar%2C%20and%20Ding%20Wang.%20A%20survey%20of%20transfer%20learning%202016"
        },
        {
            "id": "15",
            "entry": "[15] B. Geng, D. Tao, and C. Xu. Daml: Domain adaptation metric learning. IEEE Transactions on Image Processing, 20(10):2980\u20132989, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geng%2C%20B.%20Tao%2C%20D.%20Xu%2C%20C.%20Daml%3A%20Domain%20adaptation%20metric%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geng%2C%20B.%20Tao%2C%20D.%20Xu%2C%20C.%20Daml%3A%20Domain%20adaptation%20metric%20learning%202011"
        },
        {
            "id": "16",
            "entry": "[16] B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In CVPR, pages 1785\u20131792, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulis%2C%20B.%20Saenko%2C%20K.%20Darrell%2C%20T.%20What%20you%20saw%20is%20not%20what%20you%20get%3A%20Domain%20adaptation%20using%20asymmetric%20kernel%20transforms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulis%2C%20B.%20Saenko%2C%20K.%20Darrell%2C%20T.%20What%20you%20saw%20is%20not%20what%20you%20get%3A%20Domain%20adaptation%20using%20asymmetric%20kernel%20transforms%202011"
        },
        {
            "id": "17",
            "entry": "[17] Bin Cao, Xiaochuan Ni, Jian-Tao Sun, Gang Wang, and Qiang Yang. Distance metric learning under covariate shift. In IJCAI, pages 1204\u20131210, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20Bin%20Ni%2C%20Xiaochuan%20Sun%2C%20Jian-Tao%20Wang%2C%20Gang%20and%20Qiang%20Yang.%20Distance%20metric%20learning%20under%20covariate%20shift%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20Bin%20Ni%2C%20Xiaochuan%20Sun%2C%20Jian-Tao%20Wang%2C%20Gang%20and%20Qiang%20Yang.%20Distance%20metric%20learning%20under%20covariate%20shift%202011"
        },
        {
            "id": "18",
            "entry": "[18] Emilie Morvant, Amaury Habrard, and St\u00e9phane Ayache. Parsimonious unsupervised and semi-supervised domain adaptation with good similarity functions. Knowledge and Information Systems, 33(2):309\u2013349, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morvant%2C%20Emilie%20Habrard%2C%20Amaury%20Ayache%2C%20St%C3%A9phane%20Parsimonious%20unsupervised%20and%20semi-supervised%20domain%20adaptation%20with%20good%20similarity%20functions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morvant%2C%20Emilie%20Habrard%2C%20Amaury%20Ayache%2C%20St%C3%A9phane%20Parsimonious%20unsupervised%20and%20semi-supervised%20domain%20adaptation%20with%20good%20similarity%20functions%202012"
        },
        {
            "id": "19",
            "entry": "[19] Micha\u00ebl Perrot and Amaury Habrard. A theoretical analysis of metric hypothesis transfer learning. In ICML, pages 1708\u20131707, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perrot%2C%20Micha%C3%ABl%20Habrard%2C%20Amaury%20A%20theoretical%20analysis%20of%20metric%20hypothesis%20transfer%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perrot%2C%20Micha%C3%ABl%20Habrard%2C%20Amaury%20A%20theoretical%20analysis%20of%20metric%20hypothesis%20transfer%20learning%202015"
        },
        {
            "id": "20",
            "entry": "[20] Kun Zhang, Bernhard Sch\u00f6lkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In ICML, pages 819\u2013827, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Kun%20Sch%C3%B6lkopf%2C%20Bernhard%20Muandet%2C%20Krikamol%20Wang%2C%20Zhikun%20Domain%20adaptation%20under%20target%20and%20conditional%20shift%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Kun%20Sch%C3%B6lkopf%2C%20Bernhard%20Muandet%2C%20Krikamol%20Wang%2C%20Zhikun%20Domain%20adaptation%20under%20target%20and%20conditional%20shift%202013"
        },
        {
            "id": "21",
            "entry": "[21] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine Learning, 79(1-2):151\u2013175, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20A%20theory%20of%20learning%20from%20different%20domains%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20A%20theory%20of%20learning%20from%20different%20domains%202010"
        },
        {
            "id": "22",
            "entry": "[22] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%3A%20Learning%20bounds%20and%20algorithms%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%3A%20Learning%20bounds%20and%20algorithms%202009"
        },
        {
            "id": "23",
            "entry": "[23] Corinna Cortes and Mehryar Mohri. Domain adaptation in regression. In ALT, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Domain%20adaptation%20in%20regression%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Domain%20adaptation%20in%20regression%202011"
        },
        {
            "id": "24",
            "entry": "[24] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and the r\u00c9nyi divergence. In UAI, pages 367\u2013374, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Multiple%20source%20adaptation%20and%20the%20r%C3%89nyi%20divergence%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Multiple%20source%20adaptation%20and%20the%20r%C3%89nyi%20divergence%202009"
        },
        {
            "id": "25",
            "entry": "[25] Pascal Germain, Amaury Habrard, Fran\u00e7ois Laviolette, and Emilie Morvant. A new pac-bayesian perspective on domain adaptation. In ICML, volume 48, pages 859\u2013868, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Pascal%20Habrard%2C%20Amaury%20Laviolette%2C%20Fran%C3%A7ois%20Morvant%2C%20Emilie%20A%20new%20pac-bayesian%20perspective%20on%20domain%20adaptation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Pascal%20Habrard%2C%20Amaury%20Laviolette%2C%20Fran%C3%A7ois%20Morvant%2C%20Emilie%20A%20new%20pac-bayesian%20perspective%20on%20domain%20adaptation%202016"
        },
        {
            "id": "26",
            "entry": "[26] Olivier Bousquet and Andr\u00e9 Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499\u2013526, March 2002. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olivier%20Bousquet%20and%20Andr%C3%A9%20Elisseeff%20Stability%20and%20generalization%20J%20Mach%20Learn%20Res%202499526%20March%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olivier%20Bousquet%20and%20Andr%C3%A9%20Elisseeff%20Stability%20and%20generalization%20J%20Mach%20Learn%20Res%202499526%20March%202002"
        }
    ]
}
