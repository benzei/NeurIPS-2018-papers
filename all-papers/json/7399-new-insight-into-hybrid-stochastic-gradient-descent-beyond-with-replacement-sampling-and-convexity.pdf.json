{
    "filename": "7399-new-insight-into-hybrid-stochastic-gradient-descent-beyond-with-replacement-sampling-and-convexity.pdf",
    "metadata": {
        "title": "New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity",
        "author": "Pan Zhou, Xiaotong Yuan, Jiashi Feng",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7399-new-insight-into-hybrid-stochastic-gradient-descent-beyond-with-replacement-sampling-and-convexity.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "As an incremental-gradient algorithm, the hybrid stochastic gradient descent (HSGD) enjoys merits of both stochastic and full gradient methods for finitesum problem optimization. However, the existing rate-of-convergence analysis for HSGD is made under with-replacement sampling (WRS) and is restricted to convex problems. It is not clear whether HSGD still carries these advantages under the common practice of without-replacement sampling (WoRS) for non-convex problems. In this paper, we affirmatively answer this open question by showing that under WoRS and for both convex and non-convex problems, it is still possible for HSGD (with constant step-size) to match full gradient descent in rate of convergence, while maintaining comparable sample-size-independent incremental first-order oracle complexity to stochastic gradient descent. For a special class of finite-sum problems with linear prediction models, our convergence results can be further improved in some cases. Extensive numerical results confirm our theoretical affirmation and demonstrate the favorable efficiency of WoRS-based HSGD."
    },
    "keywords": [
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "Natural Science Foundation of China",
            "url": "https://en.wikipedia.org/wiki/Natural_Science_Foundation_of_China"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "convex problem",
            "url": "https://en.wikipedia.org/wiki/convex_problem"
        }
    ],
    "highlights": [
        "Ying et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] proved that for strongly convex problems, both SAGA [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and their proposed AVRG algorithm achieve linear convergence rate with without-replacement sampling",
        "We address the aforementioned three limitations in the existing analysis of Hybrid SGD",
        "Our work differs from these prior works: 1) For the first time, we provide without-replacement sampling based theoretical analysis for Hybrid SGD. 2) Our analysis covers non-strongly convex and non-convex cases which are not covered by current without-replacement sampling theories",
        "We can derive the Incremental First-order Oracle complexity of Hybrid SGD for strongly-convex problems in the following corollary, for which proof is given in Appendix B.2",
        "From Corollary 1, the Incremental First-order Oracle complexity of Hybrid SGD for strongly convex problems is at the order of O \u03ba2 , which is independent of the sample size n",
        "We proved under without-replacement sampling, Hybrid SGD with constant step-size can match FG descent in convergence rate, while maintaining comparable sample-size-independent Incremental First-order Oracle complexity to SGD"
    ],
    "key_statements": [
        "Ying et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] proved that for strongly convex problems, both SAGA [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and their proposed AVRG algorithm achieve linear convergence rate with without-replacement sampling",
        "We address the aforementioned three limitations in the existing analysis of Hybrid SGD",
        "Our work differs from these prior works: 1) For the first time, we provide without-replacement sampling based theoretical analysis for Hybrid SGD. 2) Our analysis covers non-strongly convex and non-convex cases which are not covered by current without-replacement sampling theories",
        "We can derive the Incremental First-order Oracle complexity of Hybrid SGD for strongly-convex problems in the following corollary, for which proof is given in Appendix B.2",
        "From Corollary 1, the Incremental First-order Oracle complexity of Hybrid SGD for strongly convex problems is at the order of O \u03ba2 , which is independent of the sample size n",
        "On the simulated well-conditioned tasks most algorithms achieve high accuracy after one epoch, while Hybrid SGD (WoRS) converges much faster. This confirms Corollary 1 that Hybrid SGD is cheaper in Incremental First-order Oracle complexity (O \u03ba2 ) than other considered variance-reduced algorithms (O n\u03ba2 log 1 ) when the desired accuracy is moderately low and data size is large",
        "We proved under without-replacement sampling, Hybrid SGD with constant step-size can match FG descent in convergence rate, while maintaining comparable sample-size-independent Incremental First-order Oracle complexity to SGD"
    ],
    "summary": [
        "Ying et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] proved that for strongly convex problems, both SAGA [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and their proposed AVRG algorithm achieve linear convergence rate with WoRS.",
        "We can derive the IFO complexity of HSGD for strongly-convex problems in the following corollary, for which proof is given in Appendix B.2.",
        "From Corollary 1, the IFO complexity of HSGD for strongly convex problems is at the order of O \u03ba2 , which is independent of the sample size n.",
        "This is because at each iteration, unlike FGD requiring to access all data, HSGD only samples a mini-batch for gradient estimation without sacrificing convergence rate.",
        "Shamir [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] proved that for linearly structured problems, SGD under WoRS has IFO complexity O \u03ba log \u03ba by measuring the objective.",
        "Theorem 2 shows that HSGD enjoys linear convergence rate on the objective by using exponentially mini-batch size.",
        "We proceed to analyze the convergence performance of HSGD for non-strongly convex problems.",
        "This is the first convergence guarantee of WoRS-based methods for non-strongly convex problems.",
        "Theorem 5 shows if the function h is strongly convex in terms of the linear prediction ai x, by exponentially sampling the data at each iteration, HSGD converges linearly even though f (x) might be non-strongly convex.",
        "Based on Theorem 5, we further derive the IFO complexity of Algorithm 1 for such a special problem, as summarized in Corollary 4 with proof in Appendix C.2.",
        "As the first set of problems are strongly convex, we follow Theorem 2 to exponentially expand the mini-batch size sk in HSGD with \u03c4 = 1.",
        "This confirms Corollary 1 that HSGD is cheaper in IFO complexity (O \u03ba2 ) than other considered variance-reduced algorithms (O n\u03ba2 log 1 ) when the desired accuracy is moderately low and data size is large.",
        "HSGD-lin gives more accurate solutions as it requires smaller batch size without harming convergence rate, consistent with Theorem 4 that advocates linearly increasing batch size.",
        "We analyzed the rate-of-convergence of HSGD under WoRS for strongly/non-strongly convex and non-convex problems.",
        "We proved under WoRS, HSGD with constant step-size can match FG descent in convergence rate, while maintaining comparable sample-size-independent IFO complexity to SGD.",
        "Compared to the variance-reduced SGD methods such as SVRG and SAGA, HSGD has lower cost in cases of large sample number and and moderately low required accuracy."
    ],
    "headline": "We address the aforementioned three limitations in the existing analysis of Hybrid SGD",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. A. Cauchy. M\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution des syst\u00e8mes d\u2019\u00e9quations simultan\u00e9es. Comptesrendus des s\u00e9ances de l\u2019Acad\u00e9mie des sciences de Paris, 25:536\u2013538, 1847. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cauchy%2C%20M.A.%20M%C3%A9thode%20g%C3%A9n%C3%A9rale%20pour%20la%20r%C3%A9solution%20des%20syst%C3%A8mes%20d%E2%80%99%C3%A9quations%20simultan%C3%A9es.%20Comptesrendus%20des%20s%C3%A9ances%20de%20l%E2%80%99Acad%C3%A9mie%20des%20sciences%20de",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cauchy%2C%20M.A.%20M%C3%A9thode%20g%C3%A9n%C3%A9rale%20pour%20la%20r%C3%A9solution%20des%20syst%C3%A8mes%20d%E2%80%99%C3%A9quations%20simultan%C3%A9es.%20Comptesrendus%20des%20s%C3%A9ances%20de%20l%E2%80%99Acad%C3%A9mie%20des%20sciences%20de"
        },
        {
            "id": "2",
            "entry": "[2] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400\u2013407, 1951. 1, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "3",
            "entry": "[3] D. P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM Journal on Optimization, 7(4):913\u2013926, 1997. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20A%20new%20class%20of%20incremental%20gradient%20methods%20for%20least%20squares%20problems%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20D.P.%20A%20new%20class%20of%20incremental%20gradient%20methods%20for%20least%20squares%20problems%201997"
        },
        {
            "id": "4",
            "entry": "[4] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. J. of Machine Learning Research, 14(Feb):567\u2013599, 2013. 1, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "5",
            "entry": "[5] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Proc. Conf. Neutral Information Processing Systems, pages 315\u2013323, 2013. 1, 3, 4, 7, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "6",
            "entry": "[6] M. P. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3):A1380\u2013A1405, 2012. 1, 2, 5, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedlander%2C%20M.P.%20Schmidt%2C%20M.%20Hybrid%20deterministic-stochastic%20methods%20for%20data%20fitting%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedlander%2C%20M.P.%20Schmidt%2C%20M.%20Hybrid%20deterministic-stochastic%20methods%20for%20data%20fitting%202012"
        },
        {
            "id": "7",
            "entry": "[7] P. Zhou, X. Yuan, and J. Feng. Efficient stochastic gradient hard thresholding. In Proc. Conf. Neutral Information Processing Systems, 2018. 1, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20P.%20Yuan%2C%20X.%20Feng%2C%20J.%20Efficient%20stochastic%20gradient%20hard%20thresholding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20P.%20Yuan%2C%20X.%20Feng%2C%20J.%20Efficient%20stochastic%20gradient%20hard%20thresholding%202018"
        },
        {
            "id": "8",
            "entry": "[8] L. Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proc. Symposium on Learning and Data Science, Paris, 2009. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Curiously%20fast%20convergence%20of%20some%20stochastic%20gradient%20descent%20algorithms%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Curiously%20fast%20convergence%20of%20some%20stochastic%20gradient%20descent%20algorithms%202009"
        },
        {
            "id": "9",
            "entry": "[9] M. G\u00fcrb\u00fczbalaban, A. Ozdaglar, and P. Parrilo. Why random reshuffling beats stochastic gradient descent. arXiv preprint arXiv:1510.08560, 2015. 2, 3, 5, 7",
            "arxiv_url": "https://arxiv.org/pdf/1510.08560"
        },
        {
            "id": "10",
            "entry": "[10] S. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In Proc. Int\u2019l Conf. Machine Learning, pages 314\u2013323, 2016. 2, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "11",
            "entry": "[11] Z. Allen-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In Proc. Int\u2019l Conf. Machine Learning, pages 699\u2013707, 2016. 2, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Hazan%2C%20E.%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Hazan%2C%20E.%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "12",
            "entry": "[12] B. Ying, K. Yuan, and A. H. Sayed. Convergence of variance-reduced stochastic learning under random reshuffling. arXiv preprint arXiv:1708.01383, 2017. 2, 3, 6, 7, 8",
            "arxiv_url": "https://arxiv.org/pdf/1708.01383"
        },
        {
            "id": "13",
            "entry": "[13] O. Shamir. Without-replacement sampling for stochastic gradient methods. In Proc. Conf. Neutral Information Processing Systems, pages 46\u201354, 2016. 2, 3, 4, 6, 7, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20O.%20Without-replacement%20sampling%20for%20stochastic%20gradient%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20O.%20Without-replacement%20sampling%20for%20stochastic%20gradient%20methods%202016"
        },
        {
            "id": "14",
            "entry": "[14] B. Recht and C. R\u00e9. Toward a noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences. In Conf. on Learning Theory, pages 1\u201311, 2012. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20B.%20R%C3%A9%2C%20C.%20Toward%20a%20noncommutative%20arithmetic-geometric%20mean%20inequality%3A%20conjectures%2C%20case-studies%2C%20and%20consequences%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20B.%20R%C3%A9%2C%20C.%20Toward%20a%20noncommutative%20arithmetic-geometric%20mean%20inequality%3A%20conjectures%2C%20case-studies%2C%20and%20consequences%202012"
        },
        {
            "id": "15",
            "entry": "[15] A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Proc. Conf. Neutral Information Processing Systems, pages 1646\u20131654, 2014. 3, 4, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "16",
            "entry": "[16] N. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In Proc. Conf. Neutral Information Processing Systems, pages 2663\u20132671, 2012. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20N.L.%20Schmidt%2C%20M.%20Bach%2C%20F.R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20N.L.%20Schmidt%2C%20M.%20Bach%2C%20F.R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "17",
            "entry": "[17] A. Defazio, J. Domke, and T. S. Caetano. Finito: A faster, permutable incremental gradient method for big data problems. In Proc. Int\u2019l Conf. Machine Learning, pages 1125\u20131133, 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Domke%2C%20J.%20Caetano%2C%20T.S.%20Finito%3A%20A%20faster%2C%20permutable%20incremental%20gradient%20method%20for%20big%20data%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Domke%2C%20J.%20Caetano%2C%20T.S.%20Finito%3A%20A%20faster%2C%20permutable%20incremental%20gradient%20method%20for%20big%20data%20problems%202014"
        },
        {
            "id": "18",
            "entry": "[18] H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In Proc. Conf. Neutral Information Processing Systems, pages 3384\u20133392, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015"
        },
        {
            "id": "19",
            "entry": "[19] Z. Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In ACM SIGACT Symposium on Theory of Computing, pages 1200\u20131205, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "20",
            "entry": "[20] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In Proc. Int\u2019l Conf. Machine Learning, pages 353\u2013361, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Xiao%2C%20L.%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Xiao%2C%20L.%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "21",
            "entry": "[21] Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient method. In Proc. Conf. Neutral Information Processing Systems, pages 3059\u20133067, 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Q.%20Lu%2C%20Z.%20Xiao%2C%20L.%20An%20accelerated%20proximal%20coordinate%20gradient%20method%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Q.%20Lu%2C%20Z.%20Xiao%2C%20L.%20An%20accelerated%20proximal%20coordinate%20gradient%20method%202014"
        },
        {
            "id": "22",
            "entry": "[22] J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization. In Proc. Int\u2019l Conf. Machine Learning, 2017. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017"
        },
        {
            "id": "23",
            "entry": "[23] L. Lei and M. Jordan. Less than a single pass: Stochastically controlled stochastic gradient. In Artificial Intelligence and Statistics, pages 148\u2013156, 2017. 4, 7, 8 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20L.%20Jordan%2C%20M.%20Less%20than%20a%20single%20pass%3A%20Stochastically%20controlled%20stochastic%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20L.%20Jordan%2C%20M.%20Less%20than%20a%20single%20pass%3A%20Stochastically%20controlled%20stochastic%20gradient%202017"
        }
    ]
}
