{
    "filename": "7347-quantifying-learning-guarantees-for-convex-but-inconsistent-surrogates.pdf",
    "metadata": {
        "title": "Quantifying Learning Guarantees for Convex but Inconsistent Surrogates",
        "author": "Kirill Struminsky, Simon Lacoste-Julien, Anton Osokin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7347-quantifying-learning-guarantees-for-convex-but-inconsistent-surrogates.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] for the quantitative analysis of consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in a new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difficulty. We apply our theory to two concrete cases: multi-class classification with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential benefits."
    },
    "keywords": [
        {
            "term": "Mean Average Precision",
            "url": "https://en.wikipedia.org/wiki/Mean_Average_Precision"
        },
        {
            "term": "structured prediction",
            "url": "https://en.wikipedia.org/wiki/structured_prediction"
        },
        {
            "term": "quadratic assignment problem",
            "url": "https://en.wikipedia.org/wiki/quadratic_assignment_problem"
        },
        {
            "term": "fisher consistency",
            "url": "https://en.wikipedia.org/wiki/fisher_consistency"
        },
        {
            "term": "non trivial",
            "url": "https://en.wikipedia.org/wiki/non_trivial"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "Consistency is a desirable property of any statistical estimator, which informally means that in the limit of infinite data, the estimator converges to the correct quantity",
        "In the context of machine learning algorithms based on surrogate loss minimization, we usually use the notion of Fisher consistency, which means that the exact minimization of the expected surrogate loss leads to the exact minimization of the actual task loss",
        "It can be shown that Fisher consistency is closely related to the question of infinite-sample consistency (a.k.a. classification calibration) of the surrogate loss with respect to the task loss",
        "We further study the behavior of our bound in two practical scenarios: multi-class classification with a tree-structured loss and ranking with the mean average precision loss",
        "Osokin et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] proved a lower bound on the calibration functions for the quadratic surrogate (11) [14, Theorem 7], which we present to contrast our result presented in Section 4",
        "For the quadratic surrogate (11), we can bound the calibration function from below in such a way that the bound is non-trivial in inconsistent settings"
    ],
    "key_statements": [
        "Consistency is a desirable property of any statistical estimator, which informally means that in the limit of infinite data, the estimator converges to the correct quantity",
        "In the context of machine learning algorithms based on surrogate loss minimization, we usually use the notion of Fisher consistency, which means that the exact minimization of the expected surrogate loss leads to the exact minimization of the actual task loss",
        "It can be shown that Fisher consistency is closely related to the question of infinite-sample consistency (a.k.a. classification calibration) of the surrogate loss with respect to the task loss",
        "Consistency of convex surrogates has been the central question of many studies for such problems as binary classification [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], multi-class classification [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], ranking [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and, more recently, structured prediction [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]",
        "We further study the behavior of our bound in two practical scenarios: multi-class classification with a tree-structured loss and ranking with the mean average precision loss",
        "In Section 4, we prove our main theoretical result, which is a new lower bound on the calibration function",
        "The calibration function can fully characterize consistency of the setting defined by the surrogate loss, the subspace of scores and the task loss",
        "Osokin et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] proved a lower bound on the calibration functions for the quadratic surrogate (11) [14, Theorem 7], which we present to contrast our result presented in Section 4",
        "We study the calibration functions for the loss matrix LmAP and score matrices FmAP and Fsort",
        "For the quadratic surrogate (11), we can bound the calibration function from below in such a way that the bound is non-trivial in inconsistent settings"
    ],
    "summary": [
        "Consistency is a desirable property of any statistical estimator, which informally means that in the limit of infinite data, the estimator converges to the correct quantity.",
        "The calibration function can fully characterize consistency of the setting defined by the surrogate loss, the subspace of scores and the task loss.",
        "A surrogate loss \u03a6 is consistent up to level \u03b7 \u2265 0 w.r.t. a task loss L and a set of scores F if and only if the calibration function satisfies H\u03a6,L,F (\u03b5) > 0 for all \u03b5 > \u03b7 and there exists \u03b5 > \u03b7 such that H\u03a6,L,F (\u03b5) is finite.",
        "The main contribution of this paper is a lower bound on the calibration function (Theorem 3), which is non-zero for \u03b7 > 0 and can be used to obtain convergence rates in inconsistent settings.",
        "Osokin et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] proved a lower bound on the calibration functions for the quadratic surrogate (11) [14, Theorem 7], which we present to contrast our result presented in Section 4.",
        "For any task loss L, its quadratic surrogate \u03a6quad, and a score subspace F , the calibration function is bounded from below: H\u03a6quad,L,F (\u03b5)",
        "In the mAP setting, the ground-truth labels are binary vectors y \u2208 Y = {0, 1}r that indicate the items relevant for the query (a subset of r items-to-rank) and the prediction consists in producing a permutation of items \u03c3 \u2208 Y, Y = Sr. The mAP loss is based on averaging the precision at different levels of recall and is defined as follows: r \u03c3(p)",
        "We study the calibration functions for the loss matrix LmAP and score matrices FmAP and Fsort.",
        "Note that to run the ASGD algorithm for the quadratic surrogate (11), mAP loss and score matrix Fsort, we need to efficiently compute FsTortFsort and FsTortLmAP(:, y).",
        "Despite a large number of works studying consistency and calibration in the context of machine learning, there have been relatively few attempts to obtain guarantees for inconsistent surrogates.",
        "From the previous approaches, we do not put constraints on the data generating distribution, but instead study the connection between the surrogate and task losses by the means of the calibration function, which represents the worst-case scenario.",
        "It would be interesting to combine our quantitative analysis with the constraints on the data distribution, which might give adaptive calibration functions, and with the recent results of Pillaud-Vivien et al [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] showing that under some low-noise assumptions even slow convergence of the surrogate objective can imply exponentially fast convergence of the task loss.",
        "We illustrate the behavior of our bound for two tasks and show examples of conclusions that our approach can give"
    ],
    "headline": "We study consistency properties of machine learning methods based on minimizing convex surrogates",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] \u00c1vila Pires, Bernardo, Ghavamzadeh, Mohammad, and Szepesv\u00e1ri, Csaba. Cost-sensitive multiclass classification risk bounds. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pires%2C%20%C3%81vila%20Bernardo%2C%20Ghavamzadeh%20Mohammad%20Szepesv%C3%A1ri%2C%20Csaba%20Cost-sensitive%20multiclass%20classification%20risk%20bounds%202013"
        },
        {
            "id": "2",
            "entry": "[2] Bartlett, Peter L., Jordan, Michael I., and McAuliffe, Jon D. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Jordan%2C%20Michael%20I.%20McAuliffe%2C%20Jon%20D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Jordan%2C%20Michael%20I.%20McAuliffe%2C%20Jon%20D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006"
        },
        {
            "id": "3",
            "entry": "[3] Ben-David, Shai, Loker, David, Srebro, Nathan, and Sridharan, Karthik. Minimizing the misclassification error rate using a surrogate convex loss. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20Shai%20Loker%2C%20David%20Srebro%2C%20Nathan%20Sridharan%2C%20Karthik%20Minimizing%20the%20misclassification%20error%20rate%20using%20a%20surrogate%20convex%20loss%202012"
        },
        {
            "id": "4",
            "entry": "[4] Buffoni, David, Gallinari, Patrick, Usunier, Nicolas, and Calauz\u00e8nes, Cl\u00e9ment. Learning scoring functions with order-preserving losses and standardized supervision. In ICML, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buffoni%2C%20David%20Gallinari%2C%20Patrick%20Usunier%2C%20Nicolas%20Calauz%C3%A8nes%2C%20Cl%C3%A9ment%20Learning%20scoring%20functions%20with%20order-preserving%20losses%20and%20standardized%20supervision%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buffoni%2C%20David%20Gallinari%2C%20Patrick%20Usunier%2C%20Nicolas%20Calauz%C3%A8nes%2C%20Cl%C3%A9ment%20Learning%20scoring%20functions%20with%20order-preserving%20losses%20and%20standardized%20supervision%202011"
        },
        {
            "id": "5",
            "entry": "[5] Calauz\u00e8nes, Cl\u00e9ment, Usunier, Nicolas, and Gallinari, Patrick. On the (non-)existence of convex, calibrated surrogate losses for ranking. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calauz%C3%A8nes%2C%20Cl%C3%A9ment%20Usunier%2C%20Nicolas%20Gallinari%2C%20Patrick%20On%20the%20%28non-%29existence%20of%20convex%2C%20calibrated%20surrogate%20losses%20for%20ranking%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calauz%C3%A8nes%2C%20Cl%C3%A9ment%20Usunier%2C%20Nicolas%20Gallinari%2C%20Patrick%20On%20the%20%28non-%29existence%20of%20convex%2C%20calibrated%20surrogate%20losses%20for%20ranking%202012"
        },
        {
            "id": "6",
            "entry": "[6] Choromanska, Anna, Agarwal, Alekh, and Langford, John. Extreme multi class classification. In NIPS Workshop: eXtreme Classification, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Agarwal%2C%20Alekh%20Langford%2C%20John%20Extreme%20multi%20class%20classification%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Agarwal%2C%20Alekh%20Langford%2C%20John%20Extreme%20multi%20class%20classification%202013"
        },
        {
            "id": "7",
            "entry": "[7] Ciliberto, Carlo, Rudi, Alessandro, and Rosasco, Lorenzo. A consistent regularization approach for structured prediction. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016"
        },
        {
            "id": "8",
            "entry": "[8] Crammer, Koby and Singer, Yoram. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research (JMLR), 2:265\u2013292, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crammer%2C%20Koby%20Singer%2C%20Yoram%20On%20the%20algorithmic%20implementation%20of%20multiclass%20kernel-based%20vector%20machines%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crammer%2C%20Koby%20Singer%2C%20Yoram%20On%20the%20algorithmic%20implementation%20of%20multiclass%20kernel-based%20vector%20machines%202001"
        },
        {
            "id": "9",
            "entry": "[9] Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "10",
            "entry": "[10] Dorn, William S. Duality in quadratic programming. Quarterly of Applied Mathematics, 18(2):155\u2013162, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dorn%2C%20William%20S.%20Duality%20in%20quadratic%20programming%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dorn%2C%20William%20S.%20Duality%20in%20quadratic%20programming%201960"
        },
        {
            "id": "11",
            "entry": "[11] Duchi, John C., Mackey, Lester W., and Jordan, Michael I. On the consistency of ranking algorithms. In ICML, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Mackey%2C%20Lester%20W.%20Jordan%2C%20Michael%20I.%20On%20the%20consistency%20of%20ranking%20algorithms%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Mackey%2C%20Lester%20W.%20Jordan%2C%20Michael%20I.%20On%20the%20consistency%20of%20ranking%20algorithms%202010"
        },
        {
            "id": "12",
            "entry": "[12] Long, Phil and Servedio, Rocco. Consistency versus realizable H-consistency for multiclass classification. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Phil%20Servedio%2C%20Rocco%20Consistency%20versus%20realizable%20H-consistency%20for%20multiclass%20classification%202013"
        },
        {
            "id": "13",
            "entry": "[13] Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "14",
            "entry": "[14] Osokin, Anton, Bach, Francis, and Lacoste-Julien, Simon. On structured prediction theory with calibrated convex surrogate losses. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017"
        },
        {
            "id": "15",
            "entry": "[15] Pedregosa, Fabian, Bach, Francis, and Gramfort, Alexandre. On the consistency of ordinal regression methods. Journal of Machine Learning Research (JMLR), 18(55):1\u201335, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20Fabian%20Bach%2C%20Francis%20Gramfort%2C%20Alexandre%20On%20the%20consistency%20of%20ordinal%20regression%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20Fabian%20Bach%2C%20Francis%20Gramfort%2C%20Alexandre%20On%20the%20consistency%20of%20ordinal%20regression%20methods%202017"
        },
        {
            "id": "16",
            "entry": "[16] Pillaud-Vivien, Loucas, Rudi, Alessandro, and Bach, Francis. Exponential convergence of testing error for stochastic gradient methods. In COLT, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pillaud-Vivien%2C%20Loucas%20Rudi%2C%20Alessandro%20Bach%2C%20Francis%20Exponential%20convergence%20of%20testing%20error%20for%20stochastic%20gradient%20methods%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pillaud-Vivien%2C%20Loucas%20Rudi%2C%20Alessandro%20Bach%2C%20Francis%20Exponential%20convergence%20of%20testing%20error%20for%20stochastic%20gradient%20methods%202018"
        },
        {
            "id": "17",
            "entry": "[17] Ramaswamy, Harish G. and Agarwal, Shivani. Convex calibration dimension for multiclass loss matrices. Journal of Machine Learning Research (JMLR), 17(14):1\u201345, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramaswamy%2C%20Harish%20G.%20Agarwal%2C%20Shivani%20Convex%20calibration%20dimension%20for%20multiclass%20loss%20matrices%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramaswamy%2C%20Harish%20G.%20Agarwal%2C%20Shivani%20Convex%20calibration%20dimension%20for%20multiclass%20loss%20matrices%202016"
        },
        {
            "id": "18",
            "entry": "[18] Ramaswamy, Harish G., Agarwal, Shivani, and Tewari, Ambuj. Convex calibrated surrogates for low-rank loss matrices with applications to subset ranking losses. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramaswamy%2C%20Harish%20G.%20Agarwal%2C%20Shivani%20Tewari%2C%20Ambuj%20Convex%20calibrated%20surrogates%20for%20low-rank%20loss%20matrices%20with%20applications%20to%20subset%20ranking%20losses%202013"
        },
        {
            "id": "19",
            "entry": "[19] Steinwart, Ingo. How to compare different loss functions and their risks. Constructive Approximation, 26 (2):225\u2013287, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20How%20to%20compare%20different%20loss%20functions%20and%20their%20risks.%20Constructive%20Approximation%202007"
        },
        {
            "id": "20",
            "entry": "[20] Taskar, Ben, Guestrin, Carlos, and Koller, Daphne. Max-margin markov networks. In NIPS, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taskar%2C%20Ben%20Guestrin%2C%20Carlos%20Koller%2C%20Daphne%20Max-margin%20markov%20networks%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taskar%2C%20Ben%20Guestrin%2C%20Carlos%20Koller%2C%20Daphne%20Max-margin%20markov%20networks%202003"
        },
        {
            "id": "21",
            "entry": "[21] Tewari, Ambuj and Bartlett, Peter L. On the consistency of multiclass classification methods. Journal of Machine Learning Research (JMLR), 8:1007\u20131025, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tewari%2C%20Ambuj%20Bartlett%2C%20Peter%20L.%20On%20the%20consistency%20of%20multiclass%20classification%20methods%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tewari%2C%20Ambuj%20Bartlett%2C%20Peter%20L.%20On%20the%20consistency%20of%20multiclass%20classification%20methods%202007"
        },
        {
            "id": "22",
            "entry": "[22] Tsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research (JMLR), 6:1453\u20131484, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsochantaridis%2C%20I.%20Joachims%2C%20T.%20Hofmann%2C%20T.%20Altun%2C%20Y.%20Large%20margin%20methods%20for%20structured%20and%20interdependent%20output%20variables%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsochantaridis%2C%20I.%20Joachims%2C%20T.%20Hofmann%2C%20T.%20Altun%2C%20Y.%20Large%20margin%20methods%20for%20structured%20and%20interdependent%20output%20variables%202005"
        },
        {
            "id": "23",
            "entry": "[23] Zhang, Tong. Statistical analysis of some multi-category large margin classification methods. Journal of Machine Learning Research (JMLR), 5:1225\u20131251, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Statistical%20analysis%20of%20some%20multi-category%20large%20margin%20classification%20methods%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Statistical%20analysis%20of%20some%20multi-category%20large%20margin%20classification%20methods%202004"
        },
        {
            "id": "24",
            "entry": "[24] Zhang, Tong. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, 32(1):56\u2013134, 2004. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Statistical%20behavior%20and%20consistency%20of%20classification%20methods%20based%20on%20convex%20risk%20minimization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Statistical%20behavior%20and%20consistency%20of%20classification%20methods%20based%20on%20convex%20risk%20minimization%202004"
        }
    ]
}
