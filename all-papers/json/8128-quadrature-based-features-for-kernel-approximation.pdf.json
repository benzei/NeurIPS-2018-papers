{
    "filename": "8128-quadrature-based-features-for-kernel-approximation.pdf",
    "metadata": {
        "title": "Quadrature-based features for kernel approximation",
        "author": "Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, Ivan Oseledets",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8128-quadrature-based-features-for-kernel-approximation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider the problem of improving kernel approximation via randomized feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. Based on an efficient numerical integration technique, we propose a unifying approach that reinterprets the previous random features methods and extends to better estimates of the kernel approximation. We derive the convergence behaviour and conduct an extensive empirical study that supports our hypothesis1."
    },
    "keywords": [
        {
            "term": "kernel method",
            "url": "https://en.wikipedia.org/wiki/kernel_method"
        },
        {
            "term": "kernel function",
            "url": "https://en.wikipedia.org/wiki/kernel_function"
        },
        {
            "term": "Gaussian quadratures",
            "url": "https://en.wikipedia.org/wiki/Gaussian_Quadrature"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "time complexity",
            "url": "https://en.wikipedia.org/wiki/time_complexity"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "kernel machine",
            "url": "https://en.wikipedia.org/wiki/kernel_machine"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "space x",
            "url": "https://en.wikipedia.org/wiki/space_x"
        }
    ],
    "highlights": [
        "Kernel methods proved to be an efficient technique in numerous real-world problems",
        "It is a common knowledge that kernel methods incur space and time complexity infeasible to be used with large-scale datasets directly",
        "Unlike other research studies we refrain from using simple Monte Carlo estimate of the integral, instead, we propose to use specific quadrature rules",
        "In what follows we show a brief derivation of the quadrature rules that allow for an explicit mapping of the form: \u03c8(x) = [ a0\u03c6(0) a1\u03c6(w1 x) . . . aD\u03c6 ], where the choice of the weights ai and the points wi is dictated by the quadrature",
        "The Monte Carlo approach has a variety of ways to generate samples: unstructured Gaussian [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], structured Gaussian [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], random orthogonal matrices (ROM) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "We propose an approach for the random features methods for kernel approximation, revealing a new interpretation of Random Fourier Features and ORF"
    ],
    "key_statements": [
        "Kernel methods proved to be an efficient technique in numerous real-world problems",
        "It is a common knowledge that kernel methods incur space and time complexity infeasible to be used with large-scale datasets directly",
        "Unlike other research studies we refrain from using simple Monte Carlo estimate of the integral, instead, we propose to use specific quadrature rules",
        "We propose to use spherical-radial quadrature rules to improve kernel approximation accuracy",
        "We show that these quadrature rules generalize the Random Fourier Features-based techniques",
        "In what follows we show a brief derivation of the quadrature rules that allow for an explicit mapping of the form: \u03c8(x) = [ a0\u03c6(0) a1\u03c6(w1 x) . . . aD\u03c6 ], where the choice of the weights ai and the points wi is dictated by the quadrature",
        "We present a comparison of our method (B) with estimators based on a simple Monte Carlo, quasi-Monte Carlo [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] and Gaussian quadratures [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "The Monte Carlo approach has a variety of ways to generate samples: unstructured Gaussian [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], structured Gaussian [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], random orthogonal matrices (ROM) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "We propose an approach for the random features methods for kernel approximation, revealing a new interpretation of Random Fourier Features and ORF"
    ],
    "summary": [
        "Kernel methods proved to be an efficient technique in numerous real-world problems.",
        "We propose to use spherical-radial quadrature rules to improve kernel approximation accuracy.",
        "Stochastic spherical rules SQ(s) = wjs(Qzj), where Q is a random orthogonal matrix, j=1 approximate an integral of a function s(z) over the surface of unit d-sphere Ud, where zj are points on Ud, i.e. zj zj = 1.",
        "RFF [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] makes approximation of the RBF kernel in exactly the same way: it generates random vector from Gaussian distribution and calculates the corresponding feature map.",
        "Random Fourier Features for RBF kernel are SR rules of degree (1, 1).",
        "Let us compare SR1,3 rules with Orthogonal Random Features [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] for the RBF kernel.",
        "Orthogonal Random Features for RBF kernel are SR rules of degree (1, 3).",
        "We test both methods for random orthogonal matrix generation and, since their performance coincides, we leave this one out for cleaner figures in the Experiments section.",
        "The Monte Carlo approach has a variety of ways to generate samples: unstructured Gaussian [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], structured Gaussian [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], random orthogonal matrices (ROM) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>].",
        "Quasi-Monte Carlo integration boasts improved rate of convergence 1/D compared to 1/\u221aD of Monte Carlo, as empirical results illustrate its performance is poorer than that of orthogonal random features [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "In line with previous work we run experiments for the kernel approximation on a random subset of a dataset.",
        "Figure 1 shows the results for kernel approximation error on LETTER, MNIST, CIFAR100 and LEUKEMIA datasets.",
        "The empirical results in Figure 1 support our hypothesis about the advantages of SR quadratures applied to kernel approximation compared to SOTA methods.",
        "We examine the performance with the same setting as in experiments for kernel approximation error, except we map the whole dataset.",
        "The construction of basis functions in these techniques utilizes the given training set making them more attractive for some problems compared to Random Fourier Features approach.",
        "In [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>] the authors make attempt to improve quality of the approximation of Random Fourier Features by optimizing sequences conditioning on a given dataset.",
        "Among the recent papers there are works that, similar to our approach, use the numerical integration methods to approximate kernels.",
        "While [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] carefully inspects the connection between random features and quadratures, they did not provide any practically useful explicit mappings for kernels.",
        "We propose an approach for the random features methods for kernel approximation, revealing a new interpretation of RFF and ORF.",
        "The results showed that the quality of the downstream task is superior or comparable to the state-of-the-art baselines"
    ],
    "headline": "Based on an efficient numerical integration technique, we propose a unifying approach that reinterprets the previous random features methods and extends to better estimates of the kernel approximation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Theodore W Anderson, Ingram Olkin, and Les G Underhill. Generation of random orthogonal matrices. SIAM Journal on Scientific and Statistical Computing, 8(4):625\u2013629, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Theodore%20W.%20Olkin%2C%20Ingram%20Underhill%2C%20Les%20G.%20Generation%20of%20random%20orthogonal%20matrices%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Theodore%20W.%20Olkin%2C%20Ingram%20Underhill%2C%20Les%20G.%20Generation%20of%20random%20orthogonal%20matrices%201987"
        },
        {
            "id": "2",
            "entry": "[2] Haim Avron and Vikas Sindhwani. High-performance kernel machines with implicit distributed optimization and randomization. Technometrics, 58(3):341\u2013349, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avron%2C%20Haim%20Sindhwani%2C%20Vikas%20High-performance%20kernel%20machines%20with%20implicit%20distributed%20optimization%20and%20randomization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avron%2C%20Haim%20Sindhwani%2C%20Vikas%20High-performance%20kernel%20machines%20with%20implicit%20distributed%20optimization%20and%20randomization%202016"
        },
        {
            "id": "3",
            "entry": "[3] Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions. Journal of Machine Learning Research, 18(21):1\u201338, 2017. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017"
        },
        {
            "id": "4",
            "entry": "[4] John A Baker. Integration over spheres and the divergence theorem for balls. The American Mathematical Monthly, 104(1):36\u201347, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20John%20A.%20Integration%20over%20spheres%20and%20the%20divergence%20theorem%20for%20balls%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20John%20A.%20Integration%20over%20spheres%20and%20the%20divergence%20theorem%20for%20balls%201997"
        },
        {
            "id": "5",
            "entry": "[5] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International Conference on Machine Learning, pages 115\u2013123, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20James%20Yamins%2C%20Daniel%20Cox%2C%20David%20Making%20a%20science%20of%20model%20search%3A%20Hyperparameter%20optimization%20in%20hundreds%20of%20dimensions%20for%20vision%20architectures%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20James%20Yamins%2C%20Daniel%20Cox%2C%20David%20Making%20a%20science%20of%20model%20search%3A%20Hyperparameter%20optimization%20in%20hundreds%20of%20dimensions%20for%20vision%20architectures%202013"
        },
        {
            "id": "6",
            "entry": "[6] Salomon Bochner. Monotone funktionen, stieltjessche integrale und harmonische analyse. Mathematische Annalen, 108(1):378\u2013410, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bochner%2C%20Salomon%20Monotone%20funktionen%2C%20stieltjessche%20integrale%20und%20harmonische%20analyse%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bochner%2C%20Salomon%20Monotone%20funktionen%2C%20stieltjessche%20integrale%20und%20harmonische%20analyse%201933"
        },
        {
            "id": "7",
            "entry": "[7] Xixian Chen, Haiqin Yang, Irwin King, and Michael R Lyu. Training-efficient feature map for shift-invariant kernels. In IJCAI, pages 3395\u20133401, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xixian%20Yang%2C%20Haiqin%20King%2C%20Irwin%20Lyu%2C%20Michael%20R.%20Training-efficient%20feature%20map%20for%20shift-invariant%20kernels%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xixian%20Yang%2C%20Haiqin%20King%2C%20Irwin%20Lyu%2C%20Michael%20R.%20Training-efficient%20feature%20map%20for%20shift-invariant%20kernels%202015"
        },
        {
            "id": "8",
            "entry": "[8] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural Information Processing Systems, pages 342\u2013350, 2009. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009"
        },
        {
            "id": "9",
            "entry": "[9] Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear time kernel expansions. arXiv preprint arXiv:1605.09049, 2016. 1",
            "arxiv_url": "https://arxiv.org/pdf/1605.09049"
        },
        {
            "id": "10",
            "entry": "[10] Krzysztof Choromanski, Mark Rowland, and Adrian Weller. The unreasonable effectiveness of random orthogonal embeddings. arXiv preprint arXiv:1703.00864, 2017. 6, 8",
            "arxiv_url": "https://arxiv.org/pdf/1703.00864"
        },
        {
            "id": "11",
            "entry": "[11] Tri Dao, Christopher M De Sa, and Christopher R\u00e9. Gaussian quadrature for kernel features. In Advances in Neural Information Processing Systems, pages 6109\u20136119, 2017. 6, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dao%2C%20Tri%20Sa%2C%20Christopher%20M.De%20R%C3%A9%2C%20Christopher%20Gaussian%20quadrature%20for%20kernel%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dao%2C%20Tri%20Sa%2C%20Christopher%20M.De%20R%C3%A9%2C%20Christopher%20Gaussian%20quadrature%20for%20kernel%20features%202017"
        },
        {
            "id": "12",
            "entry": "[12] Petros Drineas and Michael W Mahoney. On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6(Dec):2153\u20132175, 2005. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20On%20the%20Nystr%C3%B6m%20method%20for%20approximating%20a%20Gram%20matrix%20for%20improved%20kernel-based%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20On%20the%20Nystr%C3%B6m%20method%20for%20approximating%20a%20Gram%20matrix%20for%20improved%20kernel-based%20learning%202005"
        },
        {
            "id": "13",
            "entry": "[13] Kai-Tai Fang and Run-Ze Li. Some methods for generating both an NT-net and the uniform distribution on a Stiefel manifold and their applications. Computational Statistics & Data Analysis, 24(1):29\u201346, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fang%2C%20Kai-Tai%20Li%2C%20Run-Ze%20Some%20methods%20for%20generating%20both%20an%20NT-net%20and%20the%20uniform%20distribution%20on%20a%20Stiefel%20manifold%20and%20their%20applications%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fang%2C%20Kai-Tai%20Li%2C%20Run-Ze%20Some%20methods%20for%20generating%20both%20an%20NT-net%20and%20the%20uniform%20distribution%20on%20a%20Stiefel%20manifold%20and%20their%20applications%201997"
        },
        {
            "id": "14",
            "entry": "[14] X Yu Felix, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and Sanjiv Kumar. Orthogonal Random Features. In Advances in Neural Information Processing Systems, pages 1975\u20131983, 2016. 1, 3, 6, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Felix%2C%20X.Yu%20Suresh%2C%20Ananda%20Theertha%20Choromanski%2C%20Krzysztof%20M.%20Holtmann-Rice%2C%20Daniel%20N.%20Orthogonal%20Random%20Features%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Felix%2C%20X.Yu%20Suresh%2C%20Ananda%20Theertha%20Choromanski%2C%20Krzysztof%20M.%20Holtmann-Rice%2C%20Daniel%20N.%20Orthogonal%20Random%20Features%201975"
        },
        {
            "id": "15",
            "entry": "[15] Shai Fine and Katya Scheinberg. Efficient SVM training using low-rank kernel representations. Journal of Machine Learning Research, 2(Dec):243\u2013264, 2001. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fine%2C%20Shai%20Scheinberg%2C%20Katya%20Efficient%20SVM%20training%20using%20low-rank%20kernel%20representations%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fine%2C%20Shai%20Scheinberg%2C%20Katya%20Efficient%20SVM%20training%20using%20low-rank%20kernel%20representations%202001"
        },
        {
            "id": "16",
            "entry": "[16] Alexander Forrester, Andy Keane, et al. Engineering design via surrogate modelling: a practical guide. John Wiley & Sons, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forrester%2C%20Alexander%20Keane%2C%20Andy%20Engineering%20design%20via%20surrogate%20modelling%3A%20a%20practical%20guide%202008"
        },
        {
            "id": "17",
            "entry": "[17] Alan Genz. Methods for generating random orthogonal matrices. Monte Carlo and Quasi-Monte Carlo Methods, pages 199\u2013213, 1998. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genz%2C%20Alan%20Methods%20for%20generating%20random%20orthogonal%20matrices%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genz%2C%20Alan%20Methods%20for%20generating%20random%20orthogonal%20matrices%201998"
        },
        {
            "id": "18",
            "entry": "[18] Alan Genz and John Monahan. Stochastic integration rules for infinite regions. SIAM journal on scientific computing, 19(2):426\u2013439, 1998. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genz%2C%20Alan%20Monahan%2C%20John%20Stochastic%20integration%20rules%20for%20infinite%20regions%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genz%2C%20Alan%20Monahan%2C%20John%20Stochastic%20integration%20rules%20for%20infinite%20regions%201998"
        },
        {
            "id": "19",
            "entry": "[19] Alan Genz and John Monahan. A stochastic algorithm for high-dimensional integrals over unbounded regions with gaussian weight. Journal of Computational and Applied Mathematics, 112(1):71\u201381, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genz%2C%20Alan%20Monahan%2C%20John%20A%20stochastic%20algorithm%20for%20high-dimensional%20integrals%20over%20unbounded%20regions%20with%20gaussian%20weight%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genz%2C%20Alan%20Monahan%2C%20John%20A%20stochastic%20algorithm%20for%20high-dimensional%20integrals%20over%20unbounded%20regions%20with%20gaussian%20weight%201999"
        },
        {
            "id": "20",
            "entry": "[20] Todd R Golub, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, Mignon L Loh, James R Downing, Mark A Caligiuri, et al. Molecular classification of cancer: class discovery and class prediction by gene expression monitoring. Science, 286(5439):531\u2013537, 1999. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golub%2C%20Todd%20R.%20Slonim%2C%20Donna%20K.%20Tamayo%2C%20Pablo%20Huard%2C%20Christine%20Molecular%20classification%20of%20cancer%3A%20class%20discovery%20and%20class%20prediction%20by%20gene%20expression%20monitoring%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golub%2C%20Todd%20R.%20Slonim%2C%20Donna%20K.%20Tamayo%2C%20Pablo%20Huard%2C%20Christine%20Molecular%20classification%20of%20cancer%3A%20class%20discovery%20and%20class%20prediction%20by%20gene%20expression%20monitoring%201999"
        },
        {
            "id": "21",
            "entry": "[21] Simon Haykin. Cognitive dynamic systems: perception-action cycle, radar and radio. Cambridge University Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haykin%2C%20Simon%20Cognitive%20dynamic%20systems%3A%20perception-action%20cycle%2C%20radar%20and%20radio%202012"
        },
        {
            "id": "22",
            "entry": "[22] Po-Sen Huang, Haim Avron, Tara N Sainath, Vikas Sindhwani, and Bhuvana Ramabhadran. Kernel methods match deep neural networks on timit. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 205\u2013209. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Po-Sen%20Avron%2C%20Haim%20Sainath%2C%20Tara%20N.%20Sindhwani%2C%20Vikas%20Kernel%20methods%20match%20deep%20neural%20networks%20on%20timit%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Po-Sen%20Avron%2C%20Haim%20Sainath%2C%20Tara%20N.%20Sindhwani%2C%20Vikas%20Kernel%20methods%20match%20deep%20neural%20networks%20on%20timit%202014"
        },
        {
            "id": "23",
            "entry": "[23] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "24",
            "entry": "[24] Quoc Le, Tam\u00e1s Sarl\u00f3s, and Alex Smola. Fastfood-approximating kernel expansions in loglinear time. In Proceedings of the International Conference on Machine Learning, 2013. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20Sarl%C3%B3s%2C%20Tam%C3%A1s%20Smola%2C%20Alex%20Fastfood-approximating%20kernel%20expansions%20in%20loglinear%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20Sarl%C3%B3s%2C%20Tam%C3%A1s%20Smola%2C%20Alex%20Fastfood-approximating%20kernel%20expansions%20in%20loglinear%20time%202013"
        },
        {
            "id": "25",
            "entry": "[25] Francesco Mezzadri. How to generate random matrices from the classical compact groups. arXiv preprint math-ph/0609050, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mezzadri%2C%20Francesco%20How%20to%20generate%20random%20matrices%20from%20the%20classical%20compact%20groups.%20arXiv%20preprint%20math-ph/0609050%202006"
        },
        {
            "id": "26",
            "entry": "[26] John Monahan and Alan Genz. Spherical-radial integration rules for bayesian computation. Journal of the American Statistical Association, 92(438):664\u2013674, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Monahan%2C%20John%20Genz%2C%20Alan%20Spherical-radial%20integration%20rules%20for%20bayesian%20computation%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Monahan%2C%20John%20Genz%2C%20Alan%20Spherical-radial%20integration%20rules%20for%20bayesian%20computation%201997"
        },
        {
            "id": "27",
            "entry": "[27] Art B Owen. Latin supercube sampling for very high-dimensional simulations. ACM Transactions on Modeling and Computer Simulation (TOMACS), 8(1):71\u2013102, 1998. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Owen%2C%20Art%20B.%20Latin%20supercube%20sampling%20for%20very%20high-dimensional%20simulations%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Owen%2C%20Art%20B.%20Latin%20supercube%20sampling%20for%20very%20high-dimensional%20simulations%201998"
        },
        {
            "id": "28",
            "entry": "[28] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, pages 1177\u20131184, 2008. 1, 3, 6, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "29",
            "entry": "[29] Walter Rudin. Fourier analysis on groups. Courier Dover Publications, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudin%2C%20Walter%20Fourier%20analysis%20on%20groups%202017"
        },
        {
            "id": "30",
            "entry": "[30] Alex J Smola and Bernhard Sch\u00f6lkopf. Sparse greedy matrix approximation for machine learning. 2000. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20Alex%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20Sparse%20greedy%20matrix%20approximation%20for%20machine%20learning%202000"
        },
        {
            "id": "31",
            "entry": "[31] G. W. Stewart. The efficient generation of random orthogonal matrices with an application to condition estimators. SIAM Journal on Numerical Analysis, 17(3):403\u2013409, 1980. ISSN 00361429. URL http://www.jstor.org/stable/2156882.",
            "url": "http://www.jstor.org/stable/2156882",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stewart%2C%20G.W.%20The%20efficient%20generation%20of%20random%20orthogonal%20matrices%20with%20an%20application%20to%20condition%20estimators%201980"
        },
        {
            "id": "32",
            "entry": "[32] Dougal J Sutherland and Jeff Schneider. On the error of random fourier features. arXiv preprint arXiv:1506.02785, 2015. 5",
            "arxiv_url": "https://arxiv.org/pdf/1506.02785"
        },
        {
            "id": "33",
            "entry": "[33] Christopher KI Williams. Computing with infinite networks. In Advances in Neural Information Processing Systems, pages 295\u2013301, 1997. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20K.I.%20Computing%20with%20infinite%20networks%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Christopher%20K.I.%20Computing%20with%20infinite%20networks%201997"
        },
        {
            "id": "34",
            "entry": "[34] Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney. Quasi-Monte Carlo feature maps for shift-invariant kernels. In Proceedings of The 31st International Conference on Machine Learning (ICML-14), pages 485\u2013493, 2014. 1, 6, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Jiyan%20Sindhwani%2C%20Vikas%20Avron%2C%20Haim%20Mahoney%2C%20Michael%20Quasi-Monte%20Carlo%20feature%20maps%20for%20shift-invariant%20kernels%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Jiyan%20Sindhwani%2C%20Vikas%20Avron%2C%20Haim%20Mahoney%2C%20Michael%20Quasi-Monte%20Carlo%20feature%20maps%20for%20shift-invariant%20kernels%202014"
        },
        {
            "id": "35",
            "entry": "[35] Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystr\u00f6m Method vs Random Fourier Features: A Theoretical and Empirical Comparison. In Advances in Neural Information Processing Systems, pages 476\u2013484, 2012. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Tianbao%20Li%2C%20Yu-Feng%20Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Nystr%C3%B6m%20Method%20vs%20Random%20Fourier%20Features%3A%20A%20Theoretical%20and%20Empirical%20Comparison%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Tianbao%20Li%2C%20Yu-Feng%20Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Nystr%C3%B6m%20Method%20vs%20Random%20Fourier%20Features%3A%20A%20Theoretical%20and%20Empirical%20Comparison%202012"
        },
        {
            "id": "36",
            "entry": "[36] Felix X Yu, Sanjiv Kumar, Henry Rowley, and Shih-Fu Chang. Compact nonlinear maps and circulant extensions. arXiv preprint arXiv:1503.03893, 2015. 8 ",
            "arxiv_url": "https://arxiv.org/pdf/1503.03893"
        }
    ]
}
