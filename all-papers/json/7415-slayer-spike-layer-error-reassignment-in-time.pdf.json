{
    "filename": "7415-slayer-spike-layer-error-reassignment-in-time.pdf",
    "metadata": {
        "title": "SLAYER: Spike Layer Error Reassignment in Time",
        "author": "Sumit Bam Shrestha, Garrick Orchard",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Configuring deep Spiking Neural Networks (SNNs) is an exciting research avenue for low power spike event based computation. However, the spike generation function is non-differentiable and therefore not directly compatible with the standard error backpropagation algorithm. In this paper, we introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers. We describe and release a GPU accelerated software implementation of our method which allows training both fully connected and convolutional neural network (CNN) architectures. Using our software, we compare our method against existing SNN based learning approaches and standard ANN to SNN conversion techniques and show that our method achieves state of the art performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS datasets."
    },
    "keywords": [
        {
            "term": "Self Organizing Map",
            "url": "https://en.wikipedia.org/wiki/Self_Organizing_Map"
        },
        {
            "term": "Artificial Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Artificial_Neural_Networks"
        },
        {
            "term": "SLAYER",
            "url": "https://en.wikipedia.org/wiki/Slayer"
        },
        {
            "term": "Probability Density Function",
            "url": "https://en.wikipedia.org/wiki/Probability_Density_Function"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "object recognition",
            "url": "https://en.wikipedia.org/wiki/object_recognition"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "low power",
            "url": "https://en.wikipedia.org/wiki/low_power"
        },
        {
            "term": "computational power",
            "url": "https://en.wikipedia.org/wiki/computational_power"
        }
    ],
    "highlights": [
        "Artificial Neural Networks (ANNs), especially Deep Neural Networks, have become the go-to tool for many machine learning tasks",
        "Artificial Neural Networks achieve state of the art performance in applications ranging from image classification and object recognition, to object tracking, signal processing, natural language processing, self driving cars, health care diagnostics, and many more",
        "Unlike the activation functions used in non-spiking Artificial Neural Networks, the derivative of the spike function is undefined which is a major obstacle for backpropagating error from output to input for spiking neurons",
        "Afterwards we present results of classification tasks performed on both spiking datasets and non-spiking datasets converted to spikes",
        "We have proposed a new error backpropagation for spiking neurons which properly considers the temporal dependency between input and output signals of a spiking neuron, handles the non-differentiable nature of the spike function, and is not prone to the dead neuron problem",
        "We have demonstrated Spike LAYer Error Reassignment\u2019s effectiveness in achieving state of the art accuracy for an spiking neurons on spoken digit and visual digit recognition as well as visual action recognition"
    ],
    "key_statements": [
        "Artificial Neural Networks (ANNs), especially Deep Neural Networks, have become the go-to tool for many machine learning tasks",
        "Artificial Neural Networks achieve state of the art performance in applications ranging from image classification and object recognition, to object tracking, signal processing, natural language processing, self driving cars, health care diagnostics, and many more",
        "The main contribution of this paper is a general method of error backpropagation for spiking neurons (Section 3) which we call Spike LAYer Error Reassignment (SLAYER)",
        "We demonstrate Spike LAYer Error Reassignment achieving state of the art accuracy for an spiking neurons on neuromorphic datasets (Section 4) for visual digit recognition, action recognition, and spoken digit recognition",
        "Unlike the activation functions used in non-spiking Artificial Neural Networks, the derivative of the spike function is undefined which is a major obstacle for backpropagating error from output to input for spiking neurons",
        "Afterwards we present results of classification tasks performed on both spiking datasets and non-spiking datasets converted to spikes",
        "We use our CUDA accelerated spiking neurons deep learning framework for Spike LAYer Error Reassignment to perform all the simulations for which results are presented in this paper",
        "Since Spike LAYer Error Reassignment is a spike based learning algorithm, the images are converted into spike trains spanning 25 ms using Generalized Integrate and Fire Model of neuron [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>]",
        "The TrueNorth approach uses additional neurons before the convolutional neural network classifier for pre-processing, whereas in Spike LAYer Error Reassignment, the spike data from the Dynamic Vision Sensor is directly fed into the classifier.\n4.5",
        "We use audio data converted to spikes using the MFCC transform followed by a Self Organizing Map (SOM) as described in [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>].For training, we specify a target of 5 spikes for false classes and 20 spikes for the true class",
        "Spike LAYer Error Reassignment significantly improves upon the testing accuracy results of spiking neurons based approach using Self Organizing Map-spiking neurons [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] on the same encoded spike inputs",
        "We have proposed a new error backpropagation for spiking neurons which properly considers the temporal dependency between input and output signals of a spiking neuron, handles the non-differentiable nature of the spike function, and is not prone to the dead neuron problem",
        "We have demonstrated Spike LAYer Error Reassignment\u2019s effectiveness in achieving state of the art accuracy for an spiking neurons on spoken digit and visual digit recognition as well as visual action recognition",
        "The desired spike count was chosen to be roughly proportional to the simulation interval"
    ],
    "summary": [
        "Artificial Neural Networks (ANNs), especially Deep Neural Networks, have become the go-to tool for many machine learning tasks.",
        "The main contribution of this paper is a general method of error backpropagation for SNNs (Section 3) which we call Spike LAYer Error Reassignment (SLAYER).",
        "Unlike the activation functions used in non-spiking ANNs, the derivative of the spike function is undefined which is a major obstacle for backpropagating error from output to input for SNNs. note that the effect of an input spike is distributed in future via the spike response kernels which is the reason for temporal dependency in the spiking neuron.",
        "The other two categories train directly on the SNN but differ in how they approximate the derivative of the spike function.",
        "The third category of methods backpropagate errors based on the membrane potential of a spiking neuron at a single time step only.",
        "Where s(t) is the target spike train, L(s(t), s(t)) is the loss at time instance t and e(s(t), s(t)) is the error signal at the final layer.",
        "We use our CUDA accelerated SNN deep learning framework for SLAYER to perform all the simulations for which results are presented in this paper.",
        "The task is to learn to fire the desired spike train for the random spike inputs using an SNN with 25 hidden neurons.",
        "Since SLAYER is a spike based learning algorithm, the images are converted into spike trains spanning 25 ms using Generalized Integrate and Fire Model of neuron [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>].",
        "The TrueNorth approach uses additional neurons before the CNN classifier for pre-processing, whereas in SLAYER, the spike data from the DVS is directly fed into the classifier.",
        "We use audio data converted to spikes using the MFCC transform followed by a Self Organizing Map (SOM) as described in [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>].For training, we specify a target of 5 spikes for false classes and 20 spikes for the true class.",
        "SLAYER significantly improves upon the testing accuracy results of SNN based approach using SOM-SNN [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] on the same encoded spike inputs.",
        "We have proposed a new error backpropagation for SNNs which properly considers the temporal dependency between input and output signals of a spiking neuron, handles the non-differentiable nature of the spike function, and is not prone to the dead neuron problem.",
        "The temporal error credit assignment and axonal delay do increase the computational complexity requirements, respectively comprising 8.03% and 2.55% of the computation time for the training of fully connected NMNIST network, the computational overhead is not significant."
    ],
    "headline": "We introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Wolfgang Maass. Lower bounds for the computational power of networks of spiking neurons. Neural Computation, 8(1):1\u201340, January 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maass%2C%20Wolfgang%20Lower%20bounds%20for%20the%20computational%20power%20of%20networks%20of%20spiking%20neurons%201996-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maass%2C%20Wolfgang%20Lower%20bounds%20for%20the%20computational%20power%20of%20networks%20of%20spiking%20neurons%201996-01"
        },
        {
            "id": "2",
            "entry": "[2] Wolfgang Maass. Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons. In Michael Mozer, Michael I. Jordan, and Thomas Petsche, editors, Advances in Neural Information Processing Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996, pages 211\u2013217. MIT Press, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maass%2C%20Wolfgang%20Noisy%20spiking%20neurons%20with%20temporal%20coding%20have%20more%20computational%20power%20than%20sigmoidal%20neurons%201996-12-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maass%2C%20Wolfgang%20Noisy%20spiking%20neurons%20with%20temporal%20coding%20have%20more%20computational%20power%20than%20sigmoidal%20neurons%201996-12-02"
        },
        {
            "id": "3",
            "entry": "[3] Wolfgang Maass and Michael Schmitt. On the complexity of learning for a spiking neuron. In Proceedings of the tenth annual conference on Computational learning theory, pages 54\u201361. ACM, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maass%2C%20Wolfgang%20Schmitt%2C%20Michael%20On%20the%20complexity%20of%20learning%20for%20a%20spiking%20neuron%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maass%2C%20Wolfgang%20Schmitt%2C%20Michael%20On%20the%20complexity%20of%20learning%20for%20a%20spiking%20neuron%201997"
        },
        {
            "id": "4",
            "entry": "[4] Paul A. Merolla, John V. Arthur, Rodrigo Alvarez-Icaza, Andrew S. Cassidy, Jun Sawada, Filipp Akopyan, Bryan L. Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, Bernard Brezzo, Ivan Vo, Steven K. Esser, Rathinakumar Appuswamy, Brian Taba, Arnon Amir, Myron D. Flickner, William P. Risk, Rajit Manohar, and Dharmendra S. Modha. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197):668\u2013673, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Alvarez-Icaza%2C%20Rodrigo%20Cassidy%2C%20Andrew%20S.%20A%20million%20spiking-neuron%20integrated%20circuit%20with%20a%20scalable%20communication%20network%20and%20interface%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Alvarez-Icaza%2C%20Rodrigo%20Cassidy%2C%20Andrew%20S.%20A%20million%20spiking-neuron%20integrated%20circuit%20with%20a%20scalable%20communication%20network%20and%20interface%202014"
        },
        {
            "id": "5",
            "entry": "[5] Steve B. Furber, Francesco Galluppi, Steve Temple, and Luis A. Plana. The spinnaker project. Proceedings of the IEEE, 102(5):652\u2013665, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Furber%2C%20Steve%20B.%20Galluppi%2C%20Francesco%20Temple%2C%20Steve%20Plana%2C%20Luis%20A.%20The%20spinnaker%20project%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Furber%2C%20Steve%20B.%20Galluppi%2C%20Francesco%20Temple%2C%20Steve%20Plana%2C%20Luis%20A.%20The%20spinnaker%20project%202014"
        },
        {
            "id": "6",
            "entry": "[6] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro, 38(1):82\u201399, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davies%2C%20Mike%20Srinivasa%2C%20Narayan%20Lin%2C%20Tsung-Han%20Chinya%2C%20Gautham%20Loihi%3A%20A%20neuromorphic%20manycore%20processor%20with%20on-chip%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davies%2C%20Mike%20Srinivasa%2C%20Narayan%20Lin%2C%20Tsung-Han%20Chinya%2C%20Gautham%20Loihi%3A%20A%20neuromorphic%20manycore%20processor%20with%20on-chip%20learning%202018"
        },
        {
            "id": "7",
            "entry": "[7] Filip Ponulak and Andrzej Kasinski. Supervised learning in spiking neural networks with ReSuMe: Sequence learning, classification, and spike shifting. Neural Computation, 22(2):467\u2013510, October 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ponulak%2C%20Filip%20Kasinski%2C%20Andrzej%20Supervised%20learning%20in%20spiking%20neural%20networks%20with%20ReSuMe%3A%20Sequence%20learning%2C%20classification%2C%20and%20spike%20shifting%202009-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ponulak%2C%20Filip%20Kasinski%2C%20Andrzej%20Supervised%20learning%20in%20spiking%20neural%20networks%20with%20ReSuMe%3A%20Sequence%20learning%2C%20classification%2C%20and%20spike%20shifting%202009-10"
        },
        {
            "id": "8",
            "entry": "[8] Ammar Mohemmed, Stefan Schliebs, Satoshi Matsuda, and Nikola Kasabov. Span: Spike pattern association neuron for learning spatio-temporal spike patterns. International Journal of Neural Systems, 22(04):1250012, 2012. PMID: 22830962.",
            "pubmed_url": "https://www.ncbi.nlm.nih.gov/pubmed/22830962",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohemmed%2C%20Ammar%20Schliebs%2C%20Stefan%20Matsuda%2C%20Satoshi%20Kasabov%2C%20Nikola%20Span%3A%20Spike%20pattern%20association%20neuron%20for%20learning%20spatio-temporal%20spike%20patterns%202012"
        },
        {
            "id": "9",
            "entry": "[9] Robert G\u00fctig and Haim Sompolinsky. The tempotron: a neuron that learns spike timing\u2013based decisions. Nature neuroscience, 9(3):420\u2013428, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%C3%BCtig%2C%20Robert%20Sompolinsky%2C%20Haim%20The%20tempotron%3A%20a%20neuron%20that%20learns%20spike%20timing%E2%80%93based%20decisions%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%C3%BCtig%2C%20Robert%20Sompolinsky%2C%20Haim%20The%20tempotron%3A%20a%20neuron%20that%20learns%20spike%20timing%E2%80%93based%20decisions%202006"
        },
        {
            "id": "10",
            "entry": "[10] Sander M. Bohte, Joost N. Kok, and Han La Poutre. Error-backpropagation in temporally encoded networks of spiking neurons. Neurocomputing, 48(1):17\u201337, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bohte%2C%20Sander%20M.%20Kok%2C%20Joost%20N.%20Poutre%2C%20Han%20La%20Error-backpropagation%20in%20temporally%20encoded%20networks%20of%20spiking%20neurons%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bohte%2C%20Sander%20M.%20Kok%2C%20Joost%20N.%20Poutre%2C%20Han%20La%20Error-backpropagation%20in%20temporally%20encoded%20networks%20of%20spiking%20neurons%202002"
        },
        {
            "id": "11",
            "entry": "[11] Sumit Bam Shrestha and Qing Song. Robust spike-train learning in spike-event based weight update. Neural Networks, 96:33 \u2013 46, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrestha%2C%20Sumit%20Bam%20Song%2C%20Qing%20Robust%20spike-train%20learning%20in%20spike-event%20based%20weight%20update%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrestha%2C%20Sumit%20Bam%20Song%2C%20Qing%20Robust%20spike-train%20learning%20in%20spike-event%20based%20weight%20update%202017"
        },
        {
            "id": "12",
            "entry": "[12] Priyadarshini Panda and Kaushik Roy. Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition. In 2016 International Joint Conference on Neural Networks (IJCNN), pages 299\u2013306, July 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panda%2C%20Priyadarshini%20Roy%2C%20Kaushik%20Unsupervised%20regenerative%20learning%20of%20hierarchical%20features%20in%20spiking%20deep%20networks%20for%20object%20recognition%202016-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panda%2C%20Priyadarshini%20Roy%2C%20Kaushik%20Unsupervised%20regenerative%20learning%20of%20hierarchical%20features%20in%20spiking%20deep%20networks%20for%20object%20recognition%202016-07"
        },
        {
            "id": "13",
            "entry": "[13] Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. Training deep spiking neural networks using backpropagation. Frontiers in Neuroscience, 10:508, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jun%20Haeng%20Delbruck%2C%20Tobi%20Pfeiffer%2C%20Michael%20Training%20deep%20spiking%20neural%20networks%20using%20backpropagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jun%20Haeng%20Delbruck%2C%20Tobi%20Pfeiffer%2C%20Michael%20Training%20deep%20spiking%20neural%20networks%20using%20backpropagation%202016"
        },
        {
            "id": "14",
            "entry": "[14] Friedemann Zenke and Surya Ganguli. Superspike: Supervised learning in multi-layer spiking neural networks. arXiv preprint arXiv:1705.11146, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.11146"
        },
        {
            "id": "15",
            "entry": "[15] Benjamin Schrauwen and Jan Van Campenhout. Extending SpikeProp. In Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, volume 1. IEEE, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benjamin%20Schrauwen%20and%20Jan%20Van%20Campenhout%20Extending%20SpikeProp%20In%20Neural%20Networks%202004%20Proceedings%202004%20IEEE%20International%20Joint%20Conference%20on%20volume%201%20IEEE%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benjamin%20Schrauwen%20and%20Jan%20Van%20Campenhout%20Extending%20SpikeProp%20In%20Neural%20Networks%202004%20Proceedings%202004%20IEEE%20International%20Joint%20Conference%20on%20volume%201%20IEEE%202004"
        },
        {
            "id": "16",
            "entry": "[16] Aboozar Taherkhani, Ammar Belatreche, Yuhua Li, and Liam P Maguire. DL-ReSuMe: a delay learningbased remote supervised method for spiking neurons. IEEE transactions on neural networks and learning systems, 26(12):3137\u20133149, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taherkhani%2C%20Aboozar%20Belatreche%2C%20Ammar%20Li%2C%20Yuhua%20Maguire%2C%20Liam%20P.%20DL-ReSuMe%3A%20a%20delay%20learningbased%20remote%20supervised%20method%20for%20spiking%20neurons%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taherkhani%2C%20Aboozar%20Belatreche%2C%20Ammar%20Li%2C%20Yuhua%20Maguire%2C%20Liam%20P.%20DL-ReSuMe%3A%20a%20delay%20learningbased%20remote%20supervised%20method%20for%20spiking%20neurons%202015"
        },
        {
            "id": "17",
            "entry": "[17] Wulfram Gerstner. Time structure of the activity in neural network models. Phys. Rev. E, 51:738\u2013758, Jan 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerstner%2C%20Wulfram%20Time%20structure%20of%20the%20activity%20in%20neural%20network%20models%201995-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gerstner%2C%20Wulfram%20Time%20structure%20of%20the%20activity%20in%20neural%20network%20models%201995-01"
        },
        {
            "id": "18",
            "entry": "[18] Steve K. Esser, Rathinakumar Appuswamy, Paul Merolla, John V. Arthur, and Dharmendra S. Modha. Backpropagation for energy-efficient neuromorphic computing. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1117\u20131125. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esser%2C%20Steve%20K.%20Appuswamy%2C%20Rathinakumar%20Merolla%2C%20Paul%20Arthur%2C%20John%20V.%20Backpropagation%20for%20energy-efficient%20neuromorphic%20computing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esser%2C%20Steve%20K.%20Appuswamy%2C%20Rathinakumar%20Merolla%2C%20Paul%20Arthur%2C%20John%20V.%20Backpropagation%20for%20energy-efficient%20neuromorphic%20computing%202015"
        },
        {
            "id": "19",
            "entry": "[19] Eric Hunsberger and Chris Eliasmith. Spiking deep networks with LIF neurons. CoRR, abs/1510.08829, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.08829"
        },
        {
            "id": "20",
            "entry": "[20] Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, and Dharmendra S. Modha. Convolutional networks for fast, energy-efficient neuromorphic computing. Proceedings of the National Academy of Sciences, 113(41):11441\u201311446, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esser%2C%20Steven%20K.%20Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Cassidy%2C%20Andrew%20S.%20Convolutional%20networks%20for%20fast%2C%20energy-efficient%20neuromorphic%20computing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esser%2C%20Steven%20K.%20Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Cassidy%2C%20Andrew%20S.%20Convolutional%20networks%20for%20fast%2C%20energy-efficient%20neuromorphic%20computing%202016"
        },
        {
            "id": "21",
            "entry": "[21] Peter O\u2019Connor, Daniel Neil, Shih-Chii Liu, Tobi Delbruck, and Michael Pfeiffer. Real-time classification and sensor fusion with a spiking deep belief network. Frontiers in neuroscience, 7:178, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Connor%2C%20Peter%20Neil%2C%20Daniel%20Liu%2C%20Shih-Chii%20Delbruck%2C%20Tobi%20Real-time%20classification%20and%20sensor%20fusion%20with%20a%20spiking%20deep%20belief%20network%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=O%E2%80%99Connor%2C%20Peter%20Neil%2C%20Daniel%20Liu%2C%20Shih-Chii%20Delbruck%2C%20Tobi%20Real-time%20classification%20and%20sensor%20fusion%20with%20a%20spiking%20deep%20belief%20network%202013"
        },
        {
            "id": "22",
            "entry": "[22] Qian Liu, Yunhua Chen, and Steve B. Furber. Noisy softplus: an activation function that enables snns to be trained as anns. CoRR, abs/1706.03609, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03609"
        },
        {
            "id": "23",
            "entry": "[23] Peter U. Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer. Fastclassifying, high-accuracy spiking deep networks through weight and threshold balancing. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diehl%2C%20Peter%20U.%20Neil%2C%20Daniel%20Binas%2C%20Jonathan%20Cook%2C%20Matthew%20Fastclassifying%2C%20high-accuracy%20spiking%20deep%20networks%20through%20weight%20and%20threshold%20balancing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diehl%2C%20Peter%20U.%20Neil%2C%20Daniel%20Binas%2C%20Jonathan%20Cook%2C%20Matthew%20Fastclassifying%2C%20high-accuracy%20spiking%20deep%20networks%20through%20weight%20and%20threshold%20balancing%202015"
        },
        {
            "id": "24",
            "entry": "[24] Peter U. Diehl, Bruno U. Pedroni, Andrew S. Cassidy, Paul Merolla, Emre Neftci, and Guido Zarrella. Truehappiness: Neuromorphic emotion recognition on truenorth. In 2016 International Joint Conference on Neural Networks (IJCNN), pages 4278\u20134285, July 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peter%20U%20Diehl%20Bruno%20U%20Pedroni%20Andrew%20S%20Cassidy%20Paul%20Merolla%20Emre%20Neftci%20and%20Guido%20Zarrella%20Truehappiness%20Neuromorphic%20emotion%20recognition%20on%20truenorth%20In%202016%20International%20Joint%20Conference%20on%20Neural%20Networks%20IJCNN%20pages%2042784285%20July%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peter%20U%20Diehl%20Bruno%20U%20Pedroni%20Andrew%20S%20Cassidy%20Paul%20Merolla%20Emre%20Neftci%20and%20Guido%20Zarrella%20Truehappiness%20Neuromorphic%20emotion%20recognition%20on%20truenorth%20In%202016%20International%20Joint%20Conference%20on%20Neural%20Networks%20IJCNN%20pages%2042784285%20July%202016"
        },
        {
            "id": "25",
            "entry": "[25] Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. Frontiers in Neuroscience, 11:682, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rueckauer%2C%20Bodo%20Lungu%2C%20Iulia-Alexandra%20Hu%2C%20Yuhuang%20Pfeiffer%2C%20Michael%20Conversion%20of%20continuous-valued%20deep%20networks%20to%20efficient%20event-driven%20networks%20for%20image%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rueckauer%2C%20Bodo%20Lungu%2C%20Iulia-Alexandra%20Hu%2C%20Yuhuang%20Pfeiffer%2C%20Michael%20Conversion%20of%20continuous-valued%20deep%20networks%20to%20efficient%20event-driven%20networks%20for%20image%20classification%202017"
        },
        {
            "id": "26",
            "entry": "[26] Sam McKennoch, Dingding Liu, and Linda G Bushnell. Fast modifications of the SpikeProp algorithm. In Neural Networks, 2006. IJCNN\u201906. International Joint Conference on, pages 3970\u20133977. IEEE, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McKennoch%2C%20Sam%20Liu%2C%20Dingding%20Bushnell%2C%20Linda%20G.%20Fast%20modifications%20of%20the%20SpikeProp%20algorithm%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McKennoch%2C%20Sam%20Liu%2C%20Dingding%20Bushnell%2C%20Linda%20G.%20Fast%20modifications%20of%20the%20SpikeProp%20algorithm%202006"
        },
        {
            "id": "27",
            "entry": "[27] Justin Dauwels, Fran\u00e7ois Vialatte, Theophane Weber, and Andrzej Cichocki. On similarity measures for spike trains. In Advances in Neuro-Information Processing, pages 177\u2013185.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauwels%2C%20Justin%20Vialatte%2C%20Fran%C3%A7ois%20Weber%2C%20Theophane%20Cichocki%2C%20Andrzej%20On%20similarity%20measures%20for%20spike%20trains",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauwels%2C%20Justin%20Vialatte%2C%20Fran%C3%A7ois%20Weber%2C%20Theophane%20Cichocki%2C%20Andrzej%20On%20similarity%20measures%20for%20spike%20trains"
        },
        {
            "id": "28",
            "entry": "[28] Wulfram Gerstner and Werner M Kistler. Spiking neuron models: Single neurons, populations, plasticity. Cambridge university press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerstner%2C%20Wulfram%20Kistler%2C%20Werner%20M.%20Spiking%20neuron%20models%3A%20Single%20neurons%2C%20populations%2C%20plasticity%202002"
        },
        {
            "id": "29",
            "entry": "[29] Renaud Jolivet, J Timothy, and Wulfram Gerstner. The spike response model: a framework to predict neuronal spike trains. In Artificial Neural Networks and Neural Information Processing\u2013ICANN/ICONIP 2003, pages 846\u2013853.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Renaud%20Jolivet%2C%20J.Timothy%20Gerstner%2C%20Wulfram%20The%20spike%20response%20model%3A%20a%20framework%20to%20predict%20neuronal%20spike%20trains%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Renaud%20Jolivet%2C%20J.Timothy%20Gerstner%2C%20Wulfram%20The%20spike%20response%20model%3A%20a%20framework%20to%20predict%20neuronal%20spike%20trains%202003"
        },
        {
            "id": "30",
            "entry": "[30] Gregory K. Cohen, Garrick Orchard, Sio-Hoi Leng, Jonathan Tapson, Ryad B. Benosman, and Andre van Schaik. Skimming digits: Neuromorphic classification of spike-encoded images. Frontiers in Neuroscience, 10:184, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Gregory%20K.%20Orchard%2C%20Garrick%20Leng%2C%20Sio-Hoi%20Tapson%2C%20Jonathan%20Skimming%20digits%3A%20Neuromorphic%20classification%20of%20spike-encoded%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Gregory%20K.%20Orchard%2C%20Garrick%20Leng%2C%20Sio-Hoi%20Tapson%2C%20Jonathan%20Skimming%20digits%3A%20Neuromorphic%20classification%20of%20spike-encoded%20images%202016"
        },
        {
            "id": "31",
            "entry": "[31] Bharath Ramesh, Hong Yang, Garrick Orchard, Ngoc Anh Le Thi, and Cheng Xiang. DART: distribution aware retinal transform for event-based cameras. CoRR, abs/1710.10800, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10800"
        },
        {
            "id": "32",
            "entry": "[32] Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo di Nolfo, Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, Jeff Kusnitz, Michael Debole, Steve Esser, Tobi Delbruck, Myron Flickner, and Dharmendra Modha. A low power, fully event-based gesture recognition system. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amir%2C%20Arnon%20Taba%2C%20Brian%20Berg%2C%20David%20Melano%2C%20Timothy%20A%20low%20power%2C%20fully%20event-based%20gesture%20recognition%20system%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amir%2C%20Arnon%20Taba%2C%20Brian%20Berg%2C%20David%20Melano%2C%20Timothy%20A%20low%20power%2C%20fully%20event-based%20gesture%20recognition%20system%202017-07"
        },
        {
            "id": "33",
            "entry": "[33] Jibin Wu, Yansong Chua, and Haizhou Li. A biologically plausible speech recognition framework based on spiking neural networks. In International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2018 (Accepted).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jibin%20Chua%2C%20Yansong%20Li%2C%20Haizhou%20A%20biologically%20plausible%20speech%20recognition%20framework%20based%20on%20spiking%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jibin%20Chua%2C%20Yansong%20Li%2C%20Haizhou%20A%20biologically%20plausible%20speech%20recognition%20framework%20based%20on%20spiking%20neural%20networks%202018"
        },
        {
            "id": "34",
            "entry": "[34] Amirhossein Tavanaei and Anthony Maida. Bio-inspired multi-layer spiking neural network extracts discriminative features from speech signals. In Derong Liu, Shengli Xie, Yuanqing Li, Dongbin Zhao, and El-Sayed M. El-Alfy, editors, Neural Information Processing, pages 899\u2013908, Cham, 2017. Springer International Publishing.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tavanaei%2C%20Amirhossein%20Maida%2C%20Anthony%20Bio-inspired%20multi-layer%20spiking%20neural%20network%20extracts%20discriminative%20features%20from%20speech%20signals%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tavanaei%2C%20Amirhossein%20Maida%2C%20Anthony%20Bio-inspired%20multi-layer%20spiking%20neural%20network%20extracts%20discriminative%20features%20from%20speech%20signals%202017"
        },
        {
            "id": "35",
            "entry": "[35] Jonathan W. Pillow, Liam Paninski, Valerie J. Uzzell, Eero P. Simoncelli, and E. J. Chichilnisky. Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model. The Journal of Neuroscience, 25(47):11003\u201311013, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pillow%2C%20Jonathan%20W.%20Paninski%2C%20Liam%20Uzzell%2C%20Valerie%20J.%20Simoncelli%2C%20Eero%20P.%20Prediction%20and%20decoding%20of%20retinal%20ganglion%20cell%20responses%20with%20a%20probabilistic%20spiking%20model%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pillow%2C%20Jonathan%20W.%20Paninski%2C%20Liam%20Uzzell%2C%20Valerie%20J.%20Simoncelli%2C%20Eero%20P.%20Prediction%20and%20decoding%20of%20retinal%20ganglion%20cell%20responses%20with%20a%20probabilistic%20spiking%20model%202005"
        },
        {
            "id": "36",
            "entry": "[36] Garrick Orchard, Ajinkya Jayawant, Gregory K. Cohen, and Nitish Thakor. Converting static image datasets to spiking neuromorphic datasets using saccades. Frontiers in Neuroscience, 9:437, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orchard%2C%20Garrick%20Jayawant%2C%20Ajinkya%20Cohen%2C%20Gregory%20K.%20Thakor%2C%20Nitish%20Converting%20static%20image%20datasets%20to%20spiking%20neuromorphic%20datasets%20using%20saccades%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orchard%2C%20Garrick%20Jayawant%2C%20Ajinkya%20Cohen%2C%20Gregory%20K.%20Thakor%2C%20Nitish%20Converting%20static%20image%20datasets%20to%20spiking%20neuromorphic%20datasets%20using%20saccades%202015"
        },
        {
            "id": "37",
            "entry": "[37] R Gary Leonard and George Doddington. Tidigits speech corpus. Texas Instruments, Inc, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leonard%2C%20R.Gary%20Doddington%2C%20George%20Tidigits%20speech%20corpus%201993"
        },
        {
            "id": "38",
            "entry": "[38] M. Abdollahi and S. C. Liu. Speaker-independent isolated digit recognition using an aer silicon cochlea. In 2011 IEEE Biomedical Circuits and Systems Conference (BioCAS), pages 269\u2013272, Nov 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abdollahi%2C%20M.%20Liu%2C%20S.C.%20Speaker-independent%20isolated%20digit%20recognition%20using%20an%20aer%20silicon%20cochlea%202011-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abdollahi%2C%20M.%20Liu%2C%20S.C.%20Speaker-independent%20isolated%20digit%20recognition%20using%20an%20aer%20silicon%20cochlea%202011-11"
        }
    ]
}
