{
    "filename": "7504-metagan-an-adversarial-approach-to-few-shot-learning.pdf",
    "metadata": {
        "title": "MetaGAN: An Adversarial Approach to Few-Shot Learning",
        "author": "Ruixiang ZHANG, Tong Che, Zoubin Ghahramani, Yoshua Bengio, Yangqiu Song",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7504-metagan-an-adversarial-approach-to-few-shot-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we propose a conceptually simple and general framework called MetaGAN for few-shot learning problems. Most state-of-the-art few-shot classification models can be integrated with MetaGAN in a principled and straightforward way. By introducing an adversarial generator conditioned on tasks, we augment vanilla few-shot classification models with the ability to discriminate between real and fake data. We argue that this GAN-based approach can help few-shot classifiers to learn sharper decision boundary, which could generalize better. We show that with our MetaGAN framework, we can extend supervised few-shot learning models to naturally cope with unlabeled data. Different from previous work in semi-supervised few-shot learning, our algorithms can deal with semi-supervision at both sample-level and task-level. We give theoretical justifications of the strength of MetaGAN, and validate the effectiveness of MetaGAN on challenging few-shot image classification benchmarks."
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        }
    ],
    "highlights": [
        "FEW-SHOT LEARNING<br/><br/>We formally define few-shot learning problems as following: Given a distribution of tasks P (T ), a sample task T from P (T ) is given by a joint distribution PXT \u00d7Y (x, y), where the task is to predict y given x",
        "We argue that adversarial training can help few-shot learning models by making it easier to learn better decision boundaries between different classes",
        "Following the ideas behind the theoretical justifications studied in the semi-supervised learning setting, we provide similar justifications in the few-shot learning problem",
        "We propose MetaGAN, a simple and generic framework to boost the performance of few-shot learning models",
        "Our approach is based on the idea that fake samples produced by the generator can help classifiers learn a sharper decision boundary between different classes from a few samples",
        "We modified the techniques used for semi-supervised learning with GANs to work in the few-shot learning scenario"
    ],
    "key_statements": [
        "FEW-SHOT LEARNING<br/><br/>We formally define few-shot learning problems as following: Given a distribution of tasks P (T ), a sample task T from P (T ) is given by a joint distribution PXT \u00d7Y (x, y), where the task is to predict y given x",
        "We argue that adversarial training can help few-shot learning models by making it easier to learn better decision boundaries between different classes",
        "Training data is usually very limited for each task, we show that how fake data generated by a non-perfect generator in MetaGAN can help the classifier identify much tighter decision boundaries and can help boost the performance of few-shot learning",
        "Following the ideas behind the theoretical justifications studied in the semi-supervised learning setting, we provide similar justifications in the few-shot learning problem",
        "We proposed a new learning setting for the few-shot learning problem in section 3.4: task-level semi-supervised few-shot learning",
        "Existing few-shot learning models[<a class=\"ref-link\" id=\"cRavi_2017_a\" href=\"#rRavi_2017_a\">Ravi and Larochelle, 2017</a>, <a class=\"ref-link\" id=\"cSung_et+al_2018_a\" href=\"#rSung_et+al_2018_a\">Sung et al, 2018</a>, <a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\">Ren et al, 2018</a>] are unable to effectively leverage purely unsupervised tasks, which consist of only unlabeled samples in both support set and query set",
        "We propose MetaGAN, a simple and generic framework to boost the performance of few-shot learning models",
        "Our approach is based on the idea that fake samples produced by the generator can help classifiers learn a sharper decision boundary between different classes from a few samples",
        "We modified the techniques used for semi-supervised learning with GANs to work in the few-shot learning scenario"
    ],
    "summary": [
        "FEW-SHOT LEARNING<br/><br/>We formally define few-shot learning problems as following: Given a distribution of tasks P (T ), a sample task T from P (T ) is given by a joint distribution PXT \u00d7Y (x, y), where the task is to predict y given x.",
        "For sample-level semi-supervised few-shot learning, we allow some training samples to be unlabeled within a task.",
        "For task-level semi-supervised few-shot learning, we allow purely unsupervised tasks, in which both support and query samples are all unlabeled.",
        "The MetaGAN algorithm is able to learn to infer the shape and boundaries of data manifolds of the task-specific data distribution from both labeled and unlabeled examples.",
        "Training data is usually very limited for each task, we show that how fake data generated by a non-perfect generator in MetaGAN can help the classifier identify much tighter decision boundaries and can help boost the performance of few-shot learning.",
        "The key idea behind MetaGAN is that imperfect generators in GAN models can provide fake data between the manifolds of different real data classes, providing additional training signals to the classifier as well as making the decision boundaries much sharper.",
        "As proposed in [<a class=\"ref-link\" id=\"cSalimans_et+al_2016_a\" href=\"#rSalimans_et+al_2016_a\">Salimans et al, 2016</a>] we adopt the \"feature matching loss\" as the generator loss LG in both sample-level and task-level semi-supervised few-shot learning.",
        "In a few-shot classification problem, the model tries to optimize a decision boundary for each task with just a few samples in each class.",
        "As introduced in section 3.4, we evaluate the effectiveness of our proposed MetaGAN in the samplelevel semi-supervised few-shot learning setting, following a similar training and evaluation scheme without \"distractors\" to that proposed in [<a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\"><a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\"><a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\">Ren et al, 2018</a></a></a>] (We will point out the differences in the scheme later on).",
        "The classifier trained with our proposed MetaGAN formulation is encouraged to form better decision boundaries by utilizing unlabeled and fake data, and is free from the demands of unlabeled samples during testing, different from the kmeans-based refining model [<a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\"><a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\"><a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\">Ren et al, 2018</a></a></a>] which strongly relies on the unlabeled data for testing.",
        "Existing few-shot learning models[<a class=\"ref-link\" id=\"cRavi_2017_a\" href=\"#rRavi_2017_a\">Ravi and Larochelle, 2017</a>, <a class=\"ref-link\" id=\"cSung_et+al_2018_a\" href=\"#rSung_et+al_2018_a\">Sung et al, 2018</a>, <a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\"><a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\"><a class=\"ref-link\" id=\"cRen_et+al_2018_a\" href=\"#rRen_et+al_2018_a\">Ren et al, 2018</a></a></a>] are unable to effectively leverage purely unsupervised tasks, which consist of only unlabeled samples in both support set and query set.",
        "To demonstrate that our proposed MetaGAN model can successfully learn from unsupervised tasks, we create new splits of Omniglot and Mini-Imagenet datasets.",
        "We propose MetaGAN, a simple and generic framework to boost the performance of few-shot learning models.",
        "Our approach is based on the idea that fake samples produced by the generator can help classifiers learn a sharper decision boundary between different classes from a few samples.",
        "We give intuitive as well as theoretical justifications of the proposed approach"
    ],
    "headline": "We propose a conceptually simple and general framework called MetaGAN for few-shot learning problems",
    "reference_links": [
        {
            "id": "Antoniou_et+al_2018_a",
            "entry": "Anthreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. 2018. URL https://openreview.net/forum?id=S1Auv-WRZ.",
            "url": "https://openreview.net/forum?id=S1Auv-WRZ"
        },
        {
            "id": "Che_et+al_2017_a",
            "entry": "Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Che%2C%20Tong%20Li%2C%20Yanran%20Jacob%2C%20Athul%20Paul%20Bengio%2C%20Yoshua%20Mode%20regularized%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Che%2C%20Tong%20Li%2C%20Yanran%20Jacob%2C%20Athul%20Paul%20Bengio%2C%20Yoshua%20Mode%20regularized%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2172\u20132180. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "Dai_et+al_2017_a",
            "entry": "Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semisupervised learning that requires a bad gan. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6510\u20136520. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7229-good-semi-supervised-learning-that-requires-a-bad-gan.pdf.",
            "url": "http://papers.nips.cc/paper/7229-good-semi-supervised-learning-that-requires-a-bad-gan.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Zihang%20Yang%2C%20Zhilin%20Yang%2C%20Fan%20Cohen%2C%20William%20W.%20Good%20semisupervised%20learning%20that%20requires%20a%20bad%202017"
        },
        {
            "id": "Edwards_2017_a",
            "entry": "Harrison Edwards and Amos Storkey. Towards a Neural Statistician. 5th International Conference on Learning Representations (ICLR 2017), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edwards%2C%20Harrison%20Storkey%2C%20Amos%20Towards%20a%20Neural%20Statistician%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edwards%2C%20Harrison%20Storkey%2C%20Amos%20Towards%20a%20Neural%20Statistician%202017"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126\u20131135, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/finn17a.html.",
            "url": "http://proceedings.mlr.press/v70/finn17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017-08"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2672\u20132680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.",
            "url": "http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Gulrajani_et+al_2017_a",
            "entry": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4565\u20134573. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf.",
            "url": "http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Sepp_2001_a",
            "entry": "Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent. In Proceedings of the International Conference on Artificial Neural Networks, ICANN \u201901, pages 87\u201394, London, UK, UK, 2001. Springer-Verlag. ISBN 3-540-42486-5. URL http://dl.acm.org/citation.cfm?id=646258.684281.",
            "url": "http://dl.acm.org/citation.cfm?id=646258.684281",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sepp%20Hochreiter%2C%20A.Steven%20Younger%20Conwell%2C%20Peter%20R.%20Learning%20to%20learn%20using%20gradient%20descent%202001"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Mishra_et+al_2018_a",
            "entry": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1DmUzWAW.",
            "url": "https://openreview.net/forum?id=B1DmUzWAW",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20Nikhil%20Rohaninejad%2C%20Mostafa%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20A%20simple%20neural%20attentive%20meta-learner%202018"
        },
        {
            "id": "Munkhdalai_2017_a",
            "entry": "Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2554\u20132563, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/munkhdalai17a.html.",
            "url": "http://proceedings.mlr.press/v70/munkhdalai17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munkhdalai%2C%20Tsendsuren%20Yu%2C%20Hong%20Meta%20networks%202017-08"
        },
        {
            "id": "Munkhdalai_2017_b",
            "entry": "Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, Tong Wang, and Adam Trischler. Learning rapid-temporal adaptations. CoRR, abs/1712.09926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09926"
        },
        {
            "id": "Ravi_2017_a",
            "entry": "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Ren_et+al_2018_a",
            "entry": "Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classification. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HJcSzz-CZ.",
            "url": "https://openreview.net/forum?id=HJcSzz-CZ",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Mengye%20Ravi%2C%20Sachin%20Triantafillou%2C%20Eleni%20Snell%2C%20Jake%20Meta-learning%20for%20semi-supervised%20few-shot%20classification%202018"
        },
        {
            "id": "Salimans_et+al_2016_a",
            "entry": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2234\u20132242. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf.",
            "url": "http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1842\u20131850, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/santoro16.html.",
            "url": "http://proceedings.mlr.press/v48/santoro16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016-06-20"
        },
        {
            "id": "Snell_et+al_2017_a",
            "entry": "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4077\u20134087. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf.",
            "url": "http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20Jake%20Swersky%2C%20Kevin%20Zemel%2C%20Richard%20Prototypical%20networks%20for%20few-shot%20learning%202017"
        },
        {
            "id": "Sung_et+al_2018_a",
            "entry": "Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sung%2C%20Flood%20Yang%2C%20Yongxin%20Zhang%2C%20Li%20Xiang%2C%20Tao%20Learning%20to%20compare%3A%20Relation%20network%20for%20few-shot%20learning%202018"
        },
        {
            "id": "Thrun_1998_a",
            "entry": "Sebastian Thrun. Learning to learn. chapter Lifelong Learning Algorithms, pages 181\u2013209. Kluwer Academic Publishers, Norwell, MA, USA, 1998. ISBN 0-7923-8047-9. URL http://dl.acm.org/citation.cfm?id=296635.296651.",
            "url": "http://dl.acm.org/citation.cfm?id=296635.296651"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3630\u20133638. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf.",
            "url": "http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20koray%20kavukcuoglu%20Matching%20networks%20for%20one%20shot%20learning%202016"
        }
    ]
}
