{
    "filename": "7800-a-simple-proximal-stochastic-gradient-method-for-nonsmooth-nonconvex-optimization.pdf",
    "metadata": {
        "title": "A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization",
        "author": "Zhize Li, Jian Li",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7800-a-simple-proximal-stochastic-gradient-method-for-nonsmooth-nonconvex-optimization.pdf"
        },
        "abstract": "We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth finite-sum problems. In particular, the objective function is given by the summation of a differentiable (possibly nonconvex) component, together with a possibly nondifferentiable but convex component. We propose a proximal stochastic gradient algorithm based on variance reduction, called ProxSVRG+. Our main contribution lies in the analysis of ProxSVRG+. It recovers several existing convergence results and improves/generalizes them (in terms of the number of stochastic gradient oracle calls and proximal oracle calls). In particular, ProxSVRG+ generalizes the best results given by the SCSG algorithm, recently proposed by [Lei et al., NIPS\u201917] for the smooth nonconvex case. ProxSVRG+ is also more straightforward than SCSG and yields simpler analysis. Moreover, ProxSVRG+ outperforms the deterministic proximal gradient descent (ProxGD) for a wide range of minibatch sizes, which partially solves an open problem proposed in [Reddi et al., NIPS\u201916]. Also, ProxSVRG+ uses much less proximal oracle calls than ProxSVRG [Reddi et al., NIPS\u201916]. Moreover, for nonconvex functions satisfied Polyak-\u0141ojasiewicz condition, we prove that ProxSVRG+ achieves a global linear convergence rate without restart unlike ProxSVRG. Thus, it can automatically switch to the faster linear convergence in some regions as long as the objective function satisfies the PL condition locally in these regions. Finally, we conduct several experiments and the experimental results are consistent with the theoretical results."
    },
    "keywords": [
        {
            "term": "quadratic growth",
            "url": "https://en.wikipedia.org/wiki/quadratic_growth"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        }
    ],
    "highlights": [
        "Reddi et al [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] provided the first non-asymptotic convergence rates for ProxSVRG with minibatch size at most O(n2/3), for the nonsmooth nonconvex problems",
        "See Figure 1 for an overview.\n2) Assuming that the variance of the stochastic gradient is bounded, i.e. online/stochastic setting, ProvSVRG+ generalizes the best result achieved by SCSG, recently proposed by Lei et al [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] for the smooth nonconvex case, i.e., h(x) \u2261 0 in form (1)",
        "We present the main theorem for our ProxSVRG+ which corresponds to the last two rows in Table 1 and give some remarks",
        "We propose a simple proximal stochastic method called ProxSVRG+ for nonsmooth nonconvex optimization",
        "We prove that ProxSVRG+ improves/generalizes several well-known convergence results by choosing proper minibatch sizes",
        "ProxSVRG+ generalizes the results of SCSG [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] to this nonsmooth nonconvex case, and it is more straightforward than SCSG and yields simpler analysis"
    ],
    "key_statements": [
        "Reddi et al [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] provided the first non-asymptotic convergence rates for ProxSVRG with minibatch size at most O(n2/3), for the nonsmooth nonconvex problems",
        "Our Contribution: In this paper, we propose a very straightforward algorithm called ProxSVRG+",
        "See Figure 1 for an overview.\n2) Assuming that the variance of the stochastic gradient is bounded, i.e. online/stochastic setting, ProvSVRG+ generalizes the best result achieved by SCSG, recently proposed by Lei et al [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] for the smooth nonconvex case, i.e., h(x) \u2261 0 in form (1)",
        "The best b for ProxSVRG and ProxSVRG+ in the MNIST experiments is 4096 and 256, respectively.\n3) For the nonconvex functions satisfying Polyak-\u0141ojasiewicz condition [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], we prove that ProxSVRG+ achieves a global linear convergence rate without restart, while Reddi et al [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] used PL-SVRG to restart ProxSVRG O) times to obtain the linear convergence rate",
        "We present the main theorem for our ProxSVRG+ which corresponds to the last two rows in Table 1 and give some remarks",
        "Similar to Table 2, ProxSVRG+ is better than proximal gradient descent and ProxSVRG/SAGA, and generalizes the SCSG by choosing different minibatch size b",
        "The step sizes \u03b7 for different algorithms are set to be the ones used in their convergence results: For proximal gradient descent, it is \u03b7 = 1/L; for ProxSGD, \u03b7 = 1/(2L); for ProxSVRG, \u03b7 = b3/2/(3Ln)",
        "We considered B = n/10, the performance among these algorithms are similar to the case B = n/5",
        "We propose a simple proximal stochastic method called ProxSVRG+ for nonsmooth nonconvex optimization",
        "We prove that ProxSVRG+ improves/generalizes several well-known convergence results by choosing proper minibatch sizes",
        "ProxSVRG+ generalizes the results of SCSG [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] to this nonsmooth nonconvex case, and it is more straightforward than SCSG and yields simpler analysis"
    ],
    "summary": [
        "Reddi et al [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] provided the first non-asymptotic convergence rates for ProxSVRG with minibatch size at most O(n2/3), for the nonsmooth nonconvex problems.",
        "Note that their algorithms (i.e., ProxSVRG/SAGA) outperform the ProxGD only if they use quite large minibatch size b = O(n2/3).",
        "2) Assuming that the variance of the stochastic gradient is bounded, i.e. online/stochastic setting, ProvSVRG+ generalizes the best result achieved by SCSG, recently proposed by Lei et al [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] for the smooth nonconvex case, i.e., h(x) \u2261 0 in form (1).",
        "In order to analyze the convergence results for nonsmooth nonconvex problems, we need to define the gradient mapping as follows: G\u03b7(x) := \u03b7 x \u2212 prox\u03b7h x \u2212 \u03b7\u2207f (x) .",
        "Due to the nonsmooth term h(x) in problem (1), we use the gradient mapping) to define a more general form of PL condition as follows:",
        "Similar to Table 2, ProxSVRG+ is better than ProxGD and ProxSVRG/SAGA, and generalizes the SCSG by choosing different minibatch size b.",
        "Similar to Theorem 1, we provide the convergence result of ProxSVRG+ (Algorithm 1) under PL-",
        "To the more general nonsmooth nonconvex case and is better than ProxGD and ProxSVRG/SAGA.",
        "The step sizes \u03b7 for different algorithms are set to be the ones used in their convergence results: For ProxGD, it is \u03b7 = 1/L; for ProxSGD, \u03b7 = 1/(2L); for ProxSVRG, \u03b7 = b3/2/(3Ln).",
        "Note that we amortize the batch size (n or B in Line 5 of Algorithm 1) into the inner loops, so that the curves in the figures are smoother, i.e., the number of SFO calls for each iteration of ProxSVRG and ProxSVRG+ is counted as b + n/m and b + B/m, respectively.",
        "We propose a simple proximal stochastic method called ProxSVRG+ for nonsmooth nonconvex optimization.",
        "We prove that ProxSVRG+ improves/generalizes several well-known convergence results (e.g., ProxGD, ProxSGD,\u221aProxSV\u221aRG/SAGA and SCSG) by choosing proper minibatch sizes.",
        "ProxSVRG+ generalizes the results of SCSG [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] to this nonsmooth nonconvex case, and it is more straightforward than SCSG and yields simpler analysis.",
        "For nonconvex functions satisfying Polyak\u0141ojasiewicz condition, we prove that ProxSVRG+ achieves the global linear convergence rate without restart.",
        "ProxSVRG+ can automatically switch to the faster linear convergence rate (i.e., O) from sublinear convergence rate (i.e., O(1/ )) in some regions as long as the objective function satisfies the PL condition locally in these regions.",
        "This is impossible for ProxSVRG [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] since it needs to be restarted O times"
    ],
    "headline": "We propose a proximal stochastic gradient algorithm based on variance reduction, called ProxSVRG+",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Zeyuan Allen-Zhu. Katyusha: the first direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200\u20131205. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20the%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20the%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arXiv:1708.08694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International Conference on Machine Learning, pages 699\u2013707, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] Mihai Anitescu. Degenerate nonlinear programming with a quadratic growth condition. SIAM Journal on Optimization, 10(4):1116\u20131135, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anitescu%2C%20Mihai%20Degenerate%20nonlinear%20programming%20with%20a%20quadratic%20growth%20condition%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anitescu%2C%20Mihai%20Degenerate%20nonlinear%20programming%20with%20a%20quadratic%20growth%20condition%202000"
        },
        {
            "id": "5",
            "entry": "[5] Aleksandr Aravkin and Damek Davis. A smart stochastic algorithm for nonconvex optimization with applications to robust machine learning. arXiv preprint arXiv:1610.01101, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.01101"
        },
        {
            "id": "6",
            "entry": "[6] Dominik Csiba and Peter Richt\u00e1rik. Global convergence of arbitrary-block gradient methods for generalized polyak-\u0142ojasiewicz functions. arXiv preprint arXiv:1709.03014, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03014"
        },
        {
            "id": "7",
            "entry": "[7] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "8",
            "entry": "[8] Haoyu Fu, Yuejie Chi, and Yingbin Liang. Local geometry of one-hidden-layer neural networks for logistic regression. arXiv preprint arXiv:1802.06463, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06463"
        },
        {
            "id": "9",
            "entry": "[9] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "10",
            "entry": "[10] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(12):267\u2013305, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Zhang%2C%20Hongchao%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Zhang%2C%20Hongchao%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016"
        },
        {
            "id": "11",
            "entry": "[11] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "12",
            "entry": "[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "13",
            "entry": "[13] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795\u2013811.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition"
        },
        {
            "id": "14",
            "entry": "[14] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "15",
            "entry": "[15] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint arXiv:1507.02000, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.02000"
        },
        {
            "id": "16",
            "entry": "[16] Guanghui Lan and Yi Zhou. Random gradient extrapolation for distributed and stochastic optimization. SIAM Journal on Optimization, 28(4):2753\u20132782, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Guanghui%20Zhou%2C%20Yi%20Random%20gradient%20extrapolation%20for%20distributed%20and%20stochastic%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Guanghui%20Zhou%2C%20Yi%20Random%20gradient%20extrapolation%20for%20distributed%20and%20stochastic%20optimization%202018"
        },
        {
            "id": "17",
            "entry": "[17] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017"
        },
        {
            "id": "18",
            "entry": "[18] Qunwei Li, Yi Zhou, Yingbin Liang, and Pramod K Varshney. Convergence analysis of proximal gradient with momentum for nonconvex optimization. In International Conference on Machine Learning, pages 2111\u20132119, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Qunwei%20Zhou%2C%20Yi%20Liang%2C%20Yingbin%20Varshney%2C%20Pramod%20K.%20Convergence%20analysis%20of%20proximal%20gradient%20with%20momentum%20for%20nonconvex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Qunwei%20Zhou%2C%20Yi%20Liang%2C%20Yingbin%20Varshney%2C%20Pramod%20K.%20Convergence%20analysis%20of%20proximal%20gradient%20with%20momentum%20for%20nonconvex%20optimization%202017"
        },
        {
            "id": "19",
            "entry": "[19] Zhi-Quan Luo and Paul Tseng. Error bounds and convergence analysis of feasible descent methods: a general approach. Annals of Operations Research, 46(1):157\u2013178, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Zhi-Quan%20Tseng%2C%20Paul%20Error%20bounds%20and%20convergence%20analysis%20of%20feasible%20descent%20methods%3A%20a%20general%20approach%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Zhi-Quan%20Tseng%2C%20Paul%20Error%20bounds%20and%20convergence%20analysis%20of%20feasible%20descent%20methods%3A%20a%20general%20approach%201993"
        },
        {
            "id": "20",
            "entry": "[20] Ion Necoara, Yurii Nesterov, and Francois Glineur. Linear convergence of first order methods for non-strongly convex optimization. arXiv preprint arXiv:1504.06298, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.06298"
        },
        {
            "id": "21",
            "entry": "[21] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20on%20Convex%20Optimization%3A%20A%20Basic%20Course%202004"
        },
        {
            "id": "22",
            "entry": "[22] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 3(4):643\u2013653, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20Teodorovich%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20Teodorovich%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963"
        },
        {
            "id": "23",
            "entry": "[23] Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pages 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "24",
            "entry": "[24] Sashank J Reddi, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. In Advances in Neural Information Processing Systems, pages 1145\u20131153, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Smola%2C%20Alexander%20J.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Smola%2C%20Alexander%20J.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016"
        },
        {
            "id": "25",
            "entry": "[25] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014"
        },
        {
            "id": "26",
            "entry": "[26] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        },
        {
            "id": "27",
            "entry": "[27] Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex optimization. arXiv preprint arXiv:1806.07811, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1806.07811"
        }
    ]
}
