{
    "filename": "7293-adapted-deep-embeddings-a-synthesis-of-methods-for-k-shot-inductive-transfer-learning.pdf",
    "metadata": {
        "title": "Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning",
        "author": "Tyler Scott, Karl Ridgeway, Michael C. Mozer",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7293-adapted-deep-embeddings-a-synthesis-of-methods-for-k-shot-inductive-transfer-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Three active lines of research have independently explored transfer learning using neural networks. In weight transfer, a model trained on the source domain is used as an initialization point for a network to be trained on the target domain. In deep metric learning, the source domain is used to construct an embedding that captures class structure in both the source and target domains. In few-shot learning, the focus is on generalizing well in the target domain based on a limited number of labeled examples. We compare state-of-the-art methods from these three paradigms and also explore hybrid adapted-embedding methods that use limited target-domain data to fine tune embeddings constructed from sourcedomain data. We conduct a systematic comparison of methods in a variety of domains, varying the number of labeled instances available in the target domain (k), as well as the number of target-domain classes. We reach three principal conclusions: (1) Deep embeddings are far superior, compared to weight transfer, as a starting point for inter-domain transfer or model re-use (2) Our hybrid methods robustly outperform every few-shot learning and every deep metric learning method previously proposed, with a mean error reduction of 34% over state-of-the-art. (3) Among loss functions for discovering embeddings, the histogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our results will motivate a unification of research in weight transfer, deep metric learning, and few-shot learning."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "transfer learning",
            "url": "https://en.wikipedia.org/wiki/transfer_learning"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "weight transfer",
            "url": "https://en.wikipedia.org/wiki/weight_transfer"
        },
        {
            "term": "metric learning",
            "url": "https://en.wikipedia.org/wiki/metric_learning"
        }
    ],
    "highlights": [
        "Since the introduction of backpropagation, researchers in neural networks have investigated inductive transfer learning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "With the deep learning movement, there has been a resurgence of interest in inductive transfer learning (ITL) for classification, which we will refer to as k-inductive transfer learning, where k denotes the number of labeled examples available for each class in the target domain",
        "Deep metric learning methods are evaluated using a variation of k-shot learning in which a support set of k examples of n classes is embedded, and a mean Recall@r score is obtained for a query set, held-out examples of this domain",
        "The results from our k-inductive transfer learning simulations are remarkably consistent across data sets and offer unambiguous prescriptions for significantly improving current practice in inductive transfer learning",
        "Adapted embeddings are the method of choice for k-inductive transfer learning",
        "We proposed adapted-embedding methods, ADAPTHISTLOSS and ADAPTPROTONET, that combine deep embedding losses for training on the source domain with weight adaptation on the target domain"
    ],
    "key_statements": [
        "Since the introduction of backpropagation, researchers in neural networks have investigated inductive transfer learning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We hope our results will motivate a unification of research in weight transfer, deep metric learning, and few-shot learning",
        "Inductive transfer learning refers to the use of labeled data from a source domain to improve generalization accuracy on a related target domain with limited labeled data [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "With the deep learning movement, there has been a resurgence of interest in inductive transfer learning (ITL) for classification, which we will refer to as k-inductive transfer learning, where k denotes the number of labeled examples available for each class in the target domain",
        "Deep metric learning methods are evaluated using a variation of k-shot learning in which a support set of k examples of n classes is embedded, and a mean Recall@r score is obtained for a query set, held-out examples of this domain",
        "Weight transfer in neural networks [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] is an instance of a more general framework in which parameters of a machine-learning model trained on a source domain are applied to a target domain",
        "MNIST was split into a source domain, with the digit classes 0\u20134, and a target domain, with 5\u20139",
        "The results from our k-inductive transfer learning simulations are remarkably consistent across data sets and offer unambiguous prescriptions for significantly improving current practice in inductive transfer learning",
        "Adapted embeddings are the method of choice for k-inductive transfer learning",
        "We proposed adapted-embedding methods, ADAPTHISTLOSS and ADAPTPROTONET, that combine deep embedding losses for training on the source domain with weight adaptation on the target domain"
    ],
    "summary": [
        "Since the introduction of backpropagation, researchers in neural networks have investigated inductive transfer learning [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>].",
        "Deep metric learning methods are evaluated using a variation of k-shot learning in which a support set of k examples of n classes is embedded, and a mean Recall@r score is obtained for a query set, held-out examples of this domain.",
        "Weight transfer in neural networks [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] is an instance of a more general framework in which parameters of a machine-learning model trained on a source domain are applied to a target domain.",
        "Yosinki et al found that the best classification accuracy on the target domain is obtained when all network weights up to the penultimate layer are transferred and adapted.",
        "The notion of weight adaptation is extremely popular in deep learning because it can provide a large time savings over training models from scratch, and it may prevent overfitting when the target domain is data constrained [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>].",
        "We tested six methods: WEIGHTADAPT, HISTLOSS, PROTONET, ADAPTHISTLOSS, ADAPTPROTONET, and a non-transfer BASELINE that ignores the source domain and trains a classifier solely on the limited labeled data in the target domain.",
        "Previous research on deep-metric and few-shot learning has addressed problems in which the number of available classes in the source domain, Nsrc, is much larger than the number of classes to be discriminated in the target domain, n.",
        "6In the supplementary materials, we show results from testing embeddings on the Omniglot data set using the methodology from previous few-shot learning studies.",
        "Given the small k available for target domain adaptation, a validation set would have had high variance and the transfer of weights from the source should impose a strong inductive bias.",
        "All six methods used the same underlying network architecture with two exceptions: (1) the BASELINE and WEIGHTADAPT architectures had an additional class-output layer which was not transferred from source to target; and (2) for training HISTLOSS and ADAPTHISTLOSS, the embeddings were L2 normalized, allowing for the use of the cosine distance function with a 200-bin histogram.",
        "We proposed adapted-embedding methods, ADAPTHISTLOSS and ADAPTPROTONET, that combine deep embedding losses for training on the source domain with weight adaptation on the target domain.",
        "WEIGHTADAPT does beat BASELINE for small k, but for our data sets, any advantage of WEIGHTADAPT seems to vanish for k \u2265 100, in contrast to the adapted embedding methods that still benefit from increasing k."
    ],
    "headline": "We propose a hybrid approach, adapted embeddings, that combines loss functions for deep embeddings with weight adaptation in the target domain",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Amiriparian, S., Gerczuk, M., Ottl, S., Cummins, N., Freitag, M., Pugachevskiy, S., Baird, A., and Schuller, B. W. (2017). Snore sound classification using image-based deep spectrum features. In Interspeech 2017, 18th Annual Conference of the International Speech Communication Association.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amiriparian%2C%20S.%20Gerczuk%2C%20M.%20Ottl%2C%20S.%20Cummins%2C%20N.%20Snore%20sound%20classification%20using%20image-based%20deep%20spectrum%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amiriparian%2C%20S.%20Gerczuk%2C%20M.%20Ottl%2C%20S.%20Cummins%2C%20N.%20Snore%20sound%20classification%20using%20image-based%20deep%20spectrum%20features%202017"
        },
        {
            "id": "2",
            "entry": "[2] Bellet, A., Habrard, A., and Sebban, M. (2013). A Survey on Metric Learning for Feature Vectors and Structured Data. arXiv e-prints, 1306.6709.",
            "arxiv_url": "https://arxiv.org/pdf/1306.6709"
        },
        {
            "id": "3",
            "entry": "[3] Caruana, R. (1997). Multitask Learning. In Machine Learning, volume 28, pages 41\u201375.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20R.%20Multitask%20Learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20R.%20Multitask%20Learning%201997"
        },
        {
            "id": "4",
            "entry": "[4] Chopra, S., Hadsell, R., and LeCun, Y. (2005). Learning a similarity metric discriminatively, with application to face verification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chopra%2C%20S.%20Hadsell%2C%20R.%20LeCun%2C%20Y.%20Learning%20a%20similarity%20metric%20discriminatively%2C%20with%20application%20to%20face%20verification%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chopra%2C%20S.%20Hadsell%2C%20R.%20LeCun%2C%20Y.%20Learning%20a%20similarity%20metric%20discriminatively%2C%20with%20application%20to%20face%20verification%202005"
        },
        {
            "id": "5",
            "entry": "[5] Cole, R. and Fanty, M. (1994). ISOLET Dataset.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cole%20R%20and%20Fanty%20M%201994%20ISOLET%20Dataset",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cole%20R%20and%20Fanty%20M%201994%20ISOLET%20Dataset"
        },
        {
            "id": "6",
            "entry": "[6] Daum\u00e9, H. (2007). Domain adaptation vs. transfer learning. https://nlpers.blogspot.com/2007/11/domain-adaptation-vs-transfer-learning.html.",
            "url": "https://nlpers.blogspot.com/2007/11/domain-adaptation-vs-transfer-learning.html"
        },
        {
            "id": "7",
            "entry": "[7] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical Image Database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "8",
            "entry": "[8] Edwards, H. and Storkey, A. (2017). Towards a Neural Statistician. In International Conference on Learning Representations (ICLR 2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edwards%2C%20H.%20Storkey%2C%20A.%20Towards%20a%20Neural%20Statistician%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edwards%2C%20H.%20Storkey%2C%20A.%20Towards%20a%20Neural%20Statistician%202017"
        },
        {
            "id": "9",
            "entry": "[9] Fei-Fei, L., Johnson, J., and Yeung, S. (2018). Tiny ImageNet Visual Recognition Challenge.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fei-Fei%2C%20L.%20Johnson%2C%20J.%20Yeung%2C%20S.%20Tiny%20ImageNet%20Visual%20Recognition%20Challenge%202018"
        },
        {
            "id": "10",
            "entry": "[10] Finn, C., Abbeel, P., and Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126\u20131135.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Abbeel%2C%20P.%20Levine%2C%20S.%20Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaptation%20of%20Deep%20Networks%202017"
        },
        {
            "id": "11",
            "entry": "[11] Ioffe, S. and Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448\u2013456.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20Normalization%3A%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20Normalization%3A%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%202015"
        },
        {
            "id": "12",
            "entry": "[12] Kaiser, L., Nachum, O., Roy, A., and Bengio, S. (2017). Learning to Remember Rare Events. In International Conference on Learning Representations (ICLR 2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiser%2C%20L.%20Nachum%2C%20O.%20Roy%2C%20A.%20Bengio%2C%20S.%20Learning%20to%20Remember%20Rare%20Events%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiser%2C%20L.%20Nachum%2C%20O.%20Roy%2C%20A.%20Bengio%2C%20S.%20Learning%20to%20Remember%20Rare%20Events%202017"
        },
        {
            "id": "13",
            "entry": "[13] Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv e-prints, 1412.6980.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "14",
            "entry": "[14] Koch, G., Zemel, R., and Salakhutdinov, R. (2015). Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop, volume 2.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koch%2C%20G.%20Zemel%2C%20R.%20Salakhutdinov%2C%20R.%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koch%2C%20G.%20Zemel%2C%20R.%20Salakhutdinov%2C%20R.%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015"
        },
        {
            "id": "15",
            "entry": "[15] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems 25, pages 1097\u20131105.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%202012"
        },
        {
            "id": "16",
            "entry": "[16] Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "17",
            "entry": "[17] LeCun, Y. and Cortes, C. (2010). MNIST handwritten digit database.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Cortes%2C%20C.%20MNIST%20handwritten%20digit%20database%202010"
        },
        {
            "id": "18",
            "entry": "[18] Li, W., Zhao, R., Xiao, T., and Wang, X. (2014). DeepReID: Deep Filter Pairing Neural Network for Person Re-identification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20W.%20Zhao%2C%20R.%20Xiao%2C%20T.%20Wang%2C%20X.%20DeepReID%3A%20Deep%20Filter%20Pairing%20Neural%20Network%20for%20Person%20Re-identification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20W.%20Zhao%2C%20R.%20Xiao%2C%20T.%20Wang%2C%20X.%20DeepReID%3A%20Deep%20Filter%20Pairing%20Neural%20Network%20for%20Person%20Re-identification%202014"
        },
        {
            "id": "19",
            "entry": "[19] Long, M., Cao, Y., Wang, J., and Jordan, M. (2015). Learning Transferable Features with Deep Adaptation Networks. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 97\u2013105.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20M.%20Cao%2C%20Y.%20Wang%2C%20J.%20Jordan%2C%20M.%20Learning%20Transferable%20Features%20with%20Deep%20Adaptation%20Networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20M.%20Cao%2C%20Y.%20Wang%2C%20J.%20Jordan%2C%20M.%20Learning%20Transferable%20Features%20with%20Deep%20Adaptation%20Networks%202015"
        },
        {
            "id": "20",
            "entry": "[20] Lu, J., Hu, J., and Zhou, J. (2017). Deep Metric Learning for Visual Understanding: An Overview of Recent Advances. IEEE Signal Processing Magazine, 34(6):76\u201384.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20J.%20Hu%2C%20J.%20Zhou%2C%20J.%20Deep%20Metric%20Learning%20for%20Visual%20Understanding%3A%20An%20Overview%20of%20Recent%20Advances%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20J.%20Hu%2C%20J.%20Zhou%2C%20J.%20Deep%20Metric%20Learning%20for%20Visual%20Understanding%3A%20An%20Overview%20of%20Recent%20Advances%202017"
        },
        {
            "id": "21",
            "entry": "[21] Oh Song, H., Xiang, Y., Jegelka, S., and Savarese, S. (2016). Deep metric learning via lifted structured feature embedding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%20Song%2C%20H.%20Xiang%2C%20Y.%20Jegelka%2C%20S.%20Savarese%2C%20S.%20Deep%20metric%20learning%20via%20lifted%20structured%20feature%20embedding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%20Song%2C%20H.%20Xiang%2C%20Y.%20Jegelka%2C%20S.%20Savarese%2C%20S.%20Deep%20metric%20learning%20via%20lifted%20structured%20feature%20embedding%202016"
        },
        {
            "id": "22",
            "entry": "[22] Oquab, M., Bottou, L., Laptev, I., and Sivic, J. (2014). Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1717\u20131724.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oquab%2C%20M.%20Bottou%2C%20L.%20Laptev%2C%20I.%20Sivic%2C%20J.%20Learning%20and%20Transferring%20Mid-level%20Image%20Representations%20Using%20Convolutional%20Neural%20Networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oquab%2C%20M.%20Bottou%2C%20L.%20Laptev%2C%20I.%20Sivic%2C%20J.%20Learning%20and%20Transferring%20Mid-level%20Image%20Representations%20Using%20Convolutional%20Neural%20Networks%202014"
        },
        {
            "id": "23",
            "entry": "[23] Pan, S. J. and Yang, Q. (2010). A Survey on Transfer Learning. volume 22, pages 1345\u20131359.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20S.J.%20Yang%2C%20Q.%20A%20Survey%20on%20Transfer%20Learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20S.J.%20Yang%2C%20Q.%20A%20Survey%20on%20Transfer%20Learning%202010"
        },
        {
            "id": "24",
            "entry": "[24] Pratt, L. Y., Mostow, J., and Kamm, C. A. (1991). Direct Transfer of Learned Information Among Neural Networks. In Proceedings of the American Association for Artificial Intelligence, volume 91, pages 584\u2013589.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pratt%2C%20L.Y.%20Mostow%2C%20J.%20Kamm%2C%20C.A.%20Direct%20Transfer%20of%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pratt%2C%20L.Y.%20Mostow%2C%20J.%20Kamm%2C%20C.A.%20Direct%20Transfer%20of%201991"
        },
        {
            "id": "25",
            "entry": "[25] Ravi, S. and Larochelle, H. (2017). Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR 2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "26",
            "entry": "[26] Ridgeway, K. and Mozer, M. C. (2018). Learning Deep Disentangled Embeddings with the F-Statistic Loss. arXiv e-prints, 1802.05312.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05312"
        },
        {
            "id": "27",
            "entry": "[27] Rozantsev, A., Salzmann, M., and Fua, P. (2018). Beyond Sharing Weights for Deep Domain Adaptation.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rozantsev%2C%20A.%20Salzmann%2C%20M.%20Fua%2C%20P.%20Beyond%20Sharing%20Weights%20for%20Deep%20Domain%20Adaptation%202018"
        },
        {
            "id": "28",
            "entry": "[28] Schroff, F., Kalenichenko, D., and Philbin, J. (2015). FaceNet: A Unified Embedding for Face Recognition and Clustering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schroff%2C%20F.%20Kalenichenko%2C%20D.%20Philbin%2C%20J.%20FaceNet%3A%20A%20Unified%20Embedding%20for%20Face%20Recognition%20and%20Clustering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schroff%2C%20F.%20Kalenichenko%2C%20D.%20Philbin%2C%20J.%20FaceNet%3A%20A%20Unified%20Embedding%20for%20Face%20Recognition%20and%20Clustering%202015"
        },
        {
            "id": "29",
            "entry": "[29] Snell, J., Swersky, K., and Zemel, R. (2017). Prototypical Networks for Few-shot Learning. In Advances in Neural Information Processing Systems 30, pages 4077\u20134087.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20J.%20Swersky%2C%20K.%20Zemel%2C%20R.%20Prototypical%20Networks%20for%20Few-shot%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20J.%20Swersky%2C%20K.%20Zemel%2C%20R.%20Prototypical%20Networks%20for%20Few-shot%20Learning%202017"
        },
        {
            "id": "30",
            "entry": "[30] TensorFlow (2018). Tensorflow Hub.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20TensorFlow%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20TensorFlow%202018"
        },
        {
            "id": "31",
            "entry": "[31] Triantafillou, E., Zemel, R., and Urtasun, R. (2017). Few-Shot Learning Through an Information Retrieval",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Triantafillou%2C%20E.%20Zemel%2C%20R.%20Urtasun%2C%20R.%20Few-Shot%20Learning%20Through%20an%20Information%20Retrieval%202017"
        },
        {
            "id": "32",
            "entry": "[32] Ustinova, E. and Lempitsky, V. (2016). Learning Deep Embeddings with Histogram Loss. In Advances in",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ustinova%2C%20E.%20Lempitsky%2C%20V.%20Learning%20Deep%20Embeddings%20with%20Histogram%20Loss%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ustinova%2C%20E.%20Lempitsky%2C%20V.%20Learning%20Deep%20Embeddings%20with%20Histogram%20Loss%202016"
        },
        {
            "id": "33",
            "entry": "[33] Vinyals, O., Blundell, C., Lillicrap, T., kavukcuoglu, k., and Wierstra, D. (2016). Matching Networks for",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Blundell%2C%20C.%20Lillicrap%2C%20T.%20kavukcuoglu%2C%20k%20Matching%20Networks%20for%202016"
        },
        {
            "id": "34",
            "entry": "[34] Wang, J., Zhou, F., Wen, S., Liu, X., and Lin, Y. (2017). Deep Metric Learning with Angular Loss. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 2612\u20132620.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20J.%20Zhou%2C%20F.%20Wen%2C%20S.%20Liu%2C%20X.%20Deep%20Metric%20Learning%20with%20Angular%20Loss%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20J.%20Zhou%2C%20F.%20Wen%2C%20S.%20Liu%2C%20X.%20Deep%20Metric%20Learning%20with%20Angular%20Loss%202017-10"
        },
        {
            "id": "35",
            "entry": "[35] Yi, D., Lei, Z., Liao, S., and Li, S. Z. (2014). Deep Metric Learning for Person Re-identification. In 2014 22nd International Conference on Pattern Recognition, pages 34\u201339.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20D.%20Lei%2C%20Z.%20Liao%2C%20S.%20Li%2C%20S.Z.%20Deep%20Metric%20Learning%20for%20Person%20Re-identification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yi%2C%20D.%20Lei%2C%20Z.%20Liao%2C%20S.%20Li%2C%20S.Z.%20Deep%20Metric%20Learning%20for%20Person%20Re-identification%202014"
        },
        {
            "id": "36",
            "entry": "[36] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems 27, pages 3320\u20133328.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20J.%20Clune%2C%20J.%20Bengio%2C%20Y.%20Lipson%2C%20H.%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20J.%20Clune%2C%20J.%20Bengio%2C%20Y.%20Lipson%2C%20H.%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "37",
            "entry": "[37] Zhang, Z., Ning, G., and He, Z. (2017). Knowledge Projection for Deep Neural Networks. arXiv e-prints, 1710.09505.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09505"
        },
        {
            "id": "38",
            "entry": "[38] Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., and Tian, Q. (2015). Scalable Person Re-identification: A Benchmark. In IEEE International Conference on Computer Vision. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20L.%20Shen%2C%20L.%20Tian%2C%20L.%20Wang%2C%20S.%20Scalable%20Person%20Re-identification%3A%20A%20Benchmark%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20L.%20Shen%2C%20L.%20Tian%2C%20L.%20Wang%2C%20S.%20Scalable%20Person%20Re-identification%3A%20A%20Benchmark%202015"
        }
    ]
}
