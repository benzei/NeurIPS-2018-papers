{
    "filename": "7408-frage-frequency-agnostic-word-representation.pdf",
    "metadata": {
        "title": "FRAGE: Frequency-Agnostic Word Representation",
        "author": "Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7408-frage-frequency-agnostic-word-representation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop FRequency-AGnostic word Embedding (FRAGE) which is a neat, simple yet effective way to learn word representation using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation, and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks."
    },
    "keywords": [
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "low frequency",
            "url": "https://en.wikipedia.org/wiki/low_frequency"
        },
        {
            "term": "network model",
            "url": "https://en.wikipedia.org/wiki/network_model"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "language processing",
            "url": "https://en.wikipedia.org/wiki/language_processing"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "FRAGE",
            "url": "https://en.wikipedia.org/wiki/Frage"
        },
        {
            "term": "text classification",
            "url": "https://en.wikipedia.org/wiki/text_classification"
        },
        {
            "term": "high frequency",
            "url": "https://en.wikipedia.org/wiki/high_frequency"
        },
        {
            "term": "language modeling",
            "url": "https://en.wikipedia.org/wiki/language_modeling"
        },
        {
            "term": "Penn Treebank",
            "url": "https://en.wikipedia.org/wiki/Penn_Treebank"
        }
    ],
    "highlights": [
        "Word Representation<br/><br/>Words are the basic units of natural languages, and distributed word representations are the basic units of many models in natural language processing tasks including language modeling [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "For a given natural language processing task, in addition to minimizing the task-specific loss by optimizing the task-specific parameters together with word embeddings, we introduce another discriminator, which takes a word embedding as input and classifies whether it is a popular/rare word",
        "Words are the basic units of natural languages, and distributed word representations are the basic units of many models in natural language processing tasks including language modeling [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "While word embeddings play an important role in neural network-based models in natural language processing and achieve great success, one technical challenge is that the embeddings of rare words are difficult to train due to their low frequency of occurrences. [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] develops a novel way to split each word into sub-word units which is widely used in neural machine translation",
        "We find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space",
        "Simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks"
    ],
    "key_statements": [
        "Word Representation<br/><br/>Words are the basic units of natural languages, and distributed word representations are the basic units of many models in natural language processing tasks including language modeling [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "We find a more general problem which is rooted in low-frequency words in the text corpus",
        "We argue that the different behaviors of the embeddings of popular words and rare words are problematic",
        "In this paper, we propose an adversarial training method to learn FRequencyAGnostic word Embedding (FRAGE)",
        "For a given natural language processing task, in addition to minimizing the task-specific loss by optimizing the task-specific parameters together with word embeddings, we introduce another discriminator, which takes a word embedding as input and classifies whether it is a popular/rare word",
        "Words are the basic units of natural languages, and distributed word representations are the basic units of many models in natural language processing tasks including language modeling [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "While word embeddings play an important role in neural network-based models in natural language processing and achieve great success, one technical challenge is that the embeddings of rare words are difficult to train due to their low frequency of occurrences. [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] develops a novel way to split each word into sub-word units which is widely used in neural machine translation",
        "For an natural language processing task and its neural network model, we introduce a discriminator to differentiate embeddings of popular words and rare words; while the NN model aims to fool the discriminator and minimize the task-specific loss simultaneously",
        "We study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English-German translation task using Transformer [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>]",
        "We can see that the embeddings encode frequencies to a certain degree: the rare words and popular words lie in different regions after this linear projection, 1https://code.google.com/archive/p/word2vec/ 2Cosine distance is the most popularly used metric in literature to measure semantic similarity [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "They may share some similar topics to popular entities, e.g., big companies and cities; around 10% percent of rare words include a hyphen, and over 30% rare words are different PoSs of popular words. These words should have mixed or similar semantics to some popular words. These facts show that rare words and popular words should lie in the same region of the embedding space, which is different from what we observed",
        "As we have a strong prior that many rare words should share the same region in the embedding space as popular words, the basic idea of our algorithm is to train the word embeddings in an adversarial framework: We introduce a discriminator to categorize word embeddings into two classes: popular ones or rare ones",
        "The frequency information is removed from the embedding and we call our method frequency-agnostic word embedding (FRAGE)",
        "We outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our method improves the representation of words, especially the rare words",
        "We find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space",
        "Simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks"
    ],
    "summary": [
        "Word Representation<br/><br/>Words are the basic units of natural languages, and distributed word representations are the basic units of many models in NLP tasks including language modeling [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "Similar phenomena are observed from the word embeddings learned from translation tasks.",
        "In this paper, we propose an adversarial training method to learn FRequencyAGnostic word Embedding (FRAGE).",
        "The low-frequency sub-word units are still difficult to train: [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] provides a comprehensive study which shows that the rarewords are usually under-estimated in neural machine translation: during inference step, the model tends to choose popular words over their rare alternatives.",
        "For an NLP task and its neural network model, we introduce a discriminator to differentiate embeddings of popular words and rare words; while the NN model aims to fool the discriminator and minimize the task-specific loss simultaneously.",
        "The difference between this work and adversarial domain adaptation is that we do not target at the mismatch between training and testing; instead, we aim to improve the effectiveness of word embeddings and improve the performance of end-to-end NLP tasks.",
        "We study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English-German translation task using Transformer [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>].",
        "We randomly sampled some rare/popular words and checked the embeddings trained from different tasks.",
        "We can see that the embeddings encode frequencies to a certain degree: the rare words and popular words lie in different regions after this linear projection, 1https://code.google.com/archive/p/word2vec/ 2Cosine distance is the most popularly used metric in literature to measure semantic similarity [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "It is not good to use such word embeddings into semantic understanding tasks, e.g., text classification, language modeling, language understanding, and translation.",
        "We hope the learned word embeddings not only minimize the task-specific training loss but fool the discriminator.",
        "We test our method on a wide range of tasks, including word similarity, language modeling, machine translation, and text classification.",
        "5https://github.com/tensorflow/models/blob/master/tutorials/embedding 6http://mattmahoney.net/dc/textdata.html 7https://github.com/salesforce/awd-lstm-lm 8https://github.com/zihangdai/mos 9https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl 10To improve the training for imbalanced labeled data, a common method is to adjust loss function by reweighting the training samples; To regularize the parameter space, a common method is to use l2 regularization.",
        "On PTB dataset, our method improves the AWD-LSTM and AWD-LSTM-MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints.",
        "This makes learned word embeddings ineffective, especially for rare words, and limits the performance of these neural network models.",
        "Simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks."
    ],
    "headline": "It is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. Al-Rfou, B. Perozzi, and S. Skiena. Polyglot: Distributed word representations for multilingual nlp. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bulgaria, August 2013. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Al-Rfou%2C%20R.%20Perozzi%2C%20B.%20Skiena%2C%20S.%20Polyglot%3A%20Distributed%20word%20representations%20for%20multilingual%20nlp%202013-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Rfou%2C%20R.%20Perozzi%2C%20B.%20Skiena%2C%20S.%20Polyglot%3A%20Distributed%20word%20representations%20for%20multilingual%20nlp%202013-08"
        },
        {
            "id": "2",
            "entry": "[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "3",
            "entry": "[3] S. Arora, Y. Liang, and T. Ma. A simple but tough-to-beat baseline for sentence embeddings. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20S.%20Liang%2C%20Y.%20Ma%2C%20T.%20A%20simple%20but%20tough-to-beat%20baseline%20for%20sentence%20embeddings%202016"
        },
        {
            "id": "4",
            "entry": "[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "5",
            "entry": "[5] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "6",
            "entry": "[6] A. Conneau, G. Lample, M. Ranzato, L. Denoyer, and H. J\u00e9gou. Word translation without parallel data. arXiv preprint arXiv:1710.04087, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.04087"
        },
        {
            "id": "7",
            "entry": "[7] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, pages 3079\u20133087, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20A.M.%20Le%2C%20Q.V.%20Semi-supervised%20sequence%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20A.M.%20Le%2C%20Q.V.%20Semi-supervised%20sequence%20learning%202015"
        },
        {
            "id": "8",
            "entry": "[8] S. Edunov, M. Ott, M. Auli, D. Grangier, and M. Ranzato. Classical structured prediction losses for sequence to sequence learning. arXiv preprint arXiv:1711.04956, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04956"
        },
        {
            "id": "9",
            "entry": "[9] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pages 1180\u20131189, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Y.%20Lempitsky%2C%20V.%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Y.%20Lempitsky%2C%20V.%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015"
        },
        {
            "id": "10",
            "entry": "[10] J. Gehring, M. Auli, D. Grangier, and Y. N. Dauphin. A convolutional encoder model for neural machine translation. arXiv preprint arXiv:1611.02344, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02344"
        },
        {
            "id": "11",
            "entry": "[11] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03122"
        },
        {
            "id": "12",
            "entry": "[12] C. Gong, D. He, X. Tan, T. Qin, L. Wang, and T. Liu. FRAGE: frequency-agnostic word representation. CoRR, abs/1809.06858, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.06858"
        },
        {
            "id": "13",
            "entry": "[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "14",
            "entry": "[14] E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. CoRR, abs/1612.04426, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04426"
        },
        {
            "id": "15",
            "entry": "[15] E. Hoffer, I. Hubara, and D. Soudry. Fix your classifier: the marginal value of training the last weight layer. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20E.%20Hubara%2C%20I.%20Soudry%2C%20D.%20Fix%20your%20classifier%3A%20the%20marginal%20value%20of%20training%20the%20last%20weight%20layer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20E.%20Hubara%2C%20I.%20Soudry%2C%20D.%20Fix%20your%20classifier%3A%20the%20marginal%20value%20of%20training%20the%20last%20weight%20layer%202018"
        },
        {
            "id": "16",
            "entry": "[16] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "17",
            "entry": "[17] N. Kalchbrenner, L. Espeholt, K. Simonyan, A. v. d. Oord, A. Graves, and K. Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.10099"
        },
        {
            "id": "18",
            "entry": "[18] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush. Character-aware neural language models. In AAAI, pages 2741\u20132749, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.%20Jernite%2C%20Y.%20Sontag%2C%20D.%20Rush%2C%20A.M.%20Character-aware%20neural%20language%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.%20Jernite%2C%20Y.%20Sontag%2C%20D.%20Rush%2C%20A.M.%20Character-aware%20neural%20language%20models%202016"
        },
        {
            "id": "19",
            "entry": "[19] B. Krause, E. Kahembwe, I. Murray, and S. Renals. Dynamic evaluation of neural sequence models. CoRR, abs/1709.07432, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.07432"
        },
        {
            "id": "20",
            "entry": "[20] S. Lai, L. Xu, K. Liu, and J. Zhao. Recurrent convolutional neural networks for text classification. In AAAI, volume 333, pages 2267\u20132273, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20S.%20Xu%2C%20L.%20Liu%2C%20K.%20Zhao%2C%20J.%20Recurrent%20convolutional%20neural%20networks%20for%20text%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20S.%20Xu%2C%20L.%20Liu%2C%20K.%20Zhao%2C%20J.%20Recurrent%20convolutional%20neural%20networks%20for%20text%20classification%202015"
        },
        {
            "id": "21",
            "entry": "[21] A. M. Lamb, A. G. A. P. GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances In Neural Information Processing Systems, pages 4601\u20134609, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lamb%2C%20A.M.%20GOYAL%2C%20A.G.A.P.%20Zhang%2C%20Y.%20Zhang%2C%20S.%20Professor%20forcing%3A%20A%20new%20algorithm%20for%20training%20recurrent%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lamb%2C%20A.M.%20GOYAL%2C%20A.G.A.P.%20Zhang%2C%20Y.%20Zhang%2C%20S.%20Professor%20forcing%3A%20A%20new%20algorithm%20for%20training%20recurrent%20networks%202016"
        },
        {
            "id": "22",
            "entry": "[22] G. Lample, L. Denoyer, and M. Ranzato. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00043"
        },
        {
            "id": "23",
            "entry": "[23] R. R. Larson. Introduction to information retrieval. Journal of the American Society for Information Science and Technology, 61(4):852\u2013853, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larson%2C%20R.R.%20Introduction%20to%20information%20retrieval%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larson%2C%20R.R.%20Introduction%20to%20information%20retrieval%202010"
        },
        {
            "id": "24",
            "entry": "[24] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142\u2013150. Association for Computational Linguistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20A.L.%20Daly%2C%20R.E.%20Pham%2C%20P.T.%20Huang%2C%20D.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20A.L.%20Daly%2C%20R.E.%20Pham%2C%20P.T.%20Huang%2C%20D.%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011"
        },
        {
            "id": "25",
            "entry": "[25] S. Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. CoRR, abs/1708.02182, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02182"
        },
        {
            "id": "26",
            "entry": "[26] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. CoRR, abs/1609.07843, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07843"
        },
        {
            "id": "27",
            "entry": "[27] T. Mikolov, M. Karafi\u00e1t, L. Burget, J. Cernock\u00fd, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045\u20131048, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernock%C3%BD%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%202010-09-26",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernock%C3%BD%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%202010-09-26"
        },
        {
            "id": "28",
            "entry": "[28] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Sutskever%2C%20I.%20Chen%2C%20K.%20Corrado%2C%20G.S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20T.%20Sutskever%2C%20I.%20Chen%2C%20K.%20Corrado%2C%20G.S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "29",
            "entry": "[29] J. Mu, S. Bhat, and P. Viswanath. All-but-the-top: simple and effective postprocessing for word representations. arXiv preprint arXiv:1702.01417, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.01417"
        },
        {
            "id": "30",
            "entry": "[30] J. Mu, S. Bhat, and P. Viswanath. All-but-the-top: Simple and effective postprocessing for word representations. CoRR, abs/1702.01417, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.01417"
        },
        {
            "id": "31",
            "entry": "[31] M. Ott, M. Auli, D. Granger, and M. Ranzato. Analyzing uncertainty in neural machine translation. arXiv preprint arXiv:1803.00047, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.00047"
        },
        {
            "id": "32",
            "entry": "[32] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10-25",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014-10-25"
        },
        {
            "id": "33",
            "entry": "[33] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "34",
            "entry": "[34] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "35",
            "entry": "[35] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.07909"
        },
        {
            "id": "36",
            "entry": "[36] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2962\u20132971, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tzeng%2C%20E.%20Hoffman%2C%20J.%20Saenko%2C%20K.%20Darrell%2C%20T.%20Adversarial%20discriminative%20domain%20adaptation%202017-07-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tzeng%2C%20E.%20Hoffman%2C%20J.%20Saenko%2C%20K.%20Darrell%2C%20T.%20Adversarial%20discriminative%20domain%20adaptation%202017-07-21"
        },
        {
            "id": "37",
            "entry": "[37] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07416"
        },
        {
            "id": "38",
            "entry": "[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "39",
            "entry": "[39] Y. Wang, Y. Xia, L. Zhao, J. Bian, T. Qin, G. Liu, and L. Tie-Yan. Dual transfer learning for neural machine translation with marginal distribution regularization.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Xia%2C%20Y.%20Zhao%2C%20L.%20Bian%2C%20J.%20Dual%20transfer%20learning%20for%20neural%20machine%20translation%20with%20marginal%20distribution%20regularization"
        },
        {
            "id": "40",
            "entry": "[40] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "41",
            "entry": "[41] Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. Breaking the softmax bottleneck: A high-rank RNN language model. CoRR, abs/1711.03953, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03953"
        },
        {
            "id": "42",
            "entry": "[42] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        }
    ]
}
