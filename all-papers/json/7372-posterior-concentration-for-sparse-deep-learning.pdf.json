{
    "filename": "7372-posterior-concentration-for-sparse-deep-learning.pdf",
    "metadata": {
        "title": "Posterior Concentration for Sparse Deep Learning",
        "author": "Veronika Rockova, nicholas polson",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7372-posterior-concentration-for-sparse-deep-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables provable recovery of smooth input-output maps with unknown levels of smoothness. Indeed, we show that the posterior distribution concentrates at the near minimax rate for \u03b1-H\u00f6lder smooth maps, performing as well as if we knew the smoothness level \u03b1 ahead of time. Our result sheds light on architecture design for deep neural networks, namely the choice of depth, width and sparsity level. These network attributes typically depend on unknown smoothness in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside, we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "linear combination",
            "url": "https://en.wikipedia.org/wiki/linear_combination"
        },
        {
            "term": "half space",
            "url": "https://en.wikipedia.org/wiki/half_space"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "variable selection",
            "url": "https://en.wikipedia.org/wiki/variable_selection"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "nonparametric regression",
            "url": "https://en.wikipedia.org/wiki/nonparametric_regression"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Deep learning constructs are powerful tools for pattern matching and prediction",
        "Their empirical success has been accompanied by a number of theoretical developments addressing (a) why and when neural networks generalize well, (b) when do deep networks out-perform shallow ones and (c) which activation functions and with how many layers",
        "We provide several new insights by studying the speed of posterior concentration around the optimal predictor, and in doing so we make a contribution to the Bayesian literature on deep learning rates",
        "Our contribution builds on this work in three fundamental aspects: (a) we focus on deep rather than single-layer, (b) we focus on rectified linear units (ReLU) rather than sigmoidal squashing functions, (c) deploying 0 regularization, we show that the posterior converges at an optimal speed beyond the mere fact that it is consistent",
        "Casting deep rectified linear units networks with 0 penalization as a Bayesian hierarchical model, we study the speed of posterior convergence around H\u00f6lder smooth regression functions",
        "We build on this result and show that the entire posterior distribution for deep sparse rectified linear units neural networks is concentrating at the near-minimax rate, when \u03b1 is known and when f0 is a H\u00f6lder smooth function"
    ],
    "key_statements": [
        "Deep learning constructs are powerful tools for pattern matching and prediction",
        "Their empirical success has been accompanied by a number of theoretical developments addressing (a) why and when neural networks generalize well, (b) when do deep networks out-perform shallow ones and (c) which activation functions and with how many layers",
        "We provide several new insights by studying the speed of posterior concentration around the optimal predictor, and in doing so we make a contribution to the Bayesian literature on deep learning rates",
        "Our contribution builds on this work in three fundamental aspects: (a) we focus on deep rather than single-layer, (b) we focus on rectified linear units (ReLU) rather than sigmoidal squashing functions, (c) deploying 0 regularization, we show that the posterior converges at an optimal speed beyond the mere fact that it is consistent",
        "Casting deep rectified linear units networks with 0 penalization as a Bayesian hierarchical model, we study the speed of posterior convergence around H\u00f6lder smooth regression functions",
        "This was the first result on rate-optimality of deep rectified linear units networks in non-parametric regression, obtained assuming that the sparsity s is known and that the function f0 is a composition of H\u00f6lder functions",
        "We build on this result and show that the entire posterior distribution for deep sparse rectified linear units neural networks is concentrating at the near-minimax rate, when \u03b1 is known and when f0 is a H\u00f6lder smooth function",
        "As we have shown in Theorem 6.1, Bayesian deep rectified linear units networks are near-minimax when 0 < \u03b1 < p, where p can be much larger than 1",
        "We conclude the paper with the following important corollary stating that Bayesian deep rectified linear units networks with adaptive spike-and-slab priors do not overfit in the sense that the posterior probability of using more than the optimal number of nodes and links goes to zero as n \u2192 \u221e",
        "In Theorem 6.1 we show that Bayesian deep rectified linear units networks can be near-minimax, if tuned properly",
        "In Theorem 6.2 we show that, by assigning suitable complexity priors over the network architecture, Bayesian deep rectified linear units networks can be near-minimax tuning-free",
        "In Corollary 6.1 we provide some arguments for why Bayesian deep rectified linear units networks are less eager to overfit"
    ],
    "summary": [
        "Deep learning constructs are powerful tools for pattern matching and prediction.",
        "We exploit spike-and-slab constructions not necessarily as a tool for model selection, but rather as a fully Bayesian alternative to dropout in order to (a) inject sparsity in deep learning to build stable network architectures, (b) achieve adaptation to the unknown aspects of the regression function in order to achieve near-minimax performance for estimating smooth regression surfaces.",
        "Casting deep ReLU networks with 0 penalization as a Bayesian hierarchical model, we study the speed of posterior convergence around H\u00f6lder smooth regression functions.",
        "Following the theoretical results of <a class=\"ref-link\" id=\"cMhaskar_et+al_2017_a\" href=\"#rMhaskar_et+al_2017_a\">Mhaskar et al (2017</a>), we build an 11-layer deep ReLU network is used to approximate this polynomial.",
        "We will leverage the fully Bayes framework and devise a hierarchical procedure which can learn the optimal level of sparsity needed to achieve near-minimax rates of posterior convergence of neural networks.",
        "Our first result provides guidance for calibrating Bayesian deep sparse ReLU networks when the level of smoothness \u03b1 is known.",
        "The result can be regarded as a Bayesian analogue of Theorem 1 of <a class=\"ref-link\" id=\"cSchmidt-Hieber_2017_a\" href=\"#rSchmidt-Hieber_2017_a\"><a class=\"ref-link\" id=\"cSchmidt-Hieber_2017_a\" href=\"#rSchmidt-Hieber_2017_a\">Schmidt-Hieber (2017</a></a>), who showed near-minimax rate-optimality of a sparse multilayer ReLU network estimator that minimizes empirical least-squares.",
        "This was the first result on rate-optimality of deep ReLU networks in non-parametric regression, obtained assuming that the sparsity s is known and that the function f0 is a composition of H\u00f6lder functions.",
        "We build on this result and show that the entire posterior distribution for deep sparse ReLU neural networks is concentrating at the near-minimax rate, when \u03b1 is known and when f0 is a H\u00f6lder smooth function.",
        "We conclude the paper with the following important corollary stating that Bayesian deep ReLU networks with adaptive spike-and-slab priors do not overfit in the sense that the posterior probability of using more than the optimal number of nodes and links goes to zero as n \u2192 \u221e",
        "The goal of this paper was to study posterior concentration for Bayesian deep learning and to provide new theoretical justifications for neural networks from a Bayesian point of view.",
        "In Theorem 6.2 we show that, by assigning suitable complexity priors over the network architecture, Bayesian deep ReLU networks can be near-minimax tuning-free.",
        "Posterior concentration rate results of this type are slowly entering the machine learning community as a tool for (a) obtaining more insights into Bayesian methods, Rockova and van der Pas (2017)) and (b) prior calibrations.",
        "<a class=\"ref-link\" id=\"cSchmidt-Hieber_2017_a\" href=\"#rSchmidt-Hieber_2017_a\"><a class=\"ref-link\" id=\"cSchmidt-Hieber_2017_a\" href=\"#rSchmidt-Hieber_2017_a\">Schmidt-Hieber (2017</a></a>) showed that sparsely connected deep ReLU networks achieve a near-minimax rate in learning for compositions of smooth functions."
    ],
    "headline": "We introduce Spike-and-Slab Deep Learning , a fully Bayesian alternative to dropout for improving generalizability of deep rectified linear units networks",
    "reference_links": [
        {
            "id": "Bauer_2017_a",
            "entry": "Bauer, B. and Kohler, M. (2017). On Deep Learning as a remedy for the curse of dimensionality in nonparametric regression. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauer%2C%20B.%20Kohler%2C%20M.%20On%20Deep%20Learning%20as%20a%20remedy%20for%20the%20curse%20of%20dimensionality%20in%20nonparametric%20regression.%20arXiv%202017"
        },
        {
            "id": "Blundell_et+al_2015_a",
            "entry": "Blundell, C., Cornebise, J., Kavukcuoglu, K. and Wierstra, D. (2015). On Deep Learning as a remedy for the curse of dimensionality in nonparametric regression. International Conference on Machine Learning, 37, 1613-1622.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20C.%20Cornebise%2C%20J.%20Kavukcuoglu%2C%20K.%20Wierstra%2C%20D.%20On%20Deep%20Learning%20as%20a%20remedy%20for%20the%20curse%20of%20dimensionality%20in%20nonparametric%20regression%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20C.%20Cornebise%2C%20J.%20Kavukcuoglu%2C%20K.%20Wierstra%2C%20D.%20On%20Deep%20Learning%20as%20a%20remedy%20for%20the%20curse%20of%20dimensionality%20in%20nonparametric%20regression%202015"
        },
        {
            "id": "Castillo_2012_a",
            "entry": "Castillo, I. and van der Vaart (2012). Needles and straw in a haystack: Posterior concentration for possibly sparse sequences. Annals of Statistics, 40, 2069-2101.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Castillo%2C%20I.%20van%20der%20Vaart%20Needles%20and%20straw%20in%20a%20haystack%3A%20Posterior%20concentration%20for%20possibly%20sparse%20sequences%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Castillo%2C%20I.%20van%20der%20Vaart%20Needles%20and%20straw%20in%20a%20haystack%3A%20Posterior%20concentration%20for%20possibly%20sparse%20sequences%202012"
        },
        {
            "id": "Cheang_2010_a",
            "entry": "Cheang, G. H. (2010). Approximation with neural networks activated by ramp sigmoids. Journal of Approximation Theory, 162, 1450-1465.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheang%2C%20G.H.%20Approximation%20with%20neural%20networks%20activated%20by%20ramp%20sigmoids%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheang%2C%20G.H.%20Approximation%20with%20neural%20networks%20activated%20by%20ramp%20sigmoids%202010"
        },
        {
            "id": "Cheang_2000_a",
            "entry": "Cheang, G. H., and Barron, A. R. (2000). A better approximation for balls. Journal of Approximation Theory, 104, 183-203.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheang%2C%20G.H.%20Barron%2C%20A.R.%20A%20better%20approximation%20for%20balls%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheang%2C%20G.H.%20Barron%2C%20A.R.%20A%20better%20approximation%20for%20balls%202000"
        },
        {
            "id": "Coram_2010_a",
            "entry": "Coram, M. and Lalley, S. (2010). Consistency of Bayes estimators of a binary regression function. Annals of Statistics, 34, 1233-1269.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coram%2C%20M.%20Lalley%2C%20S.%20Consistency%20of%20Bayes%20estimators%20of%20a%20binary%20regression%20function%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coram%2C%20M.%20Lalley%2C%20S.%20Consistency%20of%20Bayes%20estimators%20of%20a%20binary%20regression%20function%202010"
        },
        {
            "id": "Dinh_et+al_2017_a",
            "entry": "Dinh, R., Pascanu, R., Bengio, S. and Bengio, Y. (2017). Sharp Minima Can Generalize For Deep Nets. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20R.%20Pascanu%2C%20R.%20Bengio%2C%20S.%20Bengio%2C%20Y.%20Sharp%20Minima%20Can%20Generalize%20For%20Deep%20Nets.%20arXiv%202017"
        },
        {
            "id": "George_1993_a",
            "entry": "George, E.I. and McCulloch, R. (1993). Variable selection via Gibbs sampling. Journal of the American Statistical Association, 88, 881-889.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=George%2C%20E.I.%20McCulloch%2C%20R.%20Variable%20selection%20via%20Gibbs%20sampling%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=George%2C%20E.I.%20McCulloch%2C%20R.%20Variable%20selection%20via%20Gibbs%20sampling%201993"
        },
        {
            "id": "Ghosal_et+al_2000_a",
            "entry": "Ghosal, S., Ghosh, J. and van der Vaart, A. (2000). Convergence rates of posterior distributions. Annals of Statistics, 28, 500-531.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosal%2C%20S.%20Ghosh%2C%20J.%20van%20der%20Vaart%2C%20A.%20Convergence%20rates%20of%20posterior%20distributions%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosal%2C%20S.%20Ghosh%2C%20J.%20van%20der%20Vaart%2C%20A.%20Convergence%20rates%20of%20posterior%20distributions%202000"
        },
        {
            "id": "Ghosal_2007_a",
            "entry": "Ghosal, S. and van der Vaart, A. (2007). Convergence rates of posterior distributions for noniid observations. Annals of Statistics, 35, 192-223.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosal%2C%20S.%20van%20der%20Vaart%2C%20A.%20Convergence%20rates%20of%20posterior%20distributions%20for%20noniid%20observations%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosal%2C%20S.%20van%20der%20Vaart%2C%20A.%20Convergence%20rates%20of%20posterior%20distributions%20for%20noniid%20observations%202007"
        },
        {
            "id": "Ghosh_2017_a",
            "entry": "Ghosh, S. and Doshi-Velez, F. (2017). Model selection in Bayesian neural networks via horseshoe priors. Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20S.%20Doshi-Velez%2C%20F.%20Model%20selection%20in%20Bayesian%20neural%20networks%20via%20horseshoe%20priors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20S.%20Doshi-Velez%2C%20F.%20Model%20selection%20in%20Bayesian%20neural%20networks%20via%20horseshoe%20priors%202017"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "Glorot, X., Border, A. and Bengio, Y. (2011). Deep sparse rectifier neural networks. Proceedings of the 14th International Conference on Artificial Intelligence and Statistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Border%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Border%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "Goodfellow, I., Bengio, Y. and Courville, A. (2016). Deep Learning. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "Ismailov_2017_a",
            "entry": "Ismailov, V. (2017). Approximation by sums of ridge functions with fixed directions. St. Petersburg Mathematical Journal, 28, 741-772.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ismailov%2C%20V.%20Approximation%20by%20sums%20of%20ridge%20functions%20with%20fixed%20directions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ismailov%2C%20V.%20Approximation%20by%20sums%20of%20ridge%20functions%20with%20fixed%20directions%202017"
        },
        {
            "id": "Kainen_et+al_2003_a",
            "entry": "Kainen, P. C., Kurkov\u00e1, V., and Vogt, A. (2003). Best approximation by linear combinations of characteristic functions of half-spaces. Journal of Approximation Theory, 122, 151-159.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kainen%2C%20P.C.%20Kurkov%C3%A1%2C%20V.%20Vogt%2C%20A.%20Best%20approximation%20by%20linear%20combinations%20of%20characteristic%20functions%20of%20half-spaces%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kainen%2C%20P.C.%20Kurkov%C3%A1%2C%20V.%20Vogt%2C%20A.%20Best%20approximation%20by%20linear%20combinations%20of%20characteristic%20functions%20of%20half-spaces%202003"
        },
        {
            "id": "Kainen_et+al_2007_a",
            "entry": "Kainen, P. C., Krkov\u00e1, V., and Vogt, A. (2007). A Sobolev-type upper bound for rates of approximation by linear combinations of Heaviside plane waves. Journal of Approximation Theory, 147, 1-10.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kainen%2C%20P.C.%20Krkov%C3%A1%2C%20V.%20Vogt%2C%20A.%20A%20Sobolev-type%20upper%20bound%20for%20rates%20of%20approximation%20by%20linear%20combinations%20of%20Heaviside%20plane%20waves%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kainen%2C%20P.C.%20Krkov%C3%A1%2C%20V.%20Vogt%2C%20A.%20A%20Sobolev-type%20upper%20bound%20for%20rates%20of%20approximation%20by%20linear%20combinations%20of%20Heaviside%20plane%20waves%202007"
        },
        {
            "id": "Kawaguchi_et+al_2017_a",
            "entry": "Kawaguchi, K., Kaelbling, L. P. and Bengio, Y. (2017). Generalization in deep learning. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20K.%20Kaelbling%2C%20L.P.%20Bengio%2C%20Y.%20Generalization%20in%20deep%20learning.%20arXiv%202017"
        },
        {
            "id": "Klusowki_2016_a",
            "entry": "Klusowki, J.M. and Barron, A.R. (2016). Risk bounds for high-dimensional ridge function combinations including neural networks. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klusowki%2C%20J.M.%20Barron%2C%20A.R.%20Risk%20bounds%20for%20high-dimensional%20ridge%20function%20combinations%20including%20neural%20networks.%20arXiv%202016"
        },
        {
            "id": "Klusowki_2017_a",
            "entry": "Klusowki, J.M. and Barron, A.R. (2017). Minimax lower bounds for ridge combinations including neural networks. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klusowki%2C%20J.M.%20Barron%2C%20A.R.%20Minimax%20lower%20bounds%20for%20ridge%20combinations%20including%20neural%20networks.%20arXiv%202017"
        },
        {
            "id": "Kolmogorov_1963_a",
            "entry": "Kolmogorov, A. (1963). On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition. American Mathematical Society Translation, 28, 55?59.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolmogorov%2C%20A.%20On%20the%20representation%20of%20continuous%20functions%20of%20many%20variables%20by%20superposition%20of%20continuous%20functions%20of%20one%20variable%20and%20addition%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolmogorov%2C%20A.%20On%20the%20representation%20of%20continuous%20functions%20of%20many%20variables%20by%20superposition%20of%20continuous%20functions%20of%20one%20variable%20and%20addition%201963"
        },
        {
            "id": "Krkov_et+al_1997_a",
            "entry": "Krkov\u00e1, V., Kainen, P. C., and Kreinovich, V. (1997). Estimates of the number of hidden units and variation with respect to half-spaces. Neural Networks, 10, 1061-1068.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krkov%C3%A1%2C%20V.%20Kainen%2C%20P.C.%20Kreinovich%2C%20V.%20Estimates%20of%20the%20number%20of%20hidden%20units%20and%20variation%20with%20respect%20to%20half-spaces%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krkov%C3%A1%2C%20V.%20Kainen%2C%20P.C.%20Kreinovich%2C%20V.%20Estimates%20of%20the%20number%20of%20hidden%20units%20and%20variation%20with%20respect%20to%20half-spaces%201997"
        },
        {
            "id": "Le_1973_a",
            "entry": "Le Cam, L. (1973). Convergence of estimates under dimensionality restrictions. Annals of Statistics, 1, 38-53.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%20Cam%2C%20L.%20Convergence%20of%20estimates%20under%20dimensionality%20restrictions%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%20Cam%2C%20L.%20Convergence%20of%20estimates%20under%20dimensionality%20restrictions%201973"
        },
        {
            "id": "Lee_2000_a",
            "entry": "Lee, H. (2000). Consistency of posterior distributions for neural networks. Neural Networks, 13, 629-642.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20H.%20Consistency%20of%20posterior%20distributions%20for%20neural%20networks%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20H.%20Consistency%20of%20posterior%20distributions%20for%20neural%20networks%202000"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Liu, L., Li, D. and Wong, W.H. (2017). Convergence rates of a partition based Bayesian multivariate density estimation methods. Advances in Neural Information Processing Systems, 30.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20L.%20Li%2C%20D.%20Wong%2C%20W.H.%20Convergence%20rates%20of%20a%20partition%20based%20Bayesian%20multivariate%20density%20estimation%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20L.%20Li%2C%20D.%20Wong%2C%20W.H.%20Convergence%20rates%20of%20a%20partition%20based%20Bayesian%20multivariate%20density%20estimation%20methods%202017"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "Liu, Y., Rockova, V. and Wang, Y. (2018). ABC Bayesian Forests for Variable Selection. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Y.%20Rockova%2C%20V.%20Wang%2C%20Y.%20ABC%20Bayesian%20Forests%20for%20Variable%20Selection.%20arXiv%202018"
        },
        {
            "id": "Mhaskar_1996_a",
            "entry": "Mhaskar, H. N. (1996). Neural networks for optimal approximation of smooth and analytic functions. Neural Computation, 8(1), 164-177.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.N.%20Neural%20networks%20for%20optimal%20approximation%20of%20smooth%20and%20analytic%20functions%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.N.%20Neural%20networks%20for%20optimal%20approximation%20of%20smooth%20and%20analytic%20functions%201996"
        },
        {
            "id": "Mhaskar_et+al_2017_a",
            "entry": "Mhaskar, H., Liao, Q., and Poggio, T. A. (2017). When and why are deep networks better than shallow ones? In AAAI, 2343-2349.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20H.%20Liao%2C%20Q.%20Poggio%2C%20T.A.%20When%20and%20why%20are%20deep%20networks%20better%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20H.%20Liao%2C%20Q.%20Poggio%2C%20T.A.%20When%20and%20why%20are%20deep%20networks%20better%202017"
        },
        {
            "id": "Montufar_et+al_2014_a",
            "entry": "Montufar, G.F., R. Pascanu, K. Cho and Y. Bengio (2014). On the number of linear regions of deep neural networks. Advances in Neural Information Processing Systems, 27, 2924-2932.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20G.F.%20Pascanu%2C%20R.%20Cho%2C%20K.%20Y.%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20G.F.%20Pascanu%2C%20R.%20Cho%2C%20K.%20Y.%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014"
        },
        {
            "id": "Newton_et+al_2018_a",
            "entry": "Newton, M., Polson, N.G. and Xu, J. (2018). Weighted Bayesian bootstrap for scalable Bayes. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newton%2C%20M.%20Polson%2C%20N.G.%20Xu%2C%20J.%20Weighted%20Bayesian%20bootstrap%20for%20scalable%20Bayes.%20arXiv%202018"
        },
        {
            "id": "Van_2017_a",
            "entry": "van der Pas, S. and Rockova, V. (2017). Bayesian dyadic trees and histograms for regression. Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Pas%2C%20S.%20Rockova%2C%20V.%20Bayesian%20dyadic%20trees%20and%20histograms%20for%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Pas%2C%20S.%20Rockova%2C%20V.%20Bayesian%20dyadic%20trees%20and%20histograms%20for%20regression%202017"
        },
        {
            "id": "Petersen_2017_a",
            "entry": "Petersen, P. and F. Voigtlaender (2017). Optimal approximation of piecewise smooth functions using deep ReLU neural networks. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petersen%2C%20P.%20F.%20Optimal%20approximation%20of%20piecewise%20smooth%20functions%20using%20deep%20ReLU%20neural%20networks.%20arXiv%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petersen%2C%20P.%20F.%20Optimal%20approximation%20of%20piecewise%20smooth%20functions%20using%20deep%20ReLU%20neural%20networks.%20arXiv%202017"
        },
        {
            "id": "Petrushev_1999_a",
            "entry": "Petrushev, P. P. (1999). Approximation by ridge functions and neural networks. SIAM J. Math Anal., 30, 155-189.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petrushev%2C%20P.P.%20Approximation%20by%20ridge%20functions%20and%20neural%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petrushev%2C%20P.P.%20Approximation%20by%20ridge%20functions%20and%20neural%20networks%201999"
        },
        {
            "id": "Pinkus_1999_a",
            "entry": "Pinkus, A. (1999). Approximation theory of the MLP model is neural networks. Acta Numerica, 143-195.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinkus%2C%20A.%20Approximation%20theory%20of%20the%20MLP%20model%20is%20neural%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinkus%2C%20A.%20Approximation%20theory%20of%20the%20MLP%20model%20is%20neural%20networks%201999"
        },
        {
            "id": "Poggio_et+al_2017_a",
            "entry": "Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q. (2017). Why and when can deepbut not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14, 503-519.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poggio%2C%20T.%20Mhaskar%2C%20H.%20Rosasco%2C%20L.%20Miranda%2C%20B.%20Why%20and%20when%20can%20deepbut%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poggio%2C%20T.%20Mhaskar%2C%20H.%20Rosasco%2C%20L.%20Miranda%2C%20B.%20Why%20and%20when%20can%20deepbut%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017"
        },
        {
            "id": "Polson_2017_a",
            "entry": "Polson, N. and Sokolov, V. (2017). Deep learning: a Bayesian perspective. Bayesian Analysis, 12, 1275-1304.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polson%2C%20N.%20Sokolov%2C%20V.%20Deep%20learning%3A%20a%20Bayesian%20perspective%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polson%2C%20N.%20Sokolov%2C%20V.%20Deep%20learning%3A%20a%20Bayesian%20perspective%202017"
        },
        {
            "id": "Rockova_2014_a",
            "entry": "Rockova, V. and George, E.I. (2014). EMVS: The EM Approach to Bayesian Variable Selection. Journal of the American Statistical Association, 109, 828-846.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockova%2C%20V.%20George%2C%20E.I.%20EMVS%3A%20The%20EM%20Approach%20to%20Bayesian%20Variable%20Selection%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rockova%2C%20V.%20George%2C%20E.I.%20EMVS%3A%20The%20EM%20Approach%20to%20Bayesian%20Variable%20Selection%202014"
        },
        {
            "id": "Rockova_2018_a",
            "entry": "Rockova, V. and George, E.I. (2018). The Spike-and-Slab LASSO. Journal of the American Statistical Association, 113, 431-444.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockova%2C%20V.%20George%2C%20E.I.%20The%20Spike-and-Slab%20LASSO%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rockova%2C%20V.%20George%2C%20E.I.%20The%20Spike-and-Slab%20LASSO%202018"
        },
        {
            "id": "Rockova_2017_a",
            "entry": "Rockova, V. and van der Pas, S. (2017). Posterior Concentration for Bayesian Regression Trees and their Ensembles. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockova%2C%20V.%20van%20der%20Pas%2C%20S.%20Posterior%20Concentration%20for%20Bayesian%20Regression%20Trees%20and%20their%20Ensembles.%20arXiv%202017"
        },
        {
            "id": "Schmidt-Hieber_2017_a",
            "entry": "Schmidt-Hieber, J. (2017). Nonparametric regression using deep neural networks with ReLU activation function. arXiv:1708.06633.",
            "arxiv_url": "https://arxiv.org/pdf/1708.06633"
        },
        {
            "id": "Shen_2001_a",
            "entry": "Shen, X. and Wasserman, L. (2001). Rates of convergence of posterior distributions. Annals of Statistics, 29, 687-714.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20X.%20Wasserman%2C%20L.%20Rates%20of%20convergence%20of%20posterior%20distributions%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20X.%20Wasserman%2C%20L.%20Rates%20of%20convergence%20of%20posterior%20distributions%202001"
        },
        {
            "id": "Srivastava_et+al_2015_a",
            "entry": "Srivastava, N., Hinton, G., Krizhevsky, A.,Sutskever, I. and Salakhutdinov, R. (2015). Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15, 1929-1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202015"
        },
        {
            "id": "Telgarsky_2016_a",
            "entry": "Telgarsky, M. (2016). Benefits of depth in neural networks. JMLR: Workshop and Conference Proceedings, 49,1-23.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Telgarsky%2C%20M.%20Benefits%20of%20depth%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Telgarsky%2C%20M.%20Benefits%20of%20depth%20in%20neural%20networks%202016"
        },
        {
            "id": "Telgarsky_2017_a",
            "entry": "Telgarsky, M. (2017). Neural Networks and Rational functions. arXiv. Ullrich, K., Meeds, E. and Welling, M. (2017). Soft weight-sharing for neural network compression. International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Telgarsky%2C%20M.%20Neural%20Networks%20and%20Rational%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Telgarsky%2C%20M.%20Neural%20Networks%20and%20Rational%202017"
        },
        {
            "id": "Vitushkin_1964_a",
            "entry": "Vitushkin, A. G. (1964). Proof of the existence of analytic functions of several complex variables which are not representable by linear superpositions of continuously differentiable functions of fewer variables. Soviet Mathematics, 5, 793-796.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vitushkin%2C%20A.G.%20Proof%20of%20the%20existence%20of%20analytic%20functions%20of%20several%20complex%20variables%20which%20are%20not%20representable%20by%20linear%20superpositions%20of%20continuously%20differentiable%20functions%20of%20fewer%20variables%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vitushkin%2C%20A.G.%20Proof%20of%20the%20existence%20of%20analytic%20functions%20of%20several%20complex%20variables%20which%20are%20not%20representable%20by%20linear%20superpositions%20of%20continuously%20differentiable%20functions%20of%20fewer%20variables%201964"
        },
        {
            "id": "Walker_et+al_2007_a",
            "entry": "Walker, S., Lijoi, A. and Prunster, I. (2007). On rates of Convergence of Posterior Distributions in Infinite Dimensional Models. Annals of Statistics, 35, 738-746.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20S.%20Lijoi%2C%20A.%20Prunster%2C%20I.%20On%20rates%20of%20Convergence%20of%20Posterior%20Distributions%20in%20Infinite%20Dimensional%20Models%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20S.%20Lijoi%2C%20A.%20Prunster%2C%20I.%20On%20rates%20of%20Convergence%20of%20Posterior%20Distributions%20in%20Infinite%20Dimensional%20Models%202007"
        },
        {
            "id": "Wager_et+al_2014_a",
            "entry": "Wager, S., Wang, S. and Liang, P. (2014). Dropout training as adaptive regularization. Advances in Neural Information Processing Systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wager%2C%20S.%20Wang%2C%20S.%20Liang%2C%20P.%20Dropout%20training%20as%20adaptive%20regularization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wager%2C%20S.%20Wang%2C%20S.%20Liang%2C%20P.%20Dropout%20training%20as%20adaptive%20regularization%202014"
        },
        {
            "id": "Wong_1995_a",
            "entry": "Wong, W. H. and X. Shen (1995). Probability inequalities for Likelihood ratios and convergence rates of sieve mles. Annals of Statistics, 23, 339-362.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wong%2C%20W.H.%20X.%20Probability%20inequalities%20for%20Likelihood%20ratios%20and%20convergence%20rates%20of%20sieve%20mles%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wong%2C%20W.H.%20X.%20Probability%20inequalities%20for%20Likelihood%20ratios%20and%20convergence%20rates%20of%20sieve%20mles%201995"
        },
        {
            "id": "Yarotsky_2017_a",
            "entry": "Yarotsky, D. (2017). Error bounds for approximations with deep ReLU networks. Neural Networks, 94, 103-114.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20ReLU%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20D.%20Error%20bounds%20for%20approximations%20with%20deep%20ReLU%20networks%202017"
        }
    ]
}
