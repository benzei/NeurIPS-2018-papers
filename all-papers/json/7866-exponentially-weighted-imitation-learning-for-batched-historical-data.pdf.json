{
    "filename": "7866-exponentially-weighted-imitation-learning-for-batched-historical-data.pdf",
    "metadata": {
        "title": "Exponentially Weighted Imitation Learning for Batched Historical Data",
        "author": "Qing Wang, Jiechao Xiong, Lei Han, peng sun, Han Liu, Tong Zhang",
        "date": 2004,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider deep policy learning with only batched historical trajectories. The main challenge of this problem is that the learner no longer has a simulator or \u201cenvironment oracle\u201d as in most reinforcement learning settings. To solve this problem, we propose a monotonic advantage reweighted imitation learning strategy that is applicable to problems with complex nonlinear function approximation and works well with hybrid (discrete and continuous) action space. The method does not rely on the knowledge of the behavior policy, thus can be used to learn from data generated by an unknown policy. Under mild conditions, our algorithm, though surprisingly simple, has a policy improvement bound and outperforms most competing methods empirically. Thorough numerical results are also provided to demonstrate the efficacy of the proposed methodology."
    },
    "keywords": [
        {
            "term": "importance sampling",
            "url": "https://en.wikipedia.org/wiki/importance_sampling"
        },
        {
            "term": "historical trajectory",
            "url": "https://en.wikipedia.org/wiki/historical_trajectory"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Monte-Carlo Tree Search",
            "url": "https://en.wikipedia.org/wiki/Monte-Carlo_Tree_Search"
        },
        {
            "term": "function approximation",
            "url": "https://en.wikipedia.org/wiki/function_approximation"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "We consider the problem of learning a deep policy with batched historical trajectories",
        "We provide empirical evidence that the algorithm is well suited for off-policy reinforcement learning tasks, as it does not need to know the probability of the behavior policy, is robust when learning from replays from an unknown policy",
        "We present an off-policy learning algorithm that can form a better policy from trajectories generated by a possibly unknown policy",
        "The algorithm is preferable in real-world application, including playing video games",
        "Experimental results over several real world datasets validate the effectiveness of the proposed algorithm",
        "Due to the space limitation, a thorough study of our method for full reinforcement learning is left to a future work"
    ],
    "key_statements": [
        "We consider the problem of learning a deep policy with batched historical trajectories",
        "As in many real-world tasks, we usually have numerous historical data generated by different policies, but is lack of a perfect simulator of the environment",
        "We propose a novel yet simple method, to imitate a better policy by monotonic advantage reweighting",
        "From theoretical analysis, we show that the algorithm as proposed has policy improvement lower bound under mild condition",
        "The proposed method works well with function approximation and hybrid action space, which is crucial for the success of deep reinforcement learning in practical problems",
        "For off-policy learning, the method does not rely on the knowledge of action probability of the behavior policy, can be used to learn from data generated by an unknown policy, and is robust when current policy is deviated from the behavior policy",
        "To the best of our knowledge, these prior methods cannot be directly applied in our problem of learning in a complex game environment with large scale replay data, as they either need full-knowledge of the Markov decision process or consider tabular case mainly for finite states and discrete actions, with prohibitive computational complexity",
        "We found value iteration method failed to converge for the tasks in the Half Field Offense environment",
        "We propose a monotonic advantage reweighted imitation learning method (Algorithm 1) which maximizes",
        "We show that the proposed algorithm has policy improvement bound by theoretical analysis",
        "We provide a policy improvement lower bound under mild conditions.\n5.1",
        "We show that in the ideal case when we know the advantage A\u03c0, Algorithm 1 is equivalent to minimizing KL divergence between \u03c0\u03b8 and a hypothetic \u03c0",
        "In practice we use a heuristic approximation to sample data from trajectories generated by the base policy \u03c0 as in Algorithm 1, which is equivalent to imitating \u03c0under a slightly different state distribution d as discussed in Sec.5.1.\n6 Experimental Results",
        "We provide empirical evidence that the algorithm is well suited for off-policy reinforcement learning tasks, as it does not need to know the probability of the behavior policy, is robust when learning from replays from an unknown policy",
        "We evaluate the proposed algorithm with Half Field Offense environment under different settings (Sec. 6.1)",
        "We provide two other environments (TORCS and mobile MOBA game) to evaluate the algorithm in learning from replay data (Sec. 6.2, 6.3)",
        "For the Half Field Offense game, we model the 3 discrete actions with multinomial probabilities and the 2 continuous parameters for each action with normal distributions of known \u03c3 = 0.2 but unknown \u03bc",
        "We evaluate the imitation learning and the proposed method within the TORCS [Wymann et al, 2014] environment",
        "We evaluate the proposed algorithm with King of Glory \u2013 a mobile MOBA (Multi-player Online Battle Arena) game popular in China",
        "We present an off-policy learning algorithm that can form a better policy from trajectories generated by a possibly unknown policy",
        "The algorithm is preferable in real-world application, including playing video games",
        "Experimental results over several real world datasets validate the effectiveness of the proposed algorithm",
        "Due to the space limitation, a thorough study of our method for full reinforcement learning is left to a future work"
    ],
    "summary": [
        "We consider the problem of learning a deep policy with batched historical trajectories.",
        "We want to learn a good policy from these data, to make decisions in a complex environment with possibly continuous state space and hybrid action space of discrete and continuous parts.",
        "We propose a novel yet simple method, to imitate a better policy by monotonic advantage reweighting.",
        "The proposed method works well with function approximation and hybrid action space, which is crucial for the success of deep RL in practical problems.",
        "To the best of our knowledge, these prior methods cannot be directly applied in our problem of learning in a complex game environment with large scale replay data, as they either need full-knowledge of the MDP or consider tabular case mainly for finite states and discrete actions, with prohibitive computational complexity.",
        "Suppose we have state-action pairs in the data generated by a behavior policy \u03c0, we can minimize the KL divergence between \u03c0 and \u03c0\u03b8.",
        "We propose a monotonic advantage reweighted imitation learning method (Algorithm 1) which maximizes",
        "In practice we use a heuristic approximation to sample data from trajectories generated by the base policy \u03c0 as in Algorithm 1, which is equivalent to imitating \u03c0under a slightly different state distribution d as discussed in Sec.5.1.",
        "Note that IL imitates all the actions in the data, while PG needs the on-policy assumption to be a reasonable algorithm.",
        "We expect the proposed algorithm to work better when learning from a possibly unknown behavior policy.",
        "The base policy (13) is used to generate trajectories into a replay memory D, and the policy network is updated by different algorithms, respectively.",
        "We generate noisy trajectories with random actions to intentionally confuse the learning algorithms, and see whether the proposed method can learn a better policy from the data generated by the deteriorated policy.",
        "We see that our proposed algorithm is effective at learning a better policy from these noisy trajectories.",
        "When learning from replay data, the proposed algorithm does not require the bahavior probability \u03c0 over the actions, which is usually missing in human generated data, and works well with function approximation and hybrid action space.",
        "We note that the proposed MARWIL algorithm can work as a full reinforcement learning method, when applied iteratively on self-generated replay data.",
        "Due to the space limitation, a thorough study of our method for full reinforcement learning is left to a future work"
    ],
    "headline": "We propose a monotonic advantage reweighted imitation learning strategy that is applicable to problems with complex nonlinear function approximation and works well with hybrid  action space",
    "reference_links": [
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "Abdolmaleki_et+al_2018_a",
            "entry": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abdolmaleki%2C%20Abbas%20Springenberg%2C%20Jost%20Tobias%20Tassa%2C%20Yuval%20Munos%2C%20Remi%20Maximum%20a%20posteriori%20policy%20optimisation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abdolmaleki%2C%20Abbas%20Springenberg%2C%20Jost%20Tobias%20Tassa%2C%20Yuval%20Munos%2C%20Remi%20Maximum%20a%20posteriori%20policy%20optimisation%202018"
        },
        {
            "id": "Achiam_et+al_2017_a",
            "entry": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning, pages 22\u201331, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achiam%2C%20Joshua%20Held%2C%20David%20Tamar%2C%20Aviv%20Abbeel%2C%20Pieter%20Constrained%20policy%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achiam%2C%20Joshua%20Held%2C%20David%20Tamar%2C%20Aviv%20Abbeel%2C%20Pieter%20Constrained%20policy%20optimization%202017"
        },
        {
            "id": "Mohammad_2012_a",
            "entry": "Mohammad Gheshlaghi Azar, Vicen\u00e7 G\u00f3mez, and Hilbert J Kappen. Dynamic policy programming. Journal of Machine Learning Research, 13(Nov):3207\u20133245, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohammad%20Gheshlaghi%20Azar%2C%20Vicen%C3%A7%20G%C3%B3mez%20Kappen%2C%20Hilbert%20J%20Dynamic%20policy%20programming%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohammad%20Gheshlaghi%20Azar%2C%20Vicen%C3%A7%20G%C3%B3mez%20Kappen%2C%20Hilbert%20J%20Dynamic%20policy%20programming%202012"
        },
        {
            "id": "Bain_1999_a",
            "entry": "Michael Bain and Claude Sommut. A framework for behavioural cloning. Machine intelligence, 15(15):103, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bain%2C%20Michael%20Sommut%2C%20Claude%20A%20framework%20for%20behavioural%20cloning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bain%2C%20Michael%20Sommut%2C%20Claude%20A%20framework%20for%20behavioural%20cloning%201999"
        },
        {
            "id": "Clevert_et+al_2015_a",
            "entry": "Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "Coulom_2005_a",
            "entry": "R\u00e9mi Coulom. Bayesian elo rating. https://www.remi-coulom.fr/Bayesian-Elo/, 2005.[Online; accessed 9-Feb-2018].",
            "url": "https://www.remi-coulom.fr/Bayesian-Elo/"
        },
        {
            "id": "Csiszar_2011_a",
            "entry": "Imre Csiszar and J\u00e1nos K\u00f6rner. Information theory: coding theorems for discrete memoryless systems. Cambridge University Press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Csiszar%2C%20Imre%20K%C3%B6rner%2C%20J%C3%A1nos%20Information%20theory%3A%20coding%20theorems%20for%20discrete%20memoryless%20systems%202011"
        },
        {
            "id": "Garc_2015_a",
            "entry": "Javier Garc\u0131a and Fernando Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garc%C4%B1a%2C%20Javier%20Fern%C3%A1ndez%2C%20Fernando%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garc%C4%B1a%2C%20Javier%20Fern%C3%A1ndez%2C%20Fernando%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015"
        },
        {
            "id": "Geist_2014_a",
            "entry": "Matthieu Geist and Bruno Scherrer. Off-policy learning with eligibility traces: A survey. The Journal of Machine Learning Research, 15(1):289\u2013333, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geist%2C%20Matthieu%20Scherrer%2C%20Bruno%20Off-policy%20learning%20with%20eligibility%20traces%3A%20A%20survey%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geist%2C%20Matthieu%20Scherrer%2C%20Bruno%20Off-policy%20learning%20with%20eligibility%20traces%3A%20A%20survey%202014"
        },
        {
            "id": "Ghavamzadeh_et+al_2016_a",
            "entry": "Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow. Safe policy improvement by minimizing robust baseline regret. In Advances in Neural Information Processing Systems, pages 2298\u20132306, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghavamzadeh%2C%20Mohammad%20Petrik%2C%20Marek%20Chow%2C%20Yinlam%20Safe%20policy%20improvement%20by%20minimizing%20robust%20baseline%20regret%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghavamzadeh%2C%20Mohammad%20Petrik%2C%20Marek%20Chow%2C%20Yinlam%20Safe%20policy%20improvement%20by%20minimizing%20robust%20baseline%20regret%202016"
        },
        {
            "id": "Guo_et+al_2014_a",
            "entry": "Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, and Xiaoshi Wang. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In Advances in neural information processing systems, pages 3338\u20133346, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Xiaoxiao%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Deep%20learning%20for%20real-time%20atari%20game%20play%20using%20offline%20monte-carlo%20tree%20search%20planning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Xiaoxiao%20Singh%2C%20Satinder%20Lee%2C%20Honglak%20Lewis%2C%20Richard%20L.%20Deep%20learning%20for%20real-time%20atari%20game%20play%20using%20offline%20monte-carlo%20tree%20search%20planning%202014"
        },
        {
            "id": "Hausknecht_0000_a",
            "entry": "Matthew Hausknecht. Robocup 2d half field offense technical manual. https://github.com/LARG/HFO/blob/master/doc/manual.pdf, 2017.[Online; accessed 9-Feb-2018].",
            "url": "https://github.com/LARG/HFO/blob/master/doc/manual.pdf"
        },
        {
            "id": "Hausknecht_2016_a",
            "entry": "Matthew Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausknecht%2C%20Matthew%20Stone%2C%20Peter%20Deep%20reinforcement%20learning%20in%20parameterized%20action%20space%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausknecht%2C%20Matthew%20Stone%2C%20Peter%20Deep%20reinforcement%20learning%20in%20parameterized%20action%20space%202016"
        },
        {
            "id": "Hester_et+al_2018_a",
            "entry": "Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, Gabriel Dulac-Arnold, John Agapiou, Joel Leibo, and Audrunas Gruslys. Deep q-learning from demonstrations. In AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hester%2C%20Todd%20Vecerik%2C%20Matej%20Pietquin%2C%20Olivier%20Lanctot%2C%20Marc%20and%20Audrunas%20Gruslys.%20Deep%20q-learning%20from%20demonstrations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hester%2C%20Todd%20Vecerik%2C%20Matej%20Pietquin%2C%20Olivier%20Lanctot%2C%20Marc%20and%20Audrunas%20Gruslys.%20Deep%20q-learning%20from%20demonstrations%202018"
        },
        {
            "id": "Jiang_et+al_2018_a",
            "entry": "Daniel Jiang, Emmanuel Ekwedike, and Han Liu. Feedback-based tree search for reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 2284\u20132293, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Daniel%20Ekwedike%2C%20Emmanuel%20Liu%2C%20Han%20Feedback-based%20tree%20search%20for%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Daniel%20Ekwedike%2C%20Emmanuel%20Liu%2C%20Han%20Feedback-based%20tree%20search%20for%20reinforcement%20learning%202018"
        },
        {
            "id": "Jiang_2016_a",
            "entry": "Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 652\u2013661, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Nan%20Li%2C%20Lihong%20Doubly%20robust%20off-policy%20value%20evaluation%20for%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Nan%20Li%2C%20Lihong%20Doubly%20robust%20off-policy%20value%20evaluation%20for%20reinforcement%20learning%202016"
        },
        {
            "id": "Kakade_2002_a",
            "entry": "Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, pages 267\u2013274, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Mnih_et+al_2013_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "Munos_et+al_2016_a",
            "entry": "R\u00e9mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pages 1054\u20131062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R%C3%A9mi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R%C3%A9mi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016"
        },
        {
            "id": "Peters_et+al_2010_a",
            "entry": "Jan Peters, Katharina M\u00fclling, and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth AAAI Conference on Artificial Intelligence, pages 1607\u20131612, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Altun%2C%20Yasemin%20Relative%20entropy%20policy%20search%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Altun%2C%20Yasemin%20Relative%20entropy%20policy%20search%202010"
        },
        {
            "id": "Pirotta_et+al_2013_a",
            "entry": "Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy iteration. In International Conference on Machine Learning, pages 307\u2013315, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pirotta%2C%20Matteo%20Restelli%2C%20Marcello%20Pecorino%2C%20Alessio%20Calandriello%2C%20Daniele%20Safe%20policy%20iteration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pirotta%2C%20Matteo%20Restelli%2C%20Marcello%20Pecorino%2C%20Alessio%20Calandriello%2C%20Daniele%20Safe%20policy%20iteration%202013"
        },
        {
            "id": "Ross_et+al_2011_a",
            "entry": "St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20St%C3%A9phane%20Gordon%2C%20Geoffrey%20Bagnell%2C%20Drew%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press Cambridge, 1998. Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "International_2016_a",
            "entry": "International Conference on Machine Learning, pages 2139\u20132148, 2016. Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Philip%20Thomas%2C%20Georgios%20Theocharous%2C%20and%20Mohammad%20Ghavamzadeh.%20High%20confidence%20policy%20improvement%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Philip%20Thomas%2C%20Georgios%20Theocharous%2C%20and%20Mohammad%20Ghavamzadeh.%20High%20confidence%20policy%20improvement%202016"
        },
        {
            "id": "Wang_et+al_2015_a",
            "entry": "In International Conference on Machine Learning, pages 2380\u20132388, 2015. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pages 5285\u20135294, 2017. Bernhard Wymann, Eric Espi\u00e9, Christophe Guionneau, Christos Dimitrakakis, R\u00e9mi Coulom, and Andrew Sumner\". TORCS, The Open Racing Car Simulator. http://www.torcs.org, 2014. Jiechao Xiong, Qing Wang, Zhuoran Yang, Peng Sun, Lei Han, Yang Zheng, Haobo Fu, Tong Zhang, Ji Liu, and Han Liu. Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space.arXiv preprint arXiv:1810.06394, 2018.",
            "url": "http://www.torcs.org",
            "arxiv_url": "https://arxiv.org/pdf/1611.01224"
        }
    ]
}
