{
    "filename": "7832-binary-classification-from-positive-confidence-data.pdf",
    "metadata": {
        "title": "Binary Classification from Positive-Confidence Data",
        "author": "Takashi Ishida, Gang Niu, Masashi Sugiyama",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7832-binary-classification-from-positive-confidence-data.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Can we learn a binary classifier from only positive data, without any negative data or unlabeled data? We show that if one can equip positive data with confidence (positive-confidence), one can successfully learn a binary classifier, which we name positive-confidence (Pconf) classification. Our work is related to one-class classification which is aimed at \u201cdescribing\u201d the positive class by clustering-related methods, but one-class classification does not have the ability to tune hyper-parameters and their aim is not on \u201cdiscriminating\u201d positive and negative classes. For the Pconf classification problem, we provide a simple empirical risk minimization framework that is model-independent and optimization-independent. We theoretically establish the consistency and an estimation error bound, and demonstrate the usefulness of the proposed method for training deep neural networks through experiments."
    },
    "keywords": [
        {
            "term": "Empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/Empirical_risk_minimization"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "binary classifier",
            "url": "https://en.wikipedia.org/wiki/binary_classifier"
        },
        {
            "term": "binary classification",
            "url": "https://en.wikipedia.org/wiki/binary_classification"
        }
    ],
    "highlights": [
        "Machine learning with big labeled data has been highly successful in applications such as image recognition, speech recognition, recommendation, and machine translation [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]",
        "Machine learning from weak supervision has been actively explored recently, including semi-supervised classification [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], one-class classification [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>], positive-unlabeled (PU) classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], label-proportion classification [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>], unlabeled-unlabeled classification [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], complementary-label classification [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], and similar-unlabeled classification [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "Organization In this paper, we propose a simple Empirical risk minimization framework for Pconf classification and theoretically establish the consistency and an estimation error bound",
        "We propose an Empirical risk minimization framework for Pconf classification and derive an estimation error bound for the proposed method",
        "Results: The results in Table 3 and Table 4 show that in most cases, Pconf classification either outperforms or is comparable to the weighted classification baseline, outperforms Auto-Encoder, and is even comparable to the fully-supervised method in some cases.\n5https://github.com/zalandoresearch/fashion-mnist 6Both positive and negative data are used to train the probabilistic classifier to estimate confidence, and this data is separated from any other process of experiments. 7https://www.cs.toronto.edu/ \u0303kriz/cifar.html",
        "We proposed a novel problem setting and algorithm for binary classification from positive data equipped with confidence"
    ],
    "key_statements": [
        "Machine learning with big labeled data has been highly successful in applications such as image recognition, speech recognition, recommendation, and machine translation [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]",
        "Machine learning from weak supervision has been actively explored recently, including semi-supervised classification [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], one-class classification [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>], positive-unlabeled (PU) classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], label-proportion classification [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>], unlabeled-unlabeled classification [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], complementary-label classification [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], and similar-unlabeled classification [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We consider a novel setting of classification from weak supervision called positiveconfidence (Pconf) classification, which is aimed at training a binary classifier only from positive data equipped with confidence, without negative data",
        "Organization In this paper, we propose a simple Empirical risk minimization framework for Pconf classification and theoretically establish the consistency and an estimation error bound",
        "We propose an Empirical risk minimization framework for Pconf classification and derive an estimation error bound for the proposed method",
        "We propose the following Empirical risk minimization framework for Pconf classification: Xn h min gg",
        "Results: The results in Table 3 and Table 4 show that in most cases, Pconf classification either outperforms or is comparable to the weighted classification baseline, outperforms Auto-Encoder, and is even comparable to the fully-supervised method in some cases.\n5https://github.com/zalandoresearch/fashion-mnist 6Both positive and negative data are used to train the probabilistic classifier to estimate confidence, and this data is separated from any other process of experiments. 7https://www.cs.toronto.edu/ \u0303kriz/cifar.html",
        "We proposed a novel problem setting and algorithm for binary classification from positive data equipped with confidence"
    ],
    "summary": [
        "Machine learning with big labeled data has been highly successful in applications such as image recognition, speech recognition, recommendation, and machine translation [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "We consider a novel setting of classification from weak supervision called positiveconfidence (Pconf) classification, which is aimed at training a binary classifier only from positive data equipped with confidence, without negative data.",
        "This may be transformed into a probability between 0 and 1 by pre-processing, and it can be used as positive-confidence, which is all we need for Pconf classification.",
        "Pconf classification is aimed at constructing a discriminative classifier and hyper-parameters can be objectively chosen to discriminate between positive and negative data.",
        "We propose an ERM framework for Pconf classification and derive an estimation error bound for the proposed method.",
        "We compared our proposed method (3) with the weighted classification method (4), a regression based method, one-class support vector machine (O-SVM, [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>]) with the Gaussian kernel, and a fully-supervised method based on the empirical version of (1).",
        "Note that the proposed method, weighted method, and regression based method only use Pconf data, O-SVM only uses positive data, and the fully-supervised method uses both positive and negative data.",
        "Weighted, fully-supervised methods, linear-in-input model g(x) = \u21b5>x + b and the logistic loss were commonly used and vanilla gradient descent with 5, 000 epochs and learning rate 0.001 was used for optimization.",
        "3If we naively use default parameters in Sklearn [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>] instead, which is the usual case in the real world without negative data for validation, the classification accuracy of O-SVM is worse for all setups except D in Table 1, which demonstrates the difficulty of using O-SVM.",
        "To select hyper-parameters with validation data, we used the zero-one loss versions of (3) and (4) for Pconf classification and weighted classification, respectively, since no negative data were available in the validation process and we could not directly use the classification accuracy.",
        "Auto-Encoder was trained with positive data, and we classified test data into positive class if the mean squared error (MSE) is below a threshold of 70% quantile, and into negative class otherwise.",
        "5https://github.com/zalandoresearch/fashion-mnist 6Both positive and negative data are used to train the probabilistic classifier to estimate confidence, and this data is separated from any other process of experiments.",
        "We proposed a novel problem setting and algorithm for binary classification from positive data equipped with confidence.",
        "Our key contribution was to show that an unbiased estimator of the classification risk can be obtained for positive-confidence data, without negative data or even unlabeled data.",
        "We established an estimation error bound, and experimentally demonstrated the usefulness of our algorithm"
    ],
    "headline": "Can we learn a binary classifier from only positive data, without any negative data or unlabeled data? We show that if one can equip positive data with confidence , one can successfully learn a binary classifier, which we name positive-confidence  classification",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] H. Bao, G. Niu, and M. Sugiyama. Classification from pairwise similarity and unlabeled data. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bao%2C%20H.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Classification%20from%20pairwise%20similarity%20and%20unlabeled%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bao%2C%20H.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Classification%20from%20pairwise%20similarity%20and%20unlabeled%20data%202018"
        },
        {
            "id": "2",
            "entry": "[2] G. Blanchard, G. Lee, and C. Scott. Semi-supervised novelty detection. Journal of Machine Learning Research, 11:2973\u20133009, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blanchard%2C%20G.%20Lee%2C%20G.%20Scott%2C%20C.%20Semi-supervised%20novelty%20detection%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blanchard%2C%20G.%20Lee%2C%20G.%20Scott%2C%20C.%20Semi-supervised%20novelty%20detection%202010"
        },
        {
            "id": "3",
            "entry": "[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20Optimization%202004"
        },
        {
            "id": "4",
            "entry": "[4] M. M. Breunig, H. P. Kriegel, R. T. Ng, and J. Sander. Lof: Identifying density-based local outliers. In ACM SIGMOD, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breunig%2C%20M.M.%20Kriegel%2C%20H.P.%20Ng%2C%20R.T.%20Sander%2C%20J.%20Lof%3A%20Identifying%20density-based%20local%20outliers%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breunig%2C%20M.M.%20Kriegel%2C%20H.P.%20Ng%2C%20R.T.%20Sander%2C%20J.%20Lof%3A%20Identifying%20density-based%20local%20outliers%202000"
        },
        {
            "id": "5",
            "entry": "[5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Computing Surveys, 41(3), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chandola%2C%20V.%20Banerjee%2C%20A.%20Kumar%2C%20V.%20Anomaly%20detection%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chandola%2C%20V.%20Banerjee%2C%20A.%20Kumar%2C%20V.%20Anomaly%20detection%3A%20A%20survey%202009"
        },
        {
            "id": "6",
            "entry": "[6] O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Semi-Supervised%20Learning%202006"
        },
        {
            "id": "7",
            "entry": "[7] M. C. du Plessis, G. Niu, and M. Sugiyama. Clustering unclustered data: Unsupervised binary labeling of two datasets having different class balances. In TAAI, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Clustering%20unclustered%20data%3A%20Unsupervised%20binary%20labeling%20of%20two%20datasets%20having%20different%20class%20balances%202013"
        },
        {
            "id": "8",
            "entry": "[8] M. C. du Plessis, G. Niu, and M. Sugiyama. Analysis of learning from positive and unlabeled data. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Analysis%20of%20learning%20from%20positive%20and%20unlabeled%20data%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Analysis%20of%20learning%20from%20positive%20and%20unlabeled%20data%202014"
        },
        {
            "id": "9",
            "entry": "[9] M. C. du Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive and unlabeled data. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Convex%20formulation%20for%20learning%20from%20positive%20and%20unlabeled%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Convex%20formulation%20for%20learning%20from%20positive%20and%20unlabeled%20data%202015"
        },
        {
            "id": "10",
            "entry": "[10] M. C. du Plessis, G. Niu, and M. Sugiyama. Class-prior estimation for learning from positive and unlabeled data. Machine Learning, 106(4):463\u2013492, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Class-prior%20estimation%20for%20learning%20from%20positive%20and%20unlabeled%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Class-prior%20estimation%20for%20learning%20from%20positive%20and%20unlabeled%20data%202017"
        },
        {
            "id": "11",
            "entry": "[11] M. C. du Plessis and M. Sugiyama. Class prior estimation from positive and unlabeled data. IEICE Transactions on Information and Systems, E97-D(5):1358\u20131362, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=du%20Plessis%2C%20M.C.%20Sugiyama%2C%20M.%20Class%20prior%20estimation%20from%20positive%20and%20unlabeled%20data%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=du%20Plessis%2C%20M.C.%20Sugiyama%2C%20M.%20Class%20prior%20estimation%20from%20positive%20and%20unlabeled%20data%202014"
        },
        {
            "id": "12",
            "entry": "[12] C. Elkan and K. Noto. Learning classifiers from only positive and unlabeled data. In KDD, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elkan%2C%20C.%20Noto%2C%20K.%20Learning%20classifiers%20from%20only%20positive%20and%20unlabeled%20data%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elkan%2C%20C.%20Noto%2C%20K.%20Learning%20classifiers%20from%20only%20positive%20and%20unlabeled%20data%202008"
        },
        {
            "id": "13",
            "entry": "[13] G. S. Fishman. Monte Carlo: Concepts, Algorithms, and Applications. Springer-Verlag, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fishman%2C%20G.S.%20Monte%20Carlo%3A%20Concepts%2C%20Algorithms%2C%20and%20Applications%201996"
        },
        {
            "id": "14",
            "entry": "[14] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "15",
            "entry": "[15] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20T.%20Tibshirani%2C%20R.%20Friedman%2C%20J.%20The%20Elements%20of%20Statistical%20Learning%3A%20Data%20Mining%2C%20Inference%2C%20and%20Prediction%202009"
        },
        {
            "id": "16",
            "entry": "[16] S. Hido, Y. Tsuboi, H. Kashima, M. Sugiyama, and T. Kanamori. Inlier-based outlier detection via direct density ratio estimation. In ICDM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hido%2C%20S.%20Tsuboi%2C%20Y.%20Kashima%2C%20H.%20Sugiyama%2C%20M.%20Inlier-based%20outlier%20detection%20via%20direct%20density%20ratio%20estimation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hido%2C%20S.%20Tsuboi%2C%20Y.%20Kashima%2C%20H.%20Sugiyama%2C%20M.%20Inlier-based%20outlier%20detection%20via%20direct%20density%20ratio%20estimation%202008"
        },
        {
            "id": "17",
            "entry": "[17] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks%202006"
        },
        {
            "id": "18",
            "entry": "[18] T. Ishida, G. Niu, W. Hu, and M. Sugiyama. Learning from complementary labels. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ishida%2C%20T.%20Niu%2C%20G.%20Hu%2C%20W.%20Sugiyama%2C%20M.%20Learning%20from%20complementary%20labels%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ishida%2C%20T.%20Niu%2C%20G.%20Hu%2C%20W.%20Sugiyama%2C%20M.%20Learning%20from%20complementary%20labels%202017"
        },
        {
            "id": "19",
            "entry": "[19] T. Ishida, G. Niu, A. K. Menon, and M. Sugiyama. Complementary-label learning for arbitrary losses and models. arXiv preprint arXiv:1810.04327, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.04327"
        },
        {
            "id": "20",
            "entry": "[20] F. Johansson et al. mpmath: a Python library for arbitrary-precision floating-point arithmetic (version 0.18), December 2013. http://mpmath.org/.",
            "url": "http://mpmath.org/"
        },
        {
            "id": "21",
            "entry": "[21] S. S. Khan and M. G. Madden. A survey of recent trends in one class classification. In Irish Conference on Artificial Intelligence and Cognitive Science, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20S.S.%20Madden%2C%20M.G.%20A%20survey%20of%20recent%20trends%20in%20one%20class%20classification%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20S.S.%20Madden%2C%20M.G.%20A%20survey%20of%20recent%20trends%20in%20one%20class%20classification%202009"
        },
        {
            "id": "22",
            "entry": "[22] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "23",
            "entry": "[23] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20T.N.%20Welling%2C%20M.%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20T.N.%20Welling%2C%20M.%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "24",
            "entry": "[24] R. Kiryo, G. Niu, M. C. du Plessis, and M. Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiryo%2C%20R.%20Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sugiyama%2C%20M.%20Positive-unlabeled%20learning%20with%20non-negative%20risk%20estimator%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiryo%2C%20R.%20Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sugiyama%2C%20M.%20Positive-unlabeled%20learning%20with%20non-negative%20risk%20estimator%202017"
        },
        {
            "id": "25",
            "entry": "[25] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20M.%20Talagrand%2C%20M.%20Probability%20in%20Banach%20Spaces%3A%20Isoperimetry%20and%20Processes%201991"
        },
        {
            "id": "26",
            "entry": "[26] N. Lu, G. Niu, A. K. Menon, and M. Sugiyama. On the minimal supervision for training any binary classifier from only unlabeled data. arXiv preprint arXiv:1808.10585v2, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.10585v2"
        },
        {
            "id": "27",
            "entry": "[27] C. McDiarmid. On the method of bounded differences. In J. Siemons, editor, Surveys in Combinatorics, pages 148\u2013188. Cambridge University Press, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989"
        },
        {
            "id": "28",
            "entry": "[28] S. Mendelson. Lower bounds for the empirical minimization algorithm. IEEE Transactions on Information Theory, 54(8):3797\u20133803, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mendelson%2C%20S.%20Lower%20bounds%20for%20the%20empirical%20minimization%20algorithm%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mendelson%2C%20S.%20Lower%20bounds%20for%20the%20empirical%20minimization%20algorithm%202008"
        },
        {
            "id": "29",
            "entry": "[29] A. Menon, B. Van Rooyen, C. S. Ong, and B. Williamson. Learning from corrupted binary labels via class-probability estimation. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20A.%20Rooyen%2C%20B.Van%20Ong%2C%20C.S.%20Williamson%2C%20B.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20A.%20Rooyen%2C%20B.Van%20Ong%2C%20C.S.%20Williamson%2C%20B.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015"
        },
        {
            "id": "30",
            "entry": "[30] T. Miyato, S. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing with virtual adversarial training. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20smoothing%20with%20virtual%20adversarial%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20smoothing%20with%20virtual%20adversarial%20training%202016"
        },
        {
            "id": "31",
            "entry": "[31] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20M.%20Rostamizadeh%2C%20A.%20Talwalkar%2C%20A.%20Foundations%20of%20Machine%20Learning%202012"
        },
        {
            "id": "32",
            "entry": "[32] V. Nair and G.E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "33",
            "entry": "[33] N. Natarajan, I. S. Dhillon, P. Ravikumar, and A. Tewari. Learning with noisy labels. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20N.%20Dhillon%2C%20I.S.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20N.%20Dhillon%2C%20I.S.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013"
        },
        {
            "id": "34",
            "entry": "[34] G. Niu, M. C. du Plessis, T. Sakai, Y. Ma, and M. Sugiyama. Theoretical comparisons of positive-unlabeled learning against positive-negative learning. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sakai%2C%20T.%20Ma%2C%20Y.%20Theoretical%20comparisons%20of%20positive-unlabeled%20learning%20against%20positive-negative%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niu%2C%20G.%20du%20Plessis%2C%20M.C.%20Sakai%2C%20T.%20Ma%2C%20Y.%20Theoretical%20comparisons%20of%20positive-unlabeled%20learning%20against%20positive-negative%20learning%202016"
        },
        {
            "id": "35",
            "entry": "[35] J. Nocedal and S. Wright. Numerical Optimization. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Nocedal%20and%20S%20Wright%20Numerical%20Optimization%20Springer%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Nocedal%20and%20S%20Wright%20Numerical%20Optimization%20Springer%202006"
        },
        {
            "id": "36",
            "entry": "[36] A. Oliver, A. Odena, C. Raffel, E. D. Cubuk, and I. J. Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In NIPS, 2018. To appear.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oliver%2C%20A.%20Odena%2C%20A.%20Raffel%2C%20C.%20Cubuk%2C%20E.D.%20Realistic%20evaluation%20of%20deep%20semi-supervised%20learning%20algorithms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oliver%2C%20A.%20Odena%2C%20A.%20Raffel%2C%20C.%20Cubuk%2C%20E.D.%20Realistic%20evaluation%20of%20deep%20semi-supervised%20learning%20algorithms%202018"
        },
        {
            "id": "37",
            "entry": "[37] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. In Autodiff Workshop in NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Paszke%20S%20Gross%20S%20Chintala%20G%20Chanan%20E%20Yang%20Z%20DeVito%20Z%20Lin%20A%20Desmaison%20L%20Antiga%20and%20A%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20Autodiff%20Workshop%20in%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Paszke%20S%20Gross%20S%20Chintala%20G%20Chanan%20E%20Yang%20Z%20DeVito%20Z%20Lin%20A%20Desmaison%20L%20Antiga%20and%20A%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20Autodiff%20Workshop%20in%20NIPS%202017"
        },
        {
            "id": "38",
            "entry": "[38] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011"
        },
        {
            "id": "39",
            "entry": "[39] N. Quadrianto, A. Smola, T. Caetano, and Q. Le. Estimating labels from label proportions. In ICML, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quadrianto%2C%20N.%20Smola%2C%20A.%20Caetano%2C%20T.%20Le%2C%20Q.%20Estimating%20labels%20from%20label%20proportions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quadrianto%2C%20N.%20Smola%2C%20A.%20Caetano%2C%20T.%20Le%2C%20Q.%20Estimating%20labels%20from%20label%20proportions%202008"
        },
        {
            "id": "40",
            "entry": "[40] T. Sakai, M. C. du Plessis, G. Niu, and M. Sugiyama. Semi-supervised classification based on classification from positive and unlabeled data. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sakai%2C%20T.%20du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Semi-supervised%20classification%20based%20on%20classification%20from%20positive%20and%20unlabeled%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sakai%2C%20T.%20du%20Plessis%2C%20M.C.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Semi-supervised%20classification%20based%20on%20classification%20from%20positive%20and%20unlabeled%20data%202017"
        },
        {
            "id": "41",
            "entry": "[41] T. Sakai, G. Niu, and M. Sugiyama. Semi-supervised AUC optimization based on positiveunlabeled learning. Machine Learning, 107(4):767\u2013794, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sakai%2C%20T.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Semi-supervised%20AUC%20optimization%20based%20on%20positiveunlabeled%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sakai%2C%20T.%20Niu%2C%20G.%20Sugiyama%2C%20M.%20Semi-supervised%20AUC%20optimization%20based%20on%20positiveunlabeled%20learning%202018"
        },
        {
            "id": "42",
            "entry": "[42] B. Sch\u00f6lkopf, J. C. Platt, J Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of a high-dimensional distribution. Neural Computation, 13:1443\u20131471, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20B.%20Platt%2C%20J.C.%20Shawe-Taylor%2C%20J.%20Smola%2C%20A.J.%20Estimating%20the%20support%20of%20a%20high-dimensional%20distribution%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6lkopf%2C%20B.%20Platt%2C%20J.C.%20Shawe-Taylor%2C%20J.%20Smola%2C%20A.J.%20Estimating%20the%20support%20of%20a%20high-dimensional%20distribution%202001"
        },
        {
            "id": "43",
            "entry": "[43] B. Sch\u00f6lkopf and A. Smola. Learning with Kernels. MIT Press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20B.%20Smola%2C%20A.%20Learning%20with%20Kernels%202001"
        },
        {
            "id": "44",
            "entry": "[44] C. Scott and G. Blanchard. Novelty detection: Unlabeled data definitely help. In AISTATS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20C.%20Blanchard%2C%20G.%20Novelty%20detection%3A%20Unlabeled%20data%20definitely%20help%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scott%2C%20C.%20Blanchard%2C%20G.%20Novelty%20detection%3A%20Unlabeled%20data%20definitely%20help%202009"
        },
        {
            "id": "45",
            "entry": "[45] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Ben-David%2C%20S.%20Understanding%20Machine%20Learning%3A%20From%20Theory%20to%20Algorithms%202014"
        },
        {
            "id": "46",
            "entry": "[46] A. Smola, L. Song, and C. H. Teo. Relative novelty detection. In AISTATS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20A.%20Song%2C%20L.%20Teo%2C%20C.H.%20Relative%20novelty%20detection%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smola%2C%20A.%20Song%2C%20L.%20Teo%2C%20C.H.%20Relative%20novelty%20detection%202009"
        },
        {
            "id": "47",
            "entry": "[47] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "48",
            "entry": "[48] M. Sugiyama and M. Kawanabe. Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation. MIT Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20M.%20Kawanabe%2C%20M.%20Machine%20Learning%20in%20Non-Stationary%20Environments%3A%20Introduction%20to%20Covariate%20Shift%20Adaptation%202012"
        },
        {
            "id": "49",
            "entry": "[49] M. Sugiyama, G. Niu, M. Yamada, M. Kimura, and H. Hachiya. Information-maximization clustering based on squared-loss mutual information. Neural Computation, 26(1):84\u2013131, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20M.%20Niu%2C%20G.%20Yamada%2C%20M.%20Kimura%2C%20M.%20Information-maximization%20clustering%20based%20on%20squared-loss%20mutual%20information%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sugiyama%2C%20M.%20Niu%2C%20G.%20Yamada%2C%20M.%20Kimura%2C%20M.%20Information-maximization%20clustering%20based%20on%20squared-loss%20mutual%20information%202014"
        },
        {
            "id": "50",
            "entry": "[50] D. Tax and R. Duin. Support vextor domain description. In Pattern Recognition Letters, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tax%2C%20D.%20Duin%2C%20R.%20Support%20vextor%20domain%20description.%20In%20Pattern%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tax%2C%20D.%20Duin%2C%20R.%20Support%20vextor%20domain%20description.%20In%20Pattern%201999"
        },
        {
            "id": "51",
            "entry": "[51] D. M. J. Tax and R. P. W. Duin. Support vector data description. Machine Learning, 54(1):45\u201366, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tax%2C%20D.M.J.%20Duin%2C%20R.P.W.%20Support%20vector%20data%20description%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tax%2C%20D.M.J.%20Duin%2C%20R.P.W.%20Support%20vector%20data%20description%202004"
        },
        {
            "id": "52",
            "entry": "[52] V. N. Vapnik. Statistical Learning Theory. John Wiley and Sons, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20V.N.%20Statistical%20Learning%20Theory%201998"
        },
        {
            "id": "53",
            "entry": "[53] Z. Yang, W. W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Cohen%2C%20W.W.%20Salakhutdinov%2C%20R.%20Revisiting%20semi-supervised%20learning%20with%20graph%20embeddings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Cohen%2C%20W.W.%20Salakhutdinov%2C%20R.%20Revisiting%20semi-supervised%20learning%20with%20graph%20embeddings%202016"
        },
        {
            "id": "54",
            "entry": "[54] F. X. Yu, D. Liu, S. Kumar, T. Jebara, and S.-F. Chang. /svm for learning with label proportions. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20F.X.%20Liu%2C%20D.%20Kumar%2C%20S.%20Jebara%2C%20T.%20Chang.%20/svm%20for%20learning%20with%20label%20proportions%202013"
        },
        {
            "id": "55",
            "entry": "[55] X. Yu, T. Liu, M. Gong, and D. Tao. Learning with biased complementary labels. In ECCV, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20X.%20Liu%2C%20T.%20Gong%2C%20M.%20Tao%2C%20D.%20Learning%20with%20biased%20complementary%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20X.%20Liu%2C%20T.%20Gong%2C%20M.%20Tao%2C%20D.%20Learning%20with%20biased%20complementary%20labels%202018"
        }
    ]
}
