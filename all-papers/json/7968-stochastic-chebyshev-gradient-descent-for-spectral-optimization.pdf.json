{
    "filename": "7968-stochastic-chebyshev-gradient-descent-for-spectral-optimization.pdf",
    "metadata": {
        "title": "Stochastic Chebyshev Gradient Descent for Spectral Optimization",
        "author": "Insu Han, Haim Avron, Jinwoo Shin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7968-stochastic-chebyshev-gradient-descent-for-spectral-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A large class of machine learning techniques requires the solution of optimization problems involving spectral functions of parametric matrices, e.g. log-determinant and nuclear norm. Unfortunately, computing the gradient of a spectral function is generally of cubic complexity, as such gradient descent methods are rather expensive for optimizing objectives involving the spectral function. Thus, one naturally turns to stochastic gradient methods in hope that they will provide a way to reduce or altogether avoid the computation of full gradients. However, here a new challenge appears: there is no straightforward way to compute unbiased stochastic gradients for spectral functions. In this paper, we develop unbiased stochastic gradients for spectral-sums, an important subclass of spectral functions. Our unbiased stochastic gradients are based on combining randomized trace estimators with stochastic truncation of the Chebyshev expansions. A careful design of the truncation distribution allows us to offer distributions that are varianceoptimal, which is crucial for fast and stable convergence of stochastic gradient methods. We further leverage our proposed stochastic gradients to devise stochastic methods for objective functions involving spectral-sums, and rigorously analyze their convergence rate. The utility of our methods is demonstrated in numerical experiments."
    },
    "keywords": [
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "symmetric matrix",
            "url": "https://en.wikipedia.org/wiki/symmetric_matrix"
        },
        {
            "term": "nuclear norm",
            "url": "https://en.wikipedia.org/wiki/nuclear_norm"
        },
        {
            "term": "SVRG",
            "url": "https://en.wikipedia.org/wiki/SVRG"
        },
        {
            "term": "Gaussian process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_process"
        },
        {
            "term": "spectral function",
            "url": "https://en.wikipedia.org/wiki/spectral_function"
        },
        {
            "term": "chebyshev expansion",
            "url": "https://en.wikipedia.org/wiki/chebyshev_expansion"
        }
    ],
    "highlights": [
        "A large class of machine learning techniques involves spectral optimization problems of the form, min F (A(\u03b8)) + g(\u03b8), (1)<br/><br/>\u03b8\u2208C where C is some finite-dimensional parameter space, A is a function that maps a parameter vector \u03b8 to a symmetric matrix A(\u03b8), F is a spectral function (i.e., a real-valued function on symmetric matrices that depends only on the eigenvalues of the input matrix), and g : C \u2192 R",
        "We propose stochastic methods for solving (1) when the spectral function F is a spectral-sum",
        "The proof of the above theorem is given in the supplementary material, where we utilize the recent analysis of stochastic variance reduced gradient method for the sum of smooth non-convex objectives [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We compare it with Algorithm 1 (SGD) which utilizes with unbiased gradient estimators while stochastic variance reduced gradient method requires the exact gradient computation at least once which is intractable to run in these cases",
        "As reported in Figure 3, Stochastic Gradient Descent converges faster than Lanczos quadrature [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] for both datasets and it runs 2 times faster to achieve RMSE 0.0375 under sound dataset and under humid dataset Lanczos quadrature [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] can be often stuck at a local optimum, while Stochastic Gradient Descent avoids it due to the use of unbiased gradient estimators.\n0.167 elapsed time [sec]",
        "We believe that the proposed stochastic methods are of broader interest in many machine learning tasks involving spectral-sums"
    ],
    "key_statements": [
        "A large class of machine learning techniques involves spectral optimization problems of the form, min F (A(\u03b8)) + g(\u03b8), (1)<br/><br/>\u03b8\u2208C where C is some finite-dimensional parameter space, A is a function that maps a parameter vector \u03b8 to a symmetric matrix A(\u03b8), F is a spectral function (i.e., a real-valued function on symmetric matrices that depends only on the eigenvalues of the input matrix), and g : C \u2192 R",
        "We propose stochastic methods for solving (1) when the spectral function F is a spectral-sum",
        "The proof of the above theorem is given in the supplementary material, where we utilize the recent analysis of stochastic variance reduced gradient method for the sum of smooth non-convex objectives [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "The above results confirm that the unbiased gradient estimation and our degree distribution (11) are crucial for Stochastic Gradient Descent.\n5.2",
        "We compare it with Algorithm 1 (SGD) which utilizes with unbiased gradient estimators while stochastic variance reduced gradient method requires the exact gradient computation at least once which is intractable to run in these cases",
        "As reported in Figure 3, Stochastic Gradient Descent converges faster than Lanczos quadrature [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] for both datasets and it runs 2 times faster to achieve RMSE 0.0375 under sound dataset and under humid dataset Lanczos quadrature [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] can be often stuck at a local optimum, while Stochastic Gradient Descent avoids it due to the use of unbiased gradient estimators.\n0.167 elapsed time [sec]",
        "We believe that the proposed stochastic methods are of broader interest in many machine learning tasks involving spectral-sums"
    ],
    "summary": [
        "A large class of machine learning techniques involves spectral optimization problems of the form, min F (A(\u03b8)) + g(\u03b8), (1)<br/><br/>\u03b8\u2208C where C is some finite-dimensional parameter space, A is a function that maps a parameter vector \u03b8 to a symmetric matrix A(\u03b8), F is a spectral function (i.e., a real-valued function on symmetric matrices that depends only on the eigenvalues of the input matrix), and g : C \u2192 R.",
        "We leverage our proposed unbiased estimators to design two stochastic gradient descent methods, one using the SGD framework and the other using the SVRG one.",
        "The goal of this section is to propose a computationally efficient method to generate unbiased stochastic gradients of small variance for \u03a3f (A(\u03b8)).",
        "Stochastic gradients of spectral-sums involving polynomials of degree n can be computed in O(M n( A mv d +",
        "The above estimator can be leveraged to approximate gradients for spectral-sums of analytic functions via the truncated Chebyshev series:",
        "The optimality of the proposed distribution (11) is illustrated by comparing it numerically to other distributio\u221ans: negative binomial and Poisson, on three analytic functions: log x, x and exp(x).",
        "We leverage unbiased gradient estimators based on (8) in conjunction with our optimal degree distribution (11) to design computationally efficient methods for solving (4).",
        "We first derive the following upper bound on the variance of gradient estimators under the optimal degree distribution (11).",
        "Let \u03c8 be the gradient estimator (12) at \u03b8 \u2208 C using Rademacher vectors {v(k)}M k=1 and degree n drawn from the optimal distribution (11).",
        "The randomness in our case is from polynomial degrees and trace probing vectors for optimizing objectives of spectral-sums.",
        "The proof of the above theorem is given in the supplementary material, where we utilize the recent analysis of SVRG for the sum of smooth non-convex objectives [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "The key additional component in our analysis is to characterize \u03b2 > 0 in terms of M, N so that the unbiased gradient estimator (12) is \u03b2-smooth in expectation under the optimal degree distribution (11).",
        "We consider a variant of SGD using a deterministic polynomial degree, referred as SGD-DET, where it uses biased gradient estimators.",
        "The above results confirm that the unbiased gradient estimation and our degree distribution (11) are crucial for SGD.",
        "We compare it with Algorithm 1 (SGD) which utilizes with unbiased gradient estimators while SVRG requires the exact gradient computation at least once which is intractable to run in these cases.",
        "As reported in Figure 3, SGD converges faster than LANCZOS for both datasets and it runs 2 times faster to achieve RMSE 0.0375 under sound dataset and under humid dataset LANCZOS can be often stuck at a local optimum, while SGD avoids it due to the use of unbiased gradient estimators.",
        "We proposed an optimal variance unbiased estimator for spectral-sums and their gradients.",
        "We believe that the proposed stochastic methods are of broader interest in many machine learning tasks involving spectral-sums"
    ],
    "headline": "We develop unbiased stochastic gradients for spectral-sums, an important subclass of spectral functions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Allen-Zhu, Zeyuan and Yuan, Yang. Improved SVRG for non-strongly-convex or sum-of-nonconvex objectives. In International Conference on Machine Learning (ICML), pp. 1080\u20131089, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Yuan%2C%20Yang%20Improved%20SVRG%20for%20non-strongly-convex%20or%20sum-of-nonconvex%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Yuan%2C%20Yang%20Improved%20SVRG%20for%20non-strongly-convex%20or%20sum-of-nonconvex%20objectives%202016"
        },
        {
            "id": "2",
            "entry": "[2] Avron, H. and Toledo, S. Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix. Journal of the ACM, 58(2):8, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avron%2C%20H.%20Toledo%2C%20S.%20Randomized%20algorithms%20for%20estimating%20the%20trace%20of%20an%20implicit%20symmetric%20positive%20semi-definite%20matrix%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avron%2C%20H.%20Toledo%2C%20S.%20Randomized%20algorithms%20for%20estimating%20the%20trace%20of%20an%20implicit%20symmetric%20positive%20semi-definite%20matrix%202011"
        },
        {
            "id": "3",
            "entry": "[3] Berge, Claude. Topological Spaces: including a treatment of multi-valued functions, vector spaces, and convexity. Courier Corporation, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berge%2C%20Claude%20Topological%20Spaces%3A%20including%20a%20treatment%20of%20multi-valued%20functions%2C%20vector%20spaces%2C%20and%20convexity%201963"
        },
        {
            "id": "4",
            "entry": "[4] Bottou, L\u00e9on. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010, pp. 177\u2013186.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent"
        },
        {
            "id": "5",
            "entry": "[5] Broniatowski, Michel and Celant, Giorgio. Some overview on unbiased interpolation and extrapolation designs. arXiv preprint arXiv:1403.5113, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1403.5113"
        },
        {
            "id": "6",
            "entry": "[6] Budincsevity, Norbert. Weather in Szeged 2006-2016. https://www.kaggle.com/budincsevity/szeged-weather/data, 2016.",
            "url": "https://www.kaggle.com/budincsevity/szeged-weather/data"
        },
        {
            "id": "7",
            "entry": "[7] Davidson, Ernest R. The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices. Journal of Computational Physics, 17(1): 87\u201394, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davidson%2C%20Ernest%20R.%20The%20iterative%20calculation%20of%20a%20few%20of%20the%20lowest%20eigenvalues%20and%20corresponding%20eigenvectors%20of%20large%20real-symmetric%20matrices%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davidson%2C%20Ernest%20R.%20The%20iterative%20calculation%20of%20a%20few%20of%20the%20lowest%20eigenvalues%20and%20corresponding%20eigenvectors%20of%20large%20real-symmetric%20matrices%201975"
        },
        {
            "id": "8",
            "entry": "[8] Dong, Kun, Eriksson, David, Nickisch, Hannes, Bindel, David, and Wilson, Andrew G. Scalable log determinants for Gaussian process kernel learning. In Advances in Neural Information Processing Systems, pp. 6330\u20136340, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Kun%20Eriksson%2C%20David%20Nickisch%2C%20Hannes%20Bindel%2C%20David%20Scalable%20log%20determinants%20for%20Gaussian%20process%20kernel%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Kun%20Eriksson%2C%20David%20Nickisch%2C%20Hannes%20Bindel%2C%20David%20Scalable%20log%20determinants%20for%20Gaussian%20process%20kernel%20learning%202017"
        },
        {
            "id": "9",
            "entry": "[9] Friedlander, Michael P. and Mac\u00eado, Ives. Low-rank spectral optimization via gauge duality. SIAM Journal on Scientific Computing, 38(3):A1616\u2013A1638, 2016. doi: 10.1137/15M1034283. URL https://doi.org/10.1137/15M1034283.",
            "crossref": "https://dx.doi.org/10.1137/15M1034283",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/15M1034283"
        },
        {
            "id": "10",
            "entry": "[10] Garber, Dan and Hazan, Elad. Fast and simple PCA via convex optimization. arXiv preprint arXiv:1509.05647, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.05647"
        },
        {
            "id": "11",
            "entry": "[11] Han, Insu, Malioutov, Dmitry, and Shin, Jinwoo. Large-scale log-determinant computation through stochastic chebyshev expansions. In International Conference on Machine Learning, pp. 908\u2013917, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Insu%20Malioutov%2C%20Dmitry%20Shin%2C%20Jinwoo%20Large-scale%20log-determinant%20computation%20through%20stochastic%20chebyshev%20expansions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Insu%20Malioutov%2C%20Dmitry%20Shin%2C%20Jinwoo%20Large-scale%20log-determinant%20computation%20through%20stochastic%20chebyshev%20expansions%202015"
        },
        {
            "id": "12",
            "entry": "[12] Han, Insu, Malioutov, Dmitry, Avron, Haim, and Shin, Jinwoo. Approximating spectral sums of large-scale matrices using stochastic chebyshev approximations. SIAM Journal on Scientific Computing, 39(4):A1558\u2013A1585, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Insu%20Malioutov%2C%20Dmitry%20Avron%2C%20Haim%20Shin%2C%20Jinwoo%20Approximating%20spectral%20sums%20of%20large-scale%20matrices%20using%20stochastic%20chebyshev%20approximations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Insu%20Malioutov%2C%20Dmitry%20Avron%2C%20Haim%20Shin%2C%20Jinwoo%20Approximating%20spectral%20sums%20of%20large-scale%20matrices%20using%20stochastic%20chebyshev%20approximations%202017"
        },
        {
            "id": "13",
            "entry": "[13] Harper, F Maxwell and Konstan, Joseph A. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):19, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harper%2C%20F.Maxwell%20Konstan%2C%20Joseph%20A.%20The%20movielens%20datasets%3A%20History%20and%20context.%20Acm%20transactions%20on%20interactive%20intelligent%20systems%202016"
        },
        {
            "id": "14",
            "entry": "[14] Hutchinson, M.F. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u2013 1076, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hutchinson%2C%20M.F.%20A%20stochastic%20estimator%20of%20the%20trace%20of%20the%20influence%20matrix%20for%20Laplacian%20smoothing%20splines%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hutchinson%2C%20M.F.%20A%20stochastic%20estimator%20of%20the%20trace%20of%20the%20influence%20matrix%20for%20Laplacian%20smoothing%20splines%201989"
        },
        {
            "id": "15",
            "entry": "[15] Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pp. 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "16",
            "entry": "[16] Koltchinskii, Vladimir and Xia, Dong. Optimal estimation of low rank density matrices. J. Mach. Learn. Res., 16(1):1757\u20131792, January 2015. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=2789272.2886806.",
            "url": "http://dl.acm.org/citation.cfm?id=2789272.2886806",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koltchinskii%2C%20Vladimir%20Xia%2C%20Dong%20Optimal%20estimation%20of%20low%20rank%20density%20matrices%202015-01"
        },
        {
            "id": "17",
            "entry": "[17] Lee, Yin Tat, Sidford, Aaron, and Wong, Sam Chiu-wai. A faster cutting plane method and its implications for combinatorial and convex optimization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pp. 1049\u20131065. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Wong%2C%20Sam%20Chiu-wai%20A%20faster%20cutting%20plane%20method%20and%20its%20implications%20for%20combinatorial%20and%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Yin%20Tat%20Sidford%2C%20Aaron%20Wong%2C%20Sam%20Chiu-wai%20A%20faster%20cutting%20plane%20method%20and%20its%20implications%20for%20combinatorial%20and%20convex%20optimization%202015"
        },
        {
            "id": "18",
            "entry": "[18] Lewis, A. S. Derivatives of spectral functions. Mathematics of Operations Research, 21(3):576\u2013 588, 1996. ISSN 0364765X, 15265471. URL http://www.jstor.org/stable/3690298.",
            "url": "http://www.jstor.org/stable/3690298",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lewis%2C%20A.S.%20Derivatives%20of%20spectral%20functions%201996"
        },
        {
            "id": "19",
            "entry": "[19] Lu, Canyi, Lin, Zhouchen, and Yan, Shuicheng. Smoothed low rank and sparse matrix recovery by iteratively reweighted least squares minimization. IEEE Transactions on Image Processing, 24(2):646\u2013654, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Canyi%20Lin%2C%20Zhouchen%20Yan%2C%20Shuicheng%20Smoothed%20low%20rank%20and%20sparse%20matrix%20recovery%20by%20iteratively%20reweighted%20least%20squares%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Canyi%20Lin%2C%20Zhouchen%20Yan%2C%20Shuicheng%20Smoothed%20low%20rank%20and%20sparse%20matrix%20recovery%20by%20iteratively%20reweighted%20least%20squares%20minimization%202015"
        },
        {
            "id": "20",
            "entry": "[20] Mason, John C and Handscomb, David C. Chebyshev polynomials. CRC Press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mason%2C%20John%20C.%20Handscomb%2C%20David%20C.%20Chebyshev%20polynomials%202002"
        },
        {
            "id": "21",
            "entry": "[21] Mohan, Karthik and Fazel, Maryam. Iterative reweighted algorithms for matrix rank minimization. Journal of Machine Learning Research, 13(Nov):3441\u20133473, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohan%2C%20Karthik%20Fazel%2C%20Maryam%20Iterative%20reweighted%20algorithms%20for%20matrix%20rank%20minimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohan%2C%20Karthik%20Fazel%2C%20Maryam%20Iterative%20reweighted%20algorithms%20for%20matrix%20rank%20minimization%202012"
        },
        {
            "id": "22",
            "entry": "[22] Nemirovski, Arkadi, Juditsky, Anatoli, Lan, Guanghui, and Shapiro, Alexander. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%2C%20Guanghui%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%2C%20Guanghui%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "23",
            "entry": "[23] Rasmussen, Carl Edward. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning, pp. 63\u201371.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20Carl%20Edward%20Gaussian%20processes%20in%20machine%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20Carl%20Edward%20Gaussian%20processes%20in%20machine%20learning"
        },
        {
            "id": "24",
            "entry": "[24] Robbins, Herbert and Monro, Sutton. A stochastic approximation method. The Annals of Mathematical Statistics, pp. 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "25",
            "entry": "[25] Roosta-Khorasani, Farbod and Ascher, Uri. Improved bounds on sample size for implicit matrix trace estimators. Foundations of Computational Mathematics, 15(5):1187\u20131212, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roosta-Khorasani%2C%20Farbod%20Ascher%2C%20Uri%20Improved%20bounds%20on%20sample%20size%20for%20implicit%20matrix%20trace%20estimators%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roosta-Khorasani%2C%20Farbod%20Ascher%2C%20Uri%20Improved%20bounds%20on%20sample%20size%20for%20implicit%20matrix%20trace%20estimators%202015"
        },
        {
            "id": "26",
            "entry": "[26] Ryan P Adams, Jeffrey Pennington, Matthew J Johnson Jamie Smith Yaniv Ovadia Brian Patton James Saunderson. Estimating the spectral density of large implicit matrices. arXiv preprint arXiv:1802.03451, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03451"
        },
        {
            "id": "27",
            "entry": "[27] Saad, Yousef. Iterative methods for sparse linear systems. SIAM, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saad%2C%20Yousef%20Iterative%20methods%20for%20sparse%20linear%20systems%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saad%2C%20Yousef%20Iterative%20methods%20for%20sparse%20linear%20systems%202003"
        },
        {
            "id": "28",
            "entry": "[28] Schochetman, Irwin E and Smith, Robert L. Finite dimensional approximation in infinite dimensional mathematical programming. Mathematical Programming, 54(1-3):307\u2013333, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schochetman%2C%20Irwin%20E.%20Smith%2C%20Robert%20L.%20Finite%20dimensional%20approximation%20in%20infinite%20dimensional%20mathematical%20programming%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schochetman%2C%20Irwin%20E.%20Smith%2C%20Robert%20L.%20Finite%20dimensional%20approximation%20in%20infinite%20dimensional%20mathematical%20programming%201992"
        },
        {
            "id": "29",
            "entry": "[29] Trefethen, Lloyd N. Approximation theory and approximation practice. SIAM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trefethen%2C%20Lloyd%20N.%20Approximation%20theory%20and%20approximation%20practice%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trefethen%2C%20Lloyd%20N.%20Approximation%20theory%20and%20approximation%20practice%202013"
        },
        {
            "id": "30",
            "entry": "[30] Ubaru, Shashanka, Chen, Jie, and Saad, Yousef. Fast estimation of tr(f (a)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4):1075\u20131099, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ubaru%2C%20Shashanka%20Chen%2C%20Jie%20Saad%2C%20Yousef%20Fast%20estimation%20of%20tr%28f%20%28a%29%29%20via%20stochastic%20Lanczos%20quadrature%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ubaru%2C%20Shashanka%20Chen%2C%20Jie%20Saad%2C%20Yousef%20Fast%20estimation%20of%20tr%28f%20%28a%29%29%20via%20stochastic%20Lanczos%20quadrature%202017"
        },
        {
            "id": "31",
            "entry": "[31] Vinck, Martin, Battaglia, Francesco P, Balakirsky, Vladimir B, Vinck, AJ Han, and Pennartz, Cyriel MA. Estimation of the entropy based on its polynomial representation. Physical Review E, 85(5):051139, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinck%2C%20Martin%20Battaglia%2C%20Francesco%20P.%20Balakirsky%2C%20Vladimir%20B.%20Vinck%2C%20A.J.Han%20Estimation%20of%20the%20entropy%20based%20on%20its%20polynomial%20representation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinck%2C%20Martin%20Battaglia%2C%20Francesco%20P.%20Balakirsky%2C%20Vladimir%20B.%20Vinck%2C%20A.J.Han%20Estimation%20of%20the%20entropy%20based%20on%20its%20polynomial%20representation%202012"
        },
        {
            "id": "32",
            "entry": "[32] Wilson, Andrew and Nickisch, Hannes. Kernel interpolation for scalable structured Gaussian processes (KISS-GP). In International Conference on Machine Learning, pp. 1775\u20131784, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Andrew%20Nickisch%2C%20Hannes%20Kernel%20interpolation%20for%20scalable%20structured%20Gaussian%20processes%20%28KISS-GP%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Andrew%20Nickisch%2C%20Hannes%20Kernel%20interpolation%20for%20scalable%20structured%20Gaussian%20processes%20%28KISS-GP%29%202015"
        },
        {
            "id": "33",
            "entry": "[33] Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014"
        },
        {
            "id": "34",
            "entry": "[34] Zinkevich, Martin. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 928\u2013936, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
