{
    "filename": "7773-exploiting-numerical-sparsity-for-efficient-learning-faster-eigenvector-computation-and-regression.pdf",
    "metadata": {
        "date": 2018,
        "title": "Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression",
        "author": "Neha Gupta Department of Computer Science",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7773-exploiting-numerical-sparsity-for-efficient-learning-faster-eigenvector-computation-and-regression.pdf"
        },
        "abstract": "In this paper, we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix A \u2208 Rn\u00d7d where every row a \u2208 Rd has a"
    },
    "keywords": [
        {
            "term": "matrix approximation",
            "url": "https://en.wikipedia.org/wiki/matrix_approximation"
        }
    ],
    "highlights": [
        "Regression and top eigenvector computation are two of the most fundamental problems in learning, optimization, and numerical linear algebra",
        "We provide improved iterative methods for top eigenvector computation and regression that depend only on regularity parameters and not the specific sparsity structure of the input",
        "The results for top eigenvector computation are stated in Table 1 and the results for regression are stated in Table 2",
        "Using the framework of Stochastic Variance Reduced Gradient Descent defined in Theorem 14 and the sampling techniques presented in Section 3, we state how do we solve our problems of regression and top eigenvector computation.\n4.1",
        "Using the variance bound obtained in Lemma 11 and the framework of Stochastic Variance Reduced Gradient Descent stated in Theorem 14 for solving approximate linear systems, we show how we can obtain an algorithm for solving approximate regression in time which is faster in certain regimes when the corresponding matrix is numerically sparse"
    ],
    "key_statements": [
        "Regression and top eigenvector computation are two of the most fundamental problems in learning, optimization, and numerical linear algebra",
        "We provide improved iterative methods for top eigenvector computation and regression that depend only on regularity parameters and not the specific sparsity structure of the input",
        "The results for top eigenvector computation are stated in Table 1 and the results for regression are stated in Table 2",
        "Using the framework of Stochastic Variance Reduced Gradient Descent defined in Theorem 14 and the sampling techniques presented in Section 3, we state how do we solve our problems of regression and top eigenvector computation.\n4.1",
        "Using the variance bound obtained in Lemma 11 and the framework of Stochastic Variance Reduced Gradient Descent stated in Theorem 14 for solving approximate linear systems, we show how we can obtain an algorithm for solving approximate regression in time which is faster in certain regimes when the corresponding matrix is numerically sparse"
    ],
    "summary": [
        "Regression and top eigenvector computation are two of the most fundamental problems in learning, optimization, and numerical linear algebra.",
        "There is an extensive amount of work on regression, eigenvector computation, and finite sum optimization with far too many results to state but we have tried to include the algorithms with the best known running times.",
        "Using the framework of SVRG defined in Theorem 14 and the sampling techniques presented in Section 3, we state how do we solve our problems of regression and top eigenvector computation.",
        "We use this framework for solving the eigenvector problem using SVRG and on the top of that, give different sampling scheme for SVRG for B\u22121 which reduces the runtime for numerically sparse matrices.",
        "The following lemma states the variance bound that we get for the gradient updates for SVRG for the top eigenvector computation problem.",
        "Using the variance of the gradient estimators and per iteration running time T obtained in Lemma 8 along with the framework of SVRG [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], we can get constant multiplicative decrease in the error in solving linear systems in B = \u03bbI \u2212 A A in total running time",
        "Using the linear system solver descibed above along with the shift and invert algorithmic framework, we get the following running time for top eigenvector computation problem.",
        "Theorem 9 (Numerically Sparse Top Eigenvector Computation Runtime) Linear system solver from Theorem 18 combined with the shift and invert framework from [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] stated in Theorem 17 gives an algorithm which computes -approximate top eigenvector (Definition 1) in total running time",
        "Linear system solver from Theorem 18 combined with acceleration framework from [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] mentioned in Theorem 15 and shift and invert framework from [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] stated in Theorem 17 gives an algorithm which computes -approximate top eigenvector (Definition 1) in total running time",
        "Using the variance bound obtained in Lemma 11 and the framework of SVRG stated in Theorem 14 for solving approximate linear systems, we show how we can obtain an algorithm for solving approximate regression in time which is faster in certain regimes when the corresponding matrix is numerically sparse.",
        "(Definition 2), if \u03ba \u2264 d2, SVRG framework from Theorem 14 and the variance bound from Lemma 11 gives an algorithm with running time O",
        "Theorem 13 (Numerically Sparse Accelerated Regression Runtime) For solving -approximate regression (Definition 2) if \u03ba \u2264 d2, SVRG framework from Theorem 14, acceleration framework from Theorem 15 and the variance bound from Lemma 11 gives an algorithm with running time"
    ],
    "headline": "S, we provide faster algorithms for these problems in many parameter settings",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Dimitris Achlioptas, Zohar S Karnin, and Edo Liberty. Near-optimal entrywise sampling for data matrices. In Advances in Neural Information Processing Systems, pages 1565\u20131573, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achlioptas%2C%20Dimitris%20Karnin%2C%20Zohar%20S.%20Liberty%2C%20Edo%20Near-optimal%20entrywise%20sampling%20for%20data%20matrices%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achlioptas%2C%20Dimitris%20Karnin%2C%20Zohar%20S.%20Liberty%2C%20Edo%20Near-optimal%20entrywise%20sampling%20for%20data%20matrices%202013"
        },
        {
            "id": "2",
            "entry": "[2] Dimitris Achlioptas and Frank McSherry. Fast computation of low-rank matrix approximations. Journal of the ACM (JACM), 54(2):9, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achlioptas%2C%20Dimitris%20McSherry%2C%20Frank%20Fast%20computation%20of%20low-rank%20matrix%20approximations%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achlioptas%2C%20Dimitris%20McSherry%2C%20Frank%20Fast%20computation%20of%20low-rank%20matrix%20approximations%202007"
        },
        {
            "id": "3",
            "entry": "[3] Naman Agarwal, Sham Kakade, Rahul Kidambi, Yin Tat Lee, Praneeth Netrapalli, and Aaron Sidford. Leverage score sampling for faster accelerated regression and erm. arXiv preprint arXiv:1711.08426, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.08426"
        },
        {
            "id": "4",
            "entry": "[4] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200\u20131205. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "5",
            "entry": "[5] Sanjeev Arora, Elad Hazan, and Satyen Kale. A fast random sampling algorithm for sparsifying matrices. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 272\u2013279.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20A%20fast%20random%20sampling%20algorithm%20for%20sparsifying%20matrices",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Hazan%2C%20Elad%20Kale%2C%20Satyen%20A%20fast%20random%20sampling%20algorithm%20for%20sparsifying%20matrices"
        },
        {
            "id": "6",
            "entry": "[6] L\u00e9on Bottou and Yann L Cun. Large scale online learning. In Advances in neural information processing systems, pages 217\u2013224, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Cun%2C%20Yann%20L.%20Large%20scale%20online%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Cun%2C%20Yann%20L.%20Large%20scale%20online%20learning%202004"
        },
        {
            "id": "7",
            "entry": "[7] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input sparsity time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 81\u201390. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20Kenneth%20L.%20Woodruff%2C%20David%20P.%20Low%20rank%20approximation%20and%20regression%20in%20input%20sparsity%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20Kenneth%20L.%20Woodruff%2C%20David%20P.%20Low%20rank%20approximation%20and%20regression%20in%20input%20sparsity%20time%202013"
        },
        {
            "id": "8",
            "entry": "[8] Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsification via a matrix-valued bernstein inequality. Information Processing Letters, 111(8):385\u2013389, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Zouzias%2C%20Anastasios%20A%20note%20on%20element-wise%20matrix%20sparsification%20via%20a%20matrix-valued%20bernstein%20inequality%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Zouzias%2C%20Anastasios%20A%20note%20on%20element-wise%20matrix%20sparsification%20via%20a%20matrix-valued%20bernstein%20inequality%202011"
        },
        {
            "id": "9",
            "entry": "[9] Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning, pages 2540\u20132548, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frostig%2C%20Roy%20Ge%2C%20Rong%20Kakade%2C%20Sham%20Sidford%2C%20Aaron%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frostig%2C%20Roy%20Ge%2C%20Rong%20Kakade%2C%20Sham%20Sidford%2C%20Aaron%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "10",
            "entry": "[10] Dan Garber, Elad Hazan, Chi Jin, Cameron Musco, Praneeth Netrapalli, Aaron Sidford, et al. Faster eigenvector computation via shift-and-invert preconditioning. In International Conference on Machine Learning, pages 2626\u20132634, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Musco%2C%20Cameron%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Musco%2C%20Cameron%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016"
        },
        {
            "id": "11",
            "entry": "[11] Alex Gittens and Joel A Tropp. Error bounds for random matrix approximation schemes. arXiv preprint arXiv:0911.4108, 2009.",
            "arxiv_url": "https://arxiv.org/pdf/0911.4108"
        },
        {
            "id": "12",
            "entry": "[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "13",
            "entry": "[13] Jakub Konecny, Zheng Qu, and Peter Richt\u00e1rik. Semi-stochastic coordinate descent. Optimization Methods and Software, 32(5):993\u20131005, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konecny%2C%20Jakub%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Semi-stochastic%20coordinate%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konecny%2C%20Jakub%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Semi-stochastic%20coordinate%20descent%202017"
        },
        {
            "id": "14",
            "entry": "[14] Abhisek Kundu and Petros Drineas. A note on randomized element-wise matrix sparsification. arXiv preprint arXiv:1404.0320, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1404.0320"
        },
        {
            "id": "15",
            "entry": "[15] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization. In Advances in Neural Information Processing Systems, pages 3384\u20133392, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20universal%20catalyst%20for%20first-order%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20universal%20catalyst%20for%20first-order%20optimization%202015"
        },
        {
            "id": "16",
            "entry": "[16] Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster approximate singular value decomposition. In Advances in Neural Information Processing Systems, pages 1396\u20131404, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Randomized%20block%20krylov%20methods%20for%20stronger%20and%20faster%20approximate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Randomized%20block%20krylov%20methods%20for%20stronger%20and%20faster%20approximate%202015"
        },
        {
            "id": "17",
            "entry": "[17] Cameron Musco, Praneeth Netrapalli, Aaron Sidford, Shashanka Ubaru, and David P Woodruff. Spectrum approximation beyond fast matrix multiplication: Algorithms and hardness. arXiv preprint arXiv:1704.04163, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04163"
        },
        {
            "id": "18",
            "entry": "[18] Nam H Nguyen, Petros Drineas, and Trac D Tran. Tensor sparsification via a bound on the spectral norm of random tensors. arXiv preprint arXiv:1005.4732, 2010.",
            "arxiv_url": "https://arxiv.org/pdf/1005.4732"
        },
        {
            "id": "19",
            "entry": "[19] NH Nguyen, Petros Drineas, and TD Tran. Matrix sparsification via the khintchine inequality. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20N.H.%20Drineas%2C%20Petros%20Tran%2C%20T.D.%20Matrix%20sparsification%20via%20the%20khintchine%20inequality%202009"
        },
        {
            "id": "20",
            "entry": "[20] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "21",
            "entry": "[21] Ohad Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In International Conference on Machine Learning, pages 144\u2013152, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20A%20stochastic%20pca%20and%20svd%20algorithm%20with%20an%20exponential%20convergence%20rate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20A%20stochastic%20pca%20and%20svd%20algorithm%20with%20an%20exponential%20convergence%20rate%202015"
        }
    ]
}
