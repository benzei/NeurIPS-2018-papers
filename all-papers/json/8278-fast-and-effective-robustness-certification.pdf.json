{
    "filename": "8278-fast-and-effective-robustness-certification.pdf",
    "metadata": {
        "title": "Fast and Effective Robustness Certification",
        "author": "Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, Martin Vechev",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8278-fast-and-effective-robustness-certification.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a new method and system, called DeepZ, for certifying neural network robustness based on abstract interpretation. Compared to state-of-the-art automated verifiers for neural networks, DeepZ: (i) handles ReLU, Tanh and Sigmoid activation functions, (ii) supports feedforward and convolutional architectures, (iii) is significantly more scalable and precise, and (iv) and is sound with respect to floating point arithmetic. These benefits are due to carefully designed approximations tailored to the setting of neural networks. As an example, DeepZ achieves a verification accuracy of 97% on a large network with 88, 500 hidden units under L\u221e attack with = 0.1 with an average runtime of 133 seconds."
    },
    "keywords": [
        {
            "term": "false positive",
            "url": "https://en.wikipedia.org/wiki/false_positive"
        },
        {
            "term": "abstract interpretation",
            "url": "https://en.wikipedia.org/wiki/abstract_interpretation"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Neural networks have become an integral part of many critical applications such as vehicle control, pattern recognition, and medical diagnosis",
        "It has been shown recently that neural networks are susceptible to adversarial attacks, where the network can be tricked into making wrong classification by only slightly modifying its inputs [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "Incomplete verifiers employ a variety of methods including duality [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], abstract interpretation [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and linear approximations [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We introduce new, point-wise Zonotope abstract transformers specifically designed for the ReLU, Sigmoid, and Tanh activations often used by neural networks",
        "We introduced fast and precise Zonotope abstract transformers for key non-linear activations used in modern neural networks",
        "We evaluated the effectiveness of DeepZ on verifying robustness of large feedforward and convolutional networks against challenging L\u221e-norm attacks"
    ],
    "key_statements": [
        "Neural networks have become an integral part of many critical applications such as vehicle control, pattern recognition, and medical diagnosis",
        "It has been shown recently that neural networks are susceptible to adversarial attacks, where the network can be tricked into making wrong classification by only slightly modifying its inputs [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "Incomplete verifiers employ a variety of methods including duality [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], abstract interpretation [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and linear approximations [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We introduce new, point-wise Zonotope abstract transformers specifically designed for the ReLU, Sigmoid, and Tanh activations often used by neural networks",
        "We introduced fast and precise Zonotope abstract transformers for key non-linear activations used in modern neural networks",
        "We evaluated the effectiveness of DeepZ on verifying robustness of large feedforward and convolutional networks against challenging L\u221e-norm attacks"
    ],
    "summary": [
        "Neural networks have become an integral part of many critical applications such as vehicle control, pattern recognition, and medical diagnosis.",
        "We introduce new, point-wise Zonotope abstract transformers specifically designed for the ReLU, Sigmoid, and Tanh activations often used by neural networks.",
        "DeepZ precisely verifies large networks with > 50, 000 hidden units under L\u221e-norm based perturbations within few minutes, while being sound w.r.t to floating point arithmetic.",
        "The analysis with abstract interpretation proceeds by executing them on the particular program (e.g., a neural network) and computing a final approximation (a fixed point).",
        "We introduce our fast and precise pointwise Zonotope abstract transformers for the ReLU, Sigmoid, and Tanh activations (Sigmoid and Tanh are not supported by Gehr et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]) and show their optimality in terms of area in the input-output plane.",
        "Fig. 2 shows the projections into the xy-plane of a set of sound zonotope approximations for the output of the Sigmoid function which have ly = \u03c3, uy = \u03c3.",
        "We evaluate the effectiveness of our new Zonotope transformers for verifying local robustness of neural networks.",
        "We compare the precision and the performance of the sequential version of DeepZ against two state-of-the-art certifiers Fast-Lin [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and AI2 [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] on the FFNNSmall MNIST network with ReLU activation.",
        "Fig. 4 shows the percentage of verified robustness and the average analysis time of DeepZ for the MNIST networks with ReLU activations.",
        "DeepZ analyzes all FFNNBig networks with average runtime \u2264 22 seconds and proves 95% of the robustness properties for = 0.04 for the defended P GD =0.3 network.",
        "Fig. 5 shows the precision and the performance of DeepZ on the MNIST FFNNBig and ConvMed networks with Sigmoid and Tanh activations.",
        "It can be seen that DeepZ verifies 74% of the robustness properties on the FFNNBig Sigmoid and Tanh networks trained with P GD =0.3 for = 0.03.",
        "Fig. 6 shows that DeepZ has an average runtime of \u2264 50 seconds for the CIFAR10 FFNNBig ReLU networks.",
        "DeepZ is able to verify 50% of robustness properties for ConvBig network defended with DiffAI with an average runtime of 39 seconds as shown in Table 2.",
        "DeepZ verifies 82% of robustness properties on the Sigmoid network defended with P GD =0.0078 for = 0.012 in Fig. 7.",
        "We introduced fast and precise Zonotope abstract transformers for key non-linear activations used in modern neural networks.",
        "Our results show that DeepZ is more precise and faster than prior work, while ensuring soundness w.r.t floating point operations."
    ],
    "headline": "We present a new method and system, called DeepZ, for certifying neural network robustness based on abstract interpretation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] ELINA: ETH Library for Numerical Analysis. http://elina.ethz.ch.",
            "url": "http://elina.ethz.ch"
        },
        {
            "id": "2",
            "entry": "[2] Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. In Proc. International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Athalye%2C%20Anish%20Sutskever%2C%20Ilya%20Synthesizing%20robust%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Athalye%2C%20Anish%20Sutskever%2C%20Ilya%20Synthesizing%20robust%20adversarial%20examples%202018"
        },
        {
            "id": "3",
            "entry": "[3] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In Proc. IEEE Symposium on Security and Privacy (SP), pages 39\u201357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Ground-truth adversarial examples. CoRR, abs/1709.10207, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10207"
        },
        {
            "id": "5",
            "entry": "[5] Patrick Cousot and Radhia Cousot. Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. In Proc. Symposium on Principles of Programming Languages (POPL), pages 238\u2013252, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cousot%2C%20Patrick%20Cousot%2C%20Radhia%20Abstract%20interpretation%3A%20A%20unified%20lattice%20model%20for%20static%20analysis%20of%20programs%20by%20construction%20or%20approximation%20of%20fixpoints%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cousot%2C%20Patrick%20Cousot%2C%20Radhia%20Abstract%20interpretation%3A%20A%20unified%20lattice%20model%20for%20static%20analysis%20of%20programs%20by%20construction%20or%20approximation%20of%20fixpoints%201977"
        },
        {
            "id": "6",
            "entry": "[6] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Yinpeng%20Liao%2C%20Fangzhou%20Pang%2C%20Tianyu%20Su%2C%20Hang%20Boosting%20adversarial%20attacks%20with%20momentum%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Yinpeng%20Liao%2C%20Fangzhou%20Pang%2C%20Tianyu%20Su%2C%20Hang%20Boosting%20adversarial%20attacks%20with%20momentum%202018"
        },
        {
            "id": "7",
            "entry": "[7] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. In Proc. Uncertainty in Artificial Intelligence (UAI), pages 162\u2013171, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dvijotham%2C%20Krishnamurthy%20Stanforth%2C%20Robert%20Gowal%2C%20Sven%20Mann%2C%20Timothy%20A%20dual%20approach%20to%20scalable%20verification%20of%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dvijotham%2C%20Krishnamurthy%20Stanforth%2C%20Robert%20Gowal%2C%20Sven%20Mann%2C%20Timothy%20A%20dual%20approach%20to%20scalable%20verification%20of%20deep%20networks%202018"
        },
        {
            "id": "8",
            "entry": "[8] R\u00fcdiger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Automated Technology for Verification and Analysis (ATVA), pages 269\u2013286, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ehlers%2C%20R%C3%BCdiger%20Formal%20verification%20of%20piece-wise%20linear%20feed-forward%20neural%20networks.%20In%20Automated%20Technology%20for%20Verification%20and%20Analysis%20%28ATVA%29%202017"
        },
        {
            "id": "9",
            "entry": "[9] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on deep learning models. arXiv preprint arXiv:1707.08945, 1, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08945"
        },
        {
            "id": "10",
            "entry": "[10] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In Proc. IEEE Symposium on Security and Privacy (SP), pages 948\u2013963, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehr%2C%20T.%20Mirman%2C%20M.%20Drachsler-Cohen%2C%20D.%20Tsankov%2C%20P.%20Ai2%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehr%2C%20T.%20Mirman%2C%20M.%20Drachsler-Cohen%2C%20D.%20Tsankov%2C%20P.%20Ai2%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018"
        },
        {
            "id": "11",
            "entry": "[11] Khalil Ghorbal, Eric Goubault, and Sylvie Putot. The zonotope abstract domain taylor1+. In Proc. Computer Aided Verification (CAV), pages 627\u2013633, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghorbal%2C%20Khalil%20Eric%20Goubault%2C%20and%20Sylvie%20Putot.%20The%20zonotope%20abstract%20domain%20taylor1%2B%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghorbal%2C%20Khalil%20Eric%20Goubault%2C%20and%20Sylvie%20Putot.%20The%20zonotope%20abstract%20domain%20taylor1%2B%202009"
        },
        {
            "id": "12",
            "entry": "[12] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Proc. International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "13",
            "entry": "[13] Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.5068"
        },
        {
            "id": "14",
            "entry": "[14] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In Proc. Computer Aided Verification (CAV), pages 97\u2013117, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katz%2C%20Guy%20Barrett%2C%20Clark%20W.%20Dill%2C%20David%20L.%20Julian%2C%20Kyle%20Reluplex%3A%20An%20efficient%20SMT%20solver%20for%20verifying%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katz%2C%20Guy%20Barrett%2C%20Clark%20W.%20Dill%2C%20David%20L.%20Julian%2C%20Kyle%20Reluplex%3A%20An%20efficient%20SMT%20solver%20for%20verifying%20deep%20neural%20networks%202017"
        },
        {
            "id": "15",
            "entry": "[15] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. In Proc. International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolter%2C%20J.Zico%20Wong%2C%20Eric%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolter%2C%20J.Zico%20Wong%2C%20Eric%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018"
        },
        {
            "id": "16",
            "entry": "[16] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "17",
            "entry": "[17] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. CoRR, abs/1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "18",
            "entry": "[18] Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proc. of the IEEE, pages 2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lecun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lecun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "19",
            "entry": "[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In Proc. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "20",
            "entry": "[20] Antoine Min\u00e9. Relational abstract domains for the detection of floating-point run-time errors. In Proc. European Symposium on Programming (ESOP), pages 3\u201317, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Min%C3%A9%2C%20Antoine%20Relational%20abstract%20domains%20for%20the%20detection%20of%20floating-point%20run-time%20errors%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Min%C3%A9%2C%20Antoine%20Relational%20abstract%20domains%20for%20the%20detection%20of%20floating-point%20run-time%20errors%202004"
        },
        {
            "id": "21",
            "entry": "[21] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In Proc. International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirman%2C%20Matthew%20Gehr%2C%20Timon%20Vechev%2C%20Martin%20Differentiable%20abstract%20interpretation%20for%20provably%20robust%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirman%2C%20Matthew%20Gehr%2C%20Timon%20Vechev%2C%20Martin%20Differentiable%20abstract%20interpretation%20for%20provably%20robust%20neural%20networks%202018"
        },
        {
            "id": "22",
            "entry": "[22] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07277"
        },
        {
            "id": "23",
            "entry": "[23] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proc. Asia Conference on Computer and Communications Security, pages 506\u2013519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "24",
            "entry": "[24] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In Proc. International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghunathan%2C%20Aditi%20Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Certified%20defenses%20against%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghunathan%2C%20Aditi%20Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Certified%20defenses%20against%20adversarial%20examples%202018"
        },
        {
            "id": "25",
            "entry": "[25] Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. Fast polyhedra abstract domain. In Proc. Principles of Programming Languages (POPL), pages 46\u201359, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Gagandeep%20P%C3%BCschel%2C%20Markus%20Vechev%2C%20Martin%20Fast%20polyhedra%20abstract%20domain%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Gagandeep%20P%C3%BCschel%2C%20Markus%20Vechev%2C%20Martin%20Fast%20polyhedra%20abstract%20domain%202017"
        },
        {
            "id": "26",
            "entry": "[26] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "27",
            "entry": "[27] Vincent Tjeng and Russ Tedrake. Verifying neural networks with mixed integer programming. CoRR, abs/1711.07356, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07356"
        },
        {
            "id": "28",
            "entry": "[28] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis of neural networks using symbolic intervals. In USENIX Security Symposium (USENIX Security 18), pages 1599\u20131614, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shiqi%20Pei%2C%20Kexin%20Whitehouse%2C%20Justin%20Yang%2C%20Junfeng%20Formal%20security%20analysis%20of%20neural%20networks%20using%20symbolic%20intervals%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shiqi%20Pei%2C%20Kexin%20Whitehouse%2C%20Justin%20Yang%2C%20Junfeng%20Formal%20security%20analysis%20of%20neural%20networks%20using%20symbolic%20intervals%202018"
        },
        {
            "id": "29",
            "entry": "[29] Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In Proc. International Conference on Machine Learning (ICML), pages 5273\u20135282, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Hongge%20Song%2C%20Zhao%20Towards%20fast%20computation%20of%20certified%20robustness%20for%20ReLU%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Hongge%20Song%2C%20Zhao%20Towards%20fast%20computation%20of%20certified%20robustness%20for%20ReLU%20networks%202018"
        },
        {
            "id": "30",
            "entry": "[30] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses. CoRR, abs/1805.12514, 2018. URL http://arxiv.org/abs/1805.12514. ",
            "url": "http://arxiv.org/abs/1805.12514",
            "arxiv_url": "https://arxiv.org/pdf/1805.12514"
        }
    ]
}
