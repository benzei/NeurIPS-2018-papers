{
    "filename": "8203-learning-to-reason-with-third-order-tensor-products.pdf",
    "metadata": {
        "date": 2018,
        "title": "Learning to Reason with Third-Order Tensor Products",
        "author": "Imanol Schlag The Swiss AI Lab IDSIA / USI / SUPSI",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8203-learning-to-reason-with-third-order-tensor-products.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We combine Recurrent Neural Networks with Tensor Product Representations to learn combinatorial representations of sequential data. This improves symbolic interpretation and systematic generalisation. Our architecture is trained end-to-end through gradient descent on a variety of simple natural language reasoning tasks, significantly outperforming the latest state-of-the-art models in single-task and all-tasks settings. We also augment a subset of the data such that training and test data exhibit large systematic differences and show that our approach generalises better than the previous state-of-the-art."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "art model",
            "url": "https://en.wikipedia.org/wiki/art_model"
        },
        {
            "term": "tensor product",
            "url": "https://en.wikipedia.org/wiki/tensor_product"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "short term",
            "url": "https://en.wikipedia.org/wiki/short_term"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "language processing",
            "url": "https://en.wikipedia.org/wiki/language_processing"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        }
    ],
    "highlights": [
        "Certain connectionist architectures based on Recurrent Neural Networks (RNNs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] such as the Long Short-Term Memory (LSTM) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] are general computers, e.g., [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "We propose a new architecture based on the Tensor Product Representation (TPR) [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], a general method for embedding symbolic structures in a vector space",
        "We evaluate our architecture on bAbI tasks, a set of 20 different synthetic questionanswering tasks designed to evaluate natural language reasoning systems such as intelligent dialogue agents [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "Our approach is related to Fast Weight architectures, another way of increasing the memory capacity of Recurrent Neural Networks",
        "Our model generalises better than a previous state-of-the-art model when there are strong systematic differences between training and test data"
    ],
    "key_statements": [
        "Certain connectionist architectures based on Recurrent Neural Networks (RNNs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] such as the Long Short-Term Memory (LSTM) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] are general computers, e.g., [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "We propose a new architecture based on the Tensor Product Representation (TPR) [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], a general method for embedding symbolic structures in a vector space",
        "We evaluate our architecture on bAbI tasks, a set of 20 different synthetic questionanswering tasks designed to evaluate natural language reasoning systems such as intelligent dialogue agents [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "Our approach is related to Fast Weight architectures, another way of increasing the memory capacity of Recurrent Neural Networks",
        "Our model generalises better than a previous state-of-the-art model when there are strong systematic differences between training and test data"
    ],
    "summary": [
        "Certain connectionist architectures based on Recurrent Neural Networks (RNNs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] such as the Long Short-Term Memory (LSTM) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] are general computers, e.g., [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].",
        "We propose a new architecture based on the Tensor Product Representation (TPR) [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], a general method for embedding symbolic structures in a vector space.",
        "We combine gradient-based RNNs with third-order TPRs to learn combinatorial representations from natural language, training the entire system on NLR tasks via error backpropagation [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>\u2013<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "We aim at overcoming these limitations by recognising the TPR as a form of Fast Weight memory which uses multi-layer perceptron (MLP) based neural networks trained end-to-end by stochastic gradient descent.",
        "Previous outer product-based Fast Weights [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], which share strong similarities to TPRs of order 2, have shown to be powerful associative memory mechanisms [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>].",
        "A traditional RNN uses one or several tensors of order 1 (i.e. a vector usually referred to as the hidden state) to encode the information of the past sequence elements necessary to infer the correct current and future outputs.",
        "While the write operation removes the previous target entity representation wt, the move operation allows to rewrite wt back into the TPR with a different relation r(t2).",
        "Similar to the update module, first an entity n and a set of relations lj are extracted from the current sentence using four different networks.",
        "The output yof our architecture consists of the sum of the three previous inference steps followed by a linear projection Z into the symbol space VSymbols where a softmax transforms the activations into a probability distribution over all words from the vocabulary of the current task.",
        "Previous work used TPRs of order 2 for simpler associations in the context of image-caption generation [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>], question-answering [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], and general NLP [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] problems with a gradient-based optimizer similar to ours.",
        "We collect the set of unique sentences across all stories from the validation set of a task and compute their respective entity and relation representations e(1), e(2), r(1), r(2), and r(3).",
        "Our analysis and the additional experiment indicate that the model seems to learn combinatorial representations resulting in interpretable distributed representations and data efficiency due to rule-like generalisation.",
        "Since outer product-based Fast Weights [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] and our TPR system have many more time-varying variables per learnable parameter than a classic RNN such as LSTM, this makes them less scalable in terms of memory requirements.",
        "Our novel RNN-TPR combination learns to decompose natural language sentences into combinatorial components useful for reasoning.",
        "Our approach is related to Fast Weight architectures, another way of increasing the memory capacity of RNNs. An analysis of a trained model suggests straightforward interpretability of the learned representations.",
        "Our model generalises better than a previous state-of-the-art model when there are strong systematic differences between training and test data"
    ],
    "headline": "We propose a new architecture based on the Tensor Product Representation  , a general method for embedding symbolic structures in a vector space",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. J. Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, 1, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20P.J.%20Generalization%20of%20backpropagation%20with%20application%20to%20a%20recurrent%20gas%20market%20model%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Werbos%2C%20P.J.%20Generalization%20of%20backpropagation%20with%20application%20to%20a%20recurrent%20gas%20market%20model%201988"
        },
        {
            "id": "2",
            "entry": "[2] R. J. Williams and D. Zipser. Gradient-based learning algorithms for recurrent networks and their computational complexity. In Back-propagation: Theory, Architectures and Applications. Hillsdale, NJ: Erlbaum, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Zipser%2C%20D.%20Gradient-based%20learning%20algorithms%20for%20recurrent%20networks%20and%20their%20computational%20complexity.%20In%20Back-propagation%3A%20Theory%2C%20Architectures%20and%20Applications%201994"
        },
        {
            "id": "3",
            "entry": "[3] A. J. Robinson and F. Fallside. The utility driven dynamic error propagation network. Technical Report CUED/F-INFENG/TR.1, Cambridge University Engineering Department, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robinson%2C%20A.J.%20Fallside%2C%20F.%20The%20utility%20driven%20dynamic%20error%20propagation%20network%201987"
        },
        {
            "id": "4",
            "entry": "[4] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735\u2013 1780, 1997. Based on TR FKI-207-95, TUM (1995).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20Short-Term%20Memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20Short-Term%20Memory%201997"
        },
        {
            "id": "5",
            "entry": "[5] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, 12(10):2451\u20132471, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gers%2C%20F.A.%20Schmidhuber%2C%20J.%20Cummins%2C%20F.%20Learning%20to%20forget%3A%20Continual%20prediction%20with%20LSTM%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gers%2C%20F.A.%20Schmidhuber%2C%20J.%20Cummins%2C%20F.%20Learning%20to%20forget%3A%20Continual%20prediction%20with%20LSTM%202000"
        },
        {
            "id": "6",
            "entry": "[6] H. T. Siegelmann and E. D. Sontag. Turing computability with neural nets. Applied Mathematics Letters, 4(6):77\u201380, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Siegelmann%2C%20H.T.%20Sontag%2C%20E.D.%20Turing%20computability%20with%20neural%20nets%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Siegelmann%2C%20H.T.%20Sontag%2C%20E.D.%20Turing%20computability%20with%20neural%20nets%201991"
        },
        {
            "id": "7",
            "entry": "[7] Hasim Sak, Andrew Senior, Kanishka Rao, Fran\u00e7oise Beaufays, and Johan Schalkwyk. Google voice search: faster and more accurate. Google Research Blog, 2015, http://googleresearch.blogspot.ch/2015/09/google-voice-search-faster-and-more.html.",
            "url": "http://googleresearch.blogspot.ch/2015/09/google-voice-search-faster-and-more.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sak%2C%20Hasim%20Senior%2C%20Andrew%20Rao%2C%20Kanishka%20Beaufays%2C%20Fran%C3%A7oise%20Google%20voice%20search%3A%20faster%20and%20more%20accurate%202015"
        },
        {
            "id": "8",
            "entry": "[8] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. Preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "9",
            "entry": "[9] J.M. Pino, A. Sidorov, and N.F. Ayan. Transitioning entirely to neural machine translation. Facebook Research Blog, 2017, https://code.facebook.com/posts/289921871474277/transitioningentirely-to-neural-machine-translation/.",
            "url": "https://code.facebook.com/posts/289921871474277/transitioningentirely-to-neural-machine-translation/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pino%2C%20J.M.%20Sidorov%2C%20A.%20Ayan%2C%20N.F.%20Transitioning%20entirely%20to%20neural%20machine%20translation"
        },
        {
            "id": "10",
            "entry": "[10] Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fodor%2C%20Jerry%20A.%20Pylyshyn%2C%20Zenon%20W.%20Connectionism%20and%20cognitive%20architecture%3A%20A%20critical%20analysis%201988"
        },
        {
            "id": "11",
            "entry": "[11] Robert F Hadley. Systematicity in connectionist language learning. Mind & Language, 9(3):247\u2013 272, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadley%2C%20Robert%20F.%20Systematicity%20in%20connectionist%20language%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadley%2C%20Robert%20F.%20Systematicity%20in%20connectionist%20language%20learning%201994"
        },
        {
            "id": "12",
            "entry": "[12] Brenden M. Lake and Marco Baroni. Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks. CoRR, abs/1711.00350, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00350"
        },
        {
            "id": "13",
            "entry": "[13] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Ullman%2C%20Tomer%20D.%20Tenenbaum%2C%20Joshua%20B.%20Gershman%2C%20Samuel%20J.%20Building%20machines%20that%20learn%20and%20think%20like%20people%202017"
        },
        {
            "id": "14",
            "entry": "[14] Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson, and Gal Chechik. Learning to generalize to new compositions in image understanding. arXiv preprint arXiv:1608.07639, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.07639"
        },
        {
            "id": "15",
            "entry": "[15] Steven Andrew Phillips. Connectionism and the problem of systematicity. PhD thesis, University of Queensland, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Phillips%2C%20Steven%20Andrew%20Connectionism%20and%20the%20problem%20of%20systematicity%201995"
        },
        {
            "id": "16",
            "entry": "[16] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. CoRR, abs/1502.05698, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05698"
        },
        {
            "id": "17",
            "entry": "[17] Olivier J. Brousse. Generativity and Systematicity in Neural Network Combinatorial Learning. PhD thesis, Boulder, CO, USA, 1992. UMI Order No. GAX92-20396.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brousse%2C%20Olivier%20J.%20Generativity%20and%20Systematicity%20in%20Neural%20Network%20Combinatorial%20Learning%201992"
        },
        {
            "id": "18",
            "entry": "[18] Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159\u2013216, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20Paul%20Tensor%20product%20variable%20binding%20and%20the%20representation%20of%20symbolic%20structures%20in%20connectionist%20systems%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smolensky%2C%20Paul%20Tensor%20product%20variable%20binding%20and%20the%20representation%20of%20symbolic%20structures%20in%20connectionist%20systems%201990"
        },
        {
            "id": "19",
            "entry": "[19] S. Linnainmaa. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master\u2019s thesis, Univ. Helsinki, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linnainmaa%2C%20S.%20The%20representation%20of%20the%20cumulative%20rounding%20error%20of%20an%20algorithm%20as%20a%20Taylor%20expansion%20of%20the%20local%20rounding%20errors%201970"
        },
        {
            "id": "20",
            "entry": "[20] H. J. Kelley. Gradient theory of optimal flight paths. ARS Journal, 30(10):947\u2013954, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelley%2C%20H.J.%20Gradient%20theory%20of%20optimal%20flight%20paths%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kelley%2C%20H.J.%20Gradient%20theory%20of%20optimal%20flight%20paths%201960"
        },
        {
            "id": "21",
            "entry": "[21] Paul J. Werbos. Backpropagation through time: What it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20Paul%20J.%20Backpropagation%20through%20time%3A%20What%20it%20does%20and%20how%20to%20do%20it%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Werbos%2C%20Paul%20J.%20Backpropagation%20through%20time%3A%20What%20it%20does%20and%20how%20to%20do%20it%201990"
        },
        {
            "id": "22",
            "entry": "[22] von der Malsburg. The Correlation Theory of Brain Function. Internal report. Department of Neurobiology, Max-Planck-Institute for Biophysical Chemistry. 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=von%20der%20Malsburg%20The%20Correlation%20Theory%20of%20Brain%20Function.%20Internal%20report.%20Department%20of%20Neurobiology%2C%20Max-Planck-Institute%20for%20Biophysical%20Chemistry%201981"
        },
        {
            "id": "23",
            "entry": "[23] Jerome A Feldman. Dynamic connections in neural networks. Biological cybernetics, 46(1):27\u2013 39, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Jerome%20A.%20Dynamic%20connections%20in%20neural%20networks%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20Jerome%20A.%20Dynamic%20connections%20in%20neural%20networks%201982"
        },
        {
            "id": "24",
            "entry": "[24] Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pages 177\u2013186, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20C%20Plaut.%20Using%20fast%20weights%20to%20deblur%20old%20memories%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20C%20Plaut.%20Using%20fast%20weights%20to%20deblur%20old%20memories%201987"
        },
        {
            "id": "25",
            "entry": "[25] J. Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Neural Computation, 4(1):131\u2013139, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20recurrent%20nets%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20recurrent%20nets%201992"
        },
        {
            "id": "26",
            "entry": "[26] J. Schmidhuber. On decreasing the ratio between learning complexity and number of timevarying variables in fully recurrent nets. In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, pages 460\u2013463.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20On%20decreasing%20the%20ratio%20between%20learning%20complexity%20and%20number%20of%20timevarying%20variables%20in%20fully%20recurrent%20nets",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20On%20decreasing%20the%20ratio%20between%20learning%20complexity%20and%20number%20of%20timevarying%20variables%20in%20fully%20recurrent%20nets"
        },
        {
            "id": "27",
            "entry": "[27] Imanol Schlag and J\u00fcrgen Schmidhuber. Gated fast weights for on-the-fly neural program generation. In NIPS Metalearning Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schlag%2C%20Imanol%20Schmidhuber%2C%20J%C3%BCrgen%20Gated%20fast%20weights%20for%20on-the-fly%20neural%20program%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schlag%2C%20Imanol%20Schmidhuber%2C%20J%C3%BCrgen%20Gated%20fast%20weights%20for%20on-the-fly%20neural%20program%20generation%202017"
        },
        {
            "id": "28",
            "entry": "[28] Paul Smolensky. Symbolic functions from neural computation. Phil. Trans. R. Soc. A, 370(1971):3543\u20133569, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20Paul%20Symbolic%20functions%20from%20neural%20computation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smolensky%2C%20Paul%20Symbolic%20functions%20from%20neural%20computation%202012"
        },
        {
            "id": "29",
            "entry": "[29] Paul Smolensky, Moontae Lee, Xiaodong He, Wen-tau Yih, Jianfeng Gao, and Li Deng. Basic reasoning with tensor product representations. CoRR, abs/1601.02745, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.02745"
        },
        {
            "id": "30",
            "entry": "[30] Jerry Fodor and Brian P McLaughlin. Connectionism and the problem of systematicity: Why smolensky\u2019s solution doesn\u2019t work. Cognition, 35(2):183\u2013204, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fodor%2C%20Jerry%20McLaughlin%2C%20Brian%20P.%20Connectionism%20and%20the%20problem%20of%20systematicity%3A%20Why%20smolensky%E2%80%99s%20solution%20doesn%E2%80%99t%20work%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fodor%2C%20Jerry%20McLaughlin%2C%20Brian%20P.%20Connectionism%20and%20the%20problem%20of%20systematicity%3A%20Why%20smolensky%E2%80%99s%20solution%20doesn%E2%80%99t%20work%201990"
        },
        {
            "id": "31",
            "entry": "[31] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. In Advances In Neural Information Processing Systems, pages 4331\u20134339, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Hinton%2C%20Geoffrey%20E.%20Mnih%2C%20Volodymyr%20Leibo%2C%20Joel%20Z.%20Using%20fast%20weights%20to%20attend%20to%20the%20recent%20past%202016"
        },
        {
            "id": "32",
            "entry": "[32] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20arthur%20szlam%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20arthur%20szlam%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015"
        },
        {
            "id": "33",
            "entry": "[33] Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "34",
            "entry": "[34] S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). ArXiv e-prints, May 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santurkar%2C%20S.%20Tsipras%2C%20D.%20Ilyas%2C%20A.%20Madry%2C%20A.%20How%20Does%20Batch%20Normalization%20Help%20Optimization%3F%20%28No%2C%20It%20Is%20Not%20About%20Internal%20Covariate%20Shift%29.%20ArXiv%20e-prints%202018-05"
        },
        {
            "id": "35",
            "entry": "[35] Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Oliver Wu. Tensor product generation networks. CoRR, abs/1709.09118, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.09118"
        },
        {
            "id": "36",
            "entry": "[36] Hamid Palangi, Paul Smolensky, Xiaodong He, and Li Deng. Deep learning of grammaticallyinterpretable representations through question-answering. CoRR, abs/1705.08432, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08432"
        },
        {
            "id": "37",
            "entry": "[37] Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, and Xiaodong He. Attentive tensor product learning for language generation and grammar parsing. arXiv preprint arXiv:1802.07089, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07089"
        },
        {
            "id": "38",
            "entry": "[38] D. O. Hebb. The Organization of Behavior. Wiley, New York, 1949.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hebb%2C%20D.O.%20The%20Organization%20of%20Behavior%201949"
        },
        {
            "id": "39",
            "entry": "[39] J. Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Technical Report FKI-147-91, Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen, March 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Learning%20to%20control%20fast-weight%20memories%3A%20An%20alternative%20to%20recurrent%20nets%201991-03"
        },
        {
            "id": "40",
            "entry": "[40] Thomas Miconi, Jeff Clune, and Kenneth O Stanley. Differentiable plasticity: training plastic neural networks with backpropagation. arXiv preprint arXiv:1804.02464, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02464"
        },
        {
            "id": "41",
            "entry": "[41] G\u00e1bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. CoRR, abs/1707.05589, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.05589"
        },
        {
            "id": "42",
            "entry": "[42] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. CoRR, abs/1410.3916, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.3916"
        },
        {
            "id": "43",
            "entry": "[43] Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. CoRR, abs/1506.07285, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.07285"
        },
        {
            "id": "44",
            "entry": "[44] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. Weakly supervised memory networks. CoRR, abs/1503.08895, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.08895"
        },
        {
            "id": "45",
            "entry": "[45] Julien Perez and Fei Liu. Gated end-to-end memory networks. CoRR, abs/1610.04211, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04211"
        },
        {
            "id": "46",
            "entry": "[46] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering. CoRR, abs/1603.01417, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.01417"
        },
        {
            "id": "47",
            "entry": "[47] Emmanuel Dupoux. Deconstructing ai-complete question-answering: going beyond toy tasks.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dupoux%2C%20Emmanuel%20Deconstructing%20ai-complete%20question-answering%3A%20going%20beyond%20toy%20tasks"
        },
        {
            "id": "48",
            "entry": "[48] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "49",
            "entry": "[49] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adri\u00c3 Puigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471\u2013476, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016"
        },
        {
            "id": "50",
            "entry": "[50] Jack W. Rae, Jonathan J. Hunt, Tim Harley, Ivo Danihelka, Andrew W. Senior, Greg Wayne, Alex Graves, and Timothy P. Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. CoRR, abs/1610.09027, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09027"
        },
        {
            "id": "51",
            "entry": "[51] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. In International Conference on Learning Representations (ICLR2017). Preprint arXiv:1612.03969.",
            "arxiv_url": "https://arxiv.org/pdf/1612.03969"
        },
        {
            "id": "52",
            "entry": "[52] S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f\u00fcr Informatik, Lehrstuhl Prof. Brauer, Technische Universit\u00e4t M\u00fcnchen, 1991. Advisor: J. Schmidhuber.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Untersuchungen%20zu%20dynamischen%20neuronalen%20Netzen.%20Diploma%20thesis%2C%20Institut%20f%C3%BCr%20Informatik%2C%20Lehrstuhl%20Prof%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Untersuchungen%20zu%20dynamischen%20neuronalen%20Netzen.%20Diploma%20thesis%2C%20Institut%20f%C3%BCr%20Informatik%2C%20Lehrstuhl%20Prof%201991"
        },
        {
            "id": "53",
            "entry": "[53] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May 2010. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010-05"
        },
        {
            "id": "54",
            "entry": "[54] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Largescale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Agarwal%2C%20Ashish%20Barham%2C%20Paul%20Brevdo%2C%20Eugene%20TensorFlow%3A%20Largescale%20machine%20learning%20on%20heterogeneous%20systems%202015"
        },
        {
            "id": "55",
            "entry": "[55] Timothy Dozat. Incorporating nestrov momentum into adam. In International Conference on Learning Representations (ICLR2016). CBLS, April 2016. OpenReview.net ID: OM0jvwB8jIp57ZJjtNEZ. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dozat%2C%20Timothy%20Incorporating%20nestrov%20momentum%20into%20adam.%20In%20International%20Conference%20on%20Learning%20Representations%20%28ICLR2016%29.%20CBLS%202016-04"
        }
    ]
}
