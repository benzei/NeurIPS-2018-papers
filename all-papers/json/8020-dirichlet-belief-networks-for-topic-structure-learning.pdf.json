{
    "filename": "8020-dirichlet-belief-networks-for-topic-structure-learning.pdf",
    "metadata": {
        "title": "Dirichlet belief networks for topic structure learning",
        "author": "He Zhao, Lan Du, Wray Buntine, Mingyuan Zhou",
        "date": 2003,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8020-dirichlet-belief-networks-for-topic-structure-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Recently, considerable research effort has been devoted to developing deep architectures for topic models to learn topic structures. Although several deep models have been proposed to learn better topic proportions of documents, how to leverage the benefits of deep structures for learning word distributions of topics has not yet been rigorously studied. Here we propose a new multi-layer generative process on word distributions of topics, where each layer consists of a set of topics and each topic is drawn from a mixture of the topics of the layer above. As the topics in all layers can be directly interpreted by words, the proposed model is able to discover interpretable topic hierarchies. As a self-contained module, our model can be flexibly adapted to different kinds of topic models to improve their modelling accuracy and interpretability. Extensive experiments on text corpora demonstrate the advantages of the proposed model."
    },
    "keywords": [
        {
            "term": "belief network",
            "url": "https://en.wikipedia.org/wiki/belief_network"
        },
        {
            "term": "NIPS",
            "url": "https://en.wikipedia.org/wiki/NIPS"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "topic model",
            "url": "https://en.wikipedia.org/wiki/topic_model"
        },
        {
            "term": "Chinese Restaurant Process",
            "url": "https://en.wikipedia.org/wiki/Chinese_Restaurant_Process"
        },
        {
            "term": "Pitman-Yor process",
            "url": "https://en.wikipedia.org/wiki/Pitman-Yor_process"
        },
        {
            "term": "chinese restaurant",
            "url": "https://en.wikipedia.org/wiki/chinese_restaurant"
        },
        {
            "term": "Latent Dirichlet Allocation",
            "url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_Allocation"
        },
        {
            "term": "factor analysis",
            "url": "https://en.wikipedia.org/wiki/factor_analysis"
        },
        {
            "term": "Indian Buffet Process",
            "url": "https://en.wikipedia.org/wiki/Indian_Buffet_Process"
        },
        {
            "term": "ICML",
            "url": "https://en.wikipedia.org/wiki/ICML"
        }
    ],
    "highlights": [
        "Understanding text has been an important task in machine learning, natural language processing, and data mining",
        "Most existing topic models are built on top of the following generative process: Each topic is a distribution over the words in the vocabulary; each document is associated with a topic proportion (TP) vector; and a word in a document is generated by first drawing a topic according to the document\u2019s topic proportion, sampling the word according to the topic\u2019s WD",
        "The contributions of this paper include: 1) We propose DirBN, a deep structure that can be used as an advanced alternative to the Dirichlet prior on WDs with better modelling performance and interpretability.\n2) We demonstrate our model\u2019s adaptability by applying DirBN with several well-developed models, including Poisson Factor Analysis (PFA) (<a class=\"ref-link\" id=\"cZhou_et+al_2012_a\" href=\"#rZhou_et+al_2012_a\">Zhou et al, 2012</a>), MetaLDA (<a class=\"ref-link\" id=\"cZhao_et+al_2017_a\" href=\"#rZhao_et+al_2017_a\">Zhao et al, 2017a</a>), and Gamma Belief Networks (<a class=\"ref-link\" id=\"cZhou_et+al_2016_a\" href=\"#rZhou_et+al_2016_a\">Zhou et al, 2016</a>).\n3) With proper data augmentation and marginalisation techniques, DirBN enjoys full local conjugacy, which facilitates the derivation of a simple and effective inference algorithm.\n2 The proposed DirBN",
        "With the framework of Poisson Factor Analysis, we compared three options of constructing \u03c6: (1) The default setting of Poisson Factor Analysis, where \u03c6 is drawn from a symmetric Dirichlet distribution with parameter 0.05, i.e., \u03c6k \u223c DirV (0.05); (2) Poisson Factor Analysis+Mallet, where \u03c6k \u223c DirV (\u03b10) and \u03b10 is sampled by Mallet 6; (3) Poisson Factor Analysis+DirBN, the proposed model, where \u03c6k is drawn from an asymmetric Dirichlet distribution specific to k, the parameter of which is constructed with the higher-layer topics",
        "We have presented DirBN, a multi-layer process generating word distributions of topics",
        "We have demonstrated DirBN\u2019s advantages by equipping Poisson Factor Analysis, MetaLDA, and Gamma Belief Networks, with DirBN"
    ],
    "key_statements": [
        "Understanding text has been an important task in machine learning, natural language processing, and data mining",
        "Most existing topic models are built on top of the following generative process: Each topic is a distribution over the words in the vocabulary; each document is associated with a topic proportion (TP) vector; and a word in a document is generated by first drawing a topic according to the document\u2019s topic proportion, sampling the word according to the topic\u2019s WD",
        "The contributions of this paper include: 1) We propose DirBN, a deep structure that can be used as an advanced alternative to the Dirichlet prior on WDs with better modelling performance and interpretability.\n2) We demonstrate our model\u2019s adaptability by applying DirBN with several well-developed models, including Poisson Factor Analysis (PFA) (<a class=\"ref-link\" id=\"cZhou_et+al_2012_a\" href=\"#rZhou_et+al_2012_a\">Zhou et al, 2012</a>), MetaLDA (<a class=\"ref-link\" id=\"cZhao_et+al_2017_a\" href=\"#rZhao_et+al_2017_a\">Zhao et al, 2017a</a>), and Gamma Belief Networks (<a class=\"ref-link\" id=\"cZhou_et+al_2016_a\" href=\"#rZhou_et+al_2016_a\">Zhou et al, 2016</a>).\n3) With proper data augmentation and marginalisation techniques, DirBN enjoys full local conjugacy, which facilitates the derivation of a simple and effective inference algorithm.\n2 The proposed DirBN",
        "With the framework of Poisson Factor Analysis, we compared three options of constructing \u03c6: (1) The default setting of Poisson Factor Analysis, where \u03c6 is drawn from a symmetric Dirichlet distribution with parameter 0.05, i.e., \u03c6k \u223c DirV (0.05); (2) Poisson Factor Analysis+Mallet, where \u03c6k \u223c DirV (\u03b10) and \u03b10 is sampled by Mallet 6; (3) Poisson Factor Analysis+DirBN, the proposed model, where \u03c6k is drawn from an asymmetric Dirichlet distribution specific to k, the parameter of which is constructed with the higher-layer topics",
        "We have presented DirBN, a multi-layer process generating word distributions of topics",
        "DirBN can be adapted to other advanced topic models and improve the performance and interpretability, especially on sparse texts",
        "We have demonstrated DirBN\u2019s advantages by equipping Poisson Factor Analysis, MetaLDA, and Gamma Belief Networks, with DirBN"
    ],
    "summary": [
        "Understanding text has been an important task in machine learning, natural language processing, and data mining.",
        "The contributions of this paper include: 1) We propose DirBN, a deep structure that can be used as an advanced alternative to the Dirichlet prior on WDs with better modelling performance and interpretability.",
        "The generative process of a topic model equipped with DirBN is demonstrated in Figure 1.",
        "3. In DirBN, not only in the bottom layer, but in any other layer t, each hidden unit is a distribution over the vocabulary and can be viewed as real topic directly interpreted by words.",
        "We adapt the proposed DirBN structure to the following models: PFA+DirBN Poisson Factor Analysis (PFA) is a popular framework for topic analysis (DPFA (<a class=\"ref-link\" id=\"cGan_et+al_2015_a\" href=\"#rGan_et+al_2015_a\">Gan et al, 2015</a>), DPFM (<a class=\"ref-link\" id=\"cHenao_et+al_2015_a\" href=\"#rHenao_et+al_2015_a\">Henao et al, 2015</a>), GBN (<a class=\"ref-link\" id=\"cZhou_et+al_2016_a\" href=\"#rZhou_et+al_2016_a\">Zhou et al, 2016</a>) can be viewed as a deep extension to PFA).",
        "GBN+DirBN Recall that GBN (Zhou et al, 2015, 2016) imposes a hierarchical structure on \u03b8, which is able to learn multi-layer document representations and topic hierarchies.",
        "With the framework of PFA, we compared three options of constructing \u03c6: (1) The default setting of PFA, where \u03c6 is drawn from a symmetric Dirichlet distribution with parameter 0.05, i.e., \u03c6k \u223c DirV (0.05); (2) PFA+Mallet, where \u03c6k \u223c DirV (\u03b10) and \u03b10 is sampled by Mallet 6; (3) PFA+DirBN, the proposed model, where \u03c6k is drawn from an asymmetric Dirichlet distribution specific to k, the parameter of which is constructed with the higher-layer topics.",
        "We have the following remarks on the results: (1) In general, for the models with DirBN, the performance is significantly improved compared with the counterparts without DirBN, especially in terms of perplexity and topic coherence and with low proportion of the training words.",
        "(4) The dual deep model (GBN+DirBN-3) usually performs the best on topic coherence, which demonstrates the benefits of the deep structures.",
        "DirBN is able to discover layer-wise semantically meaningful topic correlations with fewer overlapping top words.",
        "Figure 4 shows the sample linkages between topic hierarchies and labels on TMN, where the documents are labelled with 7 categories: 1 sport, 2 business, 3 us, 4 entertainment, 5 world, 6 health, 7 sci-tech.",
        "We have presented DirBN, a multi-layer process generating word distributions of topics.",
        "DirBN can be adapted to other advanced topic models and improve the performance and interpretability, especially on sparse texts.",
        "With the help of data augmentation, the inference of DirBN can be done by a layer-wise Gibbs sampling, as a full conjugate model"
    ],
    "headline": "We propose a new multi-layer generative process on word distributions of topics, where each layer consists of a set of topics and each topic is drawn from a mixture of the topics of the layer above",
    "reference_links": [
        {
            "id": "Blei_et+al_2003_a",
            "entry": "D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent Dirichlet allocation,\u201d JMLR, vol. 3, pp. 993\u20131022, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20D.M.%20Ng%2C%20A.Y.%20Jordan%2C%20M.I.%20Latent%20Dirichlet%20allocation%2C%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20D.M.%20Ng%2C%20A.Y.%20Jordan%2C%20M.I.%20Latent%20Dirichlet%20allocation%2C%202003"
        },
        {
            "id": "Blei_et+al_2010_a",
            "entry": "D. M. Blei, T. L. Griffiths, and M. I. Jordan, \u201cThe nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies,\u201d Journal of the ACM (JACM), vol. 57, no. 2, p. 7, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20D.M.%20Griffiths%2C%20T.L.%20Jordan%2C%20M.I.%20The%20nested%20Chinese%20restaurant%20process%20and%20Bayesian%20nonparametric%20inference%20of%20topic%20hierarchies%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20D.M.%20Griffiths%2C%20T.L.%20Jordan%2C%20M.I.%20The%20nested%20Chinese%20restaurant%20process%20and%20Bayesian%20nonparametric%20inference%20of%20topic%20hierarchies%2C%202010"
        },
        {
            "id": "Paisley_et+al_2015_a",
            "entry": "J. Paisley, C. Wang, D. M. Blei, and M. I. Jordan, \u201cNested hierarchical Dirichlet processes,\u201d TPAMI, vol. 37, no. 2, pp. 256\u2013270, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paisley%2C%20J.%20Wang%2C%20C.%20Blei%2C%20D.M.%20Jordan%2C%20M.I.%20Nested%20hierarchical%20Dirichlet%20processes%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paisley%2C%20J.%20Wang%2C%20C.%20Blei%2C%20D.M.%20Jordan%2C%20M.I.%20Nested%20hierarchical%20Dirichlet%20processes%2C%202015"
        },
        {
            "id": "Hinton_2009_a",
            "entry": "G. E. Hinton and R. R. Salakhutdinov, \u201cReplicated softmax: An undirected topic model,\u201d in NIPS, 2009, pp. 1607\u20131614.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Replicated%20softmax%3A%20An%20undirected%20topic%20model%2C%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Salakhutdinov%2C%20R.R.%20Replicated%20softmax%3A%20An%20undirected%20topic%20model%2C%202009"
        },
        {
            "id": "Larochelle_2012_a",
            "entry": "H. Larochelle and S. Lauly, \u201cA neural autoregressive topic model,\u201d in NIPS, 2012, pp. 2708\u20132716.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larochelle%2C%20H.%20Lauly%2C%20S.%20%E2%80%9CA%20neural%20autoregressive%20topic%20model%2C%E2%80%9D%20in%20NIPS%202012"
        },
        {
            "id": "Srivastava_et+al_2013_a",
            "entry": "N. Srivastava, R. Salakhutdinov, and G. Hinton, \u201cModeling documents with a deep Boltzmann machine,\u201d in UAI, 2013, pp. 616\u2013624.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Salakhutdinov%2C%20R.%20Hinton%2C%20G.%20%E2%80%9CModeling%20documents%20with%20a%20deep%20Boltzmann%20machine%2C%E2%80%9D%20in%20UAI%202013"
        },
        {
            "id": "Srivastava_2017_a",
            "entry": "A. Srivastava and C. Sutton, \u201cAutoencoding variational inference for topic models,\u201d 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20A.%20Sutton%2C%20C.%20Autoencoding%20variational%20inference%20for%20topic%20models%2C%202017"
        },
        {
            "id": "Miao_et+al_2017_a",
            "entry": "Y. Miao, E. Grefenstette, and P. Blunsom, \u201cDiscovering discrete latent topics with neural variational inference,\u201d in ICML, 2017, pp. 2410\u20132419.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miao%2C%20Y.%20Grefenstette%2C%20E.%20Blunsom%2C%20P.%20%E2%80%9CDiscovering%20discrete%20latent%20topics%20with%20neural%20variational%20inference%2C%E2%80%9D%20in%20ICML%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "H. Zhang, B. Chen, D. Guo, and M. Zhou, \u201cWHAI: Weibull hybrid autoencoding inference for deep topic modeling,\u201d 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Chen%2C%20B.%20Guo%2C%20D.%20Zhou%2C%20M.%20WHAI%3A%20Weibull%20hybrid%20autoencoding%20inference%20for%20deep%20topic%20modeling%2C%202018"
        },
        {
            "id": "Hinton_et+al_2006_a",
            "entry": "G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Osindero%2C%20S.%20Teh%2C%20Y.-W.%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%2C%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Osindero%2C%20S.%20Teh%2C%20Y.-W.%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%2C%202006"
        },
        {
            "id": "Gan_et+al_2015_a",
            "entry": "Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin, \u201cScalable deep Poisson factor analysis for topic modeling,\u201d in ICML, 2015, pp. 1823\u20131832.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Z.%20Chen%2C%20C.%20Henao%2C%20R.%20Carlson%2C%20D.%20%E2%80%9CScalable%20deep%20Poisson%20factor%20analysis%20for%20topic%20modeling%2C%E2%80%9D%20in%20ICML%202015"
        },
        {
            "id": "Ranganath_et+al_2015_a",
            "entry": "R. Ranganath, L. Tang, L. Charlin, and D. Blei, \u201cDeep exponential families,\u201d in AISTATS, 2015, pp. 762\u2013771.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tang%2C%20L.%20Charlin%2C%20L.%20Blei%2C%20D.%20%E2%80%9CDeep%20exponential%20families%2C%E2%80%9D%20in%20AISTATS%202015"
        },
        {
            "id": "Henao_et+al_2015_a",
            "entry": "R. Henao, Z. Gan, J. Lu, and L. Carin, \u201cDeep Poisson factor modeling,\u201d in NIPS, 2015, pp. 2800\u20132808.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henao%2C%20R.%20Gan%2C%20Z.%20Lu%2C%20J.%20Carin%2C%20L.%20%E2%80%9CDeep%20Poisson%20factor%20modeling%2C%E2%80%9D%20in%20NIPS%202015"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "M. Zhou, Y. Cong, and B. Chen, \u201cAugmentable gamma belief networks,\u201d JMLR, vol. 17, no. 163, pp. 1\u201344, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20Augmentable%20gamma%20belief%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20Augmentable%20gamma%20belief%20networks%2C%202016"
        },
        {
            "id": "Mcauliffe_2008_a",
            "entry": "J. D. Mcauliffe and D. M. Blei, \u201cSupervised topic models,\u201d in NIPS, 2008, pp. 121\u2013128.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mcauliffe%2C%20J.D.%20Blei%2C%20D.M.%20%E2%80%9CSupervised%20topic%20models%2C%E2%80%9D%20in%20NIPS%202008"
        },
        {
            "id": "Rosen-Zvi_et+al_2004_a",
            "entry": "M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth, \u201cThe author-topic model for authors and documents,\u201d in UAI, 2004, pp. 487\u2013494.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosen-Zvi%2C%20M.%20Griffiths%2C%20T.%20Steyvers%2C%20M.%20Smyth%2C%20P.%20%E2%80%9CThe%20author-topic%20model%20for%20authors%20and%20documents%2C%E2%80%9D%20in%20UAI%202004"
        },
        {
            "id": "Zhou_et+al_2012_a",
            "entry": "M. Zhou, L. Hannah, D. B. Dunson, and L. Carin, \u201cBeta-negative binomial process and Poisson factor analysis,\u201d in AISTATS, 2012, pp. 1462\u20131471.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Hannah%2C%20L.%20Dunson%2C%20D.B.%20Carin%2C%20L.%20%E2%80%9CBeta-negative%20binomial%20process%20and%20Poisson%20factor%20analysis%2C%E2%80%9D%20in%20AISTATS%202012"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "H. Zhao, L. Du, W. Buntine, and G. Liu, \u201cMetalda: A topic model that efficiently incorporates meta information,\u201d in ICDM, 2017, pp. 635\u2013644.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20H.%20Du%2C%20L.%20Buntine%2C%20W.%20Liu%2C%20G.%20%E2%80%9CMetalda%3A%20A%20topic%20model%20that%20efficiently%20incorporates%20meta%20information%2C%E2%80%9D%20in%20ICDM%202017"
        },
        {
            "id": "Lafferty_2006_a",
            "entry": "J. D. Lafferty and D. M. Blei, \u201cCorrelated topic models,\u201d in NIPS, 2006, pp. 147\u2013154.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lafferty%2C%20J.D.%20Blei%2C%20D.M.%20%E2%80%9CCorrelated%20topic%20models%2C%E2%80%9D%20in%20NIPS%202006"
        },
        {
            "id": "Teh_et+al_2012_a",
            "entry": "Y. Teh, M. Jordan, M. Beal, and D. Blei, \u201cHierarchical Dirichlet processes,\u201d Journal of the American Statistical Association, vol. 101, no. 476, pp. 1566\u20131581, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Y.%20Jordan%2C%20M.%20Beal%2C%20M.%20Blei%2C%20D.%20Hierarchical%20Dirichlet%20processes%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Y.%20Jordan%2C%20M.%20Beal%2C%20M.%20Blei%2C%20D.%20Hierarchical%20Dirichlet%20processes%2C%202012"
        },
        {
            "id": "Tang_2013_a",
            "entry": "Y. Tang and R. R. Salakhutdinov, \u201cLearning stochastic feedforward neural networks,\u201d in NIPS, 2013, pp. 530\u2013538.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Y.%20Salakhutdinov%2C%20R.R.%20Learning%20stochastic%20feedforward%20neural%20networks%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Y.%20Salakhutdinov%2C%20R.R.%20Learning%20stochastic%20feedforward%20neural%20networks%2C%202013"
        },
        {
            "id": "Zhou_2015_a",
            "entry": "M. Zhou, \u201cInfinite edge partition models for overlapping community detection and link prediction,\u201d in AISTATS, 2015, pp. 1135\u2014-1143.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20%E2%80%9CInfinite%20edge%20partition%20models%20for%20overlapping%20community%20detection%20and%20link%20prediction%2C%E2%80%9D%20in%20AISTATS%202015"
        },
        {
            "id": "Zhou_2015_b",
            "entry": "M. Zhou and L. Carin, \u201cNegative binomial process count and mixture modeling,\u201d TPAMI, vol. 37, no. 2, pp. 307\u2013320, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Carin%2C%20L.%20Negative%20binomial%20process%20count%20and%20mixture%20modeling%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Carin%2C%20L.%20Negative%20binomial%20process%20count%20and%20mixture%20modeling%2C%202015"
        },
        {
            "id": "Zhao_et+al_2017_b",
            "entry": "H. Zhao, L. Du, and W. Buntine, \u201cLeveraging node attributes for incomplete relational data,\u201d in ICML, 2017, pp. 4072\u20134081.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20H.%20Du%2C%20L.%20Buntine%2C%20W.%20%E2%80%9CLeveraging%20node%20attributes%20for%20incomplete%20relational%20data%2C%E2%80%9D%20in%20ICML%202017"
        },
        {
            "id": "Zhou_2018_a",
            "entry": "M. Zhou, \u201cNonparametric Bayesian negative binomial factor analysis,\u201d Bayesian Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Nonparametric%20Bayesian%20negative%20binomial%20factor%20analysis%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Nonparametric%20Bayesian%20negative%20binomial%20factor%20analysis%2C%202018"
        },
        {
            "id": "Zhao_et+al_2018_a",
            "entry": "H. Zhao, L. Du, W. Buntine, and G. Liu, \u201cLeveraging external information in topic modelling,\u201d KAIS, pp. 1\u201333, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20H.%20Du%2C%20L.%20Buntine%2C%20W.%20Liu%2C%20G.%20Leveraging%20external%20information%20in%20topic%20modelling%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20H.%20Du%2C%20L.%20Buntine%2C%20W.%20Liu%2C%20G.%20Leveraging%20external%20information%20in%20topic%20modelling%2C%202018"
        },
        {
            "id": "Zhou_et+al_2015_c",
            "entry": "M. Zhou, Y. Cong, and B. Chen, \u201cThe Poisson gamma belief network,\u201d in NIPS, 2015, pp. 3043\u20133051.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20%E2%80%9CThe%20Poisson%20gamma%20belief%20network%2C%E2%80%9D%20in%20NIPS%202015"
        },
        {
            "id": "Wallach_et+al_2009_a",
            "entry": "H. M. Wallach, D. M. Mimno, and A. McCallum, \u201cRethinking LDA: Why priors matter,\u201d in NIPS, 2009, pp. 1973\u20131981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wallach%2C%20H.M.%20Mimno%2C%20D.M.%20McCallum%2C%20A.%20Rethinking%20LDA%3A%20Why%20priors%20matter%2C%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wallach%2C%20H.M.%20Mimno%2C%20D.M.%20McCallum%2C%20A.%20Rethinking%20LDA%3A%20Why%20priors%20matter%2C%202009"
        },
        {
            "id": "Zhao_et+al_2017_c",
            "entry": "H. Zhao, L. Du, and W. Buntine, \u201cA word embeddings informed focused topic model,\u201d in ACML, 2017, pp. 423\u2013438.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20H.%20Du%2C%20L.%20Buntine%2C%20W.%20%E2%80%9CA%20word%20embeddings%20informed%20focused%20topic%20model%2C%E2%80%9D%20in%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20H.%20Du%2C%20L.%20Buntine%2C%20W.%20%E2%80%9CA%20word%20embeddings%20informed%20focused%20topic%20model%2C%E2%80%9D%20in%202017"
        },
        {
            "id": "Zhao_et+al_2018_b",
            "entry": "H. Zhao, L. Du, W. Buntine, and M. Zhou, \u201cInter and intra topic structure learning with word embeddings,\u201d in ICML, 2018, pp. 5887\u20135896.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20H.%20Du%2C%20L.%20Buntine%2C%20W.%20Zhou%2C%20M.%20%E2%80%9CInter%20and%20intra%20topic%20structure%20learning%20with%20word%20embeddings%2C%E2%80%9D%20in%20ICML%202018"
        },
        {
            "id": "Sato_2010_a",
            "entry": "I. Sato and H. Nakagawa, \u201cTopic models with power-law using Pitman-Yor process,\u201d in SIGKDD, 2010, pp. 673\u2013682.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sato%2C%20I.%20Nakagawa%2C%20H.%20Topic%20models%20with%20power-law%20using%20Pitman-Yor%20process%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sato%2C%20I.%20Nakagawa%2C%20H.%20Topic%20models%20with%20power-law%20using%20Pitman-Yor%20process%2C%202010"
        },
        {
            "id": "Buntine_2014_a",
            "entry": "W. L. Buntine and S. Mishra, \u201cExperiments with non-parametric topic models,\u201d in SIGKDD, 2014, pp. 881\u2013890.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buntine%2C%20W.L.%20Mishra%2C%20S.%20Experiments%20with%20non-parametric%20topic%20models%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buntine%2C%20W.L.%20Mishra%2C%20S.%20Experiments%20with%20non-parametric%20topic%20models%2C%202014"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "C. Chen, W. Buntine, N. Ding, L. Xie, and L. Du, \u201cDifferential topic models,\u201d TPAMI, vol. 37, no. 2, pp. 230\u2013242, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20C.%20Buntine%2C%20W.%20Ding%2C%20N.%20Xie%2C%20L.%20Differential%20topic%20models%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20C.%20Buntine%2C%20W.%20Ding%2C%20N.%20Xie%2C%20L.%20Differential%20topic%20models%2C%202015"
        },
        {
            "id": "Lindsey_et+al_2012_a",
            "entry": "R. V. Lindsey, W. P. Headden III, and M. J. Stipicevic, \u201cA phrase-discovering topic model using hierarchical Pitman-Yor processes,\u201d in EMNLP, 2012, pp. 214\u2013222.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lindsey%2C%20R.V.%20Headden%2C%20III%2C%20W.P.%20Stipicevic%2C%20M.J.%20%E2%80%9CA%20phrase-discovering%20topic%20model%20using%20hierarchical%20Pitman-Yor%20processes%2C%E2%80%9D%20in%20EMNLP%202012"
        },
        {
            "id": "Archambeau_et+al_2015_a",
            "entry": "C. Archambeau, B. Lakshminarayanan, and G. Bouchard, \u201cLatent IBP compound Dirichlet allocation,\u201d TPAMI, vol. 37, no. 2, pp. 321\u2013333, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Archambeau%2C%20C.%20Lakshminarayanan%2C%20B.%20Bouchard%2C%20G.%20Latent%20IBP%20compound%20Dirichlet%20allocation%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Archambeau%2C%20C.%20Lakshminarayanan%2C%20B.%20Bouchard%2C%20G.%20Latent%20IBP%20compound%20Dirichlet%20allocation%2C%202015"
        },
        {
            "id": "Wood_2009_a",
            "entry": "F. Wood and Y. W. Teh, \u201cA hierarchical nonparametric Bayesian approach to statistical language model domain adaptation,\u201d in AISTATS, 2009, pp. 607\u2013614.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wood%2C%20F.%20Teh%2C%20Y.W.%20%E2%80%9CA%20hierarchical%20nonparametric%20Bayesian%20approach%20to%20statistical%20language%20model%20domain%20adaptation%2C%E2%80%9D%20in%20AISTATS%202009"
        },
        {
            "id": "Du_et+al_2012_a",
            "entry": "L. Du, W. Buntine, and H. Jin, \u201cModelling sequential text with an adaptive topic model,\u201d in EMNLP, 2012, pp. 535\u2013545.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20L.%20Buntine%2C%20W.%20Jin%2C%20H.%20Modelling%20sequential%20text%20with%20an%20adaptive%20topic%20model%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20L.%20Buntine%2C%20W.%20Jin%2C%20H.%20Modelling%20sequential%20text%20with%20an%20adaptive%20topic%20model%2C%202012"
        },
        {
            "id": "Kim_et+al_2012_a",
            "entry": "J. H. Kim, D. Kim, S. Kim, and A. Oh, \u201cModeling topic hierarchies with the recursive chinese restaurant process,\u201d in CIKM, 2012, pp. 783\u2013792.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20J.H.%20Kim%2C%20D.%20Kim%2C%20S.%20Oh%2C%20A.%20%E2%80%9CModeling%20topic%20hierarchies%20with%20the%20recursive%20chinese%20restaurant%20process%2C%E2%80%9D%20in%20CIKM%202012"
        },
        {
            "id": "Ahmed_et+al_2013_a",
            "entry": "A. Ahmed, L. Hong, and A. Smola, \u201cNested Chinese restaurant franchise process: Applications to user tracking and document modeling,\u201d in ICML, 2013, pp. 1426\u20131434.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahmed%2C%20A.%20Hong%2C%20L.%20Smola%2C%20A.%20Nested%20Chinese%20restaurant%20franchise%20process%3A%20Applications%20to%20user%20tracking%20and%20document%20modeling%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahmed%2C%20A.%20Hong%2C%20L.%20Smola%2C%20A.%20Nested%20Chinese%20restaurant%20franchise%20process%3A%20Applications%20to%20user%20tracking%20and%20document%20modeling%2C%202013"
        },
        {
            "id": "Li_2006_a",
            "entry": "W. Li and A. McCallum, \u201cPachinko allocation: DAG-structured mixture models of topic correlations,\u201d in ICML, 2006, pp. 577\u2013584.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20W.%20McCallum%2C%20A.%20%E2%80%9CPachinko%20allocation%3A%20DAG-structured%20mixture%20models%20of%20topic%20correlations%2C%E2%80%9D%20in%20ICML%202006"
        },
        {
            "id": "Cong_et+al_2017_a",
            "entry": "Y. Cong, B. Chen, H. Liu, and M. Zhou, \u201cDeep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC,\u201d in ICML, 2017, pp. 864\u2013873.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cong%2C%20Y.%20Chen%2C%20B.%20Liu%2C%20H.%20Zhou%2C%20M.%20%E2%80%9CDeep%20latent%20Dirichlet%20allocation%20with%20topic-layer-adaptive%20stochastic%20gradient%20Riemannian%20MCMC%2C%E2%80%9D%20in%20ICML%202017"
        },
        {
            "id": "Aletras_2013_a",
            "entry": "N. Aletras and M. Stevenson, \u201cEvaluating topic coherence using distributional semantics,\u201d in Proc. of the 10th Intnl. Conf. on Computational Semantics, 2013, pp. 13\u201322.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aletras%2C%20N.%20Stevenson%2C%20M.%20%E2%80%9CEvaluating%20topic%20coherence%20using%20distributional%20semantics%2C%E2%80%9D%20in%20Proc.%20of%20the%2010th%20Intnl%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aletras%2C%20N.%20Stevenson%2C%20M.%20%E2%80%9CEvaluating%20topic%20coherence%20using%20distributional%20semantics%2C%E2%80%9D%20in%20Proc.%20of%20the%2010th%20Intnl%202013"
        },
        {
            "id": "Lau_et+al_2014_a",
            "entry": "J. H. Lau, D. Newman, and T. Baldwin, \u201cMachine reading tea leaves: Automatically evaluating topic coherence and topic model quality,\u201d in EACL, 2014, pp. 530\u2013539.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lau%2C%20J.H.%20Newman%2C%20D.%20Baldwin%2C%20T.%20Machine%20reading%20tea%20leaves%3A%20Automatically%20evaluating%20topic%20coherence%20and%20topic%20model%20quality%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lau%2C%20J.H.%20Newman%2C%20D.%20Baldwin%2C%20T.%20Machine%20reading%20tea%20leaves%3A%20Automatically%20evaluating%20topic%20coherence%20and%20topic%20model%20quality%2C%202014"
        },
        {
            "id": "Yang_et+al_2015_a",
            "entry": "Y. Yang, D. Downey, and J. Boyd-Graber, \u201cEfficient methods for incorporating knowledge into topic models,\u201d in EMNLP, 2015, pp. 308\u2013317.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Y.%20Downey%2C%20D.%20Boyd-Graber%2C%20J.%20%E2%80%9CEfficient%20methods%20for%20incorporating%20knowledge%20into%20topic%20models%2C%E2%80%9D%20in%20EMNLP%202015"
        },
        {
            "id": "Hoffman_et+al_2013_a",
            "entry": "M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley, \u201cStochastic variational inference,\u201d JMLR, vol. 14, no. 1, pp. 1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%2C%202013"
        },
        {
            "id": "Guhaniyogi_et+al_2018_a",
            "entry": "R. Guhaniyogi, S. Qamar, and D. B. Dunson, \u201cBayesian conditional density filtering,\u201d Journal of Computational and Graphical Statistics, no. just-accepted, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guhaniyogi%2C%20R.%20Qamar%2C%20S.%20Dunson%2C%20D.B.%20Bayesian%20conditional%20density%20filtering%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guhaniyogi%2C%20R.%20Qamar%2C%20S.%20Dunson%2C%20D.B.%20Bayesian%20conditional%20density%20filtering%2C%202018"
        },
        {
            "id": "Chen_et+al_2014_a",
            "entry": "T. Chen, E. Fox, and C. Guestrin, \u201cStochastic gradient Hamiltonian Monte Carlo,\u201d in ICML, 2014, pp. 1683\u20131691.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%2C%202014"
        },
        {
            "id": "Ding_et+al_2014_a",
            "entry": "N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven, \u201cBayesian sampling using stochastic gradient thermostats,\u201d in NIPS, 2014, pp. 3203\u20133211.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20N.%20Fang%2C%20Y.%20Babbush%2C%20R.%20Chen%2C%20C.%20Bayesian%20sampling%20using%20stochastic%20gradient%20thermostats%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20N.%20Fang%2C%20Y.%20Babbush%2C%20R.%20Chen%2C%20C.%20Bayesian%20sampling%20using%20stochastic%20gradient%20thermostats%2C%202014"
        },
        {
            "id": "Welling_2011_a",
            "entry": "M. Welling and Y. W. Teh, \u201cBayesian learning via stochastic gradient Langevin dynamics,\u201d in ICML, 2011, pp. 681\u2013688.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20M.%20W%2C%20Y.%20Teh%2C%20%E2%80%9CBayesian%20learning%20via%20stochastic%20gradient%20Langevin%20dynamics%2C%E2%80%9D%20in%20ICML%202011"
        }
    ]
}
