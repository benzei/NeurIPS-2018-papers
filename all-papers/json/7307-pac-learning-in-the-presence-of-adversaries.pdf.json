{
    "filename": "7307-pac-learning-in-the-presence-of-adversaries.pdf",
    "metadata": {
        "title": "PAC-learning in the presence of adversaries",
        "author": "Daniel Cullina, Arjun Nitin Bhagoji, Prateek Mittal",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7307-pac-learning-in-the-presence-of-adversaries.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The existence of evasion attacks during the test phase of machine learning algorithms represents a significant challenge to both their deployment and understanding. These attacks can be carried out by adding imperceptible perturbations to inputs to generate adversarial examples and finding effective defenses and detectors has proven to be difficult. In this paper, we step away from the attack-defense arms race and seek to understand the limits of what can be learned in the presence of an evasion adversary. In particular, we extend the Probably Approximately Correct (PAC)-learning framework to account for the presence of an adversary. We first define corrupted hypothesis classes which arise from standard binary hypothesis classes in the presence of an evasion adversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted as the adversarial VC-dimension. We then show that sample complexity upper bounds from the Fundamental Theorem of Statistical learning can be extended to the case of evasion adversaries, where the sample complexity is controlled by the adversarial VC-dimension. We then explicitly derive the adversarial VC-dimension for halfspace classifiers in the presence of a sample-wise norm-constrained adversary of the type commonly studied for evasion attacks and show that it is the same as the standard VC-dimension, closing an open question. Finally, we prove that the adversarial VC-dimension can be either larger or smaller than the standard VC-dimension depending on the hypothesis class and adversary, making it an interesting object of study in its own right."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        }
    ],
    "highlights": [
        "Machine learning (ML) has become ubiquitous due to its impressive performance in domains as varied as image recognition [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>, <a class=\"ref-link\" id=\"c71\" href=\"#r71\">71</a>], natural language and speech processing [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>], gameplaying [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>, <a class=\"ref-link\" id=\"c70\" href=\"#r70\">70</a>] and aircraft collision avoidance [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>]",
        "Our focus in this paper is on evasion attacks targeting the test phase, particularly those based on adversarial examples which add imperceptible perturbations to the input in order to cause misclassification",
        "We close this open question by showing that the sample complexity of Probably Approximately Correct-learning when the hypothesis class is the set of halfspace classifers does not increase in the presence of adversaries bounded by convex constraint sets",
        "We prove that this does not increase in the presence of an adversary, i.e., the adversarial VC-dimension is equal to the VC-dimension for the hypothesis class comprising all halfspace classifiers",
        "A hypothesis class H is learnable by empirical risk minimization in the presence of an evasion adversary constrained by R if there is a function mH,R : (0, 1)2 \u2192 N with the following property",
        "A number of heuristics are used to enable the efficient solution of this problem, such as replacing the 0-1 loss with smooth surrogates like the logistic loss and approximating the inner maximum by a Projected Gradient Descent (PGD)-based adversary [<a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>] or by an upper bound [<a class=\"ref-link\" id=\"c64\" href=\"#r64\">64</a>]"
    ],
    "key_statements": [
        "Machine learning (ML) has become ubiquitous due to its impressive performance in domains as varied as image recognition [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>, <a class=\"ref-link\" id=\"c71\" href=\"#r71\">71</a>], natural language and speech processing [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>], gameplaying [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>, <a class=\"ref-link\" id=\"c70\" href=\"#r70\">70</a>] and aircraft collision avoidance [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>]",
        "Our focus in this paper is on evasion attacks targeting the test phase, particularly those based on adversarial examples which add imperceptible perturbations to the input in order to cause misclassification",
        "A large number of adversarial example-based evasion attacks have been proposed against supervised Machine learning algorithms used for image classification [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c61\" href=\"#r61\">61</a>, <a class=\"ref-link\" id=\"c75\" href=\"#r75\">75</a>], object detection [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c52\" href=\"#r52\">52</a>, <a class=\"ref-link\" id=\"c80\" href=\"#r80\">80</a>], image segmentation [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>], speech recognition [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c83\" href=\"#r83\">83</a>] as well as other tasks [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c82\" href=\"#r82\">82</a>]; generative models for image data [<a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>] and even reinforcement learning algorithms [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>]",
        "We close this open question by showing that the sample complexity of Probably Approximately Correct-learning when the hypothesis class is the set of halfspace classifers does not increase in the presence of adversaries bounded by convex constraint sets",
        "We prove that this does not increase in the presence of an adversary, i.e., the adversarial VC-dimension is equal to the VC-dimension for the hypothesis class comprising all halfspace classifiers",
        "Contributions: In this paper, we are the first to provide sample complexity bounds for the problem of Probably Approximately Correct-learning in the presence of an evasion adversary",
        "We extend the agnostic Probably Approximately Correct-learning setting introduced by Haussler [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] to include an evasion adversary",
        "A hypothesis class H is learnable by empirical risk minimization in the presence of an evasion adversary constrained by R if there is a function mH,R : (0, 1)2 \u2192 N with the following property",
        "AVC(H, R) = sup{n \u2208 N : \u03c3 (\u03bb(H), n) = 2n}. These definitions and lemmas can be combined to obtain a sample complexity upper bound for Probably Approximately Correct-learning in the presence of an evasion adversary",
        "We are the first to demonstrate sample complexity bounds on Probably Approximately Correct-learning in the presence of an evasion adversary",
        "A number of heuristics are used to enable the efficient solution of this problem, such as replacing the 0-1 loss with smooth surrogates like the logistic loss and approximating the inner maximum by a Projected Gradient Descent (PGD)-based adversary [<a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>] or by an upper bound [<a class=\"ref-link\" id=\"c64\" href=\"#r64\">64</a>]"
    ],
    "summary": [
        "Machine learning (ML) has become ubiquitous due to its impressive performance in domains as varied as image recognition [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>, <a class=\"ref-link\" id=\"c71\" href=\"#r71\">71</a>], natural language and speech processing [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>], gameplaying [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>, <a class=\"ref-link\" id=\"c70\" href=\"#r70\">70</a>] and aircraft collision avoidance [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>].",
        "We close this open question by showing that the sample complexity of PAC-learning when the hypothesis class is the set of halfspace classifers does not increase in the presence of adversaries bounded by convex constraint sets.",
        "We explicitly compute the adversarial VC-dimension for the hypothesis class comprising all halfspace classifiers, which directly gives us the sample complexity of PAC-learning in the presence of adversaries.",
        "Contributions: In this paper, we are the first to provide sample complexity bounds for the problem of PAC-learning in the presence of an evasion adversary.",
        "We show that an analog of the VC-dimension which we term the adversarial VC-dimension allows us to establish learnability and upper bound sample complexity for the case of binary hypothesis classes with the 0-1 loss in the presence of evasion adversaries.",
        "A hypothesis class H is learnable by empirical risk minimization in the presence of an evasion adversary constrained by R if there is a function mH,R : (0, 1)2 \u2192 N with the following property.",
        "We compute the VC-dimension of these hypotheses, which we term the adversarial VC-dimension and use it to prove the sample complexity upper bounds learning in the presence of an evasion adversary.",
        "We begin by providing two equivalent definitions of a shattering coefficient, which we use to determine VC-dimension for standard binary hypothesis classes and adversarial VC-dimension for their corrupted counterparts.",
        "These definitions and lemmas can be combined to obtain a sample complexity upper bound for PAC-learning in the presence of an evasion adversary.",
        "Let H be a family of classfiers on X = Rd. For an example x \u2208 X and a classifier h \u2208 H, define the signed distance to the boundary to be \u03b4B(h, x, c) = c \u00b7 h(x) \u00b7 inf dB(x, y)",
        "If the examples are all labeled with \u22121, this subset of the corrupted classifier family achieves all 2d possible error patterns.",
        "We are the first to demonstrate sample complexity bounds on PAC-learning in the presence of an evasion adversary.",
        "Objective functions for robust classifiers: Raghunathan et al [<a class=\"ref-link\" id=\"c64\" href=\"#r64\">64</a>] and Kolter and Wong [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>] take similar approaches to setting up a solvable optimization problem that approximates the worst-case adversary in order to carry out adversarial training.",
        "While our results provide a useful theoretical understanding of the problem of learning with adversaries, the nature of the 0-1 loss prevents the efficient implementation of Adversarial ERM to obtain robust classifiers.",
        "Another natural step is to understand the behavior of convex learning problems in the presence of adversaries, in particular the Regularized Loss Minimization framework"
    ],
    "headline": "We extend the Probably Approximately Correct -learning framework to account for the presence of an adversary",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mahdieh Abbasi and Christian Gagne. Robustness to adversarial examples through an ensemble of specialists. arXiv preprint arXiv:1702.06856, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06856"
        },
        {
            "id": "2",
            "entry": "[2] Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343\u2013370, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angluin%2C%20Dana%20Laird%2C%20Philip%20Learning%20from%20noisy%20examples%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angluin%2C%20Dana%20Laird%2C%20Philip%20Learning%20from%20noisy%20examples%201988"
        },
        {
            "id": "3",
            "entry": "[3] Anurag Arnab, Ondrej Miksik, and Philip H. S. Torr. On the robustness of semantic segmentation models to adversarial attacks. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arnab%2C%20Anurag%20Miksik%2C%20Ondrej%20Torr%2C%20Philip%20H.S.%20On%20the%20robustness%20of%20semantic%20segmentation%20models%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arnab%2C%20Anurag%20Miksik%2C%20Ondrej%20Torr%2C%20Philip%20H.S.%20On%20the%20robustness%20of%20semantic%20segmentation%20models%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "4",
            "entry": "[4] Alexander Bagnall, Razvan Bunescu, and Gordon Stewart. Training ensembles to detect adversarial examples. arXiv preprint arXiv:1712.04006, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04006"
        },
        {
            "id": "5",
            "entry": "[5] Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Dimensionality reduction as a defense against evasion attacks on machine learning classifiers. arXiv preprint arXiv:1704.02654, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02654"
        },
        {
            "id": "6",
            "entry": "[6] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Black-box attacks on deep neural networks via gradient estimation. In ICLR Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhagoji%2C%20Arjun%20Nitin%20He%2C%20Warren%20Li%2C%20Bo%20Song%2C%20Dawn%20Black-box%20attacks%20on%20deep%20neural%20networks%20via%20gradient%20estimation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhagoji%2C%20Arjun%20Nitin%20He%2C%20Warren%20Li%2C%20Bo%20Song%2C%20Dawn%20Black-box%20attacks%20on%20deep%20neural%20networks%20via%20gradient%20estimation%202018"
        },
        {
            "id": "7",
            "entry": "[7] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 387\u2013402.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20Battista%20Corona%2C%20Igino%20Maiorca%2C%20Davide%20Nelson%2C%20Blaine%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20Battista%20Corona%2C%20Igino%20Maiorca%2C%20Davide%20Nelson%2C%20Blaine%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time"
        },
        {
            "id": "8",
            "entry": "[8] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1807\u20131814, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20Battista%20Nelson%2C%20Blaine%20Laskov%2C%20Pavel%20Poisoning%20attacks%20against%20support%20vector%20machines%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20Battista%20Nelson%2C%20Blaine%20Laskov%2C%20Pavel%20Poisoning%20attacks%20against%20support%20vector%20machines%202012"
        },
        {
            "id": "9",
            "entry": "[9] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. arXiv preprint arXiv:1712.03141, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03141"
        },
        {
            "id": "10",
            "entry": "[10] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brendel%2C%20Wieland%20Rauber%2C%20Jonas%20Bethge%2C%20Matthias%20Decision-based%20adversarial%20attacks%3A%20Reliable%20attacks%20against%20black-box%20machine%20learning%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brendel%2C%20Wieland%20Rauber%2C%20Jonas%20Bethge%2C%20Matthias%20Decision-based%20adversarial%20attacks%3A%20Reliable%20attacks%20against%20black-box%20machine%20learning%20models%202018"
        },
        {
            "id": "11",
            "entry": "[11] Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, page eaao1733, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20Noam%20Sandholm%2C%20Tuomas%20Superhuman%20ai%20for%20heads-up%20no-limit%20poker%3A%20Libratus%20beats%20top%20professionals%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brown%2C%20Noam%20Sandholm%2C%20Tuomas%20Superhuman%20ai%20for%20heads-up%20no-limit%20poker%3A%20Libratus%20beats%20top%20professionals%202017"
        },
        {
            "id": "12",
            "entry": "[12] Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04311"
        },
        {
            "id": "13",
            "entry": "[13] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In AISec, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017"
        },
        {
            "id": "14",
            "entry": "[14] Nicholas Carlini and David Wagner. Magnet and \u201cefficient defenses against adversarial attacks\u201d are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.08478"
        },
        {
            "id": "15",
            "entry": "[15] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In DLS (IEEE SP), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Audio%20adversarial%20examples%3A%20Targeted%20attacks%20on%20speech-to-text%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Audio%20adversarial%20examples%3A%20Targeted%20attacks%20on%20speech-to-text%202018"
        },
        {
            "id": "17",
            "entry": "[17] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. CoRR, abs/1608.04644, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04644"
        },
        {
            "id": "18",
            "entry": "[18] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks to deep neural networks via adversarial examples. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Pin-Yu%20Sharma%2C%20Yash%20Zhang%2C%20Huan%20Yi%2C%20Jinfeng%20Ead%3A%20elastic-net%20attacks%20to%20deep%20neural%20networks%20via%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Pin-Yu%20Sharma%2C%20Yash%20Zhang%2C%20Huan%20Yi%2C%20Jinfeng%20Ead%3A%20elastic-net%20attacks%20to%20deep%20neural%20networks%20via%20adversarial%20examples%202018"
        },
        {
            "id": "19",
            "entry": "[19] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 15\u201326. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017"
        },
        {
            "id": "20",
            "entry": "[20] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Chau. Robust physical adversarial attack on faster r-cnn object detector. arXiv preprint arXiv:1804.05810, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05810"
        },
        {
            "id": "21",
            "entry": "[21] Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Fooling deep structured prediction models. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20Moustapha%20Adi%2C%20Yossi%20Neverova%2C%20Natalia%20Keshet%2C%20Joseph%20Houdini%3A%20Fooling%20deep%20structured%20prediction%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20Moustapha%20Adi%2C%20Yossi%20Neverova%2C%20Natalia%20Keshet%2C%20Joseph%20Houdini%3A%20Fooling%20deep%20structured%20prediction%20models%202017"
        },
        {
            "id": "22",
            "entry": "[22] Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493\u2013 2537, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collobert%2C%20Ronan%20Weston%2C%20Jason%20Bottou%2C%20Leon%20Karlen%2C%20Michael%20Natural%20language%20processing%20%28almost%29%20from%20scratch%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collobert%2C%20Ronan%20Weston%2C%20Jason%20Bottou%2C%20Leon%20Karlen%2C%20Michael%20Natural%20language%20processing%20%28almost%29%20from%20scratch%202011"
        },
        {
            "id": "23",
            "entry": "[23] Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Siwei Li, Li Chen, Michael E Kounavis, and Duen Horng Chau. Shield: Fast, practical defense and vaccination for deep learning using jpeg compression. arXiv preprint arXiv:1802.06816, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06816"
        },
        {
            "id": "24",
            "entry": "[24] Li Deng, Geoffrey Hinton, and Brian Kingsbury. New types of deep neural network learning for speech recognition and related applications: An overview. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8599\u20138603. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Li%20Hinton%2C%20Geoffrey%20Kingsbury%2C%20Brian%20New%20types%20of%20deep%20neural%20network%20learning%20for%20speech%20recognition%20and%20related%20applications%3A%20An%20overview%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Li%20Hinton%2C%20Geoffrey%20Kingsbury%2C%20Brian%20New%20types%20of%20deep%20neural%20network%20learning%20for%20speech%20recognition%20and%20related%20applications%3A%20An%20overview%202013"
        },
        {
            "id": "25",
            "entry": "[25] R Dudley. Sizes of compact subsets of hilbert space and continuity of gaussian processes. J. Funct. Anal., 1:290\u2013330, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dudley%2C%20R.%20Sizes%20of%20compact%20subsets%20of%20hilbert%20space%20and%20continuity%20of%20gaussian%20processes%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dudley%2C%20R.%20Sizes%20of%20compact%20subsets%20of%20hilbert%20space%20and%20continuity%20of%20gaussian%20processes%201967"
        },
        {
            "id": "26",
            "entry": "[26] Gintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M Roy. A study of the effect of JPG compression on adversarial images. arXiv preprint arXiv:1608.00853, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.00853"
        },
        {
            "id": "27",
            "entry": "[27] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evtimov%2C%20Ivan%20Eykholt%2C%20Kevin%20Fernandes%2C%20Earlence%20Kohno%2C%20Tadayoshi%20Robust%20physical-world%20attacks%20on%20machine%20learning%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evtimov%2C%20Ivan%20Eykholt%2C%20Kevin%20Fernandes%2C%20Earlence%20Kohno%2C%20Tadayoshi%20Robust%20physical-world%20attacks%20on%20machine%20learning%20models%202018"
        },
        {
            "id": "28",
            "entry": "[28] Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classifiers\u2019 robustness to adversarial perturbations. Machine Learning, 107(3):481\u2013508, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Analysis%20of%20classifiers%E2%80%99%20robustness%20to%20adversarial%20perturbations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Analysis%20of%20classifiers%E2%80%99%20robustness%20to%20adversarial%20perturbations%202018"
        },
        {
            "id": "29",
            "entry": "[29] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016"
        },
        {
            "id": "30",
            "entry": "[30] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00410"
        },
        {
            "id": "31",
            "entry": "[31] Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, and Thomas Brox. Adversarial examples for semantic image segmentation. In ICLR Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fischer%2C%20Volker%20Kumar%2C%20Mummadi%20Chaithanya%20Metzen%2C%20Jan%20Hendrik%20Brox%2C%20Thomas%20Adversarial%20examples%20for%20semantic%20image%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fischer%2C%20Volker%20Kumar%2C%20Mummadi%20Chaithanya%20Metzen%2C%20Jan%20Hendrik%20Brox%2C%20Thomas%20Adversarial%20examples%20for%20semantic%20image%20segmentation%202017"
        },
        {
            "id": "32",
            "entry": "[32] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilmer%2C%20Justin%20Metz%2C%20Luke%20Faghri%2C%20Fartash%20Schoenholz%2C%20Samuel%20S.%20Adversarial%20spheres%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilmer%2C%20Justin%20Metz%2C%20Luke%20Faghri%2C%20Fartash%20Schoenholz%2C%20Samuel%20S.%20Adversarial%20spheres%202018"
        },
        {
            "id": "33",
            "entry": "[33] Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. Adversarial and clean data are not twins. arXiv preprint arXiv:1704.04960, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04960"
        },
        {
            "id": "34",
            "entry": "[34] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "35",
            "entry": "[35] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06280"
        },
        {
            "id": "36",
            "entry": "[36] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. Adversarial examples for malware detection. In European Symposium on Research in Computer Security, pages 62\u201379.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grosse%2C%20Kathrin%20Papernot%2C%20Nicolas%20Manoharan%2C%20Praveen%20Backes%2C%20Michael%20Adversarial%20examples%20for%20malware%20detection",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grosse%2C%20Kathrin%20Papernot%2C%20Nicolas%20Manoharan%2C%20Praveen%20Backes%2C%20Michael%20Adversarial%20examples%20for%20malware%20detection"
        },
        {
            "id": "37",
            "entry": "[37] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100(1), 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haussler%2C%20D.%20Decision%20theoretic%20generalizations%20of%20the%20PAC%20model%20for%20neural%20net%20and%20other%20learning%20applications%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haussler%2C%20D.%20Decision%20theoretic%20generalizations%20of%20the%20PAC%20model%20for%20neural%20net%20and%20other%20learning%20applications%201992"
        },
        {
            "id": "38",
            "entry": "[38] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems, pages 2263\u20132273, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "39",
            "entry": "[39] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012"
        },
        {
            "id": "40",
            "entry": "[40] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Sandy%20Papernot%2C%20Nicolas%20Goodfellow%2C%20Ian%20Duan%2C%20Yan%20Adversarial%20attacks%20on%20neural%20network%20policies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Sandy%20Papernot%2C%20Nicolas%20Goodfellow%2C%20Ian%20Duan%2C%20Yan%20Adversarial%20attacks%20on%20neural%20network%20policies%202017"
        },
        {
            "id": "41",
            "entry": "[41] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In IEEE Security and Privacy, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jagielski%2C%20Matthew%20Oprea%2C%20Alina%20Biggio%2C%20Battista%20Liu%2C%20Chang%20Manipulating%20machine%20learning%3A%20Poisoning%20attacks%20and%20countermeasures%20for%20regression%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jagielski%2C%20Matthew%20Oprea%2C%20Alina%20Biggio%2C%20Battista%20Liu%2C%20Chang%20Manipulating%20machine%20learning%3A%20Poisoning%20attacks%20and%20countermeasures%20for%20regression%20learning%202018"
        },
        {
            "id": "42",
            "entry": "[42] Kyle D Julian, Jessica Lopez, Jeffrey S Brush, Michael P Owen, and Mykel J Kochenderfer. Policy compression for aircraft collision avoidance systems. In Digital Avionics Systems Conference (DASC), 2016 IEEE/AIAA 35th, pages 1\u201310. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Julian%2C%20Kyle%20D.%20Lopez%2C%20Jessica%20Brush%2C%20Jeffrey%20S.%20Owen%2C%20Michael%20P.%20Policy%20compression%20for%20aircraft%20collision%20avoidance%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Julian%2C%20Kyle%20D.%20Lopez%2C%20Jessica%20Brush%2C%20Jeffrey%20S.%20Owen%2C%20Michael%20P.%20Policy%20compression%20for%20aircraft%20collision%20avoidance%20systems%202016"
        },
        {
            "id": "43",
            "entry": "[43] Alex Kantchelian, JD Tygar, and Anthony D Joseph. Evasion and hardening of tree ensemble classifiers. In Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alex%20Kantchelian%2C%20J.D.Tygar%20Joseph%2C%20Anthony%20D.%20Evasion%20and%20hardening%20of%20tree%20ensemble%20classifiers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alex%20Kantchelian%2C%20J.D.Tygar%20Joseph%2C%20Anthony%20D.%20Evasion%20and%20hardening%20of%20tree%20ensemble%20classifiers%202016"
        },
        {
            "id": "44",
            "entry": "[44] Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4):807\u2013837, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20Michael%20Li%2C%20Ming%20Learning%20in%20the%20presence%20of%20malicious%20errors%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20Michael%20Li%2C%20Ming%20Learning%20in%20the%20presence%20of%20malicious%20errors%201993"
        },
        {
            "id": "45",
            "entry": "[45] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolter%2C%20J.Zico%20Wong%2C%20Eric%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolter%2C%20J.Zico%20Wong%2C%20Eric%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018"
        },
        {
            "id": "46",
            "entry": "[46] Jernej Kos, Ian Fischer, and Dawn Song. Adversarial examples for generative models. arXiv preprint arXiv:1702.06832, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06832"
        },
        {
            "id": "47",
            "entry": "[47] Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. In ICLR Workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kos%2C%20Jernej%20Song%2C%20Dawn%20Delving%20into%20adversarial%20attacks%20on%20deep%20policies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kos%2C%20Jernej%20Song%2C%20Dawn%20Delving%20into%20adversarial%20attacks%20on%20deep%20policies%202017"
        },
        {
            "id": "48",
            "entry": "[48] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201912, pages 1097\u20131105, USA, 2012. Curran Associates Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "49",
            "entry": "[49] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "50",
            "entry": "[50] Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor CM Leung. A survey on security threats and defensive techniques of machine learning: A data driven view. IEEE access, 6:12103\u201312117, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Li%2C%20Pan%20Zhao%2C%20Wentao%20Cai%2C%20Wei%20A%20survey%20on%20security%20threats%20and%20defensive%20techniques%20of%20machine%20learning%3A%20A%20data%20driven%20view%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Li%2C%20Pan%20Zhao%2C%20Wentao%20Cai%2C%20Wei%20A%20survey%20on%20security%20threats%20and%20defensive%20techniques%20of%20machine%20learning%3A%20A%20data%20driven%20view%202018"
        },
        {
            "id": "51",
            "entry": "[51] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yanpei%20Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Yanpei%20Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%202017"
        },
        {
            "id": "52",
            "entry": "[52] Jiajun Lu, Hussein Sibai, and Evan Fabry. Adversarial examples that fool detectors. arXiv preprint arXiv:1712.02494, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02494"
        },
        {
            "id": "53",
            "entry": "[53] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "54",
            "entry": "[54] Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 135\u2013147. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20Dongyu%20Chen%2C%20Hao%20Magnet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20Dongyu%20Chen%2C%20Hao%20Magnet%3A%20a%20two-pronged%20defense%20against%20adversarial%20examples%202017"
        },
        {
            "id": "55",
            "entry": "[55] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Universal%20adversarial%20perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Universal%20adversarial%20perturbations%202017"
        },
        {
            "id": "56",
            "entry": "[56] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "57",
            "entry": "[57] Matej Moravc\u0131k, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moravc%C4%B1k%2C%20Matej%20Schmid%2C%20Martin%20Burch%2C%20Neil%20Lisy%2C%20Viliam%20Deepstack%3A%20Expert-level%20artificial%20intelligence%20in%20heads-up%20no-limit%20poker%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moravc%C4%B1k%2C%20Matej%20Schmid%2C%20Martin%20Burch%2C%20Neil%20Lisy%2C%20Viliam%20Deepstack%3A%20Expert-level%20artificial%20intelligence%20in%20heads-up%20no-limit%20poker%202017"
        },
        {
            "id": "58",
            "entry": "[58] Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand Raghunathan, and Niraj K Jha. Systematic poisoning attacks on and defenses for machine learning in healthcare. IEEE journal of biomedical and health informatics, 19(6):1893\u20131905, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mozaffari-Kermani%2C%20Mehran%20Sur-Kolay%2C%20Susmita%20Raghunathan%2C%20Anand%20Jha%2C%20Niraj%20K.%20Systematic%20poisoning%20attacks%20on%20and%20defenses%20for%20machine%20learning%20in%20healthcare%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mozaffari-Kermani%2C%20Mehran%20Sur-Kolay%2C%20Susmita%20Raghunathan%2C%20Anand%20Jha%2C%20Niraj%20K.%20Systematic%20poisoning%20attacks%20on%20and%20defenses%20for%20machine%20learning%20in%20healthcare%202015"
        },
        {
            "id": "59",
            "entry": "[59] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07277"
        },
        {
            "id": "60",
            "entry": "[60] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial examples. In Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20deep%20learning%20systems%20using%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20deep%20learning%20systems%20using%20adversarial%20examples%202017"
        },
        {
            "id": "61",
            "entry": "[61] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pages 372\u2013387. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "62",
            "entry": "[62] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of security and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03814"
        },
        {
            "id": "63",
            "entry": "[63] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pages 582\u2013597. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "64",
            "entry": "[64] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghunathan%2C%20Aditi%20Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Certified%20defenses%20against%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghunathan%2C%20Aditi%20Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Certified%20defenses%20against%20adversarial%20examples%202018"
        },
        {
            "id": "65",
            "entry": "[65] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and JD Tygar. Stealthy poisoning attacks on pca-based anomaly detectors. ACM SIGMETRICS Performance Evaluation Review, 37(2):73\u201374, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Benjamin%20I.P.%20Nelson%2C%20Blaine%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Stealthy%20poisoning%20attacks%20on%20pca-based%20anomaly%20detectors%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20Benjamin%20I.P.%20Nelson%2C%20Blaine%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Stealthy%20poisoning%20attacks%20on%20pca-based%20anomaly%20detectors%202009"
        },
        {
            "id": "66",
            "entry": "[66] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11285"
        },
        {
            "id": "67",
            "entry": "[67] Uri Shaham, James Garritano, Yutaro Yamada, Ethan Weinberger, Alex Cloninger, Xiuyuan Cheng, Kelly Stanton, and Yuval Kluger. Defending against adversarial images using basis functions transformations. arXiv preprint arXiv:1803.10840, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10840"
        },
        {
            "id": "68",
            "entry": "[68] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Ben-David%2C%20Shai%20Understanding%20machine%20learning%3A%20From%20theory%20to%20algorithms%202014"
        },
        {
            "id": "69",
            "entry": "[69] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 1528\u20131540. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "70",
            "entry": "[70] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "71",
            "entry": "[71] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "72",
            "entry": "[72] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018"
        },
        {
            "id": "73",
            "entry": "[73] Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Prateek Mittal, and Mung Chiang. Rogue signs: Deceiving traffic sign recognition with malicious ads and logos. In DLS (IEEE SP), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sitawarin%2C%20Chawin%20Bhagoji%2C%20Arjun%20Nitin%20Mosenia%2C%20Arsalan%20Mittal%2C%20Prateek%20Rogue%20signs%3A%20Deceiving%20traffic%20sign%20recognition%20with%20malicious%20ads%20and%20logos%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sitawarin%2C%20Chawin%20Bhagoji%2C%20Arjun%20Nitin%20Mosenia%2C%20Arsalan%20Mittal%2C%20Prateek%20Rogue%20signs%3A%20Deceiving%20traffic%20sign%20recognition%20with%20malicious%20ads%20and%20logos%202018"
        },
        {
            "id": "74",
            "entry": "[74] Charles Smutz and Angelos Stavrou. When a tree falls: Using diversity in ensemble classifiers to identify evasion in malware detectors. In NDSS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smutz%2C%20Charles%20Stavrou%2C%20Angelos%20When%20a%20tree%20falls%3A%20Using%20diversity%20in%20ensemble%20classifiers%20to%20identify%20evasion%20in%20malware%20detectors%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smutz%2C%20Charles%20Stavrou%2C%20Angelos%20When%20a%20tree%20falls%3A%20Using%20diversity%20in%20ensemble%20classifiers%20to%20identify%20evasion%20in%20malware%20detectors%202016"
        },
        {
            "id": "75",
            "entry": "[75] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "76",
            "entry": "[76] Florian Tramer, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tramer%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Boneh%2C%20Dan%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tramer%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Boneh%2C%20Dan%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018"
        },
        {
            "id": "77",
            "entry": "[77] Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G Ororbia II, Xinyu Xing, Xue Liu, and C Lee Giles. Adversary resistant deep neural networks with an application to malware detection. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1145\u20131153. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Qinglong%20Guo%2C%20Wenbo%20Zhang%2C%20Kaixuan%20Ororbia%2C%20II%2C%20Alexander%20G.%20Adversary%20resistant%20deep%20neural%20networks%20with%20an%20application%20to%20malware%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Qinglong%20Guo%2C%20Wenbo%20Zhang%2C%20Kaixuan%20Ororbia%2C%20II%2C%20Alexander%20G.%20Adversary%20resistant%20deep%20neural%20networks%20with%20an%20application%20to%20malware%20detection%202017"
        },
        {
            "id": "78",
            "entry": "[78] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors to adversarial examples. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yizhen%20Jha%2C%20Somesh%20Chaudhuri%2C%20Kamalika%20Analyzing%20the%20robustness%20of%20nearest%20neighbors%20to%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yizhen%20Jha%2C%20Somesh%20Chaudhuri%2C%20Kamalika%20Analyzing%20the%20robustness%20of%20nearest%20neighbors%20to%20adversarial%20examples%202018"
        },
        {
            "id": "79",
            "entry": "[79] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%202018"
        },
        {
            "id": "80",
            "entry": "[80] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In International Conference on Computer Vision. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Zhou%2C%20Yuyin%20Adversarial%20examples%20for%20semantic%20segmentation%20and%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Zhou%2C%20Yuyin%20Adversarial%20examples%20for%20semantic%20segmentation%20and%20object%20detection%202017"
        },
        {
            "id": "81",
            "entry": "[81] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. In NDSS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Weilin%20Evans%2C%20David%20Qi%2C%20Yanjun%20Feature%20squeezing%3A%20Detecting%20adversarial%20examples%20in%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Weilin%20Evans%2C%20David%20Qi%2C%20Yanjun%20Feature%20squeezing%3A%20Detecting%20adversarial%20examples%20in%20deep%20neural%20networks%202018"
        },
        {
            "id": "82",
            "entry": "[82] Weilin Xu, Yanjun Qi, and David Evans. Automatically evading classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Weilin%20Qi%2C%20Yanjun%20Evans%2C%20David%20Automatically%20evading%20classifiers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Weilin%20Qi%2C%20Yanjun%20Evans%2C%20David%20Automatically%20evading%20classifiers%202016"
        },
        {
            "id": "83",
            "entry": "[83] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A Gunter. Commandersong: A systematic approach for practical adversarial voice recognition. In USENIX Security, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Xuejing%20Chen%2C%20Yuxuan%20Zhao%2C%20Yue%20Long%2C%20Yunhui%20Heqing%20Huang%2C%20Xiaofeng%20Wang%2C%20and%20Carl%20A%20Gunter.%20Commandersong%3A%20A%20systematic%20approach%20for%20practical%20adversarial%20voice%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Xuejing%20Chen%2C%20Yuxuan%20Zhao%2C%20Yue%20Long%2C%20Yunhui%20Heqing%20Huang%2C%20Xiaofeng%20Wang%2C%20and%20Carl%20A%20Gunter.%20Commandersong%3A%20A%20systematic%20approach%20for%20practical%20adversarial%20voice%20recognition%202018"
        }
    ]
}
