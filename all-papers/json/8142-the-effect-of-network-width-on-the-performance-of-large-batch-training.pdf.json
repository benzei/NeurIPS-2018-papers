{
    "filename": "8142-the-effect-of-network-width-on-the-performance-of-large-batch-training.pdf",
    "metadata": {
        "title": "The Effect of Network Width on the Performance of  Large-batch Training",
        "author": "Lingjiao Chen, Hongyi Wang, Jinman Zhao, Dimitris Papailiopoulos, Paraschos Koutris",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8142-the-effect-of-network-width-on-the-performance-of-large-batch-training.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Distributed implementations of mini-batch stochastic gradient descent (SGD) suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance. In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that\u2013for a fixed number of parameters\u2013wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Distributed implementations of stochastic optimization algorithms have become the standard in largescale model training [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "Experimental Results We first verify whether gradient diversity reflects the amenability to large batch training",
        "As shown in Figure 2(c), the largest batch size that does not impact the convergence rate grows monotonically w.r.t the gradient diversity. This validates our theoretical analysis that gradient diversity can be used to capture the amenability to large batch training",
        "We study how the structure of a neural network affects the performance of large-batch training",
        "Along with theoretical analysis, demonstrate that for a large class of neural networks, increasing width leads to larger gradient diversity and allows for a larger batch training that is always beneficial for distributed computation",
        "We argue that it is important to consider the architecture of a network with regards to its amenability for speedups in a distributed setting"
    ],
    "key_statements": [
        "Distributed implementations of stochastic optimization algorithms have become the standard in largescale model training [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "During a distributed iteration of mini-batch stochastic gradient descent a parameter server (PS) stores the global model, and P compute nodes evaluate a total of B gradients; B is commonly referred to as the batch size",
        "Between the structure of neural networks and their gradient diversity, which is an indicator of how large batch size can become, without slowing down the speed of convergence [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "We prove how gradient diversity varies as a function of width and depth for two types of networks: 2-layer fully-connected linear and non-linear neural networks, and multi-layer fully-connected linear neural networks",
        "While previous work mainly focuses on convex models, or specific models, our work studies how neural network structure can affect the optimal batch size",
        "Our results are presented as probabilistic statements, and for almost all weight matrices",
        "The same trends can be observed here as in the case of 2-layer LNNs: (i) larger width leads to a larger gradient diversity, and faster convergence of distributed mini-batch stochastic gradient descent, and the ratio can never exceed \u03a9(d)",
        "Experimental Results We first verify whether gradient diversity reflects the amenability to large batch training",
        "Figure 2(a) shows how the averaged gradient diversity varies as depth/width changes, while Figure 2(b) presents the largest batch to converge for each network within a pre-set number of epochs",
        "As shown in Figure 2(c), the largest batch size that does not impact the convergence rate grows monotonically w.r.t the gradient diversity. This validates our theoretical analysis that gradient diversity can be used to capture the amenability to large batch training",
        "We study how the structure of a network affects the largest possible batch size to converge within a fixed number of epochs/data passes to a pre-specified accuracy",
        "As shown in Figure 5, neural networks with larger width K usually allow much larger batch sizes to converge within a small, pre-set number of total epochs",
        "We study how the structure of a neural network affects the performance of large-batch training",
        "Along with theoretical analysis, demonstrate that for a large class of neural networks, increasing width leads to larger gradient diversity and allows for a larger batch training that is always beneficial for distributed computation",
        "We argue that it is important to consider the architecture of a network with regards to its amenability for speedups in a distributed setting"
    ],
    "summary": [
        "Distributed implementations of stochastic optimization algorithms have become the standard in largescale model training [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "Between the structure of neural networks and their gradient diversity, which is an indicator of how large batch size can become, without slowing down the speed of convergence [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>].",
        "We fix the number of network parameters, vary the depth and width, and measure how many passes over the data it takes to reach an accuracy of with batch size B.",
        "[<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] shows that this is a worst-case optimal bound, i.e., if the batch size is larger than n times the gradient diversity, there exists some model such that the convergence rate of mini-batch SGD is slower than that of serial SGD.",
        "We present a theoretical analysis on how structural properties of a neural network, and in particular the depth and width, influence the gradient diversity, and the convergence rate of mini-batch SGD for varying batch size B.",
        "The same trends can be observed here as in the case of 2-layer LNNs: (i) larger width leads to a larger gradient diversity, and faster convergence of distributed mini-batch SGD, and the ratio can never exceed \u03a9(d).",
        "For each linear FC network with fixed width and depth, we measure its gradient diversity every ten epochs and compute the average.",
        "Figure 2(a) shows how the averaged gradient diversity varies as depth/width changes, while Figure 2(b) presents the largest batch to converge for each network within a pre-set number of epochs.",
        "In Figure 3(b), when the batch size is smaller than 256, the FC network with width K = 17 and depth L = 10 needs a small number (2 to 3) of epochs to converge.",
        "As shown in Figures 3(e) and 3(f), for a fixed batch size, increasing the width almost always leads to a decrease in number of epochs for convergence.",
        "We study how the structure of a network affects the largest possible batch size to converge within a fixed number of epochs/data passes to a pre-specified accuracy.",
        "As shown in Figure 5, neural networks with larger width K usually allow much larger batch sizes to converge within a small, pre-set number of total epochs.",
        "Along with theoretical analysis, demonstrate that for a large class of neural networks, increasing width leads to larger gradient diversity and allows for a larger batch training that is always beneficial for distributed computation.",
        "Another direction is to quantitatively study how the generalization error is affected"
    ],
    "headline": "We present new theoretical results which suggest that\u2013for a fixed number of parameters\u2013wider networks are more amenable to fast large-batch training compared to deeper ones",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In OSDI 2016, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In OSDI 2014, pages 583\u2013598, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "3",
            "entry": "[3] John C. Duchi, Alekh Agarwal, and Martin J. Wainwright. Distributed dual averaging in networks. In NIPS 2010, pages 550\u2013558, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Agarwal%2C%20Alekh%20Wainwright%2C%20Martin%20J.%20Distributed%20dual%20averaging%20in%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Agarwal%2C%20Alekh%20Wainwright%2C%20Martin%20J.%20Distributed%20dual%20averaging%20in%20networks%202010"
        },
        {
            "id": "4",
            "entry": "[4] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. CoRR, abs/1512.01274, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.01274"
        },
        {
            "id": "5",
            "entry": "[5] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal J\u00f3zefowicz. Revisiting distributed synchronous SGD. CoRR, abs/1604.00981, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.00981"
        },
        {
            "id": "6",
            "entry": "[6] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc\u2019Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In NIPS 2012, pages 1232\u20131240, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012"
        },
        {
            "id": "7",
            "entry": "[7] Caffe2: A new lightweight, modular, and scalable deep learning framework.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Caffe2%3A%20A%20new%20lightweight%2C%20modular%2C%20and%20scalable%20deep%20learning%20framework"
        },
        {
            "id": "8",
            "entry": "[8] Hang Qi, Evan R. Sparks, and Ameet Talwalkar. Paleo: A performance model for deep neural networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qi%2C%20Hang%20Sparks%2C%20Evan%20R.%20Talwalkar%2C%20Ameet%20Paleo%3A%20A%20performance%20model%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qi%2C%20Hang%20Sparks%2C%20Evan%20R.%20Talwalkar%2C%20Ameet%20Paleo%3A%20A%20performance%20model%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "9",
            "entry": "[9] Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over decentralized data. CoRR, abs/1803.07068, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07068"
        },
        {
            "id": "10",
            "entry": "[10] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: communicationefficient SGD via gradient quantization and encoding. In NIPS 2017, pages 1707\u20131718, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alistarh%2C%20Dan%20Grubic%2C%20Demjan%20Li%2C%20Jerry%20Tomioka%2C%20Ryota%20QSGD%3A%20communicationefficient%20SGD%20via%20gradient%20quantization%20and%20encoding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alistarh%2C%20Dan%20Grubic%2C%20Demjan%20Li%2C%20Jerry%20Tomioka%2C%20Ryota%20QSGD%3A%20communicationefficient%20SGD%20via%20gradient%20quantization%20and%20encoding%202017"
        },
        {
            "id": "11",
            "entry": "[11] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In NIPS 2017, pages 1508\u20131518, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017"
        },
        {
            "id": "12",
            "entry": "[12] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J. Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. CoRR, abs/1712.01887, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01887"
        },
        {
            "id": "13",
            "entry": "[13] Priya Goyal, Piotr Doll\u00e1r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "14",
            "entry": "[14] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In NIPS 2017, pages 1729\u20131739, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "15",
            "entry": "[15] Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training. CoRR, abs/1708.03888, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03888"
        },
        {
            "id": "16",
            "entry": "[16] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "17",
            "entry": "[17] Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification. CoRR, abs/1610.03774, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1610.03774"
        },
        {
            "id": "18",
            "entry": "[18] Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. CoRR, abs/1712.06559, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06559"
        },
        {
            "id": "19",
            "entry": "[19] Dong Yin, Ashwin Pananjady, Maximilian Lam, Dimitris S. Papailiopoulos, Kannan Ramchandran, and Peter Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In AISTATS 2018, pages 1998\u20132007, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20Dong%20Pananjady%2C%20Ashwin%20Lam%2C%20Maximilian%20Papailiopoulos%2C%20Dimitris%20S.%20Gradient%20diversity%3A%20a%20key%20ingredient%20for%20scalable%20distributed%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20Dong%20Pananjady%2C%20Ashwin%20Lam%2C%20Maximilian%20Papailiopoulos%2C%20Dimitris%20S.%20Gradient%20diversity%3A%20a%20key%20ingredient%20for%20scalable%20distributed%20learning%202018"
        },
        {
            "id": "20",
            "entry": "[20] Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. CoRR, abs/1804.07612, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07612"
        },
        {
            "id": "21",
            "entry": "[21] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13:165\u2013202, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20Ofer%20Gilad-Bachrach%2C%20Ran%20Shamir%2C%20Ohad%20Xiao%2C%20Lin%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012"
        },
        {
            "id": "22",
            "entry": "[22] Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic approximation through mini-batching and tail-averaging. CoRR, abs/1610.03774, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.03774"
        },
        {
            "id": "23",
            "entry": "[23] Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. IEEE Trans. Neural Netw. Learning Syst., 25(8):1553\u20131565, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bianchini%2C%20Monica%20Scarselli%2C%20Franco%20On%20the%20complexity%20of%20neural%20network%20classifiers%3A%20A%20comparison%20between%20shallow%20and%20deep%20architectures%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bianchini%2C%20Monica%20Scarselli%2C%20Franco%20On%20the%20complexity%20of%20neural%20network%20classifiers%3A%20A%20comparison%20between%20shallow%20and%20deep%20architectures%202014"
        },
        {
            "id": "24",
            "entry": "[24] Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. In COLT 1991, pages 243\u2013249, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barron%2C%20Andrew%20R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barron%2C%20Andrew%20R.%20Approximation%20and%20estimation%20bounds%20for%20artificial%20neural%20networks%201991"
        },
        {
            "id": "25",
            "entry": "[25] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. In NIPS 2017, pages 6232\u20136240, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Zhou%20Pu%2C%20Hongming%20Wang%2C%20Feicheng%20Hu%2C%20Zhiqiang%20The%20expressive%20power%20of%20neural%20networks%3A%20A%20view%20from%20the%20width%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Zhou%20Pu%2C%20Hongming%20Wang%2C%20Feicheng%20Hu%2C%20Zhiqiang%20The%20expressive%20power%20of%20neural%20networks%3A%20A%20view%20from%20the%20width%202017"
        },
        {
            "id": "26",
            "entry": "[26] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In NIPS 2011, pages 666\u2013674, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Delalleau%2C%20Olivier%20Bengio%2C%20Yoshua%20Shallow%20vs.%20deep%20sum-product%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Delalleau%2C%20Olivier%20Bengio%2C%20Yoshua%20Shallow%20vs.%20deep%20sum-product%20networks%202011"
        },
        {
            "id": "27",
            "entry": "[27] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR 2017, pages 2261\u20132269, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "28",
            "entry": "[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR 2016, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "29",
            "entry": "[29] Martin Tak\u00e1c, Avleen Singh Bijral, Peter Richt\u00e1rik, and Nati Srebro. Mini-batch primal and dual methods for svms. In ICML 2013, pages 1022\u20131030, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tak%C3%A1c%2C%20Martin%20Bijral%2C%20Avleen%20Singh%20Richt%C3%A1rik%2C%20Peter%20Srebro%2C%20Nati%20Mini-batch%20primal%20and%20dual%20methods%20for%20svms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tak%C3%A1c%2C%20Martin%20Bijral%2C%20Avleen%20Singh%20Richt%C3%A1rik%2C%20Peter%20Srebro%2C%20Nati%20Mini-batch%20primal%20and%20dual%20methods%20for%20svms%202013"
        },
        {
            "id": "30",
            "entry": "[30] Michael P. Friedlander and Mark W. Schmidt. Erratum: Hybrid deterministic-stochastic methods for data fitting. SIAM J. Scientific Computing, 35(4), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedlander%2C%20Michael%20P.%20Schmidt%2C%20Mark%20W.%20Erratum%3A%20Hybrid%20deterministic-stochastic%20methods%20for%20data%20fitting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedlander%2C%20Michael%20P.%20Schmidt%2C%20Mark%20W.%20Erratum%3A%20Hybrid%20deterministic-stochastic%20methods%20for%20data%20fitting%202013"
        },
        {
            "id": "31",
            "entry": "[31] Soham De, Abhay Kumar Yadav, David W. Jacobs, and Tom Goldstein. Big batch SGD: automated inference using adaptive batch sizes. CoRR, abs/1610.05792, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05792"
        },
        {
            "id": "32",
            "entry": "[32] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In NIPS 2011, pages 1647\u20131655, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cotter%2C%20Andrew%20Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Sridharan%2C%20Karthik%20Better%20mini-batch%20algorithms%20via%20accelerated%20gradient%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cotter%2C%20Andrew%20Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Sridharan%2C%20Karthik%20Better%20mini-batch%20algorithms%20via%20accelerated%20gradient%20methods%202011"
        },
        {
            "id": "33",
            "entry": "[33] Shai Shalev-Shwartz and Tong Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In NIPS 2013, pages 378\u2013385, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20mini-batch%20stochastic%20dual%20coordinate%20ascent%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20mini-batch%20stochastic%20dual%20coordinate%20ascent%202013"
        },
        {
            "id": "34",
            "entry": "[34] Martin Tak\u00e1c, Peter Richt\u00e1rik, and Nathan Srebro. Distributed mini-batch SDCA. CoRR, abs/1507.08322, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.08322"
        },
        {
            "id": "35",
            "entry": "[35] Jialei Wang, Weiran Wang, and Nathan Srebro. Memory and communication efficient distributed stochastic optimization with minibatch prox. In COLT 2017, pages 1882\u20131919, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jialei%20Wang%2C%20Weiran%20Srebro%2C%20Nathan%20Memory%20and%20communication%20efficient%20distributed%20stochastic%20optimization%20with%20minibatch%20prox%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jialei%20Wang%2C%20Weiran%20Srebro%2C%20Nathan%20Memory%20and%20communication%20efficient%20distributed%20stochastic%20optimization%20with%20minibatch%20prox%202017"
        },
        {
            "id": "36",
            "entry": "[36] Peilin Zhao and Tong Zhang. Accelerating minibatch stochastic gradient descent using stratified sampling. CoRR, abs/1405.3080, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1405.3080"
        },
        {
            "id": "37",
            "entry": "[37] Cheng Zhang, Hedvig Kjellstr\u00f6m, and Stephan Mandt. Stochastic learning on imbalanced data: Determinantal point processes for mini-batch diversification. CoRR, abs/1705.00607, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.00607"
        },
        {
            "id": "38",
            "entry": "[38] Cheng Zhang, Cengiz \u00d6ztireli, Stephan Mandt, and Giampiero Salvi. Active mini-batch sampling using repulsive point processes. CoRR, abs/1804.02772, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02772"
        },
        {
            "id": "39",
            "entry": "[39] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC 2016, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016"
        },
        {
            "id": "40",
            "entry": "[40] Junting Pan, Elisa Sayrol, Xavier Gir\u00f3 i Nieto, Kevin McGuinness, and Noel E. O\u2019Connor. Shallow and deep convolutional networks for saliency prediction. In CVPR 2016, pages 598\u2013606, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Junting%20Sayrol%2C%20Elisa%20i%20Nieto%2C%20Xavier%20Gir%C3%B3%20McGuinness%2C%20Kevin%20Shallow%20and%20deep%20convolutional%20networks%20for%20saliency%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Junting%20Sayrol%2C%20Elisa%20i%20Nieto%2C%20Xavier%20Gir%C3%B3%20McGuinness%2C%20Kevin%20Shallow%20and%20deep%20convolutional%20networks%20for%20saliency%20prediction%202016"
        },
        {
            "id": "41",
            "entry": "[41] Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet model for visual recognition. CoRR, abs/1611.10080, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.10080"
        },
        {
            "id": "42",
            "entry": "[42] Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, pages 3727\u20133736, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20cnns%202018-07-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20cnns%202018-07-10"
        },
        {
            "id": "43",
            "entry": "[43] Fran\u00e7ois Chollet. keras. https://github.com/fchollet/keras, 2015.",
            "url": "https://github.com/fchollet/keras"
        },
        {
            "id": "44",
            "entry": "[44] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.",
            "url": "http://yann.lecun.com/exdb/mnist"
        },
        {
            "id": "45",
            "entry": "[45] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00e9 van Schaik. Emnist: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05373"
        },
        {
            "id": "46",
            "entry": "[46] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):27, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20Libsvm%3A%20a%20library%20for%20support%20vector%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20Libsvm%3A%20a%20library%20for%20support%20vector%20machines%202011"
        },
        {
            "id": "47",
            "entry": "[47] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "48",
            "entry": "[48] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "49",
            "entry": "[49] L. Isserlis. On a formula for the product-moment coefficient of any order of a normal frequency distribution in any number of variables. Biometrika, 12(1/2):134\u2013139, 1918. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isserlis%2C%20L.%20On%20a%20formula%20for%20the%20product-moment%20coefficient%20of%20any%20order%20of%20a%20normal%20frequency%20distribution%20in%20any%20number%20of%20variables%201918",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isserlis%2C%20L.%20On%20a%20formula%20for%20the%20product-moment%20coefficient%20of%20any%20order%20of%20a%20normal%20frequency%20distribution%20in%20any%20number%20of%20variables%201918"
        }
    ]
}
