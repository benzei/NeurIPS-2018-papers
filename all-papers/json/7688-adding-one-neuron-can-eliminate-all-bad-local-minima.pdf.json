{
    "filename": "7688-adding-one-neuron-can-eliminate-all-bad-local-minima.pdf",
    "metadata": {
        "title": "Adding One Neuron Can Eliminate All Bad Local Minima",
        "author": "SHIYU LIANG, Ruoyu Sun, Jason D. Lee, R. Srikant",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7688-adding-one-neuron-can-eliminate-all-bad-local-minima.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "One of the main difficulties in analyzing neural networks is the non-convexity of the loss function which may have many bad local minima. In this paper, we study the landscape of neural networks for binary classification tasks. Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum."
    },
    "keywords": [
        {
            "term": "local minimum",
            "url": "https://en.wikipedia.org/wiki/local_minimum"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "global minimum",
            "url": "https://en.wikipedia.org/wiki/global_minimum"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        },
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "deep convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/deep_convolutional_neural_network"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "Deep neural networks have recently achieved huge success in various machine learning tasks",
        "One of the difficulties in analyzing neural networks is the non-convexity of the loss function which allows the existence of many local minima with large losses",
        "One of the difficulties in analyzing neural networks is the non-convexity of the loss functions which allows the existence of many spurious minima with large loss values",
        "We prove that for any neural network, by adding a special neuron and an associated regularizer, the new loss function has no spurious local minimum",
        "We prove that, at every local minimum of this new loss function, the exponential neuron is inactive and this means that the augmented neuron and regularizer improve the landscape of the loss surface without affecting the representing power of the original neural network",
        "\u201cgood\u201d can mean various things such as nice landscape, stronger representation power or better generalization, and in this paper we focus on the landscape \u2013in particular, the very specific property \u201cevery local minimum is a global minimum\u201d"
    ],
    "key_statements": [
        "Deep neural networks have recently achieved huge success in various machine learning tasks",
        "One of the difficulties in analyzing neural networks is the non-convexity of the loss function which allows the existence of many local minima with large losses",
        "Prior works empirically showed that neural networks with identical architectures but different initialization points can converge to local minima with similar classification performance (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a>; <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a>; <a class=\"ref-link\" id=\"cHuang_2017_a\" href=\"#rHuang_2017_a\">Huang & Liu, 2017</a>)",
        "Our main result is quite surprising: for a neural network with a special type of neurons, every local minimum is a global minimum of the loss function",
        "We prove the following result: for any neural network, by adding a special neuron to the network and adding a quadratic regularizer of this neuron, the new loss function has no bad local minimum",
        "One of the difficulties in analyzing neural networks is the non-convexity of the loss functions which allows the existence of many spurious minima with large loss values",
        "We prove that for any neural network, by adding a special neuron and an associated regularizer, the new loss function has no spurious local minimum",
        "We prove that, at every local minimum of this new loss function, the exponential neuron is inactive and this means that the augmented neuron and regularizer improve the landscape of the loss surface without affecting the representing power of the original neural network",
        "This paper is an effort in designing neural networks that are \u201cgood\u201d",
        "\u201cgood\u201d can mean various things such as nice landscape, stronger representation power or better generalization, and in this paper we focus on the landscape \u2013in particular, the very specific property \u201cevery local minimum is a global minimum\u201d",
        "It is an interesting direction to improve the landscape results by considering other aspects, such as studying when a specific algorithm will converge to local minima and global minima"
    ],
    "summary": [
        "Deep neural networks have recently achieved huge success in various machine learning tasks.",
        "Our main result is quite surprising: for a neural network with a special type of neurons, every local minimum is a global minimum of the loss function.",
        "We prove the following result: for any neural network, by adding a special neuron to the network and adding a quadratic regularizer of this neuron, the new loss function has no bad local minimum.",
        "Corollary 1 Under the conditions of Theorem 1, if \u03b8\u2217 = (\u03b8\u2217, a\u2217, w\u2217, b\u2217) is a local minimum of the empirical loss function Ln(\u03b8), two neural networks f (\u00b7; \u03b8\u2217) and f(\u00b7; \u03b8\u2217) are equivalent, i.e., f (x; \u03b8\u2217) = f(x; \u03b8\u2217), \u2200x \u2208 Rd. Corollary 1 shows that at every local minimum, the exponential neuron does not contribute to the output of the neural network f.",
        "To prove the main result under any dataset, the regularizer is necessary, since <a class=\"ref-link\" id=\"cLiang_et+al_2018_a\" href=\"#rLiang_et+al_2018_a\">Liang et al (2018</a>) has already shown that even with an augmented exponential neuron, the empirical loss without the regularizer still have bad local minima under some datasets.",
        "We will present the following theorem to show that if all samples in the dataset D can be correctly classified by a polynomial of degree t and the degree of the augmented monomial is not smaller than t (i.e., p \u2265 t), every local minimum of the empirical loss function Ln(\u03b8) is a global minimum.",
        "One implication of Proposition 3 is that if a dataset is linearly separable, every second order stationary point of the empirical loss function is a global minimum and, at this stationary point, the neural network achieves zero training error.",
        "Since at every local minimum, the exponential neuron is inactive, a\u2217 = 0, the set of parameters \u03b8\u2217 is a global minimum of the loss function Ln(\u03b8).",
        "We prove that for any neural network, by adding a special neuron and an associated regularizer, the new loss function has no spurious local minimum.",
        "We prove that, at every local minimum of this new loss function, the exponential neuron is inactive and this means that the augmented neuron and regularizer improve the landscape of the loss surface without affecting the representing power of the original neural network.",
        "It is an interesting direction to improve the landscape results by considering other aspects, such as studying when a specific algorithm will converge to local minima and global minima"
    ],
    "headline": "We study the landscape of neural networks for binary classification tasks",
    "reference_links": [
        {
            "id": "Andoni_et+al_2014_a",
            "entry": "Andoni, A., Panigrahy, R., Valiant, G., and Zhang, L. Learning polynomials with neural networks. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andoni%2C%20A.%20Panigrahy%2C%20R.%20Valiant%2C%20G.%20Zhang%2C%20L.%20Learning%20polynomials%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andoni%2C%20A.%20Panigrahy%2C%20R.%20Valiant%2C%20G.%20Zhang%2C%20L.%20Learning%20polynomials%20with%20neural%20networks%202014"
        },
        {
            "id": "Baldi_1989_a",
            "entry": "Baldi, P. and Hornik, K. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53\u201358, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20P.%20Hornik%2C%20K.%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20P.%20Hornik%2C%20K.%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Brutzkus, A. and Globerson, A. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07966"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "Choromanska, A., Henaff, M., Mathieu, M., Arous, G., and LeCun, Y. The loss surfaces of multilayer networks. In AISTATS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Cortes_1995_a",
            "entry": "Cortes, C. and Vapnik, V. Support-vector networks. Machine learning, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20C.%20Vapnik%2C%20V.%20Support-vector%20networks%201995"
        },
        {
            "id": "Du_2018_a",
            "entry": "Du, S. S and Lee, J. D. On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01206"
        },
        {
            "id": "Du_et+al_2017_a",
            "entry": "Du, S. S., Lee, J. D., and Tian, Y. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06129"
        },
        {
            "id": "Freeman_2016_a",
            "entry": "Freeman, C D. and Bruna, J. Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01540"
        },
        {
            "id": "Gautier_et+al_2016_a",
            "entry": "Gautier, A., Nguyen, Q. N., and Hein, M. Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods. In NIPS, pp. 1687\u20131695, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gautier%2C%20A.%20Nguyen%2C%20Q.N.%20Hein%2C%20M.%20Globally%20optimal%20training%20of%20generalized%20polynomial%20neural%20networks%20with%20nonlinear%20spectral%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gautier%2C%20A.%20Nguyen%2C%20Q.N.%20Hein%2C%20M.%20Globally%20optimal%20training%20of%20generalized%20polynomial%20neural%20networks%20with%20nonlinear%20spectral%20methods%202016"
        },
        {
            "id": "Ge_et+al_2018_a",
            "entry": "Ge, R., Lee, J. D, and Ma, T. Learning one-hidden-layer neural networks with landscape design. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Lee%2C%20J.D.%20Ma%2C%20T.%20Learning%20one-hidden-layer%20neural%20networks%20with%20landscape%20design%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Lee%2C%20J.D.%20Ma%2C%20T.%20Learning%20one-hidden-layer%20neural%20networks%20with%20landscape%20design%202018"
        },
        {
            "id": "Goel_2017_a",
            "entry": "Goel, S. and Klivans, A. Learning depth-three neural networks in polynomial time. arXiv preprint arXiv:1709.06010, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06010"
        },
        {
            "id": "Goodfellow_et+al_2013_a",
            "entry": "Goodfellow, I. J, Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1302.4389"
        },
        {
            "id": "Haeffele_et+al_2014_a",
            "entry": "Haeffele, B., Young, E., and Vidal, R. Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haeffele%2C%20B.%20Young%2C%20E.%20Vidal%2C%20R.%20Structured%20low-rank%20matrix%20factorization%3A%20Optimality%2C%20algorithm%2C%20and%20applications%20to%20image%20processing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haeffele%2C%20B.%20Young%2C%20E.%20Vidal%2C%20R.%20Structured%20low-rank%20matrix%20factorization%3A%20Optimality%2C%20algorithm%2C%20and%20applications%20to%20image%20processing%202014"
        },
        {
            "id": "Haeffele_2015_a",
            "entry": "Haeffele, B. D and Vidal, R. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.07540"
        },
        {
            "id": "Hardt_2017_a",
            "entry": "Hardt, M. and Ma, T. Identity matters in deep learning. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20M.%20Ma%2C%20T.%20Identity%20matters%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20M.%20Ma%2C%20T.%20Identity%20matters%20in%20deep%20learning%202017"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Huang_2017_a",
            "entry": "Huang, G. and Liu, Z. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "Janzamin_et+al_2015_a",
            "entry": "Janzamin, M., Sedghi, H., and Anandkumar, A. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.08473"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kawaguchi, K. Deep learning without poor local minima. In NIPS, pp. 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "LeCun, Y., Bengio, Y., and Hinton, G. E. Deep learning. Nature, 521(7553):436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.E.%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.E.%20Deep%20learning%202015"
        },
        {
            "id": "Li_2017_a",
            "entry": "Li, Y. and Yuan, Y. Convergence analysis of two-layer neural networks with relu activation. In NIPS, pp. 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Yuan%2C%20Y.%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Yuan%2C%20Y.%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017"
        },
        {
            "id": "Liang_et+al_2018_a",
            "entry": "Liang, S., Sun, R., Li, Y., and Srikant, R. Understanding the loss surface of neural networks for binary classification. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20S.%20Sun%2C%20R.%20Li%2C%20Y.%20Srikant%2C%20R.%20Understanding%20the%20loss%20surface%20of%20neural%20networks%20for%20binary%20classification%202018"
        },
        {
            "id": "Livni_et+al_2014_a",
            "entry": "Livni, R., Shalev-Shwartz, S., and Shamir, O. On the computational efficiency of training neural networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Livni%2C%20R.%20Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Livni%2C%20R.%20Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014"
        },
        {
            "id": "Mei_et+al_2018_a",
            "entry": "Mei, Song, Montanari, Andrea, and Nguyen, Phan-Minh. A mean field view of the landscape of two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06561"
        },
        {
            "id": "Nguyen_0000_a",
            "entry": "Nguyen, Q. and Hein, M. The loss surface and expressivity of deep convolutional neural networks. arXiv preprint arXiv:1710.10928, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10928"
        },
        {
            "id": "Nguyen_0000_b",
            "entry": "Nguyen, Q. and Hein, M. The loss surface and expressivity of deep convolutional neural networks. arXiv preprint arXiv:1710.10928, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10928"
        },
        {
            "id": "Safran_2018_a",
            "entry": "Safran, Itay and Shamir, Ohad. Spurious local minima are common in two-layer relu neural networks. ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Spurious%20local%20minima%20are%20common%20in%20two-layer%20relu%20neural%20networks%202018"
        },
        {
            "id": "Sedghi_2014_a",
            "entry": "Sedghi, H. and Anandkumar, A. Provable methods for training neural networks with sparse connectivity. arXiv preprint arXiv:1412.2693, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.2693"
        },
        {
            "id": "Shamir_2018_a",
            "entry": "Shamir, O. Are resnets provably better than linear predictors? arXiv preprint arXiv:1804.06739, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06739"
        },
        {
            "id": "Soltanolkotabi_2017_a",
            "entry": "Soltanolkotabi, M. Learning relus via gradient descent. In NIPS, pp. 2004\u20132014, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20M.%20Learning%20relus%20via%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20M.%20Learning%20relus%20via%20gradient%20descent%202017"
        },
        {
            "id": "Soudry_2016_a",
            "entry": "Soudry, D. and Carmon, Y. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08361"
        },
        {
            "id": "Soudry_2017_a",
            "entry": "Soudry, D. and Hoffer, E. Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv preprint arXiv:1702.05777, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05777"
        },
        {
            "id": "Wan_et+al_2013_a",
            "entry": "Wan, L., Zeiler, M., Zhang, S., Le Cun, Y., and Fergus, R. Regularization of neural networks using dropconnect. In ICML, pp. 1058\u20131066, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wan%2C%20L.%20Zeiler%2C%20M.%20Zhang%2C%20S.%20Le%20Cun%2C%20Y.%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wan%2C%20L.%20Zeiler%2C%20M.%20Zhang%2C%20S.%20Le%20Cun%2C%20Y.%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013"
        },
        {
            "id": "Yun_et+al_2017_a",
            "entry": "Yun, C., Sra, S., and Jadbabaie, A. Global optimality conditions for deep neural networks. arXiv preprint arXiv:1707.02444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02444"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202016"
        },
        {
            "id": "Zhang_et+al_2012_a",
            "entry": "Zhang, X., Ling, C., and Qi, L. The best rank-1 approximation of a symmetric tensor and related spherical optimization problems. SIAM Journal on Matrix Analysis and Applications, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20X.%20Ling%2C%20C.%20Qi%2C%20L.%20The%20best%20rank-1%20approximation%20of%20a%20symmetric%20tensor%20and%20related%20spherical%20optimization%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20X.%20Ling%2C%20C.%20Qi%2C%20L.%20The%20best%20rank-1%20approximation%20of%20a%20symmetric%20tensor%20and%20related%20spherical%20optimization%20problems%202012"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Zhong, K., Song, Z., Jain, P., Bartlett, P. L, and Dhillon, I. S. Recovery guarantees for one-hiddenlayer neural networks. arXiv preprint arXiv:1706.03175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        }
    ]
}
