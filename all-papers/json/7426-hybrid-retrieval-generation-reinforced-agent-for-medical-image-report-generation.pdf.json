{
    "filename": "7426-hybrid-retrieval-generation-reinforced-agent-for-medical-image-report-generation.pdf",
    "metadata": {
        "title": "Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation",
        "author": "Yuan Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7426-hybrid-retrieval-generation-reinforced-agent-for-medical-image-report-generation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decisionmaking procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection precision of medical abnormality terminologies, and improved human evaluation performance."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "highlights": [
        "Beyond the traditional visual captioning task [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] that produces one single sentence, generating long and topic-coherent stories or reports to describe visual contents has recently attracted increasing research interests [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], posed as a more challenging and realistic goal towards bridging visual patterns with human linguistic descriptions",
        "Report generation has several challenges to be resolved: 1) The generated report is a long narrative consisting of multiple sentences or paragraphs, which must have a plausible logic and consistent topics; 2) There is a presumed content coverage and specific terminology/phrases, depending on the task at hand",
        "On CX-CHR, HRGR-Agent increases CIDEr score by 0.73 compared to HRG, demonstrating that reinforcement fine-tuning is crucial to performance increase since it directly optimizes the evaluation metric",
        "Retrieval surpasses Generation by relatively large margins, showing that retrievalbased method is beneficial to generating structured reports, which leads to boosted performance of HRGR-Agent when combined with neural generation approaches",
        "We introduce a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) to perform robust medical image report generation",
        "Experiments show that HRGR-Agent does not only achieve state-of-the-art performance on two medical image report datasets, but generates robust reports that has high precision on medical abnormal findings detection and best human preference"
    ],
    "key_statements": [
        "Beyond the traditional visual captioning task [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] that produces one single sentence, generating long and topic-coherent stories or reports to describe visual contents has recently attracted increasing research interests [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], posed as a more challenging and realistic goal towards bridging visual patterns with human linguistic descriptions",
        "Report generation has several challenges to be resolved: 1) The generated report is a long narrative consisting of multiple sentences or paragraphs, which must have a plausible logic and consistent topics; 2) There is a presumed content coverage and specific terminology/phrases, depending on the task at hand",
        "On CX-CHR, HRGR-Agent increases CIDEr score by 0.73 compared to HRG, demonstrating that reinforcement fine-tuning is crucial to performance increase since it directly optimizes the evaluation metric",
        "Retrieval surpasses Generation by relatively large margins, showing that retrievalbased method is beneficial to generating structured reports, which leads to boosted performance of HRGR-Agent when combined with neural generation approaches",
        "We introduce a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) to perform robust medical image report generation",
        "Experiments show that HRGR-Agent does not only achieve state-of-the-art performance on two medical image report datasets, but generates robust reports that has high precision on medical abnormal findings detection and best human preference"
    ],
    "summary": [
        "Beyond the traditional visual captioning task [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] that produces one single sentence, generating long and topic-coherent stories or reports to describe visual contents has recently attracted increasing research interests [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], posed as a more challenging and realistic goal towards bridging visual patterns with human linguistic descriptions.",
        "To enable effective and robust report generation, we jointly train the retrieval policy module and generation module via reinforcement learning (RL) [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] guided by sentence-level and word-level rewards, respectively.",
        "Figure 1 shows an example generated report by our HRGR-Agent which correctly describes \"a small effusion\" from the chest x-ray image, and successfully supports its finding by providing the appearance (\"blunting\") and location (\"costophrenic sulcus\") of the evidence.",
        "Our HRGR-Agent achieves the state-of-the-art performance on both datasets under three kinds of evaluation metrics: automatic metrics such as CIDEr [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], BLEU [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] and ROUGE [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], human evaluation, and detection precision of medical terminologies.",
        "Such sentences typically describe general observations, and are often inserted into medical reports, e.g., \"the heart size is normal\" and \"there is no pleural effusion or pneumothorax\".",
        "We set the maximum number of sentences of a report and maximum number of tokens in a sentence as 18 and 44 for CX-CHR and 7 and 15 for IU X-Ray. Besides, as observed from baseline models which overly predict most popular and normal reports for all testing samples and the fact that most medical reports describe normal cases, we add postprocessing to increase the length and comprehensiveness of the generated reports for both datasets while maintaining the design of HRGR-Agent to better predict abnormalities.",
        "As observed in our experiments, this step maintains the same medical abnormality term detection results, and improves the automatic report generation metrics, especially on BLEU-n metrics.",
        "Due to the relatively large size of CX-CHR, we conduct additional experiments on it to compare HRGR-Agent with its different variants by removing individual components (Retrieval, Generation, RL).",
        "Note that Retrieval uses the same model as HRG-Agent whose training involves automatic generation of sentences, the results of which may be higher than a general retrieval-based system.",
        "To better understand HRGR-Agent\u2019s performance, each generated report at testing has on average 7.2 and 4.8 sentences for CX-CHR and IU X-Ray dataset, respectively.",
        "The high medical abnormality term detection precision and low average false positive of HRGR-Agent verifies that its generation module learns to describe abnormal findings.",
        "Table 3 shows average human preference percentage of HRGR-Agent compared with Generation and CoAtt [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] on CX-CHR and IU X-Ray respectively, evaluated in terms of content coverage, specific terminology accuracy and language fluency.",
        "Experiments show that HRGR-Agent does not only achieve state-of-the-art performance on two medical image report datasets, but generates robust reports that has high precision on medical abnormal findings detection and best human preference."
    ],
    "headline": "We propose a novel Hybrid Retrieval-Generation Reinforced Agent  which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] \"jieba\" (chinese for \"to stutter\") chinese text segmentation: built to be the best python chinese word segmentation module. https://github.com/fxsjy/jieba. Accessed:2018-05-01.",
            "url": "https://github.com/fxsjy/jieba"
        },
        {
            "id": "2",
            "entry": "[2] C. D. M. Abigail See, Peter J. Liu. Get to the point: Summarization with pointer-generator networks. in ACL 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=See%2C%20C.D.M.Abigail%20Liu%2C%20Peter%20J.%20Get%20to%20the%20point%3A%20Summarization%20with%20pointer-generator%20networks.%20in%20ACL%202017"
        },
        {
            "id": "3",
            "entry": "[3] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An actor-critic algorithm for sequence prediction. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Brakel%2C%20P.%20Xu%2C%20K.%20Goyal%2C%20A.%20An%20actor-critic%20algorithm%20for%20sequence%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Brakel%2C%20P.%20Xu%2C%20K.%20Goyal%2C%20A.%20An%20actor-critic%20algorithm%20for%20sequence%20prediction%202017"
        },
        {
            "id": "4",
            "entry": "[4] S. Banerjee and A. Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20S.%20Lavie%2C%20A.%20Meteor%3A%20An%20automatic%20metric%20for%20mt%20evaluation%20with%20improved%20correlation%20with%20human%20judgments%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20S.%20Lavie%2C%20A.%20Meteor%3A%20An%20automatic%20metric%20for%20mt%20evaluation%20with%20improved%20correlation%20with%20human%20judgments%202005"
        },
        {
            "id": "5",
            "entry": "[5] J. M. Bosmans, J. J. Weyler, A. M. De Schepper, and P. M. Parizel. The radiology report as seen by radiologists and referring clinicians: results of the cover and rover surveys. Radiology, 259(1):184\u2013195, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bosmans%2C%20J.M.%20Weyler%2C%20J.J.%20Schepper%2C%20A.M.De%20Parizel%2C%20P.M.%20The%20radiology%20report%20as%20seen%20by%20radiologists%20and%20referring%20clinicians%3A%20results%20of%20the%20cover%20and%20rover%20surveys%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bosmans%2C%20J.M.%20Weyler%2C%20J.J.%20Schepper%2C%20A.M.De%20Parizel%2C%20P.M.%20The%20radiology%20report%20as%20seen%20by%20radiologists%20and%20referring%20clinicians%3A%20results%20of%20the%20cover%20and%20rover%20surveys%202011"
        },
        {
            "id": "6",
            "entry": "[6] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06349"
        },
        {
            "id": "7",
            "entry": "[7] P. Dayan and G. E. Hinton. Feudal reinforcement learning. In Advances in neural information processing systems, pages 271\u2013278, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20P.%20Hinton%2C%20G.E.%20Feudal%20reinforcement%20learning%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20P.%20Hinton%2C%20G.E.%20Feudal%20reinforcement%20learning%201993"
        },
        {
            "id": "8",
            "entry": "[8] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan, L. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald. Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Demner-Fushman%2C%20D.%20Kohli%2C%20M.D.%20Rosenman%2C%20M.B.%20Shooshan%2C%20S.E.%20Preparing%20a%20collection%20of%20radiology%20examinations%20for%20distribution%20and%20retrieval%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Demner-Fushman%2C%20D.%20Kohli%2C%20M.D.%20Rosenman%2C%20M.B.%20Shooshan%2C%20S.E.%20Preparing%20a%20collection%20of%20radiology%20examinations%20for%20distribution%20and%20retrieval%202015"
        },
        {
            "id": "9",
            "entry": "[9] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20J.%20Hendricks%2C%20L.Anne%20Guadarrama%2C%20S.%20Rohrbach%2C%20M.%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description.%20In%20CVPR%202015"
        },
        {
            "id": "10",
            "entry": "[10] S. K. Goergen, F. J. Pool, T. J. Turner, J. E. Grimm, M. N. Appleyard, C. Crock, M. C. Fahey, M. F. Fay, N. J. Ferris, S. M. Liew, et al. Evidence-based guideline for the written radiology report: Methods, recommendations and implementation challenges. Journal of medical imaging and radiation oncology, 57(1):1\u20137, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goergen%2C%20S.K.%20Pool%2C%20F.J.%20Turner%2C%20T.J.%20Grimm%2C%20J.E.%20Evidence-based%20guideline%20for%20the%20written%20radiology%20report%3A%20Methods%2C%20recommendations%20and%20implementation%20challenges%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goergen%2C%20S.K.%20Pool%2C%20F.J.%20Turner%2C%20T.J.%20Grimm%2C%20J.E.%20Evidence-based%20guideline%20for%20the%20written%20radiology%20report%3A%20Methods%2C%20recommendations%20and%20implementation%20challenges%202013"
        },
        {
            "id": "11",
            "entry": "[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "12",
            "entry": "[12] Y. Hong and C. E. Kahn. Content analysis of reporting templates and free-text radiology reports. Journal of digital imaging, 26(5):843\u2013849, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20Y.%20Kahn%2C%20C.E.%20Content%20analysis%20of%20reporting%20templates%20and%20free-text%20radiology%20reports%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20Y.%20Kahn%2C%20C.E.%20Content%20analysis%20of%20reporting%20templates%20and%20free-text%20radiology%20reports%202013"
        },
        {
            "id": "13",
            "entry": "[13] Z. Hu, H. Shi, Z. Yang, B. Tan, T. Zhao, J. He, W. Wang, X. Yu, L. Qin, D. Wang, et al. Texar: A modularized, versatile, and extensible toolkit for text generation. arXiv preprint arXiv:1809.00794, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.00794"
        },
        {
            "id": "14",
            "entry": "[14] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017"
        },
        {
            "id": "15",
            "entry": "[15] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20Weinberger%2C%20K.Q.%20van%20der%20Maaten%2C%20L.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20Weinberger%2C%20K.Q.%20van%20der%20Maaten%2C%20L.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] B. Jing, P. Xie, and E. Xing. On the automatic generation of medical imaging reports. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jing%2C%20B.%20Xie%2C%20P.%20Xing%2C%20E.%20On%20the%20automatic%20generation%20of%20medical%20imaging%20reports%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jing%2C%20B.%20Xie%2C%20P.%20Xing%2C%20E.%20On%20the%20automatic%20generation%20of%20medical%20imaging%20reports%202018"
        },
        {
            "id": "17",
            "entry": "[17] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20A.%20Fei-Fei%2C%20L.%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20A.%20Fei-Fei%2C%20L.%20Deep%20visual-semantic%20alignments%20for%20generating%20image%20descriptions%202015"
        },
        {
            "id": "18",
            "entry": "[18] L. Li and B. Gong. End-to-end video captioning with multitask reinforcement learning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20Gong%2C%20B.%20End-to-end%20video%20captioning%20with%20multitask%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20Gong%2C%20B.%20End-to-end%20video%20captioning%20with%20multitask%20reinforcement%20learning%202017"
        },
        {
            "id": "19",
            "entry": "[19] X. Liang, Z. Hu, H. Zhang, C. Gan, and E. P. Xing. Recurrent topic-transition gan for visual paragraph generation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20X.%20Hu%2C%20Z.%20Zhang%2C%20H.%20Gan%2C%20C.%20Recurrent%20topic-transition%20gan%20for%20visual%20paragraph%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20X.%20Hu%2C%20Z.%20Zhang%2C%20H.%20Gan%2C%20C.%20Recurrent%20topic-transition%20gan%20for%20visual%20paragraph%20generation%202017"
        },
        {
            "id": "20",
            "entry": "[20] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In ACL, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20C.-Y.%20Rouge%3A%20A%20package%20for%20automatic%20evaluation%20of%20summaries%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20C.-Y.%20Rouge%3A%20A%20package%20for%20automatic%20evaluation%20of%20summaries%202013"
        },
        {
            "id": "21",
            "entry": "[21] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy. Improved image captioning via policy gradient optimization of spider. In Proc. IEEE Int. Conf. Comp. Vis, volume 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20S.%20Zhu%2C%20Z.%20Ye%2C%20N.%20Guadarrama%2C%20S.%20Improved%20image%20captioning%20via%20policy%20gradient%20optimization%20of%20spider%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20S.%20Zhu%2C%20Z.%20Ye%2C%20N.%20Guadarrama%2C%20S.%20Improved%20image%20captioning%20via%20policy%20gradient%20optimization%20of%20spider%202017"
        },
        {
            "id": "22",
            "entry": "[22] Y. Liu, J. Fu, T. Mei, and C. W. Chen. Let your photos talk: Generating narrative paragraph for photo stream via bidirectional attention recurrent neural networks. In AAAI, pages 1445\u20131452, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Y.%20Fu%2C%20J.%20Mei%2C%20T.%20Chen%2C%20C.W.%20Let%20your%20photos%20talk%3A%20Generating%20narrative%20paragraph%20for%20photo%20stream%20via%20bidirectional%20attention%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Y.%20Fu%2C%20J.%20Mei%2C%20T.%20Chen%2C%20C.W.%20Let%20your%20photos%20talk%3A%20Generating%20narrative%20paragraph%20for%20photo%20stream%20via%20bidirectional%20attention%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "23",
            "entry": "[23] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20J.%20Xiong%2C%20C.%20Parikh%2C%20D.%20Socher%2C%20R.%20Knowing%20when%20to%20look%3A%20Adaptive%20attention%20via%20a%20visual%20sentinel%20for%20image%20captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20J.%20Xiong%2C%20C.%20Parikh%2C%20D.%20Socher%2C%20R.%20Knowing%20when%20to%20look%3A%20Adaptive%20attention%20via%20a%20visual%20sentinel%20for%20image%20captioning%202017"
        },
        {
            "id": "24",
            "entry": "[24] J. Lu, J. Yang, D. Batra, and D. Parikh. Neural baby talk. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Lu%20J%20Yang%20D%20Batra%20and%20D%20Parikh%20Neural%20baby%20talk%20In%20CVPR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Lu%20J%20Yang%20D%20Batra%20and%20D%20Parikh%20Neural%20baby%20talk%20In%20CVPR%202018"
        },
        {
            "id": "25",
            "entry": "[25] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.-J.%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.-J.%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "26",
            "entry": "[26] R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paulus%2C%20R.%20Xiong%2C%20C.%20Socher%2C%20R.%20A%20deep%20reinforced%20model%20for%20abstractive%20summarization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paulus%2C%20R.%20Xiong%2C%20C.%20Socher%2C%20R.%20A%20deep%20reinforced%20model%20for%20abstractive%20summarization%202018"
        },
        {
            "id": "27",
            "entry": "[27] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20M.%20Chopra%2C%20S.%20Auli%2C%20M.%20Zaremba%2C%20W.%20Sequence%20level%20training%20with%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20M.%20Chopra%2C%20S.%20Auli%2C%20M.%20Zaremba%2C%20W.%20Sequence%20level%20training%20with%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "28",
            "entry": "[28] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. Self-critical sequence training for image captioning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rennie%2C%20S.J.%20Marcheret%2C%20E.%20Mroueh%2C%20Y.%20Ross%2C%20J.%20Self-critical%20sequence%20training%20for%20image%20captioning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rennie%2C%20S.J.%20Marcheret%2C%20E.%20Mroueh%2C%20Y.%20Ross%2C%20J.%20Self-critical%20sequence%20training%20for%20image%20captioning%202017"
        },
        {
            "id": "29",
            "entry": "[29] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "30",
            "entry": "[30] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "31",
            "entry": "[31] B. Tan, Z. Hu, Z. Yang, R. Salakhutdinov, and E. Xing. Connecting the dots between mle and rl for text generation.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tan%2C%20B.%20Hu%2C%20Z.%20Yang%2C%20Z.%20Salakhutdinov%2C%20R.%20Connecting%20the%20dots%20between%20mle%20and%20rl%20for%20text%20generation"
        },
        {
            "id": "32",
            "entry": "[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017"
        },
        {
            "id": "33",
            "entry": "[33] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vedantam%2C%20R.%20Zitnick%2C%20C.Lawrence%20Parikh%2C%20D.%20Cider%3A%20Consensus-based%20image%20description%20evaluation.%20In%20CVPR%202015"
        },
        {
            "id": "34",
            "entry": "[34] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Toshev%2C%20A.%20Bengio%2C%20S.%20Erhan%2C%20D.%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Toshev%2C%20A.%20Bengio%2C%20S.%20Erhan%2C%20D.%20Show%20and%20tell%3A%20A%20neural%20image%20caption%20generator%202015"
        },
        {
            "id": "35",
            "entry": "[35] X. Wang, W. Chen, Y.-F. Wang, and W. Y. Wang. No metrics are perfect: Adversarial reward learning for visual storytelling. In ACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Chen%2C%20W.%20Wang%2C%20Y.-F.%20Wang%2C%20W.Y.%20No%20metrics%20are%20perfect%3A%20Adversarial%20reward%20learning%20for%20visual%20storytelling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Chen%2C%20W.%20Wang%2C%20Y.-F.%20Wang%2C%20W.Y.%20No%20metrics%20are%20perfect%3A%20Adversarial%20reward%20learning%20for%20visual%20storytelling%202018"
        },
        {
            "id": "36",
            "entry": "[36] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Peng%2C%20Y.%20Lu%2C%20L.%20Lu%2C%20Z.%20Chestx-ray8%3A%20Hospital-scale%20chest%20x-ray%20database%20and%20benchmarks%20on%20weakly-supervised%20classification%20and%20localization%20of%20common%20thorax%20diseases%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Peng%2C%20Y.%20Lu%2C%20L.%20Lu%2C%20Z.%20Chestx-ray8%3A%20Hospital-scale%20chest%20x-ray%20database%20and%20benchmarks%20on%20weakly-supervised%20classification%20and%20localization%20of%20common%20thorax%20diseases%202017"
        },
        {
            "id": "38",
            "entry": "[38] S. Wiseman, S. M. Shieber, and A. M. Rush. Challenges in data-to-document generation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiseman%2C%20S.%20Shieber%2C%20S.M.%20Rush%2C%20A.M.%20Challenges%20in%20data-to-document%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiseman%2C%20S.%20Shieber%2C%20S.M.%20Rush%2C%20A.M.%20Challenges%20in%20data-to-document%20generation%202017"
        },
        {
            "id": "39",
            "entry": "[39] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "40",
            "entry": "[40] Z. Y. Y. Y. Y. Wu and R. S. W. W. Cohen. Encode, review, and decode: Reviewer module for caption generation. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Z.Y.Y.Y.Y.%20Cohen%2C%20R.S.W.W.%20Encode%2C%20review%2C%20and%20decode%3A%20Reviewer%20module%20for%20caption%20generation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Z.Y.Y.Y.Y.%20Cohen%2C%20R.S.W.W.%20Encode%2C%20review%2C%20and%20decode%3A%20Reviewer%20module%20for%20caption%20generation%202016"
        },
        {
            "id": "41",
            "entry": "[41] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20K.%20Ba%2C%20J.%20Kiros%2C%20R.%20Cho%2C%20K.%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20K.%20Ba%2C%20J.%20Kiros%2C%20R.%20Cho%2C%20K.%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015"
        },
        {
            "id": "42",
            "entry": "[42] D. Yarats and M. Lewis. Hierarchical text generation and planning for strategic dialogue. In EMNLP, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarats%2C%20D.%20Lewis%2C%20M.%20Hierarchical%20text%20generation%20and%20planning%20for%20strategic%20dialogue%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarats%2C%20D.%20Lewis%2C%20M.%20Hierarchical%20text%20generation%20and%20planning%20for%20strategic%20dialogue%202017"
        },
        {
            "id": "43",
            "entry": "[43] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image captioning with semantic attention. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=You%2C%20Q.%20Jin%2C%20H.%20Wang%2C%20Z.%20Fang%2C%20C.%20Image%20captioning%20with%20semantic%20attention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=You%2C%20Q.%20Jin%2C%20H.%20Wang%2C%20Z.%20Fang%2C%20C.%20Image%20captioning%20with%20semantic%20attention%202016"
        },
        {
            "id": "44",
            "entry": "[44] S. L. Ziqiang Cao, Wenjie Li and F. Wei. Retrieve, rerank and rewrite: Soft template based neural summarization. In ACL, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20S.L.Ziqiang%20Li%2C%20Wenjie%20Wei%2C%20F.%20Retrieve%2C%20rerank%20and%20rewrite%3A%20Soft%20template%20based%20neural%20summarization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20S.L.Ziqiang%20Li%2C%20Wenjie%20Wei%2C%20F.%20Retrieve%2C%20rerank%20and%20rewrite%3A%20Soft%20template%20based%20neural%20summarization%202018"
        }
    ]
}
