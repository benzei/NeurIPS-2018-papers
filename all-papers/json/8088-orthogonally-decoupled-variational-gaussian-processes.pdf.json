{
    "filename": "8088-orthogonally-decoupled-variational-gaussian-processes.pdf",
    "metadata": {
        "date": 2018,
        "title": "Orthogonally Decoupled Variational Gaussian Processes",
        "author": "Hugh Salimbeni\u2217 Imperial College London",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8088-orthogonally-decoupled-variational-gaussian-processes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Gaussian processes (GPs) provide a powerful non-parametric framework for reasoning over functions. Despite appealing theory, its superlinear computational and memory complexities have presented a long-standing challenge. State-of-the-art sparse variational inference methods trade modeling accuracy against complexity. However, the complexities of these methods still scale superlinearly in the number of basis functions, implying that that sparse GP methods are able to learn from large datasets only when a small model is used. Recently, a decoupled approach was proposed that removes the unnecessary coupling between the complexities of modeling the mean and the covariance functions of a GP. It achieves a linear complexity in the number of mean parameters, so an expressive posterior mean function can be modeled. While promising, this approach suffers from optimization difficulties due to ill-conditioning and non-convexity. In this work, we propose an alternative decoupled parametrization. It adopts an orthogonal basis in the mean function to model the residues that cannot be learned by the standard coupled approach. Therefore, our method extends, rather than replaces, the coupled approach to achieve strictly better performance. This construction admits a straightforward natural gradient update rule, so the structure of the information manifold that is lost during decoupling can be leveraged to speed up learning. Empirically, our algorithm demonstrates significantly faster convergence in multiple experiments."
    },
    "keywords": [
        {
            "term": "latent function",
            "url": "https://en.wikipedia.org/wiki/latent_function"
        },
        {
            "term": "gaussian process regression",
            "url": "https://en.wikipedia.org/wiki/gaussian_process_regression"
        },
        {
            "term": "covariance function",
            "url": "https://en.wikipedia.org/wiki/covariance_function"
        },
        {
            "term": "mean absolute error",
            "url": "https://en.wikipedia.org/wiki/mean_absolute_error"
        },
        {
            "term": "gaussian processes",
            "url": "https://en.wikipedia.org/wiki/gaussian_processes"
        },
        {
            "term": "RKHS",
            "url": "https://en.wikipedia.org/wiki/RKHS"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        }
    ],
    "highlights": [
        "We consider the inference problem on a latent function f , the goal is of to iGnfPermtohdeetapsoetstDeri=or{o)}foNnr=a1nayndquaeGryPipnrpiourt x\u2217",
        "A common approach to approximate Gaussian processes inference is to form a sparse variational posterior, which is designed by conditioning the prior process at a small set of inducing points [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]",
        "Inspired by the success of natural gradients in variational inference [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], we propose a novel reproducing kernel Hilbert space parameterization of decoupled Gaussian processes that admits efficient natural gradient computation",
        "We first review the primal and dual representations of Gaussian processes, which form the foundation of the reproducing kernel Hilbert space reformulation",
        "A Gaussian processes is a distribution of functions, which is described by a mean function m : X \u2192 R and a covariance function k : X \u00d7 X \u2192 R",
        "We present a novel orthogonally decoupled basis for variational inference in Gaussian processes models"
    ],
    "key_statements": [
        "We consider the inference problem on a latent function f , the goal is of to iGnfPermtohdeetapsoetstDeri=or{o)}foNnr=a1nayndquaeGryPipnrpiourt x\u2217",
        "A common approach to approximate Gaussian processes inference is to form a sparse variational posterior, which is designed by conditioning the prior process at a small set of inducing points [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]",
        "Inspired by the success of natural gradients in variational inference [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], we propose a novel reproducing kernel Hilbert space parameterization of decoupled Gaussian processes that admits efficient natural gradient computation",
        "We decompose the mean parametrization into a part that shares the basis with the covariance, and an orthogonal part that models the residues that the standard coupled approach fails to capture. With this particular choice, the natural gradient update rules further decouple into the natural gradient descent of the coupled part and the functional gradient descent of the residual part",
        "We propose an efficient optimization algorithm that preserves the desired properties of decoupled Gaussian processes and converges faster than the original formulation [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We first review the primal and dual representations of Gaussian processes, which form the foundation of the reproducing kernel Hilbert space reformulation",
        "Let X \u2286 Rd be the domain of the latent function",
        "A Gaussian processes is a distribution of functions, which is described by a mean function m : X \u2192 R and a covariance function k : X \u00d7 X \u2192 R",
        "For Gaussian processes models, the variational posterior must be defined over the entire function, so a natural choice is to use another Gaussian processes",
        "Approximate Update Rule We described the natural gradient descent update for the orthogonally decoupled Gaussian processes in (12)",
        "We present a novel orthogonally decoupled basis for variational inference in Gaussian processes models",
        "We show how the natural parameters of our decoupled basis can be identified and propose an approximate natural gradient update rule, which significantly improves the optimization performance over original decoupled approach [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]"
    ],
    "summary": [
        "We consider the inference problem on a latent function f , the goal is of to iGnfPermtohdeetapsoetstDeri=or{o)}foNnr=a1nayndquaeGryPipnrpiourt x\u2217.",
        "A common approach to approximate GP inference is to form a sparse variational posterior, which is designed by conditioning the prior process at a small set of inducing points [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>].",
        "The sparse variational posterior by Titsias [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] turns out to be equivalent to a particular parameterization in the RKHS, where the mean and covariance both share the same basis.",
        "We decompose the mean parametrization into a part that shares the basis with the covariance, and an orthogonal part that models the residues that the standard coupled approach fails to capture.",
        "For GP models, the variational posterior must be defined over the entire function, so a natural choice is to use another GP.",
        "Inspired by the functional form of the exact solution in (3), Cheng and Boots [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] propose to model the approximate posterior GP in the decoupled subspace parametrization with \u03bc = \u03a8\u03b1a, \u03a3 = I + \u03a8\u03b2A\u03a8\u03b2",
        "One can show that the update for j\u03b2 and \u0398 is exactly the same as the natural gradient descent rule for the standard coupled basis [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], and that the update for the residue part j\u03b3 is equivalent to functional gradient descent [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] in the subspace orthogonal to the span of \u03a8\u03b2.",
        "Approximate Update Rule We described the natural gradient descent update for the orthogonally decoupled GPs in (12).",
        "We show that a) given fixed wall-clock time, the proposed orthogonally decoupled basis outperforms existing approaches; b) given the same number of inducing points for the covariance, our method almost always improves on the coupled approach; c) using natural gradients can dramatically improve performance, especially in regression.",
        "We compare updating our orthogonally decoupled basis with adaptive gradient descent using the Adam optimizer [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] (ORTH), and using the approximate natural gradient descent rule described in Section 3.4 (ORTHNAT).",
        "We used a computer with a Tesla K40 GPU and found that, in wall-clock time, the orthogonally decoupled basis with |\u03b3| = 3500, |\u03b2| = 1500 was equivalent to a coupled model with |\u03b2| = 2000 in our tensorflow [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] implementation.",
        "We compare to the coupled model with |\u03b2| = 300 to establish whether extra computation always improves the performance of the decoupled basis.",
        "We show how the natural parameters of our decoupled basis can be identified and propose an approximate natural gradient update rule, which significantly improves the optimization performance over original decoupled approach [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "Our method demonstrates strong performance in multiple regression and classification tasks"
    ],
    "headline": "We propose an alternative decoupled parametrization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. TensorFlow: A system for large-scale machine learning. In Symposium on Operating Systems Design and Implementation, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] S.-I. Amari. Natural gradient works efficiently in learning. Neural Computation, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "3",
            "entry": "[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20N.%20Theory%20of%20reproducing%20kernels%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aronszajn%2C%20N.%20Theory%20of%20reproducing%20kernels%201950"
        },
        {
            "id": "4",
            "entry": "[4] C.-A. Cheng and B. Boots. Incremental variational sparse Gaussian process regression. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Incremental%20variational%20sparse%20Gaussian%20process%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Incremental%20variational%20sparse%20Gaussian%20process%20regression%202016"
        },
        {
            "id": "5",
            "entry": "[5] C.-A. Cheng and B. Boots. Variational inference for Gaussian process models with linear complexity. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Variational%20inference%20for%20Gaussian%20process%20models%20with%20linear%20complexity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Variational%20inference%20for%20Gaussian%20process%20models%20with%20linear%20complexity%202017"
        },
        {
            "id": "6",
            "entry": "[6] M. P. Deisenroth and J. W. Ng. Distributed Gaussian processes. In International Conference on Machine",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.P.%20Ng%2C%20J.W.%20Distributed%20Gaussian%20processes",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.P.%20Ng%2C%20J.W.%20Distributed%20Gaussian%20processes"
        },
        {
            "id": "7",
            "entry": "[7] M. P. Deisenroth and C. E. Rasmussen. PILCO: a model-based and data-efficient approach to policy search. In International Conference on Machine Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.P.%20Rasmussen%2C%20C.E.%20PILCO%3A%20a%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.P.%20Rasmussen%2C%20C.E.%20PILCO%3A%20a%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011"
        },
        {
            "id": "8",
            "entry": "[8] P. J. Diggle and P. J. Ribeiro. Model-based Geostatistics. Springer, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diggle%2C%20P.J.%20Ribeiro%2C%20P.J.%20Model-based%20Geostatistics%202007"
        },
        {
            "id": "9",
            "entry": "[9] J. R. Gardner, G. Pleiss, R. Wu, K. Q. Weinberger, and A. G. W. Wilson. Product kernel interpolation for scalable Gaussian processes. In International Conference on Artificial Intelligence and Statistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gardner%2C%20J.R.%20Pleiss%2C%20G.%20Wu%2C%20R.%20Weinberger%2C%20K.Q.%20Product%20kernel%20interpolation%20for%20scalable%20Gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gardner%2C%20J.R.%20Pleiss%2C%20G.%20Wu%2C%20R.%20Weinberger%2C%20K.Q.%20Product%20kernel%20interpolation%20for%20scalable%20Gaussian%20processes%202018"
        },
        {
            "id": "10",
            "entry": "[10] J. Hartikainen and S. S\u00e4rkk\u00e4. Kalman filtering and smoothing solutions to temporal Gaussian process regression models. In IEEE International Workshop on Machine Learning for Signal Processing, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartikainen%2C%20J.%20S%C3%A4rkk%C3%A4%2C%20S.%20Kalman%20filtering%20and%20smoothing%20solutions%20to%20temporal%20Gaussian%20process%20regression%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hartikainen%2C%20J.%20S%C3%A4rkk%C3%A4%2C%20S.%20Kalman%20filtering%20and%20smoothing%20solutions%20to%20temporal%20Gaussian%20process%20regression%20models%202010"
        },
        {
            "id": "11",
            "entry": "[11] M. Havasi, J. M. Hern\u00e1ndez-Lobato, and J. J. Murillo-Fuentes. Deep Gaussian processes with decoupled inducing inputs. arXiv:1801.02939, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02939"
        },
        {
            "id": "12",
            "entry": "[12] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Fusi%2C%20N.%20Lawrence%2C%20N.D.%20Gaussian%20processes%20for%20big%20data%202013"
        },
        {
            "id": "13",
            "entry": "[13] J. Hensman, N. Durrande, and A. Solin. Variational Fourier features for Gaussian processes. Journal of Machine Learning Research, 18:1\u201352, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Durrande%2C%20N.%20Solin%2C%20A.%20Variational%20Fourier%20features%20for%20Gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Durrande%2C%20N.%20Solin%2C%20A.%20Variational%20Fourier%20features%20for%20Gaussian%20processes%202018"
        },
        {
            "id": "14",
            "entry": "[14] D. Hern\u00e1ndez-Lobato, J. M. Hern\u00e1ndez-Lobato, and P. Dupont. Robust multi-class Gaussian process classification. In Advances in Neural Information Processing Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Dupont%2C%20P.%20Robust%20multi-class%20Gaussian%20process%20classification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Dupont%2C%20P.%20Robust%20multi-class%20Gaussian%20process%20classification%202011"
        },
        {
            "id": "15",
            "entry": "[15] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "16",
            "entry": "[16] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "17",
            "entry": "[17] J. Kivinen, A. J. Smola, and R. C. Williamson. Online learning with kernels. IEEE Transactions on Signal Processing, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kivinen%2C%20J.%20Smola%2C%20A.J.%20Williamson%2C%20R.C.%20Online%20learning%20with%20kernels%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kivinen%2C%20J.%20Smola%2C%20A.J.%20Williamson%2C%20R.C.%20Online%20learning%20with%20kernels%202004"
        },
        {
            "id": "18",
            "entry": "[18] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter. Self-normalizing neural networks. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klambauer%2C%20G.%20Unterthiner%2C%20T.%20Mayr%2C%20A.%20Hochreiter%2C%20S.%20Self-normalizing%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klambauer%2C%20G.%20Unterthiner%2C%20T.%20Mayr%2C%20A.%20Hochreiter%2C%20S.%20Self-normalizing%20neural%20networks%202017"
        },
        {
            "id": "19",
            "entry": "[19] M. L\u00e1zaro-Gredilla, J. Qui\u00f1onero-Candela, C. E. Rasmussen, and A. R. Figueiras-Vidal. Sparse spectrum Gaussian process regression. Journal of Machine Learning Research, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L%C3%A1zaro-Gredilla%2C%20M.%20Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20Figueiras-Vidal%2C%20A.R.%20Sparse%20spectrum%20Gaussian%20process%20regression%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L%C3%A1zaro-Gredilla%2C%20M.%20Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20Figueiras-Vidal%2C%20A.R.%20Sparse%20spectrum%20Gaussian%20process%20regression%202010"
        },
        {
            "id": "20",
            "entry": "[20] J. Martens. New insights and perspectives on the natural gradient method. arXiv:1412.1193, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.1193"
        },
        {
            "id": "21",
            "entry": "[21] A. G. Matthews, M. Van Der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. Le\u00f3n-Villagr\u00e1, Z. Ghahramani, and J. Hensman. GPflow: A Gaussian process library using TensorFlow. Journal of Machine Learning Research, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthews%2C%20A.G.%20Wilk%2C%20M.Van%20Der%20Nickson%2C%20T.%20Fujii%2C%20K.%20GPflow%3A%20A%20Gaussian%20process%20library%20using%20TensorFlow%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matthews%2C%20A.G.%20Wilk%2C%20M.Van%20Der%20Nickson%2C%20T.%20Fujii%2C%20K.%20GPflow%3A%20A%20Gaussian%20process%20library%20using%20TensorFlow%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. G. d. G. Matthews. Scalable Gaussian process inference using variational methods. PhD thesis, Camrbidge Univeristy, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d.%20G.%20Matthews%2C%20A.G.%20Scalable%20Gaussian%20process%20inference%20using%20variational%20methods%202017"
        },
        {
            "id": "23",
            "entry": "[23] A. G. d. G. Matthews, J. Hensman, R. Turner, and Z. Ghahramani. On sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In International Conference on Artificial Intelligence and Statistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d.%20G.%20Matthews%2C%20A.G.%20Hensman%2C%20J.%20Turner%2C%20R.%20Ghahramani%2C%20Z.%20On%20sparse%20variational%20methods%20and%20the%20Kullback-Leibler%20divergence%20between%20stochastic%20processes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=d.%20G.%20Matthews%2C%20A.G.%20Hensman%2C%20J.%20Turner%2C%20R.%20Ghahramani%2C%20Z.%20On%20sparse%20variational%20methods%20and%20the%20Kullback-Leibler%20divergence%20between%20stochastic%20processes%202016"
        },
        {
            "id": "24",
            "entry": "[24] T. Nguyen and E. Bonilla. Fast allocation of Gaussian process experts. In International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20T.%20Bonilla%2C%20E.%20Fast%20allocation%20of%20Gaussian%20process%20experts%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20T.%20Bonilla%2C%20E.%20Fast%20allocation%20of%20Gaussian%20process%20experts%202014"
        },
        {
            "id": "25",
            "entry": "[25] J. Qui\u00f1onero-Candela, C. E. Rasmussen, and R. Herbrich. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20Herbrich%2C%20R.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20Herbrich%2C%20R.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005"
        },
        {
            "id": "26",
            "entry": "[26] C. E. Rasmussen and Z. Ghahramani. Infinite mixtures of Gaussian process experts. In Advances in Neural Information Processing Systems, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Ghahramani%2C%20Z.%20Infinite%20mixtures%20of%20Gaussian%20process%20experts%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20C.E.%20Ghahramani%2C%20Z.%20Infinite%20mixtures%20of%20Gaussian%20process%20experts%202002"
        },
        {
            "id": "27",
            "entry": "[27] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "28",
            "entry": "[28] D. Rulli\u00e8re, N. Durrande, F. Bachoc, and C. Chevalier. Nested Kriging predictions for datasets with a large number of observations. Statistics and Computing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rulli%C3%A8re%2C%20D.%20Durrande%2C%20N.%20Bachoc%2C%20F.%20Chevalier%2C%20C.%20Nested%20Kriging%20predictions%20for%20datasets%20with%20a%20large%20number%20of%20observations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rulli%C3%A8re%2C%20D.%20Durrande%2C%20N.%20Bachoc%2C%20F.%20Chevalier%2C%20C.%20Nested%20Kriging%20predictions%20for%20datasets%20with%20a%20large%20number%20of%20observations%202018"
        },
        {
            "id": "29",
            "entry": "[29] H. Salimbeni, S. Eleftheriadis, and J. Hensman. Natural gradients in practice: Non-conjugate variational inference in Gaussian process models. In International Conference on Artificial Intelligence and Statistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimbeni%2C%20H.%20Eleftheriadis%2C%20S.%20Hensman%2C%20J.%20Natural%20gradients%20in%20practice%3A%20Non-conjugate%20variational%20inference%20in%20Gaussian%20process%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimbeni%2C%20H.%20Eleftheriadis%2C%20S.%20Hensman%2C%20J.%20Natural%20gradients%20in%20practice%3A%20Non-conjugate%20variational%20inference%20in%20Gaussian%20process%20models%202018"
        },
        {
            "id": "30",
            "entry": "[30] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs%202006"
        },
        {
            "id": "31",
            "entry": "[31] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "32",
            "entry": "[32] M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In International Conference on Artificial Intelligence and Statistics, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20M.%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20M.%20Variational%20learning%20of%20inducing%20variables%20in%20sparse%20Gaussian%20processes%202009"
        },
        {
            "id": "33",
            "entry": "[33] V. Tresp. A Bayesian committee machine. Neural computation, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tresp%2C%20V.%20A%20Bayesian%20committee%20machine%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tresp%2C%20V.%20A%20Bayesian%20committee%20machine%202000"
        },
        {
            "id": "34",
            "entry": "[34] A. Wilson and H. Nickisch. Kernel interpolation for scalable structured Gaussian processes (KISS-GP). In International Conference on Machine Learning, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.%20Nickisch%2C%20H.%20Kernel%20interpolation%20for%20scalable%20structured%20Gaussian%20processes%20%28KISS-GP%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.%20Nickisch%2C%20H.%20Kernel%20interpolation%20for%20scalable%20structured%20Gaussian%20processes%20%28KISS-GP%29%202015"
        }
    ]
}
