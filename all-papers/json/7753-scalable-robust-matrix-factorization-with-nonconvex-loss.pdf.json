{
    "filename": "7753-scalable-robust-matrix-factorization-with-nonconvex-loss.pdf",
    "metadata": {
        "title": "Scalable Robust Matrix Factorization with Nonconvex Loss",
        "author": "Quanming Yao, James Kwok",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7753-scalable-robust-matrix-factorization-with-nonconvex-loss.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Matrix factorization (MF), which uses the 2-loss, and robust matrix factorization (RMF), which uses the 1-loss, are sometimes not robust enough for outliers. Moreover, even the state-of-the-art RMF solver (RMF-MM) is slow and cannot utilize data sparsity. In this paper, we propose to improve robustness by using nonconvex loss functions. The resultant optimization problem is difficult. To improve efficiency and scalability, we use majorization-minimization (MM) and optimize the MM surrogate by using the accelerated proximal gradient algorithm on its dual problem. Data sparsity can also be exploited. The resultant algorithm has low time and space complexities, and is guaranteed to converge to a critical point. Extensive experiments show that it outperforms the state-of-the-art in terms of both accuracy and speed."
    },
    "keywords": [
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "matrix factorization",
            "url": "https://en.wikipedia.org/wiki/matrix_factorization"
        },
        {
            "term": "proximal gradient",
            "url": "https://en.wikipedia.org/wiki/proximal_gradient"
        },
        {
            "term": "mean absolute error",
            "url": "https://en.wikipedia.org/wiki/mean_absolute_error"
        },
        {
            "term": "alternating direction method of multipliers",
            "url": "https://en.wikipedia.org/wiki/alternating_direction_method_of_multipliers"
        }
    ],
    "highlights": [
        "Matrix factorization (MF) is a fundamental tool in machine learning, and an important component in many applications such as computer vision [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>], social networks [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] and recommender systems [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "robust matrix factorization-MM uses the convex 1 loss, and only needs to handle nonconvexity resulting from the product U V in (1)",
        "We propose a timeand space-efficient optimization procedure based on running the accelerated proximal gradient (APG) algorithm on the surrogate optimization problem\u2019s dual",
        "Theoretical analysis shows that the proposed RMFNL algorithm generates a critical point",
        "Extensive experiments on both synthetic and real-world data sets demonstrate that RMFNL is more accurate and more scalable than the state-of-the-art"
    ],
    "key_statements": [
        "Matrix factorization (MF) is a fundamental tool in machine learning, and an important component in many applications such as computer vision [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>], social networks [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] and recommender systems [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "robust matrix factorization-MM uses the convex 1 loss, and only needs to handle nonconvexity resulting from the product U V in (1)",
        "We propose a timeand space-efficient optimization procedure based on running the accelerated proximal gradient (APG) algorithm on the surrogate optimization problem\u2019s dual",
        "Theoretical analysis shows that the proposed RMFNL algorithm generates a critical point",
        "Extensive experiments on both synthetic and real-world data sets demonstrate that RMFNL is more accurate and more scalable than the state-of-the-art"
    ],
    "summary": [
        "Matrix factorization (MF) is a fundamental tool in machine learning, and an important component in many applications such as computer vision [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>], social networks [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] and recommender systems [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>].",
        "Eriksson and van den Hengel [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] proposed robust matrix factorization (RMF), which uses the 1-loss, and obtains much better empirical performance.",
        "In [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], it is replaced by the 1-loss, leading to robust matrix factorization (RMF): min W",
        "RMF-MM uses the convex 1 loss, and only needs to handle nonconvexity resulting from the product U V in (1).",
        "We propose a timeand space-efficient optimization procedure based on running the accelerated proximal gradient (APG) algorithm on the surrogate optimization problem\u2019s dual.",
        "Problem (5) can be solved by the APG algorithm, which has a convergence rate of O(1/T 2) [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and is faster than LADMPSAP.",
        "Algorithm 1 Robust matrix factorization using nonconvex loss (RMFNL) algorithm.",
        "By exploiting sparsity, the APG algorithm has a space complexity of O + (m + n)r) and iteration time complexity of Or + (m + n)r).",
        "We use the nonconvex loss functions of LSP, Geman and Laplace in Table 5 of Appendix A, with \u03b8 = 1; and fix \u03bb = 20/(m + n) in (1) as suggested in [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "We compare three solvers for surrogate optimization in each RMFNL iteration: (i) LADMPSAP in RMF-MM; APG, which uses APG but without utilizing data sparsity; and APG in Algorithm 1, which utilizes data sparsity as in Section 3.3.",
        "Table 1 shows performance of the whole RMFNL algorithm with different surrogate optimizers.1 As can be seen, the various nonconvex losses (LSP, Geman and Laplace) lead to similar RMSE\u2019s, as has been observed in [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>].",
        "The 2-loss-based MF algorithms that will be compared include alternating gradient descent (AltGrad) [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], Riemannian preconditioning (RP) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], scaled alternating steepest descent (ScaledASD) [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], alternative minimization for large scale matrix imputing (ALT-Impute) [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] and online massive dictionary learning (OMDL) [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>].",
        "As can be seen from Table 2, RMFNL produces much lower RMSE than the MF/RMF algorithms, and the RMSEs from different nonconvex losses are similar.",
        "RMFNL with different nonconvex losses have similar convergence behavior, and they all converge to a lower testing RMSE much faster than the others.",
        "RMFNL with different nonconvex losses have similar performance and achieve the lowest RMSE.",
        "As there is no ground-truth, we follow [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] and use the (i) mean absolute error (MAE) W (U V \u2212 X) 1/nnz(W ), where Uand Vare outputs from the algorithm, X is the data matrix with observed positions indicated by the binary W ; and CPU time.",
        "Extensive experiments on both synthetic and real-world data sets demonstrate that RMFNL is more accurate and more scalable than the state-of-the-art"
    ],
    "headline": "We propose to improve robustness by using nonconvex loss functions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. Basri, D. Jacobs, and I. Kemelmacher. Photometric stereo with general, unknown lighting. International Journal of Computer Vision, 72(3):239\u2013257, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Basri%2C%20R.%20Jacobs%2C%20D.%20Kemelmacher%2C%20I.%20Photometric%20stereo%20with%20general%2C%20unknown%20lighting%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Basri%2C%20R.%20Jacobs%2C%20D.%20Kemelmacher%2C%20I.%20Photometric%20stereo%20with%20general%2C%20unknown%20lighting%202007"
        },
        {
            "id": "2",
            "entry": "[2] M. Beck, A.and Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20M.%20Teboulle%2C%20Aand%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20M.%20Teboulle%2C%20Aand%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "3",
            "entry": "[3] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1\u2013122, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Parikh%2C%20N.%20Chu%2C%20E.%20Peleato%2C%20B.%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20S.%20Parikh%2C%20N.%20Chu%2C%20E.%20Peleato%2C%20B.%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011"
        },
        {
            "id": "4",
            "entry": "[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20Optimization%202004"
        },
        {
            "id": "5",
            "entry": "[5] R. Burke, M. O\u2019Mahony, and N. Hurley. Recommender Systems Handbook. Springer, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burke%2C%20R.%20O%E2%80%99Mahony%2C%20M.%20Hurley%2C%20N.%20Recommender%20Systems%20Handbook%202015"
        },
        {
            "id": "6",
            "entry": "[6] R. Cabral, F. De la Torre, J. Costeira, and A. Bernardino. Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition. In International Conference on Computer Vision, pages 2488\u20132495, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cabral%2C%20R.%20la%20Torre%2C%20F.De%20Costeira%2C%20J.%20Bernardino%2C%20A.%20Unifying%20nuclear%20norm%20and%20bilinear%20factorization%20approaches%20for%20low-rank%20matrix%20decomposition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cabral%2C%20R.%20la%20Torre%2C%20F.De%20Costeira%2C%20J.%20Bernardino%2C%20A.%20Unifying%20nuclear%20norm%20and%20bilinear%20factorization%20approaches%20for%20low-rank%20matrix%20decomposition%202013"
        },
        {
            "id": "7",
            "entry": "[7] L. Cambier and P. Absil. Robust low-rank matrix completion by Riemannian optimization. SIAM Journal on Scientific Computing, 38(5):S440\u2013S460, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cambier%2C%20L.%20Absil%2C%20P.%20Robust%20low-rank%20matrix%20completion%20by%20Riemannian%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cambier%2C%20L.%20Absil%2C%20P.%20Robust%20low-rank%20matrix%20completion%20by%20Riemannian%20optimization%202016"
        },
        {
            "id": "8",
            "entry": "[8] E.J. Cand\u00e8s and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717\u2013772, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20E.J.%20Recht%2C%20B.%20Exact%20matrix%20completion%20via%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20E.J.%20Recht%2C%20B.%20Exact%20matrix%20completion%20via%20convex%20optimization%202009"
        },
        {
            "id": "9",
            "entry": "[9] E.J. Cand\u00e8s, M.B. Wakin, and S. Boyd. Enhancing sparsity by reweighted 1 minimization. Journal of Fourier Analysis and Applications, 14(5-6):877\u2013905, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20E.J.%20Wakin%2C%20M.B.%20Boyd%2C%20S.%20Enhancing%20sparsity%20by%20reweighted%201%20minimization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20E.J.%20Wakin%2C%20M.B.%20Boyd%2C%20S.%20Enhancing%20sparsity%20by%20reweighted%201%20minimization%202008"
        },
        {
            "id": "10",
            "entry": "[10] F. Clarke. Optimization and Nonsmooth Analysis. SIAM, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarke%2C%20F.%20Optimization%20and%20Nonsmooth%20Analysis%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarke%2C%20F.%20Optimization%20and%20Nonsmooth%20Analysis%201990"
        },
        {
            "id": "11",
            "entry": "[11] F. De La Torre and M. Black. A framework for robust subspace learning. International Journal of Computer Vision, 54(1):117\u2013142, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Torre%2C%20F.De%20La%20Black%2C%20M.%20A%20framework%20for%20robust%20subspace%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Torre%2C%20F.De%20La%20Black%2C%20M.%20A%20framework%20for%20robust%20subspace%20learning%202003"
        },
        {
            "id": "12",
            "entry": "[12] A. Eriksson and A. Van Den Hengel. Efficient computation of robust low-rank matrix approximations in the presence of missing data using the 1-norm. In International Conference on Computer Vision and Pattern Recognition, pages 771\u2013778, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eriksson%2C%20A.%20Hengel%2C%20A.Van%20Den%20Efficient%20computation%20of%20robust%20low-rank%20matrix%20approximations%20in%20the%20presence%20of%20missing%20data%20using%20the%201-norm%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eriksson%2C%20A.%20Hengel%2C%20A.Van%20Den%20Efficient%20computation%20of%20robust%20low-rank%20matrix%20approximations%20in%20the%20presence%20of%20missing%20data%20using%20the%201-norm%202010"
        },
        {
            "id": "13",
            "entry": "[13] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20J.%20Li%2C%20R.%20Variable%20selection%20via%20nonconcave%20penalized%20likelihood%20and%20its%20oracle%20properties%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20J.%20Li%2C%20R.%20Variable%20selection%20via%20nonconcave%20penalized%20likelihood%20and%20its%20oracle%20properties%202001"
        },
        {
            "id": "14",
            "entry": "[14] D. Geman and C. Yang. Nonlinear image recovery with half-quadratic regularization. IEEE Transactions on Image Processing, 4(7):932\u2013946, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geman%2C%20D.%20Yang%2C%20C.%20Nonlinear%20image%20recovery%20with%20half-quadratic%20regularization%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geman%2C%20D.%20Yang%2C%20C.%20Nonlinear%20image%20recovery%20with%20half-quadratic%20regularization%201995"
        },
        {
            "id": "15",
            "entry": "[15] P. Gong and J. Ye. HONOR: Hybrid optimization for non-convex regularized problems. In Advance in Neural Information Processing Systems, pages 415\u2013423, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20P.%20Ye%2C%20J.%20HONOR%3A%20Hybrid%20optimization%20for%20non-convex%20regularized%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20P.%20Ye%2C%20J.%20HONOR%3A%20Hybrid%20optimization%20for%20non-convex%20regularized%20problems%202015"
        },
        {
            "id": "16",
            "entry": "[16] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. In International Conference on Machine Learning, pages 37\u201345, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20P.%20Zhang%2C%20C.%20Lu%2C%20Z.%20Huang%2C%20J.%20A%20general%20iterative%20shrinkage%20and%20thresholding%20algorithm%20for%20non-convex%20regularized%20optimization%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20P.%20Zhang%2C%20C.%20Lu%2C%20Z.%20Huang%2C%20J.%20A%20general%20iterative%20shrinkage%20and%20thresholding%20algorithm%20for%20non-convex%20regularized%20optimization%20problems%202013"
        },
        {
            "id": "17",
            "entry": "[17] T. Hastie, R. Mazumder, J. Lee, and R. Zadeh. Matrix completion and low-rank SVD via fast alternating least squares. Journal of Machine Learning Research, 16:3367\u20133402, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20T.%20Mazumder%2C%20R.%20Lee%2C%20J.%20Zadeh%2C%20R.%20Matrix%20completion%20and%20low-rank%20SVD%20via%20fast%20alternating%20least%20squares%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastie%2C%20T.%20Mazumder%2C%20R.%20Lee%2C%20J.%20Zadeh%2C%20R.%20Matrix%20completion%20and%20low-rank%20SVD%20via%20fast%20alternating%20least%20squares%202015"
        },
        {
            "id": "18",
            "entry": "[18] J. He, L. Balzano, and A. Szlam. Incremental gradient on the Grassmannian for online foreground and background separation in subsampled video. In Computer Vision and Pattern Recognition, pages 1568\u20131575, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20J.%20Balzano%2C%20L.%20Szlam%2C%20A.%20Incremental%20gradient%20on%20the%20Grassmannian%20for%20online%20foreground%20and%20background%20separation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20J.%20Balzano%2C%20L.%20Szlam%2C%20A.%20Incremental%20gradient%20on%20the%20Grassmannian%20for%20online%20foreground%20and%20background%20separation%202012"
        },
        {
            "id": "19",
            "entry": "[19] P. Huber. Robust Statistics. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huber%2C%20P.%20Robust%20Statistics%202011"
        },
        {
            "id": "20",
            "entry": "[20] D. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician, 58(1):30\u201337, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hunter%2C%20D.%20Lange%2C%20K.%20A%20tutorial%20on%20MM%20algorithms%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hunter%2C%20D.%20Lange%2C%20K.%20A%20tutorial%20on%20MM%20algorithms%202004"
        },
        {
            "id": "21",
            "entry": "[21] W. Jiang, F. Nie, and H. Huang. Robust dictionary learning with capped 1-norm. In International Joint Conference on Artificial Intelligence, pages 3590\u20133596, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20W.%20Nie%2C%20F.%20Huang%2C%20H.%20Robust%20dictionary%20learning%20with%20capped%201-norm%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20W.%20Nie%2C%20F.%20Huang%2C%20H.%20Robust%20dictionary%20learning%20with%20capped%201-norm%202015"
        },
        {
            "id": "22",
            "entry": "[22] E. Kim, M. Lee, C. Choi, N. Kwak, and S. Oh. Efficient 1-norm-based low-rank matrix approximations for large-scale problems using alternating rectified gradient method. IEEE Transactions on Neural Networks and Learning Systems, 26(2):237\u2013251, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20E.%20Lee%2C%20M.%20Choi%2C%20C.%20Kwak%2C%20N.%20Efficient%201-norm-based%20low-rank%20matrix%20approximations%20for%20large-scale%20problems%20using%20alternating%20rectified%20gradient%20method%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20E.%20Lee%2C%20M.%20Choi%2C%20C.%20Kwak%2C%20N.%20Efficient%201-norm-based%20low-rank%20matrix%20approximations%20for%20large-scale%20problems%20using%20alternating%20rectified%20gradient%20method%202015"
        },
        {
            "id": "23",
            "entry": "[23] J. Koenderink and A. Van Doorn. Affine structure from motion. Journal of the Optical Society of America, 8(2):377\u2013385, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koenderink%2C%20J.%20Doorn%2C%20A.Van%20Affine%20structure%20from%20motion%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koenderink%2C%20J.%20Doorn%2C%20A.Van%20Affine%20structure%20from%20motion%201991"
        },
        {
            "id": "24",
            "entry": "[24] K. Lange, R. Hunter, and I. Yang. Optimization transfer using surrogate objective functions. Journal of Computational and Graphical Statistics, 9(1):1\u201320, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lange%2C%20K.%20Hunter%2C%20R.%20Yang%2C%20I.%20Optimization%20transfer%20using%20surrogate%20objective%20functions%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lange%2C%20K.%20Hunter%2C%20R.%20Yang%2C%20I.%20Optimization%20transfer%20using%20surrogate%20objective%20functions%202000"
        },
        {
            "id": "25",
            "entry": "[25] Z. Lin, R. Liu, and H. Li. Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning. Machine Learning, 2(99):287\u2013325, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Z.%20Liu%2C%20R.%20Li%2C%20H.%20Linearized%20alternating%20direction%20method%20with%20parallel%20splitting%20and%20adaptive%20penalty%20for%20separable%20convex%20programs%20in%20machine%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Z.%20Liu%2C%20R.%20Li%2C%20H.%20Linearized%20alternating%20direction%20method%20with%20parallel%20splitting%20and%20adaptive%20penalty%20for%20separable%20convex%20programs%20in%20machine%20learning%202015"
        },
        {
            "id": "26",
            "entry": "[26] Z. Lin, C. Xu, and H. Zha. Robust matrix factorization by majorization minimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, (99), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Z.%20Xu%2C%20C.%20Zha%2C%20H.%20Robust%20matrix%20factorization%20by%20majorization%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Z.%20Xu%2C%20C.%20Zha%2C%20H.%20Robust%20matrix%20factorization%20by%20majorization%20minimization%202017"
        },
        {
            "id": "27",
            "entry": "[27] D. Meng, Z. Xu, L. Zhang, and J. Zhao. A cyclic weighted median method for 1 low-rank matrix factorization with missing entries. In AAAI Conference on Artificial Intelligence, pages 704\u2013710, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20D.%20Xu%2C%20Z.%20Zhang%2C%20L.%20Zhao%2C%20J.%20A%20cyclic%20weighted%20median%20method%20for%201%20low-rank%20matrix%20factorization%20with%20missing%20entries%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20D.%20Xu%2C%20Z.%20Zhang%2C%20L.%20Zhao%2C%20J.%20A%20cyclic%20weighted%20median%20method%20for%201%20low-rank%20matrix%20factorization%20with%20missing%20entries%202013"
        },
        {
            "id": "28",
            "entry": "[28] A. Mensch, J. Mairal, B. Thirion, and G. Varoquaux. Dictionary learning for massive matrix factorization. In International Conference on Machine Learning, pages 1737\u20131746, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mensch%2C%20A.%20Mairal%2C%20J.%20Thirion%2C%20B.%20Varoquaux%2C%20G.%20Dictionary%20learning%20for%20massive%20matrix%20factorization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mensch%2C%20A.%20Mairal%2C%20J.%20Thirion%2C%20B.%20Varoquaux%2C%20G.%20Dictionary%20learning%20for%20massive%20matrix%20factorization%202016"
        },
        {
            "id": "29",
            "entry": "[29] B. Mishra and R. Sepulchre. Riemannian preconditioning. SIAM Journal on Optimization, 26(1):635\u2013660, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20B.%20Sepulchre%2C%20R.%20Riemannian%20preconditioning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20B.%20Sepulchre%2C%20R.%20Riemannian%20preconditioning%202016"
        },
        {
            "id": "30",
            "entry": "[30] A. Mnih and R. Salakhutdinov. Probabilistic matrix factorization. In Advance in Neural Information Processing Systems, pages 1257\u20131264, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20A.%20Salakhutdinov%2C%20R.%20Probabilistic%20matrix%20factorization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20A.%20Salakhutdinov%2C%20R.%20Probabilistic%20matrix%20factorization%202008"
        },
        {
            "id": "31",
            "entry": "[31] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams. Toward trustworthy recommender systems: An analysis of attack models and algorithm robustness. ACM Transactions on Internet Technology, 7(4):23, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mobasher%2C%20B.%20Burke%2C%20R.%20Bhaumik%2C%20R.%20Williams%2C%20C.%20Toward%20trustworthy%20recommender%20systems%3A%20An%20analysis%20of%20attack%20models%20and%20algorithm%20robustness%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mobasher%2C%20B.%20Burke%2C%20R.%20Bhaumik%2C%20R.%20Williams%2C%20C.%20Toward%20trustworthy%20recommender%20systems%3A%20An%20analysis%20of%20attack%20models%20and%20algorithm%20robustness%202007"
        },
        {
            "id": "32",
            "entry": "[32] Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming, 140(1):125\u2013161, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Gradient%20methods%20for%20minimizing%20composite%20functions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Gradient%20methods%20for%20minimizing%20composite%20functions%202013"
        },
        {
            "id": "33",
            "entry": "[33] J. Tanner and K. Wei. Low rank matrix completion by alternating steepest descent methods. Applied and Computational Harmonic Analysis, 40(2):417\u2013429, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tanner%2C%20J.%20Wei%2C%20K.%20Low%20rank%20matrix%20completion%20by%20alternating%20steepest%20descent%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tanner%2C%20J.%20Wei%2C%20K.%20Low%20rank%20matrix%20completion%20by%20alternating%20steepest%20descent%20methods%202016"
        },
        {
            "id": "34",
            "entry": "[34] J. Trzasko and A. Manduca. Highly undersampled magnetic resonance image reconstruction via homotopicminimization. IEEE Transactions on Medical Imaging, 28(1):106\u2013121, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trzasko%2C%20J.%20Manduca%2C%20A.%20Highly%20undersampled%20magnetic%20resonance%20image%20reconstruction%20via%20homotopicminimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trzasko%2C%20J.%20Manduca%2C%20A.%20Highly%20undersampled%20magnetic%20resonance%20image%20reconstruction%20via%20homotopicminimization%202009"
        },
        {
            "id": "35",
            "entry": "[35] M. Yan. Restoration of images corrupted by impulse noise and mixed Gaussian impulse noise using blind inpainting. SIAM Journal on Imaging Sciences, 6(3):1227\u20131245, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20M.%20Restoration%20of%20images%20corrupted%20by%20impulse%20noise%20and%20mixed%20Gaussian%20impulse%20noise%20using%20blind%20inpainting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20M.%20Restoration%20of%20images%20corrupted%20by%20impulse%20noise%20and%20mixed%20Gaussian%20impulse%20noise%20using%20blind%20inpainting%202013"
        },
        {
            "id": "36",
            "entry": "[36] M. Yan, Y. Yang, and S. Osher. Exact low-rank matrix completion from sparsely corrupted entries via adaptive outlier pursuit. Journal of Scientific Computing, 56(3):433\u2013449, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20M.%20Yang%2C%20Y.%20Osher%2C%20S.%20Exact%20low-rank%20matrix%20completion%20from%20sparsely%20corrupted%20entries%20via%20adaptive%20outlier%20pursuit%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20M.%20Yang%2C%20Y.%20Osher%2C%20S.%20Exact%20low-rank%20matrix%20completion%20from%20sparsely%20corrupted%20entries%20via%20adaptive%20outlier%20pursuit%202013"
        },
        {
            "id": "37",
            "entry": "[37] J. Yang and J. Leskovec. Overlapping community detection at scale: a nonnegative matrix factorization approach. In Web Search and Data Mining, pages 587\u2013596, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20J.%20Leskovec%2C%20J.%20Overlapping%20community%20detection%20at%20scale%3A%20a%20nonnegative%20matrix%20factorization%20approach.%20In%20Web%20Search%20and%20Data%20Mining%202013"
        },
        {
            "id": "38",
            "entry": "[38] Q. Yao, J. Kwok, T. Wang, and T. Liu. Large-scale low-rank matrix learning with nonconvex regularizers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Q.%20Kwok%2C%20J.%20Wang%2C%20T.%20Liu%2C%20T.%20Large-scale%20low-rank%20matrix%20learning%20with%20nonconvex%20regularizers%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Q.%20Kwok%2C%20J.%20Wang%2C%20T.%20Liu%2C%20T.%20Large-scale%20low-rank%20matrix%20learning%20with%20nonconvex%20regularizers%202018"
        },
        {
            "id": "39",
            "entry": "[39] C. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(2):894\u2013942, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Nearly%20unbiased%20variable%20selection%20under%20minimax%20concave%20penalty%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Nearly%20unbiased%20variable%20selection%20under%20minimax%20concave%20penalty%202010"
        },
        {
            "id": "40",
            "entry": "[40] Y. Zheng, G. Liu, S. Sugimoto, S. Yan, and M. Okutomi. Practical low-rank matrix approximation under robust 1-norm. In International Conference on Computer Vision and Pattern Recognition, pages 1410\u20131417, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Y.%20Liu%2C%20G.%20Sugimoto%2C%20S.%20Yan%2C%20S.%20Practical%20low-rank%20matrix%20approximation%20under%20robust%201-norm%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Y.%20Liu%2C%20G.%20Sugimoto%2C%20S.%20Yan%2C%20S.%20Practical%20low-rank%20matrix%20approximation%20under%20robust%201-norm%202012"
        },
        {
            "id": "41",
            "entry": "[41] W. Zuo, D. Meng, L. Zhang, X. Feng, and D. Zhang. A generalized iterated shrinkage algorithm for non-convex sparse coding. In International Conference on Computer Vision, pages 217\u2013224, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zuo%2C%20W.%20Meng%2C%20D.%20Zhang%2C%20L.%20Feng%2C%20X.%20A%20generalized%20iterated%20shrinkage%20algorithm%20for%20non-convex%20sparse%20coding%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zuo%2C%20W.%20Meng%2C%20D.%20Zhang%2C%20L.%20Feng%2C%20X.%20A%20generalized%20iterated%20shrinkage%20algorithm%20for%20non-convex%20sparse%20coding%202013"
        }
    ]
}
