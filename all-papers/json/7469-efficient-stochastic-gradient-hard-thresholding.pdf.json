{
    "filename": "7469-efficient-stochastic-gradient-hard-thresholding.pdf",
    "metadata": {
        "title": "Efficient Stochastic Gradient Hard Thresholding",
        "author": "Pan Zhou, Xiaotong Yuan, Jiashi Feng",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7469-efficient-stochastic-gradient-hard-thresholding.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Stochastic gradient hard thresholding methods have recently been shown to work favorably in solving large-scale empirical risk minimization problems under sparsity or rank constraint. Despite the improved iteration complexity over full gradient methods, the gradient evaluation and hard thresholding complexity of the existing stochastic algorithms usually scales linearly with data size, which could still be expensive when data is huge and the hard thresholding step could be as expensive as singular value decomposition in rank-constrained problems. To address these deficiencies, we propose an efficient hybrid stochastic gradient hard thresholding (HSG-HT) method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. Specifically, we prove that the stochastic gradient evaluation complexity of HSG-HT scales linearly with inverse of sub-optimality and its hard thresholding complexity scales logarithmically. By applying the heavy ball acceleration technique, we further propose an accelerated variant of HSG-HT which can be shown to have improved factor dependence on restricted condition number. Numerical results confirm our theoretical affirmation and demonstrate the computational efficiency of the proposed methods."
    },
    "keywords": [
        {
            "term": "Natural Science Foundation of China",
            "url": "https://en.wikipedia.org/wiki/Natural_Science_Foundation_of_China"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "singular value decomposition",
            "url": "https://en.wikipedia.org/wiki/singular_value_decomposition"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        }
    ],
    "highlights": [
        "We consider the following sparsityor rank-constrained finite-sum minimization problems which are widely applied in high-dimensional statistical estimation: 1n min f (x) :=<br/><br/>x n i=1 fi(x), s.t. x 0 \u2264 k or rank (x) \u2264 k, (1)<br/><br/>where each individual loss fi(x) is associated with the i-th sample, x 0 denotes the number of nonzero entries in x as a vector variable, rank (x) denotes the rank of x as a matrix variable, and k represents the sparsity/low-rankness level",
        "Inspired by the success of hybrid stochastic gradient descent, we propose the hybrid stochastic gradient hard thresholding (HSG-HT) method which has the following variable update form: xt+1 = \u03a6k xt \u2212 \u03b7gt",
        "Given a k\u2217-sparse/low-rank target solution x\u2217, for objective function with restricted condition number \u03bas and s = 2k + k\u2217, we show that O \u03bas rounds of incremental first order oracle update and O \u03bas log 1 steps of hard thresholding operation are sufficient for Hybrid Stochastic Gradient Hard Thresholding to find xsuch that x \u2212x\u2217 2 \u2264 +O \u2207If (x\u2217) 2",
        "We proposed Hybrid Stochastic Gradient Hard Thresholding as a hybrid stochastic gradient hard thresholding method for sparsity/rank-constrained empirical risk minimization problems",
        "We proved that Hybrid Stochastic Gradient Hard Thresholding enjoys the O \u03bas log 1 hard thresholding complexity like full gradient methods, while possessing sample-sizeindependent incremental first order oracle complexity of O \u03bas",
        "We showed that Hybrid Stochastic Gradient Hard Thresholding can be effectively accelerated via applying the heavy ball acceleration technique to attain improved dependence on restricted condition number"
    ],
    "key_statements": [
        "We consider the following sparsityor rank-constrained finite-sum minimization problems which are widely applied in high-dimensional statistical estimation: 1n min f (x) :=<br/><br/>x n i=1 fi(x), s.t. x 0 \u2264 k or rank (x) \u2264 k, (1)<br/><br/>where each individual loss fi(x) is associated with the i-th sample, x 0 denotes the number of nonzero entries in x as a vector variable, rank (x) denotes the rank of x as a matrix variable, and k represents the sparsity/low-rankness level",
        "When considering gradient hard thresholding methods, two main sources of computational complexity are at play: the gradient evaluation complexity and the hard thresholding complexity",
        "The method we propose can be viewed as a simple yet efficient extension of the hybrid stochastic gradient descent (HSGD) method [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] from unconstrained finite-sum minimization to the cardinality-constrained finite-sum problem (1)",
        "Inspired by the success of hybrid stochastic gradient descent, we propose the hybrid stochastic gradient hard thresholding (HSG-HT) method which has the following variable update form: xt+1 = \u03a6k xt \u2212 \u03b7gt",
        "Given a k\u2217-sparse/low-rank target solution x\u2217, for objective function with restricted condition number \u03bas and s = 2k + k\u2217, we show that O \u03bas rounds of incremental first order oracle update and O \u03bas log 1 steps of hard thresholding operation are sufficient for Hybrid Stochastic Gradient Hard Thresholding to find xsuch that x \u2212x\u2217 2 \u2264 +O \u2207If (x\u2217) 2",
        "For sparsity-constrained problem, we further investigate the convergence behavior of Hybrid Stochastic Gradient Hard Thresholding in terms of the objective value f (x) to the optimal loss f (x\u2217)",
        "accelerated HSG-HT converge significantly faster than the other considered algorithms in one pass over data. This confirms our theoretical predictions in Corollary 1 and 2 that Hybrid Stochastic Gradient Hard Thresholding and accelerated HSG-HT are cheaper in incremental first order oracle complexity than the sample-size-dependent algorithms when the desired accuracy is moderately small and data scale is large",
        "We proposed Hybrid Stochastic Gradient Hard Thresholding as a hybrid stochastic gradient hard thresholding method for sparsity/rank-constrained empirical risk minimization problems",
        "We proved that Hybrid Stochastic Gradient Hard Thresholding enjoys the O \u03bas log 1 hard thresholding complexity like full gradient methods, while possessing sample-sizeindependent incremental first order oracle complexity of O \u03bas",
        "We showed that Hybrid Stochastic Gradient Hard Thresholding can be effectively accelerated via applying the heavy ball acceleration technique to attain improved dependence on restricted condition number",
        "The provable efficiency of HSGT-HT and its accelerated variant has been confirmed by extensive numerical evaluation with comparison against the state-of-the-art algorithms"
    ],
    "summary": [
        "We consider the following sparsityor rank-constrained finite-sum minimization problems which are widely applied in high-dimensional statistical estimation: 1n min f (x) :=<br/><br/>x n i=1 fi(x), s.t. x 0 \u2264 k or rank (x) \u2264 k, (1)<br/><br/>where each individual loss fi(x) is associated with the i-th sample, x 0 denotes the number of nonzero entries in x as a vector variable, rank (x) denotes the rank of x as a matrix variable, and k represents the sparsity/low-rankness level.",
        "Given a k\u2217-sparse/low-rank target solution x\u2217, for objective function with restricted condition number \u03bas and s = 2k + k\u2217, we show that O \u03bas rounds of IFO update and O \u03bas log 1 steps of hard thresholding operation are sufficient for HSG-HT to find xsuch that x \u2212x\u2217 2 \u2264 +O \u2207If (x\u2217) 2 .",
        "Table 1 summarizes our main results on computational complexity and statistical estimation accuracy of HSG-HT and AHSG-HT, along with the results for the above mentioned state-of-the-art gradient hard thresholding algorithms.",
        "Both HSG-HT and AHSG-HT enjoy sample-size-independent IFO and hard thresholding complexity.",
        "We first introduce the Hybrid Stochastic Gradient Hard Thresholding (HSG-HT) algorithm and analyze its convergence performance for sparsityand rank-constrained problems.",
        "HSG-HT better trades-off between IFO and hard thresholding complexity than FG-HT and SVRG-HT when n is much larger than 1 in large-scale learning problems.",
        "Theorem 2 shows that for sparsity-constrained problem, HSG-HT enjoys linear convergence rate in terms of the objective value by gradually exponentially expanding the mini-batch size.",
        "Since comparing the support set I in statistical error O \u2207If (x\u2217) with other algorithms requiring larger truncation level k = \u03a9(\u03ba2sk\u2217), AHSG-HT can enjoy smaller statistical error, especially for the problems with large restricted condition number \u03bas.",
        "This confirms our theoretical predictions in Corollary 1 and 2 that HSG-HT and AHSG-HT are cheaper in IFO complexity than the sample-size-dependent algorithms when the desired accuracy is moderately small and data scale is large.",
        "From Figure 1 and the magnifying figures in Appendix E for better displaying objective loss decrease along with hard thresholding iteration, one can observe that AHSG-HT has shaper convergence behavior than HSG-HT, which demonstrates the acceleration power of AHSG-HT.",
        "We further evaluate the considered algorithms on sparsityconstrained softmax regression and rank-constrained linear regression problems, for which an approach usually needs multiple cycles of data processing to reach high accuracy solution.",
        "We proposed HSG-HT as a hybrid stochastic gradient hard thresholding method for sparsity/rank-constrained empirical risk minimization problems.",
        "We proved that HSG-HT enjoys the O \u03bas log 1 hard thresholding complexity like full gradient methods, while possessing sample-sizeindependent IFO complexity of O \u03bas .",
        "Compared to the existing variance-reduced hard thresholding algorithms, HSG-HT enjoys lower overall computational cost when sample size is large and the accuracy is moderately small.",
        "The provable efficiency of HSGT-HT and its accelerated variant has been confirmed by extensive numerical evaluation with comparison against the state-of-the-art algorithms"
    ],
    "headline": "We propose an efficient hybrid stochastic gradient hard thresholding  method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Donoho. Compressed sensing. IEEE Trans. on Information Theory, 52(4):1289\u20131306, 2006. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.%20Compressed%20sensing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.%20Compressed%20sensing%202006"
        },
        {
            "id": "2",
            "entry": "[2] J. Tropp and A. Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Trans. on Information Theory, 53(12):4655\u20134666, 2007. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20J.%20Gilbert%2C%20A.%20Signal%20recovery%20from%20random%20measurements%20via%20orthogonal%20matching%20pursuit%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20J.%20Gilbert%2C%20A.%20Signal%20recovery%20from%20random%20measurements%20via%20orthogonal%20matching%20pursuit%202007"
        },
        {
            "id": "3",
            "entry": "[3] S. Bahmani, B. Raj, and P. Boufounos. Greedy sparsity-constrained optimization. Journal of Machine Learning Research, 14:807\u2013841, 2013. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahmani%2C%20S.%20Raj%2C%20B.%20Boufounos%2C%20P.%20Greedy%20sparsity-constrained%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahmani%2C%20S.%20Raj%2C%20B.%20Boufounos%2C%20P.%20Greedy%20sparsity-constrained%20optimization%202013"
        },
        {
            "id": "4",
            "entry": "[4] A. Jalali, C. Johnson, and P. Ravikumar. On learning discrete graphical models using greedy methods. In Proc. Conf. Neutral Information Processing Systems, pages 1935\u20131943, 2011. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jalali%2C%20A.%20Johnson%2C%20C.%20Ravikumar%2C%20P.%20On%20learning%20discrete%20graphical%20models%20using%20greedy%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jalali%2C%20A.%20Johnson%2C%20C.%20Ravikumar%2C%20P.%20On%20learning%20discrete%20graphical%20models%20using%20greedy%20methods%202011"
        },
        {
            "id": "5",
            "entry": "[5] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, pages 1069\u20131097, 2011. 1, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20S.%20Wainwright%2C%20M.%20Estimation%20of%20%28near%29%20low-rank%20matrices%20with%20noise%20and%20high-dimensional%20scaling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20S.%20Wainwright%2C%20M.%20Estimation%20of%20%28near%29%20low-rank%20matrices%20with%20noise%20and%20high-dimensional%20scaling%202011"
        },
        {
            "id": "6",
            "entry": "[6] P. Zhou and J. Feng. Outlier-robust tensor PCA. In Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 1\u20139, 2017. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20P.%20Feng%2C%20J.%20Outlier-robust%20tensor%20PCA%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20P.%20Feng%2C%20J.%20Outlier-robust%20tensor%20PCA%202017"
        },
        {
            "id": "7",
            "entry": "[7] Y. Wang, C. Xu, C. Xu, and D. Tao. Beyond RPCA: Flattening complex noise in the frequency domain. In AAAI Conf. Artificial Intelligence, 2017. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Xu%2C%20C.%20Xu%2C%20C.%20Tao%2C%20D.%20Beyond%20RPCA%3A%20Flattening%20complex%20noise%20in%20the%20frequency%20domain%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Xu%2C%20C.%20Xu%2C%20C.%20Tao%2C%20D.%20Beyond%20RPCA%3A%20Flattening%20complex%20noise%20in%20the%20frequency%20domain%202017"
        },
        {
            "id": "8",
            "entry": "[8] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. The Annals of Statistics, 39(2):887\u2013930, 2011. 1, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rohde%2C%20A.%20Tsybakov%2C%20A.%20Estimation%20of%20high-dimensional%20low-rank%20matrices%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rohde%2C%20A.%20Tsybakov%2C%20A.%20Estimation%20of%20high-dimensional%20low-rank%20matrices%202011"
        },
        {
            "id": "9",
            "entry": "[9] P. Zhou, C. Lu, Z. Lin, and C. Zhang. Tensor factorization for low-rank tensor completion. IEEE Transactions on Image Processing, 27(3):1152 \u2013 1163, 2017. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20P.%20Lu%2C%20C.%20Lin%2C%20Z.%20Zhang%2C%20C.%20Tensor%20factorization%20for%20low-rank%20tensor%20completion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20P.%20Lu%2C%20C.%20Lin%2C%20Z.%20Zhang%2C%20C.%20Tensor%20factorization%20for%20low-rank%20tensor%20completion%202017"
        },
        {
            "id": "10",
            "entry": "[10] T. Blumensath and M. Davies. Iterative hard thresholding for compressed sensing. Applied and computational harmonic analysis, 27(3):265\u2013274, 2009. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumensath%2C%20T.%20Davies%2C%20M.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumensath%2C%20T.%20Davies%2C%20M.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009"
        },
        {
            "id": "11",
            "entry": "[11] S. Foucart. Hard thresholding pursuit: an algorithm for compressive sensing. SIAM Journal on Numerical Analysis, 49(6):2543\u20132563, 2011. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foucart%2C%20S.%20Hard%20thresholding%20pursuit%3A%20an%20algorithm%20for%20compressive%20sensing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foucart%2C%20S.%20Hard%20thresholding%20pursuit%3A%20an%20algorithm%20for%20compressive%20sensing%202011"
        },
        {
            "id": "12",
            "entry": "[12] X. Yuan, P. Li, and T. Zhang. Gradient hard thresholding pursuit for sparsity-constrained optimization. In Proc. Int\u2019l Conf. Machine Learning, pages 127\u2013135, 2014. 1, 2, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20X.%20Li%2C%20P.%20Zhang%2C%20T.%20Gradient%20hard%20thresholding%20pursuit%20for%20sparsity-constrained%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20X.%20Li%2C%20P.%20Zhang%2C%20T.%20Gradient%20hard%20thresholding%20pursuit%20for%20sparsity-constrained%20optimization%202014"
        },
        {
            "id": "13",
            "entry": "[13] P. Jain, A. Tewari, and P. Kar. On iterative hard thresholding methods for high-dimensional M-estimation. In Proc. Conf. Neutral Information Processing Systems, pages 685\u2013693, 2014. 1, 2, 4, 5, 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20P.%20Tewari%2C%20A.%20Kar%2C%20P.%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20M-estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20P.%20Tewari%2C%20A.%20Kar%2C%20P.%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20M-estimation%202014"
        },
        {
            "id": "14",
            "entry": "[14] R. Garg and Rohit R. Khandekar. Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property. In Proc. Int\u2019l Conf. Machine Learning, pages 337\u2013344. ACM, 2009. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garg%2C%20R.%20Khandekar%2C%20Rohit%20R.%20Gradient%20descent%20with%20sparsification%3A%20an%20iterative%20algorithm%20for%20sparse%20recovery%20with%20restricted%20isometry%20property%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garg%2C%20R.%20Khandekar%2C%20Rohit%20R.%20Gradient%20descent%20with%20sparsification%3A%20an%20iterative%20algorithm%20for%20sparse%20recovery%20with%20restricted%20isometry%20property%202009"
        },
        {
            "id": "15",
            "entry": "[15] X. Yuan, P. Li, and T. Zhang. Exact recovery of hard thresholding pursuit. In Proc. Conf. Neutral Information Processing Systems, pages 3558\u20133566, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20X.%20Li%2C%20P.%20Zhang%2C%20T.%20Exact%20recovery%20of%20hard%20thresholding%20pursuit%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20X.%20Li%2C%20P.%20Zhang%2C%20T.%20Exact%20recovery%20of%20hard%20thresholding%20pursuit%202016"
        },
        {
            "id": "16",
            "entry": "[16] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288, 1996. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20LASSO%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20R.%20Regression%20shrinkage%20and%20selection%20via%20the%20LASSO%201996"
        },
        {
            "id": "17",
            "entry": "[17] S. Van de Geer. High-dimensional generalized linear models and the LASSO. The Annals of Statistics, 36(2):614\u2013645, 2008. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Geer%2C%20S.Van%20High-dimensional%20generalized%20linear%20models%20and%20the%20LASSO%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Geer%2C%20S.Van%20High-dimensional%20generalized%20linear%20models%20and%20the%20LASSO%202008"
        },
        {
            "id": "18",
            "entry": "[18] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471\u2013501, 2010. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20B.%20Fazel%2C%20M.%20Parrilo%2C%20P.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20B.%20Fazel%2C%20M.%20Parrilo%2C%20P.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010"
        },
        {
            "id": "19",
            "entry": "[19] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In Proc. Conf. Neutral Information Processing Systems, pages 1329\u20131336, 2005. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srebro%2C%20N.%20Rennie%2C%20J.%20Jaakkola%2C%20T.%20Maximum-margin%20matrix%20factorization%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srebro%2C%20N.%20Rennie%2C%20J.%20Jaakkola%2C%20T.%20Maximum-margin%20matrix%20factorization%202005"
        },
        {
            "id": "20",
            "entry": "[20] N. Nguyen, D. Needell, and T. Woolf. Linear convergence of stochastic iterative greedy algorithms with sparse constraints. IEEE Trans. on Information Theory, 63(11):6869\u20136895, 2017. 2, 4, 5, 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20N.%20Needell%2C%20D.%20Woolf%2C%20T.%20Linear%20convergence%20of%20stochastic%20iterative%20greedy%20algorithms%20with%20sparse%20constraints%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20N.%20Needell%2C%20D.%20Woolf%2C%20T.%20Linear%20convergence%20of%20stochastic%20iterative%20greedy%20algorithms%20with%20sparse%20constraints%202017"
        },
        {
            "id": "21",
            "entry": "[21] X. Li, R. Arora, H. Liu, J. Haupt, and T. Zhao. Nonconvex sparse learning via stochastic optimization with progressive variance reduction. Proc. Int\u2019l Conf. Machine Learning, 2016. 2, 4, 5, 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20X.%20Arora%2C%20R.%20Liu%2C%20H.%20Haupt%2C%20J.%20Nonconvex%20sparse%20learning%20via%20stochastic%20optimization%20with%20progressive%20variance%20reduction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20X.%20Arora%2C%20R.%20Liu%2C%20H.%20Haupt%2C%20J.%20Nonconvex%20sparse%20learning%20via%20stochastic%20optimization%20with%20progressive%20variance%20reduction%202016"
        },
        {
            "id": "22",
            "entry": "[22] J. Shen and P. Li. A tight bound of hard thresholding. arXiv preprint arXiv:1605.01656, 2016. 2, 4, 6, 7",
            "arxiv_url": "https://arxiv.org/pdf/1605.01656"
        },
        {
            "id": "23",
            "entry": "[23] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Proc. Conf. Neutral Information Processing Systems, pages 315\u2013323, 2013. 2, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "24",
            "entry": "[24] D. P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM Journal on Optimization, 7(4):913\u2013926, 1997. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20A%20new%20class%20of%20incremental%20gradient%20methods%20for%20least%20squares%20problems%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20D.P.%20A%20new%20class%20of%20incremental%20gradient%20methods%20for%20least%20squares%20problems%201997"
        },
        {
            "id": "25",
            "entry": "[25] M. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3):A1380\u2013A1405, 2012. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedlander%2C%20M.%20Schmidt%2C%20M.%20Hybrid%20deterministic-stochastic%20methods%20for%20data%20fitting%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedlander%2C%20M.%20Schmidt%2C%20M.%20Hybrid%20deterministic-stochastic%20methods%20for%20data%20fitting%202012"
        },
        {
            "id": "26",
            "entry": "[26] P. Zhou, X. Yuan, and J. Feng. New insight into hybrid stochastic gradient descent: Beyond withreplacement sampling and convexity. In Proc. Conf. Neutral Information Processing Systems, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20P.%20Yuan%2C%20X.%20Feng%2C%20J.%20New%20insight%20into%20hybrid%20stochastic%20gradient%20descent%3A%20Beyond%20withreplacement%20sampling%20and%20convexity%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20P.%20Yuan%2C%20X.%20Feng%2C%20J.%20New%20insight%20into%20hybrid%20stochastic%20gradient%20descent%3A%20Beyond%20withreplacement%20sampling%20and%20convexity%202018"
        },
        {
            "id": "27",
            "entry": "[27] B. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317, 1964. 3, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20B.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20B.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964"
        },
        {
            "id": "28",
            "entry": "[28] A. Govan. Introduction to optimization. In North Carolina State University, SAMSI NDHS, Undergraduate workshop, 2006. 3, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Govan%2C%20A.%20Introduction%20to%20optimization.%20In%20North%20Carolina%20State%20University%2C%20SAMSI%20NDHS%2C%20Undergraduate%20workshop%202006"
        },
        {
            "id": "29",
            "entry": "[29] N. Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):145\u2013151, 1999. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qian%2C%20N.%20On%20the%20momentum%20term%20in%20gradient%20descent%20learning%20algorithms%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qian%2C%20N.%20On%20the%20momentum%20term%20in%20gradient%20descent%20learning%20algorithms%201999"
        },
        {
            "id": "30",
            "entry": "[30] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "31",
            "entry": "[31] P. Jain, S. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford. Accelerating stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017. 3",
            "arxiv_url": "https://arxiv.org/pdf/1704.08227"
        },
        {
            "id": "32",
            "entry": "[32] S. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In Proc. Int\u2019l Conf. Learning Representations, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20adam%20and%20beyond%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.%20Kale%2C%20S.%20Kumar%2C%20S.%20On%20the%20convergence%20of%20adam%20and%20beyond%202018"
        },
        {
            "id": "33",
            "entry": "[33] K. Rajiv and K. Anastasios. IHT dies hard: Provable accelerated iterative hard thresholding. In Proc. Int\u2019l Conf. Artificial Intelligence and Statistics, volume 84, pages 188\u2013198, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajiv%2C%20K.%20Anastasios%2C%20K.%20IHT%20dies%20hard%3A%20Provable%20accelerated%20iterative%20hard%20thresholding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajiv%2C%20K.%20Anastasios%2C%20K.%20IHT%20dies%20hard%3A%20Provable%20accelerated%20iterative%20hard%20thresholding%202018"
        },
        {
            "id": "34",
            "entry": "[34] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In Proc. Int\u2019l Conf. Machine Learning, pages 353\u2013361, 2015. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Xiao%2C%20L.%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Xiao%2C%20L.%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "35",
            "entry": "[35] Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient method. In Proc. Conf. Neutral Information Processing Systems, pages 3059\u20133067, 2014. 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Q.%20Lu%2C%20Z.%20Xiao%2C%20L.%20An%20accelerated%20proximal%20coordinate%20gradient%20method",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Q.%20Lu%2C%20Z.%20Xiao%2C%20L.%20An%20accelerated%20proximal%20coordinate%20gradient%20method"
        },
        {
            "id": "36",
            "entry": "[36] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classification. In Proc. Int\u2019l Conf. Machine Learning, pages 17\u201324. ACM, 2007. 7 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amit%2C%20Y.%20Fink%2C%20M.%20Srebro%2C%20N.%20Ullman%2C%20S.%20Uncovering%20shared%20structures%20in%20multiclass%20classification%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amit%2C%20Y.%20Fink%2C%20M.%20Srebro%2C%20N.%20Ullman%2C%20S.%20Uncovering%20shared%20structures%20in%20multiclass%20classification%202007"
        }
    ]
}
