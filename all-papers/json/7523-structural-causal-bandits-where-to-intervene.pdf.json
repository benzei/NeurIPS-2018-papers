{
    "filename": "7523-structural-causal-bandits-where-to-intervene.pdf",
    "metadata": {
        "title": "Structural Causal Bandits: Where to Intervene?",
        "author": "Sanghack Lee, Elias Bareinboim",
        "date": 1985,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7523-structural-causal-bandits-where-to-intervene.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the problem of identifying the best action in a sequential decisionmaking setting when the reward distributions of the arms exhibit a non-trivial dependence structure, which is governed by the underlying causal model of the domain where the agent is deployed. In this setting, playing an arm corresponds to intervening on a set of variables and setting them to specific values. In this paper, we show that whenever the underlying causal model is not taken into account during the decision-making process, the standard strategies of simultaneously intervening on all variables or on all the subsets of the variables may, in general, lead to suboptimal policies, regardless of the number of interventions performed by the agent in the environment. We formally acknowledge this phenomenon and investigate structural properties implied by the underlying causal model, which lead to a complete characterization of the relationships between the arms\u2019 distributions. We leverage this characterization to build a new algorithm that takes as input a causal structure and finds a minimal, sound, and complete set of qualified arms that an agent should play to maximize its expected reward. We empirically demonstrate that the new strategy learns an optimal policy and leads to orders of magnitude faster convergence rates when compared with its causal-insensitive counterparts."
    },
    "keywords": [
        {
            "term": "multi-armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi-armed_bandit"
        },
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "causal structure",
            "url": "https://en.wikipedia.org/wiki/causal_structure"
        },
        {
            "term": "causal model",
            "url": "https://en.wikipedia.org/wiki/causal_model"
        },
        {
            "term": "instrumental variable",
            "url": "https://en.wikipedia.org/wiki/instrumental_variable"
        },
        {
            "term": "thompson sampling",
            "url": "https://en.wikipedia.org/wiki/thompson_sampling"
        }
    ],
    "highlights": [
        "The multi-armed bandit (MAB) problem is one of the prototypical settings studied in the sequential decision-making literature [<a class=\"ref-link\" id=\"cLai_1985_a\" href=\"#rLai_1985_a\"><a class=\"ref-link\" id=\"cLai_1985_a\" href=\"#rLai_1985_a\">Lai and Robbins, 1985</a></a>, <a class=\"ref-link\" id=\"cEven-Dar_et+al_2006_a\" href=\"#rEven-Dar_et+al_2006_a\"><a class=\"ref-link\" id=\"cEven-Dar_et+al_2006_a\" href=\"#rEven-Dar_et+al_2006_a\">Even-Dar et al, 2006</a></a>, <a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\"><a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\">Bubeck and Cesa-Bianchi, 2012</a></a>]",
        "We derive the structural properties of a Structural Causal Models-multi-armed bandit, which are computable from any causal model, including arms\u2019 equivalence based on do-calculus [Pearl, 1995], and partial orderedness among sets of variables associated with arms in regards to the maximum rewards achievable",
        "We investigate some key structural properties that follow from the causal structure G of the Structural Causal Models-multi-armed bandit",
        "The problem was formalized using the logic of structural causal models (SCMs) and formalized through a new type of multi-armed bandit called Structural Causal Models-multi-armed bandit",
        "We started by noting that whenever the agent cannot measure all the variables in the environment, standard multi-armed bandit algorithms that are oblivious to the underlying causal structure may not converge, regardless of the number of interventions performed in the environment. (We note that the causal structure can be learned in a typical multi-armed bandit setting since the agent always has interventional capabilities.) We introduced a novel decision-making strategy based on properties following the do-calculus, which allowed the removal of redundant arms, and the partial-orders among the sets of variables existent in the underlying causal system, which led to the understanding of the maximum achievable reward of each interventional set",
        "Leveraging this new strategy based on the possibly-optimal minimal intervention sets, we developed an algorithm that decides whether interventions should be performed in the underlying system"
    ],
    "key_statements": [
        "The multi-armed bandit (MAB) problem is one of the prototypical settings studied in the sequential decision-making literature [<a class=\"ref-link\" id=\"cLai_1985_a\" href=\"#rLai_1985_a\"><a class=\"ref-link\" id=\"cLai_1985_a\" href=\"#rLai_1985_a\">Lai and Robbins, 1985</a></a>, <a class=\"ref-link\" id=\"cEven-Dar_et+al_2006_a\" href=\"#rEven-Dar_et+al_2006_a\"><a class=\"ref-link\" id=\"cEven-Dar_et+al_2006_a\" href=\"#rEven-Dar_et+al_2006_a\">Even-Dar et al, 2006</a></a>, <a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\"><a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\">Bubeck and Cesa-Bianchi, 2012</a></a>]",
        "There is a wide range of assumptions underlying multi-armed bandit, but in most of the traditional settings, the arms\u2019 rewards are assumed to be independent, which means that knowing the reward distribution of one arm has no implication to the reward of the other arms",
        "We focus on the challenge of identifying the best action in multi-armed bandit where the arms correspond to interventions on an arbitrary causal graph, including when latent variables confound the observed relations (i..e, semi-Markovian causal models)",
        "We investigate this phenomenon, and more broadly, causal multi-armed bandit with non-trivial dependency structure between the arms",
        "We derive the structural properties of a Structural Causal Models-multi-armed bandit, which are computable from any causal model, including arms\u2019 equivalence based on do-calculus [Pearl, 1995], and partial orderedness among sets of variables associated with arms in regards to the maximum rewards achievable",
        "We introduce an algorithm that identifies a complete set of POMISs so that only the subset of arms associated with them can be explored in a multi-armed bandit algorithm",
        "We investigate some key structural properties that follow from the causal structure G of the Structural Causal Models-multi-armed bandit",
        "We provide in Appendix A a Structural Causal Models such that \u03bc; < \u03bcx\u21e4 holds true",
        "One can see in Fig. 4b that interventional border(GX , Y ) = {T , W , X} where intervening on X lets Y be the only element of MUCT making its parents an interventional border, a POMIS",
        "We considered four strategies for selecting arms, includingSPOMISs, MISs, Brute-force, and All-at-once, where Brute-force evaluates all combinations of arms X\u2713V\\{Y } D (X), and All-at-once considers intervening in all variables simultaneously, D (V\\{Y }), oblivious to the causal structure and any knowledge about the action space",
        "At an\n6All the code is available at https://github.com/sanghack81/SCMMAB-NIPS2018 7One may surmise that combinatorial bandit (CB) algorithms can be used to solve Structural Causal Models-multi-armed bandit instances by noting that an intervention can be encoded as a binary vector, where each dimension in the vector corresponds to intervening on a single variable with a specific value",
        "The problem was formalized using the logic of structural causal models (SCMs) and formalized through a new type of multi-armed bandit called Structural Causal Models-multi-armed bandit",
        "We started by noting that whenever the agent cannot measure all the variables in the environment, standard multi-armed bandit algorithms that are oblivious to the underlying causal structure may not converge, regardless of the number of interventions performed in the environment. (We note that the causal structure can be learned in a typical multi-armed bandit setting since the agent always has interventional capabilities.) We introduced a novel decision-making strategy based on properties following the do-calculus, which allowed the removal of redundant arms, and the partial-orders among the sets of variables existent in the underlying causal system, which led to the understanding of the maximum achievable reward of each interventional set",
        "Leveraging this new strategy based on the possibly-optimal minimal intervention sets, we developed an algorithm that decides whether interventions should be performed in the underlying system"
    ],
    "summary": [
        "The multi-armed bandit (MAB) problem is one of the prototypical settings studied in the sequential decision-making literature [<a class=\"ref-link\" id=\"cLai_1985_a\" href=\"#rLai_1985_a\"><a class=\"ref-link\" id=\"cLai_1985_a\" href=\"#rLai_1985_a\">Lai and Robbins, 1985</a></a>, <a class=\"ref-link\" id=\"cEven-Dar_et+al_2006_a\" href=\"#rEven-Dar_et+al_2006_a\"><a class=\"ref-link\" id=\"cEven-Dar_et+al_2006_a\" href=\"#rEven-Dar_et+al_2006_a\">Even-Dar et al, 2006</a></a>, <a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\"><a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\">Bubeck and Cesa-Bianchi, 2012</a></a>].",
        "We derive the structural properties of a SCM-MAB, which are computable from any causal model, including arms\u2019 equivalence based on do-calculus [Pearl, 1995], and partial orderedness among sets of variables associated with arms in regards to the maximum rewards achievable.",
        "Allowing the UC to affect X.5 one can see in Fig. 4b that IB(GX , Y ) = {T , W , X} where intervening on X lets Y be the only element of MUCT making its parents an interventional border, a POMIS.",
        "We considered four strategies for selecting arms, includingSPOMISs, MISs, Brute-force, and All-at-once, where Brute-force evaluates all combinations of arms X\u2713V\\{Y } D (X), and All-at-once considers intervening in all variables simultaneously, D (V\\{Y }), oblivious to the causal structure and any knowledge about the action space.",
        "6All the code is available at https://github.com/sanghack81/SCMMAB-NIPS2018 7One may surmise that combinatorial bandit (CB) algorithms can be used to solve SCM-MAB instances by noting that an intervention can be encoded as a binary vector, where each dimension in the vector corresponds to intervening on a single variable with a specific value.",
        "The current generation of CB algorithms is oblivious to the underlying causal structure, which makes them resemble very closely the Brute-force strategy, the worst possible method for SCM-MABs. Further, the assumption of linearity is arguably one of the most popular considered by CB solvers.",
        "We start by noticing that the reduction in the CRs is approximately proportional to the reduction in the number of non-optimal arms pulled by (PO)MIS by the corresponding algorithm, which makes the POMIS-based solver the clear winner throughout the simulations.",
        "The problem was formalized using the logic of structural causal models (SCMs) and formalized through a new type of multi-armed bandit called SCM-MABs. We started by noting that whenever the agent cannot measure all the variables in the environment, standard MAB algorithms that are oblivious to the underlying causal structure may not converge, regardless of the number of interventions performed in the environment.",
        "(We note that the causal structure can be learned in a typical MAB setting since the agent always has interventional capabilities.) We introduced a novel decision-making strategy based on properties following the do-calculus, which allowed the removal of redundant arms, and the partial-orders among the sets of variables existent in the underlying causal system, which led to the understanding of the maximum achievable reward of each interventional set.",
        "We hope that formal machinery and the algorithms developed here can help decision-makers to make more principled and efficient decisions"
    ],
    "headline": "We study the problem of identifying the best action in a sequential decisionmaking setting when the reward distributions of the arms exhibit a non-trivial dependence structure, which is governed by the underlying causal model of the domain where the agent is deployed",
    "reference_links": [
        {
            "id": "Peter_2002_a",
            "entry": "Peter Auer, Nicol\u00f2 Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2/3):235\u2013256, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peter%20Auer%2C%20Nicol%C3%B2%20Cesa-Bianchi%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peter%20Auer%2C%20Nicol%C3%B2%20Cesa-Bianchi%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "Bareinboim_2016_a",
            "entry": "Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 113(27):7345\u20137352, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bareinboim%2C%20Elias%20Pearl%2C%20Judea%20Causal%20inference%20and%20the%20data-fusion%20problem%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bareinboim%2C%20Elias%20Pearl%2C%20Judea%20Causal%20inference%20and%20the%20data-fusion%20problem%202016"
        },
        {
            "id": "Bareinboim_et+al_2015_a",
            "entry": "Elias Bareinboim, Andrew Forney, and Judea Pearl. Bandits with unobserved confounders: A causal approach. In Advances in Neural Information Processing Systems 28, pages 1342\u20131350. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bareinboim%2C%20Elias%20Forney%2C%20Andrew%20Pearl%2C%20Judea%20Bandits%20with%20unobserved%20confounders%3A%20A%20causal%20approach%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bareinboim%2C%20Elias%20Forney%2C%20Andrew%20Pearl%2C%20Judea%20Bandits%20with%20unobserved%20confounders%3A%20A%20causal%20approach%202015"
        },
        {
            "id": "Bubeck_2012_a",
            "entry": "S\u00e9bastien Bubeck and Nicol\u00f2 Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. Foundations and Trends in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multiarmed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multiarmed%20bandit%20problems%202012"
        },
        {
            "id": "Capp_et+al_2013_a",
            "entry": "Olivier Capp\u00e9, Aur\u00e9lien Garivier, Odalric-Ambrym Maillard, R\u00e9mi Munos, and Gilles Stoltz. Kullback-Leibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, 41(3):1516\u20131541, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Capp%C3%A9%2C%20Olivier%20Garivier%2C%20Aur%C3%A9lien%20Maillard%2C%20Odalric-Ambrym%20Munos%2C%20R%C3%A9mi%20Kullback-Leibler%20upper%20confidence%20bounds%20for%20optimal%20sequential%20allocation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Capp%C3%A9%2C%20Olivier%20Garivier%2C%20Aur%C3%A9lien%20Maillard%2C%20Odalric-Ambrym%20Munos%2C%20R%C3%A9mi%20Kullback-Leibler%20upper%20confidence%20bounds%20for%20optimal%20sequential%20allocation%202013"
        },
        {
            "id": "Cesa-Bianchi_2012_a",
            "entry": "Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences, 78(5):1404 \u2013 1422, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicol%C3%B2%20Lugosi%2C%20G%C3%A1bor%20Combinatorial%20bandits%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20Nicol%C3%B2%20Lugosi%2C%20G%C3%A1bor%20Combinatorial%20bandits%202012"
        },
        {
            "id": "Combes_2014_a",
            "entry": "Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In International Conference on Machine Learning, pages 521\u2013529, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Unimodal%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Unimodal%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014"
        },
        {
            "id": "Dani_et+al_2008_a",
            "entry": "Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of Conference On Learning Theory (COLT), pages 355\u2013366, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dani%2C%20Varsha%20Hayes%2C%20Thomas%20P.%20Kakade%2C%20Sham%20M.%20Stochastic%20linear%20optimization%20under%20bandit%20feedback%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dani%2C%20Varsha%20Hayes%2C%20Thomas%20P.%20Kakade%2C%20Sham%20M.%20Stochastic%20linear%20optimization%20under%20bandit%20feedback%202008"
        },
        {
            "id": "Even-Dar_et+al_2006_a",
            "entry": "Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun):1079\u20131105, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20Eyal%20Mannor%2C%20Shie%20Mansour%2C%20Yishay%20Action%20elimination%20and%20stopping%20conditions%20for%20the%20multi-armed%20bandit%20and%20reinforcement%20learning%20problems%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20Eyal%20Mannor%2C%20Shie%20Mansour%2C%20Yishay%20Action%20elimination%20and%20stopping%20conditions%20for%20the%20multi-armed%20bandit%20and%20reinforcement%20learning%20problems%202006"
        },
        {
            "id": "Forney_et+al_2017_a",
            "entry": "Andrew Forney, Judea Pearl, and Elias Bareinboim. Counterfactual data-fusion for online reinforcement learners. In Proceedings of the 34th International Conference on Machine Learning, pages 1156\u20131164, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forney%2C%20Andrew%20Pearl%2C%20Judea%20Bareinboim%2C%20Elias%20Counterfactual%20data-fusion%20for%20online%20reinforcement%20learners%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Forney%2C%20Andrew%20Pearl%2C%20Judea%20Bareinboim%2C%20Elias%20Counterfactual%20data-fusion%20for%20online%20reinforcement%20learners%202017"
        },
        {
            "id": "Garivier_2011_a",
            "entry": "Aur\u00e9lien Garivier and Olivier Capp\u00e9. The KL-UCB algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th annual Conference On Learning Theory, pages 359\u2013376, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garivier%2C%20Aur%C3%A9lien%20Capp%C3%A9%2C%20Olivier%20The%20KL-UCB%20algorithm%20for%20bounded%20stochastic%20bandits%20and%20beyond%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garivier%2C%20Aur%C3%A9lien%20Capp%C3%A9%2C%20Olivier%20The%20KL-UCB%20algorithm%20for%20bounded%20stochastic%20bandits%20and%20beyond%202011"
        },
        {
            "id": "Emilie_2012_a",
            "entry": "Emilie Kaufmann, Nathaniel Korda, and R\u00e9mi Munos. Thompson sampling: An asymptotically optimal finite-time analysis. In Algorithmic Learning Theory, pages 199\u2013213, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Emilie%20Kaufmann%2C%20Nathaniel%20Korda%20Munos%2C%20R%C3%A9mi%20Thompson%20sampling%3A%20An%20asymptotically%20optimal%20finite-time%20analysis%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Emilie%20Kaufmann%2C%20Nathaniel%20Korda%20Munos%2C%20R%C3%A9mi%20Thompson%20sampling%3A%20An%20asymptotically%20optimal%20finite-time%20analysis%202012"
        },
        {
            "id": "Kocaoglu_et+al_2017_a",
            "entry": "Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Experimental design for learning causal graphs with latent variables. In Advances in Neural Information Processing Systems 30, pages 7021\u20137031, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kocaoglu%2C%20Murat%20Shanmugam%2C%20Karthikeyan%20Bareinboim%2C%20Elias%20Experimental%20design%20for%20learning%20causal%20graphs%20with%20latent%20variables%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kocaoglu%2C%20Murat%20Shanmugam%2C%20Karthikeyan%20Bareinboim%2C%20Elias%20Experimental%20design%20for%20learning%20causal%20graphs%20with%20latent%20variables%202017"
        },
        {
            "id": "Lai_1985_a",
            "entry": "Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4\u201322, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20Tze%20Leung%20Robbins%2C%20Herbert%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20Tze%20Leung%20Robbins%2C%20Herbert%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985"
        },
        {
            "id": "Lattimore_et+al_2016_a",
            "entry": "Finnian Lattimore, Tor Lattimore, and Mark D. Reid. Causal bandits: Learning good interventions via causal inference. In Advances in Neural Information Processing Systems 29, pages 1181\u20131189. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lattimore%2C%20Finnian%20Lattimore%2C%20Tor%20Reid%2C%20Mark%20D.%20Causal%20bandits%3A%20Learning%20good%20interventions%20via%20causal%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lattimore%2C%20Finnian%20Lattimore%2C%20Tor%20Reid%2C%20Mark%20D.%20Causal%20bandits%3A%20Learning%20good%20interventions%20via%20causal%20inference%202016"
        },
        {
            "id": "Lee_2018_a",
            "entry": "Sanghack Lee and Elias Bareinboim. Structural causal bandits: Where to intervene? Technical Report R-36, Purdue AI Lab, Department of Computer Science, Purdue University, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Sanghack%20Bareinboim%2C%20Elias%20Structural%20causal%20bandits%3A%20Where%20to%20intervene%3F%202018"
        },
        {
            "id": "Magureanu_et+al_2014_a",
            "entry": "Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bound and optimal algorithms. In Proceedings of The 27th Conference on Learning Theory, pages 975\u2013999, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magureanu%2C%20Stefan%20Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Lipschitz%20bandits%3A%20Regret%20lower%20bound%20and%20optimal%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Magureanu%2C%20Stefan%20Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Lipschitz%20bandits%3A%20Regret%20lower%20bound%20and%20optimal%20algorithms%202014"
        },
        {
            "id": "Ortega_2014_a",
            "entry": "Pedro A. Ortega and Daniel A. Braun. Generalized Thompson sampling for sequential decisionmaking and causal inference. Complex Adaptive Systems Modeling, 2(2), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ortega%2C%20Pedro%20A.%20Braun%2C%20Daniel%20A.%20Generalized%20Thompson%20sampling%20for%20sequential%20decisionmaking%20and%20causal%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ortega%2C%20Pedro%20A.%20Braun%2C%20Daniel%20A.%20Generalized%20Thompson%20sampling%20for%20sequential%20decisionmaking%20and%20causal%20inference%202014"
        }
    ]
}
