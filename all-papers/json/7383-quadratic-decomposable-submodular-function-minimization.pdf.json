{
    "filename": "7383-quadratic-decomposable-submodular-function-minimization.pdf",
    "metadata": {
        "title": "Quadratic Decomposable Submodular Function Minimization",
        "author": "Pan Li, Niao He, Olgica Milenkovic",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7383-quadratic-decomposable-submodular-function-minimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization. The problem is closely related to decomposable submodular function minimization and arises in many learning on graphs and hypergraphs settings, such as graph-based semi-supervised learning and PageRank. We approach the problem via a new dual strategy and describe an objective that may be optimized via random coordinate descent (RCD) methods and projections onto cones. We also establish the linear convergence rate of the RCD algorithm and develop efficient projection algorithms with provable performance guarantees. Numerical experiments in semi-supervised learning on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate the significant improvements in prediction accuracy with respect to state-of-the-art methods.1"
    },
    "keywords": [
        {
            "term": "total variation",
            "url": "https://en.wikipedia.org/wiki/total_variation"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "random coordinate descent",
            "url": "https://en.wikipedia.org/wiki/random_coordinate_descent"
        },
        {
            "term": "diffusion processes",
            "url": "https://en.wikipedia.org/wiki/diffusion_processes"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "hypergraph",
            "url": "https://en.wikipedia.org/wiki/hypergraph"
        },
        {
            "term": "set function",
            "url": "https://en.wikipedia.org/wiki/set_function"
        }
    ],
    "highlights": [
        "Nishihara et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] established the linear convergence rate of alternating projection methods for solving the dual problem",
        "Submodular functions and their Lov\u00e1sz extensions are frequently used in applications such as learning on directed/undirected graphs and hypergraphs [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], image denoising via total variation regularization [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] and MAP inference in high-order Markov random fields [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "We develop a linearly convergent random coordinate descent method algorithm for solving the dual quadratic DSFM",
        "We develop generalized Frank-Wolfe and min-norm-point methods for efficiently computing the conic projection required in each step of random coordinate descent method and provide a 1/k-rate convergence analysis",
        "We evaluate our methods on semi-supervised learning over hypergraphs using synthetic and real datasets, and demonstrate superior performance both in convergence rate and prediction accuracy compared to existing methods",
        "For quadratic DSFM, we propose to solve the dual problem (6) using the random coordinate descent method method exploiting the separable structure of the feasible set, and to solve (7) using the alternative projection method method"
    ],
    "key_statements": [
        "Nishihara et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] established the linear convergence rate of alternating projection methods for solving the dual problem",
        "Submodular functions and their Lov\u00e1sz extensions are frequently used in applications such as learning on directed/undirected graphs and hypergraphs [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], image denoising via total variation regularization [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] and MAP inference in high-order Markov random fields [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "We develop a linearly convergent random coordinate descent method algorithm for solving the dual quadratic DSFM",
        "We develop generalized Frank-Wolfe and min-norm-point methods for efficiently computing the conic projection required in each step of random coordinate descent method and provide a 1/k-rate convergence analysis",
        "We evaluate our methods on semi-supervised learning over hypergraphs using synthetic and real datasets, and demonstrate superior performance both in convergence rate and prediction accuracy compared to existing methods",
        "We describe the first known linearly convergent algorithms for solving the quadratic DSFM problem",
        "There is a significant difference between [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and the quadratic DSFM problem, since in the latter setting we use a conic set constructed from base polytopes of submodular functions",
        "For quadratic DSFM, we propose to solve the dual problem (6) using the random coordinate descent method method exploiting the separable structure of the feasible set, and to solve (7) using the alternative projection method method",
        "Inspired by the work for decomposable submodular function minimization [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], in what follows, we show that this objective satisfies a weak strong convexity condition, which guarantees linear convergence of the random coordinate descent method algorithm",
        "Unlike quadratic DSFM, the decomposable submodular function minimization involves the computation of projections onto the base polytopes of submodular functions",
        "By using the same trick in (5) for the quadratic term, one may show the dual problem of mix-decomposable submodular function minimization is essentially to find the best approximation of an affine space in terms of a mixture product of cones and base polytopes",
        "All other related results, including the weak-strong duality of the dual, the linear convergence of random coordinate descent method/alternative projection method and the 1/k-rate convergence of the MNP/FW methods can be generalized to the mix-decomposable submodular function minimization case via the same technique developed in this work",
        "We evaluated the proposed algorithms on three UCI datasets: Mushroom, Covertype45, Covertype67, used as standard datasets for supervised Learning on hypergraphs [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "We focused on comparing the convergence of different solvers for quadratic DSFM"
    ],
    "summary": [
        "Nishihara et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] established the linear convergence rate of alternating projection methods for solving the dual problem.",
        "This work takes a substantial step towards solving the QDSFM problem in its most general form by developing a family of algorithms with linear convergence rate and small iteration cost, including the randomized coordinate descent (RCD) and alternative projection (AP) algorithms.",
        "We develop generalized Frank-Wolfe and min-norm-point methods for efficiently computing the conic projection required in each step of RCD and provide a 1/k-rate convergence analysis.",
        "There is a significant difference between [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and the QDSFM problem, since in the latter setting we use a conic set constructed from base polytopes of submodular functions.",
        "Optimizing over the dual variables is equivalent to computing a projection onto the cone Cr. This gives rise to the RCD method summarized in Algorithm 1.",
        "Inspired by the work for DSFM [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], in what follows, we show that this objective satisfies a weak strong convexity condition, which guarantees linear convergence of the RCD algorithm.",
        "Unlike QDSFM, the DSFM involves the computation of projections onto the base polytopes of submodular functions.",
        "By using the same trick in (5) for the quadratic term, one may show the dual problem of mix-DSFM is essentially to find the best approximation of an affine space in terms of a mixture product of cones and base polytopes.",
        "All other related results, including the weak-strong duality of the dual, the linear convergence of RCD/AP and the 1/k-rate convergence of the MNP/FW methods can be generalized to the mix-DSFM case via the same technique developed in this work.",
        "The first three methods all have outer-loops that execute RCD, but with different inner-loop projections computed via the exact projection algorithm for undirected hyperedges, or the generic MNP and FW.",
        "According to Theorem 4.4, the required number of RCD iterations for achieving an -optimal solution for (11) is roughly O(N 2R max(1, 9/(2\u03b2)) maxi,j\u2208[N] Wii/Wjj log 1/ ).",
        "We mainly focus on testing the dependence on the parameters \u03b2 and maxi,j\u2208[N] Wii/Wjj, as the term N 2R is included in the iteration complexity of DSFM and was shown to be necessary given certain submodular structures [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "Note that we did not plot the results for QRCD-MNP and QRCD-FW as the methods converge extremely slowly due to the large sizes of the hyperedges."
    ],
    "headline": "We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] F. Bach, \u201cLearning with submodular functions: A convex optimization perspective,\u201d Foundations and Trends R in Machine Learning, vol. 6, no. 2-3, pp. 145\u2013373, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%2C%202013"
        },
        {
            "id": "2",
            "entry": "[2] L. Lov\u00e1sz, \u201cSubmodular functions and convexity,\u201d in Mathematical Programming The State of the Art. Springer, 1983, pp. 235\u2013257.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lov%C3%A1sz%2C%20L.%20%E2%80%9CSubmodular%20functions%20and%20convexity%2C%E2%80%9D%20in%20Mathematical%20Programming%20The%20State%20of%20the%20Art%201983"
        },
        {
            "id": "3",
            "entry": "[3] X. Zhu, Z. Ghahramani, and J. D. Lafferty, \u201cSemi-supervised learning using gaussian fields and harmonic functions,\u201d in Proceedings of the 20th International Conference on Machine learning, 2003, pp. 912\u2013919.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20X.%20Ghahramani%2C%20Z.%20Lafferty%2C%20J.D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%2C%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20X.%20Ghahramani%2C%20Z.%20Lafferty%2C%20J.D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%2C%202003"
        },
        {
            "id": "4",
            "entry": "[4] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch\u00f6lkopf, \u201cLearning with local and global consistency,\u201d in Advances in Neural Information Processing Systems, 2004, pp. 321\u2013328.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20D.%20Bousquet%2C%20O.%20Lal%2C%20T.N.%20Weston%2C%20J.%20Learning%20with%20local%20and%20global%20consistency%2C%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20D.%20Bousquet%2C%20O.%20Lal%2C%20T.N.%20Weston%2C%20J.%20Learning%20with%20local%20and%20global%20consistency%2C%202004"
        },
        {
            "id": "5",
            "entry": "[5] S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin, \u201cAn iterative regularization method for total variation-based image restoration,\u201d Multiscale Modeling & Simulation, vol. 4, no. 2, pp. 460\u2013489, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osher%2C%20S.%20Burger%2C%20M.%20Goldfarb%2C%20D.%20Xu%2C%20J.%20An%20iterative%20regularization%20method%20for%20total%20variation-based%20image%20restoration%2C%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osher%2C%20S.%20Burger%2C%20M.%20Goldfarb%2C%20D.%20Xu%2C%20J.%20An%20iterative%20regularization%20method%20for%20total%20variation-based%20image%20restoration%2C%202005"
        },
        {
            "id": "6",
            "entry": "[6] A. Chambolle and J. Darbon, \u201cOn total variation minimization and surface evolution using parametric maximum flows,\u201d International Journal of Computer Vision, vol. 84, no. 3, p. 288, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20A.%20Darbon%2C%20J.%20On%20total%20variation%20minimization%20and%20surface%20evolution%20using%20parametric%20maximum%20flows%2C%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20A.%20Darbon%2C%20J.%20On%20total%20variation%20minimization%20and%20surface%20evolution%20using%20parametric%20maximum%20flows%2C%202009"
        },
        {
            "id": "7",
            "entry": "[7] S. Kumar and M. Hebert, \u201cDiscriminative random fields: A discriminative framework for contextual interaction in classification,\u201d in Proceedings of IEEE International Conference on Computer Vision. IEEE, 2003, pp. 1150\u20131157.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20S.%20Hebert%2C%20M.%20Discriminative%20random%20fields%3A%20A%20discriminative%20framework%20for%20contextual%20interaction%20in%20classification%2C%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20S.%20Hebert%2C%20M.%20Discriminative%20random%20fields%3A%20A%20discriminative%20framework%20for%20contextual%20interaction%20in%20classification%2C%202003"
        },
        {
            "id": "8",
            "entry": "[8] N. Z. Shor, Minimization Methods for Non-differentiable Functions. Springer Science & Business Media, 2012, vol. 3.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shor%2C%20N.Z.%20Minimization%20Methods%20for%20Non-differentiable%20Functions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shor%2C%20N.Z.%20Minimization%20Methods%20for%20Non-differentiable%20Functions%202012"
        },
        {
            "id": "9",
            "entry": "[9] P. Stobbe and A. Krause, \u201cEfficient minimization of decomposable submodular functions,\u201d in Advances in Neural Information Processing Systems, 2010, pp. 2208\u20132216.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stobbe%2C%20P.%20Krause%2C%20A.%20Efficient%20minimization%20of%20decomposable%20submodular%20functions%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stobbe%2C%20P.%20Krause%2C%20A.%20Efficient%20minimization%20of%20decomposable%20submodular%20functions%2C%202010"
        },
        {
            "id": "10",
            "entry": "[10] S. Fujishige, Submodular functions and optimization. Elsevier, 2005, vol. 58.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fujishige%2C%20S.%20Submodular%20functions%20and%20optimization%202005"
        },
        {
            "id": "11",
            "entry": "[11] S. Jegelka, F. Bach, and S. Sra, \u201cReflection methods for user-friendly submodular optimization,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 1313\u20131321.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jegelka%2C%20S.%20Bach%2C%20F.%20Sra%2C%20S.%20Reflection%20methods%20for%20user-friendly%20submodular%20optimization%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jegelka%2C%20S.%20Bach%2C%20F.%20Sra%2C%20S.%20Reflection%20methods%20for%20user-friendly%20submodular%20optimization%2C%202013"
        },
        {
            "id": "12",
            "entry": "[12] R. Nishihara, S. Jegelka, and M. I. Jordan, \u201cOn the convergence rate of decomposable submodular function minimization,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 640\u2013648.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nishihara%2C%20R.%20Jegelka%2C%20S.%20Jordan%2C%20M.I.%20On%20the%20convergence%20rate%20of%20decomposable%20submodular%20function%20minimization%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nishihara%2C%20R.%20Jegelka%2C%20S.%20Jordan%2C%20M.I.%20On%20the%20convergence%20rate%20of%20decomposable%20submodular%20function%20minimization%2C%202014"
        },
        {
            "id": "13",
            "entry": "[13] A. Ene and H. Nguyen, \u201cRandom coordinate descent methods for minimizing decomposable submodular functions,\u201d in Proceedings of the International Conference on Machine Learning, 2015, pp. 787\u2013795.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ene%2C%20A.%20Nguyen%2C%20H.%20Random%20coordinate%20descent%20methods%20for%20minimizing%20decomposable%20submodular%20functions%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ene%2C%20A.%20Nguyen%2C%20H.%20Random%20coordinate%20descent%20methods%20for%20minimizing%20decomposable%20submodular%20functions%2C%202015"
        },
        {
            "id": "14",
            "entry": "[14] A. Ene, H. Nguyen, and L. A. V\u00e9gh, \u201cDecomposable submodular function minimization: discrete and continuous,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 2874\u20132884.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ene%2C%20A.%20Nguyen%2C%20H.%20V%C3%A9gh%2C%20L.A.%20Decomposable%20submodular%20function%20minimization%3A%20discrete%20and%20continuous%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ene%2C%20A.%20Nguyen%2C%20H.%20V%C3%A9gh%2C%20L.A.%20Decomposable%20submodular%20function%20minimization%3A%20discrete%20and%20continuous%2C%202017"
        },
        {
            "id": "15",
            "entry": "[15] P. Li and O. Milenkovic, \u201cRevisiting decomposable submodular function minimization with incidence relations,\u201d in Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20P.%20Milenkovic%2C%20O.%20Revisiting%20decomposable%20submodular%20function%20minimization%20with%20incidence%20relations%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20P.%20Milenkovic%2C%20O.%20Revisiting%20decomposable%20submodular%20function%20minimization%20with%20incidence%20relations%2C%202018"
        },
        {
            "id": "16",
            "entry": "[16] R. Johnson and T. Zhang, \u201cOn the effectiveness of laplacian normalization for graph semisupervised learning,\u201d Journal of Machine Learning Research, vol. 8, no. Jul, pp. 1489\u20131517, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20On%20the%20effectiveness%20of%20laplacian%20normalization%20for%20graph%20semisupervised%20learning%2C%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20On%20the%20effectiveness%20of%20laplacian%20normalization%20for%20graph%20semisupervised%20learning%2C%202007"
        },
        {
            "id": "17",
            "entry": "[17] M. Hein, S. Setzer, L. Jost, and S. S. Rangapuram, \u201cThe total variation on hypergraphs-learning on hypergraphs revisited,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 2427\u20132435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20M.%20Setzer%2C%20S.%20Jost%2C%20L.%20Rangapuram%2C%20S.S.%20The%20total%20variation%20on%20hypergraphs-learning%20on%20hypergraphs%20revisited%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20M.%20Setzer%2C%20S.%20Jost%2C%20L.%20Rangapuram%2C%20S.S.%20The%20total%20variation%20on%20hypergraphs-learning%20on%20hypergraphs%20revisited%2C%202013"
        },
        {
            "id": "18",
            "entry": "[18] C. Zhang, S. Hu, Z. G. Tang, and T. H. Chan, \u201cRe-revisiting learning on hypergraphs: confidence interval and subgradient method,\u201d in Proceedings of the International Conference on Machine Learning, 2017, pp. 4026\u20134034.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Hu%2C%20S.%20Tang%2C%20Z.G.%20Chan%2C%20T.H.%20Re-revisiting%20learning%20on%20hypergraphs%3A%20confidence%20interval%20and%20subgradient%20method%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Hu%2C%20S.%20Tang%2C%20Z.G.%20Chan%2C%20T.H.%20Re-revisiting%20learning%20on%20hypergraphs%3A%20confidence%20interval%20and%20subgradient%20method%2C%202017"
        },
        {
            "id": "19",
            "entry": "[19] A. Gammerman, V. Vovk, and V. Vapnik, \u201cLearning by transduction,\u201d in Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1998, pp. 148\u2013155.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gammerman%2C%20A.%20Vovk%2C%20V.%20Vapnik%2C%20V.%20Learning%20by%20transduction%2C%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gammerman%2C%20A.%20Vovk%2C%20V.%20Vapnik%2C%20V.%20Learning%20by%20transduction%2C%201998"
        },
        {
            "id": "20",
            "entry": "[20] T. Joachims, \u201cTransductive learning via spectral graph partitioning,\u201d in Proceedings of the 20th International Conference on Machine Learning, 2003, pp. 290\u2013297.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joachims%2C%20T.%20Transductive%20learning%20via%20spectral%20graph%20partitioning%2C%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joachims%2C%20T.%20Transductive%20learning%20via%20spectral%20graph%20partitioning%2C%202003"
        },
        {
            "id": "21",
            "entry": "[21] X. Zhu, J. Lafferty, and Z. Ghahramani, \u201cCombining active learning and semi-supervised learning using gaussian fields and harmonic functions,\u201d in ICML 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining, vol. 3, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=X%20Zhu%20J%20Lafferty%20and%20Z%20Ghahramani%20Combining%20active%20learning%20and%20semisupervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%20in%20ICML%202003%20workshop%20on%20the%20continuum%20from%20labeled%20to%20unlabeled%20data%20in%20machine%20learning%20and%20data%20mining%20vol%203%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=X%20Zhu%20J%20Lafferty%20and%20Z%20Ghahramani%20Combining%20active%20learning%20and%20semisupervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%20in%20ICML%202003%20workshop%20on%20the%20continuum%20from%20labeled%20to%20unlabeled%20data%20in%20machine%20learning%20and%20data%20mining%20vol%203%202003"
        },
        {
            "id": "22",
            "entry": "[22] P. Li and O. Milenkovic, \u201cInhomogeneous hypergraph clustering with applications,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 2305\u20132315.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20P.%20Milenkovic%2C%20O.%20Inhomogeneous%20hypergraph%20clustering%20with%20applications%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20P.%20Milenkovic%2C%20O.%20Inhomogeneous%20hypergraph%20clustering%20with%20applications%2C%202017"
        },
        {
            "id": "23",
            "entry": "[23] \u2014\u2014, \u201cSubmodular hypergraphs: p-laplacians, cheeger inequalities and spectral clustering,\u201d in Proceedings of the International Conference on Machine learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Submodular%20hypergraphs%3A%20p-laplacians%2C%20cheeger%20inequalities%20and%20spectral%20clustering%2C%E2%80%9D%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Submodular%20hypergraphs%3A%20p-laplacians%2C%20cheeger%20inequalities%20and%20spectral%20clustering%2C%E2%80%9D%202018"
        },
        {
            "id": "24",
            "entry": "[24] L. Page, S. Brin, R. Motwani, and T. Winograd, \u201cThe pagerank citation ranking: Bringing order to the web.\u201d Stanford InfoLab, Tech. Rep., 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Page%2C%20L.%20Brin%2C%20S.%20Motwani%2C%20R.%20Winograd%2C%20T.%20The%20pagerank%20citation%20ranking%3A%20Bringing%20order%20to%20the%20web.%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Page%2C%20L.%20Brin%2C%20S.%20Motwani%2C%20R.%20Winograd%2C%20T.%20The%20pagerank%20citation%20ranking%3A%20Bringing%20order%20to%20the%20web.%201999"
        },
        {
            "id": "25",
            "entry": "[25] T.-H. H. Chan, A. Louis, Z. G. Tang, and C. Zhang, \u201cSpectral properties of hypergraph laplacian and approximation algorithms,\u201d Journal of the ACM (JACM), vol. 65, no. 3, p. 15, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20T.-H.H.%20Louis%2C%20A.%20Tang%2C%20Z.G.%20Zhang%2C%20C.%20Spectral%20properties%20of%20hypergraph%20laplacian%20and%20approximation%20algorithms%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20T.-H.H.%20Louis%2C%20A.%20Tang%2C%20Z.G.%20Zhang%2C%20C.%20Spectral%20properties%20of%20hypergraph%20laplacian%20and%20approximation%20algorithms%2C%202018"
        },
        {
            "id": "26",
            "entry": "[26] T. Chan, Z. G. Tang, X. Wu, and C. Zhang, \u201cDiffusion operator and spectral analysis for directed hypergraph laplacian,\u201d arXiv preprint arXiv:1711.01560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01560"
        },
        {
            "id": "27",
            "entry": "[27] Y. Yoshida, \u201cCheeger inequalities for submodular transformations,\u201d arXiv preprint arXiv:1708.08781, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08781"
        },
        {
            "id": "28",
            "entry": "[28] D. F. Gleich, L.-H. Lim, and Y. Yu, \u201cMultilinear pagerank,\u201d SIAM Journal on Matrix Analysis and Applications, vol. 36, no. 4, pp. 1507\u20131541, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gleich%2C%20D.F.%20Lim%2C%20L.-H.%20Yu%2C%20Y.%20Multilinear%20pagerank%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gleich%2C%20D.F.%20Lim%2C%20L.-H.%20Yu%2C%20Y.%20Multilinear%20pagerank%2C%202015"
        },
        {
            "id": "29",
            "entry": "[29] A. Chambolle and T. Pock, \u201cA first-order primal-dual algorithm for convex problems with applications to imaging,\u201d Journal of Mathematical Imaging and Vision, vol. 40, no. 1, pp. 120\u2013145, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%2C%202011"
        },
        {
            "id": "30",
            "entry": "[30] M. Frank and P. Wolfe, \u201cAn algorithm for quadratic programming,\u201d Naval Research Logistics, vol. 3, no. 1-2, pp. 95\u2013110, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frank%2C%20M.%20Wolfe%2C%20P.%20An%20algorithm%20for%20quadratic%20programming%2C%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frank%2C%20M.%20Wolfe%2C%20P.%20An%20algorithm%20for%20quadratic%20programming%2C%201956"
        },
        {
            "id": "31",
            "entry": "[31] S. Fujishige and S. Isotani, \u201cA submodular function minimization algorithm based on the minimum-norm base,\u201d Pacific Journal of Optimization, vol. 7, no. 1, pp. 3\u201317, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fujishige%2C%20S.%20Isotani%2C%20S.%20A%20submodular%20function%20minimization%20algorithm%20based%20on%20the%20minimum-norm%20base%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fujishige%2C%20S.%20Isotani%2C%20S.%20A%20submodular%20function%20minimization%20algorithm%20based%20on%20the%20minimum-norm%20base%2C%202011"
        },
        {
            "id": "32",
            "entry": "[32] D. Chakrabarty, P. Jain, and P. Kothari, \u201cProvable submodular minimization using Wolfe\u2019s algorithm,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 802\u2013809.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chakrabarty%2C%20D.%20Jain%2C%20P.%20Kothari%2C%20P.%20Provable%20submodular%20minimization%20using%20Wolfe%E2%80%99s%20algorithm%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chakrabarty%2C%20D.%20Jain%2C%20P.%20Kothari%2C%20P.%20Provable%20submodular%20minimization%20using%20Wolfe%E2%80%99s%20algorithm%2C%202014"
        },
        {
            "id": "33",
            "entry": "[33] D. Zhou, J. Huang, and B. Sch\u00f6lkopf, \u201cLearning with hypergraphs: Clustering, classification, and embedding,\u201d in Advances in Neural Information Processing Systems, 2007, pp. 1601\u20131608. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20D.%20Huang%2C%20J.%20Sch%C3%B6lkopf%2C%20B.%20Learning%20with%20hypergraphs%3A%20Clustering%2C%20classification%2C%20and%20embedding%2C%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20D.%20Huang%2C%20J.%20Sch%C3%B6lkopf%2C%20B.%20Learning%20with%20hypergraphs%3A%20Clustering%2C%20classification%2C%20and%20embedding%2C%202007"
        }
    ]
}
