{
    "filename": "7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.pdf",
    "metadata": {
        "title": "Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making",
        "author": "Nishant Desai, Andrew Critch, Stuart J. Russell",
        "date": 2014,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "It is commonly believed that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a Pareto optimal policy, i.e. a policy that cannot be improved upon for one principal without making sacrifices for another. Harsanyi\u2019s theorem shows that when the principals have a common prior on the outcome distributions of all policies, a Pareto optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals\u2019 utilities. In this paper, we derive a more precise generalization for the sequential decision setting in the case of principals with different priors on the dynamics of the environment. We refer to this generalization as the Negotiable Reinforcement Learning (NRL) framework. In this more general case, the relative weight given to each principal\u2019s utility should evolve over time according to how well the agent\u2019s observations conform with that principal\u2019s prior. To gain insight into the dynamics of this new framework, we implement a simple NRL agent and empirically examine its behavior in a simple environment."
    },
    "keywords": [
        {
            "term": "pareto optimal",
            "url": "https://en.wikipedia.org/wiki/pareto_optimal"
        },
        {
            "term": "social choice theory",
            "url": "https://en.wikipedia.org/wiki/social_choice_theory"
        },
        {
            "term": "utility function",
            "url": "https://en.wikipedia.org/wiki/utility_function"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Inverse_reinforcement_learning"
        }
    ],
    "highlights": [
        "It has been argued that the first AI systems with generally super-human cognitive abilities will play a pivotal decision-making role in directing the future of civilization [Bostrom, 2014]",
        "If that is the case, an important question will arise: Whose values will the first super-human AI systems serve? Since safety is a crucial consideration in developing such systems, assuming the institutions building them come to understand the risks and the time investments needed to address them [<a class=\"ref-link\" id=\"cBaum_2016_a\" href=\"#rBaum_2016_a\">Baum, 2016</a>], they will have a large incentive to cooperate in their design rather than racing under time-pressure to build competing systems [<a class=\"ref-link\" id=\"cArmstrong_et+al_2016_a\" href=\"#rArmstrong_et+al_2016_a\">Armstrong et al, 2016</a>]",
        "We model each principal\u2019s prior on the environment and utility function as a Partially Observable Markov Decision Process (POMDP)",
        "An intuition about this property is gained by noting that as the Negotiable Reinforcement Learning agent takes actions in the single Partially Observable Markov Decision Process w(1)D(1) + w(2)D(2), its posterior belief about the value of the latent variable B is exactly equal to its belief about which utility function is \u201ccorrect\u201d for the Partially Observable Markov Decision Process it is acting in",
        "Insofar as Theorem 4 is not particularly mathematically sophisticated\u2014it employs only basic facts about convexity and linear algebra\u2014this suggests there may be more low-hanging fruit to be found in the domain of \u201cmachine implementable social choice theory.\" To recapitulate, Theorem 4 represents two deviations from the intuition of na\u00efve utility aggregation: to achieve Pareto optimality for principals with differing beliefs, an agent must (1) use each principal\u2019s own beliefs in evaluating how well an action will serve that principal\u2019s utility function, and (2) shift the relative priority it assigns to each principal\u2019s expected utilities over time by a factor proportional to how well that principal\u2019s beliefs predict the machine\u2019s inputs",
        "Consider that social choice theory and bargaining theory were both pioneered during the Cold War, when it was particularly compelling to understand the potential for cooperation between human institutions that might behave competitively"
    ],
    "key_statements": [
        "It has been argued that the first AI systems with generally super-human cognitive abilities will play a pivotal decision-making role in directing the future of civilization [Bostrom, 2014]",
        "If that is the case, an important question will arise: Whose values will the first super-human AI systems serve? Since safety is a crucial consideration in developing such systems, assuming the institutions building them come to understand the risks and the time investments needed to address them [<a class=\"ref-link\" id=\"cBaum_2016_a\" href=\"#rBaum_2016_a\">Baum, 2016</a>], they will have a large incentive to cooperate in their design rather than racing under time-pressure to build competing systems [<a class=\"ref-link\" id=\"cArmstrong_et+al_2016_a\" href=\"#rArmstrong_et+al_2016_a\">Armstrong et al, 2016</a>]",
        "We model each principal\u2019s prior on the environment and utility function as a Partially Observable Markov Decision Process (POMDP)",
        "Note that given an action-observation history hi, the Negotiable Reinforcement Learning agent\u2019s posterior belief over the value of B is proportional to the probabilities assigned by each principal\u2019s outlook to the realized observation sequence: P(B = j|hi) \u221d w(j)P(j)(o\u2264i | a<i). This relation, combined with the expected value expression in Equation 4, reveals a pattern in how the weights on the principals\u2019 conditionally expected utilities must change over time, which is the main result of this paper: Theorem 4 (Pareto optimal policy recursion)",
        "An intuition about this property is gained by noting that as the Negotiable Reinforcement Learning agent takes actions in the single Partially Observable Markov Decision Process w(1)D(1) + w(2)D(2), its posterior belief about the value of the latent variable B is exactly equal to its belief about which utility function is \u201ccorrect\u201d for the Partially Observable Markov Decision Process it is acting in",
        "Theorem 4 shows that a Pareto optimal policy must tend, over time, toward prioritizing the expected utility of whichever principal\u2019s beliefs best predict the machine\u2019s inputs better",
        "Insofar as Theorem 4 is not particularly mathematically sophisticated\u2014it employs only basic facts about convexity and linear algebra\u2014this suggests there may be more low-hanging fruit to be found in the domain of \u201cmachine implementable social choice theory.\" To recapitulate, Theorem 4 represents two deviations from the intuition of na\u00efve utility aggregation: to achieve Pareto optimality for principals with differing beliefs, an agent must (1) use each principal\u2019s own beliefs in evaluating how well an action will serve that principal\u2019s utility function, and (2) shift the relative priority it assigns to each principal\u2019s expected utilities over time by a factor proportional to how well that principal\u2019s beliefs predict the machine\u2019s inputs",
        "Consider that social choice theory and bargaining theory were both pioneered during the Cold War, when it was particularly compelling to understand the potential for cooperation between human institutions that might behave competitively"
    ],
    "summary": [
        "It has been argued that the first AI systems with generally super-human cognitive abilities will play a pivotal decision-making role in directing the future of civilization [Bostrom, 2014].",
        "Following directly from this reduction is the intriguing property that a Pareto optimal policy must, over time, prefer the utility function of the principal whose beliefs are a better predictor of the agent\u2019s observations.",
        "During negotiation, the principals will be seeking a Pareto optimal policy for the agent to follow, relative to the POMDPs D(1) and D(2) describing each principal\u2019s outlook.",
        "This relation, combined with the expected value expression in Equation 4, reveals a pattern in how the weights on the principals\u2019 conditionally expected utilities must change over time, which is the main result of this paper: Theorem 4 (Pareto optimal policy recursion).",
        "An intuition about this property is gained by noting that as the NRL agent takes actions in the single POMDP w(1)D(1) + w(2)D(2), its posterior belief about the value of the latent variable B is exactly equal to its belief about which utility function is \u201ccorrect\u201d for the POMDP it is acting in.",
        "Theorem 4 shows that a Pareto optimal policy must tend, over time, toward prioritizing the expected utility of whichever principal\u2019s beliefs best predict the machine\u2019s inputs better.",
        "In these experiments Principal 1 assigns utility to the agent reaching each goal labeled 1 and has the belief that the environment has a deterministic transition model.",
        "Principal 2 assigns utility to the agent reaching each goal labeled 2 and has the belief that the environment has a stochastic transition model.",
        "Since both principals know the agent\u2019s policy, physical state, and belief state at each timestep, each principal can estimate their future rewards by simulating the agent acting in their respective MDP with initial configuration given by the current configuration.",
        "Insofar as Theorem 4 is not particularly mathematically sophisticated\u2014it employs only basic facts about convexity and linear algebra\u2014this suggests there may be more low-hanging fruit to be found in the domain of \u201cmachine implementable social choice theory.\" To recapitulate, Theorem 4 represents two deviations from the intuition of na\u00efve utility aggregation: to achieve Pareto optimality for principals with differing beliefs, an agent must (1) use each principal\u2019s own beliefs in evaluating how well an action will serve that principal\u2019s utility function, and (2) shift the relative priority it assigns to each principal\u2019s expected utilities over time by a factor proportional to how well that principal\u2019s beliefs predict the machine\u2019s inputs.",
        "New technical aspects of social choice and bargaining, along the lines of this paper, will likely continue to emerge"
    ],
    "headline": "We show that building a decision agent whose policy optimizes such an objective is not, in general, ex-ante Pareto optimal ",
    "reference_links": [
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "Armstrong_et+al_2016_a",
            "entry": "Stuart Armstrong, Nick Bostrom, and Carl Shulman. Racing to the Precipice: A Model of Artificial Intelligence Development. AI & SOCIETY, 31(2):201\u2013206, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Armstrong%2C%20Stuart%20Bostrom%2C%20Nick%20Shulman%2C%20Carl%20Racing%20to%20the%20Precipice%3A%20A%20Model%20of%20Artificial%20Intelligence%20Development%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Armstrong%2C%20Stuart%20Bostrom%2C%20Nick%20Shulman%2C%20Carl%20Racing%20to%20the%20Precipice%3A%20A%20Model%20of%20Artificial%20Intelligence%20Development%202016"
        },
        {
            "id": "Baum_2016_a",
            "entry": "Seth D Baum. On the Promotion of Safe and Socially Beneficial Artificial Intelligence. AI & SOCIETY, pages 1\u20139, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baum%2C%20Seth%20D.%20On%20the%20Promotion%20of%20Safe%20and%20Socially%20Beneficial%20Artificial%20Intelligence%202016"
        },
        {
            "id": "Bellman_1957_a",
            "entry": "Richard Bellman. Dynamic Programming. Princeton University Press, 1957. Nick Bostrom. Superintelligence: Paths, dangers, strategies. OUP Oxford, 2014. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellman%2C%20Richard%20Dynamic%20Programming%201957",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellman%2C%20Richard%20Dynamic%20Programming%201957"
        },
        {
            "id": "Gym_2016_a",
            "entry": "Wojciech Zaremba. OpenAI Gym, 2016. Zolt\u00e1n G\u00e1bor, Zsolt Kalm\u00e1r, and Csaba Szepesv\u00e1ri. Multi-Criteria Reinforcement Learning. In ICML, volume 98, pages 197\u2013205, 1998. Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian Reinforcement",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gym%2C%20Wojciech%20Zaremba%20OpenAI%20Multi-Criteria%20Reinforcement%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gym%2C%20Wojciech%20Zaremba%20OpenAI%20Multi-Criteria%20Reinforcement%20Learning%202016"
        },
        {
            "id": "Hadfield-Menell_et+al_2016_a",
            "entry": "Learning: A Survey. ArXiv e-prints, September 2016. Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. Cooperative Inverse",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadfield-Menell%2C%20Anca%20Dragan%20Abbeel%2C%20Pieter%20Russell%2C%20Stuart%20Learning%3A%20A%20Survey.%20ArXiv%20e-prints%202016-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadfield-Menell%2C%20Anca%20Dragan%20Abbeel%2C%20Pieter%20Russell%2C%20Stuart%20Learning%3A%20A%20Survey.%20ArXiv%20e-prints%202016-09"
        },
        {
            "id": "Learning_2016_a",
            "entry": "Reinforcement Learning, 2016. John C Harsanyi. Cardinal Welfare, Individualistic Ethics, and Interpersonal Comparisons of Utility.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Learning%2C%20Reinforcement%20John%20C%20Harsanyi.%20Cardinal%20Welfare%2C%20Individualistic%20Ethics%2C%20and%20Interpersonal%20Comparisons%20of%20Utility%202016"
        },
        {
            "id": "Springer_1980_a",
            "entry": "Springer, 1980. Andrew Y Ng and Stuart J Russell. Algorithms for Inverse Reinforcement Learning. In ICML, pages",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%2C%201980.%20Andrew%20Y%20Ng%20Russell%2C%20Stuart%20J%20Algorithms%20for%20Inverse%20Reinforcement%20Learning%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springer%2C%201980.%20Andrew%20Y%20Ng%20Russell%2C%20Stuart%20J%20Algorithms%20for%20Inverse%20Reinforcement%20Learning%201980"
        },
        {
            "id": "Pineau_et+al_0000_a",
            "entry": "663\u2013670, 2000. Joelle Pineau, Geoff Gordon, and Sebastian Thrun. Point-based Value Iteration: An Anytime",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pineau%2C%20Joelle%20Gordon%2C%20Geoff%20Thrun%2C%20Sebastian%20Point-based%20Value%20Iteration%3A%20An%20Anytime"
        }
    ]
}
