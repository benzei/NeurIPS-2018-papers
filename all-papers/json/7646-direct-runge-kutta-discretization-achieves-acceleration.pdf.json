{
    "filename": "7646-direct-runge-kutta-discretization-achieves-acceleration.pdf",
    "metadata": {
        "title": "Direct Runge-Kutta Discretization Achieves Acceleration",
        "author": "Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, Ali Jadbabaie",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7646-direct-runge-kutta-discretization-achieves-acceleration.pdf"
        },
        "abstract": "We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov\u2019s accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-(s + 2) differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE"
    },
    "keywords": [
        {
            "term": "differential equation",
            "url": "https://en.wikipedia.org/wiki/differential_equation"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "ordinary differential equation",
            "url": "https://en.wikipedia.org/wiki/ordinary_differential_equation"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "runge kutta",
            "url": "https://en.wikipedia.org/wiki/runge_kutta"
        }
    ],
    "highlights": [
        "We study accelerated first-order optimization algorithms for the problem min f (x), (1)<br/><br/>x\u2208Rd where f is convex and sufficiently smooth",
        "Assuming that the objective function is convex and sufficiently smooth, we establish the following: We propose a second-order ordinary differential equation, and show that the sequence of iterates generated by discretizing",
        "\u22122s using a Runge-Kutta integrator converges to the optimal solution at the rate O(N s+1 ), where s is the order of the integrator",
        "We introduce a new local flatness condition for the objective function (Assumption 1), under which Runge-Kutta discretization obtains convergence rates even faster than O(N \u22122), without requiring high-order integrators",
        "We show that if the objective is locally flat around a minimum, by using only gradient information we can obtain a convergence rate of O(N \u2212p), where p quantifies the degree of local flatness",
        "Unlike Nesterov\u2019s accelerated gradient method that only requires first order differentiability, our results require the objective function to be (s + 2)-times differentiable"
    ],
    "key_statements": [
        "We study accelerated first-order optimization algorithms for the problem min f (x), (1)<br/><br/>x\u2208Rd where f is convex and sufficiently smooth",
        "We study accelerated first-order optimization algorithms for the problem min f (x), (1)",
        "Nesterov\u2019s seminal accelerated gradient method [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] matches the oracle lower bound of O(N \u22122) [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and is a central result in the theory of convex optimization",
        "Assuming that the objective function is convex and sufficiently smooth, we establish the following: We propose a second-order ordinary differential equation, and show that the sequence of iterates generated by discretizing",
        "\u22122s using a Runge-Kutta integrator converges to the optimal solution at the rate O(N s+1 ), where s is the order of the integrator",
        "We introduce a new local flatness condition for the objective function (Assumption 1), under which Runge-Kutta discretization obtains convergence rates even faster than O(N \u22122), without requiring high-order integrators",
        "We show that if the objective is locally flat around a minimum, by using only gradient information we can obtain a convergence rate of O(N \u2212p), where p quantifies the degree of local flatness",
        "We introduce a second-order ordinary differential equation and use explicit RK integrators to generate iterates that converge to the optimal solution at a rate faster than O(1/t)",
        "We focus purely on first-order methods and study the stability of discretizing the ordinary differential equation directly when p \u2265 2",
        "Theorem 1 indicates that if the objective has bounded high order derivatives and satisfies the flatness condition in Assumption 1 with p > 0, discretizing the ordinary differential equation in (11) with a high order integrator results in an algorithm that converges to the optimal solution at a rate that is close to O(N \u2212p)",
        "First(Proposition 5), we show that the suboptimality f (x(t)) \u2212 f (x\u2217) along the continuous trajectory of the ordinary differential equation (11) converges to zero sufficiently fast",
        "As in Section 2.1, for the dynamical system y = F (y), let \u03a6h(y0) denote the solution generated by a numerical integrator starting at point y0 with step size h",
        "Let y0 be the initial state of the dynamical system and yN be the final point generated by a Runge-Kutta integrator of order s after N iterations",
        "Unlike Nesterov\u2019s accelerated gradient method that only requires first order differentiability, our results require the objective function to be (s + 2)-times differentiable",
        "The precision of numerical integrators only increases with their order when the function is sufficiently differentiable",
        "We identified a new condition in Assumption 1 that quantifies the local flatness of convex functions"
    ],
    "summary": [
        "We study accelerated first-order optimization algorithms for the problem min f (x), (1)<br/><br/>x\u2208Rd where f is convex and sufficiently smooth.",
        "We introduce a second-order ODE that generates an accelerated first-order method for smooth functions if we discretize it using any Runge-Kutta numerical integrator and choose a suitable step size.",
        "Assuming that the objective function is convex and sufficiently smooth, we establish the following: We propose a second-order ODE, and show that the sequence of iterates generated by discretizing",
        "We introduce a new local flatness condition for the objective function (Assumption 1), under which Runge-Kutta discretization obtains convergence rates even faster than O(N \u22122), without requiring high-order integrators.",
        "We introduce a second-order ODE and use explicit RK integrators to generate iterates that converge to the optimal solution at a rate faster than O(1/t).",
        "Assumption 1 implies that as the iterates approach a minimum, the high order derivatives of the function f , in addition to the gradient, converge to zero.",
        "Theorem 1 indicates that if the objective has bounded high order derivatives and satisfies the flatness condition in Assumption 1 with p > 0, discretizing the ODE in (11) with a high order integrator results in an algorithm that converges to the optimal solution at a rate that is close to O(N \u2212p).",
        "If the function f is convex with L-Lipschitz gradients and is 4th order differentiable, simulating the ODE (11) for p = 2 with a numerical integrator of order s = 2 for N iterations results in the suboptimality bound f \u2212 f (x\u2217) \u2264 C2(f (x0) \u2212 f (x\u2217) +",
        "With an order s numerical integrator for N iterations with step size h = O(N \u22121/(s+1)) results in a convergence rate of",
        "As in Section 2.1, for the dynamical system y = F (y), let \u03a6h(y0) denote the solution generated by a numerical integrator starting at point y0 with step size h.",
        "The flatness assumption (Assumption 1) provides bounds on the operator norm of high order derivatives relative to the objective function suboptimality, and proves crucial in completing the inductive step.",
        "Let y0 be the initial state of the dynamical system and yN be the final point generated by a Runge-Kutta integrator of order s after N iterations.",
        "(c/d) Minimizing L4/logistic loss by discretizing different ODEs with a second order integrator.",
        "We discretize ODEs with different parameter q for L4 loss and logistic loss on the same set of data points using a second order RK integrator.",
        "Acceleration due to local flatness may seem counterintuitive at first, but our analysis reveals why it helps"
    ],
    "headline": "We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation  related to the continuous limit of Nesterov\u2019s accelerated gradient method",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Z. Allen-Zhu and L. Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.1537"
        },
        {
            "id": "2",
            "entry": "[2] F. Alvarez. On the minimizing property of a second order dissipative system in hilbert spaces. SIAM Journal on Control and Optimization, 38(4):1102\u20131119, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alvarez%2C%20F.%20On%20the%20minimizing%20property%20of%20a%20second%20order%20dissipative%20system%20in%20hilbert%20spaces%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alvarez%2C%20F.%20On%20the%20minimizing%20property%20of%20a%20second%20order%20dissipative%20system%20in%20hilbert%20spaces%202000"
        },
        {
            "id": "3",
            "entry": "[3] H. Attouch and R. Cominetti. A dynamical approach to convex minimization coupling approximation with the steepest descent method. Journal of Differential Equations, 128(2):519\u2013540, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Attouch%2C%20H.%20Cominetti%2C%20R.%20A%20dynamical%20approach%20to%20convex%20minimization%20coupling%20approximation%20with%20the%20steepest%20descent%20method%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Attouch%2C%20H.%20Cominetti%2C%20R.%20A%20dynamical%20approach%20to%20convex%20minimization%20coupling%20approximation%20with%20the%20steepest%20descent%20method%201996"
        },
        {
            "id": "4",
            "entry": "[4] H. Attouch and J. Peypouquet. The rate of convergence of nesterov\u2019s accelerated forwardbackward method is actually faster than 1/k2. SIAM Journal on Optimization, 26(3):1824\u2013 1834, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Attouch%2C%20H.%20Peypouquet%2C%20J.%20The%20rate%20of%20convergence%20of%20nesterov%E2%80%99s%20accelerated%20forwardbackward%20method%20is%20actually%20faster%20than%201/k2%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Attouch%2C%20H.%20Peypouquet%2C%20J.%20The%20rate%20of%20convergence%20of%20nesterov%E2%80%99s%20accelerated%20forwardbackward%20method%20is%20actually%20faster%20than%201/k2%202016"
        },
        {
            "id": "5",
            "entry": "[5] H. Attouch, X. Goudou, and P. Redont. The heavy ball with friction method, i. the continuous dynamical system: global exploration of the local minima of a real-valued function by asymptotic analysis of a dissipative dynamical system. Communications in Contemporary Mathematics, 2(01):1\u201334, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Attouch%2C%20H.%20Goudou%2C%20X.%20Redont%2C%20P.%20The%20heavy%20ball%20with%20friction%20method%2C%20i.%20the%20continuous%20dynamical%20system%3A%20global%20exploration%20of%20the%20local%20minima%20of%20a%20real-valued%20function%20by%20asymptotic%20analysis%20of%20a%20dissipative%20dynamical%20system%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Attouch%2C%20H.%20Goudou%2C%20X.%20Redont%2C%20P.%20The%20heavy%20ball%20with%20friction%20method%2C%20i.%20the%20continuous%20dynamical%20system%3A%20global%20exploration%20of%20the%20local%20minima%20of%20a%20real-valued%20function%20by%20asymptotic%20analysis%20of%20a%20dissipative%20dynamical%20system%202000"
        },
        {
            "id": "6",
            "entry": "[6] H. Attouch, J. Bolte, P. Redont, and A. Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-\u0142ojasiewicz inequality. Mathematics of Operations Research, 35(2):438\u2013457, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Attouch%2C%20H.%20Bolte%2C%20J.%20Redont%2C%20P.%20Soubeyran%2C%20A.%20Proximal%20alternating%20minimization%20and%20projection%20methods%20for%20nonconvex%20problems%3A%20An%20approach%20based%20on%20the%20kurdyka-%C5%82ojasiewicz%20inequality%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Attouch%2C%20H.%20Bolte%2C%20J.%20Redont%2C%20P.%20Soubeyran%2C%20A.%20Proximal%20alternating%20minimization%20and%20projection%20methods%20for%20nonconvex%20problems%3A%20An%20approach%20based%20on%20the%20kurdyka-%C5%82ojasiewicz%20inequality%202010"
        },
        {
            "id": "7",
            "entry": "[7] M. Betancourt, M. I. Jordan, and A. C. Wilson. On symplectic optimization. arXiv preprint arXiv:1802.03653, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03653"
        },
        {
            "id": "8",
            "entry": "[8] R. E. Bruck Jr. Asymptotic convergence of nonlinear contraction semigroups in hilbert space. Journal of Functional Analysis, 18(1):15\u201326, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruck%2C%20Jr%2C%20R.E.%20Asymptotic%20convergence%20of%20nonlinear%20contraction%20semigroups%20in%20hilbert%20space%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruck%2C%20Jr%2C%20R.E.%20Asymptotic%20convergence%20of%20nonlinear%20contraction%20semigroups%20in%20hilbert%20space%201975"
        },
        {
            "id": "9",
            "entry": "[9] S. Bubeck, Y. T. Lee, and M. Singh. A geometric alternative to nesterov\u2019s accelerated gradient descent. arXiv preprint arXiv:1506.08187, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.08187"
        },
        {
            "id": "10",
            "entry": "[10] J. Diakonikolas and L. Orecchia. The approximate duality gap technique: A unified theory of first-order methods. arXiv preprint arXiv:1712.02485, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02485"
        },
        {
            "id": "11",
            "entry": "[11] M. Fazlyab, A. Ribeiro, M. Morari, and V. M. Preciado. Analysis of optimization algorithms via integral quadratic constraints: Nonstrongly convex problems. arXiv preprint arXiv:1705.03615, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03615"
        },
        {
            "id": "12",
            "entry": "[12] E. Hairer, C. Lubich, and G. Wanner. Geometric numerical integration: structure-preserving algorithms for ordinary differential equations, volume 31. Springer Science & Business Media, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hairer%2C%20E.%20Lubich%2C%20C.%20Wanner%2C%20G.%20Geometric%20numerical%20integration%3A%20structure-preserving%20algorithms%20for%20ordinary%20differential%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hairer%2C%20E.%20Lubich%2C%20C.%20Wanner%2C%20G.%20Geometric%20numerical%20integration%3A%20structure-preserving%20algorithms%20for%20ordinary%20differential%202006"
        },
        {
            "id": "13",
            "entry": "[13] B. Hu and L. Lessard. Dissipativity theory for nesterov\u2019s accelerated method. arXiv preprint arXiv:1706.04381, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04381"
        },
        {
            "id": "14",
            "entry": "[14] E. Isaacson and H. B. Keller. Analysis of numerical methods. Courier Corporation, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isaacson%2C%20E.%20Keller%2C%20H.B.%20Analysis%20of%20numerical%20methods%201994"
        },
        {
            "id": "15",
            "entry": "[15] W. Krichene, A. Bayen, and P. L. Bartlett. Accelerated mirror descent in continuous and discrete time. In Advances in neural information processing systems, pages 2845\u20132853, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krichene%2C%20W.%20Bayen%2C%20A.%20Bartlett%2C%20P.L.%20Accelerated%20mirror%20descent%20in%20continuous%20and%20discrete%20time%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krichene%2C%20W.%20Bayen%2C%20A.%20Bartlett%2C%20P.L.%20Accelerated%20mirror%20descent%20in%20continuous%20and%20discrete%20time%202015"
        },
        {
            "id": "16",
            "entry": "[16] L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57\u201395, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lessard%2C%20L.%20Recht%2C%20B.%20Packard%2C%20A.%20Analysis%20and%20design%20of%20optimization%20algorithms%20via%20integral%20quadratic%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lessard%2C%20L.%20Recht%2C%20B.%20Packard%2C%20A.%20Analysis%20and%20design%20of%20optimization%20algorithms%20via%20integral%20quadratic%20constraints%202016"
        },
        {
            "id": "17",
            "entry": "[17] S. Lojasiewicz. Ensembles semi-analytiques. Lectures Notes IHES (Bures-sur-Yvette), 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lojasiewicz%2C%20S.%20Ensembles%20semi-analytiques.%20Lectures%20Notes%20IHES%201965"
        },
        {
            "id": "18",
            "entry": "[18] A. Nemirovskii, D. B. Yudin, and E. R. Dawson. Problem complexity and method efficiency in optimization. 1983. (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovskii%2C%20A.%20Yudin%2C%20D.B.%20Dawson%2C%20E.R.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovskii%2C%20A.%20Yudin%2C%20D.B.%20Dawson%2C%20E.R.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983"
        },
        {
            "id": "20",
            "entry": "[20] M. Raginsky and J. Bouvrie. Continuous-time stochastic mirror descent on a network: Variance reduction, consensus, convergence. In Decision and Control (CDC), 2012 IEEE 51st Annual Conference on, pages 6793\u20136800. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raginsky%2C%20M.%20Bouvrie%2C%20J.%20Continuous-time%20stochastic%20mirror%20descent%20on%20a%20network%3A%20Variance%20reduction%2C%20consensus%2C%20convergence%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raginsky%2C%20M.%20Bouvrie%2C%20J.%20Continuous-time%20stochastic%20mirror%20descent%20on%20a%20network%3A%20Variance%20reduction%2C%20consensus%2C%20convergence%202012"
        },
        {
            "id": "21",
            "entry": "[21] D. Scieur, A. d\u2019Aspremont, and F. Bach. Regularized nonlinear acceleration. In Advances In Neural Information Processing Systems, pages 712\u2013720, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scieur%2C%20D.%20d%E2%80%99Aspremont%2C%20A.%20Bach%2C%20F.%20Regularized%20nonlinear%20acceleration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scieur%2C%20D.%20d%E2%80%99Aspremont%2C%20A.%20Bach%2C%20F.%20Regularized%20nonlinear%20acceleration%202016"
        },
        {
            "id": "22",
            "entry": "[22] D. Scieur, V. Roulet, F. Bach, and A. d\u2019Aspremont. Integration methods and accelerated optimization algorithms. arXiv preprint arXiv:1702.06751, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06751"
        },
        {
            "id": "23",
            "entry": "[23] W. Su, S. Boyd, and E. Candes. A differential equation for modeling nesterovs accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pages 2510\u20132518, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20W.%20Boyd%2C%20S.%20Candes%2C%20E.%20A%20differential%20equation%20for%20modeling%20nesterovs%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Su%2C%20W.%20Boyd%2C%20S.%20Candes%2C%20E.%20A%20differential%20equation%20for%20modeling%20nesterovs%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014"
        },
        {
            "id": "24",
            "entry": "[24] J. Verner. High-order explicit runge-kutta pairs with low stage order. Applied numerical mathematics, 22(1-3):345\u2013357, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verner%2C%20J.%20High-order%20explicit%20runge-kutta%20pairs%20with%20low%20stage%20order%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verner%2C%20J.%20High-order%20explicit%20runge-kutta%20pairs%20with%20low%20stage%20order%201996"
        },
        {
            "id": "25",
            "entry": "[25] M. West. Variational integrators. PhD thesis, California Institute of Technology, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=West%2C%20M.%20Variational%20integrators%202004"
        },
        {
            "id": "26",
            "entry": "[26] A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351\u2013E7358, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wibisono%2C%20A.%20Wilson%2C%20A.C.%20Jordan%2C%20M.I.%20A%20variational%20perspective%20on%20accelerated%20methods%20in%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wibisono%2C%20A.%20Wilson%2C%20A.C.%20Jordan%2C%20M.I.%20A%20variational%20perspective%20on%20accelerated%20methods%20in%20optimization%202016"
        }
    ]
}
