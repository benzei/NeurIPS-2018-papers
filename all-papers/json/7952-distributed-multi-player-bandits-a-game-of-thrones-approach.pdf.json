{
    "filename": "7952-distributed-multi-player-bandits-a-game-of-thrones-approach.pdf",
    "metadata": {
        "title": "Distributed Multi-Player Bandits - a Game of Thrones Approach",
        "author": "Ilai Bistritz, Amir Leshem",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7952-distributed-multi-player-bandits-a-game-of-thrones-approach.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider a multi-armed bandit game where N players compete for K arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed. Performance is measured using the expected sum of regrets, compared to the optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O log2 T . This is the first algorithm to achieve a poly-logarithmic regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible."
    },
    "keywords": [
        {
            "term": "game of thrones",
            "url": "https://en.wikipedia.org/wiki/game_of_thrones"
        }
    ],
    "highlights": [
        "An agent needs to learn on the run how to behave optimally",
        "The goal of the game of thrones phase is to let the players distributedly identify the optimal solution for the assignment problem on the estimated expected rewards from the exploration phase",
        "We can prove the main lemma of this section that gives an upper bound for the probability that the game of thrones phase does not lead to the optimal solution",
        "We proposed a novel fully distributed algorithm that achieves a poly-logarithmic expected total regret of near-O log2 T when the horizon T is unknown to the players"
    ],
    "key_statements": [
        "An agent needs to learn on the run how to behave optimally",
        "The goal of the game of thrones phase is to let the players distributedly identify the optimal solution for the assignment problem on the estimated expected rewards from the exploration phase",
        "We can prove the main lemma of this section that gives an upper bound for the probability that the game of thrones phase does not lead to the optimal solution",
        "We proposed a novel fully distributed algorithm that achieves a poly-logarithmic expected total regret of near-O log2 T when the horizon T is unknown to the players"
    ],
    "summary": [
        "An agent needs to learn on the run how to behave optimally.",
        "We present a fully distributed multi-player multi-armed bandit algorithm for the resource allocation and collision scenario, based on these modified dynamics.",
        "We can assume instead that {\u03bcn,i} are generated at random using a continuous distribution, so the optimal solution is unique with probability 1 (i.e., \u201cfor almost all games\u201d), with arbitrary distributions for the rewards that have expectations {\u03bcn,i}.",
        "This throne is the arm they must play in the allocation that maximizes the sum of the expected rewards of all players.",
        "Players estimate the expected reward of each arm.",
        "The goal of the GoT phase is to let the players distributedly identify the optimal solution for the assignment problem on the estimated expected rewards from the exploration phase.",
        "It is done by playing a game with the estimated expectations as utilities, using random dynamics that probabilistically prefer strategy profiles with a higher sum of utilities.",
        "If the exploration phase is long enough, players know their arm expectations accurately enough with a very small failure probability.",
        "Let \u03bcnk,i be the estimated reward expectations using all the exploration phases up to epoch k.",
        "These dynamics guarantee that the optimal state will be played a significant amount of time, and only require the players to know their own action and the received payoff on each turn.",
        "Where \u03bckn,an is the estimation of the expected reward of arm an, from all the exploration phases that have ended, up to epoch k.",
        "These dynamics guarantee that the optimal sum of utilities strategy profiles will be played a sufficiently large portion of the turns.",
        "Since she chooses her action at random, there is a positive probability that she will pick the same arm as any of the content players.",
        "We can prove the main lemma of this section that gives an upper bound for the probability that the GoT phase does not lead to the optimal solution.",
        "While visiting near-optimal solutions inflicts linear regret at the beginning, it is very satisfying in practice considering that players cannot communicate and have a matrix of expected rewards.",
        "In contrast to all existing multi-player bandit problems, we allow for different arm expected rewards between players and assume each player only knows her own actions and rewards.",
        "We proposed a novel fully distributed algorithm that achieves a poly-logarithmic expected total regret of near-O log2 T when the horizon T is unknown to the players.",
        "The algorithm designer can do so by simulating a random model for the unknown environment and varying the parameters, knowing that only a very slack accuracy is needed for the tuning"
    ],
    "headline": "We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O log2 T",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Shahrampour, A. Rakhlin, and A. Jadbabaie, \u201cMulti-armed bandits in multi-agent networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, 2017, pp. 2786\u20132790.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shahrampour%2C%20S.%20Rakhlin%2C%20A.%20Jadbabaie%2C%20A.%20%E2%80%9CMulti-armed%20bandits%20in%20multi-agent%20networks%2C%E2%80%9D%20in%20Acoustics%2C%20Speech%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shahrampour%2C%20S.%20Rakhlin%2C%20A.%20Jadbabaie%2C%20A.%20%E2%80%9CMulti-armed%20bandits%20in%20multi-agent%20networks%2C%E2%80%9D%20in%20Acoustics%2C%20Speech%202017"
        },
        {
            "id": "2",
            "entry": "[2] E. Hillel, Z. S. Karnin, T. Koren, R. Lempel, and O. Somekh, \u201cDistributed exploration in multiarmed bandits,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 854\u2013862.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hillel%2C%20E.%20Karnin%2C%20Z.S.%20Koren%2C%20T.%20Lempel%2C%20R.%20Distributed%20exploration%20in%20multiarmed%20bandits%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hillel%2C%20E.%20Karnin%2C%20Z.S.%20Koren%2C%20T.%20Lempel%2C%20R.%20Distributed%20exploration%20in%20multiarmed%20bandits%2C%202013"
        },
        {
            "id": "3",
            "entry": "[3] P. Landgren, V. Srivastava, and N. E. Leonard, \u201cDistributed cooperative decision-making in multiarmed bandits: Frequentist and Bayesian algorithms,\u201d in Decision and Control (CDC), 2016 IEEE 55th Conference on, 2016, pp. 167\u2013172.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Landgren%2C%20P.%20Srivastava%2C%20V.%20Leonard%2C%20N.E.%20Distributed%20cooperative%20decision-making%20in%20multiarmed%20bandits%3A%20Frequentist%20and%20Bayesian%20algorithms%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Landgren%2C%20P.%20Srivastava%2C%20V.%20Leonard%2C%20N.E.%20Distributed%20cooperative%20decision-making%20in%20multiarmed%20bandits%3A%20Frequentist%20and%20Bayesian%20algorithms%2C%202016"
        },
        {
            "id": "4",
            "entry": "[4] N. Cesa-Bianchi, C. Gentile, Y. Mansour, and A. Minora, \u201cDelay and cooperation in nonstochastic bandits,\u201d in Conference on Learning Theory, 2016, pp. 605\u2013622.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20N.%20Gentile%2C%20C.%20Mansour%2C%20Y.%20Minora%2C%20A.%20%E2%80%9CDelay%20and%20cooperation%20in%20nonstochastic%20bandits%2C%E2%80%9D%20in%20Conference%20on%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20N.%20Gentile%2C%20C.%20Mansour%2C%20Y.%20Minora%2C%20A.%20%E2%80%9CDelay%20and%20cooperation%20in%20nonstochastic%20bandits%2C%E2%80%9D%20in%20Conference%20on%20Learning%202016"
        },
        {
            "id": "5",
            "entry": "[5] B. Szorenyi, R. Busa-Fekete, I. Hegedus, R. Ormandi, M. Jelasity, and B. Kegl, \u201cGossip-based distributed stochastic bandit algorithms,\u201d in International Conference on Machine Learning, 2013, pp. 19\u201327.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szorenyi%2C%20B.%20Busa-Fekete%2C%20R.%20Hegedus%2C%20I.%20Ormandi%2C%20R.%20Gossip-based%20distributed%20stochastic%20bandit%20algorithms%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szorenyi%2C%20B.%20Busa-Fekete%2C%20R.%20Hegedus%2C%20I.%20Ormandi%2C%20R.%20Gossip-based%20distributed%20stochastic%20bandit%20algorithms%2C%202013"
        },
        {
            "id": "6",
            "entry": "[6] N. Korda, B. Szorenyi, and L. Shuai, \u201cDistributed clustering of linear bandits in peer to peer networks,\u201d in International Conference on Machine Learning, vol. 48, 2016, pp. 1301\u20131309.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Korda%2C%20N.%20Szorenyi%2C%20B.%20Shuai%2C%20L.%20Distributed%20clustering%20of%20linear%20bandits%20in%20peer%20to%20peer%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Korda%2C%20N.%20Szorenyi%2C%20B.%20Shuai%2C%20L.%20Distributed%20clustering%20of%20linear%20bandits%20in%20peer%20to%20peer%20networks%2C%202016"
        },
        {
            "id": "7",
            "entry": "[7] J. Rosenski, O. Shamir, and L. Szlak, \u201cMulti-player bandits\u2013a musical chairs approach,\u201d in International Conference on Machine Learning, 2016, pp. 155\u2013163.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenski%2C%20J.%20Shamir%2C%20O.%20Szlak%2C%20L.%20Multi-player%20bandits%E2%80%93a%20musical%20chairs%20approach%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosenski%2C%20J.%20Shamir%2C%20O.%20Szlak%2C%20L.%20Multi-player%20bandits%E2%80%93a%20musical%20chairs%20approach%2C%202016"
        },
        {
            "id": "8",
            "entry": "[8] N. Nayyar, D. Kalathil, and R. Jain, \u201cOn regret-optimal learning in decentralized multi-player multi-armed bandits,\u201d IEEE Transactions on Control of Network Systems, vol. PP, no. 99, pp. 1\u20131, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nayyar%2C%20N.%20Kalathil%2C%20D.%20Jain%2C%20R.%20On%20regret-optimal%20learning%20in%20decentralized%20multi-player%20multi-armed%20bandits%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nayyar%2C%20N.%20Kalathil%2C%20D.%20Jain%2C%20R.%20On%20regret-optimal%20learning%20in%20decentralized%20multi-player%20multi-armed%20bandits%2C%202016"
        },
        {
            "id": "9",
            "entry": "[9] D. Kalathil, N. Nayyar, and R. Jain, \u201cDecentralized learning for multiplayer multiarmed bandits,\u201d IEEE Transactions on Information Theory, vol. 60, no. 4, pp. 2331\u20132345, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalathil%2C%20D.%20Nayyar%2C%20N.%20Jain%2C%20R.%20Decentralized%20learning%20for%20multiplayer%20multiarmed%20bandits%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalathil%2C%20D.%20Nayyar%2C%20N.%20Jain%2C%20R.%20Decentralized%20learning%20for%20multiplayer%20multiarmed%20bandits%2C%202014"
        },
        {
            "id": "10",
            "entry": "[10] K. Liu and Q. Zhao, \u201cDistributed learning in multi-armed bandit with multiple players,\u201d IEEE Transactions on Signal Processing, vol. 58, no. 11, pp. 5667\u20135681, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20K.%20Zhao%2C%20Q.%20Distributed%20learning%20in%20multi-armed%20bandit%20with%20multiple%20players%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20K.%20Zhao%2C%20Q.%20Distributed%20learning%20in%20multi-armed%20bandit%20with%20multiple%20players%2C%202010"
        },
        {
            "id": "11",
            "entry": "[11] S. Vakili, K. Liu, and Q. Zhao, \u201cDeterministic sequencing of exploration and exploitation for multi-armed bandit problems,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 7, no. 5, pp. 759\u2013767, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vakili%2C%20S.%20Liu%2C%20K.%20Zhao%2C%20Q.%20Deterministic%20sequencing%20of%20exploration%20and%20exploitation%20for%20multi-armed%20bandit%20problems%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vakili%2C%20S.%20Liu%2C%20K.%20Zhao%2C%20Q.%20Deterministic%20sequencing%20of%20exploration%20and%20exploitation%20for%20multi-armed%20bandit%20problems%2C%202013"
        },
        {
            "id": "12",
            "entry": "[12] L. Lai, H. Jiang, and H. V. Poor, \u201cMedium access in cognitive radio networks: A competitive multi-armed bandit framework,\u201d in Signals, Systems and Computers, 2008 42nd Asilomar Conference on, 2008, pp. 98\u2013102.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20L.%20Jiang%2C%20H.%20Poor%2C%20H.V.%20Medium%20access%20in%20cognitive%20radio%20networks%3A%20A%20competitive%20multi-armed%20bandit%20framework%2C%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20L.%20Jiang%2C%20H.%20Poor%2C%20H.V.%20Medium%20access%20in%20cognitive%20radio%20networks%3A%20A%20competitive%20multi-armed%20bandit%20framework%2C%202008"
        },
        {
            "id": "13",
            "entry": "[13] A. Anandkumar, N. Michael, A. K. Tang, and A. Swami, \u201cDistributed algorithms for learning and cognitive medium access with logarithmic regret,\u201d IEEE Journal on Selected Areas in Communications, vol. 29, no. 4, pp. 731\u2013745, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20A.%20Michael%2C%20N.%20Tang%2C%20A.K.%20Swami%2C%20A.%20Distributed%20algorithms%20for%20learning%20and%20cognitive%20medium%20access%20with%20logarithmic%20regret%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20A.%20Michael%2C%20N.%20Tang%2C%20A.K.%20Swami%2C%20A.%20Distributed%20algorithms%20for%20learning%20and%20cognitive%20medium%20access%20with%20logarithmic%20regret%2C%202011"
        },
        {
            "id": "14",
            "entry": "[14] H. Liu, K. Liu, and Q. Zhao, \u201cLearning in a changing world: Restless multiarmed bandit with unknown dynamics,\u201d IEEE Transactions on Information Theory, vol. 59, no. 3, pp. 1902\u20131916, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20H.%20Liu%2C%20K.%20Zhao%2C%20Q.%20Learning%20in%20a%20changing%20world%3A%20Restless%20multiarmed%20bandit%20with%20unknown%20dynamics%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20H.%20Liu%2C%20K.%20Zhao%2C%20Q.%20Learning%20in%20a%20changing%20world%3A%20Restless%20multiarmed%20bandit%20with%20unknown%20dynamics%2C%202013"
        },
        {
            "id": "15",
            "entry": "[15] O. Avner and S. Mannor, \u201cConcurrent bandits and cognitive radio networks,\u201d in Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2014, pp. 66\u201381.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avner%2C%20O.%20Mannor%2C%20S.%20Concurrent%20bandits%20and%20cognitive%20radio%20networks%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avner%2C%20O.%20Mannor%2C%20S.%20Concurrent%20bandits%20and%20cognitive%20radio%20networks%2C%202014"
        },
        {
            "id": "16",
            "entry": "[16] N. Evirgen and A. Kose, \u201cThe effect of communication on noncooperative multiplayer multiarmed bandit problems,\u201d in arXiv preprint arXiv:1711.01628, 2017, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01628"
        },
        {
            "id": "17",
            "entry": "[17] L. Besson and E. Kaufmann, \u201cMulti-player bandits revisited,\u201d in Algorithmic Learning Theory, 2018, pp. 56\u201392.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Besson%2C%20L.%20Kaufmann%2C%20E.%20%E2%80%9CMulti-player%20bandits%20revisited%2C%E2%80%9D%20in%20Algorithmic%20Learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Besson%2C%20L.%20Kaufmann%2C%20E.%20%E2%80%9CMulti-player%20bandits%20revisited%2C%E2%80%9D%20in%20Algorithmic%20Learning%202018"
        },
        {
            "id": "18",
            "entry": "[18] J. Cohen, A. Heliou, and P. Mertikopoulos, \u201cLearning with bandit feedback in potential games,\u201d in Proceedings of the 31th International Conference on Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20J.%20Heliou%2C%20A.%20Mertikopoulos%2C%20P.%20Learning%20with%20bandit%20feedback%20in%20potential%20games%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20J.%20Heliou%2C%20A.%20Mertikopoulos%2C%20P.%20Learning%20with%20bandit%20feedback%20in%20potential%20games%2C%202017"
        },
        {
            "id": "19",
            "entry": "[19] O. Avner and S. Mannor, \u201cMulti-user lax communications: a multi-armed bandit approach,\u201d in INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications, IEEE, 2016, pp. 1\u20139.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avner%2C%20O.%20Mannor%2C%20S.%20Multi-user%20lax%20communications%3A%20a%20multi-armed%20bandit%20approach%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avner%2C%20O.%20Mannor%2C%20S.%20Multi-user%20lax%20communications%3A%20a%20multi-armed%20bandit%20approach%2C%202016"
        },
        {
            "id": "20",
            "entry": "[20] C. H. Papadimitriou and K. Steiglitz, Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papadimitriou%2C%20C.H.%20Steiglitz%2C%20K.%20Combinatorial%20optimization%3A%20algorithms%20and%20complexity%201998"
        },
        {
            "id": "21",
            "entry": "[21] D. P. Bertsekas, \u201cThe auction algorithm: A distributed relaxation method for the assignment problem,\u201d Annals of operations research, vol. 14, no. 1, pp. 105\u2013123, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20The%20auction%20algorithm%3A%20A%20distributed%20relaxation%20method%20for%20the%20assignment%20problem%2C%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20D.P.%20The%20auction%20algorithm%3A%20A%20distributed%20relaxation%20method%20for%20the%20assignment%20problem%2C%201988"
        },
        {
            "id": "22",
            "entry": "[22] M. M. Zavlanos, L. Spesivtsev, and G. J. Pappas, \u201cA distributed auction algorithm for the assignment problem,\u201d in Decision and Control, 2008. CDC 2008. 47th IEEE Conference on, 2008, pp. 1212\u20131217.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zavlanos%2C%20M.M.%20Spesivtsev%2C%20L.%20Pappas%2C%20G.J.%20%E2%80%9CA%20distributed%20auction%20algorithm%20for%20the%20assignment%20problem%2C%E2%80%9D%20in%20Decision%20and%20Control%202008"
        },
        {
            "id": "23",
            "entry": "[23] B. S. Pradelski and H. P. Young, \u201cLearning efficient Nash equilibria in distributed systems,\u201d Games and Economic behavior, vol. 75, no. 2, pp. 882\u2013897, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pradelski%2C%20B.S.%20Young%2C%20H.P.%20Learning%20efficient%20Nash%20equilibria%20in%20distributed%20systems%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pradelski%2C%20B.S.%20Young%2C%20H.P.%20Learning%20efficient%20Nash%20equilibria%20in%20distributed%20systems%2C%202012"
        },
        {
            "id": "24",
            "entry": "[24] J. R. Marden, H. P. Young, and L. Y. Pao, \u201cAchieving pareto optimality through distributed learning,\u201d SIAM Journal on Control and Optimization, vol. 52, no. 5, pp. 2753\u20132770, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marden%2C%20J.R.%20Young%2C%20H.P.%20Pao%2C%20L.Y.%20Achieving%20pareto%20optimality%20through%20distributed%20learning%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marden%2C%20J.R.%20Young%2C%20H.P.%20Pao%2C%20L.Y.%20Achieving%20pareto%20optimality%20through%20distributed%20learning%2C%202014"
        },
        {
            "id": "25",
            "entry": "[25] A. Menon and J. S. Baras, \u201cConvergence guarantees for a decentralized algorithm achieving pareto optimality,\u201d in American Control Conference (ACC), 2013, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20A.%20Baras%2C%20J.S.%20Convergence%20guarantees%20for%20a%20decentralized%20algorithm%20achieving%20pareto%20optimality%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20A.%20Baras%2C%20J.S.%20Convergence%20guarantees%20for%20a%20decentralized%20algorithm%20achieving%20pareto%20optimality%2C%202013"
        },
        {
            "id": "26",
            "entry": "[26] T. L. Lai and H. Robbins, \u201cAsymptotically efficient adaptive allocation rules,\u201d Advances in applied mathematics, vol. 6, no. 1, pp. 4\u201322, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%2C%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%2C%201985"
        },
        {
            "id": "27",
            "entry": "[27] K.-M. Chung, H. Lam, Z. Liu, and M. Mitzenmacher, \u201cChernoff-Hoeffding bounds for Markov chains: Generalized and simplified,\u201d in 29th International Symposium on Theoretical Aspects of Computer Science, 2012, p. 124. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20K.-M.%20Lam%2C%20H.%20Liu%2C%20Z.%20Mitzenmacher%2C%20M.%20Chernoff-Hoeffding%20bounds%20for%20Markov%20chains%3A%20Generalized%20and%20simplified%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20K.-M.%20Lam%2C%20H.%20Liu%2C%20Z.%20Mitzenmacher%2C%20M.%20Chernoff-Hoeffding%20bounds%20for%20Markov%20chains%3A%20Generalized%20and%20simplified%2C%202012"
        }
    ]
}
