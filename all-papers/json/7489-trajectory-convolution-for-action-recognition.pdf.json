{
    "filename": "7489-trajectory-convolution-for-action-recognition.pdf",
    "metadata": {
        "title": "Trajectory Convolution for Action Recognition",
        "author": "Yue Zhao, Yuanjun Xiong, Dahua Lin",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7489-trajectory-convolution-for-action-recognition.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "How to leverage the temporal dimension is one major question in video analysis. Recent works [<a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>] suggest an efficient approach to video feature learning, i.e., factorizing 3D convolutions into separate components respectively for spatial and temporal convolutions. The temporal convolution, however, comes with an implicit assumption \u2013 the feature maps across time steps are well aligned so that the features at the same locations can be aggregated. This assumption can be overly strong in practical applications, especially in action recognition where the motion serves as a crucial cue. In this work, we propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the existing temporal convolution. This operation explicitly takes into account the changes in contents caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths, trajectories. On two large-scale action recognition datasets, Something-Something V1 and Kinetics, the proposed network architecture achieves notable improvement over strong baselines."
    },
    "keywords": [
        {
            "term": "temporal dimension",
            "url": "https://en.wikipedia.org/wiki/temporal_dimension"
        },
        {
            "term": "human action",
            "url": "https://en.wikipedia.org/wiki/human_action"
        },
        {
            "term": "action recognition",
            "url": "https://en.wikipedia.org/wiki/action_recognition"
        },
        {
            "term": "feature learning",
            "url": "https://en.wikipedia.org/wiki/feature_learning"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        }
    ],
    "highlights": [
        "The past decade has witnessed significant progress in action recognition [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], especially due to the advances in deep learning",
        "Deep learning based methods for action recognition mostly fall into two categories, two-stream architectures [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] with 2D convolutional networks and 3D convolutional networks [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]",
        "There have been attempts to move beyond 3D convolution and further improve the efficiency and effectiveness of joint spatio-temporal analysis",
        "By revisiting the idea of trajectory modeling in the action recognition literature, we introduce the concept of trajectory convolution",
        "Runtime Cost In Table 6, we report the runtime of the proposed TrajectoryNet with two settings: (1) the one whose trajectories are from pre-computed TV-L1 and (2) the one whose trajectories are inferred from MotionNet-(17)",
        "We propose a unified end-to-end architecture called TrajectoryNet for action recognition"
    ],
    "key_statements": [
        "The past decade has witnessed significant progress in action recognition [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], especially due to the advances in deep learning",
        "Deep learning based methods for action recognition mostly fall into two categories, two-stream architectures [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] with 2D convolutional networks and 3D convolutional networks [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]",
        "There have been attempts to move beyond 3D convolution and further improve the efficiency and effectiveness of joint spatio-temporal analysis",
        "Both methods are based on an implicit assumption that the feature maps across frames are well aligned so that the features at the same locations can be aggregated via temporal convolution",
        "Inspired by the Separable-3D network [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>], our design involves a cascade of convolutional operations respectively along the spatial and temporal dimensions",
        "To sum up the discussion above, we provide a comparison of our approach with previous works on action recognition in Table 1",
        "The TrajectoryNet model is built with the trajectory convolution operation",
        "By revisiting the idea of trajectory modeling in the action recognition literature, we introduce the concept of trajectory convolution",
        "The appearance feature map for trajectory convolution is optionally concatenated with the down-sampled motion field to introduce extra motion information",
        "Adding trajectory convolution at lower levels increases the precision of motion estimation, but the receptive field for sampling position is limited.\n3.5",
        "Generating trajectories As stated above, we study two methods to generate trajectories: one is based on variational methods and the other is based on CNNs",
        "Trajectories with step greater than one Here we evaluate the model which accepts an input of 16 frames but at a sampling step of 2",
        "Runtime Cost In Table 6, we report the runtime of the proposed TrajectoryNet with two settings: (1) the one whose trajectories are from pre-computed TV-L1 and (2) the one whose trajectories are inferred from MotionNet-(17)",
        "We propose a unified end-to-end architecture called TrajectoryNet for action recognition"
    ],
    "summary": [
        "The past decade has witnessed significant progress in action recognition [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], especially due to the advances in deep learning.",
        "In pursuit of this question, we develop a new CNN architecture for learning video features, called TrajectoryNet. Inspired by the Separable-3D network [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>], our design involves a cascade of convolutional operations respectively along the spatial and temporal dimensions.",
        "The trajectories can be derived from either a precomputed optical flow field or a dense flow prediction network trained jointly with the features.",
        "Zhu et al proposed MotionNet [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] to learn dense flow fields in an unsupervised manner and plugged it into a two-stream network [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] to be finetuned for action recognition task.",
        "While maintaining filter weights invariant to the input, the proposed deformable convolution first learns a dense offset map from the input, and applies it to the regular feature map for re-sampling.",
        "We achieve the combination of appearance feature and motion information in terms of trajectory with minimal increase of network parameters.",
        "The 1D temporal convolution component of a (2+1)D-convolutional block is replaced by a trajectory convolution with down-sampled motion field, such as a pre-computed optical flow, in the middle level of the network.",
        "The appearance feature map for trajectory convolution is optionally concatenated with the down-sampled motion field to introduce extra motion information.",
        "Adding trajectory convolution at higher levels is likely to provide less motion information since spatial resolution is reduced and down-sampled optical flow may be inaccurate.",
        "Adding trajectory convolution at lower levels increases the precision of motion estimation, but the receptive field for sampling position is limited.",
        "Once pre-trained, the MotionNet can be plugged into the TrajectoryNet architecture to substitute the input of pre-computed optical flow.",
        "We modify the original model in [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] to produce optical flow map of the same resolution of feature maps where the trajectory convolution operates on.",
        "Top-1 Acc. Top-5 Acc. Combining motion and appearance features We compare the results of incorporating motion information into the trajectory convolution in Table 3.",
        "We compare with several other methods, such as the early spatial fusion by concatenation with motion feature map [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] and the late fusion used in the two-stream network [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "The approach is to incorporate the repeatedly proven idea of trajectory modeling into the Separable-3D network by introducing a new operation named trajectory convolution.",
        "The proposed architecture achieves notable improvements over the Separable-3D baseline, providing a new perspective of explicitly considering motion dynamics in the deep networks."
    ],
    "headline": "We propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the existing temporal convolution",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4724\u20134733. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20Joao%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20Joao%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20a%20new%20model%20and%20the%20kinetics%20dataset%202017"
        },
        {
            "id": "2",
            "entry": "[2] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In The IEEE International Conference on Computer Vision (ICCV), pages 764\u2013773, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Jifeng%20Qi%2C%20Haozhi%20Xiong%2C%20Yuwen%20Li%2C%20Yi%20Deformable%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Jifeng%20Qi%2C%20Haozhi%20Xiong%2C%20Yuwen%20Li%2C%20Yi%20Deformable%20convolutional%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pages 886\u2013893. IEEE, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalal%2C%20Navneet%20Triggs%2C%20Bill%20Histograms%20of%20oriented%20gradients%20for%20human%20detection%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalal%2C%20Navneet%20Triggs%2C%20Bill%20Histograms%20of%20oriented%20gradients%20for%20human%20detection%202005"
        },
        {
            "id": "4",
            "entry": "[4] Navneet Dalal, Bill Triggs, and Cordelia Schmid. Human detection using oriented histograms of flow and appearance. In European Conference on Computer Vision (ECCV), pages 428\u2013441.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalal%2C%20Navneet%20Triggs%2C%20Bill%20Schmid%2C%20Cordelia%20Human%20detection%20using%20oriented%20histograms%20of%20flow%20and%20appearance",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalal%2C%20Navneet%20Triggs%2C%20Bill%20Schmid%2C%20Cordelia%20Human%20detection%20using%20oriented%20histograms%20of%20flow%20and%20appearance"
        },
        {
            "id": "5",
            "entry": "[5] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In The IEEE International Conference on Computer Vision (ICCV), pages 2758\u20132766, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20Alexey%20Fischer%2C%20Philipp%20Ilg%2C%20Eddy%20Hausser%2C%20Philip%20Flownet%3A%20Learning%20optical%20flow%20with%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dosovitskiy%2C%20Alexey%20Fischer%2C%20Philipp%20Ilg%2C%20Eddy%20Hausser%2C%20Philip%20Flownet%3A%20Learning%20optical%20flow%20with%20convolutional%20networks%202015"
        },
        {
            "id": "6",
            "entry": "[6] Christoph Feichtenhofer, Axel Pinz, and Richard P Wildes. Spatiotemporal multiplier networks for video action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 7445\u20137454. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feichtenhofer%2C%20Christoph%20Pinz%2C%20Axel%20Wildes%2C%20Richard%20P.%20Spatiotemporal%20multiplier%20networks%20for%20video%20action%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feichtenhofer%2C%20Christoph%20Pinz%2C%20Axel%20Wildes%2C%20Richard%20P.%20Spatiotemporal%20multiplier%20networks%20for%20video%20action%20recognition%202017"
        },
        {
            "id": "7",
            "entry": "[7] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion for video action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feichtenhofer%2C%20Christoph%20Pinz%2C%20Axel%20Zisserman%2C%20Andrew%20Convolutional%20two-stream%20network%20fusion%20for%20video%20action%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feichtenhofer%2C%20Christoph%20Pinz%2C%20Axel%20Zisserman%2C%20Andrew%20Convolutional%20two-stream%20network%20fusion%20for%20video%20action%20recognition%202016"
        },
        {
            "id": "8",
            "entry": "[8] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\u201d something something\u201d video database for learning and evaluating visual common sense. In The IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Raghav%20Kahou%2C%20Samira%20Ebrahimi%20Michalski%2C%20Vincent%20Materzynska%2C%20Joanna%20The%E2%80%9D%20something%20something%E2%80%9D%20video%20database%20for%20learning%20and%20evaluating%20visual%20common%20sense%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Raghav%20Kahou%2C%20Samira%20Ebrahimi%20Michalski%2C%20Vincent%20Materzynska%2C%20Joanna%20The%E2%80%9D%20something%20something%E2%80%9D%20video%20database%20for%20learning%20and%20evaluating%20visual%20common%20sense%202017"
        },
        {
            "id": "9",
            "entry": "[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "10",
            "entry": "[10] Berthold KP Horn and Brian G Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185\u2013203, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horn%2C%20Berthold%20K.P.%20Schunck%2C%20Brian%20G.%20Determining%20optical%20flow%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Horn%2C%20Berthold%20K.P.%20Schunck%2C%20Brian%20G.%20Determining%20optical%20flow%201981"
        },
        {
            "id": "11",
            "entry": "[11] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ilg%2C%20Eddy%20Mayer%2C%20Nikolaus%20Saikia%2C%20Tonmoy%20Keuper%2C%20Margret%20Flownet%202.0%3A%20Evolution%20of%20optical%20flow%20estimation%20with%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ilg%2C%20Eddy%20Mayer%2C%20Nikolaus%20Saikia%2C%20Tonmoy%20Keuper%2C%20Margret%20Flownet%202.0%3A%20Evolution%20of%20optical%20flow%20estimation%20with%20deep%20networks%202017"
        },
        {
            "id": "12",
            "entry": "[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning(ICML), pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "13",
            "entry": "[13] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in Neural Information Processing Systems (NIPS), pages 2017\u20132025, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Spatial%20transformer%20networks%202017"
        },
        {
            "id": "14",
            "entry": "[14] Herve Jegou, Florent Perronnin, Matthijs Douze, Jorge S\u00e1nchez, Patrick Perez, and Cordelia Schmid. Aggregating local image descriptors into compact codes. IEEE transactions on pattern analysis and machine intelligence, 34(9):1704\u20131716, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aggregating%20local%20image%20descriptors%20into%20compact%20codes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aggregating%20local%20image%20descriptors%20into%20compact%20codes%202012"
        },
        {
            "id": "15",
            "entry": "[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221\u2013231, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ji%2C%20Shuiwang%20Xu%2C%20Wei%20Yang%2C%20Ming%20Yu%2C%20Kai%203d%20convolutional%20neural%20networks%20for%20human%20action%20recognition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ji%2C%20Shuiwang%20Xu%2C%20Wei%20Yang%2C%20Ming%20Yu%2C%20Kai%203d%20convolutional%20neural%20networks%20for%20human%20action%20recognition%202013"
        },
        {
            "id": "16",
            "entry": "[16] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In Advances in Neural Information Processing Systems (NIPS), pages 667\u2013675, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20Xu%20Brabandere%2C%20Bert%20De%20Tuytelaars%2C%20Tinne%20Gool%2C%20Luc%20V.%20Dynamic%20filter%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20Xu%20Brabandere%2C%20Bert%20De%20Tuytelaars%2C%20Tinne%20Gool%2C%20Luc%20V.%20Dynamic%20filter%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] Gunnar Johansson. Visual perception of biological motion and a model for its analysis. Perception & psychophysics, 14(2):201\u2013211, 1973.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johansson%2C%20Gunnar%20Visual%20perception%20of%20biological%20motion%20and%20a%20model%20for%20its%20analysis%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johansson%2C%20Gunnar%20Visual%20perception%20of%20biological%20motion%20and%20a%20model%20for%20its%20analysis%201973"
        },
        {
            "id": "18",
            "entry": "[18] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In The IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 1725\u20131732, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014"
        },
        {
            "id": "19",
            "entry": "[19] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06950"
        },
        {
            "id": "20",
            "entry": "[20] Ivan Laptev. On space-time interest points. International Journal of Computer Vision, 64(2-3):107\u2013123, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laptev%2C%20Ivan%20On%20space-time%20interest%20points%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laptev%2C%20Ivan%20On%20space-time%20interest%20points%202005"
        },
        {
            "id": "21",
            "entry": "[21] Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Benjamin Rozenfeld. Learning realistic human actions from movies. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1\u20138. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laptev%2C%20Ivan%20Marszalek%2C%20Marcin%20Schmid%2C%20Cordelia%20Rozenfeld%2C%20Benjamin%20Learning%20realistic%20human%20actions%20from%20movies%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laptev%2C%20Ivan%20Marszalek%2C%20Marcin%20Schmid%2C%20Cordelia%20Rozenfeld%2C%20Benjamin%20Learning%20realistic%20human%20actions%20from%20movies%202008"
        },
        {
            "id": "22",
            "entry": "[22] Bruce D Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In International Joint Conference on Artificial intelligence (IJCAI), pages 674\u2013679. Morgan Kaufmann Publishers Inc., 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucas%2C%20Bruce%20D.%20Kanade%2C%20Takeo%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucas%2C%20Bruce%20D.%20Kanade%2C%20Takeo%20An%20iterative%20image%20registration%20technique%20with%20an%20application%20to%20stereo%20vision%201981"
        },
        {
            "id": "23",
            "entry": "[23] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4040\u20134048, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mayer%2C%20Nikolaus%20Ilg%2C%20Eddy%20Hausser%2C%20Philip%20Fischer%2C%20Philipp%20A%20large%20dataset%20to%20train%20convolutional%20networks%20for%20disparity%2C%20optical%20flow%2C%20and%20scene%20flow%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mayer%2C%20Nikolaus%20Ilg%2C%20Eddy%20Hausser%2C%20Philip%20Fischer%2C%20Philipp%20A%20large%20dataset%20to%20train%20convolutional%20networks%20for%20disparity%2C%20optical%20flow%2C%20and%20scene%20flow%20estimation%202016"
        },
        {
            "id": "24",
            "entry": "[24] Ross Messing, Chris Pal, and Henry Kautz. Activity recognition using the velocity histories of tracked keypoints. In The IEEE International Conference on Computer Vision (ICCV), pages 104\u2013111. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Messing%2C%20Ross%20Pal%2C%20Chris%20Kautz%2C%20Henry%20Activity%20recognition%20using%20the%20velocity%20histories%20of%20tracked%20keypoints%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Messing%2C%20Ross%20Pal%2C%20Chris%20Kautz%2C%20Henry%20Activity%20recognition%20using%20the%20velocity%20histories%20of%20tracked%20keypoints%202009"
        },
        {
            "id": "25",
            "entry": "[25] Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. arXiv preprint arXiv:1801.03150, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03150"
        },
        {
            "id": "26",
            "entry": "[26] Anurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranjan%2C%20Anurag%20Black%2C%20Michael%20J.%20Optical%20flow%20estimation%20using%20a%20spatial%20pyramid%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranjan%2C%20Anurag%20Black%2C%20Michael%20J.%20Optical%20flow%20estimation%20using%20a%20spatial%20pyramid%20network%202017"
        },
        {
            "id": "27",
            "entry": "[27] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "28",
            "entry": "[28] Laura Sevilla-Lara, Yiyi Liao, Fatma Guney, Varun Jampani, Andreas Geiger, and Michael J Black. On the integration of optical flow and action recognition. arXiv preprint arXiv:1712.08416, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08416"
        },
        {
            "id": "29",
            "entry": "[29] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In Advances in Neural Information Processing Systems (NIPS), pages 568\u2013576, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014"
        },
        {
            "id": "30",
            "entry": "[30] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.0402"
        },
        {
            "id": "31",
            "entry": "[31] Deqing Sun, Stefan Roth, and Michael J Black. Secrets of optical flow estimation and their principles. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2432\u20132439. IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Deqing%20Roth%2C%20Stefan%20Black%2C%20Michael%20J.%20Secrets%20of%20optical%20flow%20estimation%20and%20their%20principles%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Deqing%20Roth%2C%20Stefan%20Black%2C%20Michael%20J.%20Secrets%20of%20optical%20flow%20estimation%20and%20their%20principles%202010"
        },
        {
            "id": "32",
            "entry": "[32] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Deqing%20Yang%2C%20Xiaodong%20Liu%2C%20Ming-Yu%20Kautz%2C%20Jan%20Pwc-net%3A%20Cnns%20for%20optical%20flow%20using%20pyramid%2C%20warping%2C%20and%20cost%20volume%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Deqing%20Yang%2C%20Xiaodong%20Liu%2C%20Ming-Yu%20Kautz%2C%20Jan%20Pwc-net%3A%20Cnns%20for%20optical%20flow%20using%20pyramid%2C%20warping%2C%20and%20cost%20volume%202018"
        },
        {
            "id": "33",
            "entry": "[33] Ju Sun, Xiao Wu, Shuicheng Yan, Loong-Fah Cheong, Tat-Seng Chua, and Jintao Li. Hierarchical spatiotemporal context modeling for action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2004\u20132011. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ju%20Wu%2C%20Xiao%20Yan%2C%20Shuicheng%20Cheong%2C%20Loong-Fah%20Hierarchical%20spatiotemporal%20context%20modeling%20for%20action%20recognition%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ju%20Wu%2C%20Xiao%20Yan%2C%20Shuicheng%20Cheong%2C%20Loong-Fah%20Hierarchical%20spatiotemporal%20context%20modeling%20for%20action%20recognition%202004"
        },
        {
            "id": "34",
            "entry": "[34] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In The IEEE International Conference on Computer Vision (ICCV), pages 4489\u20134497. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015"
        },
        {
            "id": "35",
            "entry": "[35] Du Tran, Jamie Ray, Zheng Shou, Shih-Fu Chang, and Manohar Paluri. Convnet architecture search for spatiotemporal feature learning. arXiv preprint arXiv:1708.05038, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05038"
        },
        {
            "id": "36",
            "entry": "[36] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Wang%2C%20Heng%20Torresani%2C%20Lorenzo%20Ray%2C%20Jamie%20A%20closer%20look%20at%20spatiotemporal%20convolutions%20for%20action%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Du%20Wang%2C%20Heng%20Torresani%2C%20Lorenzo%20Ray%2C%20Jamie%20A%20closer%20look%20at%20spatiotemporal%20convolutions%20for%20action%20recognition%202018"
        },
        {
            "id": "37",
            "entry": "[37] Heng Wang, Alexander Kl\u00e4ser, Cordelia Schmid, and Cheng-Lin Liu. Action recognition by dense trajectories. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3169\u20133176. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Heng%20Kl%C3%A4ser%2C%20Alexander%20Schmid%2C%20Cordelia%20Liu%2C%20Cheng-Lin%20Action%20recognition%20by%20dense%20trajectories%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Heng%20Kl%C3%A4ser%2C%20Alexander%20Schmid%2C%20Cordelia%20Liu%2C%20Cheng-Lin%20Action%20recognition%20by%20dense%20trajectories%202011"
        },
        {
            "id": "38",
            "entry": "[38] Heng Wang and Cordelia Schmid. Action recognition with improved trajectories. In The IEEE International Conference on Computer Vision (ICCV), pages 3551\u20133558, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Heng%20Schmid%2C%20Cordelia%20Action%20recognition%20with%20improved%20trajectories%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Heng%20Schmid%2C%20Cordelia%20Action%20recognition%20with%20improved%20trajectories%202013"
        },
        {
            "id": "39",
            "entry": "[39] Heng Wang, Muhammad Muneeb Ullah, Alexander Klaser, Ivan Laptev, and Cordelia Schmid. Evaluation of local spatio-temporal features for action recognition. In British Machine Vision Conference (BMVC), pages 124\u20131. BMVA Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Heng%20Ullah%2C%20Muhammad%20Muneeb%20Klaser%2C%20Alexander%20Laptev%2C%20Ivan%20Evaluation%20of%20local%20spatio-temporal%20features%20for%20action%20recognition%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Heng%20Ullah%2C%20Muhammad%20Muneeb%20Klaser%2C%20Alexander%20Laptev%2C%20Ivan%20Evaluation%20of%20local%20spatio-temporal%20features%20for%20action%20recognition%202009"
        },
        {
            "id": "40",
            "entry": "[40] Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Limin%20Li%2C%20Wei%20Li%2C%20Wen%20Gool%2C%20Luc%20Van%20Appearance-and-relation%20networks%20for%20video%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Limin%20Li%2C%20Wei%20Li%2C%20Wen%20Gool%2C%20Luc%20Van%20Appearance-and-relation%20networks%20for%20video%20classification%202018"
        },
        {
            "id": "41",
            "entry": "[41] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recognition with trajectory-pooled deep-convolutional descriptors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4305\u2013 4314, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Limin%20Qiao%2C%20Yu%20Tang%2C%20Xiaoou%20Action%20recognition%20with%20trajectory-pooled%20deep-convolutional%20descriptors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Limin%20Qiao%2C%20Yu%20Tang%2C%20Xiaoou%20Action%20recognition%20with%20trajectory-pooled%20deep-convolutional%20descriptors%202015"
        },
        {
            "id": "42",
            "entry": "[42] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European Conference on Computer Vision (ECCV), pages 20\u201336.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Wang%2C%20Zhe%20Qiao%2C%20Yu%20Temporal%20segment%20networks%3A%20Towards%20good%20practices%20for%20deep%20action%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Limin%20Xiong%2C%20Yuanjun%20Wang%2C%20Zhe%20Qiao%2C%20Yu%20Temporal%20segment%20networks%3A%20Towards%20good%20practices%20for%20deep%20action%20recognition"
        },
        {
            "id": "43",
            "entry": "[43] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiaolong%20Girshick%2C%20Ross%20Gupta%2C%20Abhinav%20He%2C%20Kaiming%20Non-local%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xiaolong%20Girshick%2C%20Ross%20Gupta%2C%20Abhinav%20He%2C%20Kaiming%20Non-local%20neural%20networks%202018"
        },
        {
            "id": "44",
            "entry": "[44] Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In European Conference on Computer Vision (ECCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiaolong%20Gupta%2C%20Abhinav%20Videos%20as%20space-time%20region%20graphs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xiaolong%20Gupta%2C%20Abhinav%20Videos%20as%20space-time%20region%20graphs%202018"
        },
        {
            "id": "45",
            "entry": "[45] Xingxing Wang, Limin Wang, and Yu Qiao. A comparative study of encoding, pooling and normalization methods for action recognition. In Asian Conference on Computer Vision (ACCV), pages 572\u2013585.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Xingxing%20Wang%2C%20Limin%20Wang%2C%20and%20Yu%20Qiao.%20A%20comparative%20study%20of%20encoding%2C%20pooling%20and%20normalization%20methods%20for%20action%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Xingxing%20Wang%2C%20Limin%20Wang%2C%20and%20Yu%20Qiao.%20A%20comparative%20study%20of%20encoding%2C%20pooling%20and%20normalization%20methods%20for%20action%20recognition"
        },
        {
            "id": "46",
            "entry": "[46] Geert Willems, Tinne Tuytelaars, and Luc Van Gool. An efficient dense and scale-invariant spatio-temporal interest point detector. In European Conference on Computer Vision (ECCV), pages 650\u2013663.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Willems%2C%20Geert%20Tuytelaars%2C%20Tinne%20Gool%2C%20Luc%20Van%20An%20efficient%20dense%20and%20scale-invariant%20spatio-temporal%20interest%20point%20detector",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Willems%2C%20Geert%20Tuytelaars%2C%20Tinne%20Gool%2C%20Luc%20Van%20An%20efficient%20dense%20and%20scale-invariant%20spatio-temporal%20interest%20point%20detector"
        },
        {
            "id": "47",
            "entry": "[47] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: : Speed-accuracy trade-offs in video classification. In European Conference on Computer Vision (ECCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Sun%2C%20Chen%20Huang%2C%20Jonathan%20Tu%2C%20Zhuowen%20Rethinking%20spatiotemporal%20feature%20learning%3A%20%3A%20Speed-accuracy%20trade-offs%20in%20video%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Sun%2C%20Chen%20Huang%2C%20Jonathan%20Tu%2C%20Zhuowen%20Rethinking%20spatiotemporal%20feature%20learning%3A%20%3A%20Speed-accuracy%20trade-offs%20in%20video%20classification%202018"
        },
        {
            "id": "48",
            "entry": "[48] Christopher Zach, Thomas Pock, and Horst Bischof. A duality based approach for realtime tv-l 1 optical flow. In Joint Pattern Recognition Symposium, pages 214\u2013223.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zach%2C%20Christopher%20Pock%2C%20Thomas%20Bischof%2C%20Horst%20A%20duality%20based%20approach%20for%20realtime%20tv-l%201%20optical%20flow",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zach%2C%20Christopher%20Pock%2C%20Thomas%20Bischof%2C%20Horst%20A%20duality%20based%20approach%20for%20realtime%20tv-l%201%20optical%20flow"
        },
        {
            "id": "49",
            "entry": "[49] Yue Zhao, Yuanjun Xiong, and Dahua Lin. Recognize actions by disentangling components of dynamics. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6566\u20136575, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Yue%20Xiong%2C%20Yuanjun%20Lin%2C%20Dahua%20Recognize%20actions%20by%20disentangling%20components%20of%20dynamics%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Yue%20Xiong%2C%20Yuanjun%20Lin%2C%20Dahua%20Recognize%20actions%20by%20disentangling%20components%20of%20dynamics%202018"
        },
        {
            "id": "50",
            "entry": "[50] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos. In European Conference on Computer Vision (ECCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Andonian%2C%20Alex%20Oliva%2C%20Aude%20Torralba%2C%20Antonio%20Temporal%20relational%20reasoning%20in%20videos%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Andonian%2C%20Alex%20Oliva%2C%20Aude%20Torralba%2C%20Antonio%20Temporal%20relational%20reasoning%20in%20videos%202018"
        },
        {
            "id": "51",
            "entry": "[51] Yi Zhu, Zhenzhong Lan, Shawn Newsam, and Alexander G Hauptmann. Hidden two-stream convolutional networks for action recognition. arXiv preprint arXiv:1704.00389, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00389"
        },
        {
            "id": "52",
            "entry": "[52] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient convolutional network for online video understanding. In European Conference on Computer Vision (ECCV), 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zolfaghari%2C%20Mohammadreza%20Singh%2C%20Kamaljeet%20Brox%2C%20Thomas%20Eco%3A%20Efficient%20convolutional%20network%20for%20online%20video%20understanding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zolfaghari%2C%20Mohammadreza%20Singh%2C%20Kamaljeet%20Brox%2C%20Thomas%20Eco%3A%20Efficient%20convolutional%20network%20for%20online%20video%20understanding%202018"
        }
    ]
}
