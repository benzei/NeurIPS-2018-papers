{
    "filename": "8130-a-stein-variational-newton-method.pdf",
    "metadata": {
        "title": "A Stein variational Newton method",
        "author": "Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, Robert Scheichl",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8130-a-stein-variational-newton-method.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Stein variational gradient descent (SVGD) was recently proposed as a general purpose nonparametric variational inference algorithm [Liu & Wang, NIPS 2016]: it minimizes the Kullback\u2013Leibler divergence between the target distribution and its approximation by implementing a form of functional gradient descent on a reproducing kernel Hilbert space. In this paper, we accelerate and generalize the SVGD algorithm by including second-order information, thereby approximating a Newton-like iteration in function space. We also show how second-order information can lead to more effective choices of kernel. We observe significant computational gains over the original SVGD algorithm in multiple test cases."
    },
    "keywords": [
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "MCMC",
            "url": "https://en.wikipedia.org/wiki/MCMC"
        },
        {
            "term": "empirical measure",
            "url": "https://en.wikipedia.org/wiki/empirical_measure"
        },
        {
            "term": "newton method",
            "url": "https://en.wikipedia.org/wiki/newton_method"
        },
        {
            "term": "Markov chain Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        }
    ],
    "highlights": [
        "Suppose we wish to approximate an intractable target distribution with density \u03c0 on Rd via an empirical measure, i.e., a collection of samples",
        "We focus on Stein variational gradient descent (SVGD) [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which lies somewhere in the middle of the spectrum and can be described as a particular nonparametric variational inference method [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], with close links to the density estimation approach in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "In order to satisfy (5), the Stein variational gradient descent algorithm defines Tl+1 as a perturbation of the identity map along the steepest descent direction of the functional Jpl evaluated at the zero map, i.e., Tl+1 = I \u2212 \u03b5\u2207Jpl [0], (8)",
        "We propose a new method that incorporates second-order information to accelerate the convergence of the Stein variational gradient descent algorithm",
        "After 50 iterations, the algorithm has already converged, accurately reconstructing the posterior mean and the posterior credible intervals. (See Figure 3 and below for a validation of these results against a reference Markov chain Monte Carlo simulation.) Stein variational Newton method-I manages to provide a reasonable reconstruction of the target distribution: the posterior mean shows fair agreement with the true solution, but the credible intervals are slightly overestimated, compared to Stein variational Newton method-H and the reference Markov chain Monte Carlo",
        "While we observe in Section 4 that using a properly rescaled Gaussian kernel can improve the performance of the Stein variational Newton method method in high dimensions, we believe that a truly general purpose nonparametric algorithm using local kernels will inevitably face further challenges in high-dimensional settings"
    ],
    "key_statements": [
        "Suppose we wish to approximate an intractable target distribution with density \u03c0 on Rd via an empirical measure, i.e., a collection of samples",
        "We focus on Stein variational gradient descent (SVGD) [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which lies somewhere in the middle of the spectrum and can be described as a particular nonparametric variational inference method [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], with close links to the density estimation approach in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "In order to satisfy (5), the Stein variational gradient descent algorithm defines Tl+1 as a perturbation of the identity map along the steepest descent direction of the functional Jpl evaluated at the zero map, i.e., Tl+1 = I \u2212 \u03b5\u2207Jpl [0], (8)",
        "We propose a new method that incorporates second-order information to accelerate the convergence of the Stein variational gradient descent algorithm",
        "We provide a performance comparison between Stein variational gradient descent and Stein variational Newton method on a high-dimensional Bayesian neural network",
        "After 50 iterations, the algorithm has already converged, accurately reconstructing the posterior mean and the posterior credible intervals. (See Figure 3 and below for a validation of these results against a reference Markov chain Monte Carlo simulation.) Stein variational Newton method-I manages to provide a reasonable reconstruction of the target distribution: the posterior mean shows fair agreement with the true solution, but the credible intervals are slightly overestimated, compared to Stein variational Newton method-H and the reference Markov chain Monte Carlo",
        "While we observe in Section 4 that using a properly rescaled Gaussian kernel can improve the performance of the Stein variational Newton method method in high dimensions, we believe that a truly general purpose nonparametric algorithm using local kernels will inevitably face further challenges in high-dimensional settings"
    ],
    "summary": [
        "Suppose we wish to approximate an intractable target distribution with density \u03c0 on Rd via an empirical measure, i.e., a collection of samples.",
        "We design an algorithm where each map Tk is computed as the perturbation of the identity function along the direction that minimizes a certain local quadratic approximation of J.",
        "A second contribution of this paper is to design geometry-aware Gaussian kernels that exploit second-order information, yielding substantially faster convergence towards the target distribution than SVGD or SVN with an isotropic kernel.",
        "We focus on nonparametric variational approximations, where the corresponding optimisation problem is defined over an infinite-dimensional RKHS of transport maps.",
        "Where \u00b7, \u00b7 Hd denotes an inner product on Hd. In order to satisfy (5), the SVGD algorithm defines Tl+1 as a perturbation of the identity map along the steepest descent direction of the functional Jpl evaluated at the zero map, i.e., Tl+1 = I \u2212 \u03b5\u2207Jpl [0], (8)",
        "In the Stein variational method, the kernel weighs the contribution of each particle to a locally averaged steepest descent direction of the target distribution, and it spreads the particles along the support of the target distribution.",
        "For high-dimensional target distributions that do not have a well-defined limit with increasing dimension, an appropriately chosen scaling function g(d) can still improve the ability of the kernel to discriminate inter-particle distances.",
        "We evaluate our new SVN method with the scaled Hessian kernel on a set of test cases drawn from various Bayesian inference tasks.",
        "The iteration numbers are scaled, so that we can compare outputs generated by various algorithms using approximately the same amount of CPU time.",
        "(See Figure 3 and below for a validation of these results against a reference MCMC simulation.) SVN-I manages to provide a reasonable reconstruction of the target distribution: the posterior mean shows fair agreement with the true solution, but the credible intervals are slightly overestimated, compared to SVN-H and the reference MCMC.",
        "While we observe in Section 4 that using a properly rescaled Gaussian kernel can improve the performance of the SVN method in high dimensions, we believe that a truly general purpose nonparametric algorithm using local kernels will inevitably face further challenges in high-dimensional settings.",
        "A sensible approach to coping with high dimensionality is to design algorithms that can detect and exploit essential structure in the target distribution, whether it be decaying correlation, conditional independence, low rank, multiple scales, and so on."
    ],
    "headline": "We show how second-order information can lead to more effective choices of kernel",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] http://github.com/gianlucadetommaso/Stein-variational-samplers",
            "url": "http://github.com/gianlucadetommaso/Stein-variational-samplers"
        },
        {
            "id": "2",
            "entry": "[2] E. Anderes, M. Coram. A general spline representation for nonparametric and semiparametric density estimates using diffeomorphisms. arXiv preprint arXiv:1205.5314, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1205.5314"
        },
        {
            "id": "3",
            "entry": "[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, p. 337\u2013404, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20N.%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%20society%201950"
        },
        {
            "id": "4",
            "entry": "[4] D. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, p. 859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20D.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20D.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "5",
            "entry": "[5] W. Y. Chen, L. Mackey, J. Gorham, F. X. Briol, C. J. Oates. Stein points. In International Conference on Machine Learning. arXiv:1803.10161, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.10161"
        },
        {
            "id": "6",
            "entry": "[6] T. Cui, K. J. H. Law, Y. M. Marzouk. Dimension-independent likelihood-informed MCMC. Journal of Computational Physics, 304: 109\u2013137, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cui%2C%20T.%20Law%2C%20K.J.H.%20Marzouk%2C%20Y.M.%20Dimension-independent%20likelihood-informed%20MCMC%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cui%2C%20T.%20Law%2C%20K.J.H.%20Marzouk%2C%20Y.M.%20Dimension-independent%20likelihood-informed%20MCMC%202016"
        },
        {
            "id": "7",
            "entry": "[7] T. Cui, J. Martin, Y. M. Marzouk, A. Solonen, and A. Spantini. Likelihood-informed dimension reduction for nonlinear inverse problems. Inverse Problems, 30(11):114015, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cui%2C%20T.%20Martin%2C%20J.%20Marzouk%2C%20Y.M.%20Solonen%2C%20A.%20Likelihood-informed%20dimension%20reduction%20for%20nonlinear%20inverse%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cui%2C%20T.%20Martin%2C%20J.%20Marzouk%2C%20Y.M.%20Solonen%2C%20A.%20Likelihood-informed%20dimension%20reduction%20for%20nonlinear%20inverse%20problems%202014"
        },
        {
            "id": "8",
            "entry": "[8] D. Francois, V. Wertz, and M. Verleysen. About the locality of kernels in high-dimensional spaces. International Symposium on Applied Stochastic Models and Data Analysis, p. 238\u2013245, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Francois%2C%20D.%20Wertz%2C%20V.%20Verleysen%2C%20M.%20About%20the%20locality%20of%20kernels%20in%20high-dimensional%20spaces%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Francois%2C%20D.%20Wertz%2C%20V.%20Verleysen%2C%20M.%20About%20the%20locality%20of%20kernels%20in%20high-dimensional%20spaces%202005"
        },
        {
            "id": "9",
            "entry": "[9] S. Gershman, M. Hoffman, D. Blei. Nonparametric variational inference. arXiv preprint arXiv:1206.4665, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.4665"
        },
        {
            "id": "10",
            "entry": "[10] W. R. Gilks, S. Richardson, and D. Spiegelhalter. Markov chain Monte Carlo in practice. CRC press, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilks%2C%20W.R.%20Richardson%2C%20S.%20Spiegelhalter%2C%20D.%20Markov%20chain%20Monte%20Carlo%20in%20practice%201995"
        },
        {
            "id": "11",
            "entry": "[11] M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123\u2013 214, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girolami%2C%20M.%20Calderhead%2C%20B.%20Riemann%20manifold%20Langevin%20and%20Hamiltonian%20Monte%20Carlo%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girolami%2C%20M.%20Calderhead%2C%20B.%20Riemann%20manifold%20Langevin%20and%20Hamiltonian%20Monte%20Carlo%20methods%202011"
        },
        {
            "id": "12",
            "entry": "[12] J. Han and Q. Liu. Stein variational adaptive importance sampling. arXiv preprint arXiv:1704.05201, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05201"
        },
        {
            "id": "13",
            "entry": "[13] M. E. Khan, Z. Liu, V. Tangkaratt, Y. Gal. Vprop: Variational inference using RMSprop. arXiv preprint arXiv:1712.01038, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01038"
        },
        {
            "id": "14",
            "entry": "[14] M. E. Khan, W. Lin, V. Tangkaratt, Z. Liu, D. Nielsen. Adaptive-Newton method for explorative learning. arXiv preprint arXiv:1711.05560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05560"
        },
        {
            "id": "15",
            "entry": "[15] Q. Liu. Stein variational gradient descent as gradient flow. In Advances in Neural Information Processing systems (I. Guyon et al., Eds.), Vol. 30, p. 3118\u20133126, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Q.%20Stein%20variational%20gradient%20descent%20as%20gradient%20flow%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Q.%20Stein%20variational%20gradient%20descent%20as%20gradient%20flow%202017"
        },
        {
            "id": "16",
            "entry": "[16] Y. Liu, P. Ramachandran, Q. Liu, and J. Peng. Stein variational policy gradient. arXiv preprint arXiv:1704.02399, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02399"
        },
        {
            "id": "17",
            "entry": "[17] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In Advances In Neural Information Processing Systems (D. D. Lee et al., Eds.), Vol. 29, p. 2378\u20132386, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Q.%20Wang%2C%20D.%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20Bayesian%20inference%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Q.%20Wang%2C%20D.%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20Bayesian%20inference%20algorithm%202016"
        },
        {
            "id": "18",
            "entry": "[18] C. Liu and J. Zhu. Riemannian Stein variational gradient descent for Bayesian inference. arXiv preprint arXiv:1711.11216, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11216"
        },
        {
            "id": "19",
            "entry": "[19] D. G. Luenberger. Optimization by vector space methods. John Wiley & Sons, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luenberger%2C%20D.G.%20Optimization%20by%20vector%20space%20methods%201997"
        },
        {
            "id": "20",
            "entry": "[20] J. Martin, L. C. Wilcox, C. Burstedde, and O. Ghattas. A stochastic Newton MCMC method for large-scale statistical inverse problems with application to seismic inversion. SIAM Journal on Scientific Computing, 34(3), A1460\u2013A1487, Chapman & Hall/CRC, 2012",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%2C%20J.%20Wilcox%2C%20L.C.%20Burstedde%2C%20C.%20Ghattas%2C%20O.%20A%20stochastic%20Newton%20MCMC%20method%20for%20large-scale%20statistical%20inverse%20problems%20with%20application%20to%20seismic%20inversion%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%2C%20J.%20Wilcox%2C%20L.C.%20Burstedde%2C%20C.%20Ghattas%2C%20O.%20A%20stochastic%20Newton%20MCMC%20method%20for%20large-scale%20statistical%20inverse%20problems%20with%20application%20to%20seismic%20inversion%202012"
        },
        {
            "id": "21",
            "entry": "[21] Y. M. Marzouk, T. Moselhy, M. Parno, and A. Spantini. Sampling via measure transport: An introduction. Handbook of Uncertainty Quantification, Springer, p. 1\u201341, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marzouk%2C%20Y.M.%20Moselhy%2C%20T.%20Parno%2C%20M.%20Spantini%2C%20A.%20Sampling%20via%20measure%20transport%3A%20An%20introduction.%20Handbook%20of%20Uncertainty%20Quantification%202016"
        },
        {
            "id": "22",
            "entry": "[22] R. M. Neal. MCMC using Hamiltonian dynamics. In Handbook of Markov Chain Monte Carlo (S. Brooks et al., Eds.), Chapman & Hall/CRC, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20R.M.%20MCMC%20using%20Hamiltonian%20dynamics.%20In%20Handbook%20of%20Markov%20Chain%20Monte%20Carlo%202011"
        },
        {
            "id": "23",
            "entry": "[23] Y. Pu, Z. Gan, R. Henao, C. Li, S. Han, and L. Carin. Stein variational autoencoder. arXiv preprint arXiv:1704.05155, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05155"
        },
        {
            "id": "24",
            "entry": "[24] D. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv:1505.05770, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05770"
        },
        {
            "id": "25",
            "entry": "[25] A. Spantini, D. Bigoni, and Y. Marzouk. Inference via low-dimensional couplings. Journal of Machine Learning Research, to appear. arXiv:1703.06131, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06131"
        },
        {
            "id": "26",
            "entry": "[26] A. M. Stuart. Inverse problems: a Bayesian perspective. Acta Numerica, 19, p. 451\u2013559, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stuart%2C%20A.M.%20Inverse%20problems%3A%20a%20Bayesian%20perspective%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stuart%2C%20A.M.%20Inverse%20problems%3A%20a%20Bayesian%20perspective%202010"
        },
        {
            "id": "27",
            "entry": "[27] E. G. Tabak and T. V. Turner. A family of nonparametric density estimation algorithms. Communications on Pure and Applied Mathematics, p. 145\u2013164, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tabak%2C%20E.G.%20Turner%2C%20T.V.%20A%20family%20of%20nonparametric%20density%20estimation%20algorithms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tabak%2C%20E.G.%20Turner%2C%20T.V.%20A%20family%20of%20nonparametric%20density%20estimation%20algorithms%202013"
        },
        {
            "id": "28",
            "entry": "[28] C. Villani. Optimal Transport: Old and New. Springer-Verlag Berlin Heidelberg, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C.%20Optimal%20Transport%3A%20Old%20and%20New%202009"
        },
        {
            "id": "29",
            "entry": "[29] D. Wang, Z. Zeng, and Q. Liu. Structured Stein variational inference for continuous graphical models. arXiv:1711.07168, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07168"
        },
        {
            "id": "30",
            "entry": "[30] J. Zhuo, C. Liu, N. Chen, and B. Zhang. Analyzing and improving Stein variational gradient descent for high-dimensional marginal inference. arXiv preprint arXiv:1711.04425, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04425"
        },
        {
            "id": "31",
            "entry": "[31] S. Wright, J. Nocedal. Numerical Optimization. Springer Science, 1999. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Wright%20J%20Nocedal%20Numerical%20Optimization%20Springer%20Science%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%20Wright%20J%20Nocedal%20Numerical%20Optimization%20Springer%20Science%201999"
        }
    ]
}
