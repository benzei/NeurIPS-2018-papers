{
    "filename": "8158-pac-bayes-tree-weighted-subtrees-with-guarantees.pdf",
    "metadata": {
        "title": "PAC-Bayes Tree: weighted subtrees with guarantees",
        "author": "Tin D. Nguyen, Samory Kpotufe",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8158-pac-bayes-tree-weighted-subtrees-with-guarantees.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a weighted-majority classification approach over subtrees of a fixed tree, which provably achieves excess-risk of the same order as the best tree-pruning. Furthermore, the computational efficiency of pruning is maintained at both training and testing time despite having to aggregate over an exponential number of subtrees. We believe this is the first subtree aggregation approach with such guarantees."
    },
    "keywords": [
        {
            "term": "excess risk",
            "url": "https://en.wikipedia.org/wiki/excess_risk"
        },
        {
            "term": "decision tree",
            "url": "https://en.wikipedia.org/wiki/decision_tree"
        },
        {
            "term": "majority vote",
            "url": "https://en.wikipedia.org/wiki/majority_vote"
        },
        {
            "term": "intrinsic dimension",
            "url": "https://en.wikipedia.org/wiki/intrinsic_dimension"
        }
    ],
    "highlights": [
        "Classification trees endure as popular tools in data analysis, offering both efficient prediction and interpretability \u2013 yet they remain hard to analyze in general",
        "There are two main approaches with generalization guarantees: in both approaches, a large tree is first obtained; one approach is to prune back this tree down to a subtree2 that generalizes better; the alternative approach is to combine all possible subtrees of the tree by weighted majority vote",
        "While both approaches are competitive with other practical heuristics, it remains unclear whether the alternative of weighting subtrees enjoys the same strong generalization guarantees as pruning; in particular, no weighting scheme to date has been shown to be statistically consistent, let alone attain the same tight generalization rates as pruning approaches",
        "We consider a new weighting scheme based on PAC-Bayesian insights [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], that (a) is consistent and attains the same generalization rates as the best pruning of a tree, (b) is efficiently computable at both training and testing time, and (c) competes against pruning approaches on real-world data",
        "The best known result to date [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] presents a weighting scheme which can provably achieve an excess risk3 of the form oP (1) + C \u00b7 minT R, where R denotes the misclassification rate of a classifier hT based on subtree T"
    ],
    "key_statements": [
        "Classification trees endure as popular tools in data analysis, offering both efficient prediction and interpretability \u2013 yet they remain hard to analyze in general",
        "There are two main approaches with generalization guarantees: in both approaches, a large tree is first obtained; one approach is to prune back this tree down to a subtree2 that generalizes better; the alternative approach is to combine all possible subtrees of the tree by weighted majority vote",
        "While both approaches are competitive with other practical heuristics, it remains unclear whether the alternative of weighting subtrees enjoys the same strong generalization guarantees as pruning; in particular, no weighting scheme to date has been shown to be statistically consistent, let alone attain the same tight generalization rates as pruning approaches",
        "We consider a new weighting scheme based on PAC-Bayesian insights [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], that (a) is consistent and attains the same generalization rates as the best pruning of a tree, (b) is efficiently computable at both training and testing time, and (c) competes against pruning approaches on real-world data",
        "The best known result to date [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] presents a weighting scheme which can provably achieve an excess risk3 of the form oP (1) + C \u00b7 minT R, where R denotes the misclassification rate of a classifier hT based on subtree T"
    ],
    "summary": [
        "Classification trees endure as popular tools in data analysis, offering both efficient prediction and interpretability \u2013 yet they remain hard to analyze in general.",
        "We consider a new weighting scheme based on PAC-Bayesian insights [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], that (a) is consistent and attains the same generalization rates as the best pruning of a tree, (b) is efficiently computable at both training and testing time, and (c) competes against pruning approaches on real-world data.",
        "The best known result to date [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] presents a weighting scheme which can provably achieve an excess risk3 of the form oP (1) + C \u00b7 minT R, where R denotes the misclassification rate of a classifier hT based on subtree T .",
        "This first result is of general interest since it extends beyond subtrees to any family of classifiers, and is obtained by carefully combining existing arguments from PAC-Bayes analysis.",
        "Our basic PAC-Bayes result alone does not ensure convergence at the same rate as that of the best pruning approaches.",
        "In particular we show that our weighted-voting scheme achieves similar or better error than pruning on practical problems, as suggested by our theoretical results.",
        "Our oracle rates of Theorem 1 requires no additional assumptions; the resulting corollary is stated under standard distributional conditions that characterize convergence rates for tree-prunings.",
        "Our main theorem below follows from Proposition 2 on excess risk, by showing (a) that the above definition of \u2206n and \u2206n satisfies the conditions of Proposition 2, and (b) that there exists a proper prior P such that log(1/P (T )) \u223c |\u03c0(T )|, i.e., depends just on the subtree complexity rather than on that of T0.",
        "(1/CP )e\u22123D0\u00b7|\u03c0(T )| for a normalizing constant CP , and consider the corresponding posterior Q\u2217\u03bb as defined in Equation 2, such that, with probability at least 1 \u2212 4\u03b4 over S, for all \u03bb \u2208 (0, 2), the excess risk E of the majority-classifier is at most",
        "From Theorem 1 we can deduce that the majority classifier hQ\u03bb\u2217 is consistent whenever the approach of pruning to the best subtree is consistent + (D0 |\u03c0(T )|)/n = oP (1)).",
        "We can infer that E converges at the same rate as pruning approaches: the terms \u2206n and D0 \u00b7 |\u03c0(T )|/n can be shown to be typically, of lower or similar order as E for the best subtree classifier hT .",
        "That the above alternative theoretical approaches, SN-pruning and HS-vote, are assumed to work on a sample independent choice of T0, but are implemented here on the entire data to take advantage of larger data sizes."
    ],
    "headline": "We present a weighted-majority classification approach over subtrees of a fixed tree, which provably achieves excess-risk of the same order as the best tree-pruning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] David A McAllester. Some PAC-Bayesian theorems. Machine Learning, 37(3):355\u2013363, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20A.%20Some%20PAC-Bayesian%20theorems%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20David%20A.%20Some%20PAC-Bayesian%20theorems%201999"
        },
        {
            "id": "2",
            "entry": "[2] L\u00e1szl\u00f3 A Sz\u00e9kely and Hua Wang. On subtrees of trees. Advances in Applied Mathematics, 34(1):138\u2013155, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sz%C3%A9kely%2C%20L%C3%A1szl%C3%B3%20A.%20Wang%2C%20Hua%20On%20subtrees%20of%20trees%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sz%C3%A9kely%2C%20L%C3%A1szl%C3%B3%20A.%20Wang%2C%20Hua%20On%20subtrees%20of%20trees%202005"
        },
        {
            "id": "3",
            "entry": "[3] Trevor Hastie and Daryl Pregibon. Shrinking trees. AT & T Bell Laboratories, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20Trevor%20Pregibon%2C%20Daryl%20Shrinking%20trees%201990"
        },
        {
            "id": "4",
            "entry": "[4] Wray Buntine and Tim Niblett. A further comparison of splitting rules for decision-tree induction. Machine Learning, 8(1):75\u201385, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buntine%2C%20Wray%20Niblett%2C%20Tim%20A%20further%20comparison%20of%20splitting%20rules%20for%20decision-tree%20induction%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buntine%2C%20Wray%20Niblett%2C%20Tim%20A%20further%20comparison%20of%20splitting%20rules%20for%20decision-tree%20induction%201992"
        },
        {
            "id": "5",
            "entry": "[5] David P Helmbold and Robert E Schapire. Predicting nearly as well as the best pruning of a decision tree. Machine Learning, 27(1):51\u201368, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Helmbold%2C%20David%20P.%20Schapire%2C%20Robert%20E.%20Predicting%20nearly%20as%20well%20as%20the%20best%20pruning%20of%20a%20decision%20tree%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Helmbold%2C%20David%20P.%20Schapire%2C%20Robert%20E.%20Predicting%20nearly%20as%20well%20as%20the%20best%20pruning%20of%20a%20decision%20tree%201997"
        },
        {
            "id": "6",
            "entry": "[6] Alexandre Lacasse, Fran\u00e7ois Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier. PAC- Bayes bounds for the risk of the majority vote and the variance of the Gibbs classifier. In Advances in Neural information processing systems, pages 769\u2013776, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacasse%2C%20Alexandre%20Laviolette%2C%20Fran%C3%A7ois%20Marchand%2C%20Mario%20Germain%2C%20Pascal%20PAC-%20Bayes%20bounds%20for%20the%20risk%20of%20the%20majority%20vote%20and%20the%20variance%20of%20the%20Gibbs%20classifier%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lacasse%2C%20Alexandre%20Laviolette%2C%20Fran%C3%A7ois%20Marchand%2C%20Mario%20Germain%2C%20Pascal%20PAC-%20Bayes%20bounds%20for%20the%20risk%20of%20the%20majority%20vote%20and%20the%20variance%20of%20the%20Gibbs%20classifier%202007"
        },
        {
            "id": "7",
            "entry": "[7] John Langford and John Shawe-Taylor. PAC-Bayes & margins. In Advances in neural information processing systems, pages 439\u2013446, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Shawe-Taylor%2C%20John%20PAC-Bayes%20%26%20margins%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Shawe-Taylor%2C%20John%20PAC-Bayes%20%26%20margins%202003"
        },
        {
            "id": "8",
            "entry": "[8] Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Jean-Francis Roy. Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm. Journal of Machine Learning Research, 16:787\u2013860, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Pascal%20Lacasse%2C%20Alexandre%20Laviolette%2C%20Francois%20Marchand%2C%20Mario%20Risk%20Bounds%20for%20the%20Majority%20Vote%3A%20From%20a%20PAC-Bayesian%20Analysis%20to%20a%20Learning%20Algorithm%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Pascal%20Lacasse%2C%20Alexandre%20Laviolette%2C%20Francois%20Marchand%2C%20Mario%20Risk%20Bounds%20for%20the%20Majority%20Vote%3A%20From%20a%20PAC-Bayesian%20Analysis%20to%20a%20Learning%20Algorithm%202015"
        },
        {
            "id": "9",
            "entry": "[9] C. Scott and R.D. Nowak. Minimax-optimal classification with dyadic decision trees. IEEE Transactions on Information Theory, 52, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20C.%20Nowak%2C%20R.D.%20Minimax-optimal%20classification%20with%20dyadic%20decision%20trees%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scott%2C%20C.%20Nowak%2C%20R.D.%20Minimax-optimal%20classification%20with%20dyadic%20decision%20trees%202006"
        },
        {
            "id": "10",
            "entry": "[10] Niklas Thiemann, Christian Igel, Olivier Wintenberger, and Yevgeny Seldin. A Strongly Quasiconvex PAC-Bayesian bound. In Steve Hanneke and Lev Reyzin, editors, Proceedings of the 28th International Conference on Algorithmic Learning Theory, volume 76 of Proceedings of Machine Learning Research, pages 466\u2013492, Kyoto University, Kyoto, Japan, 15\u201317 Oct 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thiemann%2C%20Niklas%20Igel%2C%20Christian%20Wintenberger%2C%20Olivier%20Seldin%2C%20Yevgeny%20A%20Strongly%20Quasiconvex%20PAC-Bayesian%20bound%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thiemann%2C%20Niklas%20Igel%2C%20Christian%20Wintenberger%2C%20Olivier%20Seldin%2C%20Yevgeny%20A%20Strongly%20Quasiconvex%20PAC-Bayesian%20bound%202017-10"
        },
        {
            "id": "11",
            "entry": "[11] L. Gyorfi, M. Kohler, A. Krzyzak, and H. Walk. A Distribution Free Theory of Nonparametric Regression. Springer, New York, NY, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gyorfi%2C%20L.%20Kohler%2C%20M.%20Krzyzak%2C%20A.%20Walk%2C%20H.%20A%20Distribution%20Free%20Theory%20of%20Nonparametric%20Regression%202002"
        },
        {
            "id": "12",
            "entry": "[12] Nakul Verma, Samory Kpotufe, and Sanjoy Dasgupta. Which spatial partition trees are adaptive to intrinsic dimension? In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 565\u2013574. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Verma%2C%20Nakul%20Kpotufe%2C%20Samory%20Dasgupta%2C%20Sanjoy%20Which%20spatial%20partition%20trees%20are%20adaptive%20to%20intrinsic%20dimension%3F%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Verma%2C%20Nakul%20Kpotufe%2C%20Samory%20Dasgupta%2C%20Sanjoy%20Which%20spatial%20partition%20trees%20are%20adaptive%20to%20intrinsic%20dimension%3F%202009"
        },
        {
            "id": "13",
            "entry": "[13] Samory Kpotufe and Sanjoy Dasgupta. A tree-based regressor that adapts to intrinsic dimension. Journal of Computer and System Sciences, 78(5):1496\u20131515, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kpotufe%2C%20Samory%20Dasgupta%2C%20Sanjoy%20A%20tree-based%20regressor%20that%20adapts%20to%20intrinsic%20dimension%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kpotufe%2C%20Samory%20Dasgupta%2C%20Sanjoy%20A%20tree-based%20regressor%20that%20adapts%20to%20intrinsic%20dimension%202012"
        },
        {
            "id": "14",
            "entry": "[14] Santosh Vempala. Randomly-oriented kd trees adapt to intrinsic dimension. In FSTTCS, volume 18, pages 48\u201357.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vempala%2C%20Santosh%20Randomly-oriented%20kd%20trees%20adapt%20to%20intrinsic%20dimension",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vempala%2C%20Santosh%20Randomly-oriented%20kd%20trees%20adapt%20to%20intrinsic%20dimension"
        },
        {
            "id": "15",
            "entry": "[15] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The Annals of Statistics, 35(2):608\u2013633, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20Jean-Yves%20Tsybakov%2C%20Alexandre%20B.%20Fast%20learning%20rates%20for%20plug-in%20classifiers%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20Jean-Yves%20Tsybakov%2C%20Alexandre%20B.%20Fast%20learning%20rates%20for%20plug-in%20classifiers%202007"
        },
        {
            "id": "16",
            "entry": "[16] Clayton Scott. Dyadic Decision Trees. PhD thesis, Rice University, 2004. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20Clayton%20Dyadic%20Decision%20Trees%202004"
        }
    ]
}
