{
    "filename": "8069-graph-oracle-models-lower-bounds-and-gaps-for-parallel-stochastic-optimization.pdf",
    "metadata": {
        "title": "Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization",
        "author": "Blake E. Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, Nati Srebro",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8069-graph-oracle-models-lower-bounds-and-gaps-for-parallel-stochastic-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We suggest a general oracle-based framework that captures different parallel stochastic optimization settings described by a dependency graph, and derive generic lower bounds in terms of this graph. We then use the framework and derive lower bounds for several specific parallel optimization settings, including delayed updates and parallel processing with intermittent communication. We highlight gaps between lower and upper bounds on the oracle complexity, and cases where the \u201cnatural\u201d algorithms are not known to be optimal."
    },
    "keywords": [
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "parallel optimization",
            "url": "https://en.wikipedia.org/wiki/Parallel_Optimization"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "parallelism",
            "url": "https://en.wikipedia.org/wiki/parallelism"
        }
    ],
    "highlights": [
        "There has been great interest in stochastic optimization and learning algorithms that leverage parallelism, including e.g. delayed updates arising from pipelining and asynchronous concurrent processing, synchronous single-instruction-multiple-data parallelism, and parallelism across distant devices",
        "Oracle models have long been a useful framework for formalizing stochastic optimization and learning problems",
        "Finding such gaps can be very useful\u2014for example, the gap between the first order optimization lower bound of Nemirovski et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] and the best known algorithms at the time inspired Nesterov\u2019s accelerated gradient descent algorithm [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "We propose an oracle framework for formalizing different parallel optimization problems",
        "In Section 4 we study specific parallel optimization settings in which"
    ],
    "key_statements": [
        "There has been great interest in stochastic optimization and learning algorithms that leverage parallelism, including e.g. delayed updates arising from pipelining and asynchronous concurrent processing, synchronous single-instruction-multiple-data parallelism, and parallelism across distant devices",
        "Oracle models have long been a useful framework for formalizing stochastic optimization and learning problems",
        "Finding such gaps can be very useful\u2014for example, the gap between the first order optimization lower bound of Nemirovski et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] and the best known algorithms at the time inspired Nesterov\u2019s accelerated gradient descent algorithm [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "We propose an oracle framework for formalizing different parallel optimization problems",
        "In Section 4 we study specific parallel optimization settings in which"
    ],
    "summary": [
        "There has been great interest in stochastic optimization and learning algorithms that leverage parallelism, including e.g. delayed updates arising from pipelining and asynchronous concurrent processing, synchronous single-instruction-multiple-data parallelism, and parallelism across distant devices.",
        "Many algorithms have been proposed, formulate them as graph-based oracle parallel optimization problems, instantiate our lower bounds, and compare them with the performance guarantees of specific algorithms.",
        "Similar to Arjevani and Shamir [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], to establish the optimization term in the lower bounds, we construct functions that require multiple rounds of sequential oracle accesses to optimize.",
        "In the gradient oracle case, we use a single, deterministic function which resembles a standard construction for first order optimization lower bounds.",
        "These lower bounds p are matched by sequential stochastic gradient descent, yielding a tight complexity of \u21e5(L/ T ) and the familiar conclusion that SGD is optimal in this setting.",
        "The same algorithm is optimal even if prox access is allowed, since Theorem 2 implies a lower bound of:",
        "This yields a suboptimality guarantee that precisely matches (12), establishing that the lower bound from Theorem 2 is tight for the layer graph, and that smoothed A-MB-SGD is optimal.",
        "We can combine all KM oracle accesses between communication rounds in order to form a single mini-batch, giving up on the possibility of sequential computation along the \u201clocal\u201d K node sub-paths.",
        "Using all KM nodes to obtain stochastic gradient estimates at the same point, we can perform T iterations of A-MB-SGD with a mini-batch size of KM , yielding an upper bound of",
        "This raises the question of whether there is a single, natural algorithm, perhaps an accelerated variant of the parallel SGD updates (17), that at the very least matches (20), and preferably improves over them in the intermediate regime or even matches the lower bound (16).",
        "The best known upper bound for optimizing with intermittent communication using a pure stochastic oracle is (20), which combines two different algorithms.",
        "Our main contributions in this paper are: (1) presenting a precise formal oracle framework for studying parallel stochastic optimization; (2) establishing tight oracle lower bounds in this framework that can be applied to particular instances of parallel optimization; and (3) using the framework to study specific settings, obtaining optimality guarantees, understanding where additional assumptions would be needed to break barriers, and, perhaps most importantly, identifying gaps in our understanding that highlight possibilities for algorithmic improvement.",
        "For non-smooth objectives and a stochastic prox oracle, smoothing and acceleration can improve performance in the layer graph setting using.",
        "We suggest an alternative optimal algorithm, but it would be interesting and beneficial to understand the true behavior of delayed update SGD and to improve it as necessary to attain optimality"
    ],
    "headline": "We propose an oracle framework for formalizing different parallel optimization problems",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pages 873\u2013881, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Duchi%2C%20John%20C.%20Distributed%20delayed%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Duchi%2C%20John%20C.%20Distributed%20delayed%20stochastic%20optimization%202011"
        },
        {
            "id": "2",
            "entry": "[2] Naman Agarwal and Elad Hazan. Lower bounds for higher-order convex optimization. arXiv preprint arXiv:1710.10329, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10329"
        },
        {
            "id": "3",
            "entry": "[3] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. In Advances in Neural Information Processing Systems, pages 1756\u20131764, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjevani%2C%20Yossi%20Shamir%2C%20Ohad%20Communication%20complexity%20of%20distributed%20convex%20learning%20and%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjevani%2C%20Yossi%20Shamir%2C%20Ohad%20Communication%20complexity%20of%20distributed%20convex%20learning%20and%20optimization%202015"
        },
        {
            "id": "4",
            "entry": "[4] Yossi Arjevani, Ohad Shamir, and Nathan Srebro. A tight convergence analysis for stochastic gradient descent with delayed updates. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjevani%2C%20Yossi%20Shamir%2C%20Ohad%20Srebro%2C%20Nathan%20A%20tight%20convergence%20analysis%20for%20stochastic%20gradient%20descent%20with%20delayed%20updates%202018"
        },
        {
            "id": "5",
            "entry": "[5] Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator theory in Hilbert spaces, volume 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauschke%2C%20Heinz%20H.%20Combettes%2C%20Patrick%20L.%20Convex%20analysis%20and%20monotone%20operator%20theory%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bauschke%2C%20Heinz%20H.%20Combettes%2C%20Patrick%20L.%20Convex%20analysis%20and%20monotone%20operator%20theory%202011"
        },
        {
            "id": "6",
            "entry": "[6] Dimitri P Bertsekas. Parallel and distributed computation: numerical methods, volume 23. Prentice hall Englewood Cliffs, NJ, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Parallel%20and%20distributed%20computation%3A%20numerical%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20Dimitri%20P.%20Parallel%20and%20distributed%20computation%3A%20numerical%201989"
        },
        {
            "id": "7",
            "entry": "[7] Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011\u20131020. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Braverman%2C%20Mark%20Garg%2C%20Ankit%20Ma%2C%20Tengyu%20Nguyen%2C%20Huy%20L.%20Communication%20lower%20bounds%20for%20statistical%20estimation%20problems%20via%20a%20distributed%20data%20processing%20inequality%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Braverman%2C%20Mark%20Garg%2C%20Ankit%20Ma%2C%20Tengyu%20Nguyen%2C%20Huy%20L.%20Communication%20lower%20bounds%20for%20statistical%20estimation%20problems%20via%20a%20distributed%20data%20processing%20inequality%202016"
        },
        {
            "id": "8",
            "entry": "[8] Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points i. arXiv preprint arXiv:1710.11606, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11606"
        },
        {
            "id": "9",
            "entry": "[9] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In Advances in neural information processing systems, pages 1647\u20131655, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cotter%2C%20Andrew%20Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Sridharan%2C%20Karthik%20Better%20mini-batch%20algorithms%20via%20accelerated%20gradient%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cotter%2C%20Andrew%20Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Sridharan%2C%20Karthik%20Better%20mini-batch%20algorithms%20via%20accelerated%20gradient%20methods%202011"
        },
        {
            "id": "10",
            "entry": "[10] John Duchi, Feng Ruan, and Chulhee Yun. Minimax bounds on stochastic batched convex optimization. In Proceedings of the 31st Conference On Learning Theory, pages 3065\u20133162, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Ruan%2C%20Feng%20Yun%2C%20Chulhee%20Minimax%20bounds%20on%20stochastic%20batched%20convex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Ruan%2C%20Feng%20Yun%2C%20Chulhee%20Minimax%20bounds%20on%20stochastic%20batched%20convex%20optimization%202018"
        },
        {
            "id": "11",
            "entry": "[11] Hamid Reza Feyzmahdavian, Arda Aytekin, and Mikael Johansson. An asynchronous minibatch algorithm for regularized stochastic optimization. IEEE Transactions on Automatic Control, 61(12):3740\u20133754, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feyzmahdavian%2C%20Hamid%20Reza%20Aytekin%2C%20Arda%20Johansson%2C%20Mikael%20An%20asynchronous%20minibatch%20algorithm%20for%20regularized%20stochastic%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feyzmahdavian%2C%20Hamid%20Reza%20Aytekin%2C%20Arda%20Johansson%2C%20Mikael%20An%20asynchronous%20minibatch%20algorithm%20for%20regularized%20stochastic%20optimization%202016"
        },
        {
            "id": "12",
            "entry": "[12] Ankit Garg, Tengyu Ma, and Huy Nguyen. On communication cost of distributed statistical estimation and dimensionality. In Advances in Neural Information Processing Systems, pages 2726\u20132734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garg%2C%20Ankit%20Ma%2C%20Tengyu%20Nguyen%2C%20Huy%20On%20communication%20cost%20of%20distributed%20statistical%20estimation%20and%20dimensionality%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garg%2C%20Ankit%20Ma%2C%20Tengyu%20Nguyen%2C%20Huy%20On%20communication%20cost%20of%20distributed%20statistical%20estimation%20and%20dimensionality%202014"
        },
        {
            "id": "13",
            "entry": "[13] Crist\u00f3bal Guzm\u00e1n and Arkadi Nemirovski. On lower complexity bounds for large-scale smooth convex optimization. Journal of Complexity, 31(1):1\u201314, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guzm%C3%A1n%2C%20Crist%C3%B3bal%20Nemirovski%2C%20Arkadi%20On%20lower%20complexity%20bounds%20for%20large-scale%20smooth%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guzm%C3%A1n%2C%20Crist%C3%B3bal%20Nemirovski%2C%20Arkadi%20On%20lower%20complexity%20bounds%20for%20large-scale%20smooth%20convex%20optimization%202015"
        },
        {
            "id": "14",
            "entry": "[14] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "15",
            "entry": "[15] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2):365\u2013397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Guanghui%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Guanghui%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012"
        },
        {
            "id": "16",
            "entry": "[16] Jason D Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang. Distributed stochastic variance reduced gradient methods by sampling extra data with replacement. The Journal of Machine Learning Research, 18(1):4404\u20134446, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Lin%2C%20Qihang%20Ma%2C%20Tengyu%20Yang%2C%20Tianbao%20Distributed%20stochastic%20variance%20reduced%20gradient%20methods%20by%20sampling%20extra%20data%20with%20replacement%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Lin%2C%20Qihang%20Ma%2C%20Tengyu%20Yang%2C%20Tianbao%20Distributed%20stochastic%20variance%20reduced%20gradient%20methods%20by%20sampling%20extra%20data%20with%20replacement%202017"
        },
        {
            "id": "17",
            "entry": "[17] Brendan McMahan and Matthew Streeter. Delay-tolerant algorithms for asynchronous distributed online learning. In Advances in Neural Information Processing Systems, pages 2915\u2013 2923, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20Brendan%20Streeter%2C%20Matthew%20Delay-tolerant%20algorithms%20for%20asynchronous%20distributed%20online%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20Brendan%20Streeter%2C%20Matthew%20Delay-tolerant%20algorithms%20for%20asynchronous%20distributed%20online%20learning%202014"
        },
        {
            "id": "18",
            "entry": "[18] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20H.Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20H.Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202017"
        },
        {
            "id": "19",
            "entry": "[19] A Nedic, Dimitri P Bertsekas, and Vivek S Borkar. Distributed asynchronous incremental subgradient methods. Studies in Computational Mathematics, 8(C):381\u2013407, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20A.%20Bertsekas%2C%20Dimitri%20P.%20Borkar%2C%20Vivek%20S.%20Distributed%20asynchronous%20incremental%20subgradient%20methods%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20A.%20Bertsekas%2C%20Dimitri%20P.%20Borkar%2C%20Vivek%20S.%20Distributed%20asynchronous%20incremental%20subgradient%20methods%202001"
        },
        {
            "id": "20",
            "entry": "[20] Arkadi Nemirovski. On parallel complexity of nonsmooth convex optimization. Journal of Complexity, 10(4):451\u2013463, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadi%20On%20parallel%20complexity%20of%20nonsmooth%20convex%20optimization%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20Arkadi%20On%20parallel%20complexity%20of%20nonsmooth%20convex%20optimization%201994"
        },
        {
            "id": "21",
            "entry": "[21] Arkadii Nemirovski, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and method efficiency in optimization. 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadii%20Yudin%2C%20David%20Borisovich%20Dawson%2C%20Edgar%20Ronald%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983"
        },
        {
            "id": "22",
            "entry": "[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%201983"
        },
        {
            "id": "23",
            "entry": "[23] Shai Shalev-Shwartz and Nathan Srebro. Svm optimization: inverse dependence on training set size. In International Conference on Machine Learning, pages 928\u2013935, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Srebro%2C%20Nathan%20Svm%20optimization%3A%20inverse%20dependence%20on%20training%20set%20size%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Srebro%2C%20Nathan%20Svm%20optimization%3A%20inverse%20dependence%20on%20training%20set%20size%202008"
        },
        {
            "id": "24",
            "entry": "[24] Eric V Slud et al. Distribution inequalities for the binomial law. The Annals of Probability, 5 (3):404\u2013412, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Slud%2C%20Eric%20V.%20Distribution%20inequalities%20for%20the%20binomial%20law.%20The%20Annals%20of%20Probability%2C%205%201977"
        },
        {
            "id": "25",
            "entry": "[25] Suvrit Sra, Adams Wei Yu, Mu Li, and Alex Smola. Adadelay: Delay adaptive distributed stochastic optimization. In Artificial Intelligence and Statistics, pages 957\u2013965, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sra%2C%20Suvrit%20Yu%2C%20Adams%20Wei%20Li%2C%20Mu%20Smola%2C%20Alex%20Adadelay%3A%20Delay%20adaptive%20distributed%20stochastic%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sra%2C%20Suvrit%20Yu%2C%20Adams%20Wei%20Li%2C%20Mu%20Smola%2C%20Alex%20Adadelay%3A%20Delay%20adaptive%20distributed%20stochastic%20optimization%202016"
        },
        {
            "id": "26",
            "entry": "[26] Jialei Wang, Weiran Wang, and Nathan Srebro. Memory and communication efficient distributed stochastic optimization with minibatch prox. In Conference on Learning Theory, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jialei%20Wang%2C%20Weiran%20Srebro%2C%20Nathan%20Memory%20and%20communication%20efficient%20distributed%20stochastic%20optimization%20with%20minibatch%20prox%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jialei%20Wang%2C%20Weiran%20Srebro%2C%20Nathan%20Memory%20and%20communication%20efficient%20distributed%20stochastic%20optimization%20with%20minibatch%20prox%202017"
        },
        {
            "id": "27",
            "entry": "[27] Blake Woodworth and Nathan Srebro. Lower bound for randomized first order convex optimization. arXiv preprint arXiv:1709.03594, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03594"
        },
        {
            "id": "28",
            "entry": "[28] Blake Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives. In Advances in Neural Information Processing Systems, pages 3639\u20133647, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodworth%2C%20Blake%20Srebro%2C%20Nati%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodworth%2C%20Blake%20Srebro%2C%20Nati%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016"
        },
        {
            "id": "29",
            "entry": "[29] Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic lower bounds for distributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems, pages 2328\u20132336, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Duchi%2C%20John%20Jordan%2C%20Michael%20I.%20Wainwright%2C%20Martin%20J.%20Information-theoretic%20lower%20bounds%20for%20distributed%20statistical%20estimation%20with%20communication%20constraints%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Duchi%2C%20John%20Jordan%2C%20Michael%20I.%20Wainwright%2C%20Martin%20J.%20Information-theoretic%20lower%20bounds%20for%20distributed%20statistical%20estimation%20with%20communication%20constraints%202013"
        }
    ]
}
