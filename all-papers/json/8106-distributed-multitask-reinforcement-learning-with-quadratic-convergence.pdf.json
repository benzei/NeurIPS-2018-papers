{
    "filename": "8106-distributed-multitask-reinforcement-learning-with-quadratic-convergence.pdf",
    "metadata": {
        "title": "Distributed Multitask Reinforcement Learning with Quadratic Convergence",
        "author": "Rasul Tutunov, Dongho Kim, Haitham Bou Ammar",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8106-distributed-multitask-reinforcement-learning-with-quadratic-convergence.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Multitask reinforcement learning (MTRL) suffers from scalability issues when the number of tasks or trajectories grows large. The main reason behind this drawback is the reliance on centeralised solutions. Recent methods exploited the connection between MTRL and general consensus to propose scalable solutions. These methods, however, suffer from two drawbacks. First, they rely on predefined objectives, and, second, exhibit linear convergence guarantees. In this paper, we improve over state-of-the-art by deriving multitask reinforcement learning from a variational inference perspective. We then propose a novel distributed solver for MTRL with quadratic convergence guarantees."
    },
    "keywords": [
        {
            "term": "trajectory optimization",
            "url": "https://en.wikipedia.org/wiki/trajectory_optimization"
        },
        {
            "term": "lifelong learning",
            "url": "https://en.wikipedia.org/wiki/lifelong_learning"
        },
        {
            "term": "sequential decision making",
            "url": "https://en.wikipedia.org/wiki/sequential_decision_making"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "quadratic convergence",
            "url": "https://en.wikipedia.org/wiki/quadratic_convergence"
        },
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] algorithms are successful in solving sequential decision making (SDM) tasks",
        "The authors in [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] proposed a distributed solver for multi-task reinforcement learning with linear convergence guarantees based on the Alternating Direction Method of Multipliers (ADMM)",
        "Rather than defining the optimisation objective directly as done in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], we provide a probabilistic modeling view of the problem by framing multi-task reinforcement learning as an instance of variational inference",
        "We develop the first distributed multi-task reinforcement learning algorithm with provable quadratic convergence guarantees",
        "We evaluated our algorithm against five other approaches being: 1) ADD [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], 2) Alternating Direction Method of Multipliers [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], 3) distributed averaging [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], 4) network-newton [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], and 5) sub-gradients",
        "We proposed a distributed solver for multitask reinforcement learning with quadratic convergence"
    ],
    "key_statements": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] algorithms are successful in solving sequential decision making (SDM) tasks",
        "The authors in [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] proposed a distributed solver for multi-task reinforcement learning with linear convergence guarantees based on the Alternating Direction Method of Multipliers (ADMM)",
        "We show our novel distributed solver using Chebyshev polynomials has quadratic convergence guarantees",
        "Rather than defining the optimisation objective directly as done in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], we provide a probabilistic modeling view of the problem by framing multi-task reinforcement learning as an instance of variational inference",
        "We propose a novel distributed Newton method with quadratic convergence guarantees2",
        "We develop the first distributed multi-task reinforcement learning algorithm with provable quadratic convergence guarantees",
        "We evaluated our algorithm against five other approaches being: 1) ADD [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], 2) Alternating Direction Method of Multipliers [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], 3) distributed averaging [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], 4) network-newton [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], and 5) sub-gradients",
        "Our experiments ran on five systems, simple mass (SM), double mass (DM), cart-pole (CP), helicopter (HC), and humanoid robots (HR)",
        "We proposed a distributed solver for multitask reinforcement learning with quadratic convergence"
    ],
    "summary": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] algorithms are successful in solving sequential decision making (SDM) tasks.",
        "The authors in [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] proposed a distributed solver for MTRL with linear convergence guarantees based on the Alternating Direction Method of Multipliers (ADMM).",
        "We show that our new technique outperforms state-of-the-art methods from both distributed optimisation and lifelong reinforcement learning on a variety of graph topologies.",
        "Solving the problem in Equation 2 amounts to determining both variational and model parameters, i.e.,",
        "We propose a novel distributed Newton method with quadratic convergence guarantees2.",
        "Given an updated variational (i.e., E-step) and fixed task-specific parameters, the optimisation problem for \u21e5sh can be written as: X T m\u21e5ashx t",
        "The above is distributing Equation 3 among n nodes, where each computes it=s 1local copy of \u21e5sh.",
        "In a recent attempt [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], the authors propose a distributed second-order method for general consensus by using the approach in [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] to compute the Newton direction.",
        "Though adopting second-order techniques (e.g., Newton iteration) can lead to improved convergence speeds, direct application of standard Newton is difficult as we require a distributed procedure to accurately compute the direction of descent3.",
        "Before discussing the SDD properties of the dual Hessian, we still require a procedure that allows us to infer about the primal (i.e., y) given updated parameters .",
        "Inverting H( s) to determine the Newton direction is not possible in a distributed setting since computing the inverse requires global information.",
        "Please note that by exploiting the recursive properties of Chebyshev polynomials we can derive an approximate solution without the need to compute H 1 explicitly.",
        "Distributed Newton method using the Chebyshev solver exhibits the following two convergence phases for some constants c and c : Strict",
        "We conducted two sets of experiments to compare against distributed and multitask learning methods.",
        "7Please note that the solution of this system can be split into dk linear systems that can be solved efficiently using the distributed Chebyshev solver.",
        "Distributed SDD-Newton has a growth rate proportional to the condition number of the graph being much slower compared to the exponential growth observed by other techniques.",
        "These results show that our method can outperform others in terms of jump-start and asymptotic performance while requiring fewer iterations.",
        "Our steps include developing an incremental version of our algorithm using generalised Hessians, and conducting experiments running on true distributed architectures to quantify the trade-off between communication and computation."
    ],
    "headline": "We propose a novel distributed solver for multi-task reinforcement learning with quadratic convergence guarantees",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Thira Chavarnakul and David Enke. A hybrid stock trading system for intelligent technical analysis-based equivolume charting. Neurocomput., 72(16-18), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chavarnakul%2C%20Thira%20Enke%2C%20David%20A%20hybrid%20stock%20trading%20system%20for%20intelligent%20technical%20analysis-based%20equivolume%20charting%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chavarnakul%2C%20Thira%20Enke%2C%20David%20A%20hybrid%20stock%20trading%20system%20for%20intelligent%20technical%20analysis-based%20equivolume%20charting%202009"
        },
        {
            "id": "2",
            "entry": "[2] Marc Peter Deisenroth, Peter Englert, Jochen Peters, and Dieter Fox. Multi-task policy search for robotics. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 3876\u20133881. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20Marc%20Peter%20Englert%2C%20Peter%20Peters%2C%20Jochen%20Fox%2C%20Dieter%20Multi-task%20policy%20search%20for%20robotics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20Marc%20Peter%20Englert%2C%20Peter%20Peters%2C%20Jochen%20Fox%2C%20Dieter%20Multi-task%20policy%20search%20for%20robotics%202014"
        },
        {
            "id": "3",
            "entry": "[3] Jens Kober and Jan R Peters. Policy search for motor primitives in robotics. In Advances in neural information processing systems, pages 849\u2013856, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20Jens%20Peters%2C%20Jan%20R.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20Jens%20Peters%2C%20Jan%20R.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009"
        },
        {
            "id": "4",
            "entry": "[4] Matthew E. Taylor and Peter Stone. Transfer Learning for Reinforcement Learning Domains: A Survey. Journal of Machine Learning Research, 10:1633\u20131685, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Matthew%20E.%20Stone%2C%20Peter%20Transfer%20Learning%20for%20Reinforcement%20Learning%20Domains%3A%20A%20Survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Matthew%20E.%20Stone%2C%20Peter%20Transfer%20Learning%20for%20Reinforcement%20Learning%20Domains%3A%20A%20Survey%202009"
        },
        {
            "id": "5",
            "entry": "[5] Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Sequential Transfer in Multiarmed Bandit with Finite Set of Models. In Advances in Neural Information Processing Systems 26. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20Mohammad%20Gheshlaghi%20Lazaric%2C%20Alessandro%20Brunskill%2C%20Emma%20Sequential%20Transfer%20in%20Multiarmed%20Bandit%20with%20Finite%20Set%20of%20Models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azar%2C%20Mohammad%20Gheshlaghi%20Lazaric%2C%20Alessandro%20Brunskill%2C%20Emma%20Sequential%20Transfer%20in%20Multiarmed%20Bandit%20with%20Finite%20Set%20of%20Models%202013"
        },
        {
            "id": "6",
            "entry": "[6] A. Lazaric. Transfer in Reinforcement Learning: A Framework and a Survey. In M. Wiering and M. van Otterlo, editors, Reinforcement Learning: State of the Art. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaric%2C%20A.%20Transfer%20in%20Reinforcement%20Learning%3A%20A%20Framework%20and%20a%20Survey%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazaric%2C%20A.%20Transfer%20in%20Reinforcement%20Learning%3A%20A%20Framework%20and%20a%20Survey%202011"
        },
        {
            "id": "7",
            "entry": "[7] Matthijs Snel and Shimon Whiteson. Learning potential functions and their representations for multi-task reinforcement learning. Autonomous agents and multi-agent systems, 28(4):637\u2013681, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snel%2C%20Matthijs%20Whiteson%2C%20Shimon%20Learning%20potential%20functions%20and%20their%20representations%20for%20multi-task%20reinforcement%20learning.%20Autonomous%20agents%20and%20multi-agent%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snel%2C%20Matthijs%20Whiteson%2C%20Shimon%20Learning%20potential%20functions%20and%20their%20representations%20for%20multi-task%20reinforcement%20learning.%20Autonomous%20agents%20and%20multi-agent%202014"
        },
        {
            "id": "8",
            "entry": "[8] Sebastian Thrun and Joseph O\u2019Sullivan. Learning More From Less Data: Experiment in Lifelong Learning. In Seminar Digest, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20Sebastian%20O%E2%80%99Sullivan%2C%20Joseph%20Learning%20More%20From%20Less%20Data%3A%20Experiment%20in%20Lifelong%20Learning%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thrun%2C%20Sebastian%20O%E2%80%99Sullivan%2C%20Joseph%20Learning%20More%20From%20Less%20Data%3A%20Experiment%20in%20Lifelong%20Learning%201996"
        },
        {
            "id": "9",
            "entry": "[9] Paul Ruvolo and Eric Eaton. ELLA: An Efficient Lifelong Learning Algorithm. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Paul%20Ruvolo%20and%20Eric%20Eaton.%20ELLA%3A%20An%20Efficient%20Lifelong%20Learning%20Algorithm%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Paul%20Ruvolo%20and%20Eric%20Eaton.%20ELLA%3A%20An%20Efficient%20Lifelong%20Learning%20Algorithm%202013"
        },
        {
            "id": "10",
            "entry": "[10] Haitham Bou-Ammar, Eric Eaton, Paul Ruvolo, and Matthew Taylor. Online Multi-task Learning for Policy Gradient Methods. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning. JMLR Workshop and Conference Proceedings, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bou-Ammar%2C%20Haitham%20Eaton%2C%20Eric%20Ruvolo%2C%20Paul%20Taylor%2C%20Matthew%20Online%20Multi-task%20Learning%20for%20Policy%20Gradient%20Methods%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bou-Ammar%2C%20Haitham%20Eaton%2C%20Eric%20Ruvolo%2C%20Paul%20Taylor%2C%20Matthew%20Online%20Multi-task%20Learning%20for%20Policy%20Gradient%20Methods%202014"
        },
        {
            "id": "11",
            "entry": "[11] El Bsat, Bou-Ammar Haitham, and Mathew Taylor. Scalable Multitask Policy Gradient Reinforcement Learning. In AAAI, February 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bsat%2C%20El%20Haitham%2C%20Bou-Ammar%20Taylor%2C%20Mathew%20Scalable%20Multitask%20Policy%20Gradient%20Reinforcement%20Learning%202017-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bsat%2C%20El%20Haitham%2C%20Bou-Ammar%20Taylor%2C%20Mathew%20Scalable%20Multitask%20Policy%20Gradient%20Reinforcement%20Learning%202017-02"
        },
        {
            "id": "12",
            "entry": "[12] Pedro A. Forero, Alfonso Cano, and Georgios B. Giannakis. Consensus-based distributed support vector machines. J. Mach. Learn. Res., 11:1663\u20131707, August 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forero%2C%20Pedro%20A.%20Cano%2C%20Alfonso%20Giannakis%2C%20Georgios%20B.%20Consensus-based%20distributed%20support%20vector%20machines%202010-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Forero%2C%20Pedro%20A.%20Cano%2C%20Alfonso%20Giannakis%2C%20Georgios%20B.%20Consensus-based%20distributed%20support%20vector%20machines%202010-08"
        },
        {
            "id": "13",
            "entry": "[13] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Convex%20Optimization%202004"
        },
        {
            "id": "14",
            "entry": "[14] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Introduction%20to%20Reinforcement%20Learning%201998"
        },
        {
            "id": "15",
            "entry": "[15] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1049\u20131056, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009"
        },
        {
            "id": "16",
            "entry": "[16] Gerhard Neumann. Variational Inference for Policy Search in changing situations. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 817\u2013824, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neumann%2C%20Gerhard%20Variational%20Inference%20for%20Policy%20Search%20in%20changing%20situations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neumann%2C%20Gerhard%20Variational%20Inference%20for%20Policy%20Search%20in%20changing%20situations%202011"
        },
        {
            "id": "17",
            "entry": "[17] Sergey Levine and Vladlen Koltun. Variational policy search via trajectory optimization. In Advances in Neural Information Processing Systems 26, pages 207\u2013215, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Variational%20policy%20search%20via%20trajectory%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Variational%20policy%20search%20via%20trajectory%20optimization%202013"
        },
        {
            "id": "18",
            "entry": "[18] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, R\u00e9mi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimization. In ICLR, pages 1\u201322, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abdolmaleki%2C%20Abbas%20Springenberg%2C%20Jost%20Tobias%20Tassa%2C%20Yuval%20Munos%2C%20R%C3%A9mi%20Maximum%20a%20posteriori%20policy%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abdolmaleki%2C%20Abbas%20Springenberg%2C%20Jost%20Tobias%20Tassa%2C%20Yuval%20Munos%2C%20R%C3%A9mi%20Maximum%20a%20posteriori%20policy%20optimization%202018"
        },
        {
            "id": "19",
            "entry": "[19] Hui Li, Xuejun Liao, and Lawrence Carin. Multi-task reinforcement learning in partially observable stochastic environments. The Journal of Machine Learning Research, 10:1131\u20131186, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Hui%20Liao%2C%20Xuejun%20Carin%2C%20Lawrence%20Multi-task%20reinforcement%20learning%20in%20partially%20observable%20stochastic%20environments%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Hui%20Liao%2C%20Xuejun%20Carin%2C%20Lawrence%20Multi-task%20reinforcement%20learning%20in%20partially%20observable%20stochastic%20environments%202009"
        },
        {
            "id": "20",
            "entry": "[20] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20Inference%3A%20A%20Review%20for%20Statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20Inference%3A%20A%20Review%20for%20Statisticians%202017"
        },
        {
            "id": "21",
            "entry": "[21] Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic Variational Inference. Journal of Machine Learning Research, 14:1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20Variational%20Inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20Variational%20Inference%202013"
        },
        {
            "id": "22",
            "entry": "[22] A. Nedic and A.E. Ozdaglar. Distributed Subgradient Methods for Multi-Agent Optimization. IEEE Trans. Automat. Contr., (1):48\u201361, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20A.%20Ozdaglar%2C%20A.E.%20Distributed%20Subgradient%20Methods%20for%20Multi-Agent%20Optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20A.%20Ozdaglar%2C%20A.E.%20Distributed%20Subgradient%20Methods%20for%20Multi-Agent%20Optimization%202009"
        },
        {
            "id": "23",
            "entry": "[23] Ermin Wei and Asuman Ozdaglar. Distributed alternating direction method of multipliers. In Decision and Control (CDC), 2012 IEEE 51st Annual Conference on, pages 5445\u20135450. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Ermin%20Ozdaglar%2C%20Asuman%20Distributed%20alternating%20direction%20method%20of%20multipliers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Ermin%20Ozdaglar%2C%20Asuman%20Distributed%20alternating%20direction%20method%20of%20multipliers%202012"
        },
        {
            "id": "24",
            "entry": "[24] J. L. Goffin. On convergence Rates of Subgradient Optimization Methods. Mathematical Programming, 13, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goffin%2C%20J.L.%20On%20convergence%20Rates%20of%20Subgradient%20Optimization%20Methods%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goffin%2C%20J.L.%20On%20convergence%20Rates%20of%20Subgradient%20Optimization%20Methods%201977"
        },
        {
            "id": "25",
            "entry": "[25] A. Mokhtari, Q. Ling, and A. Ribeiro. Network Newton-Part I: Algorithm and Convergence. ArXiv e-prints, 2015, 1504.06017.",
            "arxiv_url": "https://arxiv.org/pdf/1504.06017"
        },
        {
            "id": "26",
            "entry": "[26] A. Mokhtari, Q. Ling, and A. Ribeiro. Network Newton-Part II: Convergence Rate and Implementation. ArXiv e-prints, 2015, 1504.06020.",
            "arxiv_url": "https://arxiv.org/pdf/1504.06020"
        },
        {
            "id": "27",
            "entry": "[27] M. Zargham, A. Ribeiro, A. E. Ozdaglar, and A. Jadbabaie. Accelerated Dual Descent for Network Flow Optimization. IEEE Transactions on Automatic Control, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zargham%2C%20M.%20Ribeiro%2C%20A.%20Ozdaglar%2C%20A.E.%20Jadbabaie%2C%20A.%20Accelerated%20Dual%20Descent%20for%20Network%20Flow%20Optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zargham%2C%20M.%20Ribeiro%2C%20A.%20Ozdaglar%2C%20A.E.%20Jadbabaie%2C%20A.%20Accelerated%20Dual%20Descent%20for%20Network%20Flow%20Optimization%202014"
        },
        {
            "id": "28",
            "entry": "[28] R. Tutunov, H. Bou-Ammar, and A. Jadbabaie. Distributed SDDM Solvers: Theory & Applications. ArXiv e-prints, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tutunov%2C%20R.%20Bou-Ammar%2C%20H.%20Jadbabaie%2C%20A.%20Distributed%20SDDM%20Solvers%3A%20Theory%20%26%20Applications%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tutunov%2C%20R.%20Bou-Ammar%2C%20H.%20Jadbabaie%2C%20A.%20Distributed%20SDDM%20Solvers%3A%20Theory%20%26%20Applications%202015"
        },
        {
            "id": "29",
            "entry": "[29] Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. CoRR, abs/cs/0607105, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spielman%2C%20Daniel%20A.%20Teng%2C%20Shang-Hua%20Nearly-linear%20time%20algorithms%20for%20preconditioning%20and%20solving%20symmetric%2C%20diagonally%20dominant%20linear%20systems%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spielman%2C%20Daniel%20A.%20Teng%2C%20Shang-Hua%20Nearly-linear%20time%20algorithms%20for%20preconditioning%20and%20solving%20symmetric%2C%20diagonally%20dominant%20linear%20systems%202006"
        },
        {
            "id": "30",
            "entry": "[30] A. Olshevsky. Linear Time Average Consensus on Fixed Graphs and Implications for Decentralized Optimization and Multi-Agent Control. ArXiv e-prints, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshevsky%2C%20A.%20Linear%20Time%20Average%20Consensus%20on%20Fixed%20Graphs%20and%20Implications%20for%20Decentralized%20Optimization%20and%20Multi-Agent%20Control.%20ArXiv%20e-prints%202014"
        },
        {
            "id": "31",
            "entry": "[31] Jens Kober and Jan Peters. Policy Search for Motor Primitives in Robotics. Machine Learning, 84(1-2), July 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20Jens%20Peters%2C%20Jan%20Policy%20Search%20for%20Motor%20Primitives%20in%20Robotics%202011-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20Jens%20Peters%2C%20Jan%20Policy%20Search%20for%20Motor%20Primitives%20in%20Robotics%202011-07"
        },
        {
            "id": "32",
            "entry": "[32] John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region Policy Optimization. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=John%20Schulman%20Sergey%20Levine%20Philipp%20Moritz%20Michael%20Jordan%20and%20Pieter%20Abbeel%20Trust%20Region%20Policy%20Optimization%20In%20ICML%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=John%20Schulman%20Sergey%20Levine%20Philipp%20Moritz%20Michael%20Jordan%20and%20Pieter%20Abbeel%20Trust%20Region%20Policy%20Optimization%20In%20ICML%202015"
        },
        {
            "id": "33",
            "entry": "[33] Abhishek Kumar and Hal Daum\u00e9 III. Learning Task Grouping and Overlap in Multi-Task Learning. In International Conference on Machine Learning (ICML), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20Abhishek%20Daum%C3%A9%2C%20III%2C%20Hal%20Learning%20Task%20Grouping%20and%20Overlap%20in%20Multi-Task%20Learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20Abhishek%20Daum%C3%A9%2C%20III%2C%20Hal%20Learning%20Task%20Grouping%20and%20Overlap%20in%20Multi-Task%20Learning%202012"
        },
        {
            "id": "34",
            "entry": "[34] Jan Peters and Stefan Schaal. Natural Actor-Critic. Neurocomputing, 71, 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jan%20Peters%20and%20Stefan%20Schaal%20Natural%20ActorCritic%20Neurocomputing%2071%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jan%20Peters%20and%20Stefan%20Schaal%20Natural%20ActorCritic%20Neurocomputing%2071%202008"
        }
    ]
}
