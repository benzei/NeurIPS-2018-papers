{
    "filename": "7718-fast-rates-of-erm-and-stochastic-approximation-adaptive-to-error-bound-conditions.pdf",
    "metadata": {
        "title": "Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions",
        "author": "Mingrui Liu, Xiaoxuan Zhang, Lijun Zhang, Jing Rong, Tianbao Yang",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7718-fast-rates-of-erm-and-stochastic-approximation-adaptive-to-error-bound-conditions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have recently received increasing attention for developing optimization algorithms with fast convergence. However, the studies of EBC in statistical learning are hitherto still limited. The main contributions of this paper are two-fold. First, we develop fast and intermediate rates of empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous, and smooth convex random functions. Second, we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization with Lipschitz continuous random functions, which requires only one pass of n samples and adapts to E\u221aBC. For both approaches, the convergence rates span a full spectrum between O(1/ n)"
    },
    "keywords": [
        {
            "term": "random function",
            "url": "https://en.wikipedia.org/wiki/random_function"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "lipschitz continuous",
            "url": "https://en.wikipedia.org/wiki/lipschitz_continuous"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "statistical learning",
            "url": "https://en.wikipedia.org/wiki/statistical_learning"
        },
        {
            "term": "stochastic approximation",
            "url": "https://en.wikipedia.org/wiki/stochastic_approximation"
        }
    ],
    "highlights": [
        "A classical result about the excess risk bound for the considered risk minimization problem is in the order of O( d/n) 1 and O( 1/n) for Empirical Risk Minimization and stochastic approximation, respectively, under appropriate conditions of the loss functions (e.g., Lipschitz continuity, convexity) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>]",
        "We develop fast and optimistic rates of Empirical Risk Minimization for non-negative, Lipschitz continuous and smooth convex loss functions in the order of O(d/n + 2\u2212\u03b8 ), and in the order of O((d/n) 2\u2212\u03b8 + 2\u2212\u03b8 ) when the sample size n is sufficiently large, which imply that when the optimal risk P\u2217 is small one can achieve a fast rate of O (d/n) even with \u03b8 < 1 and a faster rate of O((d/n) 2\u2212\u03b8 ) when n is sufficiently large",
        "We have comprehensively studied statistical learning under the error bound condition for both Empirical Risk Minimization and stochastic approximation",
        "We developed improved rates for non-negative and smooth convex loss functions, which induce faster rates that were not achieved before",
        "Applications in machine learning and other fields are considered and empirical studies corroborate the fast rate of the developed algorithms",
        "An open question is how to develop efficient stochastic approximation algorithms under the error bound condition with optimistic rates for non-negative smooth loss functions similar to the results obtained for empirical risk minimization in this paper"
    ],
    "key_statements": [
        "A classical result about the excess risk bound for the considered risk minimization problem is in the order of O( d/n) 1 and O( 1/n) for Empirical Risk Minimization and stochastic approximation, respectively, under appropriate conditions of the loss functions (e.g., Lipschitz continuity, convexity) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>]",
        "We focus on the following stochastic convex optimization problems arising in statistical learning and many other fields: min P (w)",
        "In statistical learning [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>], the problem above is referred to as risk minimization where z is interpreted as data, w is interpreted as a model, f (\u00b7, \u00b7) is interpreted as a loss function, and r(\u00b7) is a regularization",
        "A classical result about the excess risk bound for the considered risk minimization problem is in the order of O( d/n) 1 and O( 1/n) for Empirical Risk Minimization and stochastic approximation, respectively, under appropriate conditions of the loss functions (e.g., Lipschitz continuity, convexity) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>]",
        "We show that for Lipchitz continous loss error bound conditions implies a relaxed Bernstein condition, and leads to intermediate rates of O((d/n) 2\u2212\u03b8 ) for Lipschitz continuous loss",
        "We develop fast and optimistic rates of Empirical Risk Minimization for non-negative, Lipschitz continuous and smooth convex loss functions in the order of O(d/n + 2\u2212\u03b8 ), and in the order of O((d/n) 2\u2212\u03b8 + 2\u2212\u03b8 ) when the sample size n is sufficiently large, which imply that when the optimal risk P\u2217 is small one can achieve a fast rate of O (d/n) even with \u03b8 < 1 and a faster rate of O((d/n) 2\u2212\u03b8 ) when n is sufficiently large",
        "We develop an efficient stochastic approximation algorithm with almost the same per-iteration cost as stochastic subgradient methods for Lipschitz continuous loss, which achieves the same order of rate",
        "We develop intermediate rates of Empirical Risk Minimization under error bound conditions when the random function is smooth",
        "We have comprehensively studied statistical learning under the error bound condition for both Empirical Risk Minimization and stochastic approximation",
        "We developed improved rates for non-negative and smooth convex loss functions, which induce faster rates that were not achieved before",
        "Applications in machine learning and other fields are considered and empirical studies corroborate the fast rate of the developed algorithms",
        "An open question is how to develop efficient stochastic approximation algorithms under the error bound condition with optimistic rates for non-negative smooth loss functions similar to the results obtained for empirical risk minimization in this paper"
    ],
    "summary": [
        "A classical result about the excess risk bound for the considered risk minimization problem is in the order of O( d/n) 1 and O( 1/n) for ERM and SA, respectively, under appropriate conditions of the loss functions (e.g., Lipschitz continuity, convexity) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>].",
        "We develop fast and optimistic rates of ERM for non-negative, Lipschitz continuous and smooth convex loss functions in the order of O(d/n + 2\u2212\u03b8 ), and in the order of O((d/n) 2\u2212\u03b8 + 2\u2212\u03b8 ) when the sample size n is sufficiently large, which imply that when the optimal risk P\u2217 is small one can achieve a fast rate of O (d/n) even with \u03b8 < 1 and a faster rate of O((d/n) 2\u2212\u03b8 ) when n is sufficiently large.",
        "We emphasize that employing the EBC for developing fast rates has noticeable benefits: (i) it is complementary to the Bernstein condition and the central condition and enjoyed by several interesting problems whose fast rates are not exhibited yet; it can be leveraged for developing fast and intermediate optimistic rates for non-negative and smooth loss functions; it can be leveraged to develop efficient SA algorithms with intermediate and fast convergence rates.",
        "Zhang et al [<a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>] considered the general stochastic optimization problem (1) with non-negative and smooth loss functions and achieved a series of optimistic results.",
        "We establish intermediate rates of ERM under EBC when the random function is Lipschitz continuous.",
        "Under the condition that the input data x, y are bounded, ERM for hinge loss minimization with 1, \u221e norm constraints, and for minimizing a quadratic function and an 1 norm regularization enjoys an O(1/n) fast rate.",
        "We will present improved optimistic rates of ERM for non-negative smooth loss functions expanding the results in [<a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>].",
        "We will present intermediate rates of an efficient stochastic approximation algorithm for solving (1) adaptive to the EBC under the Assumption 1 and 2.",
        "Since the comparison is focused on the testing error, we include another strong baseline, i.e, averaged stochastic gradient (ASGD) with a constant step size, which enjoys an O(d/n) rate for minimizing the expected square loss without any constraints or regularizations [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "We established the connection between the error bound condition and previous conditions for developing fast rates of empirical risk minimization for Lipschitz continuous loss functions.",
        "An open question is how to develop efficient SA algorithms under the error bound condition with optimistic rates for non-negative smooth loss functions similar to the results obtained for empirical risk minimization in this paper."
    ],
    "headline": "We develop fast and intermediate rates of empirical risk minimization  under error bound conditions for risk minimization with Lipschitz continuous, and smooth convex random functions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), pages 773\u2013781, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Francis%2C%20R.%20Bach%20and%20Eric%20Moulines.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20O%281/n%29%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Francis%2C%20R.%20Bach%20and%20Eric%20Moulines.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20O%281/n%29%202013"
        },
        {
            "id": "2",
            "entry": "[2] Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The Annals of Statistics, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20rademacher%20complexities%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20rademacher%20complexities%202005"
        },
        {
            "id": "3",
            "entry": "[3] Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probability Theory and Related Fields, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Empirical%20minimization%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Empirical%20minimization%202006"
        },
        {
            "id": "4",
            "entry": "[4] Jerome Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce Suter. From error bounds to the complexity of first-order descent methods for convex functions. CoRR, abs/1510.08234, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.08234"
        },
        {
            "id": "5",
            "entry": "[5] James V. Burke and Michael C. Ferris. Weak sharp minima in mathematical programming. SIAM Journal on Control and Optimization, 31(5):1340\u20131359, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burke%2C%20James%20V.%20Ferris%2C%20Michael%20C.%20Weak%20sharp%20minima%20in%20mathematical%20programming%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burke%2C%20James%20V.%20Ferris%2C%20Michael%20C.%20Weak%20sharp%20minima%20in%20mathematical%20programming%201993"
        },
        {
            "id": "6",
            "entry": "[6] Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. arXiv:1602.06661, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.06661"
        },
        {
            "id": "7",
            "entry": "[7] John Duchi and Yoram Singer. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899\u20132934, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Singer%2C%20Yoram%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Singer%2C%20Yoram%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009"
        },
        {
            "id": "8",
            "entry": "[8] John C. Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari. Composite objective mirror descent. In COLT, pages 14\u201326. Omnipress, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Tewari%2C%20Ambuj%20Composite%20objective%20mirror%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Tewari%2C%20Ambuj%20Composite%20objective%20mirror%20descent%202010"
        },
        {
            "id": "9",
            "entry": "[9] Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes back. In NIPS. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Vitaly%20Generalization%20of%20erm%20in%20stochastic%20convex%20optimization%3A%20The%20dimension%20strikes%20back.%20In%20NIPS%202016"
        },
        {
            "id": "10",
            "entry": "[10] Dan Garber, Elad Hazan, Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Faster eigenvector computation via shift-and-invert preconditioning. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garber%2C%20Dan%20Hazan%2C%20Elad%20Jin%2C%20Chi%20Kakade%2C%20Sham%20M.%20Faster%20eigenvector%20computation%20via%20shift-and-invert%20preconditioning%202016"
        },
        {
            "id": "11",
            "entry": "[11] Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning: Implications to exp-concave empirical risk minimization. J. Mach. Learn. Res., 18(1):8245\u2013 8257, January 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gonen%2C%20Alon%20Shalev-Shwartz%2C%20Shai%20Average%20stability%20is%20invariant%20to%20data%20preconditioning%3A%20Implications%20to%20exp-concave%20empirical%20risk%20minimization%208257-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gonen%2C%20Alon%20Shalev-Shwartz%2C%20Shai%20Average%20stability%20is%20invariant%20to%20data%20preconditioning%3A%20Implications%20to%20exp-concave%20empirical%20risk%20minimization%208257-01"
        },
        {
            "id": "12",
            "entry": "[12] Peter D. Gr\u00fcnwald and Nishant A. Mehta. Fast rates with unbounded losses. CoRR, abs/1605.00252, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.00252"
        },
        {
            "id": "13",
            "entry": "[13] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Agarwal%2C%20Amit%20Kale%2C%20Satyen%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007"
        },
        {
            "id": "14",
            "entry": "[14] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In COLT, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Beyond%20the%20regret%20minimization%20barrier%3A%20an%20optimal%20algorithm%20for%20stochastic%20strongly-convex%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Beyond%20the%20regret%20minimization%20barrier%3A%20an%20optimal%20algorithm%20for%20stochastic%20strongly-convex%20optimization%202011"
        },
        {
            "id": "15",
            "entry": "[15] Anatoli Juditsky and Yuri Nesterov. Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stoch. Syst., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Juditsky%2C%20Anatoli%20Nesterov%2C%20Yuri%20Deterministic%20and%20stochastic%20primal-dual%20subgradient%20algorithms%20for%20uniformly%20convex%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Juditsky%2C%20Anatoli%20Nesterov%2C%20Yuri%20Deterministic%20and%20stochastic%20primal-dual%20subgradient%20algorithms%20for%20uniformly%20convex%20minimization%202014"
        },
        {
            "id": "16",
            "entry": "[16] Sham M. Kakade and Ambuj Tewari. On the generalization ability of online strongly convex programming algorithms. In NIPS, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Tewari%2C%20Ambuj%20On%20the%20generalization%20ability%20of%20online%20strongly%20convex%20programming%20algorithms%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Tewari%2C%20Ambuj%20On%20the%20generalization%20ability%20of%20online%20strongly%20convex%20programming%20algorithms%202008"
        },
        {
            "id": "17",
            "entry": "[17] Hamed Karimi, Julie Nutini, and Mark W. Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-\u0142ojasiewicz condition. In ECML-PKDD, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20W.%20Linear%20convergence%20of%20gradient%20and%20proximal-gradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20W.%20Linear%20convergence%20of%20gradient%20and%20proximal-gradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition%202016"
        },
        {
            "id": "18",
            "entry": "[18] Sujin Kim, Raghu Pasupathy, and Shane G. Henderson. A Guide to Sample Average Approximation, pages 207\u2013243. Springer New York, New York, NY, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Sujin%20Pasupathy%2C%20Raghu%20Henderson%2C%20Shane%20G.%20A%20Guide%20to%20Sample%20Average%20Approximation%202015"
        },
        {
            "id": "19",
            "entry": "[19] Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20Vladimir%20Local%20rademacher%20complexities%20and%20oracle%20inequalities%20in%20risk%20minimization%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koltchinskii%2C%20Vladimir%20Local%20rademacher%20complexities%20and%20oracle%20inequalities%20in%20risk%20minimization%202006"
        },
        {
            "id": "20",
            "entry": "[20] Wouter M. Koolen, Peter Gr\u00fcnwald, and Tim van Erven. Combining adversarial guarantees and stochastic fast rates in online learning. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koolen%2C%20Wouter%20M.%20Gr%C3%BCnwald%2C%20Peter%20van%20Erven%2C%20Tim%20Combining%20adversarial%20guarantees%20and%20stochastic%20fast%20rates%20in%20online%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koolen%2C%20Wouter%20M.%20Gr%C3%BCnwald%2C%20Peter%20van%20Erven%2C%20Tim%20Combining%20adversarial%20guarantees%20and%20stochastic%20fast%20rates%20in%20online%20learning%202016"
        },
        {
            "id": "21",
            "entry": "[21] Tomer Koren and Kfir Y. Levy. Fast rates for exp-concave empirical risk minimization. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koren%2C%20Tomer%20Levy%2C%20Kfir%20Y.%20Fast%20rates%20for%20exp-concave%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koren%2C%20Tomer%20Levy%2C%20Kfir%20Y.%20Fast%20rates%20for%20exp-concave%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "22",
            "entry": "[22] Wee Sun Lee, P. L. Bartlett, and R. C. Williamson. The importance of convexity in learning with squared loss. IEEE Transactions on Information Theory, 44(5):1974\u20131980, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wee%20Sun%20Lee%2C%20P.L.Bartlett%20Williamson%2C%20R.C.%20The%20importance%20of%20convexity%20in%20learning%20with%20squared%20loss%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wee%20Sun%20Lee%2C%20P.L.Bartlett%20Williamson%2C%20R.C.%20The%20importance%20of%20convexity%20in%20learning%20with%20squared%20loss%201998"
        },
        {
            "id": "23",
            "entry": "[23] Guoyin Li. Global error bounds for piecewise convex polynomials. Math. Program., 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Guoyin%20Global%20error%20bounds%20for%20piecewise%20convex%20polynomials%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Guoyin%20Global%20error%20bounds%20for%20piecewise%20convex%20polynomials%202013"
        },
        {
            "id": "24",
            "entry": "[24] Guoyin Li and Ting Kei Pong. Calculus of the exponent of kurdyka\u0142ojasiewicz inequality and its applications to linear convergence of first-order methods. CoRR, abs/1602.02915, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02915"
        },
        {
            "id": "25",
            "entry": "[25] Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. Ann. Statist., 27(6):1808\u20131829, 12 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mammen%2C%20Enno%20Tsybakov%2C%20Alexandre%20B.%20Smooth%20discrimination%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mammen%2C%20Enno%20Tsybakov%2C%20Alexandre%20B.%20Smooth%20discrimination%20analysis%201999"
        },
        {
            "id": "26",
            "entry": "[26] Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. In AISTATS, pages \u2013, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mehta%2C%20Nishant%20A.%20Fast%20rates%20with%20high%20probability%20in%20exp-concave%20statistical%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mehta%2C%20Nishant%20A.%20Fast%20rates%20with%20high%20probability%20in%20exp-concave%20statistical%20learning%202017"
        },
        {
            "id": "27",
            "entry": "[27] Nishant A. Mehta and Robert C. Williamson. From stochastic mixability to fast rates. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mehta%2C%20Nishant%20A.%20Williamson%2C%20Robert%20C.%20From%20stochastic%20mixability%20to%20fast%20rates%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mehta%2C%20Nishant%20A.%20Williamson%2C%20Robert%20C.%20From%20stochastic%20mixability%20to%20fast%20rates%202014"
        },
        {
            "id": "28",
            "entry": "[28] I. Necoara, Yu. Nesterov, and F. Glineur. Linear convergence of first order methods for nonstrongly convex optimization. CoRR, abs/1504.06298, v4, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.06298"
        },
        {
            "id": "29",
            "entry": "[29] Arkadi Nemirovski, Anatoli Juditsky, Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "30",
            "entry": "[30] Yurii Nesterov. Introductory lectures on convex optimization: a basic course. 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20a%20basic%20course%202004"
        },
        {
            "id": "31",
            "entry": "[31] Jong-Shi Pang. Error bounds in mathematical programming. Math. Program., 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pang%2C%20Jong-Shi%20Error%20bounds%20in%20mathematical%20programming%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20Jong-Shi%20Error%20bounds%20in%20mathematical%20programming%201997"
        },
        {
            "id": "32",
            "entry": "[32] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In ICML, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012"
        },
        {
            "id": "33",
            "entry": "[33] Aaditya Ramdas and Aarti Singh. Optimal rates for stochastic convex optimization under tsybakov noise condition. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramdas%2C%20Aaditya%20Singh%2C%20Aarti%20Optimal%20rates%20for%20stochastic%20convex%20optimization%20under%20tsybakov%20noise%20condition%202013"
        },
        {
            "id": "34",
            "entry": "[34] R.T. Rockafellar. Convex Analysis. 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.T.%20Convex%20Analysis%201970"
        },
        {
            "id": "35",
            "entry": "[35] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex optimization. In COLT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shai%20ShalevShwartz%20Ohad%20Shamir%20Nathan%20Srebro%20and%20Karthik%20Sridharan%20Stochastic%20convex%20optimization%20In%20COLT%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shai%20ShalevShwartz%20Ohad%20Shamir%20Nathan%20Srebro%20and%20Karthik%20Sridharan%20Stochastic%20convex%20optimization%20In%20COLT%202009"
        },
        {
            "id": "36",
            "entry": "[36] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In ICML, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Srebro%2C%20Nathan%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20svm%202007"
        },
        {
            "id": "37",
            "entry": "[37] Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for l1-regularized loss minimization. Journal of Machine Learning Research, 12:1865\u20131892, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Tewari%2C%20Ambuj%20Stochastic%20methods%20for%20l1-regularized%20loss%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Tewari%2C%20Ambuj%20Stochastic%20methods%20for%20l1-regularized%20loss%20minimization%202011"
        },
        {
            "id": "38",
            "entry": "[38] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Zhang%2C%20Tong%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%3A%20Convergence%20results%20and%20optimal%20averaging%20schemes%202013"
        },
        {
            "id": "39",
            "entry": "[39] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Programming: Modeling and Theory, Second Edition. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapiro%2C%20Alexander%20Dentcheva%2C%20Darinka%20Ruszczynski%2C%20Andrzej%20Lectures%20on%20Stochastic%20Programming%3A%20Modeling%20and%20Theory%2C%20Second%20Edition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shapiro%2C%20Alexander%20Dentcheva%2C%20Darinka%20Ruszczynski%2C%20Andrzej%20Lectures%20on%20Stochastic%20Programming%3A%20Modeling%20and%20Theory%2C%20Second%20Edition%202014"
        },
        {
            "id": "40",
            "entry": "[40] Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their approximations. Constructive Approximation, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smale%2C%20Steve%20Zhou%2C%20Ding-Xuan%20Learning%20theory%20estimates%20via%20integral%20operators%20and%20their%20approximations%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smale%2C%20Steve%20Zhou%2C%20Ding-Xuan%20Learning%20theory%20estimates%20via%20integral%20operators%20and%20their%20approximations%202007"
        },
        {
            "id": "41",
            "entry": "[41] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth loss. ArXiv e-prints, arXiv:1009.3896, 2010.",
            "arxiv_url": "https://arxiv.org/pdf/1009.3896"
        },
        {
            "id": "42",
            "entry": "[42] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In NIPS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srebro%2C%20Nathan%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20Smoothness%2C%20low%20noise%20and%20fast%20rates%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srebro%2C%20Nathan%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20Smoothness%2C%20low%20noise%20and%20fast%20rates%202010"
        },
        {
            "id": "43",
            "entry": "[43] Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. Fast rates for regularized objectives. In NIPS, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sridharan%2C%20Karthik%20Shalev-Shwartz%2C%20Shai%20Srebro%2C%20Nathan%20Fast%20rates%20for%20regularized%20objectives%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sridharan%2C%20Karthik%20Shalev-Shwartz%2C%20Shai%20Srebro%2C%20Nathan%20Fast%20rates%20for%20regularized%20objectives%202008"
        },
        {
            "id": "44",
            "entry": "[44] Alexander B. Tsybakov. Optimal aggregation of classifiers in statistical learning. Ann. Statist., 32(1):135\u2013166, 02 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexander%20B.%20Optimal%20aggregation%20of%20classifiers%20in%20statistical%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsybakov%2C%20Alexander%20B.%20Optimal%20aggregation%20of%20classifiers%20in%20statistical%20learning%202004"
        },
        {
            "id": "45",
            "entry": "[45] Alexander B Tsybakov et al. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135\u2013166, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexander%20B.%20Optimal%20aggregation%20of%20classifiers%20in%20statistical%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsybakov%2C%20Alexander%20B.%20Optimal%20aggregation%20of%20classifiers%20in%20statistical%20learning%202004"
        },
        {
            "id": "46",
            "entry": "[46] Tim van Erven, Peter D. Gr\u00fcnwald, Nishant A. Mehta, Mark D. Reid, and Robert C. Williamson. Fast rates in statistical and online learning. JMLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Erven%2C%20Tim%20Gr%C3%BCnwald%2C%20Peter%20D.%20Mehta%2C%20Nishant%20A.%20Reid%2C%20Mark%20D.%20Fast%20rates%20in%20statistical%20and%20online%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Erven%2C%20Tim%20Gr%C3%BCnwald%2C%20Peter%20D.%20Mehta%2C%20Nishant%20A.%20Reid%2C%20Mark%20D.%20Fast%20rates%20in%20statistical%20and%20online%20learning%202015"
        },
        {
            "id": "47",
            "entry": "[47] Tim van Erven and Wouter M. Koolen. Metagrad: Multiple learning rates in online learning. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Erven%2C%20Tim%20Koolen%2C%20Wouter%20M.%20Metagrad%3A%20Multiple%20learning%20rates%20in%20online%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Erven%2C%20Tim%20Koolen%2C%20Wouter%20M.%20Metagrad%3A%20Multiple%20learning%20rates%20in%20online%20learning%202016"
        },
        {
            "id": "48",
            "entry": "[48] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vladimir%20N%20Vapnik%20Statistical%20Learning%20Theory%20WileyInterscience%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vladimir%20N%20Vapnik%20Statistical%20Learning%20Theory%20WileyInterscience%201998"
        },
        {
            "id": "49",
            "entry": "[49] Yi Xu, Qihang Lin, and Tianbao Yang. Accelerate stochastic subgradient method by leveraging local error bound. CoRR, abs/1607.01027, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.01027"
        },
        {
            "id": "50",
            "entry": "[50] Yi Xu, Qihang Lin, and Tianbao Yang. Stochastic convex optimization: Faster local growth implies faster global convergence. In ICML, pages 3821\u20133830, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Yi%20Lin%2C%20Qihang%20Yang%2C%20Tianbao%20Stochastic%20convex%20optimization%3A%20Faster%20local%20growth%20implies%20faster%20global%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Yi%20Lin%2C%20Qihang%20Yang%2C%20Tianbao%20Stochastic%20convex%20optimization%3A%20Faster%20local%20growth%20implies%20faster%20global%20convergence%202017"
        },
        {
            "id": "51",
            "entry": "[51] Tianbao Yang, Zhe Li, and Lijun Zhang. A simple analysis for exp-concave empirical minimization with arbitrary convex regularizer. In AISTATS, pages 445\u2013453, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Tianbao%20Li%2C%20Zhe%20Zhang%2C%20Lijun%20A%20simple%20analysis%20for%20exp-concave%20empirical%20minimization%20with%20arbitrary%20convex%20regularizer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Tianbao%20Li%2C%20Zhe%20Zhang%2C%20Lijun%20A%20simple%20analysis%20for%20exp-concave%20empirical%20minimization%20with%20arbitrary%20convex%20regularizer%202018"
        },
        {
            "id": "52",
            "entry": "[52] Tianbao Yang and Qihang Lin. Rsg: Beating subgradient method without smoothness and strong convexity. CoRR, abs/1512.03107, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03107"
        },
        {
            "id": "53",
            "entry": "[53] W. H. Yang. Error bounds for convex polynomials. SIAM Journal on Optimization, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20W.H.%20Error%20bounds%20for%20convex%20polynomials%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20W.H.%20Error%20bounds%20for%20convex%20polynomials%202009"
        },
        {
            "id": "54",
            "entry": "[54] Hui Zhang. New analysis of linear convergence of gradient-type methods via unifying error bound conditions. CoRR, abs/1606.00269, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00269"
        },
        {
            "id": "55",
            "entry": "[55] Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochastic convex optimization: O(1/n)-and o(1/n2)-type of risk bounds. CoRR, abs/1702.02030, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02030"
        },
        {
            "id": "56",
            "entry": "[56] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML, pages 928\u2013936, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
