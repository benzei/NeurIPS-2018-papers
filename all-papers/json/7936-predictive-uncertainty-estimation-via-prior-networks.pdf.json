{
    "filename": "7936-predictive-uncertainty-estimation-via-prior-networks.pdf",
    "metadata": {
        "title": "Predictive Uncertainty Estimation via Prior Networks",
        "author": "Andrey Malinin, Mark Gales",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7936-predictive-uncertainty-estimation-via-prior-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models distributional uncertainty. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST and CIFAR-10 datasets, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Neural_Networks"
        },
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "Mutual Information",
            "url": "https://en.wikipedia.org/wiki/Mutual_Information"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "predictive distribution",
            "url": "https://en.wikipedia.org/wiki/predictive_distribution"
        },
        {
            "term": "bayesian method",
            "url": "https://en.wikipedia.org/wiki/bayesian_method"
        }
    ],
    "highlights": [
        "Neural Networks (NNs) have become the dominant approach to addressing computer vision (CV) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], natural language processing (NLP) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], speech recognition (ASR) [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] and bio-informatics (BI) [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] tasks",
        "The first experiment illustrates the advantages of a Dirichlet Prior Network over other non-Bayesian methods [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] on synthetic data and the second set of experiments evaluate Dirichlet Prior Network on MNIST and CIFAR-10 and compares them to DNNs and ensembles generated via Monte-Carlo Dropout (MCDP) on the tasks of misclassification detection and out-of-distribution data detection",
        "This work describes the limitations of previous work on predictive uncertainty estimations within the context of sources of uncertainty and proposes to treat out-of-distribution (OOD) inputs as a separate source of uncertainty, called Distributional Uncertainty",
        "A particular form of these Prior Networks are applied to classification, Dirichlet Prior Networks (DPNs)",
        "Differential entropy of Dirichlet Prior Network was best for measure of uncertainty for OOD detection, especially when classes are less distinct",
        "Uncertainty measures can be analytically calculated at test time for Dirichlet Prior Network, reducing computational cost relative to ensemble approaches"
    ],
    "key_statements": [
        "Neural Networks (NNs) have become the dominant approach to addressing computer vision (CV) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], natural language processing (NLP) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], speech recognition (ASR) [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] and bio-informatics (BI) [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] tasks",
        "This section explores a range of measures for quantifying uncertainty given a trained DNN, Dirichlet Prior Network or Bayesian MC ensemble",
        "The first experiment illustrates the advantages of a Dirichlet Prior Network over other non-Bayesian methods [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] on synthetic data and the second set of experiments evaluate Dirichlet Prior Network on MNIST and CIFAR-10 and compares them to DNNs and ensembles generated via Monte-Carlo Dropout (MCDP) on the tasks of misclassification detection and out-of-distribution data detection",
        "This work describes the limitations of previous work on predictive uncertainty estimations within the context of sources of uncertainty and proposes to treat out-of-distribution (OOD) inputs as a separate source of uncertainty, called Distributional Uncertainty",
        "A particular form of these Prior Networks are applied to classification, Dirichlet Prior Networks (DPNs)",
        "Differential entropy of Dirichlet Prior Network was best for measure of uncertainty for OOD detection, especially when classes are less distinct",
        "Uncertainty measures can be analytically calculated at test time for Dirichlet Prior Network, reducing computational cost relative to ensemble approaches"
    ],
    "summary": [
        "Neural Networks (NNs) have become the dominant approach to addressing computer vision (CV) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], natural language processing (NLP) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], speech recognition (ASR) [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] and bio-informatics (BI) [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] tasks.",
        "Non-Bayesian class of approaches derives measures of uncertainty via the predictive posteriors of regression [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] and classification [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] DNNs. Here, DNNs are explicitly trained [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] to yield high entropy posterior distributions for out-of-distribution inputs.",
        "For an input in a region with high degrees of noise or class overlap a Prior Network should yield a sharp distribution focused on the center of the simplex, which corresponds to being confident in predicting a flat categorical distribution over class labels.",
        "It is necessary to change the cost function to explicitly train a DPN to yield a sharp or flat prior distribution around the expected categorical depending on the input data.",
        "This section explores a range of measures for quantifying uncertainty given a trained DNN, DPN or Bayesian MC ensemble.",
        "The first class derives measures of uncertainty from the expected predictive categorical P(!c|x\u21e4; D), given a full marginalization of eq 4 which can be approximated either with a point estimate of the parameters \u2713\u02c6 or a Bayesian MC ensemble.",
        "Differential entropy is well suited to measuring distributional uncertainty, as it can be low even if the expected categorical under the Dirichlet prior has high entropy, and captures elements of data uncertainty.",
        "The final class of measures uses the full eq 4 and assesses the spread of p(\u03bc|x\u21e4; \u2713) due to model uncertainty via the MI between \u03bc and \u2713, which can be computed via Bayesian ensemble approaches.",
        "The first experiment illustrates the advantages of a DPN over other non-Bayesian methods [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] on synthetic data and the second set of experiments evaluate DPNs on MNIST and CIFAR-10 and compares them to DNNs and ensembles generated via Monte-Carlo Dropout (MCDP) on the tasks of misclassification detection and out-of-distribution data detection.",
        "The two considered baseline approaches derive uncertainty measures from either the class posterior of a DNN [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] or an ensemble generated via MC dropout applied to the same DNN [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "This work presents a novel framework, called Prior Networks (PN), which allows data, distributional and model uncertainty to be treated separately within a consistent probabilistically interpretable framework.",
        "DPNs are shown to yield more accurate estimates of distributional uncertainty than MC Dropout and standard DNNs on the task of OOD detection on the MNIST and CIFAR-10 datasets.",
        "It was noted that measures of total uncertainty, such as max probability or entropy of the predictive distribution, yield the best results on misclassification detection.",
        "It is necessary to explore Prior Networks for regression tasks"
    ],
    "headline": "This paper reports on research partly supported by Cambridge Assessment, University of Cambridge",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ross Girshick, \u201cFast R-CNN,\u201d in Proc. 2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 1440\u20131448.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girshick%2C%20Ross%20Fast%20R-CNN%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girshick%2C%20Ross%20Fast%20R-CNN%2C%202015"
        },
        {
            "id": "2",
            "entry": "[2] Karen Simonyan and Andrew Zisserman, \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition,\u201d in Proc. International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition%2C%202015"
        },
        {
            "id": "3",
            "entry": "[3] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee, \u201cLearning to Generate Long-term Future via Hierarchical Prediction,\u201d in Proc. International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Zou%2C%20Yuliang%20Sohn%2C%20Sungryull%20Learning%20to%20Generate%20Long-term%20Future%20via%20Hierarchical%20Prediction%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Zou%2C%20Yuliang%20Sohn%2C%20Sungryull%20Learning%20to%20Generate%20Long-term%20Future%20via%20Hierarchical%20Prediction%2C%202017"
        },
        {
            "id": "4",
            "entry": "[4] Tomas Mikolov et al., \u201cLinguistic Regularities in Continuous Space Word Representations,\u201d in Proc. NAACL-HLT, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20%E2%80%9CLinguistic%20Regularities%20in%20Continuous%20Space%20Word%20Representations%2C%E2%80%9D%20in%20Proc%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20%E2%80%9CLinguistic%20Regularities%20in%20Continuous%20Space%20Word%20Representations%2C%E2%80%9D%20in%20Proc%202013"
        },
        {
            "id": "5",
            "entry": "[5] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean, \u201cEfficient Estimation of Word Representations in Vector Space,\u201d 2013, arXiv:1301.3781.",
            "arxiv_url": "https://arxiv.org/pdf/1301.3781"
        },
        {
            "id": "6",
            "entry": "[6] Tomas Mikolov, Martin Karafi\u00e1t, Luk\u00e1s Burget, Jan Cernock\u00fd, and Sanjeev Khudanpur, \u201cRecurrent Neural Network Based Language Model,\u201d in Proc. INTERSPEECH, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Karafi%C3%A1t%2C%20Martin%20Burget%2C%20Luk%C3%A1s%20Cernock%C3%BD%2C%20Jan%20Recurrent%20Neural%20Network%20Based%20Language%20Model%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Karafi%C3%A1t%2C%20Martin%20Burget%2C%20Luk%C3%A1s%20Cernock%C3%BD%2C%20Jan%20Recurrent%20Neural%20Network%20Based%20Language%20Model%2C%202010"
        },
        {
            "id": "7",
            "entry": "[7] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition,\u201d Signal Processing Magazine, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%2C%202012"
        },
        {
            "id": "8",
            "entry": "[8] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng, \u201cDeep speech: Scaling up end-to-end speech recognition,\u201d 2014, arXiv:1412.5567.",
            "arxiv_url": "https://arxiv.org/pdf/1412.5567"
        },
        {
            "id": "9",
            "entry": "[9] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad, \u201cIntelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission,\u201d in Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, USA, 2015, KDD \u201915, pp. 1721\u20131730, ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20Rich%20Lou%2C%20Yin%20Gehrke%2C%20Johannes%20Koch%2C%20Paul%20Intelligible%20models%20for%20healthcare%3A%20Predicting%20pneumonia%20risk%20and%20hospital%2030-day%20readmission%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20Rich%20Lou%2C%20Yin%20Gehrke%2C%20Johannes%20Koch%2C%20Paul%20Intelligible%20models%20for%20healthcare%3A%20Predicting%20pneumonia%20risk%20and%20hospital%2030-day%20readmission%2C%202015"
        },
        {
            "id": "10",
            "entry": "[10] Babak Alipanahi, Andrew Delong, Matthew T. Weirauch, and Brendan J. Frey, \u201cPredicting the sequence specificities of DNA-and RNA-binding proteins by deep learning,\u201d Nature Biotechnology, vol. 33, no. 8, pp. 831\u2013838, July 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alipanahi%2C%20Babak%20Delong%2C%20Andrew%20Weirauch%2C%20Matthew%20T.%20Frey%2C%20Brendan%20J.%20Predicting%20the%20sequence%20specificities%20of%20DNA-and%20RNA-binding%20proteins%20by%20deep%20learning%2C%202015-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alipanahi%2C%20Babak%20Delong%2C%20Andrew%20Weirauch%2C%20Matthew%20T.%20Frey%2C%20Brendan%20J.%20Predicting%20the%20sequence%20specificities%20of%20DNA-and%20RNA-binding%20proteins%20by%20deep%20learning%2C%202015-07"
        },
        {
            "id": "11",
            "entry": "[11] B. Lakshminarayanan, A. Pritzel, and C. Blundell, \u201cSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles,\u201d in Proc. Conference on Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20B.%20Pritzel%2C%20A.%20Blundell%2C%20C.%20Simple%20and%20Scalable%20Predictive%20Uncertainty%20Estimation%20using%20Deep%20Ensembles%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshminarayanan%2C%20B.%20Pritzel%2C%20A.%20Blundell%2C%20C.%20Simple%20and%20Scalable%20Predictive%20Uncertainty%20Estimation%20using%20Deep%20Ensembles%2C%202017"
        },
        {
            "id": "12",
            "entry": "[12] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9, \u201cConcrete problems in AI safety,\u201d http://arxiv.org/abs/1606.06565, 2016, arXiv:1606.06565.",
            "url": "http://arxiv.org/abs/1606.06565",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "13",
            "entry": "[13] Dan Hendrycks and Kevin Gimpel, \u201cA Baseline for Detecting Misclassified and Out-ofDistribution Examples in Neural Networks,\u201d http://arxiv.org/abs/1610.02136, 2016, arXiv:1610.02136.",
            "url": "http://arxiv.org/abs/1610.02136",
            "arxiv_url": "https://arxiv.org/pdf/1610.02136"
        },
        {
            "id": "14",
            "entry": "[14] David JC MacKay, \u201cA practical bayesian framework for backpropagation networks,\u201d Neural computation, vol. 4, no. 3, pp. 448\u2013472, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20David%20J.C.%20A%20practical%20bayesian%20framework%20for%20backpropagation%20networks%2C%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacKay%2C%20David%20J.C.%20A%20practical%20bayesian%20framework%20for%20backpropagation%20networks%2C%201992"
        },
        {
            "id": "15",
            "entry": "[15] David JC MacKay, Bayesian methods for adaptive models, Ph.D. thesis, California Institute of Technology, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20David%20J.C.%20Bayesian%20methods%20for%20adaptive%20models%201992"
        },
        {
            "id": "16",
            "entry": "[16] Geoffrey E. Hinton and Drew van Camp, \u201cKeeping the neural networks simple by minimizing the description length of the weights,\u201d in Proc. Sixth Annual Conference on Computational Learning Theory, New York, NY, USA, 1993, COLT \u201993, pp. 5\u201313, ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20van%20Camp%2C%20Drew%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%2C%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20van%20Camp%2C%20Drew%20Keeping%20the%20neural%20networks%20simple%20by%20minimizing%20the%20description%20length%20of%20the%20weights%2C%201993"
        },
        {
            "id": "17",
            "entry": "[17] Radford M. Neal, Bayesian learning for neural networks, Springer Science & Business Media, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Bayesian%20learning%20for%20neural%20networks%201996"
        },
        {
            "id": "18",
            "entry": "[18] Yarin Gal and Zoubin Ghahramani, \u201cDropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,\u201d in Proc. 33rd International Conference on Machine Learning (ICML-16), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20Approximation%3A%20Representing%20Model%20Uncertainty%20in%20Deep%20Learning%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20Approximation%3A%20Representing%20Model%20Uncertainty%20in%20Deep%20Learning%2C%202016"
        },
        {
            "id": "19",
            "entry": "[19] A. Kendall, Y. Gal, and R. Cipolla, \u201cMulti-Task Learning Using Uncertainty to Weight Losses for Scene Geometry and Semantics,\u201d in Proc. Conference on Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20A.%20Gal%2C%20Y.%20Cipolla%2C%20R.%20Multi-Task%20Learning%20Using%20Uncertainty%20to%20Weight%20Losses%20for%20Scene%20Geometry%20and%20Semantics%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20A.%20Gal%2C%20Y.%20Cipolla%2C%20R.%20Multi-Task%20Learning%20Using%20Uncertainty%20to%20Weight%20Losses%20for%20Scene%20Geometry%20and%20Semantics%2C%202017"
        },
        {
            "id": "20",
            "entry": "[20] A. Kendall and Y. Gal, \u201cWhat Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision,\u201d in Proc. Conference on Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20Deep%20Learning%20for%20Computer%20Vision%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20Deep%20Learning%20for%20Computer%20Vision%2C%202017"
        },
        {
            "id": "21",
            "entry": "[21] A. Malinin, A. Ragni, M.J.F. Gales, and K.M. Knill, \u201cIncorporating Uncertainty into Deep Learning for Spoken Language Assessment,\u201d in Proc. 55th Annual Meeting of the Association for Computational Linguistics (ACL), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malinin%2C%20A.%20Ragni%2C%20A.%20Gales%2C%20M.J.F.%20Knill%2C%20K.M.%20Incorporating%20Uncertainty%20into%20Deep%20Learning%20for%20Spoken%20Language%20Assessment%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malinin%2C%20A.%20Ragni%2C%20A.%20Gales%2C%20M.J.F.%20Knill%2C%20K.M.%20Incorporating%20Uncertainty%20into%20Deep%20Learning%20for%20Spoken%20Language%20Assessment%2C%202017"
        },
        {
            "id": "22",
            "entry": "[22] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin, \u201cTraining confidence-calibrated classifiers for detecting out-of-distribution samples,\u201d International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Kimin%20Lee%2C%20Honglak%20Lee%2C%20Kibok%20Shin%2C%20Jinwoo%20Training%20confidence-calibrated%20classifiers%20for%20detecting%20out-of-distribution%20samples%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Kimin%20Lee%2C%20Honglak%20Lee%2C%20Kibok%20Shin%2C%20Jinwoo%20Training%20confidence-calibrated%20classifiers%20for%20detecting%20out-of-distribution%20samples%2C%202018"
        },
        {
            "id": "23",
            "entry": "[23] Yarin Gal, Uncertainty in Deep Learning, Ph.D. thesis, University of Cambridge, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Uncertainty%20in%20Deep%20Learning%202016"
        },
        {
            "id": "24",
            "entry": "[24] Joaquin Qui\u00f1onero-Candela, Dataset Shift in Machine Learning, The MIT Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joaquin%20Qui%C3%B1oneroCandela%20Dataset%20Shift%20in%20Machine%20Learning%20The%20MIT%20Press%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joaquin%20Qui%C3%B1oneroCandela%20Dataset%20Shift%20in%20Machine%20Learning%20The%20MIT%20Press%202009"
        },
        {
            "id": "25",
            "entry": "[25] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra, \u201cWeight Uncertainty in Neural Networks,\u201d in Proc. International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20Uncertainty%20in%20Neural%20Networks%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20Uncertainty%20in%20Neural%20Networks%2C%202015"
        },
        {
            "id": "26",
            "entry": "[26] Alex Graves, \u201cPractical variational inference for neural networks,\u201d in Advances in neural information processing systems, 2011, pp. 2348\u20132356.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20%E2%80%9CPractical%20variational%20inference%20for%20neural%20networks%2C%E2%80%9D%20in%20Advances%20in%20neural%20information%20processing%20systems%202011"
        },
        {
            "id": "27",
            "entry": "[27] Christos Louizos and Max Welling, \u201cStructured and efficient variational deep learning with matrix gaussian posteriors,\u201d in International Conference on Machine Learning, 2016, pp. 1708\u20131716.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20gaussian%20posteriors%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20gaussian%20posteriors%2C%202016"
        },
        {
            "id": "28",
            "entry": "[28] Diederik P Kingma, Tim Salimans, and Max Welling, \u201cVariational dropout and the local reparameterization trick,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 2575\u20132583.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%2C%202015"
        },
        {
            "id": "29",
            "entry": "[29] Max Welling and Yee Whye Teh, \u201cBayesian Learning via Stochastic Gradient Langevin Dynamics,\u201d in Proc. International Conference on Machine Learning (ICML), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20Max%20Teh%2C%20Yee%20Whye%20Bayesian%20Learning%20via%20Stochastic%20Gradient%20Langevin%20Dynamics%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20Max%20Teh%2C%20Yee%20Whye%20Bayesian%20Learning%20via%20Stochastic%20Gradient%20Langevin%20Dynamics%2C%202011"
        },
        {
            "id": "30",
            "entry": "[30] Shiyu Liang, Yixuan Li, and R. Srikant, \u201cEnhancing the reliability of out-of-distribution image detection in neural networks,\u201d in Proc. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Shiyu%20Li%2C%20Yixuan%20Srikant%2C%20R.%20Enhancing%20the%20reliability%20of%20out-of-distribution%20image%20detection%20in%20neural%20networks%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Shiyu%20Li%2C%20Yixuan%20Srikant%2C%20R.%20Enhancing%20the%20reliability%20of%20out-of-distribution%20image%20detection%20in%20neural%20networks%2C%202018"
        },
        {
            "id": "31",
            "entry": "[31] David M. Blei, Andrew Y. Ng, and Michael I. Jordan, \u201cLatent Dirichlet Allocation,\u201d Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, Mar. 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20Dirichlet%20Allocation%2C%202003-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20Dirichlet%20Allocation%2C%202003-03"
        },
        {
            "id": "32",
            "entry": "[32] Kevin P. Murphy, Machine Learning, The MIT Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kevin%20P%20Murphy%20Machine%20Learning%20The%20MIT%20Press%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kevin%20P%20Murphy%20Machine%20Learning%20The%20MIT%20Press%202012"
        },
        {
            "id": "33",
            "entry": "[33] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, \u201cDistilling the knowledge in a neural network,\u201d 2015, arXiv:1503.02531.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "34",
            "entry": "[34] Stefan Depeweg, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Finale Doshi-Velez, and Steffen Udluft, \u201cDecomposition of uncertainty for active learning and reliable reinforcement learning in stochastic systems,\u201d arXiv preprint arXiv:1710.07283, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07283"
        },
        {
            "id": "35",
            "entry": "[35] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%2C%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%2C%201998"
        },
        {
            "id": "36",
            "entry": "[36] Alex Krizhevsky, \u201cLearning multiple layers of features from tiny images,\u201d 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%2C%202009"
        },
        {
            "id": "37",
            "entry": "[37] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum, \u201cHuman-level concept learning through probabilistic program induction,\u201d Science, vol. 350, no. 6266, pp. 1332\u20131338, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%2C%202015"
        },
        {
            "id": "38",
            "entry": "[38] Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay D. Shet, \u201cMultidigit number recognition from street view imagery using deep convolutional neural networks,\u201d 2013, arXiv:1312.6082.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6082"
        },
        {
            "id": "39",
            "entry": "[39] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao, \u201cLSUN: construction of a large-scale image dataset using deep learning with humans in the loop,\u201d 2015, arXiv:1506.03365.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        },
        {
            "id": "40",
            "entry": "[40] Stanford CS231N, \u201cTiny ImageNet,\u201d https://tiny-imagenet.herokuapp.com/, 2017.",
            "url": "https://tiny-imagenet.herokuapp.com/"
        },
        {
            "id": "41",
            "entry": "[41] M Buscema, \u201cMetanet: The theory of independent judges,\u201d Substance Use & Misuse, vol. 33, no. 2, pp. 439\u2013461, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buscema%2C%20M.%20Metanet%3A%20The%20theory%20of%20independent%20judges%2C%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buscema%2C%20M.%20Metanet%3A%20The%20theory%20of%20independent%20judges%2C%201998"
        },
        {
            "id": "42",
            "entry": "[42] Mart\u00edn Abadi et al., \u201cTensorFlow: Large-Scale Machine Learning on Heterogeneous Systems,\u201d 2015, Software available from tensorflow.org.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20TensorFlow%3A%20Large-Scale%20Machine%20Learning%20on%20Heterogeneous%20Systems%2C",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20TensorFlow%3A%20Large-Scale%20Machine%20Learning%20on%20Heterogeneous%20Systems%2C"
        },
        {
            "id": "43",
            "entry": "[43] Timothy Dozat, \u201cIncorporating Nesterov Momentum into Adam,\u201d in Proc. International Conference on Learning Representations (ICLR), 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dozat%2C%20Timothy%20Incorporating%20Nesterov%20Momentum%20into%20Adam%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dozat%2C%20Timothy%20Incorporating%20Nesterov%20Momentum%20into%20Adam%2C%202016"
        }
    ]
}
