{
    "filename": "7580-evidential-deep-learning-to-quantify-classification-uncertainty.pdf",
    "metadata": {
        "title": "Evidential Deep Learning to Quantify Classification Uncertainty",
        "author": "Murat Sensoy, Lance Kaplan, Melih Kandemir",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7580-evidential-deep-learning-to-quantify-classification-uncertainty.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of outof-distribution queries and endurance against adversarial perturbations."
    },
    "keywords": [
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "Gaussian Processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_Processes"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "predictive distribution",
            "url": "https://en.wikipedia.org/wiki/predictive_distribution"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "dirichlet distribution",
            "url": "https://en.wikipedia.org/wiki/dirichlet_distribution"
        },
        {
            "term": "Maximum Likelihood Estimation",
            "url": "https://en.wikipedia.org/wiki/Maximum_Likelihood_Estimation"
        },
        {
            "term": "bayesian neural network",
            "url": "https://en.wikipedia.org/wiki/bayesian_neural_network"
        },
        {
            "term": "Subjective Logic",
            "url": "https://en.wikipedia.org/wiki/Subjective_Logic"
        },
        {
            "term": "Variational Bayes",
            "url": "https://en.wikipedia.org/wiki/Variational_Bayes"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "The present decade has commenced with the deep learning approach shaking the machine learning world [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation",
        "We argue that a neural network is capable of forming opinions for classification tasks as Dirichlet distributions",
        "We compared the following approaches: (a) L2 corresponds to the standard deterministic neural nets with softmax output and weight decay, (b) Dropout refers to the uncertainty estimation model used in [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], (c) Deep Ensemble refers to the model used in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], (d) FFG refers to\n1The implementation and a demo application of our model is available under https://muratsensoy.github.io/uncertainty.html",
        "We fit this predictive distribution to data by minimizing the Bayes risk with respect to the L2-Norm loss which is regularized by an information-theoretic complexity term",
        "Our predictor improves the state of the art significantly in two uncertainty modeling benchmarks: i) detection of out-of-distribution queries, and ii) endurance against adversarial perturbations"
    ],
    "key_statements": [
        "The present decade has commenced with the deep learning approach shaking the machine learning world [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "To Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic",
        "By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data",
        "The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net",
        "We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation",
        "The present decade has commenced with the deep learning approach shaking the machine learning world [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "While one side of the coin is a boost of interest and investment on deep learning research, the other is an emergent need for its robustness, sample efficiency, security, and interpretability",
        "In a set of experiments, we demonstrate that this technique outperforms state-of-the-art Bayesian Neural Nets by a large margin on two applications where high-quality uncertainty modeling is of critical importance",
        "While the classification probability computed using the softmax function is quite high for the misclassified samples, our approach proposed in this paper can accurately quantify uncertainty of its predictions.\n3 Uncertainty and the Theory of Evidence",
        "The Dirichlet parameter corresponding to this class should be incremented. This implies that the parameters of a Dirichlet distribution for the classification of a sample may account for the evidence for each class",
        "We argue that a neural network is capable of forming opinions for classification tasks as Dirichlet distributions",
        "Let us assume that \u03b1i = \u03b1i1, . . . , \u03b1iK is the parameters of a Dirichlet distribution for the classification of a sample i, is the total evidence estimated by the network for the assignment of the sample i to the jth class",
        "When counter examples are observed during training (e.g., a digit six with the same circular pattern), the parameters of the neural network should be tuned by back propagation to generate smaller amounts of evidence for this pattern and minimize the loss of these samples, as long as the overall loss decreases",
        "For the sake of commensurability, we evaluate our method following the experimental setup studied by Louizos et al [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We compared the following approaches: (a) L2 corresponds to the standard deterministic neural nets with softmax output and weight decay, (b) Dropout refers to the uncertainty estimation model used in [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], (c) Deep Ensemble refers to the model used in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], (d) FFG refers to\n1The implementation and a demo application of our model is available under https://muratsensoy.github.io/uncertainty.html",
        "The Bayesian neural net used in [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] with the additive parametrization [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], (e) MNFG refers to the structured variational inference method used in [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], (f) EDL is the method we propose",
        "On the left panel of Figure 3, we show the empirical CDFs over the range of possible entropies [0, log(10)] for all models trained with MNIST dataset",
        "We have studied the setup suggested in [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], which uses a subset of the classes in CIFAR10 for training and the rest for out-of-distribution uncertainty testing",
        "We fit this predictive distribution to data by minimizing the Bayes risk with respect to the L2-Norm loss which is regularized by an information-theoretic complexity term",
        "Our predictor improves the state of the art significantly in two uncertainty modeling benchmarks: i) detection of out-of-distribution queries, and ii) endurance against adversarial perturbations"
    ],
    "summary": [
        "The present decade has commenced with the deep learning approach shaking the machine learning world [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net.",
        "While the classification probability computed using the softmax function is quite high for the misclassified samples, our approach proposed in this paper can accurately quantify uncertainty of its predictions.",
        "This implies that the parameters of a Dirichlet distribution for the classification of a sample may account for the evidence for each class.",
        ", \u03b1iK is the parameters of a Dirichlet distribution for the classification of a sample i, is the total evidence estimated by the network for the assignment of the sample i to the jth class.",
        "In this paper, we design and train neural networks to form their multinomial opinions for the classification of a given sample i as a Dirichlet distribution D, where pi is a simplex representing class assignment probabilities.",
        "By decomposing the first and second moments, the loss aims to achieve the joint goals of minimizing the prediction error and the variance of the Dirichlet experiment generated by the neural net specifically for each sample in the training set.",
        "The model may discover patterns in the data and generate evidence for specific class labels based on these patterns to minimize the overall loss.",
        "When counter examples are observed during training (e.g., a digit six with the same circular pattern), the parameters of the neural network should be tuned by back propagation to generate smaller amounts of evidence for this pattern and minimize the loss of these samples, as long as the overall loss decreases.",
        "We compared the following approaches: (a) L2 corresponds to the standard deterministic neural nets with softmax output and weight decay, (b) Dropout refers to the uncertainty estimation model used in [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], (c) Deep Ensemble refers to the model used in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], (d) FFG refers to",
        "We bypass inferring sources of uncertainty on the predictor and directly model a Dirichlet posterior by learning its hyperparameters from data via a deterministic neural net.",
        "We design a predictive distribution for classification by placing a Dirichlet distribution on the class probabilities and assigning neural network outputs to its parameters.",
        "The resultant predictor is a Dirichlet distribution on class probabilities, which provides a more detailed uncertainty model than the point estimate of the standard softmax-output deep nets.",
        "Our predictor improves the state of the art significantly in two uncertainty modeling benchmarks: i) detection of out-of-distribution queries, and ii) endurance against adversarial perturbations"
    ],
    "headline": "To Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wiestra. Weight uncertainty in neural networks. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20Blundell%20J%20Cornebise%20K%20Kavukcuoglu%20and%20D%20Wiestra%20Weight%20uncertainty%20in%20neural%20networks%20In%20ICML%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20Blundell%20J%20Cornebise%20K%20Kavukcuoglu%20and%20D%20Wiestra%20Weight%20uncertainty%20in%20neural%20networks%20In%20ICML%202015"
        },
        {
            "id": "3",
            "entry": "[3] T. Chen, E. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202017"
        },
        {
            "id": "4",
            "entry": "[4] D. Ciresan, A. Giusti, L.M. Gambardella, and J. Schmidhuber. Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciresan%2C%20D.%20Giusti%2C%20A.%20Gambardella%2C%20L.M.%20Schmidhuber%2C%20J.%20Deep%20neural%20networks%20segment%20neuronal%20membranes%20in%20electron%20microscopy%20images%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciresan%2C%20D.%20Giusti%2C%20A.%20Gambardella%2C%20L.M.%20Schmidhuber%2C%20J.%20Deep%20neural%20networks%20segment%20neuronal%20membranes%20in%20electron%20microscopy%20images%202012"
        },
        {
            "id": "5",
            "entry": "[5] D. C. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. Multi-column deep neural network for traffic sign classification. Neural Networks, 32:333\u2013338, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciresan%2C%20D.C.%20Meier%2C%20U.%20Masci%2C%20J.%20Schmidhuber%2C%20J.%20Multi-column%20deep%20neural%20network%20for%20traffic%20sign%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciresan%2C%20D.C.%20Meier%2C%20U.%20Masci%2C%20J.%20Schmidhuber%2C%20J.%20Multi-column%20deep%20neural%20network%20for%20traffic%20sign%20classification%202012"
        },
        {
            "id": "6",
            "entry": "[6] M. Welling D. Kingma, T. Salimans. Variational dropout and the local reparameterization trick. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20M.Welling%20D.%20Salimans%2C%20T.%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20M.Welling%20D.%20Salimans%2C%20T.%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "8",
            "entry": "[8] Y. Gal and Z. Ghahramani. Bayesian convolutional neural networks with bernoulli approximate variational inference. ICLR Workshops, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20Bayesian%20convolutional%20neural%20networks%20with%20bernoulli%20approximate%20variational%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20Bayesian%20convolutional%20neural%20networks%20with%20bernoulli%20approximate%20variational%20inference%202016"
        },
        {
            "id": "9",
            "entry": "[9] Y. Gal and Z. Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "10",
            "entry": "[10] I.J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "11",
            "entry": "[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "12",
            "entry": "[12] N. Houlsby, F. Huszar, Z. Ghahramani, and J.M. Hern\u00e1ndez-Lobato. Collaborative Gaussian processes for preference learning. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houlsby%2C%20N.%20Huszar%2C%20F.%20Ghahramani%2C%20Z.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Collaborative%20Gaussian%20processes%20for%20preference%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Houlsby%2C%20N.%20Huszar%2C%20F.%20Ghahramani%2C%20Z.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Collaborative%20Gaussian%20processes%20for%20preference%20learning%202012"
        },
        {
            "id": "13",
            "entry": "[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "14",
            "entry": "[14] A. Josang. Subjective Logic: A Formalism for Reasoning Under Uncertainty. Springer, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Josang%2C%20A.%20Subjective%20Logic%3A%20A%20Formalism%20for%20Reasoning%20Under%20Uncertainty%202016"
        },
        {
            "id": "15",
            "entry": "[15] M. Kandemir. Asymmetric transfer learning with deep Gaussian processes. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kandemir%2C%20M.%20Asymmetric%20transfer%20learning%20with%20deep%20Gaussian%20processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kandemir%2C%20M.%20Asymmetric%20transfer%20learning%20with%20deep%20Gaussian%20processes%202015"
        },
        {
            "id": "16",
            "entry": "[16] A. Kendall and Y. Gal. What uncertainties do we need in Bayesian deep learning for computer vision? In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20uncertainties%20do%20we%20need%20in%20Bayesian%20deep%20learning%20for%20computer%20vision%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20uncertainties%20do%20we%20need%20in%20Bayesian%20deep%20learning%20for%20computer%20vision%3F%202017"
        },
        {
            "id": "17",
            "entry": "[17] D.P. Kingma and J. Ba. Adam: A method for stochastic optimisation. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimisation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimisation%202015"
        },
        {
            "id": "18",
            "entry": "[18] D.P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparameterization trick. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Welling%2C%20M.%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Welling%2C%20M.%20Variational%20dropout%20and%20the%20local%20reparameterization%20trick%202015"
        },
        {
            "id": "19",
            "entry": "[19] S. Kotz, N. Balakrishnan, and N.L. Johnson. Continuous Multivariate Distributions, volume 1. Wiley, New York, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kotz%2C%20S.%20Balakrishnan%2C%20N.%20Johnson%2C%20N.L.%20Continuous%20Multivariate%20Distributions%2C%20volume%201%202000"
        },
        {
            "id": "20",
            "entry": "[20] A. Krizhevsky, I. Sutskever, and G.E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "21",
            "entry": "[21] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20B.%20Pritzel%2C%20A.%20Blundell%2C%20C.%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshminarayanan%2C%20B.%20Pritzel%2C%20A.%20Blundell%2C%20C.%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017"
        },
        {
            "id": "22",
            "entry": "[22] Y. LeCun, P. Haffner, L. Bottou, and Y. Bengio. Object recognition with gradient-based learning. In Shape, Contour and Grouping in Computer Vision, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Haffner%2C%20P.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Object%20recognition%20with%20gradient-based%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Haffner%2C%20P.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Object%20recognition%20with%20gradient-based%20learning%201999"
        },
        {
            "id": "23",
            "entry": "[23] Y. Li and Y. Gal. Dropout inference in Bayesian neural networks with alpha-divergences. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Gal%2C%20Y.%20Dropout%20inference%20in%20Bayesian%20neural%20networks%20with%20alpha-divergences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Gal%2C%20Y.%20Dropout%20inference%20in%20Bayesian%20neural%20networks%20with%20alpha-divergences%202017"
        },
        {
            "id": "24",
            "entry": "[24] C. Louizos and M. Welling. Multiplicative normalizing flows for variational bayesian neural networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20C.%20Welling%2C%20M.%20Multiplicative%20normalizing%20flows%20for%20variational%20bayesian%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20C.%20Welling%2C%20M.%20Multiplicative%20normalizing%20flows%20for%20variational%20bayesian%20neural%20networks%202017"
        },
        {
            "id": "25",
            "entry": "[25] D.J. MacKay. Probable networks and plausible predictions \u2013 a review of practical Bayesian methods for supervised neural networks. Network: Computation in Neural Systems, 6(3):469\u2013 505, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20D.J.%20Probable%20networks%20and%20plausible%20predictions%20%E2%80%93%20a%20review%20of%20practical%20Bayesian%20methods%20for%20supervised%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacKay%2C%20D.J.%20Probable%20networks%20and%20plausible%20predictions%20%E2%80%93%20a%20review%20of%20practical%20Bayesian%20methods%20for%20supervised%20neural%20networks%201995"
        },
        {
            "id": "26",
            "entry": "[26] D. Molchanov, A. Ashukha, and D. Vetrov. Variational dropout sparsifies deep neural networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molchanov%2C%20D.%20Ashukha%2C%20A.%20Vetrov%2C%20D.%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Molchanov%2C%20D.%20Ashukha%2C%20A.%20Vetrov%2C%20D.%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017"
        },
        {
            "id": "27",
            "entry": "[27] D. Molchanov, A. Ashukha, and D. Vetrov. Variational dropout sparsifies deep neural networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Molchanov%2C%20D.%20Ashukha%2C%20A.%20Vetrov%2C%20D.%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Molchanov%2C%20D.%20Ashukha%2C%20A.%20Vetrov%2C%20D.%20Variational%20dropout%20sparsifies%20deep%20neural%20networks%202017"
        },
        {
            "id": "28",
            "entry": "[28] N. Papernot, N. Carlini, I. Goodfellow, R. Feinman, F. Faghri, A. Matyasko, K. Hambardzumyan, Y.-L. Juang, A. Kurakin, R. Sheatsley, et al. Cleverhans v2. 0.0: An adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.00768"
        },
        {
            "id": "29",
            "entry": "[29] C.E. Rasmussen and C.I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "30",
            "entry": "[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929\u20131958, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202016"
        },
        {
            "id": "31",
            "entry": "[31] D. Tran, R. Ranganath, and D. Blei. The variational Gaussian process. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20D.%20Ranganath%2C%20R.%20Blei%2C%20D.%20The%20variational%20Gaussian%20process%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20D.%20Ranganath%2C%20R.%20Blei%2C%20D.%20The%20variational%20Gaussian%20process%202016"
        },
        {
            "id": "32",
            "entry": "[32] A.G. Wilson. Deep kernel learning. In AISTATS, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.G.%20Deep%20kernel%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.G.%20Deep%20kernel%20learning%202016"
        }
    ]
}
