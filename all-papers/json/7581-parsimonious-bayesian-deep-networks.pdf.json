{
    "filename": "7581-parsimonious-bayesian-deep-networks.pdf",
    "metadata": {
        "title": "Parsimonious Bayesian deep networks",
        "author": "Mingyuan Zhou",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7581-parsimonious-bayesian-deep-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Combining Bayesian nonparametrics and a forward model selection strategy, we construct parsimonious Bayesian deep networks (PBDNs) that infer capacityregularized network architectures from the data and require neither cross-validation nor fine-tuning when training the model. One of the two essential components of a PBDN is the development of a special infinite-wide single-hidden-layer neural network, whose number of active hidden units can be inferred from the data. The other one is the construction of a greedy layer-wise learning algorithm that uses a forward model selection criterion to determine when to stop adding another hidden layer. We develop both Gibbs sampling and stochastic gradient descent based maximum a posteriori inference for PBDNs, providing state-of-the-art classification accuracy and interpretable data subtypes near the decision boundaries, while maintaining low computational complexity for out-of-sample prediction."
    },
    "keywords": [
        {
            "term": "multilayer perceptron",
            "url": "https://en.wikipedia.org/wiki/multilayer_perceptron"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "radial basis function",
            "url": "https://en.wikipedia.org/wiki/radial_basis_function"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "relevance vector machine",
            "url": "https://en.wikipedia.org/wiki/relevance_vector_machine"
        },
        {
            "term": "maximum a posteriori",
            "url": "https://en.wikipedia.org/wiki/maximum_a_posteriori"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        },
        {
            "term": "iSHM",
            "url": "https://en.wikipedia.org/wiki/iSHM"
        }
    ],
    "highlights": [
        "To separate two linearly separable classes, a simple linear classifier such logistic regression will often suffice, in which scenario adding the capability to model nonlinearity not only complicates the model and increases computation, but often harms rather improves the performance by increasing the risk of overfitting",
        "While PBDN1 can partially remedy their sensitivity to how the data are labeled by combining the results obtained under two opposite labeling settings, the decision boundaries of the two infinite support hyperplane machine and those of both adaptive multi-hyperplane machine and convex polytope machine are still restricted to a confined space related to a single convex polytope, which may be used to explain why on banana, image, and ijcnn1, they all clearly underperform a parsimonious Bayesian deep networks with more than one hidden layer",
        "The infinite support hyperplane machine, which interacts countably infinite non-negative weighted hyperplanes via a noisy-OR mechanism, is employed as the building unit to greedily construct a capacity-regularized parsimonious Bayesian deep network (PBDN). infinite support hyperplane machine has an inductive bias in fitting the positively labeled data, and employs the gamma process to infer a parsimonious set of active hyperplanes to enclose negatively labeled data within a convex-polytope bounded space",
        "The sequentially trained infinite support hyperplane machine pairs can be stacked into parsimonious Bayesian deep networks, a feedforward deep network that gradually enhances its modeling capacity as the network depth increases, achieving high accuracy while having low computational complexity for out-of-sample prediction",
        "parsimonious Bayesian deep networks can be trained using either Gibbs sampling that is suitable for quantifying posterior uncertainty, or SGD based maximum a posteriori inference that is scalable to big data",
        "One may potentially construct parsimonious Bayesian deep networks for regression analysis of count, categorical, and continuous response variables by following the same three-step strategy: constructing a nonparametric Bayesian model that infers the number of components for the task of interest, greedily adding layers one at a time, and using a forward model selection criterion to decide how deep is deep enough"
    ],
    "key_statements": [
        "To separate two linearly separable classes, a simple linear classifier such logistic regression will often suffice, in which scenario adding the capability to model nonlinearity not only complicates the model and increases computation, but often harms rather improves the performance by increasing the risk of overfitting",
        "Rather than making an uneasy choice in the first place between a linear classifier, which has fast computation and resists overfitting but may not provide sufficient class separation, and an overcapacitized model, which often wastes computation and requires careful regularization to prevent overfitting, we propose a parsimonious Bayesian deep network (PBDN) that builds its capacity regularization into the greedy-layer-wise construction and training of the deep network",
        "To shrink the width of a hidden layer, we propose the use of a gamma process [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], a draw from which consists of countably infinite atoms, each of which is used to represent a hyperplane in the covariate space",
        "Our intuition for why parsimonious Bayesian deep networks, constructed in this greedy-layer-wise manner, works well is that for two infinite support hyperplane machine trained on the same covariate space under two opposite labeling settings, one infinite support hyperplane machine places enough hyperplanes to define the complement of a convex polytope to sufficiently activate all data labeled as \u201c1,\u201d while the other does so for all data labeled as \u201c0.\u201d for any xi, at least one pik would be sufficiently activated, in other words, xi would be sufficiently close to at least one of the active hyperplanes of the infinite support hyperplane machine pair",
        "While PBDN1 can partially remedy their sensitivity to how the data are labeled by combining the results obtained under two opposite labeling settings, the decision boundaries of the two infinite support hyperplane machine and those of both adaptive multi-hyperplane machine and convex polytope machine are still restricted to a confined space related to a single convex polytope, which may be used to explain why on banana, image, and ijcnn1, they all clearly underperform a parsimonious Bayesian deep networks with more than one hidden layer",
        "Using logistic regression with a single hyperplane for reference, we summarize the computation complexity in Tab. 2, which indicates that in comparison to support vector machine that consistently requires the most number of support vectors, parsimonious Bayesian deep networks often requires significantly less time for predicting the class label of a new data sample",
        "The infinite support hyperplane machine, which interacts countably infinite non-negative weighted hyperplanes via a noisy-OR mechanism, is employed as the building unit to greedily construct a capacity-regularized parsimonious Bayesian deep network (PBDN). infinite support hyperplane machine has an inductive bias in fitting the positively labeled data, and employs the gamma process to infer a parsimonious set of active hyperplanes to enclose negatively labeled data within a convex-polytope bounded space",
        "Due to the inductive bias and label asymmetry, infinite support hyperplane machine are trained in pairs to ensure a sufficient coverage of the covariate space occupied by the data from both classes",
        "The sequentially trained infinite support hyperplane machine pairs can be stacked into parsimonious Bayesian deep networks, a feedforward deep network that gradually enhances its modeling capacity as the network depth increases, achieving high accuracy while having low computational complexity for out-of-sample prediction",
        "parsimonious Bayesian deep networks can be trained using either Gibbs sampling that is suitable for quantifying posterior uncertainty, or SGD based maximum a posteriori inference that is scalable to big data",
        "One may potentially construct parsimonious Bayesian deep networks for regression analysis of count, categorical, and continuous response variables by following the same three-step strategy: constructing a nonparametric Bayesian model that infers the number of components for the task of interest, greedily adding layers one at a time, and using a forward model selection criterion to decide how deep is deep enough"
    ],
    "summary": [
        "To separate two linearly separable classes, a simple linear classifier such logistic regression will often suffice, in which scenario adding the capability to model nonlinearity not only complicates the model and increases computation, but often harms rather improves the performance by increasing the risk of overfitting.",
        "Our intuition for why PBDN, constructed in this greedy-layer-wise manner, works well is that for two iSHMs trained on the same covariate space under two opposite labeling settings, one iSHM places enough hyperplanes to define the complement of a convex polytope to sufficiently activate all data labeled as \u201c1,\u201d while the other does so for all data labeled as \u201c0.\u201d for any xi, at least one pik would be sufficiently activated, in other words, xi would be sufficiently close to at least one of the active hyperplanes of the iSHM pair.",
        "We provide comprehensive comparison on eight widely used benchmark datasets between the proposed PBDNs and a variety of algorithms, including logistic regression, Gaussian radial basis function (RBF) kernel support vector machine (SVM), relevance vector machine (RVM) [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>], adaptive multi-hyperplane machine (AMM) [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], convex polytope machine (CPM) [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], and the deep neural network (DNN) classifier (DNNClassifier) provided in Tensorflow [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>].",
        "While PBDN1 can partially remedy their sensitivity to how the data are labeled by combining the results obtained under two opposite labeling settings, the decision boundaries of the two iSHMs and those of both AMM and CPM are still restricted to a confined space related to a single convex polytope, which may be used to explain why on banana, image, and ijcnn1, they all clearly underperform a PBDN with more than one hidden layer.",
        "As shown in Tab. 2, DNN (8-4) clearly underperforms DNN (32-16) in terms of classification accuracy on both image and ijcnn1, indicating that having 8 and 4 hidden units for the first and second hidden layers, respectively, is far from enough for DNN to provide a sufficiently high nonlinear modeling capacity for these two datasets.",
        "Using logistic regression with a single hyperplane for reference, we summarize the computation complexity in Tab. 2, which indicates that in comparison to SVM that consistently requires the most number of support vectors, PBDN often requires significantly less time for predicting the class label of a new data sample.",
        "The infinite support hyperplane machine, which interacts countably infinite non-negative weighted hyperplanes via a noisy-OR mechanism, is employed as the building unit to greedily construct a capacity-regularized parsimonious Bayesian deep network (PBDN).",
        "ISHM has an inductive bias in fitting the positively labeled data, and employs the gamma process to infer a parsimonious set of active hyperplanes to enclose negatively labeled data within a convex-polytope bounded space.",
        "The sequentially trained iSHM pairs can be stacked into PBDN, a feedforward deep network that gradually enhances its modeling capacity as the network depth increases, achieving high accuracy while having low computational complexity for out-of-sample prediction.",
        "The recently proposed Lomax distribution based racing framework [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] could be a promising candidate for both categorical and non-negative response variables, and Dirichlet process mixtures of generalized linear models [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] could be promising candidates for continuous response variables and many other types of variables via appropriate link functions"
    ],
    "headline": "We develop both Gibbs sampling and stochastic gradient descent based maximum a posteriori inference for parsimonious Bayesian deep networks, providing state-of-the-art classification accuracy and interpretable data subtypes near the decision boundaries, while maintaining low computational complexity for out-of-sample prediction",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] V. Vapnik, Statistical learning theory. Wiley New York, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20V.%20Statistical%20learning%20theory%201998"
        },
        {
            "id": "2",
            "entry": "[2] B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, Advances in Kernel Methods: Support Vector Learning. MIT Press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20B.%20Burges%2C%20C.J.C.%20Smola%2C%20A.J.%20Advances%20in%20Kernel%20Methods%3A%20Support%20Vector%20Learning%201999"
        },
        {
            "id": "3",
            "entry": "[3] G. Hinton, S. Osindero, and Y.-W. Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.%20Osindero%2C%20S.%20Teh%2C%20Y.-W.%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%2C%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.%20Osindero%2C%20S.%20Teh%2C%20Y.-W.%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%2C%202006"
        },
        {
            "id": "4",
            "entry": "[4] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%2C%202015"
        },
        {
            "id": "5",
            "entry": "[5] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "6",
            "entry": "[6] I. Steinwart, \u201cSparseness of support vector machines,\u201d J. Mach. Learn. Res., vol. 4, pp. 1071\u20131105, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20I.%20Sparseness%20of%20support%20vector%20machines%2C%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20I.%20Sparseness%20of%20support%20vector%20machines%2C%202003"
        },
        {
            "id": "7",
            "entry": "[7] C. Bucilua, R. Caruana, and A. Niculescu-Mizil, \u201cModel compression,\u201d in KDD, pp. 535\u2013541, ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bucilua%2C%20C.%20Caruana%2C%20R.%20Niculescu-Mizil%2C%20A.%20%E2%80%9CModel%20compression%2C%E2%80%9D%20in%20KDD%202006"
        },
        {
            "id": "8",
            "entry": "[8] Y. Gong, L. Liu, M. Yang, and L. Bourdev, \u201cCompressing deep convolutional networks using vector quantization,\u201d arXiv preprint arXiv:1412.6115, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6115"
        },
        {
            "id": "9",
            "entry": "[9] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "10",
            "entry": "[10] S. Han, H. Mao, and W. J. Dally, \u201cDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding,\u201d arXiv preprint arXiv:1510.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "11",
            "entry": "[11] T. S. Ferguson, \u201cA Bayesian analysis of some nonparametric problems,\u201d Ann. Statist., vol. 1, no. 2, pp. 209\u2013230, 1973.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ferguson%2C%20T.S.%20A%20Bayesian%20analysis%20of%20some%20nonparametric%20problems%2C%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ferguson%2C%20T.S.%20A%20Bayesian%20analysis%20of%20some%20nonparametric%20problems%2C%201973"
        },
        {
            "id": "12",
            "entry": "[12] M. Zhou, Y. Cong, and B. Chen, \u201cThe Poisson gamma belief network,\u201d in NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20The%20Poisson%20gamma%20belief%20network%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20The%20Poisson%20gamma%20belief%20network%2C%202015"
        },
        {
            "id": "13",
            "entry": "[13] M. Zhou, Y. Cong, and B. Chen, \u201cAugmentable gamma belief networks,\u201d J. Mach. Learn. Res., vol. 17, no. 163, pp. 1\u201344, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20Augmentable%20gamma%20belief%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Cong%2C%20Y.%20Chen%2C%20B.%20Augmentable%20gamma%20belief%20networks%2C%202016"
        },
        {
            "id": "14",
            "entry": "[14] V. Nair and G. E. Hinton, \u201cRectified linear units improve restricted Boltzmann machines,\u201d in ICML, pp. 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20Boltzmann%20machines%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20linear%20units%20improve%20restricted%20Boltzmann%20machines%2C%202010"
        },
        {
            "id": "15",
            "entry": "[15] X. Glorot, A. Bordes, and Y. Bengio, \u201cDeep sparse rectifier neural networks,\u201d in AISTATS, pp. 315\u2013323, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%2C%202011"
        },
        {
            "id": "16",
            "entry": "[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in NIPS, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%2C%202012"
        },
        {
            "id": "17",
            "entry": "[17] W. Shang, K. Sohn, D. Almeida, and H. Lee, \u201cUnderstanding and improving convolutional neural networks via concatenated rectified linear units,\u201d in ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shang%2C%20W.%20Sohn%2C%20K.%20Almeida%2C%20D.%20Lee%2C%20H.%20%E2%80%9CUnderstanding%20and%20improving%20convolutional%20neural%20networks%20via%20concatenated%20rectified%20linear%20units%2C%E2%80%9D%20in%20ICML%202016"
        },
        {
            "id": "18",
            "entry": "[18] M. Zhou, \u201cInfinite edge partition models for overlapping community detection and link prediction,\u201d in AISTATS, pp. 1135\u20131143, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Infinite%20edge%20partition%20models%20for%20overlapping%20community%20detection%20and%20link%20prediction%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Infinite%20edge%20partition%20models%20for%20overlapping%20community%20detection%20and%20link%20prediction%2C%202015"
        },
        {
            "id": "19",
            "entry": "[19] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearl%2C%20J.%20Probabilistic%20Reasoning%20in%20Intelligent%20Systems%3A%20Networks%20of%20Plausible%20Inference"
        },
        {
            "id": "20",
            "entry": "[20] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, \u201cAn introduction to variational methods for graphical models,\u201d Machine learning, vol. 37, no. 2, pp. 183\u2013233, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.I.%20Ghahramani%2C%20Z.%20Jaakkola%2C%20T.S.%20Saul%2C%20L.K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%2C%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20M.I.%20Ghahramani%2C%20Z.%20Jaakkola%2C%20T.S.%20Saul%2C%20L.K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%2C%201999"
        },
        {
            "id": "21",
            "entry": "[21] S. Arora, R. Ge, T. Ma, and A. Risteski, \u201cProvable learning of noisy-or networks,\u201d arXiv preprint arXiv:1612.08795, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08795"
        },
        {
            "id": "22",
            "entry": "[22] F. Caron and E. B. Fox, \u201cSparse graphs using exchangeable random measures,\u201d J. R. Stat. Soc.: Series B, vol. 79, no. 5, pp. 1295\u20131366, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caron%2C%20F.%20Fox%2C%20E.B.%20Sparse%20graphs%20using%20exchangeable%20random%20measures%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caron%2C%20F.%20Fox%2C%20E.B.%20Sparse%20graphs%20using%20exchangeable%20random%20measures%2C%202017"
        },
        {
            "id": "23",
            "entry": "[23] M. Zhou, \u201cDiscussion on \u201csparse graphs using exchangeable random measures\u201d by Francois Caron and Emily B. Fox,\u201d arXiv preprint arXiv:1802.07721, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07721"
        },
        {
            "id": "24",
            "entry": "[24] P. Rai, C. Hu, R. Henao, and L. Carin, \u201cLarge-scale Bayesian multi-label learning via topic-based label embeddings,\u201d in NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rai%2C%20P.%20Hu%2C%20C.%20Henao%2C%20R.%20Carin%2C%20L.%20%E2%80%9CLarge-scale%20Bayesian%20multi-label%20learning%20via%20topic-based%20label%20embeddings%2C%E2%80%9D%20in%20NIPS%202015"
        },
        {
            "id": "25",
            "entry": "[25] C. M. Bishop, Neural networks for pattern recognition. Oxford university press, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.M.%20Neural%20networks%20for%20pattern%20recognition%201995"
        },
        {
            "id": "26",
            "entry": "[26] F. Aiolli and A. Sperduti, \u201cMulticlass classification with multi-prototype support vector machines,\u201d vol. 6, pp. 817\u2013850, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aiolli%2C%20F.%20Sperduti%2C%20A.%20Multiclass%20classification%20with%20multi-prototype%20support%20vector%20machines%2C%202005"
        },
        {
            "id": "27",
            "entry": "[27] Z. Wang, N. Djuric, K. Crammer, and S. Vucetic, \u201cTrading representability for scalability: Adaptive multi-hyperplane machine for nonlinear classification,\u201d in KDD, pp. 24\u201332, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Djuric%2C%20N.%20Crammer%2C%20K.%20Vucetic%2C%20S.%20Trading%20representability%20for%20scalability%3A%20Adaptive%20multi-hyperplane%20machine%20for%20nonlinear%20classification%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Djuric%2C%20N.%20Crammer%2C%20K.%20Vucetic%2C%20S.%20Trading%20representability%20for%20scalability%3A%20Adaptive%20multi-hyperplane%20machine%20for%20nonlinear%20classification%2C%202011"
        },
        {
            "id": "28",
            "entry": "[28] N. Manwani and P. S. Sastry, \u201cLearning polyhedral classifiers using logistic function.,\u201d in ACML, pp. 17\u201330, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manwani%2C%20N.%20Sastry%2C%20P.S.%20Learning%20polyhedral%20classifiers%20using%20logistic%20function.%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manwani%2C%20N.%20Sastry%2C%20P.S.%20Learning%20polyhedral%20classifiers%20using%20logistic%20function.%2C%202010"
        },
        {
            "id": "29",
            "entry": "[29] N. Manwani and P. S. Sastry, \u201cPolyceptron: A polyhedral learning algorithm,\u201d arXiv:1107.1564, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1107.1564"
        },
        {
            "id": "30",
            "entry": "[30] A. Kantchelian, M. C. Tschantz, L. Huang, P. L. Bartlett, A. D. Joseph, and J. D. Tygar, \u201cLarge-margin convex polytope machine,\u201d in NIPS, pp. 3248\u20133256, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kantchelian%2C%20A.%20Tschantz%2C%20M.C.%20Huang%2C%20L.%20Bartlett%2C%20P.L.%20Large-margin%20convex%20polytope%20machine%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kantchelian%2C%20A.%20Tschantz%2C%20M.C.%20Huang%2C%20L.%20Bartlett%2C%20P.L.%20Large-margin%20convex%20polytope%20machine%2C%202014"
        },
        {
            "id": "31",
            "entry": "[31] M. Tipping, \u201cSparse Bayesian learning and the relevance vector machine,\u201d J. Mach. Learn. Res., vol. 1, pp. 211\u2013244, June 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tipping%2C%20M.%20Sparse%20Bayesian%20learning%20and%20the%20relevance%20vector%20machine%2C%202001-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tipping%2C%20M.%20Sparse%20Bayesian%20learning%20and%20the%20relevance%20vector%20machine%2C%202001-06"
        },
        {
            "id": "32",
            "entry": "[32] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "33",
            "entry": "[33] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man\u00e9, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi\u00e9gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, \u201cTensorFlow: Large-scale machine learning on heterogeneous systems,\u201d 2015. Software available from tensorflow.org.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Agarwal%2C%20A.%20Barham%2C%20P.%20Brevdo%2C%20E.%20TensorFlow%3A%20Large-scale%20machine%20learning%20on%20heterogeneous%20systems%2C%202015"
        },
        {
            "id": "34",
            "entry": "[34] Q. Zhang and M. Zhou, \u201cNonparametric Bayesian Lomax delegate racing for survival analysis with competing risks,\u201d in NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Q.%20Zhou%2C%20M.%20%E2%80%9CNonparametric%20Bayesian%20Lomax%20delegate%20racing%20for%20survival%20analysis%20with%20competing%20risks%2C%E2%80%9D%20in%20NIPS%202018"
        },
        {
            "id": "35",
            "entry": "[35] L. A. Hannah, D. M. Blei, and W. B. Powell, \u201cDirichlet process mixtures of generalized linear models,\u201d J. Mach. Learn. Res., vol. 12, no. Jun, pp. 1923\u20131953, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hannah%2C%20L.A.%20Blei%2C%20D.M.%20Powell%2C%20W.B.%20Dirichlet%20process%20mixtures%20of%20generalized%20linear%20models%2C%202011-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hannah%2C%20L.A.%20Blei%2C%20D.M.%20Powell%2C%20W.B.%20Dirichlet%20process%20mixtures%20of%20generalized%20linear%20models%2C%202011-06"
        },
        {
            "id": "36",
            "entry": "[36] K. Crammer and Y. Singer, \u201cOn the algorithmic implementation of multiclass kernel-based vector machines,\u201d J. Mach. Learn. Res., vol. 2, pp. 265\u2013292, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crammer%2C%20K.%20Singer%2C%20Y.%20On%20the%20algorithmic%20implementation%20of%20multiclass%20kernel-based%20vector%20machines%2C%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crammer%2C%20K.%20Singer%2C%20Y.%20On%20the%20algorithmic%20implementation%20of%20multiclass%20kernel-based%20vector%20machines%2C%202002"
        },
        {
            "id": "37",
            "entry": "[37] D. B. Dunson and A. H. Herring, \u201cBayesian latent variable models for mixed discrete outcomes,\u201d Biostatistics, vol. 6, no. 1, pp. 11\u201325, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dunson%2C%20D.B.%20Herring%2C%20A.H.%20Bayesian%20latent%20variable%20models%20for%20mixed%20discrete%20outcomes%2C%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dunson%2C%20D.B.%20Herring%2C%20A.H.%20Bayesian%20latent%20variable%20models%20for%20mixed%20discrete%20outcomes%2C%202005"
        },
        {
            "id": "38",
            "entry": "[38] M. Zhou, L. Hannah, D. Dunson, and L. Carin, \u201cBeta-negative binomial process and Poisson factor analysis,\u201d in AISTATS, pp. 1462\u20131471, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Hannah%2C%20L.%20Dunson%2C%20D.%20Carin%2C%20L.%20Beta-negative%20binomial%20process%20and%20Poisson%20factor%20analysis%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Hannah%2C%20L.%20Dunson%2C%20D.%20Carin%2C%20L.%20Beta-negative%20binomial%20process%20and%20Poisson%20factor%20analysis%2C%202012"
        },
        {
            "id": "39",
            "entry": "[39] M. Zhou and L. Carin, \u201cNegative binomial process count and mixture modeling,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 2, pp. 307\u2013320, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Carin%2C%20L.%20Negative%20binomial%20process%20count%20and%20mixture%20modeling%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Carin%2C%20L.%20Negative%20binomial%20process%20count%20and%20mixture%20modeling%2C%202015"
        },
        {
            "id": "40",
            "entry": "[40] N. G. Polson and J. G. Scott, \u201cDefault Bayesian analysis for multi-way tables: a data-augmentation approach,\u201d arXiv:1109.4180v1, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1109.4180v1"
        },
        {
            "id": "41",
            "entry": "[41] M. Zhou, L. Li, D. Dunson, and L. Carin, \u201cLognormal and gamma mixed negative binomial regression,\u201d in ICML, pp. 1343\u20131350, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Li%2C%20L.%20Dunson%2C%20D.%20Carin%2C%20L.%20Lognormal%20and%20gamma%20mixed%20negative%20binomial%20regression%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Li%2C%20L.%20Dunson%2C%20D.%20Carin%2C%20L.%20Lognormal%20and%20gamma%20mixed%20negative%20binomial%20regression%2C%202012"
        },
        {
            "id": "42",
            "entry": "[42] N. G. Polson, J. G. Scott, and J. Windle, \u201cBayesian inference for logistic models using P\u00f3lya\u2013Gamma latent variables,\u201d J. Amer. Statist. Assoc., vol. 108, no. 504, pp. 1339\u20131349, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polson%2C%20N.G.%20Scott%2C%20J.G.%20Windle%2C%20J.%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B3lya%E2%80%93Gamma%20latent%20variables%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polson%2C%20N.G.%20Scott%2C%20J.G.%20Windle%2C%20J.%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B3lya%E2%80%93Gamma%20latent%20variables%2C%202013"
        },
        {
            "id": "43",
            "entry": "[43] M. Zhou, \u201cSoftplus regressions and convex polytopes,\u201d arXiv:1608.06383, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06383"
        },
        {
            "id": "44",
            "entry": "[44] G. R\u00e4tsch, T. Onoda, and K.-R. M\u00fcller, \u201cSoft margins for AdaBoost,\u201d Machine learning, vol. 42, no. 3, pp. 287\u2013320, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R%C3%A4tsch%2C%20G.%20Onoda%2C%20T.%20M%C3%BCller%2C%20K.-R.%20Soft%20margins%20for%20AdaBoost%2C%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R%C3%A4tsch%2C%20G.%20Onoda%2C%20T.%20M%C3%BCller%2C%20K.-R.%20Soft%20margins%20for%20AdaBoost%2C%202001"
        },
        {
            "id": "45",
            "entry": "[45] T. Diethe, \u201c13 benchmark datasets derived from the UCI, DELVE and STATLOG repositories.\u201d https://github.com/tdiethe/gunnar_raetsch_benchmark_datasets/, 2015.",
            "url": "https://github.com/tdiethe/gunnar_raetsch_benchmark_datasets/"
        },
        {
            "id": "46",
            "entry": "[46] Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin, \u201cTraining and testing low-degree polynomial data mappings via linear SVM,\u201d J. Mach. Learn. Res., vol. 11, pp. 1471\u20131490, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Y.-W.%20Hsieh%2C%20C.-J.%20Chang%2C%20K.-W.%20Ringgaard%2C%20M.%20Training%20and%20testing%20low-degree%20polynomial%20data%20mappings%20via%20linear%20SVM%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Y.-W.%20Hsieh%2C%20C.-J.%20Chang%2C%20K.-W.%20Ringgaard%2C%20M.%20Training%20and%20testing%20low-degree%20polynomial%20data%20mappings%20via%20linear%20SVM%2C%202010"
        },
        {
            "id": "47",
            "entry": "[47] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, \u201cLIBLINEAR: A library for large linear classification,\u201d J. Mach. Learn. Res., pp. 1871\u20131874, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20R.-E.%20Chang%2C%20K.-W.%20Hsieh%2C%20C.-J.%20Wang%2C%20X.-R.%20LIBLINEAR%3A%20A%20library%20for%20large%20linear%20classification%2C%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20R.-E.%20Chang%2C%20K.-W.%20Hsieh%2C%20C.-J.%20Wang%2C%20X.-R.%20LIBLINEAR%3A%20A%20library%20for%20large%20linear%20classification%2C%202008"
        },
        {
            "id": "48",
            "entry": "[48] C.-C. Chang and C.-J. Lin, \u201cLIBSVM: A library for support vector machines,\u201d ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20C.-C.%20Lin%2C%20C.-J.%20LIBSVM%3A%20A%20library%20for%20support%20vector%20machines%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20C.-C.%20Lin%2C%20C.-J.%20LIBSVM%3A%20A%20library%20for%20support%20vector%20machines%2C%202011"
        },
        {
            "id": "49",
            "entry": "[49] N. Djuric, L. Lan, S. Vucetic, and Z. Wang, \u201cBudgetedsvm: A toolbox for scalable SVM approximations,\u201d J. Mach. Learn. Res., vol. 14, pp. 3813\u20133817, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Djuric%2C%20N.%20Lan%2C%20L.%20Vucetic%2C%20S.%20Wang%2C%20Z.%20Budgetedsvm%3A%20A%20toolbox%20for%20scalable%20SVM%20approximations%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Djuric%2C%20N.%20Lan%2C%20L.%20Vucetic%2C%20S.%20Wang%2C%20Z.%20Budgetedsvm%3A%20A%20toolbox%20for%20scalable%20SVM%20approximations%2C%202013"
        }
    ]
}
