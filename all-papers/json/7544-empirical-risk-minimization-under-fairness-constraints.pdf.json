{
    "filename": "7544-empirical-risk-minimization-under-fairness-constraints.pdf",
    "metadata": {
        "title": "Empirical Risk Minimization Under Fairness Constraints",
        "author": "Michele Donini, Luca Oneto, Shai Ben-David, John S. Shawe-Taylor, Massimiliano Pontil",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7544-empirical-risk-minimization-under-fairness-constraints.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our methodology. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches."
    },
    "keywords": [
        {
            "term": "kernel method",
            "url": "https://en.wikipedia.org/wiki/kernel_method"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "disparate treatment",
            "url": "https://en.wikipedia.org/wiki/disparate_treatment"
        },
        {
            "term": "support vector machines",
            "url": "https://en.wikipedia.org/wiki/support_vector_machines"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "linear model",
            "url": "https://en.wikipedia.org/wiki/linear_model"
        },
        {
            "term": "sensitive information",
            "url": "https://en.wikipedia.org/wiki/sensitive_information"
        },
        {
            "term": "Equal Opportunity",
            "url": "https://en.wikipedia.org/wiki/Equal_Opportunity"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        }
    ],
    "highlights": [
        "In recent years there has been a lot of interest on algorithmic fairness in machine learning see, e.g., [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] and references therein",
        "As a concrete example of the framework, in Section 3 we describe how kernel methods such as support vector machines (SVMs) can be enhanced to satisfy the fairness constraint",
        "The aim of this experiment is to study the behavior of our method, in terms of both difference of EO and classification accuracy, in comparison to standard support vector machines",
        "We considered only datasets with a difference of EO higher than 0.1, when the model is generated by an support vector machines validated with the NVP",
        "We have presented a generalized notion of fairness, which encompasses previously introduced notion and can be used to constrain Empirical Risk Minimization, in order to learn fair classifiers",
        "Our theoretical observations provide a statistical justification for this approach and our algorithmic observations suggest a way to implement it efficiently in the setting of kernel methods"
    ],
    "key_statements": [
        "In recent years there has been a lot of interest on algorithmic fairness in machine learning see, e.g., [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] and references therein",
        "For example if the learning problem is to decide whether a person should be offered a loan based on her previous credit card scores, we would like to build a model which does not unfairly use additional sensitive information such as race or sex",
        "Several notions of fairness and associated learning methods have been introduced in machine learning in the past few years, including Demographic Parity [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], Equal Odds and Equal Opportunities [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Disparate Treatment, Impact, and mistreatment [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "As a concrete example of the framework, in Section 3 we describe how kernel methods such as support vector machines (SVMs) can be enhanced to satisfy the fairness constraint",
        "We shown how a linear fairness constraint arises naturally in the framework and allows us to develop a novel convex learning method that is supported by consistency properties both in terms of Equal Opportunity and risk of the selected model, performing favorably against state-of-the-art alternatives on a series of benchmark datasets",
        "For the Arrhythmia, COMPAS, German and Drug datasets, this procedure is repeated 10 times, and we reported the average performance on the test set alongside its standard deviation",
        "The aim of this experiment is to study the behavior of our method, in terms of both difference of EO and classification accuracy, in comparison to standard support vector machines",
        "When a standard machine learning method is applied to this toy dataset, the generated model is unfair with respect to the group b, in that the classifier tends to negatively classify the examples in this group",
        "We considered only datasets with a difference of EO higher than 0.1, when the model is generated by an support vector machines validated with the NVP",
        "This baseline is the simplest way to inject the fairness into the model; Hardt method [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] applied to the best support vector machines; Zafar method [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], implemented with the code provided by the authors for the linear case6",
        "In order to quantify this effect, we present in Figure 2 the results of Table 1 of linear and nonlinear methods, when the error and the difference of EO are normalized in [0, 1] column-wise both when the s is included and not included in x",
        "We have presented a generalized notion of fairness, which encompasses previously introduced notion and can be used to constrain Empirical Risk Minimization, in order to learn fair classifiers",
        "Our theoretical observations provide a statistical justification for this approach and our algorithmic observations suggest a way to implement it efficiently in the setting of kernel methods",
        "It would be interesting to study whether our method can be improved by other relaxations of the fairness constraint beyond the linear loss used here",
        "It would be interesting to study how the choice of the parameter \u270f affects the statistical performance of our method and derive optimal accuracy-fairness trade-off as a function of this parameter"
    ],
    "summary": [
        "In recent years there has been a lot of interest on algorithmic fairness in machine learning see, e.g., [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] and references therein.",
        "In Section 2 we introduce a generalization of this notion of fairness which constrains the conditional risk of a classifier, associated to positive labeled samples of a group, to be approximately constant with respect to group membership.",
        "As a concrete example of the framework, in Section 3 we describe how kernel methods such as support vector machines (SVMs) can be enhanced to satisfy the fairness constraint.",
        "We shown how a linear fairness constraint arises naturally in the framework and allows us to develop a novel convex learning method that is supported by consistency properties both in terms of EO and risk of the selected model, performing favorably against state-of-the-art alternatives on a series of benchmark datasets.",
        "The second statement of Proposition 1, instead, tells us that if the hypothesis of inequality (11) holds, the linear loss based fairness is close to the EO.",
        "The combination of Theorem 1 and Proposition 1 provides conditions under which a solution fc of Problem 4, which is convex, is close, both in terms of classification accuracy and fairness, to a solution fh\u21e4 of Problem 7, which is our final goal.",
        "The aim of this experiment is to study the behavior of our method, in terms of both DEO and classification accuracy, in comparison to standard SVM.",
        "When a standard machine learning method is applied to this toy dataset, the generated model is unfair with respect to the group b, in that the classifier tends to negatively classify the examples in this group.",
        "Figure 1 (Left) shows the performance of the various generated models with respect to the classification error and DEO on the test set.",
        "Note that our method generated models that have an higher level of fairness, maintaining a good level of accuracy.",
        "Note that our method generates a model with a similar true positive rate among the two groups.",
        "This baseline is the simplest way to inject the fairness into the model; Hardt method [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] applied to the best SVM; Zafar method [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], implemented with the code provided by the authors for the linear case6.",
        "Experimental results suggest that our approach is promising for applications, generating models with improved fairness properties while maintaining classification accuracy.",
        "It would be interesting to study whether our method can be improved by other relaxations of the fairness constraint beyond the linear loss used here.",
        "It would be interesting to study how the choice of the parameter \u270f affects the statistical performance of our method and derive optimal accuracy-fairness trade-off as a function of this parameter"
    ],
    "headline": "We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. Dwork, N. Immorlica, A. T. Kalai, and M. D. M. Leiserson. Decoupled classifiers for group-fair and efficient machine learning. In Conference on Fairness, Accountability and Transparency, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20C.%20Immorlica%2C%20N.%20Kalai%2C%20A.T.%20Leiserson%2C%20M.D.M.%20Decoupled%20classifiers%20for%20group-fair%20and%20efficient%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20C.%20Immorlica%2C%20N.%20Kalai%2C%20A.T.%20Leiserson%2C%20M.D.M.%20Decoupled%20classifiers%20for%20group-fair%20and%20efficient%20machine%20learning%202018"
        },
        {
            "id": "2",
            "entry": "[2] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. In Advances in neural information processing systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20M.%20Price%2C%20E.%20Srebro%2C%20N.%20Equality%20of%20opportunity%20in%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20M.%20Price%2C%20E.%20Srebro%2C%20N.%20Equality%20of%20opportunity%20in%20supervised%20learning%202016"
        },
        {
            "id": "3",
            "entry": "[3] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In International Conference on World Wide Web, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.Gomez%20Gummadi%2C%20K.P.%20Fairness%20beyond%20disparate%20treatment%20%26%20disparate%20impact%3A%20Learning%20classification%20without%20disparate%20mistreatment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.Gomez%20Gummadi%2C%20K.P.%20Fairness%20beyond%20disparate%20treatment%20%26%20disparate%20impact%3A%20Learning%20classification%20without%20disparate%20mistreatment%202017"
        },
        {
            "id": "4",
            "entry": "[4] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In International Conference on Machine Learning, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zemel%2C%20R.%20Wu%2C%20Y.%20Swersky%2C%20K.%20Pitassi%2C%20T.%20Dwork.%20Learning%20fair%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20R.%20Wu%2C%20Y.%20Swersky%2C%20K.%20Pitassi%2C%20T.%20Dwork.%20Learning%20fair%20representations%202013"
        },
        {
            "id": "5",
            "entry": "[5] N. Kilbertus, M. Rojas-Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Sch\u00f6lkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kilbertus%2C%20N.%20Rojas-Carulla%2C%20M.%20Parascandolo%2C%20G.%20Hardt%2C%20M.%20Avoiding%20discrimination%20through%20causal%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kilbertus%2C%20N.%20Rojas-Carulla%2C%20M.%20Parascandolo%2C%20G.%20Hardt%2C%20M.%20Avoiding%20discrimination%20through%20causal%20reasoning%202017"
        },
        {
            "id": "6",
            "entry": "[6] M. J. Kusner, J. Loftus, C. Russell, and R. Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20J%20Kusner%20J%20Loftus%20C%20Russell%20and%20R%20Silva%20Counterfactual%20fairness%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20J%20Kusner%20J%20Loftus%20C%20Russell%20and%20R%20Silva%20Counterfactual%20fairness%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%202017"
        },
        {
            "id": "7",
            "entry": "[7] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney. Optimized preprocessing for discrimination prevention. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calmon%2C%20F.%20Wei%2C%20D.%20Vinzamuri%2C%20B.%20Ramamurthy%2C%20K.Natesan%20Optimized%20preprocessing%20for%20discrimination%20prevention%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calmon%2C%20F.%20Wei%2C%20D.%20Vinzamuri%2C%20B.%20Ramamurthy%2C%20K.Natesan%20Optimized%20preprocessing%20for%20discrimination%20prevention%202017"
        },
        {
            "id": "8",
            "entry": "[8] M. Joseph, M. Kearns, J. H. Morgenstern, and A. Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20M.%20Kearns%2C%20M.%20Morgenstern%2C%20J.H.%20Roth%2C%20A.%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joseph%2C%20M.%20Kearns%2C%20M.%20Morgenstern%2C%20J.H.%20Roth%2C%20A.%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016"
        },
        {
            "id": "9",
            "entry": "[9] F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chierichetti%2C%20F.%20Kumar%2C%20R.%20Lattanzi%2C%20S.%20Vassilvitskii%2C%20S.%20Fair%20clustering%20through%20fairlets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chierichetti%2C%20F.%20Kumar%2C%20R.%20Lattanzi%2C%20S.%20Vassilvitskii%2C%20S.%20Fair%20clustering%20through%20fairlets%202017"
        },
        {
            "id": "10",
            "entry": "[10] S. Jabbari, M. Joseph, M. Kearns, J. Morgenstern, and A. Roth. Fair learning in markovian environments. In Conference on Fairness, Accountability, and Transparency in Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jabbari%2C%20S.%20Joseph%2C%20M.%20Kearns%2C%20M.%20Morgenstern%2C%20J.%20Fair%20learning%20in%20markovian%20environments%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jabbari%2C%20S.%20Joseph%2C%20M.%20Kearns%2C%20M.%20Morgenstern%2C%20J.%20Fair%20learning%20in%20markovian%20environments%202016"
        },
        {
            "id": "11",
            "entry": "[11] S. Yao and B. Huang. Beyond parity: Fairness objectives for collaborative filtering. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20S.%20Huang%2C%20B.%20Beyond%20parity%3A%20Fairness%20objectives%20for%20collaborative%20filtering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20S.%20Huang%2C%20B.%20Beyond%20parity%3A%20Fairness%20objectives%20for%20collaborative%20filtering%202017"
        },
        {
            "id": "12",
            "entry": "[12] K. Lum and J. Johndrow. A statistical framework for fair predictive algorithms. arXiv preprint arXiv:1610.08077, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.08077"
        },
        {
            "id": "13",
            "entry": "[13] I. Zliobaite. On the relation between accuracy and fairness in binary classification. arXiv preprint arXiv:1505.05723, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05723"
        },
        {
            "id": "14",
            "entry": "[14] T. Calders, F. Kamiran, and M. Pechenizkiy. Building classifiers with independency constraints. In IEEE international conference on Data mining, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calders%2C%20T.%20Kamiran%2C%20F.%20Pechenizkiy%2C%20M.%20Building%20classifiers%20with%20independency%20constraints%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calders%2C%20T.%20Kamiran%2C%20F.%20Pechenizkiy%2C%20M.%20Building%20classifiers%20with%20independency%20constraints%202009"
        },
        {
            "id": "15",
            "entry": "[15] G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger. On fairness and calibration. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pleiss%2C%20G.%20Raghavan%2C%20M.%20Wu%2C%20F.%20Kleinberg%2C%20J.%20On%20fairness%20and%20calibration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pleiss%2C%20G.%20Raghavan%2C%20M.%20Wu%2C%20F.%20Kleinberg%2C%20J.%20On%20fairness%20and%20calibration%202017"
        },
        {
            "id": "16",
            "entry": "[16] A. Beutel, J. Chen, Z. Zhao, and E. H. Chi. Data decisions and theoretical implications when adversarially learning fair representations. In Conference on Fairness, Accountability, and Transparency in Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beutel%2C%20A.%20Chen%2C%20J.%20Zhao%2C%20Z.%20Chi%2C%20E.H.%20Data%20decisions%20and%20theoretical%20implications%20when%20adversarially%20learning%20fair%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beutel%2C%20A.%20Chen%2C%20J.%20Zhao%2C%20Z.%20Chi%2C%20E.H.%20Data%20decisions%20and%20theoretical%20implications%20when%20adversarially%20learning%20fair%20representations%202017"
        },
        {
            "id": "17",
            "entry": "[17] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and removing disparate impact. In International Conference on Knowledge Discovery and Data Mining, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20M.%20Friedler%2C%20S.A.%20Moeller%2C%20J.%20Scheidegger%2C%20C.%20Certifying%20and%20removing%20disparate%20impact%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20M.%20Friedler%2C%20S.A.%20Moeller%2C%20J.%20Scheidegger%2C%20C.%20Certifying%20and%20removing%20disparate%20impact%202015"
        },
        {
            "id": "18",
            "entry": "[18] A. Agarwal, A. Beygelzimer, M. Dud\u00edk, and J. Langford. A reductions approach to fair classification. In Conference on Fairness, Accountability, and Transparency in Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Beygelzimer%2C%20A.%20Dud%C3%ADk%2C%20M.%20Langford%2C%20J.%20A%20reductions%20approach%20to%20fair%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Beygelzimer%2C%20A.%20Dud%C3%ADk%2C%20M.%20Langford%2C%20J.%20A%20reductions%20approach%20to%20fair%20classification%202017"
        },
        {
            "id": "19",
            "entry": "[19] A. Agarwal, A. Beygelzimer, M. Dud\u00edk, J. Langford, and H. Wallach. A reductions approach to fair classification. arXiv preprint arXiv:1803.02453, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02453"
        },
        {
            "id": "20",
            "entry": "[20] B. Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro. Learning non-discriminatory predictors. In Computational Learning Theory, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodworth%2C%20B.%20Gunasekar%2C%20S.%20Ohannessian%2C%20M.I.%20Srebro%2C%20N.%20Learning%20non-discriminatory%20predictors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodworth%2C%20B.%20Gunasekar%2C%20S.%20Ohannessian%2C%20M.I.%20Srebro%2C%20N.%20Learning%20non-discriminatory%20predictors%202017"
        },
        {
            "id": "21",
            "entry": "[21] A. K. Menon and R. C. Williamson. The cost of fairness in binary classification. In Conference on Fairness, Accountability and Transparency, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20A.K.%20Williamson%2C%20R.C.%20The%20cost%20of%20fairness%20in%20binary%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20A.K.%20Williamson%2C%20R.C.%20The%20cost%20of%20fairness%20in%20binary%20classification%202018"
        },
        {
            "id": "22",
            "entry": "[22] M. B. Zafar, I. Valera, M. Rodriguez, K. Gummadi, and A. Weller. From parity to preference-based notions of fairness in classification. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.%20Gummadi%2C%20K.%20From%20parity%20to%20preference-based%20notions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.%20Gummadi%2C%20K.%20From%20parity%20to%20preference-based%20notions%202017"
        },
        {
            "id": "23",
            "entry": "[23] Y. Bechavod and K. Ligett. Penalizing unfairness in binary classification. arXiv preprint arXiv:1707.00044v3, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00044v3"
        },
        {
            "id": "24",
            "entry": "[24] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi. Fairness constraints: Mechanisms for fair classification. In International Conference on Artificial Intelligence and Statistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.Gomez%20Gummadi%2C%20K.P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20M.B.%20Valera%2C%20I.%20Rodriguez%2C%20M.Gomez%20Gummadi%2C%20K.P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017"
        },
        {
            "id": "25",
            "entry": "[25] T. Kamishima, S. Akaho, and J. Sakuma. Fairness-aware learning through regularization approach. In International Conference on Data Mining Workshops, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamishima%2C%20T.%20Akaho%2C%20S.%20Sakuma%2C%20J.%20Fairness-aware%20learning%20through%20regularization%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamishima%2C%20T.%20Akaho%2C%20S.%20Sakuma%2C%20J.%20Fairness-aware%20learning%20through%20regularization%20approach%202011"
        },
        {
            "id": "26",
            "entry": "[26] M. Kearns, S. Neel, A. Roth, and Z. S. Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. arXiv preprint arXiv:1711.05144, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05144"
        },
        {
            "id": "27",
            "entry": "[27] J. Adebayo and L. Kagal. Iterative orthogonal feature projection for diagnosing bias in black-box models. In Conference on Fairness, Accountability, and Transparency in Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adebayo%2C%20J.%20Kagal%2C%20L.%20Iterative%20orthogonal%20feature%20projection%20for%20diagnosing%20bias%20in%20black-box%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adebayo%2C%20J.%20Kagal%2C%20L.%20Iterative%20orthogonal%20feature%20projection%20for%20diagnosing%20bias%20in%20black-box%20models%202016"
        },
        {
            "id": "28",
            "entry": "[28] F. Kamiran and T. Calders. Classifying without discriminating. In International Conference on Computer, Control and Communication, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20F.%20Calders%2C%20T.%20Classifying%20without%20discriminating%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20F.%20Calders%2C%20T.%20Classifying%20without%20discriminating%202009"
        },
        {
            "id": "29",
            "entry": "[29] F. Kamiran and T. Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1\u201333, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20F.%20Calders%2C%20T.%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20F.%20Calders%2C%20T.%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202012"
        },
        {
            "id": "30",
            "entry": "[30] F. Kamiran and T. Calders. Classification with no discrimination by preferential sampling. In Machine Learning Conference, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20F.%20Calders%2C%20T.%20Classification%20with%20no%20discrimination%20by%20preferential%20sampling%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20F.%20Calders%2C%20T.%20Classification%20with%20no%20discrimination%20by%20preferential%20sampling%202010"
        },
        {
            "id": "31",
            "entry": "[31] A. P\u00e9rez-Suay, V. Laparra, G. Mateo-Garc\u00eda, J. Mu\u00f1oz-Mar\u00ed, L. G\u00f3mez-Chova, and G. Camps-Valls. Fair kernel learning. In Machine Learning and Knowledge Discovery in Databases, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=P%C3%A9rez-Suay%2C%20A.%20Laparra%2C%20V.%20Mateo-Garc%C3%ADa%2C%20G.%20Mu%C3%B1oz-Mar%C3%AD%2C%20J.%20Fair%20kernel%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=P%C3%A9rez-Suay%2C%20A.%20Laparra%2C%20V.%20Mateo-Garc%C3%ADa%2C%20G.%20Mu%C3%B1oz-Mar%C3%AD%2C%20J.%20Fair%20kernel%20learning%202017"
        },
        {
            "id": "32",
            "entry": "[32] R. Berk, H. Heidari, S. Jabbari, M. Joseph, M. Kearns, J. Morgenstern, S. Neel, and A. Roth. A convex framework for fair regression. arXiv preprint arXiv:1706.02409, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02409"
        },
        {
            "id": "33",
            "entry": "[33] D. Alabi, N. Immorlica, and A. T. Kalai. When optimizing nonlinear objectives is no harder than linear objectives. arXiv preprint arXiv:1804.04503, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.04503"
        },
        {
            "id": "34",
            "entry": "[34] Massimiliano Pontil John Shawe-Taylor Michele Donini, Shai Ben-David. An efficient method to impose fairness in linear models. In NIPS Workshop on Prioritising Online Content, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donini%2C%20Massimiliano%20Pontil%20John%20Shawe-Taylor%20Michele%20Ben-David%2C%20Shai%20An%20efficient%20method%20to%20impose%20fairness%20in%20linear%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donini%2C%20Massimiliano%20Pontil%20John%20Shawe-Taylor%20Michele%20Ben-David%2C%20Shai%20An%20efficient%20method%20to%20impose%20fairness%20in%20linear%20models%202017"
        },
        {
            "id": "35",
            "entry": "[35] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Ben-David%2C%20S.%20Understanding%20machine%20learning%3A%20From%20theory%20to%20algorithms%202014"
        },
        {
            "id": "36",
            "entry": "[36] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "37",
            "entry": "[37] N. Quadrianto and V. Sharmanska. Recycling privileged learning and distribution matching for fairness. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quadrianto%2C%20N.%20Sharmanska%2C%20V.%20Recycling%20privileged%20learning%20and%20distribution%20matching%20for%20fairness%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quadrianto%2C%20N.%20Sharmanska%2C%20V.%20Recycling%20privileged%20learning%20and%20distribution%20matching%20for%20fairness%202017"
        },
        {
            "id": "38",
            "entry": "[38] A. Maurer. A note on the pac bayesian theorem. arXiv preprint cs/0411099, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20A%20note%20on%20the%20pac%20bayesian%202004"
        },
        {
            "id": "39",
            "entry": "[39] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge University Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shawe-Taylor%2C%20J.%20Cristianini%2C%20N.%20Kernel%20methods%20for%20pattern%20analysis%202004"
        },
        {
            "id": "40",
            "entry": "[40] A. J. Smola and B. Sch\u00f6lkopf. Learning with Kernels. MIT Press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20A.J.%20Sch%C3%B6lkopf%2C%20B.%20Learning%20with%20Kernels%202001"
        },
        {
            "id": "41",
            "entry": "[41] B. Sch\u00f6lkopf, R. Herbrich, and A. Smola. A generalized representer theorem. In Computational Learning",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20B.%20Herbrich%2C%20R.%20Smola%2C%20A.%20A%20generalized%20representer%20theorem",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6lkopf%2C%20B.%20Herbrich%2C%20R.%20Smola%2C%20A.%20A%20generalized%20representer%20theorem"
        },
        {
            "id": "42",
            "entry": "[42] V. N. Vapnik. Statistical learning theory. Wiley New York, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20V.N.%20Statistical%20learning%20theory%201998"
        },
        {
            "id": "43",
            "entry": "[43] R. T. Rockafellar. Convex Analysis. Princeton University Press, 1970. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.T.%20Convex%20Analysis%201970"
        }
    ]
}
