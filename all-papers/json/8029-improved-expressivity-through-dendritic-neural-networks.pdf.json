{
    "filename": "8029-improved-expressivity-through-dendritic-neural-networks.pdf",
    "metadata": {
        "title": "Improved Expressivity Through Dendritic Neural Networks",
        "author": "Xundong Wu, Xiangwen Liu, wei li, qing wu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8029-improved-expressivity-through-dendritic-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A typical biological neuron, such as a pyramidal neuron of the neocortex, receives thousands of afferent synaptic inputs on its dendrite tree and sends the efferent axonal output downstream. In typical artificial neural networks, dendrite trees are modeled as linear structures that funnel weighted synaptic inputs to the cell bodies. However, numerous experimental and theoretical studies have shown that dendritic arbors are far more than simple linear accumulators. That is, synaptic inputs can actively modulate their neighboring synaptic activities; therefore, the dendritic structures are highly nonlinear. In this study, we model such local nonlinearity of dendritic trees with our dendritic neural network (DENN) structure and apply this structure to typical machine learning tasks. Equipped with localized nonlinearities, DENNs can attain greater model expressivity than regular neural networks while maintaining efficient network inference. Such strength is evidenced by the increased fitting power when we train DENNs with supervised machine learning tasks. We also empirically show that the locality structure can improve the generalization performance of DENNs, as exemplified by DENNs outranking naive deep neural network architectures when tested on 121 classification tasks from the UCI machine learning repository."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        },
        {
            "term": "piecewise linear",
            "url": "https://en.wikipedia.org/wiki/piecewise_linear"
        },
        {
            "term": "artificial neural networks",
            "url": "https://en.wikipedia.org/wiki/artificial_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "feedforward neural network",
            "url": "https://en.wikipedia.org/wiki/feedforward_neural_network"
        },
        {
            "term": "pyramidal neuron",
            "url": "https://en.wikipedia.org/wiki/pyramidal_neuron"
        }
    ],
    "highlights": [
        "Deep learning algorithms have made remarkable achievements in a vast array of fields over the past few years",
        "Inspired by biological neuronal networks in our brains, originally in the form of a perceptron [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], an artificial neural network unit is typically constructed as a simple weighted sum of synaptic inputs followed by feeding the summation result through an activation function",
        "We show that a dendritic neural network can divide function space into more linear regions than a standard feedforward neural network",
        "We construct a dendritic neural network and a Maxout network to compare against the standard feedforward neural network",
        "We have proposed the dendritic neural network, a neural network architecture constructed to study the advantage that the dendrite tree structure of biological neurons might offer in learning",
        "Through theoretical and empirical studies, we identify that dendritic neural network can provide greater expressivity than traditional neural networks"
    ],
    "key_statements": [
        "Deep learning algorithms have made remarkable achievements in a vast array of fields over the past few years",
        "Inspired by biological neuronal networks in our brains, originally in the form of a perceptron [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], an artificial neural network unit is typically constructed as a simple weighted sum of synaptic inputs followed by feeding the summation result through an activation function",
        "We show that a dendritic neural network can divide function space into more linear regions than a standard feedforward neural network",
        "We construct a dendritic neural network and a Maxout network to compare against the standard feedforward neural network",
        "In addition to evaluating the fitting power of dendritic neural network on image datasets, we evaluate the generalization performance of our dendritic neural network on a collection of 121 machine learning classification tasks from the UCI repository as used in [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>]",
        "We have proposed the dendritic neural network, a neural network architecture constructed to study the advantage that the dendrite tree structure of biological neurons might offer in learning",
        "Through theoretical and empirical studies, we identify that dendritic neural network can provide greater expressivity than traditional neural networks"
    ],
    "summary": [
        "Deep learning algorithms have made remarkable achievements in a vast array of fields over the past few years.",
        "To extract the local and nonlinear nature of dendritic structures, we design our dendritic neural network (DENN) model as shown in Fig. 1(b).",
        "Theorem 3 Universal Approximator Theorem: Any continuous function f can be approximated arbitrarily well on a compact domain C \u2282 Rn0 by a two-layer dendritic neural network, with d1 = 1 and n1 output units in the first layer and d2 \u2264 n1, d2 \u2208 N+ in the second layer with sufficiently large n1 and d2.",
        "Theorem 4 Generalized Universal Approximator Theorem: Any continuous function f can be approximated arbitrarily well on a compact domain C \u2282 Rn0 by a two-layer dendritic neural network, with d1 \u2264 n0 and n1 output units in the first layer and d2 \u2264 n1, d2 \u2208 N+ in the second layer with sufficiently large n1 and d2.",
        "Each branch of the DENN second layer neurons receive a full set of n0 network inputs and weights corresponding to each inputs that are independent of other branches.",
        "We construct a DENN and a Maxout network to compare against the standard FNNs. For a proper comparison, we keep the number of synaptic parameters constant across different architectures.",
        "When we vary the number of dendrite branches d in each neuron, the number of synaptic weights is set to n/d to keep the number of synapses the same as in the standard FNN hidden layer.",
        "In Fig. 2(a) and Fig. 2(c), the lowest training loss values of the DENNs and Maxout networks of different branch numbers d are compared against the lowest loss from standard FNNs with batch normalization or layer normalization are much higher and not shown).",
        "For the DENN layer, we optimize the model architecture over the number of dendritic branches d for each hidden unit.",
        "We have proposed the DENN, a neural network architecture constructed to study the advantage that the dendrite tree structure of biological neurons might offer in learning.",
        "Our DENN model is designed to allow efficient network inference, which is not generally accessible to the typical sparse neural networks due to their built-in irregularity.",
        "While such a sparse structure, noted as intragroup sparsity, was designed for DENNs, it can be extended beyond DENNs to allow multiple network units to share one set of inputs and enable a novel kind of inference efficient sparse network architecture."
    ],
    "headline": "We model such local nonlinearity of dendritic trees with our dendritic neural network  structure and apply this structure to typical machine learning tasks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "2",
            "entry": "[2] Tiago Branco and Michael H\u00e4usser. The single dendritic branch as a fundamental functional unit in the nervous system. Current opinion in neurobiology, 20(4):494\u2013502, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Branco%2C%20Tiago%20H%C3%A4usser%2C%20Michael%20The%20single%20dendritic%20branch%20as%20a%20fundamental%20functional%20unit%20in%20the%20nervous%20system%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Branco%2C%20Tiago%20H%C3%A4usser%2C%20Michael%20The%20single%20dendritic%20branch%20as%20a%20fundamental%20functional%20unit%20in%20the%20nervous%20system%202010"
        },
        {
            "id": "3",
            "entry": "[3] Robert Bryll, Ricardo Gutierrez-Osuna, and Francis Quek. Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets. Pattern recognition, 36(6):1291\u2013 1302, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bryll%2C%20Robert%20Gutierrez-Osuna%2C%20Ricardo%20Quek%2C%20Francis%20Attribute%20bagging%3A%20improving%20accuracy%20of%20classifier%20ensembles%20by%20using%20random%20feature%20subsets%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bryll%2C%20Robert%20Gutierrez-Osuna%2C%20Ricardo%20Quek%2C%20Francis%20Attribute%20bagging%3A%20improving%20accuracy%20of%20classifier%20ensembles%20by%20using%20random%20feature%20subsets%202003"
        },
        {
            "id": "4",
            "entry": "[4] N Alex Cayco-Gajic, Claudia Clopath, and R Angus Silver. Sparse synaptic connectivity is required for decorrelation and pattern separation in feedforward networks. Nature Communications, 8(1):1116, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cayco-Gajic%2C%20N.Alex%20Clopath%2C%20Claudia%20Silver%2C%20R.Angus%20Sparse%20synaptic%20connectivity%20is%20required%20for%20decorrelation%20and%20pattern%20separation%20in%20feedforward%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cayco-Gajic%2C%20N.Alex%20Clopath%2C%20Claudia%20Silver%2C%20R.Angus%20Sparse%20synaptic%20connectivity%20is%20required%20for%20decorrelation%20and%20pattern%20separation%20in%20feedforward%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Dmitri B Chklovskii, BW Mel, and K Svoboda. Cortical rewiring and information storage. Nature, 431(7010):782, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chklovskii%2C%20Dmitri%20B.%20Mel%2C%20B.W.%20Svoboda%2C%20K.%20Cortical%20rewiring%20and%20information%20storage%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chklovskii%2C%20Dmitri%20B.%20Mel%2C%20B.W.%20Svoboda%2C%20K.%20Cortical%20rewiring%20and%20information%20storage%202004"
        },
        {
            "id": "6",
            "entry": "[6] Manuel Fern\u00e1ndez-Delgado, Eva Cernadas, Sen\u00e9n Barro, and Dinani Amorim. Do we need hundreds of classifiers to solve real world classification problems. J. Mach. Learn. Res, 15(1):3133\u20133181, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fern%C3%A1ndez-Delgado%2C%20Manuel%20Cernadas%2C%20Eva%20Barro%2C%20Sen%C3%A9n%20Amorim%2C%20Dinani%20Do%20we%20need%20hundreds%20of%20classifiers%20to%20solve%20real%20world%20classification%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fern%C3%A1ndez-Delgado%2C%20Manuel%20Cernadas%2C%20Eva%20Barro%2C%20Sen%C3%A9n%20Amorim%2C%20Dinani%20Do%20we%20need%20hundreds%20of%20classifiers%20to%20solve%20real%20world%20classification%20problems%202014"
        },
        {
            "id": "7",
            "entry": "[7] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 315\u2013323, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Deep%20sparse%20rectifier%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Deep%20sparse%20rectifier%20neural%20networks%202011"
        },
        {
            "id": "8",
            "entry": "[8] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1302.4389"
        },
        {
            "id": "9",
            "entry": "[9] Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segregated dendrites. eLife, 6, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guerguiev%2C%20Jordan%20Lillicrap%2C%20Timothy%20P.%20and%20Blake%20A%20Richards.%20Towards%20deep%20learning%20with%20segregated%20dendrites%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guerguiev%2C%20Jordan%20Lillicrap%2C%20Timothy%20P.%20and%20Blake%20A%20Richards.%20Towards%20deep%20learning%20with%20segregated%20dendrites%202017"
        },
        {
            "id": "10",
            "entry": "[10] Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. In International Conference on Machine Learning, pages 3059\u20133068, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulcehre%2C%20Caglar%20Moczulski%2C%20Marcin%20Denil%2C%20Misha%20Bengio%2C%20Yoshua%20Noisy%20activation%20functions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulcehre%2C%20Caglar%20Moczulski%2C%20Marcin%20Denil%2C%20Misha%20Bengio%2C%20Yoshua%20Noisy%20activation%20functions%202016"
        },
        {
            "id": "11",
            "entry": "[11] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "12",
            "entry": "[12] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251\u2013257, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20Kurt%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20Kurt%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991"
        },
        {
            "id": "13",
            "entry": "[13] Shaista Hussain, Shih-Chii Liu, and Arindam Basu. Improved margin multi-class classification using dendritic neurons with morphological learning. In Circuits and Systems (ISCAS), 2014 IEEE International Symposium on, pages 2640\u20132643. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hussain%2C%20Shaista%20Liu%2C%20Shih-Chii%20Basu%2C%20Arindam%20Improved%20margin%20multi-class%20classification%20using%20dendritic%20neurons%20with%20morphological%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hussain%2C%20Shaista%20Liu%2C%20Shih-Chii%20Basu%2C%20Arindam%20Improved%20margin%20multi-class%20classification%20using%20dendritic%20neurons%20with%20morphological%20learning%202014"
        },
        {
            "id": "14",
            "entry": "[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "15",
            "entry": "[15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "16",
            "entry": "[16] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Selfnormalizing neural networks. In Advances in Neural Information Processing Systems, pages 972\u2013981, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klambauer%2C%20G%C3%BCnter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Selfnormalizing%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klambauer%2C%20G%C3%BCnter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Selfnormalizing%20neural%20networks%202017"
        },
        {
            "id": "17",
            "entry": "[17] Christof Koch, Tomaso Poggio, and Vincent Torre. Nonlinear interactions in a dendritic tree: localization, timing, and role in information processing. Proceedings of the National Academy of Sciences, 80(9):2799\u20132802, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koch%2C%20Christof%20Poggio%2C%20Tomaso%20Torre%2C%20Vincent%20Nonlinear%20interactions%20in%20a%20dendritic%20tree%3A%20localization%2C%20timing%2C%20and%20role%20in%20information%20processing%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koch%2C%20Christof%20Poggio%2C%20Tomaso%20Torre%2C%20Vincent%20Nonlinear%20interactions%20in%20a%20dendritic%20tree%3A%20localization%2C%20timing%2C%20and%20role%20in%20information%20processing%201983"
        },
        {
            "id": "18",
            "entry": "[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "19",
            "entry": "[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "20",
            "entry": "[20] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng. Efficient sparse coding algorithms. In Advances in neural information processing systems, pages 801\u2013808, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Honglak%20Battle%2C%20Alexis%20Raina%2C%20Rajat%20Ng%2C%20Andrew%20Y.%20Efficient%20sparse%20coding%20algorithms%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Honglak%20Battle%2C%20Alexis%20Raina%2C%20Rajat%20Ng%2C%20Andrew%20Y.%20Efficient%20sparse%20coding%20algorithms%202007"
        },
        {
            "id": "21",
            "entry": "[21] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.08710"
        },
        {
            "id": "22",
            "entry": "[22] Ashok Litwin-Kumar, Kameron Decker Harris, Richard Axel, Haim Sompolinsky, and LF Abbott. Optimal degrees of synaptic connectivity. Neuron, 93(5):1153\u20131164, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Litwin-Kumar%2C%20Ashok%20Harris%2C%20Kameron%20Decker%20Axel%2C%20Richard%20Sompolinsky%2C%20Haim%20Optimal%20degrees%20of%20synaptic%20connectivity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Litwin-Kumar%2C%20Ashok%20Harris%2C%20Kameron%20Decker%20Axel%2C%20Richard%20Sompolinsky%2C%20Haim%20Optimal%20degrees%20of%20synaptic%20connectivity%202017"
        },
        {
            "id": "23",
            "entry": "[23] Attila Losonczy and Jeffrey C Magee. Integrative properties of radial oblique dendrites in hippocampal ca1 pyramidal neurons. Neuron, 50(2):291\u2013307, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Losonczy%2C%20Attila%20Magee%2C%20Jeffrey%20C.%20Integrative%20properties%20of%20radial%20oblique%20dendrites%20in%20hippocampal%20ca1%20pyramidal%20neurons%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Losonczy%2C%20Attila%20Magee%2C%20Jeffrey%20C.%20Integrative%20properties%20of%20radial%20oblique%20dendrites%20in%20hippocampal%20ca1%20pyramidal%20neurons%202006"
        },
        {
            "id": "24",
            "entry": "[24] Attila Losonczy, Judit K Makara, and Jeffrey C Magee. Compartmentalized dendritic plasticity and input feature storage in neurons. Nature, 452(7186):436, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Losonczy%2C%20Attila%20Makara%2C%20Judit%20K.%20Magee%2C%20Jeffrey%20C.%20Compartmentalized%20dendritic%20plasticity%20and%20input%20feature%20storage%20in%20neurons%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Losonczy%2C%20Attila%20Makara%2C%20Judit%20K.%20Magee%2C%20Jeffrey%20C.%20Compartmentalized%20dendritic%20plasticity%20and%20input%20feature%20storage%20in%20neurons%202008"
        },
        {
            "id": "25",
            "entry": "[25] BARTLETT W Mel. Synaptic integration in an excitable dendritic tree. Journal of neurophysiology, 70(3):1086\u20131101, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mel%2C%20B.A.R.T.L.E.T.T.W.%20Synaptic%20integration%20in%20an%20excitable%20dendritic%20tree%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mel%2C%20B.A.R.T.L.E.T.T.W.%20Synaptic%20integration%20in%20an%20excitable%20dendritic%20tree%201993"
        },
        {
            "id": "26",
            "entry": "[26] Hrushikesh Mhaskar, Qianli Liao, and Tomaso A Poggio. When and why are deep networks better than shallow ones? In AAAI, pages 2343\u20132349, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20Hrushikesh%20Liao%2C%20Qianli%20and%20Tomaso%20A%20Poggio.%20When%20and%20why%20are%20deep%20networks%20better%20than%20shallow%20ones%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20Hrushikesh%20Liao%2C%20Qianli%20and%20Tomaso%20A%20Poggio.%20When%20and%20why%20are%20deep%20networks%20better%20than%20shallow%20ones%3F%202017"
        },
        {
            "id": "27",
            "entry": "[27] Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pages 2924\u20132932, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montufar%2C%20Guido%20F.%20Pascanu%2C%20Razvan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montufar%2C%20Guido%20F.%20Pascanu%2C%20Razvan%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20On%20the%20number%20of%20linear%20regions%20of%20deep%20neural%20networks%202014"
        },
        {
            "id": "28",
            "entry": "[28] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 807\u2013814. Omnipress, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "29",
            "entry": "[29] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311\u20133325, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Sparse%20coding%20with%20an%20overcomplete%20basis%20set%3A%20A%20strategy%20employed%20by%20v1%3F%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Sparse%20coding%20with%20an%20overcomplete%20basis%20set%3A%20A%20strategy%20employed%20by%20v1%3F%201997"
        },
        {
            "id": "30",
            "entry": "[30] Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6098"
        },
        {
            "id": "31",
            "entry": "[31] Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5):503\u2013519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poggio%2C%20Tomaso%20Mhaskar%2C%20Hrushikesh%20Rosasco%2C%20Lorenzo%20Miranda%2C%20Brando%20Why%20and%20when%20can%20deep-but%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poggio%2C%20Tomaso%20Mhaskar%2C%20Hrushikesh%20Rosasco%2C%20Lorenzo%20Miranda%2C%20Brando%20Why%20and%20when%20can%20deep-but%20not%20shallow-networks%20avoid%20the%20curse%20of%20dimensionality%3A%20A%20review%202017"
        },
        {
            "id": "32",
            "entry": "[32] Panayiota Poirazi, Terrence Brannon, and Bartlett W Mel. Pyramidal neuron as two-layer neural network. Neuron, 37(6):989\u2013999, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poirazi%2C%20Panayiota%20Brannon%2C%20Terrence%20Mel%2C%20Bartlett%20W.%20Pyramidal%20neuron%20as%20two-layer%20neural%20network%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poirazi%2C%20Panayiota%20Brannon%2C%20Terrence%20Mel%2C%20Bartlett%20W.%20Pyramidal%20neuron%20as%20two-layer%20neural%20network%202003"
        },
        {
            "id": "33",
            "entry": "[33] Panayiota Poirazi and Bartlett W Mel. Impact of active dendrites and structural plasticity on the memory capacity of neural tissue. Neuron, 29(3):779\u2013796, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poirazi%2C%20Panayiota%20Mel%2C%20Bartlett%20W.%20Impact%20of%20active%20dendrites%20and%20structural%20plasticity%20on%20the%20memory%20capacity%20of%20neural%20tissue%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poirazi%2C%20Panayiota%20Mel%2C%20Bartlett%20W.%20Impact%20of%20active%20dendrites%20and%20structural%20plasticity%20on%20the%20memory%20capacity%20of%20neural%20tissue%202001"
        },
        {
            "id": "34",
            "entry": "[34] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In International Conference on Machine Learning, pages 2847\u20132854, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "35",
            "entry": "[35] Gerhard X Ritter and Gonzalo Urcid. Lattice algebra approach to single-neuron computation. IEEE Transactions on Neural Networks, 14(2):282\u2013295, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ritter%2C%20Gerhard%20X.%20Urcid%2C%20Gonzalo%20Lattice%20algebra%20approach%20to%20single-neuron%20computation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ritter%2C%20Gerhard%20X.%20Urcid%2C%20Gonzalo%20Lattice%20algebra%20approach%20to%20single-neuron%20computation%202003"
        },
        {
            "id": "36",
            "entry": "[36] David Rolnick and Max Tegmark. The power of deeper networks for expressing natural functions. arXiv preprint arXiv:1705.05502, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.05502"
        },
        {
            "id": "37",
            "entry": "[37] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenblatt%2C%20Frank%20The%20perceptron%3A%20a%20probabilistic%20model%20for%20information%20storage%20and%20organization%20in%20the%20brain%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosenblatt%2C%20Frank%20The%20perceptron%3A%20a%20probabilistic%20model%20for%20information%20storage%20and%20organization%20in%20the%20brain%201958"
        },
        {
            "id": "38",
            "entry": "[38] Joao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic error backpropagation in deep cortical microcircuits. arXiv preprint arXiv:1801.00062, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00062"
        },
        {
            "id": "39",
            "entry": "[39] Jackie Schiller, Guy Major, Helmut J Koester, and Yitzhak Schiller. Nmda spikes in basal dendrites of cortical pyramidal neurons. Nature, 404(6775):285, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schiller%2C%20Jackie%20Major%2C%20Guy%20Koester%2C%20Helmut%20J.%20Schiller%2C%20Yitzhak%20Nmda%20spikes%20in%20basal%20dendrites%20of%20cortical%20pyramidal%20neurons%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schiller%2C%20Jackie%20Major%2C%20Guy%20Koester%2C%20Helmut%20J.%20Schiller%2C%20Yitzhak%20Nmda%20spikes%20in%20basal%20dendrites%20of%20cortical%20pyramidal%20neurons%202000"
        },
        {
            "id": "40",
            "entry": "[40] Suraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. Training sparse neural networks. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pages 455\u2013462. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srinivas%2C%20Suraj%20Subramanya%2C%20Akshayvarun%20Babu%2C%20R.Venkatesh%20Training%20sparse%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srinivas%2C%20Suraj%20Subramanya%2C%20Akshayvarun%20Babu%2C%20R.Venkatesh%20Training%20sparse%20neural%20networks%202017"
        },
        {
            "id": "41",
            "entry": "[41] Greg Stuart, Nelson Spruston, and Michael H\u00e4usser. Dendrites. Oxford University Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stuart%2C%20Greg%20Spruston%2C%20Nelson%20H%C3%A4usser%2C%20Michael%20Dendrites%202016"
        },
        {
            "id": "42",
            "entry": "[42] Michael Wainberg, Babak Alipanahi, and Brendan J Frey. Are random forests truly the best classifiers? The Journal of Machine Learning Research, 17(1):3837\u20133841, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainberg%2C%20Michael%20Alipanahi%2C%20Babak%20Frey%2C%20Brendan%20J.%20Are%20random%20forests%20truly%20the%20best%20classifiers%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainberg%2C%20Michael%20Alipanahi%2C%20Babak%20Frey%2C%20Brendan%20J.%20Are%20random%20forests%20truly%20the%20best%20classifiers%3F%202016"
        },
        {
            "id": "43",
            "entry": "[43] Shuning Wang. General constructive representations for continuous piecewise-linear functions. IEEE Transactions on Circuits and Systems I: Regular Papers, 51(9):1889\u20131896, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shuning%20General%20constructive%20representations%20for%20continuous%20piecewise-linear%20functions%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shuning%20General%20constructive%20representations%20for%20continuous%20piecewise-linear%20functions%202004"
        },
        {
            "id": "44",
            "entry": "[44] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pages 2074\u2013 2082, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "45",
            "entry": "[45] Xundong E Wu and Bartlett W Mel. Capacity-enhancing synaptic learning rules in a medial temporal lobe online learning model. Neuron, 62(1):31\u201341, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Xundong%20E.%20Mel%2C%20Bartlett%20W.%20Capacity-enhancing%20synaptic%20learning%20rules%20in%20a%20medial%20temporal%20lobe%20online%20learning%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Xundong%20E.%20Mel%2C%20Bartlett%20W.%20Capacity-enhancing%20synaptic%20learning%20rules%20in%20a%20medial%20temporal%20lobe%20online%20learning%20model%202009"
        },
        {
            "id": "46",
            "entry": "[46] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "47",
            "entry": "[47] Rafael Yuste. Dendritic spines and distributed circuits. Neuron, 71(5):772\u2013781, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuste%2C%20Rafael%20Dendritic%20spines%20and%20distributed%20circuits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuste%2C%20Rafael%20Dendritic%20spines%20and%20distributed%20circuits%202011"
        },
        {
            "id": "48",
            "entry": "[48] Thomas Zaslavsky. Facing up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes: Face-count Formulas for Partitions of Space by Hyperplanes, volume 154. American Mathematical Soc., 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zaslavsky%2C%20Thomas%20Facing%20up%20to%20Arrangements%3A%20Face-Count%20Formulas%20for%20Partitions%20of%20Space%20by%20Hyperplanes%3A%20Face-count%20Formulas%20for%20Partitions%20of%20Space%20by%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zaslavsky%2C%20Thomas%20Facing%20up%20to%20Arrangements%3A%20Face-Count%20Formulas%20for%20Partitions%20of%20Space%20by%20Hyperplanes%3A%20Face-count%20Formulas%20for%20Partitions%20of%20Space%20by%201975"
        },
        {
            "id": "49",
            "entry": "[49] Gabriele Zenobi and Padraig Cunningham. Using diversity in preparing ensembles of classifiers based on different feature subsets to minimize generalization error. In European Conference on Machine Learning, pages 576\u2013587. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zenobi%2C%20Gabriele%20Cunningham%2C%20Padraig%20Using%20diversity%20in%20preparing%20ensembles%20of%20classifiers%20based%20on%20different%20feature%20subsets%20to%20minimize%20generalization%20error",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zenobi%2C%20Gabriele%20Cunningham%2C%20Padraig%20Using%20diversity%20in%20preparing%20ensembles%20of%20classifiers%20based%20on%20different%20feature%20subsets%20to%20minimize%20generalization%20error"
        }
    ]
}
