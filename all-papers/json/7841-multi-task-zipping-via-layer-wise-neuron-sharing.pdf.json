{
    "filename": "7841-multi-task-zipping-via-layer-wise-neuron-sharing.pdf",
    "metadata": {
        "title": "Multi-Task Zipping via Layer-wise Neuron Sharing",
        "author": "Xiaoxi He, Zimu Zhou, Lothar Thiele",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7841-multi-task-zipping-via-layer-wise-neuron-sharing.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Future mobile devices are anticipated to perceive, understand and react to the world on their own by running multiple correlated deep neural networks on-device. Yet the complexity of these neural networks needs to be trimmed down both withinmodel and cross-model to fit in mobile storage and memory. Previous studies squeeze the redundancy within a single model. In this work, we aim to reduce the redundancy across multiple models. We propose Multi-Task Zipping (MTZ), a framework to automatically merge correlated, pre-trained deep neural networks for cross-model compression. Central in MTZ is a layer-wise neuron sharing and incoming weight updating scheme that induces a minimal change in the error function. MTZ inherits information from each model and demands light retraining to re-boost the accuracy of individual tasks. Evaluations show that MTZ is able to fully merge the hidden layers of two VGG-16 networks with a 3.18% increase in the test error averaged on ImageNet and CelebA, or share 39.61% parameters between the two networks with < 0.5% increase in the test errors for both tasks. The number of iterations to retrain the combined network is at least 17.8\u00d7 lower than that of training a single VGG-16 network. Moreover, experiments show that MTZ is also able to effectively merge multiple residual networks."
    },
    "keywords": [
        {
            "term": "mobile device",
            "url": "https://en.wikipedia.org/wiki/mobile_device"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "Multi-task learning",
            "url": "https://en.wikipedia.org/wiki/Multi-task_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "AI-powered mobile applications increasingly demand multiple deep neural networks for correlated tasks to be performed continuously and concurrently on resource-constrained devices such as wearables, smartphones, self-driving cars, and drones [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "We propose Multi-Task Zipping (MTZ), a framework to automatically and adaptively merge correlated, well-trained deep neural networks for cross-model compression via neuron sharing",
        "residual networks are much deeper and have more complex topology compared to VGG-16, Multi-Task Zipping is still able to effectively reduce the overall number of parameters, while retaining the accuracy on each task",
        "We propose Multi-Task Zipping, a framework to automatically merge multiple correlated, well-trained deep neural networks for cross-model compression via neuron sharing",
        "It selectively shares neurons and optimally updates their incoming weights on a layer basis to minimize the errors induced to each individual task",
        "Evaluations show that Multi-Task Zipping can fully merge two VGG-16 networks with an error increase of 3.76% and 2.59% on ImageNet for object classification and on CelebA for facial attribute classification, or share 39.61% parameters between the two models with < 0.5% error increase"
    ],
    "key_statements": [
        "AI-powered mobile applications increasingly demand multiple deep neural networks for correlated tasks to be performed continuously and concurrently on resource-constrained devices such as wearables, smartphones, self-driving cars, and drones [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "We propose Multi-Task Zipping (MTZ), a framework to automatically and adaptively merge correlated, well-trained deep neural networks for cross-model compression via neuron sharing",
        "The contributions and results of this work are as follows. We propose Multi-Task Zipping, a framework that automatically merges multiple correlated, pre-trained deep neural networks",
        "Multi-Task Zipping manages to share 39.61% parameters between the two VGG-16 networks pre-trained for object detection and facial attribute classification, while incurring less than 0.5% increase in test errors",
        "The memory footprint of a neural network can be further reduced by lowering the precision of parameters [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].Unlike previous research that deals with the intra-redundancy of a single network, our work reduces the inter-redundancy among multiple networks",
        "We extend Multi-Task Zipping to convolutional (CONV) layers in Sec. 3.5, sparse layers in Sec. 3.6 and residual networks (ResNets) in Sec. 3.7",
        "Since the pre-trained neural networks may have already been sparsified via weight pruning, we extend Multi-Task Zipping to support sparse models",
        "When all the 300 neurons in the first hidden layers are shared, there is an increase of 0.95% in test error even without retraining, while random sharing induces an error of 33.47%",
        "We experiment Multi-Task Zipping to fully merge the hidden layers in the two LeNet-300-100 networks without any retraining i.e., without line 10 in Algorithm 1",
        "The averaged test error increases by only 1.50%",
        "When 100% parameters of all hidden layers are shared between the two models, the joint model yields test errors of 14.07% on ImageNet and 11.09% on CelebA, i.e., increases of 3.76% and 2.59% in the original test errors",
        "Performance to Zip Multiple Networks (ResNets) Pre-trained for Different Tasks. This experiment shows the performance of Multi-Task Zipping to merge more than two neural networks for different tasks, where the model for each task is pre-trained using deeper architectures such as residual networks",
        "residual networks are much deeper and have more complex topology compared to VGG-16, Multi-Task Zipping is still able to effectively reduce the overall number of parameters, while retaining the accuracy on each task",
        "We propose Multi-Task Zipping, a framework to automatically merge multiple correlated, well-trained deep neural networks for cross-model compression via neuron sharing",
        "It selectively shares neurons and optimally updates their incoming weights on a layer basis to minimize the errors induced to each individual task",
        "Evaluations show that Multi-Task Zipping can fully merge two VGG-16 networks with an error increase of 3.76% and 2.59% on ImageNet for object classification and on CelebA for facial attribute classification, or share 39.61% parameters between the two models with < 0.5% error increase"
    ],
    "summary": [
        "AI-powered mobile applications increasingly demand multiple deep neural networks for correlated tasks to be performed continuously and concurrently on resource-constrained devices such as wearables, smartphones, self-driving cars, and drones [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "We propose Multi-Task Zipping (MTZ), a framework to automatically and adaptively merge correlated, well-trained deep neural networks for cross-model compression via neuron sharing.",
        "Experiments show that MTZ is able to merge all the hidden layers of two LeNet networks [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] without increase in test errors.",
        "MTZ manages to share 39.61% parameters between the two VGG-16 networks pre-trained for object detection and facial attribute classification, while incurring less than 0.5% increase in test errors.",
        "Different to the above methods, MTZ inherits the parameters directly from each pre-trained network when optimizing the neurons shared among tasks in each layer and demands light retraining.",
        "Algorithm 1: Multi-task Zipping via Layer-wise Neuron Sharing input :{WlA}, {WlB}: weight matrices of M A and M B XA, XB: training datum of task A and B",
        "We experiment MTZ to fully merge the hidden layers in the two LeNet-300-100 networks without any retraining i.e., without line 10 in Algorithm 1.",
        "MTZ consistently achieves lossless network zipping on fully connected and convolutional networks, either they are dense or sparse, with 100% parameters of hidden layers shared.",
        "This experiment evaluates the performance of MTZ to automatically share information among two neural networks for different tasks.",
        "We investigate: (i) what the accuracy loss is when all hidden layers of two models for different tasks are fully shared; how much neurons and parameters can be shared between the two models by MTZ with at most 0.5% increase in test errors allowed.",
        "We directly adopt the pre-trained weights from the original VGG-16 model [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] for the object classification task, which has a 10.31% error in our evaluation.",
        "Each pair of layers in the two models are adaptively merged using MTZ allowing an increase (< 0.5%) in test errors on the two datasets.",
        "This experiment shows the performance of MTZ to merge more than two neural networks for different tasks, where the model for each task is pre-trained using deeper architectures such as ResNets.",
        "We propose MTZ, a framework to automatically merge multiple correlated, well-trained deep neural networks for cross-model compression via neuron sharing.",
        "It selectively shares neurons and optimally updates their incoming weights on a layer basis to minimize the errors induced to each individual task.",
        "Evaluations show that MTZ can fully merge two VGG-16 networks with an error increase of 3.76% and 2.59% on ImageNet for object classification and on CelebA for facial attribute classification, or share 39.61% parameters between the two models with < 0.5% error increase.",
        "We plan to further investigate the integration of MTZ with weight pruning in the future"
    ],
    "headline": "We propose Multi-Task Zipping , a framework to automatically merge correlated, pre-trained deep neural networks for cross-model compression",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Rich Caruana. Multitask learning. Machine Learning, 28(1):41\u201375, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20Rich%20Multitask%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20Rich%20Multitask%20learning%201997"
        },
        {
            "id": "2",
            "entry": "[2] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Proceedings of Advances in Neural Information Processing Systems, pages 3123\u20133131, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courbariaux%2C%20Matthieu%20Bengio%2C%20Yoshua%20David%2C%20Jean-Pierre%20Binaryconnect%3A%20Training%20deep%20neural%20networks%20with%20binary%20weights%20during%20propagations%202015"
        },
        {
            "id": "3",
            "entry": "[3] Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep learning. In Proceedings of Advances in Neural Information Processing Systems, pages 2148\u20132156, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "4",
            "entry": "[4] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layerwise optimal brain surgeon. In Proceedings of Advances in Neural Information Processing Systems, pages 4860\u20134874, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20Xin%20Chen%2C%20Shangyu%20Pan%2C%20Sinno%20Learning%20to%20prune%20deep%20neural%20networks%20via%20layerwise%20optimal%20brain%20surgeon%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20Xin%20Chen%2C%20Shangyu%20Pan%2C%20Sinno%20Learning%20to%20prune%20deep%20neural%20networks%20via%20layerwise%20optimal%20brain%20surgeon%202017"
        },
        {
            "id": "5",
            "entry": "[5] Petko Georgiev, Sourav Bhattacharya, Nicholas D Lane, and Cecilia Mascolo. Low-resource multi-task audio sensing for mobile and embedded devices via shared deep neural network representations. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 1(3):50:1\u201350:19, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Georgiev%2C%20Petko%20Bhattacharya%2C%20Sourav%20Lane%2C%20Nicholas%20D.%20Mascolo%2C%20Cecilia%20Low-resource%20multi-task%20audio%20sensing%20for%20mobile%20and%20embedded%20devices%20via%20shared%20deep%20neural%20network%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Georgiev%2C%20Petko%20Bhattacharya%2C%20Sourav%20Lane%2C%20Nicholas%20D.%20Mascolo%2C%20Cecilia%20Low-resource%20multi-task%20audio%20sensing%20for%20mobile%20and%20embedded%20devices%20via%20shared%20deep%20neural%20network%20representations%202017"
        },
        {
            "id": "6",
            "entry": "[6] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Proceedings of Advances In Neural Information Processing Systems, pages 1379\u20131387, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016"
        },
        {
            "id": "7",
            "entry": "[7] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In Proceedings of International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016"
        },
        {
            "id": "8",
            "entry": "[8] Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Proceedings of Advances in Neural Information Processing Systems, pages 164\u2013171, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassibi%2C%20Babak%20Stork%2C%20David%20G.%20Second%20order%20derivatives%20for%20network%20pruning%3A%20Optimal%20brain%20surgeon%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassibi%2C%20Babak%20Stork%2C%20David%20G.%20Second%20order%20derivatives%20for%20network%20pruning%3A%20Optimal%20brain%20surgeon%201993"
        },
        {
            "id": "9",
            "entry": "[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "10",
            "entry": "[10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "11",
            "entry": "[11] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.03250"
        },
        {
            "id": "12",
            "entry": "[12] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "13",
            "entry": "[13] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20Brenden%20M.%20Salakhutdinov%2C%20Ruslan%20Tenenbaum%2C%20Joshua%20B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "14",
            "entry": "[14] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "15",
            "entry": "[15] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Proceedings of Advances in neural information processing systems, pages 598\u2013605, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Denker%2C%20John%20S.%20Solla%2C%20Sara%20A.%20Optimal%20brain%20damage%201990"
        },
        {
            "id": "16",
            "entry": "[16] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of IEEE International Conference on Computer Vision, pages 3730\u20133738, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "17",
            "entry": "[17] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rogerio Feris. Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 5334\u20135343, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Yongxi%20Kumar%2C%20Abhishek%20Zhai%2C%20Shuangfei%20Cheng%2C%20Yu%20Fully-adaptive%20feature%20sharing%20in%20multi-task%20networks%20with%20applications%20in%20person%20attribute%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Yongxi%20Kumar%2C%20Abhishek%20Zhai%2C%20Shuangfei%20Cheng%2C%20Yu%20Fully-adaptive%20feature%20sharing%20in%20multi-task%20networks%20with%20applications%20in%20person%20attribute%20classification%202017"
        },
        {
            "id": "18",
            "entry": "[18] Akhil Mathur, Nicholas D Lane, Sourav Bhattacharya, Aidan Boran, Claudio Forlivesi, and Fahim Kawsar. Deepeye: Resource efficient local execution of multiple deep vision models using wearable commodity hardware. In Proceedings of ACM Annual International Conference on Mobile Systems, Applications, and Services, pages 68\u201381, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathur%2C%20Akhil%20Lane%2C%20Nicholas%20D.%20Bhattacharya%2C%20Sourav%20Boran%2C%20Aidan%20Deepeye%3A%20Resource%20efficient%20local%20execution%20of%20multiple%20deep%20vision%20models%20using%20wearable%20commodity%20hardware%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathur%2C%20Akhil%20Lane%2C%20Nicholas%20D.%20Bhattacharya%2C%20Sourav%20Boran%2C%20Aidan%20Deepeye%3A%20Resource%20efficient%20local%20execution%20of%20multiple%20deep%20vision%20models%20using%20wearable%20commodity%20hardware%202017"
        },
        {
            "id": "19",
            "entry": "[19] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3994\u20134003, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Ishan%20Shrivastava%2C%20Abhinav%20Gupta%2C%20Abhinav%20Hebert%2C%20Martial%20Cross-stitch%20networks%20for%20multi-task%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Ishan%20Shrivastava%2C%20Abhinav%20Gupta%2C%20Abhinav%20Hebert%2C%20Martial%20Cross-stitch%20networks%20for%20multi-task%20learning%202016"
        },
        {
            "id": "20",
            "entry": "[20] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "21",
            "entry": "[21] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In Proceedings of Advances in Neural Information Processing Systems, pages 506\u2013516, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20Sylvestre-Alvise%20Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Learning%20multiple%20visual%20domains%20with%20residual%20adapters%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20Sylvestre-Alvise%20Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Learning%20multiple%20visual%20domains%20with%20residual%20adapters%202017"
        },
        {
            "id": "22",
            "entry": "[22] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6550"
        },
        {
            "id": "23",
            "entry": "[23] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a single image without facial landmarks. International Journal of Computer Vision, 126(2-4):144\u2013157, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rothe%2C%20Rasmus%20Timofte%2C%20Radu%20Gool%2C%20Luc%20Van%20Deep%20expectation%20of%20real%20and%20apparent%20age%20from%20a%20single%20image%20without%20facial%20landmarks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rothe%2C%20Rasmus%20Timofte%2C%20Radu%20Gool%2C%20Luc%20Van%20Deep%20expectation%20of%20real%20and%20apparent%20age%20from%20a%20single%20image%20without%20facial%20landmarks%202018"
        },
        {
            "id": "24",
            "entry": "[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "25",
            "entry": "[25] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "26",
            "entry": "[26] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.0402"
        },
        {
            "id": "27",
            "entry": "[27] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural Networks, 32:323\u2013332, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stallkamp%2C%20Johannes%20Schlipsing%2C%20Marc%20Salmen%2C%20Jan%20Igel%2C%20Christian%20Man%20vs.%20computer%3A%20Benchmarking%20machine%20learning%20algorithms%20for%20traffic%20sign%20recognition%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stallkamp%2C%20Johannes%20Schlipsing%2C%20Marc%20Salmen%2C%20Jan%20Igel%2C%20Christian%20Man%20vs.%20computer%3A%20Benchmarking%20machine%20learning%20algorithms%20for%20traffic%20sign%20recognition%202012"
        },
        {
            "id": "28",
            "entry": "[28] Yongxin Yang and Timothy Hospedales. Deep multi-task representation learning: A tensor factorisation approach. In Proceedings of International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yongxin%20Hospedales%2C%20Timothy%20Deep%20multi-task%20representation%20learning%3A%20A%20tensor%20factorisation%20approach%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yongxin%20Hospedales%2C%20Timothy%20Deep%20multi-task%20representation%20learning%3A%20A%20tensor%20factorisation%20approach%202016"
        },
        {
            "id": "29",
            "entry": "[29] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Proceedings of Advances in Neural Information Processing Systems, pages 3320\u20133328, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "30",
            "entry": "[30] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "31",
            "entry": "[31] Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1707.08114"
        }
    ]
}
