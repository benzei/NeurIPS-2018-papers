{
    "filename": "7789-policy-optimization-via-importance-sampling.pdf",
    "metadata": {
        "date": 2018,
        "title": "Policy Optimization via Importance Sampling",
        "author": "Alberto Maria Metelli Politecnico di Milano, Milan, Italy albertomaria.metelli@polimi.it",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7789-policy-optimization-via-importance-sampling.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating online and offline optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial, as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel, model-free, policy search algorithm, POIS, applicable in both action-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation; then we define a surrogate objective function, which is optimized offline whenever a new batch of trajectories is collected. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods."
    },
    "keywords": [
        {
            "term": "importance sampling",
            "url": "https://en.wikipedia.org/wiki/importance_sampling"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "Effective Sample Size",
            "url": "https://en.wikipedia.org/wiki/Effective_Sample_Size"
        },
        {
            "term": "POIS",
            "url": "https://en.wikipedia.org/wiki/POIS"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        }
    ],
    "highlights": [
        "Policy search methods [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] have proved to be valuable Reinforcement Learning (RL) [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] approaches thanks to their successful achievements in continuous control tasks [e.g., 22, 41, 43, 42], robotic locomotion [e.g., 52, 19] and partially observable environments [e.g., 27]",
        "We propose two versions of Policy Optimization via Importance Sampling: Action-based Policy Optimization via Importance Sampling (A-Policy Optimization via Importance Sampling), which is based on a policy gradient approach, and Parameter-based Policy Optimization via Importance Sampling (P-Policy Optimization via Importance Sampling), which adopts the Policy Gradients with Parameter-based Exploration framework",
        "We presented a new actor-only policy optimization algorithm, Policy Optimization via Importance Sampling, which alternates online and offline optimization in order to efficiently exploit the collected trajectories, and can be used in combination with action-based and parameter-based exploration",
        "In contrast to the state-ofthe-art algorithms, Policy Optimization via Importance Sampling has a strong theoretical grounding, since its surrogate objective function derives from a statistical bound on the estimated performance, that is able to capture the uncertainty induced by importance sampling",
        "The experimental evaluation showed that Policy Optimization via Importance Sampling, in both its versions, is able to achieve a performance comparable with Trust Region Policy Optimization, Policy Optimization and other classical algorithms on continuous control tasks",
        "Natural extensions of Policy Optimization via Importance Sampling could focus on employing per-decision importance sampling, adaptive batch size, and trajectory reuse"
    ],
    "key_statements": [
        "Policy search methods [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] have proved to be valuable Reinforcement Learning (RL) [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] approaches thanks to their successful achievements in continuous control tasks [e.g., 22, 41, 43, 42], robotic locomotion [e.g., 52, 19] and partially observable environments [e.g., 27]",
        "The former, usually known as policy gradient (PG) methods, perform a search in a parametric policy space by following the gradient of the utility function estimated by means of a batch of trajectories collected from the environment [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>]",
        "Online policy gradient methods are likely the most widespread policy search approaches: starting from the traditional algorithms based on stochastic policy gradient [<a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>], like REINFORCE [<a class=\"ref-link\" id=\"c63\" href=\"#r63\">63</a>] and G(PO)Markov Decision Process [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], moving toward more modern methods, such as Trust Region Policy Optimization (TRPO) [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>]",
        "The approach has been extended to Deterministic Policy Gradient (DPG) [<a class=\"ref-link\" id=\"c46\" href=\"#r46\">46</a>], which allows optimizing deterministic policies while keeping a stochastic policy for exploration.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "Trust Region Policy Optimization and Policy Optimization, together with Deep Deterministic Policy Gradient, represent the state-of-the-art policy optimization methods in Reinforcement Learning for continuous control, they do not explicitly encode in their objective function the uncertainty injected by the importance sampling procedure",
        "We propose a novel, model-free, actor-only, policy optimization algorithm, named Policy Optimization via Importance Sampling (POIS) that mixes online and offline optimization to efficiently exploit the information contained in the collected trajectories",
        "After revising some notions about importance sampling (Section 3), we propose a concentration inequality, of independent interest, for high-confidence \u201coff-distribution\u201d optimization of objective functions estimated via importance sampling (Section 4)",
        "We discuss how to customize the bound provided in Theorem 4.1 for policy optimization, developing a novel model-free actor-only policy search algorithm, named Policy Optimization via Importance Sampling (POIS)",
        "We propose two versions of Policy Optimization via Importance Sampling: Action-based Policy Optimization via Importance Sampling (A-Policy Optimization via Importance Sampling), which is based on a policy gradient approach, and Parameter-based Policy Optimization via Importance Sampling (P-Policy Optimization via Importance Sampling), which adopts the Policy Gradients with Parameter-based Exploration framework",
        "We first provide a set of empirical comparisons on classical continuous control tasks with linearly parametrized policies; we show how Policy Optimization via Importance Sampling can be adopted for learning deep neural policies",
        "We presented a new actor-only policy optimization algorithm, Policy Optimization via Importance Sampling, which alternates online and offline optimization in order to efficiently exploit the collected trajectories, and can be used in combination with action-based and parameter-based exploration",
        "In contrast to the state-ofthe-art algorithms, Policy Optimization via Importance Sampling has a strong theoretical grounding, since its surrogate objective function derives from a statistical bound on the estimated performance, that is able to capture the uncertainty induced by importance sampling",
        "The experimental evaluation showed that Policy Optimization via Importance Sampling, in both its versions, is able to achieve a performance comparable with Trust Region Policy Optimization, Policy Optimization and other classical algorithms on continuous control tasks",
        "Natural extensions of Policy Optimization via Importance Sampling could focus on employing per-decision importance sampling, adaptive batch size, and trajectory reuse"
    ],
    "summary": [
        "Policy search methods [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] have proved to be valuable Reinforcement Learning (RL) [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] approaches thanks to their successful achievements in continuous control tasks [e.g., 22, 41, 43, 42], robotic locomotion [e.g., 52, 19] and partially observable environments [e.g., 27].",
        "Off-line optimization, introduces further sources of approximation, as the gradient w.r.t. the target policy needs to be estimated with samples collected with a behavioral policy.",
        "This idea has been captured by TRPO, which optimizes via natural gradient a surrogate objective function, derived from safe RL [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>], subject to a constraint on the KullbackLeibler divergence between the behavioral and target policy.2 PPO performs a truncation of the importance weights to discourage the optimization process from going too far.",
        "TRPO and PPO, together with DDPG, represent the state-of-the-art policy optimization methods in RL for continuous control, they do not explicitly encode in their objective function the uncertainty injected by the importance sampling procedure.",
        "We propose a novel, model-free, actor-only, policy optimization algorithm, named Policy Optimization via Importance Sampling (POIS) that mixes online and offline optimization to efficiently exploit the information contained in the collected trajectories.",
        "Fully empirical concentration inequalities, like Student-T, besides the asymptotic approximation, are not suitable in this case since the empirical variance needs to be estimated with importance sampling as well injecting further uncertainty [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>].",
        "We discuss how to customize the bound provided in Theorem 4.1 for policy optimization, developing a novel model-free actor-only policy search algorithm, named Policy Optimization via Importance Sampling (POIS).",
        "The importance weights [<a class=\"ref-link\" id=\"c66\" href=\"#r66\">66</a>] must take into account all sources of randomness, derived from sampling a policy parameter \u03b8 and a trajectory \u03c4: w\u03c1",
        "A first advantage over the action-based setting is that the distribution of the importance weights is entirely known, as it is the ratio of two Gaussians and the R\u00e9nyi divergence d2 can be computed exactly [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "This problem can be mitigated in POIS by resorting to per-decision importance sampling [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>], in which the weight is assigned to individual rewards instead of trajectory returns.",
        "We presented a new actor-only policy optimization algorithm, POIS, which alternates online and offline optimization in order to efficiently exploit the collected trajectories, and can be used in combination with action-based and parameter-based exploration.",
        "In contrast to the state-ofthe-art algorithms, POIS has a strong theoretical grounding, since its surrogate objective function derives from a statistical bound on the estimated performance, that is able to capture the uncertainty induced by importance sampling.",
        "The experimental evaluation showed that POIS, in both its versions, is able to achieve a performance comparable with TRPO, PPO and other classical algorithms on continuous control tasks."
    ],
    "headline": "We propose a novel, model-free, policy search algorithm, Policy Optimization via Importance Sampling, applicable in both action-based and parameter-based settings",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-Ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20Shun-Ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "2",
            "entry": "[2] Shun-ichi Amari. Differential-geometrical methods in statistics, volume 28. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-ichi%20Differential-geometrical%20methods%20in%20statistics%2C%20volume%2028%202012"
        },
        {
            "id": "3",
            "entry": "[3] Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15:319\u2013350, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baxter%2C%20Jonathan%20Bartlett%2C%20Peter%20L.%20Infinite-horizon%20policy-gradient%20estimation%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baxter%2C%20Jonathan%20Bartlett%2C%20Peter%20L.%20Infinite-horizon%20policy-gradient%20estimation%202001"
        },
        {
            "id": "4",
            "entry": "[4] Bernard Bercu, Bernard Delyon, and Emmanuel Rio. Concentration inequalities for sums. In Concentration Inequalities for Sums and Martingales, pages 11\u201360.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bercu%2C%20Bernard%20Delyon%2C%20Bernard%20Rio%2C%20Emmanuel%20Concentration%20inequalities%20for%20sums",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bercu%2C%20Bernard%20Delyon%2C%20Bernard%20Rio%2C%20Emmanuel%20Concentration%20inequalities%20for%20sums"
        },
        {
            "id": "5",
            "entry": "[5] Jacob Burbea. The convexity with respect to gaussian distributions of divergences of order \u03b1. Utilitas Mathematica, 26:171\u2013192, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burbea%2C%20Jacob%20The%20convexity%20with%20respect%20to%20gaussian%20distributions%20of%20divergences%20of%20order%20%CE%B1%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burbea%2C%20Jacob%20The%20convexity%20with%20respect%20to%20gaussian%20distributions%20of%20divergences%20of%20order%20%CE%B1%201984"
        },
        {
            "id": "6",
            "entry": "[6] William G Cochran. Sampling techniques. John Wiley & Sons, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cochran%2C%20William%20G.%20Sampling%20techniques%202007"
        },
        {
            "id": "7",
            "entry": "[7] Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Advances in neural information processing systems, pages 442\u2013450, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Learning%20bounds%20for%20importance%20weighting%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Learning%20bounds%20for%20importance%20weighting%202010"
        },
        {
            "id": "8",
            "entry": "[8] Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1205.4839"
        },
        {
            "id": "9",
            "entry": "[9] Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1\u20132):1\u2013142, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20Marc%20Peter%20Neumann%2C%20Gerhard%20Peters%2C%20Jan%20A%20survey%20on%20policy%20search%20for%20robotics%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20Marc%20Peter%20Neumann%2C%20Gerhard%20Peters%2C%20Jan%20A%20survey%20on%20policy%20search%20for%20robotics%202013"
        },
        {
            "id": "10",
            "entry": "[10] Shayan Doroudi, Philip S Thomas, and Emma Brunskill. Importance sampling for fair policy selection. UAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doroudi%2C%20Shayan%20Thomas%2C%20Philip%20S.%20Brunskill%2C%20Emma%20Importance%20sampling%20for%20fair%20policy%20selection%202017"
        },
        {
            "id": "11",
            "entry": "[11] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u20131338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "12",
            "entry": "[12] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "13",
            "entry": "[13] Mandy Gr\u00fcttner, Frank Sehnke, Tom Schaul, and J\u00fcrgen Schmidhuber. Multi-dimensional deep memory go-player for parameter exploring policy gradients.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandy%20Gr%C3%BCttner%2C%20Frank%20Sehnke%2C%20Tom%20Schaul%20Schmidhuber%2C%20J%C3%BCrgen%20Multi-dimensional%20deep%20memory%20go-player%20for%20parameter%20exploring%20policy%20gradients"
        },
        {
            "id": "14",
            "entry": "[14] Zhaohan Guo, Philip S Thomas, and Emma Brunskill. Using options and covariance testing for long horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems, pages 2489\u20132498, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Zhaohan%20Thomas%2C%20Philip%20S.%20Brunskill%2C%20Emma%20Using%20options%20and%20covariance%20testing%20for%20long%20horizon%20off-policy%20policy%20evaluation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Zhaohan%20Thomas%2C%20Philip%20S.%20Brunskill%2C%20Emma%20Using%20options%20and%20covariance%20testing%20for%20long%20horizon%20off-policy%20policy%20evaluation%202017"
        },
        {
            "id": "15",
            "entry": "[15] Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary computation, 9(2):159\u2013195, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20Nikolaus%20Ostermeier%2C%20Andreas%20Completely%20derandomized%20self-adaptation%20in%20evolution%20strategies%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20Nikolaus%20Ostermeier%2C%20Andreas%20Completely%20derandomized%20self-adaptation%20in%20evolution%20strategies%202001"
        },
        {
            "id": "16",
            "entry": "[16] Timothy Classen Hesterberg. Advances in importance sampling. PhD thesis, Stanford University, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hesterberg%2C%20Timothy%20Classen%20Advances%20in%20importance%20sampling%201988"
        },
        {
            "id": "17",
            "entry": "[17] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML, volume 2, pages 267\u2013274, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002"
        },
        {
            "id": "18",
            "entry": "[18] Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pages 1531\u20131538, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20A%20natural%20policy%20gradient%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20A%20natural%20policy%20gradient%202002"
        },
        {
            "id": "19",
            "entry": "[19] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jens%20Kober%2C%20J.Andrew%20Bagnell%20Peters%2C%20Jan%20Reinforcement%20learning%20in%20robotics%3A%20A%20survey%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jens%20Kober%2C%20J.Andrew%20Bagnell%20Peters%2C%20Jan%20Reinforcement%20learning%20in%20robotics%3A%20A%20survey%202013"
        },
        {
            "id": "20",
            "entry": "[20] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pages 1008\u20131014, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konda%2C%20Vijay%20R.%20Tsitsiklis%2C%20John%20N.%20Actor-critic%20algorithms%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konda%2C%20Vijay%20R.%20Tsitsiklis%2C%20John%20N.%20Actor-critic%20algorithms%202000"
        },
        {
            "id": "21",
            "entry": "[21] Augustine Kong. A note on importance sampling using standardized weights. University of Chicago, Dept. of Statistics, Tech. Rep, 348, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kong%2C%20Augustine%20A%20note%20on%20importance%20sampling%20using%20standardized%20weights%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kong%2C%20Augustine%20A%20note%20on%20importance%20sampling%20using%20standardized%20weights%201992"
        },
        {
            "id": "22",
            "entry": "[22] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "23",
            "entry": "[23] Luca Martino, V\u00edctor Elvira, and Francisco Louzada. Effective sample size for importance sampling based on discrepancy measures. Signal Processing, 131:386\u2013401, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martino%2C%20Luca%20Elvira%2C%20V%C3%ADctor%20Louzada%2C%20Francisco%20Effective%20sample%20size%20for%20importance%20sampling%20based%20on%20discrepancy%20measures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martino%2C%20Luca%20Elvira%2C%20V%C3%ADctor%20Louzada%2C%20Francisco%20Effective%20sample%20size%20for%20importance%20sampling%20based%20on%20discrepancy%20measures%202017"
        },
        {
            "id": "24",
            "entry": "[24] Takamitsu Matsubara, Tetsuro Morimura, and Jun Morimoto. Adaptive step-size policy gradients with average reward metric. In Proceedings of 2nd Asian Conference on Machine Learning, pages 285\u2013298, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matsubara%2C%20Takamitsu%20Morimura%2C%20Tetsuro%20Morimoto%2C%20Jun%20Adaptive%20step-size%20policy%20gradients%20with%20average%20reward%20metric%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matsubara%2C%20Takamitsu%20Morimura%2C%20Tetsuro%20Morimoto%2C%20Jun%20Adaptive%20step-size%20policy%20gradients%20with%20average%20reward%20metric%202010"
        },
        {
            "id": "25",
            "entry": "[25] Atsushi Miyamae, Yuichi Nagata, Isao Ono, and Shigenobu Kobayashi. Natural policy gradient methods with parameter-based exploration for control tasks. In Advances in neural information processing systems, pages 1660\u20131668, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyamae%2C%20Atsushi%20Nagata%2C%20Yuichi%20Ono%2C%20Isao%20Kobayashi%2C%20Shigenobu%20Natural%20policy%20gradient%20methods%20with%20parameter-based%20exploration%20for%20control%20tasks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyamae%2C%20Atsushi%20Nagata%2C%20Yuichi%20Ono%2C%20Isao%20Kobayashi%2C%20Shigenobu%20Natural%20policy%20gradient%20methods%20with%20parameter-based%20exploration%20for%20control%20tasks%202010"
        },
        {
            "id": "26",
            "entry": "[26] R\u00e9mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pages 1054\u20131062, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R%C3%A9mi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R%C3%A9mi%20Stepleton%2C%20Tom%20Harutyunyan%2C%20Anna%20Bellemare%2C%20Marc%20Safe%20and%20efficient%20off-policy%20reinforcement%20learning%202016"
        },
        {
            "id": "27",
            "entry": "[27] Andrew Y Ng and Michael Jordan. Pegasus: A policy search method for large mdps and pomdps. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 406\u2013415. Morgan Kaufmann Publishers Inc., 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Andrew%20Y%20Ng%20and%20Michael%20Jordan.%20Pegasus%3A%20A%20policy%20search%20method%20for%20large%20mdps%20and%20pomdps%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Andrew%20Y%20Ng%20and%20Michael%20Jordan.%20Pegasus%3A%20A%20policy%20search%20method%20for%20large%20mdps%20and%20pomdps%202000"
        },
        {
            "id": "28",
            "entry": "[28] Art B. Owen. Monte Carlo theory, methods and examples. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Owen%2C%20Art%20B.%20Monte%20Carlo%20theory%2C%20methods%20and%20examples%202013"
        },
        {
            "id": "29",
            "entry": "[29] Jing Peng and Ronald J Williams. Incremental multi-step q-learning. In Machine Learning Proceedings 1994, pages 226\u2013232.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Jing%20Williams%2C%20Ronald%20J.%20Incremental%20multi-step%20q-learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Jing%20Williams%2C%20Ronald%20J.%20Incremental%20multi-step%20q-learning%201994"
        },
        {
            "id": "30",
            "entry": "[30] Jan Peters, Katharina M\u00fclling, and Yasemin Altun. Relative entropy policy search. In AAAI, pages 1607\u20131612.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Altun%2C%20Yasemin%20Relative%20entropy%20policy%20search",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Altun%2C%20Yasemin%20Relative%20entropy%20policy%20search"
        },
        {
            "id": "31",
            "entry": "[31] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pages 745\u2013750. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20Schaal%2C%20Stefan%20Reinforcement%20learning%20by%20reward-weighted%20regression%20for%20operational%20space%20control%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Jan%20Schaal%2C%20Stefan%20Reinforcement%20learning%20by%20reward-weighted%20regression%20for%20operational%20space%20control%202007"
        },
        {
            "id": "32",
            "entry": "[32] Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180\u20131190, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jan%20Peters%20and%20Stefan%20Schaal%20Natural%20actorcritic%20Neurocomputing%20717911801190%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jan%20Peters%20and%20Stefan%20Schaal%20Natural%20actorcritic%20Neurocomputing%20717911801190%202008"
        },
        {
            "id": "33",
            "entry": "[33] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 21(4):682\u2013697, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20Schaal%2C%20Stefan%20Reinforcement%20learning%20of%20motor%20skills%20with%20policy%20gradients%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Jan%20Schaal%2C%20Stefan%20Reinforcement%20learning%20of%20motor%20skills%20with%20policy%20gradients%202008"
        },
        {
            "id": "34",
            "entry": "[34] Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy iteration. In International Conference on Machine Learning, pages 307\u2013315, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pirotta%2C%20Matteo%20Restelli%2C%20Marcello%20Pecorino%2C%20Alessio%20Calandriello%2C%20Daniele%20Safe%20policy%20iteration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pirotta%2C%20Matteo%20Restelli%2C%20Marcello%20Pecorino%2C%20Alessio%20Calandriello%2C%20Daniele%20Safe%20policy%20iteration%202013"
        },
        {
            "id": "35",
            "entry": "[35] Doina Precup, Richard S Sutton, and Satinder P Singh. Eligibility traces for off-policy policy evaluation. In ICML, pages 759\u2013766.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Precup%2C%20Doina%20Sutton%2C%20Richard%20S.%20Singh%2C%20Satinder%20P.%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Precup%2C%20Doina%20Sutton%2C%20Richard%20S.%20Singh%2C%20Satinder%20P.%20Eligibility%20traces%20for%20off-policy%20policy%20evaluation"
        },
        {
            "id": "36",
            "entry": "[36] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20Martin%20L.%20Markov%20decision%20processes%3A%20discrete%20stochastic%20dynamic%20programming%202014"
        },
        {
            "id": "37",
            "entry": "[37] Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pages 6553\u20136564, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajeswaran%2C%20Aravind%20Lowrey%2C%20Kendall%20Todorov%2C%20Emanuel%20V.%20Kakade%2C%20Sham%20M.%20Towards%20generalization%20and%20simplicity%20in%20continuous%20control%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajeswaran%2C%20Aravind%20Lowrey%2C%20Kendall%20Todorov%2C%20Emanuel%20V.%20Kakade%2C%20Sham%20M.%20Towards%20generalization%20and%20simplicity%20in%20continuous%20control%202017"
        },
        {
            "id": "38",
            "entry": "[38] C Radhakrishna Rao. Information and the accuracy attainable in the estimation of statistical parameters. In Breakthroughs in statistics, pages 235\u2013247.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rao%2C%20C.Radhakrishna%20Information%20and%20the%20accuracy%20attainable%20in%20the%20estimation%20of%20statistical%20parameters",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rao%2C%20C.Radhakrishna%20Information%20and%20the%20accuracy%20attainable%20in%20the%20estimation%20of%20statistical%20parameters"
        },
        {
            "id": "39",
            "entry": "[39] Alfr\u00e9d R\u00e9nyi. On measures of entropy and information. Technical report, HUNGARIAN ACADEMY OF SCIENCES Budapest Hungary, 1961.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R%C3%A9nyi%2C%20Alfr%C3%A9d%20On%20measures%20of%20entropy%20and%20information.%20Technical%20report%2C%20HUNGARIAN%20ACADEMY%20OF%20SCIENCES%20Budapest%20Hungary%201961"
        },
        {
            "id": "40",
            "entry": "[40] Reuven Rubinstein. The cross-entropy method for combinatorial and continuous optimization. Methodology and computing in applied probability, 1(2):127\u2013190, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Reuven%20The%20cross-entropy%20method%20for%20combinatorial%20and%20continuous%20optimization%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20Reuven%20The%20cross-entropy%20method%20for%20combinatorial%20and%20continuous%20optimization%201999"
        },
        {
            "id": "41",
            "entry": "[41] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "42",
            "entry": "[42] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "43",
            "entry": "[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "44",
            "entry": "[44] Frank Sehnke, Christian Osendorfer, Thomas R\u00fcckstie\u00df, Alex Graves, Jan Peters, and J\u00fcrgen Schmidhuber. Policy gradients with parameter-based exploration for control. In International Conference on Artificial Neural Networks, pages 387\u2013396.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Policy%20gradients%20with%20parameter-based%20exploration%20for%20control",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Policy%20gradients%20with%20parameter-based%20exploration%20for%20control"
        },
        {
            "id": "45",
            "entry": "[45] Frank Sehnke, Christian Osendorfer, Thomas R\u00fcckstie\u00df, Alex Graves, Jan Peters, and J\u00fcrgen Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551\u2013559, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parameter-exploring%20policy%20gradients%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parameter-exploring%20policy%20gradients%202010"
        },
        {
            "id": "46",
            "entry": "[46] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014"
        },
        {
            "id": "47",
            "entry": "[47] Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary computation, 10(2):99\u2013127, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002"
        },
        {
            "id": "48",
            "entry": "[48] Yi Sun, Daan Wierstra, Tom Schaul, and Juergen Schmidhuber. Efficient natural evolution strategies. In Proceedings of the 11th Annual conference on Genetic and evolutionary computation, pages 539\u2013546. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Yi%20Wierstra%2C%20Daan%20Schaul%2C%20Tom%20Schmidhuber%2C%20Juergen%20Efficient%20natural%20evolution%20strategies%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Yi%20Wierstra%2C%20Daan%20Schaul%2C%20Tom%20Schmidhuber%2C%20Juergen%20Efficient%20natural%20evolution%20strategies%202009"
        },
        {
            "id": "49",
            "entry": "[49] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "50",
            "entry": "[50] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "51",
            "entry": "[51] Istv\u00e1n Szita and Andr\u00e1s L\u00f6rincz. Learning tetris using the noisy cross-entropy method. Neural computation, 18(12):2936\u20132941, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szita%2C%20Istv%C3%A1n%20L%C3%B6rincz%2C%20Andr%C3%A1s%20Learning%20tetris%20using%20the%20noisy%20cross-entropy%20method%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szita%2C%20Istv%C3%A1n%20L%C3%B6rincz%2C%20Andr%C3%A1s%20Learning%20tetris%20using%20the%20noisy%20cross-entropy%20method%202006"
        },
        {
            "id": "52",
            "entry": "[52] Russ Tedrake, Teresa Weirui Zhang, and H Sebastian Seung. Stochastic policy gradient reinforcement learning on a simple 3d biped. In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on, volume 3, pages 2849\u20132854. IEEE, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tedrake%2C%20Russ%20Zhang%2C%20Teresa%20Weirui%20Seung%2C%20H.Sebastian%20Stochastic%20policy%20gradient%20reinforcement%20learning%20on%20a%20simple%203d%20biped%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tedrake%2C%20Russ%20Zhang%2C%20Teresa%20Weirui%20Seung%2C%20H.Sebastian%20Stochastic%20policy%20gradient%20reinforcement%20learning%20on%20a%20simple%203d%20biped%202004"
        },
        {
            "id": "53",
            "entry": "[53] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139\u20132148, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomas%2C%20Philip%20Brunskill%2C%20Emma%20Data-efficient%20off-policy%20policy%20evaluation%20for%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomas%2C%20Philip%20Brunskill%2C%20Emma%20Data-efficient%20off-policy%20policy%20evaluation%20for%20reinforcement%20learning%202016"
        },
        {
            "id": "54",
            "entry": "[54] Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learning, pages 2380\u20132388, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomas%2C%20Philip%20Theocharous%2C%20Georgios%20Ghavamzadeh%2C%20Mohammad%20High%20confidence%20policy%20improvement%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomas%2C%20Philip%20Theocharous%2C%20Georgios%20Ghavamzadeh%2C%20Mohammad%20High%20confidence%20policy%20improvement%202015"
        },
        {
            "id": "55",
            "entry": "[55] Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy evaluation. In AAAI, pages 3000\u20133006, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomas%2C%20Philip%20S.%20Theocharous%2C%20Georgios%20Ghavamzadeh%2C%20Mohammad%20High-confidence%20off-policy%20evaluation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomas%2C%20Philip%20S.%20Theocharous%2C%20Georgios%20Ghavamzadeh%2C%20Mohammad%20High-confidence%20off-policy%20evaluation%202015"
        },
        {
            "id": "56",
            "entry": "[56] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "57",
            "entry": "[57] George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard E Turner, Zoubin Ghahramani, and Sergey Levine. The mirage of action-dependent baselines in reinforcement learning. arXiv preprint arXiv:1802.10031, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10031"
        },
        {
            "id": "58",
            "entry": "[58] Tim Van Erven and Peter Harremos. R\u00e9nyi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60(7):3797\u20133820, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erven%2C%20Tim%20Van%20Harremos%2C%20Peter%20R%C3%A9nyi%20divergence%20and%20kullback-leibler%20divergence%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erven%2C%20Tim%20Van%20Harremos%2C%20Peter%20R%C3%A9nyi%20divergence%20and%20kullback-leibler%20divergence%202014"
        },
        {
            "id": "59",
            "entry": "[59] Jay M Ver Hoef. Who invented the delta method? The American Statistician, 66(2):124\u2013127, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoef%2C%20Jay%20M.Ver%20Who%20invented%20the%20delta%20method%3F%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoef%2C%20Jay%20M.Ver%20Who%20invented%20the%20delta%20method%3F%202012"
        },
        {
            "id": "60",
            "entry": "[60] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01224"
        },
        {
            "id": "61",
            "entry": "[61] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279\u2013292, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christopher%20JCH%20Watkins%20and%20Peter%20Dayan%20Qlearning%20Machine%20learning%20834279292%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christopher%20JCH%20Watkins%20and%20Peter%20Dayan%20Qlearning%20Machine%20learning%20834279292%201992"
        },
        {
            "id": "62",
            "entry": "[62] Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In Evolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelligence). IEEE Congress on, pages 3381\u20133387. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daan%20Wierstra%20Tom%20Schaul%20Jan%20Peters%20and%20Juergen%20Schmidhuber%20Natural%20evolution%20strategies%20In%20Evolutionary%20Computation%202008%20CEC%202008IEEE%20World%20Congress%20on%20Computational%20Intelligence%20IEEE%20Congress%20on%20pages%2033813387%20IEEE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daan%20Wierstra%20Tom%20Schaul%20Jan%20Peters%20and%20Juergen%20Schmidhuber%20Natural%20evolution%20strategies%20In%20Evolutionary%20Computation%202008%20CEC%202008IEEE%20World%20Congress%20on%20Computational%20Intelligence%20IEEE%20Congress%20on%20pages%2033813387%20IEEE%202008"
        },
        {
            "id": "63",
            "entry": "[63] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pages 5\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning"
        },
        {
            "id": "64",
            "entry": "[64] Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade, Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent factorized baselines. arXiv preprint arXiv:1803.07246, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07246"
        },
        {
            "id": "65",
            "entry": "[65] Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of policy gradient estimation. In Advances in Neural Information Processing Systems, pages 262\u2013270, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Tingting%20Hachiya%2C%20Hirotaka%20Niu%2C%20Gang%20Sugiyama%2C%20Masashi%20Analysis%20and%20improvement%20of%20policy%20gradient%20estimation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Tingting%20Hachiya%2C%20Hirotaka%20Niu%2C%20Gang%20Sugiyama%2C%20Masashi%20Analysis%20and%20improvement%20of%20policy%20gradient%20estimation%202011"
        },
        {
            "id": "66",
            "entry": "[66] Tingting Zhao, Hirotaka Hachiya, Voot Tangkaratt, Jun Morimoto, and Masashi Sugiyama. Efficient sample reuse in policy gradients with parameter-based exploration. Neural computation, 25(6):1512\u20131547, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Tingting%20Hachiya%2C%20Hirotaka%20Tangkaratt%2C%20Voot%20Morimoto%2C%20Jun%20Efficient%20sample%20reuse%20in%20policy%20gradients%20with%20parameter-based%20exploration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Tingting%20Hachiya%2C%20Hirotaka%20Tangkaratt%2C%20Voot%20Morimoto%2C%20Jun%20Efficient%20sample%20reuse%20in%20policy%20gradients%20with%20parameter-based%20exploration%202013"
        }
    ]
}
