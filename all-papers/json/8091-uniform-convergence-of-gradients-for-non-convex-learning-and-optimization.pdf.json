{
    "filename": "8091-uniform-convergence-of-gradients-for-non-convex-learning-and-optimization.pdf",
    "metadata": {
        "date": 2018,
        "title": "Uniform Convergence of Gradients for Non-Convex Learning and Optimization",
        "author": "Dylan J. Foster Cornell University djfoster@cornell.edu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8091-uniform-convergence-of-gradients-for-non-convex-learning-and-optimization.pdf"
        },
        "abstract": "We investigate 1) the rate at which refined properties of the empirical risk\u2014in particular, gradients\u2014converge to their population counterparts in standard nonconvex learning tasks, and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple, composable, and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in nonconvex learning problems. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity, even when dimension is high or possibly infinite and multiple passes over the dataset are allowed."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "statistical learning",
            "url": "https://en.wikipedia.org/wiki/statistical_learning"
        },
        {
            "term": "rademacher complexity",
            "url": "https://en.wikipedia.org/wiki/rademacher_complexity"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        }
    ],
    "highlights": [
        "The last decade has seen a string of empirical successes for gradient-based algorithms solving large scale non-convex machine learning problems [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "As a consequence of the gradient uniform convergence bounds, we show how to use any algorithm that finds approximate stationary points for smooth functions in a black-box fashion to obtain optimal sample complexity for these models\u2014both in highand low-dimensional regimes",
        "The following theorem provides a dimensionindependent uniform convergence bound for the gradients over the class W( , Dn) for any margin function fixed in advance",
        "We showed that vector Rademacher complexities are a simple and effective tool for deriving dimension-independent uniform convergence bounds and used these bounds in conjunction with the Gradient Domination property to derive optimal algorithms for non-convex statistical learning in high and infinite dimension"
    ],
    "key_statements": [
        "The last decade has seen a string of empirical successes for gradient-based algorithms solving large scale non-convex machine learning problems [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "As a consequence of the gradient uniform convergence bounds, we show how to use any algorithm that finds approximate stationary points for smooth functions in a black-box fashion to obtain optimal sample complexity for these models\u2014both in highand low-dimensional regimes",
        "The following theorem provides a dimensionindependent uniform convergence bound for the gradients over the class W( , Dn) for any margin function fixed in advance",
        "We showed that vector Rademacher complexities are a simple and effective tool for deriving dimension-independent uniform convergence bounds and used these bounds in conjunction with the Gradient Domination property to derive optimal algorithms for non-convex statistical learning in high and infinite dimension"
    ],
    "summary": [
        "The last decade has seen a string of empirical successes for gradient-based algorithms solving large scale non-convex machine learning problems [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "These two directions yield direct bounds on the convergence of non-convex gradient-based learning algorithms to low excess risk.",
        "The condition bounds excess risk in terms of the magnitude of gradients, and is satisfied in non-convex learning problems including generalized linear models and robust regression.",
        "As a consequence of the gradient uniform convergence bounds, we show how to use any algorithm that finds approximate stationary points for smooth functions in a black-box fashion to obtain optimal sample complexity for these models\u2014both in highand low-dimensional regimes.",
        "We instantiate the general gradient uniform convergence tools and the GD condition to derive optimization consequences for two standard settings previously studied by [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]: generalized linear models and robust regression.",
        "For the generalized linear model setting, the following excess risk inequalities each hold with probability at least 1 \u2212 over the \u25cf Norm-Based/High-Dimensional Setup.",
        "Non-convex choices for \u21e2 arise in robust statistics, with a canonical example being Tukey\u2019s biweight loss.4 While optimization is clearly not possible for arbitrary choices of \u21e2, the following assumption is sufficient to guarantee that the population risk LD satisfies the GD property.",
        "Similar to the generalized linear model setup, the robust regression setup satisfies three variants of the GD depending on assumptions on the norm \uffff\u22c5\uffff and the data distribution.",
        "For the robust regression setting, the following excess risk inequalities each hold with p\u25cfroNboarbmil-itByaastelde/aHsitg1h\u2212-DimoveenrstihoenadlraSwetuopf .thWe dhaentaX{b}atnl=l1fofror any algorithm -smooth norm \u0175alg : \uffff\u22c5\uffff and",
        "In the previous section we used gradient uniform convergence to derive immediate optimization and generalization consequences by finding approximate stationary points of smooth non-convex functions.",
        "To make the problem well-defined, we consider convergence for the following representative from the subgradient: \u2207`(w ; x, y) \u2236= \u2212y {y\uffffw, x\uffff \u2264 0} \u22c5 x.6 Our first theorem shows that gradient uniform convergence for this setup must depend on dimension, even when the weight norm B and data norm R are held constant.",
        "The following theorem provides a dimensionindependent uniform convergence bound for the gradients over the class W( , Dn) for any margin function fixed in advance.",
        "We showed that vector Rademacher complexities are a simple and effective tool for deriving dimension-independent uniform convergence bounds and used these bounds in conjunction with the Gradient Domination property to derive optimal algorithms for non-convex statistical learning in high and infinite dimension."
    ],
    "headline": "We investigate 1) the rate at which refined properties of the empirical risk\u2014in particular, gradients\u2014converge to their population counterparts in standard nonconvex learning tasks, and 2) the consequences of this convergence for optimization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD. Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Natasha%202%3A%20Faster%20Non-Convex%20Optimization%20Than%20SGD%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Natasha%202%3A%20Faster%20Non-Convex%20Optimization%20Than%20SGD%202018"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically. Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20How%20To%20Make%20the%20Gradients%20Small%20Stochastically%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20How%20To%20Make%20the%20Gradients%20Small%20Stochastically%202018"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International Conference on Machine Learning, pages 699\u2013707, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] Peter L Bartlett and John Shawe-Taylor. Generalization performance of support vector machines and other pattern classifiers. In Advances in kernel methods, pages 43\u201354. MIT Press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Shawe-Taylor%2C%20John%20Generalization%20performance%20of%20support%20vector%20machines%20and%20other%20pattern%20classifiers%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Shawe-Taylor%2C%20John%20Generalization%20performance%20of%20support%20vector%20machines%20and%20other%20pattern%20classifiers%201999"
        },
        {
            "id": "5",
            "entry": "[5] Peter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The Annals of Statistics, 33(4):1497\u20131537, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20rademacher%20complexities%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Bousquet%2C%20Olivier%20Mendelson%2C%20Shahar%20Local%20rademacher%20complexities%202005"
        },
        {
            "id": "6",
            "entry": "[6] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "7",
            "entry": "[7] Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and examples. Springer Science & Business Media, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borwein%2C%20Jonathan%20Lewis%2C%20Adrian%20S.%20Convex%20analysis%20and%20nonlinear%20optimization%3A%20theory%20and%20examples%202010"
        },
        {
            "id": "8",
            "entry": "[8] Frank H Clarke. Optimization and nonsmooth analysis, volume 5.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frank%20H%20Clarke%20Optimization%20and%20nonsmooth%20analysis%20volume%205",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frank%20H%20Clarke%20Optimization%20and%20nonsmooth%20analysis%20volume%205"
        },
        {
            "id": "9",
            "entry": "[9] Damek Davis and Dmitriy Drusvyatskiy. Uniform graphical convergence of subgradients in nonconvex optimization and learning. arXiv preprint arXiv:1810.07590, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.07590"
        },
        {
            "id": "10",
            "entry": "[10] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "11",
            "entry": "[11] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. Conference on Learning Theory, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018"
        },
        {
            "id": "12",
            "entry": "[12] Alon Gonen and Shai Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems. Conference on Learning Theory, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gonen%2C%20Alon%20Shalev-Shwartz%2C%20Shai%20Fast%20rates%20for%20empirical%20risk%20minimization%20of%20strict%20saddle%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gonen%2C%20Alon%20Shalev-Shwartz%2C%20Shai%20Fast%20rates%20for%20empirical%20risk%20minimization%20of%20strict%20saddle%20problems%202017"
        },
        {
            "id": "13",
            "entry": "[13] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Identity%20matters%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Identity%20matters%20in%20deep%20learning%202017"
        },
        {
            "id": "14",
            "entry": "[14] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. Journal of Machine Learning Research, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Recht%2C%20Benjamin%20Gradient%20descent%20learns%20linear%20dynamical%20systems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Recht%2C%20Benjamin%20Gradient%20descent%20learns%20linear%20dynamical%20systems%202018"
        },
        {
            "id": "15",
            "entry": "[15] Elad Hazan. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization, 2(3-4):157\u2013325, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202016"
        },
        {
            "id": "16",
            "entry": "[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "17",
            "entry": "[17] Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. Foundations and Trends\u00ae in Machine Learning, 10(3-4):142\u2013336, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Prateek%20Kar%2C%20Purushottam%20Non-convex%20optimization%20for%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Prateek%20Kar%2C%20Purushottam%20Non-convex%20optimization%20for%20machine%20learning%202017"
        },
        {
            "id": "18",
            "entry": "[18] Chi Jin, Lydia T Liu, Rong Ge, and Michael I Jordan. Minimizing nonconvex population risk from rough empirical risk. Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Liu%2C%20Lydia%20T.%20Ge%2C%20Rong%20Jordan%2C%20Michael%20I.%20Minimizing%20nonconvex%20population%20risk%20from%20rough%20empirical%20risk%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Liu%2C%20Lydia%20T.%20Ge%2C%20Rong%20Jordan%2C%20Michael%20I.%20Minimizing%20nonconvex%20population%20risk%20from%20rough%20empirical%20risk%202018"
        },
        {
            "id": "19",
            "entry": "[19] Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems 21, pages 793\u2013800. MIT Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009"
        },
        {
            "id": "20",
            "entry": "[20] Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793\u2013800, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009"
        },
        {
            "id": "21",
            "entry": "[21] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pages 927\u2013935, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Kanade%2C%20Varun%20Shamir%2C%20Ohad%20Kalai%2C%20Adam%20Efficient%20learning%20of%20generalized%20linear%20and%20single%20index%20models%20with%20isotonic%20regression%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Kanade%2C%20Varun%20Shamir%2C%20Ohad%20Kalai%2C%20Adam%20Efficient%20learning%20of%20generalized%20linear%20and%20single%20index%20models%20with%20isotonic%20regression%202011"
        },
        {
            "id": "22",
            "entry": "[22] Sham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for learning with matrices. Journal of Machine Learning Research, 13(Jun):1865\u20131890, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Shalev-Shwartz%2C%20Shai%20Tewari%2C%20Ambuj%20Regularization%20techniques%20for%20learning%20with%20matrices%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Shalev-Shwartz%2C%20Shai%20Tewari%2C%20Ambuj%20Regularization%20techniques%20for%20learning%20with%20matrices%202012"
        },
        {
            "id": "23",
            "entry": "[23] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795\u2013811.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition"
        },
        {
            "id": "24",
            "entry": "[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "25",
            "entry": "[25] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer-Verlag, New York, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20Michel%20Talagrand%2C%20Michel%20Probability%20in%20Banach%20Spaces%201991"
        },
        {
            "id": "26",
            "entry": "[26] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017"
        },
        {
            "id": "27",
            "entry": "[27] Xiaodong Li, Shuyang Ling, Thomas Strohmer, and Ke Wei. Rapid, robust, and reliable blind deconvolution via nonconvex optimization. Applied and Computational Harmonic Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Xiaodong%20Ling%2C%20Shuyang%20Strohmer%2C%20Thomas%20Rapid%2C%20Ke%20Wei%20and%20reliable%20blind%20deconvolution%20via%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Xiaodong%20Ling%2C%20Shuyang%20Strohmer%2C%20Thomas%20Rapid%2C%20Ke%20Wei%20and%20reliable%20blind%20deconvolution%20via%20nonconvex%20optimization%202018"
        },
        {
            "id": "28",
            "entry": "[28] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pages 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20relu%20activation%202017"
        },
        {
            "id": "29",
            "entry": "[29] Huikang Liu, Weijie Wu, and Anthony Man-Cho So. Quadratic optimization with orthogonality constraints: Explicit lojasiewicz exponent and linear convergence of line-search methods. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of PMLR, pages 1158\u20131167, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Huikang%20Wu%2C%20Weijie%20So%2C%20Anthony%20Man-Cho%20Quadratic%20optimization%20with%20orthogonality%20constraints%3A%20Explicit%20lojasiewicz%20exponent%20and%20linear%20convergence%20of%20line-search%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Huikang%20Wu%2C%20Weijie%20So%2C%20Anthony%20Man-Cho%20Quadratic%20optimization%20with%20orthogonality%20constraints%3A%20Explicit%20lojasiewicz%20exponent%20and%20linear%20convergence%20of%20line-search%20methods%202016"
        },
        {
            "id": "30",
            "entry": "[30] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International Conference on Algorithmic Learning Theory, pages 3\u201317.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20Andreas%20A%20vector-contraction%20inequality%20for%20rademacher%20complexities",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20A%20vector-contraction%20inequality%20for%20rademacher%20complexities"
        },
        {
            "id": "31",
            "entry": "[31] Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. To appear in Annals of Statistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20Song%20Bai%2C%20Yu%20Montanari%2C%20Andrea%20The%20landscape%20of%20empirical%20risk%20for%20non-convex%20losses%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20Song%20Bai%2C%20Yu%20Montanari%2C%20Andrea%20The%20landscape%20of%20empirical%20risk%20for%20non-convex%20losses%202016"
        },
        {
            "id": "32",
            "entry": "[32] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Talwalkar%2C%20Ameet%20Foundations%20of%20machine%20learning%202012"
        },
        {
            "id": "33",
            "entry": "[33] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "34",
            "entry": "[34] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, 22(4):1679\u20131706, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinelis%2C%20Iosif%20Optimum%20bounds%20for%20the%20distributions%20of%20martingales%20in%20banach%20spaces%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinelis%2C%20Iosif%20Optimum%20bounds%20for%20the%20distributions%20of%20martingales%20in%20banach%20spaces%201994"
        },
        {
            "id": "35",
            "entry": "[35] Gilles Pisier. Martingales with values in uniformly convex spaces. Israel Journal of Mathematics, 20:326\u2013350, 1975. ISSN 0021-2172.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pisier%2C%20Gilles%20Martingales%20with%20values%20in%20uniformly%20convex%20spaces%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pisier%2C%20Gilles%20Martingales%20with%20values%20in%20uniformly%20convex%20spaces%201975"
        },
        {
            "id": "36",
            "entry": "[36] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 3(4):643\u2013653, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20Teodorovich%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20Teodorovich%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963"
        },
        {
            "id": "37",
            "entry": "[37] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. Journal of Machine Learning Research, 11:2241\u20132259, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010"
        },
        {
            "id": "38",
            "entry": "[38] Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pages 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "39",
            "entry": "[39] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Ben-David%2C%20Shai%20Understanding%20machine%20learning%3A%20From%20theory%20to%20algorithms%202014"
        },
        {
            "id": "40",
            "entry": "[40] Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory (ISIT), 2016 IEEE International Symposium on, pages 2379\u20132383. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20A%20geometric%20analysis%20of%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20A%20geometric%20analysis%20of%20phase%20retrieval%202016"
        },
        {
            "id": "41",
            "entry": "[41] Robert Tibshirani, Martin Wainwright, and Trevor Hastie. Statistical learning with sparsity: the lasso and generalizations. Chapman and Hall/CRC, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Wainwright%2C%20Martin%20Hastie%2C%20Trevor%20Statistical%20learning%20with%20sparsity%3A%20the%20lasso%20and%20generalizations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Wainwright%2C%20Martin%20Hastie%2C%20Trevor%20Statistical%20learning%20with%20sparsity%3A%20the%20lasso%20and%20generalizations%202015"
        },
        {
            "id": "42",
            "entry": "[42] Alexandre B Tsybakov. Introduction to Nonparametric Estimation. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexandre%20B.%20Introduction%20to%20Nonparametric%20Estimation%202008"
        },
        {
            "id": "43",
            "entry": "[43] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "44",
            "entry": "[44] Huishuai Zhang, Yuejie Chi, and Yingbin Liang. Median-truncated nonconvex approach for phase retrieval with outliers. International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Huishuai%20Chi%2C%20Yuejie%20Liang%2C%20Yingbin%20Median-truncated%20nonconvex%20approach%20for%20phase%20retrieval%20with%20outliers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Huishuai%20Chi%2C%20Yuejie%20Liang%2C%20Yingbin%20Median-truncated%20nonconvex%20approach%20for%20phase%20retrieval%20with%20outliers%202016"
        },
        {
            "id": "45",
            "entry": "[45] Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex optimization. Advances in Neural Information Processing Systems, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Dongruo%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Stochastic%20nested%20variance%20reduction%20for%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Dongruo%20Xu%2C%20Pan%20Gu%2C%20Quanquan%20Stochastic%20nested%20variance%20reduction%20for%20nonconvex%20optimization%202018"
        }
    ]
}
