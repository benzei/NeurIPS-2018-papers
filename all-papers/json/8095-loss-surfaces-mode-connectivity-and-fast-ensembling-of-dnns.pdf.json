{
    "filename": "8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf",
    "metadata": {
        "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs",
        "author": "Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, Andrew G. Wilson",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet."
    },
    "keywords": [
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "geometric property",
            "url": "https://en.wikipedia.org/wiki/geometric_property"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "local optima",
            "url": "https://en.wikipedia.org/wiki/local_optima"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "The loss surfaces of deep neural networks (DNNs) are highly non-convex and can depend on millions of parameters",
        "Using the proposed method we demonstrate that such mode connectivity holds for a wide range of modern deep neural networks, on key benchmarks such as CIFAR-100",
        "Inspired by properties of the loss function discovered by our procedure, we propose a new state-of-the-art ensembling method that can be trained in the time required to train a single deep neural networks, with compelling performance on many key benchmarks (e.g., 96.4% accuracy on CIFAR-10)",
        "We have shown that the optima of deep neural networks are connected by simple pathways, such as a polygonal chain with a single bend, with near constant accuracy",
        "We introduced a training procedure to find these pathways, with a user-specific curve of choice",
        "At a high level we have shown that even though the loss surfaces of deep neural networks are very complex, there is relatively simple structure connecting different optima"
    ],
    "key_statements": [
        "The loss surfaces of deep neural networks (DNNs) are highly non-convex and can depend on millions of parameters",
        "These two observations suggest that the local optima are isolated",
        "We provide a new training procedure which can find paths of near-constant accuracy between the modes of large deep neural networks",
        "We show that for a wide range of architectures we can find these paths in the form of a simple polygonal chain of two line segments",
        "Using the proposed method we demonstrate that such mode connectivity holds for a wide range of modern deep neural networks, on key benchmarks such as CIFAR-100",
        "We show that these paths correspond to meaningfully different representations that can be efficiently ensembled for increased accuracy",
        "Inspired by these observations, we propose Fast Geometric Ensembling (FGE), which outperforms the recent state-of-the-art Snapshot Ensembles [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], on CIFAR-10 and CIFAR100, using powerful deep neural networks such as VGG-16, Wide ResNet-28-10, and ResNet-164",
        "Section 3 introduces the proposed method to find the curves with low train loss and test error between local optima, which we investigate empirically in Section 4",
        "Inspired by properties of the loss function discovered by our procedure, we propose a new state-of-the-art ensembling method that can be trained in the time required to train a single deep neural networks, with compelling performance on many key benchmarks (e.g., 96.4% accuracy on CIFAR-10)",
        "We describe a new method to minimize the training error along a path that connects two points in the space of deep neural networks weights",
        "To find a path of high accuracy between w1 and w2, we propose to find the parameters \u03b8 that minimize the expectation over a uniform distribution on the curve, \u02c6(\u03b8): \u02c6(\u03b8) =",
        "We show that the proposed training procedure in Section 3 does find high accuracy paths connecting different modes, across a range of architectures and datasets",
        "In the supplementary material we show that the connecting curves can be found consistently as we vary the number of parameters in the network, the ratio of the arclength for the curves to the length of the line segment connecting the same endpoints decreases with increasing parametrization",
        "Gotmare et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] have recently shown that our mode connectivity approach applies to pairs of networks trained with different batch sizes, optimizers, data augmentation strategies, weight decays and learning rate schemes",
        "To motivate the ensembling procedure proposed we examine how far we need to move along a connecting curve to find a point that produces substantially different, but still useful, predictions",
        "We investigate the performance of an ensemble of two networks: the endpoint \u03c6\u03b8(0) of the curve and a point \u03c6\u03b8(t) on the curve corresponding to t \u2208 [0, 1]",
        "We propose the Fast Geometric Ensembling (FGE) method that aims to find diverse networks with relatively small steps in the weight space, without leaving a region that corresponds to low test error",
        "Despite the harder setting of only 5 epochs to construct an ensemble, Fast Geometric Ensembling performs comparably to the best result reported by Huang et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] on ImageNet, 23.33 error, which was achieved using a ResNet-50",
        "We have shown that the optima of deep neural networks are connected by simple pathways, such as a polygonal chain with a single bend, with near constant accuracy",
        "We introduced a training procedure to find these pathways, with a user-specific curve of choice",
        "We were inspired by these insights to propose a practical new ensembling approach, Fast Geometric Ensembling, which achieves state-of-the-art results on CIFAR-10, CIFAR-100, and ImageNet",
        "At a high level we have shown that even though the loss surfaces of deep neural networks are very complex, there is relatively simple structure connecting different optima",
        "We could continue to build on the new training procedure we proposed here, to find curves with particularly desirable properties, such as diversity of networks"
    ],
    "summary": [
        "The loss surfaces of deep neural networks (DNNs) are highly non-convex and can depend on millions of parameters.",
        "Section 3 introduces the proposed method to find the curves with low train loss and test error between local optima, which we investigate empirically in Section 4.",
        "Inspired by properties of the loss function discovered by our procedure, we propose a new state-of-the-art ensembling method that can be trained in the time required to train a single DNN, with compelling performance on many key benchmarks (e.g., 96.4% accuracy on CIFAR-10).",
        "We show that the proposed training procedure in Section 3 does find high accuracy paths connecting different modes, across a range of architectures and datasets.",
        "We use the proposed algorithm of Section 3 to find a path connecting these two modes in the weight space with a quadratic Bezier curve and a polygonal chain with one bend.",
        "Gotmare et al [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] have recently shown that our mode connectivity approach applies to pairs of networks trained with different batch sizes, optimizers, data augmentation strategies, weight decays and learning rate schemes.",
        "We propose the Fast Geometric Ensembling (FGE) method that aims to find diverse networks with relatively small steps in the weight space, without leaving a region that corresponds to low test error.",
        "We can train the network w using the standard 2-regularized cross-entropy loss function with the proposed learning rate schedule for n iterations.",
        "According to our analysis of the curves it is sufficient to do relatively small steps in the weight space to get diverse networks, so we only employ cyclical learning rates with a small cycle length on the scale of 2 to 4 epochs in the last stage of the training.",
        "We compare the proposed Fast Geometric Ensembling (FGE) technique against ensembles of independently trained networks (Ind), and SnapShot Ensembles (SSE) [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], a recent state-of-the-art fast ensembling approach.",
        "For a given computational budget FGE can propose many more high-performing networks than independent training, leading to better ensembling performance.",
        "Despite the harder setting of only 5 epochs to construct an ensemble, FGE performs comparably to the best result reported by Huang et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] on ImageNet, 23.33 error, which was achieved using a ResNet-50.",
        "At a high level we have shown that even though the loss surfaces of deep neural networks are very complex, there is relatively simple structure connecting different optima."
    ],
    "headline": "We show that the optima of these complex loss functions are connected by simple curves over which training and test accuracy are nearly constant",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Peter Auer, Mark Herbster, and Manfred K Warmuth. Exponentially many local minima for single neurons. In Advances in Neural Information Processing Systems, pages 316\u2013322, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Herbster%2C%20Mark%20Warmuth%2C%20Manfred%20K.%20Exponentially%20many%20local%20minima%20for%20single%20neurons%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Herbster%2C%20Mark%20Warmuth%2C%20Manfred%20K.%20Exponentially%20many%20local%20minima%20for%20single%20neurons%201996"
        },
        {
            "id": "2",
            "entry": "[2] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "3",
            "entry": "[3] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2933\u20132941, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014"
        },
        {
            "id": "4",
            "entry": "[4] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1019\u20131028, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/dinh17b.html.",
            "url": "http://proceedings.mlr.press/v70/dinh17b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017-08"
        },
        {
            "id": "5",
            "entry": "[5] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1309\u20131318, Stockholmsm\u00e4ssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/draxler18a.html.",
            "url": "http://proceedings.mlr.press/v80/draxler18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Draxler%2C%20Felix%20Veschgini%2C%20Kambis%20Salmhofer%2C%20Manfred%20Hamprecht%2C%20Fred%20Essentially%20no%20barriers%20in%20neural%20network%20energy%20landscape%202018-07"
        },
        {
            "id": "6",
            "entry": "[6] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freeman%2C%20C.Daniel%20Bruna%2C%20Joan%20Topology%20and%20geometry%20of%20half-rectified%20network%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freeman%2C%20C.Daniel%20Bruna%2C%20Joan%20Topology%20and%20geometry%20of%20half-rectified%20network%20optimization%202017"
        },
        {
            "id": "7",
            "entry": "[7] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Vinyals%2C%20Oriol%20Saxe%2C%20Andrew%20M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Vinyals%2C%20Oriol%20Saxe%2C%20Andrew%20M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015"
        },
        {
            "id": "8",
            "entry": "[8] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Using mode connectivity for loss landscape analysis. arXiv preprint arXiv:1806.06977, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06977"
        },
        {
            "id": "9",
            "entry": "[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "10",
            "entry": "[10] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural Computation, 9(1):1\u201342, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997"
        },
        {
            "id": "11",
            "entry": "[11] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles%202017"
        },
        {
            "id": "12",
            "entry": "[12] Hannes Jonsson, Greg Mills, and Karsten W Jacobsen. Nudged elastic band method for finding minimum energy paths of transitions. Classical and quantum dynamics in condensed phase simulations, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jonsson%2C%20Hannes%20Mills%2C%20Greg%20Jacobsen%2C%20Karsten%20W.%20Nudged%20elastic%20band%20method%20for%20finding%20minimum%20energy%20paths%20of%20transitions.%20Classical%20and%20quantum%20dynamics%20in%20condensed%20phase%20simulations%201998"
        },
        {
            "id": "13",
            "entry": "[13] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017"
        },
        {
            "id": "14",
            "entry": "[14] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246\u20131257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016"
        },
        {
            "id": "15",
            "entry": "[15] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David Crandall, and Dhruv Batra. Stochastic multiple choice learning for training diverse deep ensembles. In Advances in Neural Information Processing Systems, pages 2119\u20132127, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Stefan%20Prakash%2C%20Senthil%20Purushwalkam%20Shiva%20Cogswell%2C%20Michael%20Ranjan%2C%20Viresh%20Stochastic%20multiple%20choice%20learning%20for%20training%20diverse%20deep%20ensembles%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Stefan%20Prakash%2C%20Senthil%20Purushwalkam%20Shiva%20Cogswell%2C%20Michael%20Ranjan%2C%20Viresh%20Stochastic%20multiple%20choice%20learning%20for%20training%20diverse%20deep%20ensembles%202016"
        },
        {
            "id": "16",
            "entry": "[16] Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09913"
        },
        {
            "id": "17",
            "entry": "[17] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20Sgdr%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20Sgdr%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017"
        },
        {
            "id": "18",
            "entry": "[18] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202012"
        },
        {
            "id": "19",
            "entry": "[19] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "20",
            "entry": "[20] Leslie N Smith and Nicholay Topin. Exploring loss function topology with cyclical learning rates. arXiv preprint arXiv:1702.04283, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04283"
        },
        {
            "id": "21",
            "entry": "[21] Jingjing Xie, Bing Xu, and Zhang Chuang. Horizontal and vertical ensemble with deep representation for classification. arXiv preprint arXiv:1306.2759, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1306.2759"
        },
        {
            "id": "22",
            "entry": "[22] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016"
        }
    ]
}
