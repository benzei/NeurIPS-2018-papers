{
    "filename": "8051-bilevel-learning-of-the-group-lasso-structure.pdf",
    "metadata": {
        "title": "Bilevel learning of the Group Lasso structure",
        "author": "Jordan Frecon, Saverio Salzo, Massimiliano Pontil",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8051-bilevel-learning-of-the-group-lasso-structure.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Regression with group-sparsity penalty plays a central role in high-dimensional prediction problems. However, most of existing methods require the group structure to be known a priori. In practice, this may be a too strong assumption, potentially hampering the effectiveness of the regularization method. To circumvent this issue, we present a method to estimate the group structure by means of a continuous bilevel optimization problem where the data is split into training and validation sets. Our approach relies on an approximation scheme where the lower level problem is replaced by a smooth dual forward-backward algorithm with Bregman distances. We provide guarantees regarding the convergence of the approximate procedure to the exact problem and demonstrate the well behaviour of the proposed method on synthetic experiments. Finally, a preliminary application to genes expression data is tackled with the purpose of unveiling functional groups."
    },
    "keywords": [
        {
            "term": "group structure",
            "url": "https://en.wikipedia.org/wiki/group_structure"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "gene expression",
            "url": "https://en.wikipedia.org/wiki/gene_expression"
        },
        {
            "term": "bregman distance",
            "url": "https://en.wikipedia.org/wiki/bregman_distance"
        },
        {
            "term": "bilevel optimization",
            "url": "https://en.wikipedia.org/wiki/bilevel_optimization"
        }
    ],
    "highlights": [
        "High-dimensional datasets have become massively widespread in numerous applications ranging from social sciences to computational biology [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "In this paper we address the problem of learning the Group Lasso structure, within the setting of multi-task learning, through a bilevel optimization approach",
        "The principal contribution of this paper is the formulation of the problem of learning the Group Lasso structure as a continuous bilevel optimization problem",
        "We propose a new algorithm based on a dual forwardbackward scheme with Bregman distances where A and B are smooth",
        "We propose to solve Problem 3.1 by applying a forward-backward algorithm with Bregman distances to the dual Problem 3.2 [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and using calls for a Bregman proximity operator of the primal-dual g\u2217 which can be link w = \u2207f made smooth",
        "The corresponding forward-backward scheme with Bregman distance is given in Algorithm 1 where, for the sake of readability, we introduced the primal iterates w(q)(\u03b8) and the the argument of \u2207\u03c6\u2217 in (13)"
    ],
    "key_statements": [
        "High-dimensional datasets have become massively widespread in numerous applications ranging from social sciences to computational biology [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "In this paper we address the problem of learning the Group Lasso structure, within the setting of multi-task learning, through a bilevel optimization approach",
        "The principal contribution of this paper is the formulation of the problem of learning the Group Lasso structure as a continuous bilevel optimization problem",
        "We propose a new algorithm based on a dual forwardbackward scheme with Bregman distances where A and B are smooth",
        "We propose to solve Problem 3.1 by applying a forward-backward algorithm with Bregman distances to the dual Problem 3.2 [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and using calls for a Bregman proximity operator of the primal-dual g\u2217 which can be link w = \u2207f made smooth",
        "The corresponding forward-backward scheme with Bregman distance is given in Algorithm 1 where, for the sake of readability, we introduced the primal iterates w(q)(\u03b8) and the the argument of \u2207\u03c6\u2217 in (13)"
    ],
    "summary": [
        "High-dimensional datasets have become massively widespread in numerous applications ranging from social sciences to computational biology [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "In this paper we address the problem of learning the Group Lasso structure, within the setting of multi-task learning, through a bilevel optimization approach.",
        "The principal contribution of this paper is the formulation of the problem of learning the Group Lasso structure as a continuous bilevel optimization problem.",
        "We describe a bilevel framework for estimating the Group Lasso structure based on a multi-task learning problem, without any further a priori information.",
        "The following theorem gives the conditions under which the approximate problem converges to the exact one as the number of inner iterations Q grows.",
        "By relying on [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], we prove the uniform convergence of such algorithm to the solution of the lower level problem, so to meet the requirements of Theorem 2.1.",
        "In order to solve the bilevel problem, we rely on the following projected gradient descent algorithm",
        "Given some vectors of outputs y \u2208 RN , a design matrix X \u2208 RN\u00d7P , regularization parameters \u03bb > 0 and > 0, as well as some group structure \u03b8 \u2208 \u0398, find w(\u03b8) = argmin w\u2208RP",
        "Let us note that in order to solve Problem 3.1 we cannot use the standard forward-backward algorithm [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] since the proximity operator of g \u25e6 A\u03b8 cannot be computed in closed form.",
        "We propose to solve Problem 3.1 by applying a forward-backward algorithm with Bregman distances to the dual Problem 3.2 [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and using calls for a Bregman proximity operator of the primal-dual g\u2217 which can be link w = \u2207f made smooth",
        "The dual forward-backward algorithm with Bregman distances (FBB) for Problem 3.1 is as follows.",
        "The corresponding forward-backward scheme with Bregman distance is given in Algorithm 1 where, for the sake of readability, we introduced the primal iterates w(q)(\u03b8) and the the argument of \u2207\u03c6\u2217 in (13).",
        "The sequence {w(Q)(\u03b8)}Q\u2208N generated by Algorithm 1 converges to the solution w(\u03b8) of Problem 3.1 for any step-size 0 < \u03b3 < \u03bb\u22121 A\u03b8 \u22122.",
        "The proposed method still satisfactorily estimates groups of different size, as Figure 3 shows, in case L\u2217 is known as well as if L\u2217 is overestimated (L = 20).",
        "This contribution studied the problem of learning the groups by solving a continuous bilevel problem.",
        "We replaced the exact Group Lasso optimization problem by a smooth dual forward-backward algorithm with Bregman distances.",
        "Future works notably include the extension to overlapping groups [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and could aim at learning the groups in group-sparse classification problems"
    ],
    "headline": "We present a method to estimate the group structure by means of a continuous bilevel optimization problem where the data is split into training and validation sets",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Ahmed and E. Xing. Recovering time-varying networks of dependencies in social and biological studies. Proceedings of the National Academy of Sciences, 106(29):11878\u201311883, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahmed%2C%20A.%20Xing%2C%20E.%20Recovering%20time-varying%20networks%20of%20dependencies%20in%20social%20and%20biological%20studies%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahmed%2C%20A.%20Xing%2C%20E.%20Recovering%20time-varying%20networks%20of%20dependencies%20in%20social%20and%20biological%20studies%202009"
        },
        {
            "id": "2",
            "entry": "[2] H. H. Bauschke, J. Bolte, and M. Teboulle. A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications. Mathematics of Operations Research, 42(2):330\u2013348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauschke%2C%20H.H.%20Bolte%2C%20J.%20Teboulle%2C%20M.%20A%20descent%20lemma%20beyond%20Lipschitz%20gradient%20continuity%3A%20first-order%20methods%20revisited%20and%20applications%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bauschke%2C%20H.H.%20Bolte%2C%20J.%20Teboulle%2C%20M.%20A%20descent%20lemma%20beyond%20Lipschitz%20gradient%20continuity%3A%20first-order%20methods%20revisited%20and%20applications%202016"
        },
        {
            "id": "3",
            "entry": "[3] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, New York, USA, 2nd edition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauschke%2C%20H.H.%20Combettes%2C%20P.L.%20Convex%20Analysis%20and%20Monotone%20Operator%20Theory%20in%20Hilbert%20Spaces%202017"
        },
        {
            "id": "4",
            "entry": "[4] Y. Bengio. Gradient-based optimization of hyperparameters. Neural Computation, 12(8):1889\u2013 1900, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Gradient-based%20optimization%20of%20hyperparameters%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Gradient-based%20optimization%20of%20hyperparameters%202000"
        },
        {
            "id": "5",
            "entry": "[5] L. Calatroni, C. Chung, J. C. De los Reyes, C.-B. Sch\u00f6nlieb, and T. Valkonen. Bilevel approaches for learning of variational imaging models. Variational Methods, pages 252\u2013290, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calatroni%2C%20L.%20Chung%2C%20C.%20los%20Reyes%2C%20J.C.De%20Sch%C3%B6nlieb%2C%20C.-B.%20Bilevel%20approaches%20for%20learning%20of%20variational%20imaging%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calatroni%2C%20L.%20Chung%2C%20C.%20los%20Reyes%2C%20J.C.De%20Sch%C3%B6nlieb%2C%20C.-B.%20Bilevel%20approaches%20for%20learning%20of%20variational%20imaging%20models%202016"
        },
        {
            "id": "6",
            "entry": "[6] G. Chen and R. T. Rockafellar. Convergence rates in forward\u2013backward splitting. SIAM Journal on Optimization, 7(2):421\u2013444, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20G.%20Rockafellar%2C%20R.T.%20Convergence%20rates%20in%20forward%E2%80%93backward%20splitting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20G.%20Rockafellar%2C%20R.T.%20Convergence%20rates%20in%20forward%E2%80%93backward%20splitting%201997"
        },
        {
            "id": "7",
            "entry": "[7] P. L Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. SIAM Multiscale Modeling & Simulation, 4(4):1168\u20131200, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combettes%2C%20P.L.%20Wajs%2C%20V.R.%20Signal%20recovery%20by%20proximal%20forward-backward%20splitting%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combettes%2C%20P.L.%20Wajs%2C%20V.R.%20Signal%20recovery%20by%20proximal%20forward-backward%20splitting%202005"
        },
        {
            "id": "8",
            "entry": "[8] L. Condat. Fast projection onto the simplex and the 1-ball. Mathematical Programming, 158(1-2):575\u2013585, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Condat%2C%20L.%20Fast%20projection%20onto%20the%20simplex%20and%20the%201-ball%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Condat%2C%20L.%20Fast%20projection%20onto%20the%20simplex%20and%20the%201-ball%202016"
        },
        {
            "id": "9",
            "entry": "[9] A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27, pages 1646\u20131654, Montreal, Canada, 08\u201313 Dec 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014-12"
        },
        {
            "id": "10",
            "entry": "[10] S. Dempe. Foundations of Bilevel Programming. Springer, Boston, USA, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempe%2C%20S.%20Foundations%20of%20Bilevel%20Programming%202002"
        },
        {
            "id": "11",
            "entry": "[11] L. Franceschi, N. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 1165\u20131173, Sydney, Australia, 06\u201311 Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franceschi%2C%20L.%20Donini%2C%20N.%20Frasconi%2C%20P.%20Pontil%2C%20M.%20Forward%20and%20reverse%20gradient-based%20hyperparameter%20optimization%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franceschi%2C%20L.%20Donini%2C%20N.%20Frasconi%2C%20P.%20Pontil%2C%20M.%20Forward%20and%20reverse%20gradient-based%20hyperparameter%20optimization%202017-08"
        },
        {
            "id": "12",
            "entry": "[12] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter otimization and meta-learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 1568\u20131577, Stockholm, Sweden, 10\u201315 Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franceschi%2C%20L.%20Frasconi%2C%20P.%20Salzo%2C%20S.%20Grazzi%2C%20R.%20Bilevel%20programming%20for%20hyperparameter%20otimization%20and%20meta-learning%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franceschi%2C%20L.%20Frasconi%2C%20P.%20Salzo%2C%20S.%20Grazzi%2C%20R.%20Bilevel%20programming%20for%20hyperparameter%20otimization%20and%20meta-learning%202018-07"
        },
        {
            "id": "13",
            "entry": "[13] A. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Society for Industrial and Applied Mathematics, Philadelphia, USA, 2nd edition, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20A.%20Walther%2C%20A.%20Evaluating%20Derivatives%3A%20Principles%20and%20Techniques%20of%20Algorithmic%20Differentiation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griewank%2C%20A.%20Walther%2C%20A.%20Evaluating%20Derivatives%3A%20Principles%20and%20Techniques%20of%20Algorithmic%20Differentiation%202008"
        },
        {
            "id": "14",
            "entry": "[14] D. Hern\u00e1ndez-Lobato and J. M. Hern\u00e1ndez-Lobato. Learning feature selection dependencies in multi-task learning. In Advances in Neural Information Processing Systems 26, pages 746\u2013754, Lake Tahoe, USA, 05\u201310 Dec 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Learning%20feature%20selection%20dependencies%20in%20multi-task%20learning%202013-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Learning%20feature%20selection%20dependencies%20in%20multi-task%20learning%202013-12"
        },
        {
            "id": "15",
            "entry": "[15] L. Jacob, G. Obozinski, and J.-P. Vert. Group Lasso with overlap and graph Lasso. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 433\u2013440, Montreal, Canada, 14\u201318 Jun 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%2C%20L.%20Obozinski%2C%20G.%20Group%2C%20J.-P.Vert%20Lasso%20with%20overlap%20and%20graph%20Lasso%202009-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%2C%20L.%20Obozinski%2C%20G.%20Group%2C%20J.-P.Vert%20Lasso%20with%20overlap%20and%20graph%20Lasso%202009-06"
        },
        {
            "id": "16",
            "entry": "[16] R. Jenatton, J.-Y. Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms. Journal of Machine Learning Research, 12:2777\u20132824, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jenatton%2C%20R.%20Audibert%2C%20J.-Y.%20Bach%2C%20F.%20Structured%20variable%20selection%20with%20sparsity-inducing%20norms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jenatton%2C%20R.%20Audibert%2C%20J.-Y.%20Bach%2C%20F.%20Structured%20variable%20selection%20with%20sparsity-inducing%20norms%202011"
        },
        {
            "id": "17",
            "entry": "[17] S. Kim and E. Xing. Tree-guided group Lasso for multi-response regression with structured sparsity, with an application to eQTL mapping. The Annals of Applied Statistics, 6(3):1095\u2013 1117, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.%20Xing%2C%20E.%20Tree-guided%20group%20Lasso%20for%20multi-response%20regression%20with%20structured%20sparsity%2C%20with%20an%20application%20to%20eQTL%20mapping%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20S.%20Xing%2C%20E.%20Tree-guided%20group%20Lasso%20for%20multi-response%20regression%20with%20structured%20sparsity%2C%20with%20an%20application%20to%20eQTL%20mapping%202012"
        },
        {
            "id": "18",
            "entry": "[18] K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM Journal on Imaging Sciences, 6(2):938\u2013983, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kunisch%2C%20K.%20Pock%2C%20T.%20A%20bilevel%20optimization%20approach%20for%20parameter%20learning%20in%20variational%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kunisch%2C%20K.%20Pock%2C%20T.%20A%20bilevel%20optimization%20approach%20for%20parameter%20learning%20in%20variational%20models%202013"
        },
        {
            "id": "19",
            "entry": "[19] K. Lounici, M. Pontil, S. Van De Geer, and A. B. Tsybakov. Oracle inequalities and optimal inference under group sparsity. The Annals of Statistics, 39(4):2164\u20132204, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lounici%2C%20K.%20Pontil%2C%20M.%20Geer%2C%20S.Van%20De%20Tsybakov%2C%20A.B.%20Oracle%20inequalities%20and%20optimal%20inference%20under%20group%20sparsity%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lounici%2C%20K.%20Pontil%2C%20M.%20Geer%2C%20S.Van%20De%20Tsybakov%2C%20A.B.%20Oracle%20inequalities%20and%20optimal%20inference%20under%20group%20sparsity%202011"
        },
        {
            "id": "20",
            "entry": "[20] S. Ma, X. Song, and J. Huang. Supervised group Lasso with applications to microarray data analysis. BMC Bioinformatics, 8(1):60, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20S.%20Song%2C%20X.%20Huang%2C%20J.%20Supervised%20group%20Lasso%20with%20applications%20to%20microarray%20data%20analysis%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20S.%20Song%2C%20X.%20Huang%2C%20J.%20Supervised%20group%20Lasso%20with%20applications%20to%20microarray%20data%20analysis%202007"
        },
        {
            "id": "21",
            "entry": "[21] D. Maclaurin, D. Duvenaud, and R. P. Adams. Gradient-based hyperparameter optimization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 2113\u20132122, Lille, France, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclaurin%2C%20D.%20Duvenaud%2C%20D.%20Adams%2C%20R.P.%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maclaurin%2C%20D.%20Duvenaud%2C%20D.%20Adams%2C%20R.P.%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015"
        },
        {
            "id": "22",
            "entry": "[22] A. Maurer and M. Pontil. Structured sparsity and generalization. Journal of Machine Learning Research, 13:671\u2013690, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20Pontil%2C%20M.%20Structured%20sparsity%20and%20generalization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20A.%20Pontil%2C%20M.%20Structured%20sparsity%20and%20generalization%202012"
        },
        {
            "id": "23",
            "entry": "[23] C. A. Micchelli, J. Morales, and M. Pontil. Regularizers for structured sparsity. Advances in Computational Mathematics, 38(3):455\u2013489, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Micchelli%2C%20C.A.%20Morales%2C%20J.%20Pontil%2C%20M.%20Regularizers%20for%20structured%20sparsity%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Micchelli%2C%20C.A.%20Morales%2C%20J.%20Pontil%2C%20M.%20Regularizers%20for%20structured%20sparsity%202013"
        },
        {
            "id": "24",
            "entry": "[24] P. Ochs, R. Ranftl, T. Brox, and T. Pock. Techniques for gradient-based bilevel optimization with non-smooth lower level problems. Journal of Mathematical Imaging and Vision, 56(2):175\u2013194, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ochs%2C%20P.%20Ranftl%2C%20R.%20Brox%2C%20T.%20Pock%2C%20T.%20Techniques%20for%20gradient-based%20bilevel%20optimization%20with%20non-smooth%20lower%20level%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ochs%2C%20P.%20Ranftl%2C%20R.%20Brox%2C%20T.%20Pock%2C%20T.%20Techniques%20for%20gradient-based%20bilevel%20optimization%20with%20non-smooth%20lower%20level%20problems%202016"
        },
        {
            "id": "25",
            "entry": "[25] M. Re and G. Valentini. Predicting gene expression from heterogeneous data. In International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics, Genoa, Italy, 15-17 Oct 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Re%2C%20M.%20Valentini%2C%20G.%20Predicting%20gene%20expression%20from%20heterogeneous%20data.%20In%20International%20Meeting%20on%20Computational%20Intelligence%20Methods%20for%20Bioinformatics%20and%20Biostatistics%202009-10-15"
        },
        {
            "id": "26",
            "entry": "[26] S. Reddi, S. Sra, B. Poczos, and A. Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. In Advances in Neural Information Processing Systems 29, pages 1145\u20131153, Barcelona, Spain, 05\u201310 Dec 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.%20Sra%2C%20S.%20Poczos%2C%20B.%20Smola%2C%20A.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.%20Sra%2C%20S.%20Poczos%2C%20B.%20Smola%2C%20A.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016-12"
        },
        {
            "id": "27",
            "entry": "[27] N. Shervashidze and F. Bach. Learning the structure for structured sparsity. IEEE Transactions on Signal Processing, 63(18):4894\u20134902, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shervashidze%2C%20N.%20Bach%2C%20F.%20Learning%20the%20structure%20for%20structured%20sparsity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shervashidze%2C%20N.%20Bach%2C%20F.%20Learning%20the%20structure%20for%20structured%20sparsity%202015"
        },
        {
            "id": "28",
            "entry": "[28] Q. Van Nguyen. Forward-backward splitting with Bregman distances. Vietnam Journal of Mathematics, 45(3):519\u2013539, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Q.Van%20Forward-backward%20splitting%20with%20Bregman%20distances%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Q.Van%20Forward-backward%20splitting%20with%20Bregman%20distances%202017"
        },
        {
            "id": "29",
            "entry": "[29] M. J. Wainwright. Structured regularizers for high-dimensional problems: statistical and computational issues. Annual Review of Statistics and Its Application, 1:233\u2013253, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Structured%20regularizers%20for%20high-dimensional%20problems%3A%20statistical%20and%20computational%20issues%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20M.J.%20Structured%20regularizers%20for%20high-dimensional%20problems%3A%20statistical%20and%20computational%20issues%202014"
        },
        {
            "id": "30",
            "entry": "[30] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49\u201367, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20M.%20Lin%2C%20Y.%20Model%20selection%20and%20estimation%20in%20regression%20with%20grouped%20variables%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20M.%20Lin%2C%20Y.%20Model%20selection%20and%20estimation%20in%20regression%20with%20grouped%20variables%202006"
        },
        {
            "id": "31",
            "entry": "[31] P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical variable selection. The Annals of Statistics, pages 3468\u20133497, 2009. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20P.%20Rocha%2C%20G.%20Yu%2C%20B.%20The%20composite%20absolute%20penalties%20family%20for%20grouped%20and%20hierarchical%20variable%20selection%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20P.%20Rocha%2C%20G.%20Yu%2C%20B.%20The%20composite%20absolute%20penalties%20family%20for%20grouped%20and%20hierarchical%20variable%20selection%202009"
        }
    ]
}
