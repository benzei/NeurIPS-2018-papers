{
    "filename": "7412-regularization-learning-networks-deep-learning-for-tabular-datasets.pdf",
    "metadata": {
        "title": "Regularization Learning Networks: Deep Learning for Tabular Datasets",
        "author": "Ira Shavitt, Eran Segal",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7412-regularization-learning-networks-deep-learning-for-tabular-datasets.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at https://github.com/irashavitt/regularization_ learning_networks."
    },
    "keywords": [
        {
            "term": "medical imaging",
            "url": "https://en.wikipedia.org/wiki/medical_imaging"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "gradient boosting",
            "url": "https://en.wikipedia.org/wiki/gradient_boosting"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "electronic health record",
            "url": "https://en.wikipedia.org/wiki/electronic_health_record"
        },
        {
            "term": "Linear Models",
            "url": "https://en.wikipedia.org/wiki/Linear_Models"
        }
    ],
    "highlights": [
        "Despite their impressive achievements on various prediction tasks on datasets with distributed representation [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] such as images [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], speech [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], and text [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], there are many tasks in which Deep Neural Networks (DNNs) underperform compared to other models such as Gradient Boosting Trees (GBTs)",
        "We show that Regularization Learning Networks significantly and substantially outperform Deep Neural Networks with other regularization schemes, and achieve comparable results to Gradient Boosting Trees",
        "We explore the learning of datasets with non-distributed representation, such as tabular datasets",
        "We hypothesize that modular regularization could boost the performance of Deep Neural Networks on such tabular datasets",
        "We introduce the Counterfactual Loss, LCF , and Regularization Learning Networks (RLNs) which use the Counterfactual Loss to tune its regularization hyperparameters efficiently during learning together with the learning of the weights of the network",
        "The modular structure of the network is especially beneficial for datasets with high variability in the relative importance of the input features, where Regularization Learning Networks particularly shine compared to Deep Neural Networks"
    ],
    "key_statements": [
        "Despite their impressive achievements on various prediction tasks on datasets with distributed representation [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] such as images [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], speech [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], and text [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], there are many tasks in which Deep Neural Networks (DNNs) underperform compared to other models such as Gradient Boosting Trees (GBTs). This is evident in various Kaggle [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], or KDD Cup [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] competitions, which are typically won by Gradient Boosting Trees-based approaches and specifically by its XGBoost [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] implementation, either when run alone or within a combination of several different types of models",
        "The datasets in which neural networks are inferior to Gradient Boosting Trees typically have different statistical properties",
        "The relative contribution of the input features in the electronic health records example can vary greatly: Changing a single input such as the age of the patient can profoundly impact the life expectancy of the patient, while changes in other input features, such as the time that passed since the last test was taken, may have smaller effects.\n1This is not contradictory to the existence of adversarial examples [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], which are able to fool Deep Neural Networks by changing a small number of input features, but do not depict a different object, and generally are not able to fool humans.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "We show that Regularization Learning Networks significantly and substantially outperform Deep Neural Networks with other regularization schemes, and achieve comparable results to Gradient Boosting Trees",
        "We introduce a new loss function, called the Counterfactual Loss LCF , which has a non-trivial dependency on \u039b and can be evaluated efficiently",
        "We took the hyperparameter setting that achieved the best results on the HbA1c task for the Deep Neural Networks and Regularization Learning Networks models and trained a single network on the entire dataset",
        "We explore the learning of datasets with non-distributed representation, such as tabular datasets",
        "We hypothesize that modular regularization could boost the performance of Deep Neural Networks on such tabular datasets",
        "We introduce the Counterfactual Loss, LCF , and Regularization Learning Networks (RLNs) which use the Counterfactual Loss to tune its regularization hyperparameters efficiently during learning together with the learning of the weights of the network",
        "We further explore Regularization Learning Networks structure and dynamics and show that Regularization Learning Networks learn extremely sparse networks, eliminating 99.8% of the network edges and 82% of the input features",
        "The modular structure of the network is especially beneficial for datasets with high variability in the relative importance of the input features, where Regularization Learning Networks particularly shine compared to Deep Neural Networks"
    ],
    "summary": [
        "Despite their impressive achievements on various prediction tasks on datasets with distributed representation [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] such as images [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], speech [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], and text [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], there are many tasks in which Deep Neural Networks (DNNs) underperform compared to other models such as Gradient Boosting Trees (GBTs).",
        "One way to overcome this limitation could be to assign a different regularization coefficient to every weight, which might allow the network to accommodate the non-distributed representation and the variability in relative importance found in tabular datasets.",
        "In RLNs, the regularization coefficients are optimized together with learning the network weight parameters.",
        "Training RLNs doesn\u2019t require a validation set, assigns a different regularization coefficient for every weight, which results in up to millions of hyperparameters, optimized efficiently.",
        "N i=1 wi , where Z = {}m M=1 are the training samples, L is the loss function, W = {wi}in=1 are the weights of the model, \u00b7 is some norm, and \u03bb is the regularization coefficient,2 a hyperparameter of the network.",
        "We will use LCF to optimize the regularization coefficients using SGD while learning the weights of the network simultaneously using L\u2020.",
        "We took the hyperparameter setting that achieved the best results on the HbA1c task for the DNN and RLN models and trained a single network on the entire dataset.",
        "We reasoned that since RLNs assign non-zero weights to a relatively small number of inputs, they may be used to provide insights into the inputs that the model found to be more important for generating its predictions using Garson\u2019s algorithm [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>].",
        "We test our method on the task of predicting human traits from covariates and microbiome data and show that RLNs significantly and substantially improve the performance over classical DNNs, achieving an increased explained variance by a factor of 2.75 \u00b1 0.05 and comparable results with GBTs. The use of ensembles further improves the performance of RLNs, and ensembles of RLN and GBT achieve the best results on all but one of the traits, and outperform significantly any other ensemble not incorporating RLNs on 3 of the traits.",
        "The modular structure of the network is especially beneficial for datasets with high variability in the relative importance of the input features, where RLNs particularly shine compared to DNNs. The sparse structure of RLNs lends itself naturally to model interpretability, which gives meaningful insights into the relation between features and the labels, and may itself serve as a feature selection technique that can have many uses on its own [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>].",
        "Besides improving performance on tabular datasets, another important application of RLNs could be learning tasks where there are multiple data sources, one that includes features with high variability in the relative importance, and one which does not.",
        "The use of ensembles further improves the performance of RLNs, and ensembles of RLN and GBT achieve the best results on all but one of the traits, and outperform significantly any other ensemble not incorporating RLNs on 3 of the traits"
    ],
    "headline": "We propose that applying a different regularization coefficient to each weight might boost the performance of Deep Neural Networks by allowing them to make more use of the more relevant inputs",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] David Beam and Mark Schramm. Rossmann Store Sales. 2015. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20Beam%20and%20Mark%20Schramm%20Rossmann%20Store%20Sales%202015%201"
        },
        {
            "id": "2",
            "entry": "[2] Kamil Belkhayat, Abou Omar, Gino Bruner, Yuyi Wang, and Roger Wattenhofer. XGBoost and LGBM for Porto Seguro\u2019s Kaggle challenge: A comparison Semester Project. 2018. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkhayat%2C%20Kamil%20Omar%2C%20Abou%20Bruner%2C%20Gino%20Wang%2C%20Yuyi%20XGBoost%20and%20LGBM%20for%20Porto%20Seguro%E2%80%99s%20Kaggle%20challenge%3A%20A%20comparison%20Semester%20Project%202018"
        },
        {
            "id": "3",
            "entry": "[3] Yoshua Bengio. Gradient-Based Optimization of 1 Introduction. pages 1\u201318, 1999. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Gradient-Based%20Optimization%20of%201%20Introduction%201999"
        },
        {
            "id": "4",
            "entry": "[4] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New Perspectives. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20Learning%3A%20A%20Review%20and%20New%20Perspectives"
        },
        {
            "id": "5",
            "entry": "[5] Yoshua Bengio and Yann LeCun. Scaling Learning Algorithms towards AI. 2007. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20LeCun%2C%20Yann%20Scaling%20Learning%20Algorithms%20towards%20AI%202007"
        },
        {
            "id": "6",
            "entry": "[6] James Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for Hyper-Parameter Optimization. Advances in Neural Information Processing Systems (NIPS), pages 2546\u20132554, 2011. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=James%20Bergstra%2C%20R%C3%A9mi%20Bardenet%2C%20Yoshua%20Bengio%20K%C3%A9gl%2C%20Bal%C3%A1zs%20Algorithms%20for%20Hyper-Parameter%20Optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=James%20Bergstra%2C%20R%C3%A9mi%20Bardenet%2C%20Yoshua%20Bengio%20K%C3%A9gl%2C%20Bal%C3%A1zs%20Algorithms%20for%20Hyper-Parameter%20Optimization%202011"
        },
        {
            "id": "7",
            "entry": "[7] Hengxing Cai, Runxing Zhong, Chaohe Wang, Kejie Zhou, Hongyun Lee, Renxin Zhong, Yao Zhou, Da Li, Nan Jiang, Xu Cheng, and Jiawei Shen. KDD CUP 2018 Travel Time Prediction. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Hengxing%20Zhong%2C%20Runxing%20Wang%2C%20Chaohe%20Zhou%2C%20Kejie%20Travel%20Time%20Prediction%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Hengxing%20Zhong%2C%20Runxing%20Wang%2C%20Chaohe%20Zhou%2C%20Kejie%20Travel%20Time%20Prediction%202018"
        },
        {
            "id": "8",
            "entry": "[8] Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Guestrin%2C%20Carlos%20XGBoost%3A%20A%20Scalable%20Tree%20Boosting%20System"
        },
        {
            "id": "9",
            "entry": "[9] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, and Michiel Bacchiani Google. State-Of-The-Art Speech Recognition with Sequence-To-Sequence Models. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chiu%2C%20Chung-Cheng%20Sainath%2C%20Tara%20N.%20Wu%2C%20Yonghui%20Prabhavalkar%2C%20Rohit%20and%20Michiel%20Bacchiani%20Google.%20State-Of-The-Art%20Speech%20Recognition%20with%20Sequence-To-Sequence%20Models"
        },
        {
            "id": "10",
            "entry": "[10] G D Garson. Interpreting neural network connection weights. AI Expert, 6(4):47\u201351, apr 1991. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garson%2C%20G.D.%20Interpreting%20neural%20network%20connection%20weights%201991-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garson%2C%20G.D.%20Interpreting%20neural%20network%20connection%20weights%201991-04"
        },
        {
            "id": "11",
            "entry": "[11] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.1",
            "url": "http://www.deeplearningbook.org.1"
        },
        {
            "id": "12",
            "entry": "[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining And Harnessing Adversarial Examples. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20And%20Harnessing%20Adversarial%20Examples"
        },
        {
            "id": "13",
            "entry": "[13] Bryce Goodman and Seth Flaxman. European Union regulations on algorithmic decision-making and a \" right to explanation \". 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodman%2C%20Bryce%20Flaxman%2C%20Seth%20European%20Union%20regulations%20on%20algorithmic%20decision-making%20and%20a%20%22%20right%20to%20explanation%20%22"
        },
        {
            "id": "14",
            "entry": "[14] GE HINTON, JL MCCLELLAND, and DE RUMELHART. Distributed representations. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GE%20HINTON%20JL%20MCCLELLAND%20and%20DE%20RUMELHART%20Distributed%20representations%201"
        },
        {
            "id": "15",
            "entry": "[15] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating Feature Importance Estimates. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hooker%2C%20Sara%20Erhan%2C%20Dumitru%20Kindermans%2C%20Pieter-Jan%20Kim%2C%20Been%20Evaluating%20Feature%20Importance%20Estimates%206"
        },
        {
            "id": "16",
            "entry": "[16] Yide Huang. Highway Tollgates Traffic Flow Prediction Task 1. Travel Time Prediction. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Yide%20Highway%20Tollgates%20Traffic%20Flow%20Prediction%20Task%201.%20Travel%20Time%20Prediction"
        },
        {
            "id": "17",
            "entry": "[17] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential Model - Based Optimization for General Algorithm Configuration. Lecture Notes in Computer Science, 5:507\u2013223, 2011. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hutter%2C%20Frank%20Hoos%2C%20Holger%20H.%20Leyton-Brown%2C%20Kevin%20Sequential%20Model%20-%20Based%20Optimization%20for%20General%20Algorithm%20Configuration%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hutter%2C%20Frank%20Hoos%2C%20Holger%20H.%20Leyton-Brown%2C%20Kevin%20Sequential%20Model%20-%20Based%20Optimization%20for%20General%20Algorithm%20Configuration%202011"
        },
        {
            "id": "18",
            "entry": "[18] Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. 2016. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melvin%20Johnson%20Mike%20Schuster%20Quoc%20V%20Le%20Maxim%20Krikun%20Yonghui%20Wu%20Zhifeng%20Chen%20Nikhil%20Thorat%20Fernanda%20Vi%C3%A9gas%20Martin%20Wattenberg%20Greg%20Corrado%20Macduff%20Hughes%20and%20Jeffrey%20Dean%20Googles%20Multilingual%20Neural%20Machine%20Translation%20System%20Enabling%20ZeroShot%20Translation%202016%201",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melvin%20Johnson%20Mike%20Schuster%20Quoc%20V%20Le%20Maxim%20Krikun%20Yonghui%20Wu%20Zhifeng%20Chen%20Nikhil%20Thorat%20Fernanda%20Vi%C3%A9gas%20Martin%20Wattenberg%20Greg%20Corrado%20Macduff%20Hughes%20and%20Jeffrey%20Dean%20Googles%20Multilingual%20Neural%20Machine%20Translation%20System%20Enabling%20ZeroShot%20Translation%202016%201"
        },
        {
            "id": "19",
            "entry": "[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks"
        },
        {
            "id": "20",
            "entry": "[20] Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/.1",
            "url": "http://yann.lecun.com/exdb/mnist/.1"
        },
        {
            "id": "21",
            "entry": "[21] Jonathan Lorraine and David Duvenaud. Stochastic Hyperparameter Optimization through Hypernetworks. 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lorraine%2C%20Jonathan%20Duvenaud%2C%20David%20Stochastic%20Hyperparameter%20Optimization%20through%20Hypernetworks%202018"
        },
        {
            "id": "22",
            "entry": "[22] Jelena Luketina, Jelena Luketina@aalto Fi, Mathias Berglund, Mathias Berglund@aalto Fi, Klaus Greff, Klaus@idsia Ch, Tapani Raiko, and Tapani Raiko@aalto Fi. Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luketina%2C%20Jelena%20Fi%2C%20Jelena%20Luketina%40aalto%20Berglund%2C%20Mathias%20Fi%2C%20Mathias%20Berglund%40aalto%20Scalable%20Gradient-Based%20Tuning%20of%20Continuous%20Regularization%20Hyperparameters"
        },
        {
            "id": "23",
            "entry": "[23] Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Gradient-based Hyperparameter Optimization through Reversible Learning. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20P.%20Gradient-based%20Hyperparameter%20Optimization%20through%20Reversible%20Learning"
        },
        {
            "id": "24",
            "entry": "[24] Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records. Nature Publishing Group, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miotto%2C%20Riccardo%20Li%2C%20Li%20Kidd%2C%20Brian%20A.%20Dudley%2C%20Joel%20T.%20Deep%20Patient%3A%20An%20Unsupervised%20Representation%20to%20Predict%20the%20Future%20of%20Patients%20from%20the%20Electronic%20Health%20Records%202016"
        },
        {
            "id": "25",
            "entry": "[25] Nicolas Papernot, Patrick Mcdaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The Limitations of Deep Learning in Adversarial Settings. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20Mcdaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20Limitations%20of%20Deep%20Learning%20in%20Adversarial%20Settings"
        },
        {
            "id": "26",
            "entry": "[26] Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Michaela Hardt, Peter J Liu, Xiaobing Liu, Jake Marcus, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang, Yi Zhang, Gerardo Flores, Gavin E Duggan, Jamie Irvine, Quoc Le, Kurt Litsch, Alexander Mossin, Justin Tansuwan, De Wang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L Volchenboum, Katherine Chou, Michael Pearson, Srinivasan Madabushi, Nigam H Shah, Atul J Butte, Michael D Howell, Claire Cui, Greg S Corrado, and Jeffrey Dean. Scalable and accurate deep learning with electronic health records. npj Digital Medicine, 1, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajkomar%2C%20Alvin%20Oren%2C%20Eyal%20Chen%2C%20Kai%20Dai%2C%20Andrew%20M.%20Scalable%20and%20accurate%20deep%20learning%20with%20electronic%20health%20records%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajkomar%2C%20Alvin%20Oren%2C%20Eyal%20Chen%2C%20Kai%20Dai%2C%20Andrew%20M.%20Scalable%20and%20accurate%20deep%20learning%20with%20electronic%20health%20records%202018"
        },
        {
            "id": "27",
            "entry": "[27] Vlad Sandulescu, Adform Copenhagen, and Denmark Mihai Chiru. Predicting the future relevance of research institutions - The winning solution of the KDD Cup 2016. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sandulescu%2C%20Vlad%20Copenhagen%2C%20Adform%20Chiru%2C%20Denmark%20Mihai%20Predicting%20the%20future%20relevance%20of%20research%20institutions%20-%20The%20winning%20solution%20of%20the%20KDD%20Cup%202016"
        },
        {
            "id": "28",
            "entry": "[28] Avanti Shrikumar, Peyton Greenside, and Anna Y Shcherbina. Not Just A Black Box: Learning Important Features Through Propagating Activation Differences. (3). 6, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrikumar%2C%20Avanti%20Greenside%2C%20Peyton%20Shcherbina%2C%20Anna%20Y.%20Not%20Just%20A%20Black%20Box%3A%20Learning%20Important%20Features%20Through%20Propagating%20Activation%20Differences%20%283%29."
        },
        {
            "id": "29",
            "entry": "[29] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum, and weight decay. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Leslie%20N.%20A%20disciplined%20approach%20to%20neural%20network%20hyper-parameters%3A%20Part%201%20-%20learning%20rate%2C%20batch%20size%2C%20momentum%2C%20and%20weight%20decay"
        },
        {
            "id": "30",
            "entry": "[30] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. pages 1\u201312, 2012. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20Bayesian%20Optimization%20of%20Machine%20Learning%20Algorithms%202012"
        },
        {
            "id": "31",
            "entry": "[31] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Gradients of Counterfactuals. 6, 4",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sundararajan%2C%20Mukund%20Taly%2C%20Ankur%20Yan%2C%20Qiqi%20Gradients%20of%20Counterfactuals%206%2C"
        },
        {
            "id": "32",
            "entry": "[32] Kenji Suzuki. Overview of deep learning in medical imaging. Radiological Physics and Technology, 10. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suzuki%2C%20Kenji%20Overview%20of%20deep%20learning%20in%20medical%20imaging",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suzuki%2C%20Kenji%20Overview%20of%20deep%20learning%20in%20medical%20imaging"
        }
    ]
}
