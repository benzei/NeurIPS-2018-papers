{
    "filename": "7844-evolutionary-stochastic-gradient-descent-for-optimization-of-deep-neural-networks.pdf",
    "metadata": {
        "title": "Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks",
        "author": "Xiaodong Cui, Wei Zhang, Zolt\u00e1n T\u00fcske, Michael Picheny",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7844-evolutionary-stochastic-gradient-descent-for-optimization-of-deep-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD) framework for optimizing deep neural networks. ESGD combines SGD and gradient-free evolutionary algorithms as complementary algorithms in one framework in which the optimization alternates between the SGD step and evolution step to improve the average fitness of the population. With a back-off strategy in the SGD step and an elitist strategy in the evolution step, it guarantees that the best fitness in the population will never degrade. In addition, individuals in the population optimized with various SGD-based optimizers using distinct hyperparameters in the SGD step are considered as competing species in a coevolution setting such that the complementarity of the optimizers is also taken into account. The effectiveness of ESGD is demonstrated across multiple applications including speech recognition, image recognition and language modeling, using networks with a variety of deep architectures."
    },
    "keywords": [
        {
            "term": "evolutionary algorithms",
            "url": "https://en.wikipedia.org/wiki/evolutionary_algorithms"
        },
        {
            "term": "evolutionary strategies",
            "url": "https://en.wikipedia.org/wiki/evolutionary_strategies"
        },
        {
            "term": "artificial neural network",
            "url": "https://en.wikipedia.org/wiki/artificial_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "genetic algorithms",
            "url": "https://en.wikipedia.org/wiki/genetic_algorithms"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Penn Treebank",
            "url": "https://en.wikipedia.org/wiki/Penn_Treebank"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Stochastic gradient descent (SGD) is the dominant technique in deep neural network optimization [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We investigate Evolutionary Stochastic Gradient Descent in the conventional setting of supervised learning of deep neural networks with a fixed architecture without explicit tuning of hyper-parameters",
        "In the supplementary material examples are given where we show the optimizers with their training hyper-parameters selected by the best individuals in Evolutionary Stochastic Gradient Descent in each generation",
        "We have presented the population-based Evolutionary Stochastic Gradient Descent as an optimization framework to combine Stochastic gradient descent and gradient-free evolutionary algorithm to explore their complementarity",
        "The Stochastic gradient descent step can be interpreted as a coevolution mechanism where individuals under distinct optimizers evolve independently and interact with each other in the evolution step to hopefully create promising candidate solutions for the generation",
        "The experimental results have demonstrated the effectiveness of Evolutionary Stochastic Gradient Descent"
    ],
    "key_statements": [
        "Stochastic gradient descent (SGD) is the dominant technique in deep neural network optimization [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "In this paper we propose Evolutionary Stochastic Gradient Descent (ESGD) \u2013 a framework to combine the merits of Stochastic gradient descent and evolutionary algorithms by treating them as complementary optimization techniques.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "We investigate Evolutionary Stochastic Gradient Descent in the conventional setting of supervised learning of deep neural networks with a fixed architecture without explicit tuning of hyper-parameters",
        "The evolution inside each Evolutionary Stochastic Gradient Descent generation for improving Jm :\u03bc alternates between the Stochastic gradient descent step, where each individual \u03b8j is updated using the stochastic gradient of the fitness function Rn, and the evolution step, where the gradient-free evolutionary algorithms is applied using certain transformation and selection operators",
        "Generate offspring population \u03a8\u03bb(k) \u2190 {\u03b81(k), \u00b7 \u00b7 \u00b7 , \u03b8\u03bb(k)}; Sort the fitness of the parent and offspring population \u03a8\u03bb \u2190 \u03a8\u03bc(k) \u03a8 ; Select the top m (m \u2264 \u03bc) individuals with the best fitness (m-elitist); Update population \u03a8\u03bc(k) by combining m-elitist and randomly selected \u03bc\u2212m non-m-elitist candidates; end end step, an Stochastic gradient descent-based optimizer \u03c0j with certain hyper-parameters and learning schedule is selected for each individual \u03b8j which is updated by Ks epochs",
        "We evaluate the performance of the proposed Evolutionary Stochastic Gradient Descent on large vocabulary continuous speech recognition (LVCSR), image recognition and language modeling",
        "Note that the above implementation for Penn Treebank experiments can be viewed as another variant of Evolutionary Stochastic Gradient Descent: Suppose we have a well-trained model (e.g. a competitive baseline model) which is always inserted into the population in each generation of evolution",
        "In Fig.1 we show the fitness as a function of Evolutionary Stochastic Gradient Descent generations in the four investigated tasks",
        "We find that the m-elitist strategy applied to the whole population, has a better overall average fitness, can give rise to premature convergence in the early stage",
        "In the supplementary material examples are given where we show the optimizers with their training hyper-parameters selected by the best individuals in Evolutionary Stochastic Gradient Descent in each generation",
        "We have presented the population-based Evolutionary Stochastic Gradient Descent as an optimization framework to combine Stochastic gradient descent and gradient-free evolutionary algorithm to explore their complementarity",
        "The Stochastic gradient descent step can be interpreted as a coevolution mechanism where individuals under distinct optimizers evolve independently and interact with each other in the evolution step to hopefully create promising candidate solutions for the generation",
        "The experimental results have demonstrated the effectiveness of Evolutionary Stochastic Gradient Descent"
    ],
    "summary": [
        "Stochastic gradient descent (SGD) is the dominant technique in deep neural network optimization [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "ESGD works with a population of candidate solutions as individuals.",
        "The evolution inside each ESGD generation for improving Jm :\u03bc alternates between the SGD step, where each individual \u03b8j is updated using the stochastic gradient of the fitness function Rn, and the evolution step, where the gradient-free EA is applied using certain transformation and selection operators.",
        "Pick an optimizer \u03c0j(k) for individual \u03b8j(k); Select hyper-parameters of \u03c0j(k) and set a learning schedule; // Ks SGD steps for s = 1 : Ks do",
        "Generate offspring population \u03a8\u03bb(k) \u2190 {\u03b81(k), \u00b7 \u00b7 \u00b7 , \u03b8\u03bb(k)}; Sort the fitness of the parent and offspring population \u03a8\u03bb \u2190 \u03a8\u03bc(k) \u03a8 ; Select the top m (m \u2264 \u03bc) individuals with the best fitness (m-elitist); Update population \u03a8\u03bc(k) by combining m-elitist and randomly selected \u03bc\u2212m non-m-elitist candidates; end end step, an SGD-based optimizer \u03c0j with certain hyper-parameters and learning schedule is selected for each individual \u03b8j which is updated by Ks epochs.",
        "The first baseline system, denoted \u201csingle baseline\u201d when reporting experimental results, is a well-established single model system with respect to the application under investigation, trained using certain SGD-based optimizer with appropriately selected hyper-parameters following certain training schedule.",
        "Note that the above implementation for PTB experiments can be viewed as another variant of ESGD: Suppose we have a well-trained model (e.g. a competitive baseline model) which is always inserted into the population in each generation of evolution.",
        "ESGD can automatically choose optimizers and their appropriate hyper-parameters based on the fitness value during the evolution process so that the merits of both SGD and ADAM can be combined to seek a better local optimal solution to the problem of interest.",
        "In the supplementary material examples are given where we show the optimizers with their training hyper-parameters selected by the best individuals in ESGD in each generation.",
        "We have presented the population-based ESGD as an optimization framework to combine SGD and gradient-free evolutionary algorithm to explore their complementarity.",
        "ESGD alternately optimizes the m-elitist average fitness of the population between an SGD step and an evolution step.",
        "The SGD step can be interpreted as a coevolution mechanism where individuals under distinct optimizers evolve independently and interact with each other in the evolution step to hopefully create promising candidate solutions for the generation."
    ],
    "headline": "We propose a population-based Evolutionary Stochastic Gradient Descent  framework for optimizing deep neural networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. arXiv preprint arXiv:1606.04838, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04838"
        },
        {
            "id": "2",
            "entry": "[2] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "3",
            "entry": "[3] D. P. Kingma and J. L. Ba. ADAM: a method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20ADAM%3A%20a%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20ADAM%3A%20a%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "4",
            "entry": "[4] T. Tieleman and G. Hinton. Lecture 6.e. RMSProp: divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.e.%20RMSProp%3A%20divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.e.%20RMSProp%3A%20divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "5",
            "entry": "[5] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Soviet Math Docl, 269:543\u2013547, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20A%20method%20for%20unconstrained%20convex%20minimization%20problem%20with%20the%20rate%20of%20convergence%20o%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20A%20method%20for%20unconstrained%20convex%20minimization%20problem%20with%20the%20rate%20of%20convergence%20o%281/k2%29%201983"
        },
        {
            "id": "6",
            "entry": "[6] H.-G. Beyer and H.-P. Schwefel. Evolution strategies: a comprehensive introduction. Natural computing, 1(1):3\u201352, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beyer%2C%20H.-G.%20Schwefel%2C%20H.-P.%20Evolution%20strategies%3A%20a%20comprehensive%20introduction%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beyer%2C%20H.-G.%20Schwefel%2C%20H.-P.%20Evolution%20strategies%3A%20a%20comprehensive%20introduction%202002"
        },
        {
            "id": "7",
            "entry": "[7] D. E. Goldberg. Genetic algorithm in search, optimization and machine learning. AddisonWesley Publishing Co., 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberg%2C%20D.E.%20Genetic%20algorithm%20in%20search%2C%20optimization%20and%20machine%20learning%201989"
        },
        {
            "id": "8",
            "entry": "[8] C. Igel. Neuroevolution for reinforcement learning using evolution strategies. In IEEE Congress of Evolutionary Computation (CEC), page 2588\u20132595, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Igel%2C%20C.%20Neuroevolution%20for%20reinforcement%20learning%20using%20evolution%20strategies",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Igel%2C%20C.%20Neuroevolution%20for%20reinforcement%20learning%20using%20evolution%20strategies"
        },
        {
            "id": "9",
            "entry": "[9] N. Hansen. The CMA evolution strategy: a tutorial. arXiv preprint arXiv:1604.00772, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.00772"
        },
        {
            "id": "10",
            "entry": "[10] I. Loshchilov. LM-CMA: an alternative to L-BFGS for large scale black-box optimization. Evolutionary Computation, 25(1):143\u2013171, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20I.%20LM-CMA%3A%20an%20alternative%20to%20L-BFGS%20for%20large%20scale%20black-box%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20I.%20LM-CMA%3A%20an%20alternative%20to%20L-BFGS%20for%20large%20scale%20black-box%20optimization%202017"
        },
        {
            "id": "11",
            "entry": "[11] E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning (ICML), pages 2902\u20132911, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Real%2C%20E.%20Moore%2C%20S.%20Selle%2C%20A.%20Saxena%2C%20S.%20Large-scale%20evolution%20of%20image%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Real%2C%20E.%20Moore%2C%20S.%20Selle%2C%20A.%20Saxena%2C%20S.%20Large-scale%20evolution%20of%20image%20classifiers%202017"
        },
        {
            "id": "12",
            "entry": "[12] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01548"
        },
        {
            "id": "13",
            "entry": "[13] J. Liang, E. Meyerson, and R. Miikkulainen. Evolutionary architecture search for deep multitask networks. arXiv preprint arXiv:1803.03745, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03745"
        },
        {
            "id": "14",
            "entry": "[14] S. Ebrahimi, A. Rohrbach, and T. Darrell. Gradient-free policy architecture search and adaptation. In Conference on Robot Learning (CoRL), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ebrahimi%2C%20S.%20Rohrbach%2C%20A.%20Darrell%2C%20T.%20Gradient-free%20policy%20architecture%20search%20and%20adaptation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ebrahimi%2C%20S.%20Rohrbach%2C%20A.%20Darrell%2C%20T.%20Gradient-free%20policy%20architecture%20search%20and%20adaptation%202017"
        },
        {
            "id": "15",
            "entry": "[15] R. Miikkulainen, J. Liang, E. Meyerson, A. Rawal, D. Fink, O. Francon, B. Raju, H. Shahrzad, A. Navruzyan, N. Nuffy, and B. Hodjat. Evolving deep neural networks. arXiv preprint arXiv:1703.00548, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00548"
        },
        {
            "id": "16",
            "entry": "[16] I. Loshchilov and F. Hutter. CMA-ES for hyperparameter optimization of deep neural networks. In International Conference on Learning Representations (ICLR), workshop track, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20I.%20Hutter%2C%20F.%20CMA-ES%20for%20hyperparameter%20optimization%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20I.%20Hutter%2C%20F.%20CMA-ES%20for%20hyperparameter%20optimization%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fernando, and K. Kavukcuoglu. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09846"
        },
        {
            "id": "18",
            "entry": "[18] G. Morse and K. O. Stanley. Simple evolutionary optimization can rival stochastic gradient descent in neural networks. In The Genetic and Evolutionary Computation Conference (GECCO), pages 477\u2013484, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morse%2C%20G.%20Stanley%2C%20K.O.%20Simple%20evolutionary%20optimization%20can%20rival%20stochastic%20gradient%20descent%20in%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morse%2C%20G.%20Stanley%2C%20K.O.%20Simple%20evolutionary%20optimization%20can%20rival%20stochastic%20gradient%20descent%20in%20neural%20networks%202016"
        },
        {
            "id": "19",
            "entry": "[19] Z. Yang, K. Tang, and X. Yao. Large scale evolutionary optimization using cooperative coevolution. Information Sciences, 178(15):2985\u20132999, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Tang%2C%20K.%20Yao%2C%20X.%20Large%20scale%20evolutionary%20optimization%20using%20cooperative%20coevolution%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Tang%2C%20K.%20Yao%2C%20X.%20Large%20scale%20evolutionary%20optimization%20using%20cooperative%20coevolution%202008"
        },
        {
            "id": "20",
            "entry": "[20] N. Garcia-Pedrajas, C. Hervas-Martinez, and J. Munoz-Perez. COVNET: a cooperative coevolutionary model for evolving artificial neural networks. IEEE Trans. on Neural Networks, 14(3):575\u2013595, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garcia-Pedrajas%2C%20N.%20Hervas-Martinez%2C%20C.%20Munoz-Perez%2C%20J.%20COVNET%3A%20a%20cooperative%20coevolutionary%20model%20for%20evolving%20artificial%20neural%20networks%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garcia-Pedrajas%2C%20N.%20Hervas-Martinez%2C%20C.%20Munoz-Perez%2C%20J.%20COVNET%3A%20a%20cooperative%20coevolutionary%20model%20for%20evolving%20artificial%20neural%20networks%202003"
        },
        {
            "id": "21",
            "entry": "[21] X. Yao. Evolving artificial neural networks. Proceedings of the IEEE, 87(9):1423\u20131447, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20X.%20Evolving%20artificial%20neural%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20X.%20Evolving%20artificial%20neural%20networks%201999"
        },
        {
            "id": "22",
            "entry": "[22] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune. Deep neuroevolution: genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06567"
        },
        {
            "id": "23",
            "entry": "[23] M. Suganuma, S. Shirakawa, and T. Nagao. A genetic programming approach to designing convolutional neural network architectures. In The Genetic and Evolutionary Computation Conference (GECCO), pages 497\u2013504, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suganuma%2C%20M.%20Shirakawa%2C%20S.%20Nagao%2C%20T.%20A%20genetic%20programming%20approach%20to%20designing%20convolutional%20neural%20network%20architectures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suganuma%2C%20M.%20Shirakawa%2C%20S.%20Nagao%2C%20T.%20A%20genetic%20programming%20approach%20to%20designing%20convolutional%20neural%20network%20architectures%202017"
        },
        {
            "id": "24",
            "entry": "[24] C. Liu, B. Zoph, J. Shlens, W. Hua, L.-J. Li, F.-F. Li, A. Yuille, J. Huang, and K. Murphy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00559"
        },
        {
            "id": "25",
            "entry": "[25] P. Chrabaszcz, I. Loshchilov, and F. Hutter. Back to basics: benchmarking canonical evolution strategies for playing Atari. arXiv preprint arXiv:1802.08842, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08842"
        },
        {
            "id": "26",
            "entry": "[26] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "27",
            "entry": "[27] X. Zhang, J. Clune, and K. O. Stanley. On the relationship between the OpenAI evolution strategy and stochastic gradient descent. arXiv preprint arXiv:1712.06564, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06564"
        },
        {
            "id": "28",
            "entry": "[28] J. Lehman, J. Chen, J. Clune, and K. O. Stanley. ES is more than just a traditional finitedifference approximator. arXiv preprint arXiv:1712.06568, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06568"
        },
        {
            "id": "29",
            "entry": "[29] F. Gomez, J. Schmidhuber, and R. Miikkulainen. Accelerated neural evolution through cooperatively coevolved synapses. Journal of Machine Learning Research, 9:937\u2013965, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomez%2C%20F.%20Schmidhuber%2C%20J.%20Miikkulainen%2C%20R.%20Accelerated%20neural%20evolution%20through%20cooperatively%20coevolved%20synapses%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomez%2C%20F.%20Schmidhuber%2C%20J.%20Miikkulainen%2C%20R.%20Accelerated%20neural%20evolution%20through%20cooperatively%20coevolved%20synapses%202008"
        },
        {
            "id": "30",
            "entry": "[30] N. Garcia-Pedrajas, C. Hervas-Martinez, and D. Ortiz-Boyer. Cooperative coevolution of artificial neural network ensembles for pattern recognition. IEEE Trans. on Evolutionary Computation, 9(3):271\u2013302, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garcia-Pedrajas%2C%20N.%20Hervas-Martinez%2C%20C.%20Ortiz-Boyer%2C%20D.%20Cooperative%20coevolution%20of%20artificial%20neural%20network%20ensembles%20for%20pattern%20recognition%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garcia-Pedrajas%2C%20N.%20Hervas-Martinez%2C%20C.%20Ortiz-Boyer%2C%20D.%20Cooperative%20coevolution%20of%20artificial%20neural%20network%20ensembles%20for%20pattern%20recognition%202005"
        },
        {
            "id": "31",
            "entry": "[31] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Maganize, pages 82\u201397, November 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%202012-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%202012-11"
        },
        {
            "id": "32",
            "entry": "[32] H. Hermansky. Perceptual linear predictive (PLP) analysis of speech. Journal of Acoustical Society America, 87(4):1738\u20131752, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hermansky%2C%20H.%20Perceptual%20linear%20predictive%20%28PLP%29%20analysis%20of%20speech%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hermansky%2C%20H.%20Perceptual%20linear%20predictive%20%28PLP%29%20analysis%20of%20speech%201990"
        },
        {
            "id": "33",
            "entry": "[33] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u2013 1780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "34",
            "entry": "[34] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet. Front-end factor analysis for speaker verification. IEEE Transactions on Audio, Speech, and Language Processing,, 19(4):788\u2013798, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dehak%2C%20N.%20Kenny%2C%20P.%20Dehak%2C%20R.%20Dumouchel%2C%20P.%20Front-end%20factor%20analysis%20for%20speaker%20verification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dehak%2C%20N.%20Kenny%2C%20P.%20Dehak%2C%20R.%20Dumouchel%2C%20P.%20Front-end%20factor%20analysis%20for%20speaker%20verification%202011"
        },
        {
            "id": "35",
            "entry": "[35] A. Krizhevsky. Learning multiple layers of features from tiny images. In Technical Report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images.%20In%20Technical%20Report%202009"
        },
        {
            "id": "36",
            "entry": "[36] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. Conference on Computer Vision and Pattern Recognition (CVPR\u201915), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202015"
        },
        {
            "id": "37",
            "entry": "[37] https://github.com/facebook/fb.resnet.torch.",
            "url": "https://github.com/facebook/fb.resnet.torch"
        },
        {
            "id": "38",
            "entry": "[38] T. Mikolov, M. Karafi\u00e1t, L. Burget, J. Cernocky, and S. Khudanpur. Recurrent neural network based language model. In Interspeech, pages 1045\u20131048, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernocky%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernocky%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%202010"
        },
        {
            "id": "39",
            "entry": "[39] S. Merity, N. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merity%2C%20S.%20Keskar%2C%20N.%20Socher%2C%20R.%20Regularizing%20and%20optimizing%20LSTM%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20S.%20Keskar%2C%20N.%20Socher%2C%20R.%20Regularizing%20and%20optimizing%20LSTM%20language%20models%202018"
        },
        {
            "id": "40",
            "entry": "[40] K. Zolna, D. Arpit, D. Suhubdy, and Y. Bengio. Fraternal dropout. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20Zolna%20D%20Arpit%20D%20Suhubdy%20and%20Y%20Bengio%20Fraternal%20dropout%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20Zolna%20D%20Arpit%20D%20Suhubdy%20and%20Y%20Bengio%20Fraternal%20dropout%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202018"
        },
        {
            "id": "41",
            "entry": "[41] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "42",
            "entry": "[42] L. Wan, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus. Regularization of neural networks using dropconnect. In International Conference on Machine Learning (ICML), volume 28, pages 1058\u20131066, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wan%2C%20L.%20Zeiler%2C%20M.%20Zhang%2C%20S.%20LeCun%2C%20Y.%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wan%2C%20L.%20Zeiler%2C%20M.%20Zhang%2C%20S.%20LeCun%2C%20Y.%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013"
        },
        {
            "id": "43",
            "entry": "[43] Y. Gal and Z. Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In International Conference on Neural Information Processing Systems (NIPS), NIPS\u201916, pages 1027\u20131035, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "44",
            "entry": "[44] X. Ma, Y. Gao, Z. Hu, Y. Yu, Y. Deng, and E. H. Hovy. Dropout with expectation-linear regularization. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20X.%20Gao%2C%20Y.%20Hu%2C%20Z.%20Yu%2C%20Y.%20Dropout%20with%20expectation-linear%20regularization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20X.%20Gao%2C%20Y.%20Hu%2C%20Z.%20Yu%2C%20Y.%20Dropout%20with%20expectation-linear%20regularization%202017"
        },
        {
            "id": "45",
            "entry": "[45] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "46",
            "entry": "[46] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20I.%20Hutter%2C%20F.%20SGDR%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20I.%20Hutter%2C%20F.%20SGDR%3A%20Stochastic%20gradient%20descent%20with%20warm%20restarts%202017"
        }
    ]
}
