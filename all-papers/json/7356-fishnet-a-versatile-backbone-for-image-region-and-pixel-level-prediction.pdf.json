{
    "filename": "7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf",
    "metadata": {
        "title": "FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction",
        "author": "Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, Wanli Ouyang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixellevel, are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom backbone structure designed under the consideration of unifying the advantages of networks designed for pixellevel or region-level predicting tasks, which may require very deep features with high resolution. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot directly propagate the gradient information from deep layers to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate the remarkable performance of the FishNet. In particular, on ImageNet-1k, the accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. FishNet was applied as one of the modules in the winning entry of the COCO Detection 2018 challenge. The code is available at https://github.com/kevin-ssy/FishNet."
    },
    "keywords": [
        {
            "term": "human pose estimation",
            "url": "https://en.wikipedia.org/wiki/human_pose_estimation"
        },
        {
            "term": "deep layer",
            "url": "https://en.wikipedia.org/wiki/deep_layer"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "conditional random field",
            "url": "https://en.wikipedia.org/wiki/conditional_random_field"
        },
        {
            "term": "high resolution",
            "url": "https://en.wikipedia.org/wiki/high_resolution"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "network structure",
            "url": "https://en.wikipedia.org/wiki/network_structure"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "low resolution",
            "url": "https://en.wikipedia.org/wiki/low_resolution"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Convolutional Neural Network (CNN) has been found to be effective for learning better feature representations in the field of computer vision [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "The Convolutional Neural Network designed for image-level, region-level, and pixel-level tasks begin to diverge in network structure",
        "Recent works on region-level tasks like object detection use networks with up-sampling mechanism [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] so that small objects can be described by the features with relatively high resolution",
        "Among all these networks designed for image classification, the features of high resolution are extracted by the shallow layers with small receptive field, which lack the high-level semantic meaning that can only be obtained on",
        "We evaluate our network on the ImageNet 2012 classification dataset [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] that consists of 1000 classes",
        "We propose a novel Convolutional Neural Network architecture to unify the advantages of architectures designed for the tasks recognizing objects on different levels"
    ],
    "key_statements": [
        "Convolutional Neural Network (CNN) has been found to be effective for learning better feature representations in the field of computer vision [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "The Convolutional Neural Network designed for image-level, region-level, and pixel-level tasks begin to diverge in network structure",
        "Recent works on region-level tasks like object detection use networks with up-sampling mechanism [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] so that small objects can be described by the features with relatively high resolution",
        "Driven by the success of using high-resolution features for region-level and pixel-level tasks, this paper proposes a fish-like network, namely FishNet, which enables the features of high resolution to contain high-level semantic information",
        "For region and pixel level tasks like object detection and instance segmentation, our model as a backbone for Mask R-Convolutional Neural Network [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] improves the absolute AP by 2.8% and 2.3% respectively on MS COCO compared to the baseline ResNet-50.\n1.1",
        "Among all these networks designed for image classification, the features of high resolution are extracted by the shallow layers with small receptive field, which lack the high-level semantic meaning that can only be obtained on",
        "We evaluate our network on the ImageNet 2012 classification dataset [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] that consists of 1000 classes",
        "We propose a novel Convolutional Neural Network architecture to unify the advantages of architectures designed for the tasks recognizing objects on different levels"
    ],
    "summary": [
        "Convolutional Neural Network (CNN) has been found to be effective for learning better feature representations in the field of computer vision [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>].",
        "For region and pixel level tasks like object detection and instance segmentation, our model as a backbone for Mask R-CNN [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] improves the absolute AP by 2.8% and 2.3% respectively on MS COCO compared to the baseline ResNet-50.",
        "Among all these networks designed for image classification, the features of high resolution are extracted by the shallow layers with small receptive field, which lack the high-level semantic meaning that can only be obtained on",
        "Addition and residual blocks in existing works do not preserve features from both shallow and deep layers, while our design preserves and refines them.",
        "Our work preserves and refines features from both shallow and deep layers for the final task, which is not achieved in existing networks with up-sampling or MSDNet. Message passing among features/outputs.",
        "Where the T denotes residual block transferring the feature xts\u22121 from tail to the body, the up represents the feature refined from the previous stage in the fish body.",
        "The features from every stage of the whole network is able to be directly connected to the final layer through concatenation, skip connection, and max-pooling.",
        "The gradient propagation problem from the previous backbone network in the tail are solved with the FishNet by 1) excluding I-conv at the head; and 2) using concatenation at the body and the head.",
        "Since the up-sampling operation will dilute input features with lower resolution, we apply dilated convolution in the refining blocks.",
        "In FishNet, the UR-block will dilute the original low-resolution features, we adopt dilated convolution in the fish body.",
        "1When convolution with a stride of 2 is used, it is used for both the tail and the head of the FishNet. When pooling is used, we still put a 1 \u00d7 1 convolution on the skip connection of the last residual blocks for each stage at the tail to change the number of channels between two stages, but we do not use such convolution at the head.",
        "When compared with ResNeXt-50, FishNet-150 only reduces absolute error rate by 0.2% for image classification, while it improves the absolute AP by 1.3% and 1.5% respectively for object detection and instance segmentation.",
        "The design of feature preservation and refinement not only helps to handle the problem of direct gradient propagation, but is friendly to pixel-level and region-level tasks.",
        "The performance for larger models on both datasets will be reported"
    ],
    "headline": "Driven by the success of using high-resolution features for region-level and pixel-level tasks, this paper proposes a fish-like network, namely FishNet, which enables the features of high resolution to contain high-level semantic information",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, C. C. Loy, and D. Lin. mmdetection. https://github.com/open-mmlab/mmdetection, 2018.",
            "url": "https://github.com/open-mmlab/mmdetection"
        },
        {
            "id": "2",
            "entry": "[2] Y. Chen, J. Li, H. Xiao, X. Jin, S. Yan, and J. Feng. Dual path networks. In Advances in Neural Information Processing Systems, pages 4470\u20134478, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Li%2C%20J.%20Xiao%2C%20H.%20Jin%2C%20X.%20Dual%20path%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Y.%20Li%2C%20J.%20Xiao%2C%20H.%20Jin%2C%20X.%20Dual%20path%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] X. Chu, W. Ouyang, X. Wang, et al. Crf-cnn: Modeling structured information in human pose estimation. In Advances in Neural Information Processing Systems, pages 316\u2013324, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chu%2C%20X.%20Ouyang%2C%20W.%20Wang%2C%20X.%20Crf-cnn%3A%20Modeling%20structured%20information%20in%20human%20pose%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chu%2C%20X.%20Ouyang%2C%20W.%20Wang%2C%20X.%20Crf-cnn%3A%20Modeling%20structured%20information%20in%20human%20pose%20estimation%202016"
        },
        {
            "id": "4",
            "entry": "[4] P. Gao, H. Li, S. Li, P. Lu, Y. Li, S. C. Hoi, and X. Wang. Question-guided hybrid convolution for visual question answering. arXiv preprint arXiv:1808.02632, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.02632"
        },
        {
            "id": "5",
            "entry": "[5] R. Girshick, I. Radosavovic, G. Gkioxari, P. Doll\u00e1r, and K. He. Detectron. https://github.com/facebookresearch/detectron, 2018.",
            "url": "https://github.com/facebookresearch/detectron"
        },
        {
            "id": "6",
            "entry": "[6] P. Goyal, P. Doll\u00e1r, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "7",
            "entry": "[7] B. Hariharan, P. Arbel\u00e1ez, R. Girshick, and J. Malik. Hypercolumns for object segmentation and finegrained localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 447\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hariharan%2C%20B.%20Arbel%C3%A1ez%2C%20P.%20Girshick%2C%20R.%20Malik%2C%20J.%20Hypercolumns%20for%20object%20segmentation%20and%20finegrained%20localization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hariharan%2C%20B.%20Arbel%C3%A1ez%2C%20P.%20Girshick%2C%20R.%20Malik%2C%20J.%20Hypercolumns%20for%20object%20segmentation%20and%20finegrained%20localization%202015"
        },
        {
            "id": "8",
            "entry": "[8] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pages 630\u2013645.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20mappings%20in%20deep%20residual%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20mappings%20in%20deep%20residual%20networks"
        },
        {
            "id": "9",
            "entry": "[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "10",
            "entry": "[10] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE International Conference on, pages 2980\u20132988. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20He%20G%20Gkioxari%20P%20Doll%C3%A1r%20and%20R%20Girshick%20Mask%20rcnn%20In%20Computer%20Vision%20ICCV%202017%20IEEE%20International%20Conference%20on%20pages%2029802988%20IEEE%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20He%20G%20Gkioxari%20P%20Doll%C3%A1r%20and%20R%20Girshick%20Mask%20rcnn%20In%20Computer%20Vision%20ICCV%202017%20IEEE%20International%20Conference%20on%20pages%2029802988%20IEEE%202017"
        },
        {
            "id": "11",
            "entry": "[11] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01507"
        },
        {
            "id": "12",
            "entry": "[12] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and K. Q. Weinberger. Multi-scale dense convolutional networks for efficient prediction. arXiv preprint arXiv:1703.09844, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09844"
        },
        {
            "id": "13",
            "entry": "[13] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20Weinberger%2C%20K.Q.%20van%20der%20Maaten%2C%20L.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20Weinberger%2C%20K.Q.%20van%20der%20Maaten%2C%20L.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "14",
            "entry": "[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "15",
            "entry": "[15] J.-H. Jacobsen, A. Smeulders, and E. Oyallon. i-revnet: Deep invertible networks. arXiv preprint arXiv:1802.07088, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07088"
        },
        {
            "id": "16",
            "entry": "[16] E. Kim, C. Ahn, and S. Oh. Nestednet: Learning nested sparse structures in deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8669\u20138678, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20E.%20Ahn%2C%20C.%20Oh%2C%20S.%20Nestednet%3A%20Learning%20nested%20sparse%20structures%20in%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20E.%20Ahn%2C%20C.%20Oh%2C%20S.%20Nestednet%3A%20Learning%20nested%20sparse%20structures%20in%20deep%20neural%20networks%202018"
        },
        {
            "id": "17",
            "entry": "[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "18",
            "entry": "[18] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07648"
        },
        {
            "id": "19",
            "entry": "[19] H. Li, Y. Liu, W. Ouyang, and X. Wang. Zoom out-and-in network with map attention decision for region proposal and object detection. International Journal of Computer Vision, Jun 2018. ISSN 1573-1405. doi: 10.1007/s11263-018-1101-7. URL https://doi.org/10.1007/s11263-018-1101-7.",
            "crossref": "https://dx.doi.org/10.1007/s11263-018-1101-7",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-018-1101-7"
        },
        {
            "id": "20",
            "entry": "[20] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Doll%C3%A1r%20and%20C%20L%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20European%20conference%20on%20computer%20vision%20pages%20740755",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Doll%C3%A1r%20and%20C%20L%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20European%20conference%20on%20computer%20vision%20pages%20740755"
        },
        {
            "id": "21",
            "entry": "[21] T.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feature%20pyramid%20networks%20for%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feature%20pyramid%20networks%20for%20object%20detection%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In European Conference on Computer Vision, pages 483\u2013499.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newell%2C%20A.%20Yang%2C%20K.%20Deng%2C%20J.%20Stacked%20hourglass%20networks%20for%20human%20pose%20estimation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newell%2C%20A.%20Yang%2C%20K.%20Deng%2C%20J.%20Stacked%20hourglass%20networks%20for%20human%20pose%20estimation"
        },
        {
            "id": "23",
            "entry": "[23] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20A.%20Gross%2C%20S.%20Chintala%2C%20S.%20Chanan%2C%20G.%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "24",
            "entry": "[24] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013 241.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ronneberger%2C%20O.%20Fischer%2C%20P.%20Brox%2C%20T.%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ronneberger%2C%20O.%20Fischer%2C%20P.%20Brox%2C%20T.%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation"
        },
        {
            "id": "25",
            "entry": "[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "26",
            "entry": "[26] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "27",
            "entry": "[27] S. Sun, Z. Kuang, L. Sheng, W. Ouyang, and W. Zhang. Optical flow guided feature: A fast and robust motion representation for video action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1390\u20131399, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20S.%20Kuang%2C%20Z.%20Sheng%2C%20L.%20Ouyang%2C%20W.%20Optical%20flow%20guided%20feature%3A%20A%20fast%20and%20robust%20motion%20representation%20for%20video%20action%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20S.%20Kuang%2C%20Z.%20Sheng%2C%20L.%20Ouyang%2C%20W.%20Optical%20flow%20guided%20feature%3A%20A%20fast%20and%20robust%20motion%20representation%20for%20video%20action%20recognition%202018"
        },
        {
            "id": "28",
            "entry": "[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, et al. Going deeper with convolutions. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "29",
            "entry": "[29] S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 5987\u20135995. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "30",
            "entry": "[30] W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang. Learning feature pyramids for human pose estimation. In arXiv preprint arXiv:1708.01101, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.01101"
        },
        {
            "id": "31",
            "entry": "[31] Y. Yang, Z. Zhong, T. Shen, and Z. Lin. Convolutional neural networks with alternately updated clique. arXiv preprint arXiv:1802.10419, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10419"
        },
        {
            "id": "32",
            "entry": "[32] F. Yu, V. Koltun, and T. Funkhouser. Dilated residual networks. In Computer Vision and Pattern Recognition, volume 1, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20F.%20Koltun%2C%20V.%20Funkhouser%2C%20T.%20Dilated%20residual%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20F.%20Koltun%2C%20V.%20Funkhouser%2C%20T.%20Dilated%20residual%20networks%202017"
        },
        {
            "id": "33",
            "entry": "[33] F. Yu, D. Wang, and T. Darrell. Deep layer aggregation. arXiv preprint arXiv:1707.06484, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06484"
        },
        {
            "id": "34",
            "entry": "[34] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "35",
            "entry": "[35] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang. Gated bi-directional cnn for object detection. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeng%2C%20X.%20Ouyang%2C%20W.%20Yang%2C%20B.%20Yan%2C%20J.%20Gated%20bi-directional%20cnn%20for%20object%20detection",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeng%2C%20X.%20Ouyang%2C%20W.%20Yang%2C%20B.%20Yan%2C%20J.%20Gated%20bi-directional%20cnn%20for%20object%20detection"
        },
        {
            "id": "36",
            "entry": "[36] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 1529\u20131537, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20S.%20Jayasumana%2C%20S.%20Romera-Paredes%2C%20B.%20Vineet%2C%20V.%20Conditional%20random%20fields%20as%20recurrent%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20S.%20Jayasumana%2C%20S.%20Romera-Paredes%2C%20B.%20Vineet%2C%20V.%20Conditional%20random%20fields%20as%20recurrent%20neural%20networks%202015"
        },
        {
            "id": "37",
            "entry": "[37] H. Zhou, W. Ouyang, J. Cheng, X. Wang, and H. Li. Deep continuous conditional random fields with asymmetric inter-object constraints for online multi-object tracking. IEEE Transactions on Circuits and Systems for Video Technology, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20H.%20Ouyang%2C%20W.%20Cheng%2C%20J.%20Wang%2C%20X.%20Deep%20continuous%20conditional%20random%20fields%20with%20asymmetric%20inter-object%20constraints%20for%20online%20multi-object%20tracking%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20H.%20Ouyang%2C%20W.%20Cheng%2C%20J.%20Wang%2C%20X.%20Deep%20continuous%20conditional%20random%20fields%20with%20asymmetric%20inter-object%20constraints%20for%20online%20multi-object%20tracking%202018"
        }
    ]
}
