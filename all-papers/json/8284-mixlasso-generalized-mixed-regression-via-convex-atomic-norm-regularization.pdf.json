{
    "filename": "8284-mixlasso-generalized-mixed-regression-via-convex-atomic-norm-regularization.pdf",
    "metadata": {
        "title": "MixLasso: Generalized Mixed Regression via Convex Atomic-Norm Regularization",
        "author": "Ian En-Hsu Yen, Wei-Cheng Lee, Kai Zhong, Sung-En Chang, Pradeep K. Ravikumar, Shou-De Lin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8284-mixlasso-generalized-mixed-regression-via-convex-atomic-norm-regularization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider a generalization of mixed regression where the response is an additive combination of several mixture components. Standard mixed regression is a special case where each response is generated from exactly one component. Typical approaches to the mixture regression problem employ local search methods such as Expectation Maximization (EM) that are prone to spurious local optima. On the other hand, a number of recent theoretically-motivated Tensor-based methods either have high sample complexity, or require the knowledge of the input distribution, which is not available in most of practical situations. In this work, we study a novel convex estimator MixLasso for the estimation of generalized mixed regression, based on an atomic norm specifically constructed to regularize the number of mixture components. Our algorithm gives a risk bound that trades off between prediction accuracy and model sparsity without imposing stringent assumptions on the input/output distribution, and can be easily adapted to the case of non-linear functions. In our numerical experiments on mixtures of linear as well as nonlinear regressions, the proposed method yields high-quality solutions in a wider range of settings than existing approaches."
    },
    "keywords": [
        {
            "term": "Expectation Maximization",
            "url": "https://en.wikipedia.org/wiki/Expectation_Maximization"
        },
        {
            "term": "special case",
            "url": "https://en.wikipedia.org/wiki/special_case"
        },
        {
            "term": "linear function",
            "url": "https://en.wikipedia.org/wiki/linear_function"
        }
    ],
    "highlights": [
        "The Mixed Regression (MR) problem considers the estimation of K functions from a collection of input-output samples, where for each sample, the output is generated by one of the K regression functions",
        "The Mixed Regression formulation can be employed as an approach to decompose a complicated function into K simpler ones, by splitting the observations into K classes",
        "Variants of regression families such as piecewise-linear regression can be viewed as special cases of Mixed Regression",
        "We address a generalized version of Mixed Regression where the output can be an additive combination of several mixture components",
        "We propose a novel convex estimator MixLasso for the mixed regression problem, which enforces the mixture structure through minimizing a carefully constructed atomic norm that acts as a surrogate function for the number of mixture components",
        "While we have proposed a solution of the generalized version (1), in some applications, it might be of interest to solve the special case of standard mixed regression, where each observation belongs to exactly one mixture component"
    ],
    "key_statements": [
        "The Mixed Regression (MR) problem considers the estimation of K functions from a collection of input-output samples, where for each sample, the output is generated by one of the K regression functions",
        "The Mixed Regression formulation can be employed as an approach to decompose a complicated function into K simpler ones, by splitting the observations into K classes",
        "Variants of regression families such as piecewise-linear regression can be viewed as special cases of Mixed Regression",
        "We address a generalized version of Mixed Regression where the output can be an additive combination of several mixture components",
        "We propose a novel convex estimator MixLasso for the mixed regression problem, which enforces the mixture structure through minimizing a carefully constructed atomic norm that acts as a surrogate function for the number of mixture components",
        "We propose a greedy algorithm that generates a steepest-descent component at each iteration through solving a sub-problem similar to MAX-CUT",
        "We show how our proposed method can be extended to the nonlinear regression setting, to regression functions lying in a Reproducing Kernel Hilbert Space (RKHS)",
        "While we have proposed a solution of the generalized version (1), in some applications, it might be of interest to solve the special case of standard mixed regression, where each observation belongs to exactly one mixture component",
        "Generalization Analysis. In this Given section, we investigate the performance of output from Algorithm 1 in terms a coefficients c with support A, we can construct the weight matrix by W (c)",
        "In the case of Normal input distribution (Syn1, Syn2, Syn7, Syn8), both the Tensorinitialized methods and MixLasso consistently improve upon random-initialized Expectation Maximization/ALT in terms of the number of samples required to achieve a good performance, where"
    ],
    "summary": [
        "The Mixed Regression (MR) problem considers the estimation of K functions from a collection of input-output samples, where for each sample, the output is generated by one of the K regression functions.",
        "For the special case of linear function with K=2 components, [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] propose a convex nuclear norm minimization formulation that is guaranteed to estimate the two functions with minimax-optimal rates when given a sub-Gaussian design matrix.",
        "We propose a novel convex estimator MixLasso for the mixed regression problem, which enforces the mixture structure through minimizing a carefully constructed atomic norm that acts as a surrogate function for the number of mixture components.",
        "We propose a greedy algorithm that generates a steepest-descent component at each iteration through solving a sub-problem similar to MAX-CUT.",
        "While we have proposed a solution of the generalized version (1), in some applications, it might be of interest to solve the special case of standard mixed regression, where each observation belongs to exactly one mixture component.",
        "Let A, c, Wbe the set of active components, coefficients and corresponding weight matrix obtained from T iterations of Algorithm 1, and Wbe the minimizer of the population risk",
        "Note the result of Theorem (2) is obtained without distributional assumption on the input/output, so it is in general not possible to guarantee convergence to an optimal risk with exactly K components, since finding such optimal solution is NP-hard even measured by the empirical risk [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "It remains open if one can give a tighter result for the estimator (6) that achieves -suboptimal risk with number of components being a constant multiple of K, or derive a bound on the parameter estimation error, possibly with additional assumptions on the observations.",
        "In the case of Normal input distribution (Syn1, Syn2, Syn7, Syn8), both the Tensorinitialized methods and MixLasso consistently improve upon random-initialized EM/ALT in terms of the number of samples required to achieve a good performance, where",
        "The rightmost columns of Figure 1, 2 show the results on data generated from the generalized mixed regression model (Syn3, Syn6, Syn9, Syn12), where Tensor-based methods",
        "Figure 3 gives a comparison of EM-Random and MixLasso on Mixture of Kernel Regression with polynomial kernel K =d (d = 6), where we generate K=4 random 6th-degree polynomial functions {fk\u2217}kK=1 by uniform sampling their coefficients from U (\u22124, 4).",
        "We compare MixLasso and EM for fitting a mixture of polynomial regressions on a Stock data set that contains the mixed stock prices of IBM, Facebook, Microsoft and Nvidia of span 300 weeks till the Feb. of 2018."
    ],
    "headline": "We study a novel convex estimator MixLasso for the estimation of generalized mixed regression, based on an atomic norm specifically constructed to regularize the number of mixture components",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear regression. In ICML, pages 613\u2013621, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20Xinyang%20Caramanis%2C%20Constantine%20Sanghavi%2C%20Sujay%20Alternating%20minimization%20for%20mixed%20linear%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yi%2C%20Xinyang%20Caramanis%2C%20Constantine%20Sanghavi%2C%20Sujay%20Alternating%20minimization%20for%20mixed%20linear%20regression%202014"
        },
        {
            "id": "2",
            "entry": "[2] Lei Xu, Michael I Jordan, and Geoffrey E Hinton. An alternative model for mixtures of experts. In Advances in neural information processing systems, pages 633\u2013640, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Lei%20Jordan%2C%20Michael%20I.%20Hinton%2C%20Geoffrey%20E.%20An%20alternative%20model%20for%20mixtures%20of%20experts%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Lei%20Jordan%2C%20Michael%20I.%20Hinton%2C%20Geoffrey%20E.%20An%20alternative%20model%20for%20mixtures%20of%20experts%201995"
        },
        {
            "id": "3",
            "entry": "[3] Christopher M Bishop and Markus Svenskn. Bayesian hierarchical mixtures of experts. In Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence, pages 57\u201364. Morgan Kaufmann Publishers Inc., 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20Christopher%20M.%20Svenskn%2C%20Markus%20Bayesian%20hierarchical%20mixtures%20of%20experts%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bishop%2C%20Christopher%20M.%20Svenskn%2C%20Markus%20Bayesian%20hierarchical%20mixtures%20of%20experts%202002"
        },
        {
            "id": "4",
            "entry": "[4] Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In COLT, pages 560\u2013604, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Yudong%20Yi%2C%20Xinyang%20Caramanis%2C%20Constantine%20A%20convex%20formulation%20for%20mixed%20regression%20with%20two%20components%3A%20Minimax%20optimal%20rates%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Yudong%20Yi%2C%20Xinyang%20Caramanis%2C%20Constantine%20A%20convex%20formulation%20for%20mixed%20regression%20with%20two%20components%3A%20Minimax%20optimal%20rates%202014"
        },
        {
            "id": "5",
            "entry": "[5] Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In Artificial Intelligence and Statistics, pages 1223\u20131231, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sedghi%2C%20Hanie%20Janzamin%2C%20Majid%20Anandkumar%2C%20Anima%20Provable%20tensor%20methods%20for%20learning%20mixtures%20of%20generalized%20linear%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sedghi%2C%20Hanie%20Janzamin%2C%20Majid%20Anandkumar%2C%20Anima%20Provable%20tensor%20methods%20for%20learning%20mixtures%20of%20generalized%20linear%20models%202016"
        },
        {
            "id": "6",
            "entry": "[6] Arun T Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1040\u20131048, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaganty%2C%20Arun%20T.%20Liang%2C%20Percy%20Spectral%20experts%20for%20estimating%20mixtures%20of%20linear%20regressions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaganty%2C%20Arun%20T.%20Liang%2C%20Percy%20Spectral%20experts%20for%20estimating%20mixtures%20of%20linear%20regressions%202013"
        },
        {
            "id": "7",
            "entry": "[7] Kai Zhong, Prateek Jain, and Inderjit S Dhillon. Mixed linear regression with multiple components. In Advances in Neural Information Processing Systems, pages 2190\u20132198, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhong%2C%20Kai%20Jain%2C%20Prateek%20Dhillon%2C%20Inderjit%20S.%20Mixed%20linear%20regression%20with%20multiple%20components%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhong%2C%20Kai%20Jain%2C%20Prateek%20Dhillon%2C%20Inderjit%20S.%20Mixed%20linear%20regression%20with%20multiple%20components%202016"
        },
        {
            "id": "8",
            "entry": "[8] Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Solving a mixture of many random linear equations by tensor decomposition and alternating minimization. arXiv preprint arXiv:1608.05749, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.05749"
        },
        {
            "id": "9",
            "entry": "[9] Paul Hand and Babhru Joshi. A convex program for mixed linear regression with a recovery guarantee for well-separated data. arXiv preprint arXiv:1612.06067, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.06067"
        },
        {
            "id": "10",
            "entry": "[10] Ian En-Hsu Yen, Xin Lin, Kai Zhong, Pradeep Ravikumar, and Inderjit Dhillon. A convex exemplar-based approach to mad-bayes dirichlet process mixture models. In International Conference on Machine Learning, pages 2418\u20132426, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yen%2C%20Ian%20En-Hsu%20Lin%2C%20Xin%20Zhong%2C%20Kai%20Ravikumar%2C%20Pradeep%20A%20convex%20exemplar-based%20approach%20to%20mad-bayes%20dirichlet%20process%20mixture%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yen%2C%20Ian%20En-Hsu%20Lin%2C%20Xin%20Zhong%2C%20Kai%20Ravikumar%2C%20Pradeep%20A%20convex%20exemplar-based%20approach%20to%20mad-bayes%20dirichlet%20process%20mixture%20models%202015"
        },
        {
            "id": "11",
            "entry": "[11] Ian En-Hsu Yen, Xin Lin, Jiong Zhang, Pradeep Ravikumar, and Inderjit Dhillon. A convex atomic-norm approach to multiple sequence alignment and motif discovery. In International Conference on Machine Learning, pages 2272\u20132280, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yen%2C%20Ian%20En-Hsu%20Lin%2C%20Xin%20Zhang%2C%20Jiong%20Ravikumar%2C%20Pradeep%20A%20convex%20atomic-norm%20approach%20to%20multiple%20sequence%20alignment%20and%20motif%20discovery%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yen%2C%20Ian%20En-Hsu%20Lin%2C%20Xin%20Zhang%2C%20Jiong%20Ravikumar%2C%20Pradeep%20A%20convex%20atomic-norm%20approach%20to%20multiple%20sequence%20alignment%20and%20motif%20discovery%202016"
        },
        {
            "id": "12",
            "entry": "[12] Ian En-Hsu Yen, Wei-Chen Chang, Sung-En Chang, Arun Sai Suggula, Shou-De Lin, and Pradeep. Ravikumar. Latent feature lasso. International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20EnHsu%20Yen%20WeiChen%20Chang%20SungEn%20Chang%20Arun%20Sai%20Suggula%20ShouDe%20Lin%20and%20Pradeep%20Ravikumar%20Latent%20feature%20lasso%20International%20Conference%20on%20Machine%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20EnHsu%20Yen%20WeiChen%20Chang%20SungEn%20Chang%20Arun%20Sai%20Suggula%20ShouDe%20Lin%20and%20Pradeep%20Ravikumar%20Latent%20feature%20lasso%20International%20Conference%20on%20Machine%20Learning%202017"
        },
        {
            "id": "13",
            "entry": "[13] Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear inverse problems. Foundations of Computational mathematics, 12(6):805\u2013 849, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chandrasekaran%2C%20Venkat%20Recht%2C%20Benjamin%20Parrilo%2C%20Pablo%20A.%20Willsky%2C%20Alan%20S.%20The%20convex%20geometry%20of%20linear%20inverse%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chandrasekaran%2C%20Venkat%20Recht%2C%20Benjamin%20Parrilo%2C%20Pablo%20A.%20Willsky%2C%20Alan%20S.%20The%20convex%20geometry%20of%20linear%20inverse%20problems%202012"
        },
        {
            "id": "14",
            "entry": "[14] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "15",
            "entry": "[15] Yurii Nesterov et al. Quality of semidefinite relaxation for nonconvex quadratic optimization. Universit\u00e9 Catholique de Louvain. Center for Operations Research and Econometrics [CORE], 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Quality%20of%20semidefinite%20relaxation%20for%20nonconvex%20quadratic%20optimization.%20Universit%C3%A9%20Catholique%20de%20Louvain%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Quality%20of%20semidefinite%20relaxation%20for%20nonconvex%20quadratic%20optimization.%20Universit%C3%A9%20Catholique%20de%20Louvain%201997"
        },
        {
            "id": "16",
            "entry": "[16] Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex burer-monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757\u20132765, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boumal%2C%20Nicolas%20Voroninski%2C%20Vlad%20Bandeira%2C%20Afonso%20The%20non-convex%20burer-monteiro%20approach%20works%20on%20smooth%20semidefinite%20programs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boumal%2C%20Nicolas%20Voroninski%2C%20Vlad%20Bandeira%2C%20Afonso%20The%20non-convex%20burer-monteiro%20approach%20works%20on%20smooth%20semidefinite%20programs%202016"
        },
        {
            "id": "17",
            "entry": "[17] Po-Wei Wang, Wei-Cheng Chang, and J Zico Kolter. The mixing method: coordinate descent for low-rank semidefinite programming. arXiv preprint arXiv:1706.00476, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1706.00476"
        }
    ]
}
