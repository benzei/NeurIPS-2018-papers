{
    "filename": "8094-generalized-cross-entropy-loss-for-training-deep-neural-networks-with-noisy-labels.pdf",
    "metadata": {
        "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels",
        "author": "Zhilu Zhang, Mert Sabuncu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8094-generalized-cross-entropy-loss-for-training-deep-neural-networks-with-noisy-labels.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs\u2019 rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHIONMNIST datasets and synthetically generated noisy labels."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "mean absolute error",
            "url": "https://en.wikipedia.org/wiki/mean_absolute_error"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "cross entropy",
            "url": "https://en.wikipedia.org/wiki/cross_entropy"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "The resurrection of neural networks in recent years, together with the recent emergence of large scale datasets, has enabled super-human performance on many classification tasks [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "Supervised Deep neural networks often require a large number of training samples to achieve a high level of performance",
        "An algorithm that is robust against noisy labels for Deep neural networks is needed to resolve the potential problem",
        "To exploit the benefits of both the noise-robustness provided by mean absolute error and the implicit weighting scheme of categorical cross entropy, we propose using the the negative Box-Cox transformation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] as a loss function: Lq(f (x), ej) = (1 fj(x)q) , q",
        "Note that the poor performance of Lq loss compared to mean absolute error is due to the fact that test accuracy reported here is long after the model started overfitting, since a shallow CNN without data augmentation was deployed for this experiment",
        "We proposed theoretically grounded and easy-to-use classes of noise-robust loss functions, the Lq loss and the truncated Lq loss, for classification with noisy labels that can be employed with any existing Deep neural networks algorithm"
    ],
    "key_statements": [
        "The resurrection of neural networks in recent years, together with the recent emergence of large scale datasets, has enabled super-human performance on many classification tasks [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "Supervised Deep neural networks often require a large number of training samples to achieve a high level of performance",
        "An algorithm that is robust against noisy labels for Deep neural networks is needed to resolve the potential problem",
        "We propose a novel generalization of categorical cross entropy and present a theoretical analysis of proposed loss functions in the context of noisy labels",
        "Ghosh et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] proved and empirically demonstrated that mean absolute error is robust against noisy labels",
        "To exploit the benefits of both the noise-robustness provided by mean absolute error and the implicit weighting scheme of categorical cross entropy, we propose using the the negative Box-Cox transformation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] as a loss function: Lq(f (x), ej) = (1 fj(x)q) , q",
        "Relative to categorical cross entropy, Lq loss weighs each sample by an additional fyiq so that less emphasis is put on samples with weak agreement between softmax outputs and the labels, which should improve robustness against noise",
        "One interpretation of this behavior is that the classifier could learn more about predictive features before overfitting. This interpretation is supported by our plot of the average softmax values with respect to the correctly and wrongly labeled samples on the training set for categorical cross entropy and Lq (q = 0.7) loss, and with 40% uniform noise (Fig. 1(c))",
        "We observed that mean absolute error performed poorly for all datasets at all noise levels, presumably because Deep neural networks like ResNet struggled to optimize with mean absolute error loss, especially on challenging datasets such as CIFAR-100",
        "Note that the poor performance of Lq loss compared to mean absolute error is due to the fact that test accuracy reported here is long after the model started overfitting, since a shallow CNN without data augmentation was deployed for this experiment",
        "We proposed theoretically grounded and easy-to-use classes of noise-robust loss functions, the Lq loss and the truncated Lq loss, for classification with noisy labels that can be employed with any existing Deep neural networks algorithm"
    ],
    "summary": [
        "The resurrection of neural networks in recent years, together with the recent emergence of large scale datasets, has enabled super-human performance on many classification tasks [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "We report a thorough empirical evaluation of the proposed loss functions using CIFAR-10, CIFAR-100 and FASHION-MNIST datasets, and demonstrate significant improvement in terms of classification accuracy over the baselines of MAE and CCE, under both closed-set and open-set noisy labels.",
        "Little attention is given to alternative noise robust loss functions for DNNs. Ghosh et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] proved and empirically demonstrated that MAE is robust against noisy labels.",
        "Relative to CCE, Lq loss weighs each sample by an additional fyiq so that less emphasis is put on samples with weak agreement between softmax outputs and the labels, which should improve robustness against noise.",
        "When the threshold is relatively large (e.g., k = 0.5 in a 10-class classification problem), at the beginning of the training phase, most of the softmax outputs can be significantly smaller than k, resulting in a dramatic drop in the number of effective samples.",
        "To realistically mimic a noisy dataset while justifiably analyzing the performance of the proposed loss function, only the training and validation data were contaminated, and test accuracies were computed with respect to true labels.",
        "This interpretation is supported by our plot of the average softmax values with respect to the correctly and wrongly labeled samples on the training set for CCE and Lq (q = 0.7) loss, and with 40% uniform noise (Fig. 1(c)).",
        "Truncated Lq loss produced similar accuracies as Forward Tfor FASHION-MNIST and better results for CIFAR datasets, and outperformed the other baselines at most noise levels for all datasets.",
        "Classification accuracy for both uniform and class dependent noise would be further improved relative to baselines with optimized q and k values and more number of epochs.",
        "We observed that MAE performed poorly for all datasets at all noise levels, presumably because DNNs like ResNet struggled to optimize with MAE loss, especially on challenging datasets such as CIFAR-100.",
        "Note that the poor performance of Lq loss compared to MAE is due to the fact that test accuracy reported here is long after the model started overfitting, since a shallow CNN without data augmentation was deployed for this experiment.",
        "We proposed theoretically grounded and easy-to-use classes of noise-robust loss functions, the Lq loss and the truncated Lq loss, for classification with noisy labels that can be employed with any existing DNN algorithm.",
        "We empirically verified noise robustness on various datasets with both closedand open-set noise scenarios"
    ],
    "headline": "As we show in this paper, mean absolute error can perform poorly with Deep neural networks and challenging datasets",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Devansh Arpit, Stanis\u0142aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05394"
        },
        {
            "id": "2",
            "entry": "[2] Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, and Trevor Darrell. Auxiliary image regularization for deep cnns with noisy labels. arXiv preprint arXiv:1511.07069, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07069"
        },
        {
            "id": "3",
            "entry": "[3] Mokhtar S Bazaraa, Hanif D Sherali, and Chitharanjan M Shetty. Nonlinear programming: theory and algorithms. John Wiley & Sons, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bazaraa%2C%20Mokhtar%20S.%20Sherali%2C%20Hanif%20D.%20Shetty%2C%20Chitharanjan%20M.%20Nonlinear%20programming%3A%20theory%20and%20algorithms%202013"
        },
        {
            "id": "4",
            "entry": "[4] George EP Box and David R Cox. An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), pages 211\u2013252, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Box%2C%20George%20E.P.%20Cox%2C%20David%20R.%20An%20analysis%20of%20transformations%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Box%2C%20George%20E.P.%20Cox%2C%20David%20R.%20An%20analysis%20of%20transformations%201964"
        },
        {
            "id": "5",
            "entry": "[5] J Paul Brooks. Support vector machines with the ramp loss and the hard margin loss. Operations research, 59(2):467\u2013479, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brooks%2C%20J.Paul%20Support%20vector%20machines%20with%20the%20ramp%20loss%20and%20the%20hard%20margin%20loss%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brooks%2C%20J.Paul%20Support%20vector%20machines%20with%20the%20ramp%20loss%20and%20the%20hard%20margin%20loss%202011"
        },
        {
            "id": "6",
            "entry": "[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "7",
            "entry": "[7] Davide Ferrari, Yuhong Yang, et al. Maximum lq-likelihood estimation. The Annals of Statistics, 38(2):753\u2013783, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ferrari%2C%20Davide%20Yang%2C%20Yuhong%20Maximum%20lq-likelihood%20estimation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ferrari%2C%20Davide%20Yang%2C%20Yuhong%20Maximum%20lq-likelihood%20estimation%202010"
        },
        {
            "id": "8",
            "entry": "[8] Beno\u00eet Fr\u00e9nay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE transactions on neural networks and learning systems, 25(5):845\u2013869, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fr%C3%A9nay%2C%20Beno%C3%AEt%20Verleysen%2C%20Michel%20Classification%20in%20the%20presence%20of%20label%20noise%3A%20a%20survey%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fr%C3%A9nay%2C%20Beno%C3%AEt%20Verleysen%2C%20Michel%20Classification%20in%20the%20presence%20of%20label%20noise%3A%20a%20survey%202014"
        },
        {
            "id": "9",
            "entry": "[9] Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep neural networks. In AAAI, pages 1919\u20131925, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20Aritra%20Kumar%2C%20Himanshu%20Sastry%2C%20P.S.%20Robust%20loss%20functions%20under%20label%20noise%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20Aritra%20Kumar%2C%20Himanshu%20Sastry%2C%20P.S.%20Robust%20loss%20functions%20under%20label%20noise%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "10",
            "entry": "[10] Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label noise. Neurocomputing, 160:93\u2013107, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20Aritra%20Manwani%2C%20Naresh%20Sastry%2C%20P.S.%20Making%20risk%20minimization%20tolerant%20to%20label%20noise%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20Aritra%20Manwani%2C%20Naresh%20Sastry%2C%20P.S.%20Making%20risk%20minimization%20tolerant%20to%20label%20noise%202015"
        },
        {
            "id": "11",
            "entry": "[11] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberger%2C%20Jacob%20Ben-Reuven%2C%20Ehud%20Training%20deep%20neural-networks%20using%20a%20noise%20adaptation%20layer%202016"
        },
        {
            "id": "12",
            "entry": "[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "13",
            "entry": "[13] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. arXiv preprint arXiv:1802.05300, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05300"
        },
        {
            "id": "14",
            "entry": "[14] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, pages 646\u2013661.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Sun%2C%20Yu%20Liu%2C%20Zhuang%20Sedra%2C%20Daniel%20Deep%20networks%20with%20stochastic%20depth",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Sun%2C%20Yu%20Liu%2C%20Zhuang%20Sedra%2C%20Daniel%20Deep%20networks%20with%20stochastic%20depth"
        },
        {
            "id": "15",
            "entry": "[15] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing very deep neural networks on corrupted labels. arXiv preprint arXiv:1712.05055, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05055"
        },
        {
            "id": "16",
            "entry": "[16] Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learning deep networks from noisy labels with dropout regularization. In Data Mining (ICDM), 2016 IEEE 16th International Conference on, pages 967\u2013972. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jindal%2C%20Ishan%20Nokleby%2C%20Matthew%20Chen%2C%20Xuewen%20Learning%20deep%20networks%20from%20noisy%20labels%20with%20dropout%20regularization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jindal%2C%20Ishan%20Nokleby%2C%20Matthew%20Chen%2C%20Xuewen%20Learning%20deep%20networks%20from%20noisy%20labels%20with%20dropout%20regularization%202016"
        },
        {
            "id": "17",
            "entry": "[17] Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data. arXiv preprint arXiv:1712.04577, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04577"
        },
        {
            "id": "18",
            "entry": "[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "19",
            "entry": "[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "20",
            "entry": "[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "21",
            "entry": "[21] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems, pages 1189\u20131197, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-paced%20learning%20for%20latent%20variable%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-paced%20learning%20for%20latent%20variable%20models%202010"
        },
        {
            "id": "22",
            "entry": "[22] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Jia Li. Learning from noisy labels with distillation. arXiv preprint arXiv:1703.02391, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02391"
        },
        {
            "id": "23",
            "entry": "[23] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE Transactions on pattern analysis and machine intelligence, 38(3):447\u2013461, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Tongliang%20Liu%20and%20Dacheng%20Tao.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Tongliang%20Liu%20and%20Dacheng%20Tao.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016"
        },
        {
            "id": "24",
            "entry": "[24] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "25",
            "entry": "[25] Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions on cybernetics, 43(3):1146\u20131151, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manwani%2C%20Naresh%20Sastry%2C%20P.S.%20Noise%20tolerance%20under%20risk%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manwani%2C%20Naresh%20Sastry%2C%20P.S.%20Noise%20tolerance%20under%20risk%20minimization%202013"
        },
        {
            "id": "26",
            "entry": "[26] Hamed Masnadi-Shirazi and Nuno Vasconcelos. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In Advances in neural information processing systems, pages 1049\u20131056, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Masnadi-Shirazi%2C%20Hamed%20Vasconcelos%2C%20Nuno%20On%20the%20design%20of%20loss%20functions%20for%20classification%3A%20theory%2C%20robustness%20to%20outliers%2C%20and%20savageboost%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Masnadi-Shirazi%2C%20Hamed%20Vasconcelos%2C%20Nuno%20On%20the%20design%20of%20loss%20functions%20for%20classification%3A%20theory%2C%20robustness%20to%20outliers%2C%20and%20savageboost%202009"
        },
        {
            "id": "27",
            "entry": "[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "28",
            "entry": "[28] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In Advances in neural information processing systems, pages 1196\u20131204, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20Nagarajan%20Dhillon%2C%20Inderjit%20S.%20Ravikumar%2C%20Pradeep%20K.%20Tewari%2C%20Ambuj%20Learning%20with%20noisy%20labels%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20Nagarajan%20Dhillon%2C%20Inderjit%20S.%20Ravikumar%2C%20Pradeep%20K.%20Tewari%2C%20Ambuj%20Learning%20with%20noisy%20labels%202013"
        },
        {
            "id": "29",
            "entry": "[29] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1520\u20131528, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noh%2C%20Hyeonwoo%20Hong%2C%20Seunghoon%20Han%2C%20Bohyung%20Learning%20deconvolution%20network%20for%20semantic%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noh%2C%20Hyeonwoo%20Hong%2C%20Seunghoon%20Han%2C%20Bohyung%20Learning%20deconvolution%20network%20for%20semantic%20segmentation%202015"
        },
        {
            "id": "30",
            "entry": "[30] Curtis G Northcutt, Tailin Wu, and Isaac L Chuang. Learning with confident examples: Rank pruning for robust classification with noisy labels. arXiv preprint arXiv:1705.01936, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.01936"
        },
        {
            "id": "31",
            "entry": "[31] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: a loss correction approach. stat, 1050:22, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patrini%2C%20Giorgio%20Rozza%2C%20Alessandro%20Menon%2C%20Aditya%20Krishna%20Nock%2C%20Richard%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20a%20loss%20correction%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patrini%2C%20Giorgio%20Rozza%2C%20Alessandro%20Menon%2C%20Aditya%20Krishna%20Nock%2C%20Richard%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20a%20loss%20correction%20approach%202017"
        },
        {
            "id": "32",
            "entry": "[32] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6596"
        },
        {
            "id": "33",
            "entry": "[33] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. arXiv preprint arXiv:1803.09050, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09050"
        },
        {
            "id": "34",
            "entry": "[34] Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv preprint arXiv:1406.2080, 2(3):4, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.2080"
        },
        {
            "id": "35",
            "entry": "[35] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. arXiv preprint arXiv:1803.11364, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.11364"
        },
        {
            "id": "36",
            "entry": "[36] Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks. In Advances in Neural Information Processing Systems, pages 5601\u20135610, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vahdat%2C%20Arash%20Toward%20robustness%20against%20label%20noise%20in%20training%20deep%20discriminative%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vahdat%2C%20Arash%20Toward%20robustness%20against%20label%20noise%20in%20training%20deep%20discriminative%20neural%20networks%202017"
        },
        {
            "id": "37",
            "entry": "[37] Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label noise: The importance of being unhinged. In Advances in Neural Information Processing Systems, pages 10\u201318, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rooyen%2C%20Brendan%20Van%20Menon%2C%20Aditya%20Williamson%2C%20Robert%20C.%20Learning%20with%20symmetric%20label%20noise%3A%20The%20importance%20of%20being%20unhinged%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rooyen%2C%20Brendan%20Van%20Menon%2C%20Aditya%20Williamson%2C%20Robert%20C.%20Learning%20with%20symmetric%20label%20noise%3A%20The%20importance%20of%20being%20unhinged%202015"
        },
        {
            "id": "38",
            "entry": "[38] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning from noisy large-scale datasets with minimal supervision. In The Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veit%2C%20Andreas%20Alldrin%2C%20Neil%20Chechik%2C%20Gal%20Krasin%2C%20Ivan%20Learning%20from%20noisy%20large-scale%20datasets%20with%20minimal%20supervision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veit%2C%20Andreas%20Alldrin%2C%20Neil%20Chechik%2C%20Gal%20Krasin%2C%20Ivan%20Learning%20from%20noisy%20large-scale%20datasets%20with%20minimal%20supervision%202017"
        },
        {
            "id": "39",
            "entry": "[39] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia. Iterative learning with open-set noisy labels. arXiv preprint arXiv:1804.00092, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00092"
        },
        {
            "id": "40",
            "entry": "[40] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2691\u20132699, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Tong%20Xia%2C%20Tian%20Yang%2C%20Yi%20Huang%2C%20Chang%20Learning%20from%20massive%20noisy%20labeled%20data%20for%20image%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Tong%20Xia%2C%20Tian%20Yang%2C%20Yi%20Huang%2C%20Chang%20Learning%20from%20massive%20noisy%20labeled%20data%20for%20image%20classification%202015"
        },
        {
            "id": "41",
            "entry": "[41] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        }
    ]
}
