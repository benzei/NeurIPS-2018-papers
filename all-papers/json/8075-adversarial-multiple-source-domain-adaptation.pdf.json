{
    "filename": "8075-adversarial-multiple-source-domain-adaptation.pdf",
    "metadata": {
        "title": "Adversarial Multiple Source Domain Adaptation",
        "author": "Han Zhao, Shanghang Zhang, Guanhang Wu, Jos\u00e9 M. F. Moura, Joao P. Costeira, Geoffrey J. Gordon",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8075-adversarial-multiple-source-domain-adaptation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "While domain adaptation has been actively researched, most algorithms focus on the single-source-single-target adaptation setting. In this paper we propose new generalization bounds and algorithms under both classification and regression settings for unsupervised multiple source domain adaptation. Our theoretical analysis naturally leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose multisource domain adversarial networks (MDAN) that approach domain adaptation by optimizing task-adaptive generalization bounds. To demonstrate the effectiveness of MDAN, we conduct extensive experiments showing superior adaptation performance on both classification and regression problems: sentiment analysis, digit classification, and vehicle counting."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "mean absolute error",
            "url": "https://en.wikipedia.org/wiki/mean_absolute_error"
        },
        {
            "term": "Domain adaptation",
            "url": "https://en.wikipedia.org/wiki/Domain_adaptation"
        }
    ],
    "highlights": [
        "The success of machine learning has been partially attributed to rich datasets with abundant annotations [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>]",
        "Motivated by the bounds given in the last section, we propose our model, multisource domain adversarial networks (MDAN), with two versions: Hard version and Soft version",
        "We observe that with more training iterations, the performance of Hard-Max can be further improved. These results verify the effectiveness of multisource domain adversarial networks for multisource domain adaptation",
        "Does adding more source cameras always help improve the performance on the target camera? To answer this question, we analyze the counting error when we vary the number of source cameras as shown in Fig. 3a, where the x-axis refers to number of source cameras and the y-axis includes both the mean absolute error curve on the target camera as well as the proxy A-distance distance between the pair of source and target cameras",
        "We can see that multisource domain adversarial networks consistently decrease the proxy A-distance distances between all pairs of target and source domains, for both camera A and camera B. From this experiment we conclude that our proposed multisource domain adversarial networks models are effective in multiple source domain adaptation.\n6 Related Work",
        "We propose two multisource domain adversarial networks to learn feature representations that are invariant under multiple domain shifts while at the same time being discriminative for the learning task"
    ],
    "key_statements": [
        "The success of machine learning has been partially attributed to rich datasets with abundant annotations [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>]",
        "Such problem calls for an efficient technique for multiple source domain adaptation",
        "We analyze the multiple source domain adaptation problem and propose an adversarial learning strategy based on our theoretical results",
        "We introduce two versions of multisource domain adversarial networks: The hard version optimizes directly a simple worst-case generalization bound, while the soft version leads to a more data-efficient model and optimizes an average case and task-adaptive bound",
        "We provide average case generalization bounds for both classification and regression problems under the multisource domain adaptation setting.\n2)",
        "Motivated by the bounds given in the last section, we propose our model, multisource domain adversarial networks (MDAN), with two versions: Hard version and Soft version",
        "To avoid both of the problems, we propose the multisource domain adversarial networks Soft version that optimizes an upper bound of the convex combination bound given in (3)",
        "We evaluate both hard and soft multisource domain adversarial networks and compare them with state-of-the-art methods on three real-world datasets: the Amazon benchmark dataset [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] for sentiment analysis, a digit classification task that includes 4 datasets: MNIST [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], MNIST-M [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], SVHN [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], and SynthDigits [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], and a public, large-scale image dataset on vehicle counting from multiple city cameras [<a class=\"ref-link\" id=\"c52\" href=\"#r52\">52</a>]",
        "We observe that with more training iterations, the performance of Hard-Max can be further improved. These results verify the effectiveness of multisource domain adversarial networks for multisource domain adaptation",
        "MDAC [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] is a multiple source domain adaptation algorithm that explores causal models to represent the relationship between the features X and class label Y",
        "The results show that multisource domain adversarial networks outperforms all the baselines in the first two experiments and is comparable with Best-Single-DANN in the third experiment",
        "multisource domain adversarial networks always perform better than the source-only baseline (MDAN vs",
        "This conclusion comes from three observations: First, directly training DANN on a combination of multiple sources leads to worse results than the source-only baseline (Combine-DANN vs",
        "Combine-Source); Second, The performance of Combine-DANN can be even worse than the Best-Single-DANN; Third, directly training DANN on a combination of multiple sources always has lower accuracy compared with our approach (Combine-DANN vs",
        "We have similar observations for ADDA and MTAE. Such observations verify that the domain adaptation methods designed for single source lead to suboptimal solutions when applied to multiple sources",
        "Using a single domain in this case achieves the best result. This indicates that in domain adaptation the quality of data is much more important than the quantity. This experiment further demonstrates the effectiveness of multisource domain adversarial networks when there are multiple source domains available, where a naive combination of multiple sources using DANN may hurt generalization.\n5.3",
        "Does adding more source cameras always help improve the performance on the target camera? To answer this question, we analyze the counting error when we vary the number of source cameras as shown in Fig. 3a, where the x-axis refers to number of source cameras and the y-axis includes both the mean absolute error curve on the target camera as well as the proxy A-distance distance between the pair of source and target cameras",
        "We can see that multisource domain adversarial networks consistently decrease the proxy A-distance distances between all pairs of target and source domains, for both camera A and camera B. From this experiment we conclude that our proposed multisource domain adversarial networks models are effective in multiple source domain adaptation.\n6 Related Work",
        "We propose average case generalization bounds for both classification and regression problems",
        "We propose two multisource domain adversarial networks to learn feature representations that are invariant under multiple domain shifts while at the same time being discriminative for the learning task"
    ],
    "summary": [
        "The success of machine learning has been partially attributed to rich datasets with abundant annotations [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>].",
        "We analyze the multiple source domain adaptation problem and propose an adversarial learning strategy based on our theoretical results.",
        "We give new generalization bounds for both classification and regression problems under domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances.",
        "Our new result generalizes the bound [8, Thm. 2] to the case when there are multiple source domains, and",
        "We provide average case generalization bounds for both classification and regression problems under the multisource domain adaptation setting.",
        "In general our bound and the one in [9, Thm. 3] are incomparable, it is instructive to see the connections and differences between them: our bound works in the unsupervised domain adaptation setting where we do not have any labeled data from the target.",
        "Mansour et al [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] give a generalization bound for multisource domain adaptation under the assumption that the target distribution is a mixture of the k sources and the target hypothesis can be represented as a convex combination of the source hypotheses.",
        "A basic network trained on a combination of three source domains (20, 000 images for each) without domain adaptation and tested on the target domain.",
        "MDAC [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] is a multiple source domain adaptation algorithm that explores causal models to represent the relationship between the features X and class label Y .",
        "Several theoretical results have been derived in the form of upper bounds on the generalization target error by learning from the source data.",
        "These works generally outperform non-deep learning based methods, they only focus on the single-source-single-target DA problem, and much work is rather empirical design without statistical guarantees.",
        "We theoretically analyze generalization bounds for DA under the setting of multiple source domains with labeled instances and one target domain with unlabeled instances.",
        "We propose two MDAN to learn feature representations that are invariant under multiple domain shifts while at the same time being discriminative for the learning task.",
        "Both hard and soft versions of MDAN are generalizations of the popular DANN to the case when multiple source domains are available.",
        "MDAN outperforms the state-of-the-art DA methods on three real-world datasets, including a sentiment analysis task, a digit classification task, and a visual vehicle counting task, demonstrating its effectiveness in multisource domain adaptation for both classification and regression problems."
    ],
    "headline": "In this paper we propose new generalization bounds and algorithms under both classification and regression settings for unsupervised multiple source domain adaptation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Tameem Adel, Han Zhao, and Alexander Wong. Unsupervised domain adaptation with a relaxed covariate shift assumption. In AAAI, pages 1691\u20131697, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adel%2C%20Tameem%20Zhao%2C%20Han%20Wong%2C%20Alexander%20Unsupervised%20domain%20adaptation%20with%20a%20relaxed%20covariate%20shift%20assumption%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adel%2C%20Tameem%20Zhao%2C%20Han%20Wong%2C%20Alexander%20Unsupervised%20domain%20adaptation%20with%20a%20relaxed%20covariate%20shift%20assumption%202017"
        },
        {
            "id": "2",
            "entry": "[2] Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, and Mario Marchand. Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.4446"
        },
        {
            "id": "3",
            "entry": "[3] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20network%20learning%3A%20Theoretical%20foundations%202009"
        },
        {
            "id": "4",
            "entry": "[4] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. IEEE transactions on pattern analysis and machine intelligence, 33(5):898\u2013916, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arbelaez%2C%20Pablo%20Maire%2C%20Michael%20Fowlkes%2C%20Charless%20Malik%2C%20Jitendra%20Contour%20detection%20and%20hierarchical%20image%20segmentation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arbelaez%2C%20Pablo%20Maire%2C%20Michael%20Fowlkes%2C%20Charless%20Malik%2C%20Jitendra%20Contour%20detection%20and%20hierarchical%20image%20segmentation%202011"
        },
        {
            "id": "5",
            "entry": "[5] Mahsa Baktashmotlagh, Mehrtash T Harandi, Brian C Lovell, and Mathieu Salzmann. Unsupervised domain adaptation by domain invariant projection. In Proceedings of the IEEE International Conference on Computer Vision, pages 769\u2013776, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baktashmotlagh%2C%20Mahsa%20Harandi%2C%20Mehrtash%20T.%20Lovell%2C%20Brian%20C.%20Salzmann%2C%20Mathieu%20Unsupervised%20domain%20adaptation%20by%20domain%20invariant%20projection%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baktashmotlagh%2C%20Mahsa%20Harandi%2C%20Mehrtash%20T.%20Lovell%2C%20Brian%20C.%20Salzmann%2C%20Mathieu%20Unsupervised%20domain%20adaptation%20by%20domain%20invariant%20projection%202013"
        },
        {
            "id": "6",
            "entry": "[6] Carlos J Becker, Christos M Christoudias, and Pascal Fua. Non-linear domain adaptation with boosting. In Advances in Neural Information Processing Systems, pages 485\u2013493, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Becker%2C%20Carlos%20J.%20Christoudias%2C%20Christos%20M.%20Fua%2C%20Pascal%20Non-linear%20domain%20adaptation%20with%20boosting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Becker%2C%20Carlos%20J.%20Christoudias%2C%20Christos%20M.%20Fua%2C%20Pascal%20Non-linear%20domain%20adaptation%20with%20boosting%202013"
        },
        {
            "id": "7",
            "entry": "[7] Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19:137, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Pereira%2C%20Fernando%20Analysis%20of%20representations%20for%20domain%20adaptation%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Pereira%2C%20Fernando%20Analysis%20of%20representations%20for%20domain%20adaptation%202007"
        },
        {
            "id": "8",
            "entry": "[8] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79 (1-2):151\u2013175, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20A%20theory%20of%20learning%20from%20different%20domains%202010"
        },
        {
            "id": "9",
            "entry": "[9] John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning bounds for domain adaptation. In Advances in neural information processing systems, pages 129\u2013136, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20Pereira%2C%20Fernando%20Learning%20bounds%20for%20domain%20adaptation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20Pereira%2C%20Fernando%20Learning%20bounds%20for%20domain%20adaptation%202008"
        },
        {
            "id": "10",
            "entry": "[10] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation networks. In Advances in Neural Information Processing Systems, pages 343\u2013351, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousmalis%2C%20Konstantinos%20Trigeorgis%2C%20George%20Silberman%2C%20Nathan%20Krishnan%2C%20Dilip%20Domain%20separation%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousmalis%2C%20Konstantinos%20Trigeorgis%2C%20George%20Silberman%2C%20Nathan%20Krishnan%2C%20Dilip%20Domain%20separation%20networks%202016"
        },
        {
            "id": "11",
            "entry": "[11] Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. arXiv preprint arXiv:1206.4683, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.4683"
        },
        {
            "id": "12",
            "entry": "[12] Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103\u2013126, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Domain%20adaptation%20and%20sample%20bias%20correction%20theory%20and%20algorithm%20for%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Domain%20adaptation%20and%20sample%20bias%20correction%20theory%20and%20algorithm%20for%20regression%202014"
        },
        {
            "id": "13",
            "entry": "[13] Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection bias correction theory. In International Conference on Algorithmic Learning Theory, pages 38\u201353.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Riley%2C%20Michael%20Rostamizadeh%2C%20Afshin%20Sample%20selection%20bias%20correction%20theory",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Mohri%2C%20Mehryar%20Riley%2C%20Michael%20Rostamizadeh%2C%20Afshin%20Sample%20selection%20bias%20correction%20theory"
        },
        {
            "id": "14",
            "entry": "[14] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Icml, volume 32, pages 647\u2013655, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Ning%20Zhang%2C%20Eric%20Tzeng%2C%20and%20Trevor%20Darrell.%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Ning%20Zhang%2C%20Eric%20Tzeng%2C%20and%20Trevor%20Darrell.%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014"
        },
        {
            "id": "15",
            "entry": "[15] Theodoros Evgeniou and Massimiliano Pontil. Regularized multi\u2013task learning. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 109\u2013117. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Theodoros%20Evgeniou%20and%20Massimiliano%20Pontil.%20Regularized%20multi%E2%80%93task%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Theodoros%20Evgeniou%20and%20Massimiliano%20Pontil.%20Regularized%20multi%E2%80%93task%20learning%202004"
        },
        {
            "id": "16",
            "entry": "[16] Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 87\u201397, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Chuang%20Yang%2C%20Tianbao%20Gong%2C%20Boqing%20Learning%20attributes%20equals%20multi-source%20domain%20generalization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20Chuang%20Yang%2C%20Tianbao%20Gong%2C%20Boqing%20Learning%20attributes%20equals%20multi-source%20domain%20generalization%202016"
        },
        {
            "id": "17",
            "entry": "[17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1\u201335, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016"
        },
        {
            "id": "18",
            "entry": "[18] Pascal Germain, Amaury Habrard, Francois Laviolette, and Emilie Morvant. A pac-bayesian approach for domain adaptation with specialization to linear classifiers. In ICML (3), pages 738\u2013746, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Pascal%20Habrard%2C%20Amaury%20Laviolette%2C%20Francois%20Morvant%2C%20Emilie%20A%20pac-bayesian%20approach%20for%20domain%20adaptation%20with%20specialization%20to%20linear%20classifiers%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Pascal%20Habrard%2C%20Amaury%20Laviolette%2C%20Francois%20Morvant%2C%20Emilie%20A%20pac-bayesian%20approach%20for%20domain%20adaptation%20with%20specialization%20to%20linear%20classifiers%202013"
        },
        {
            "id": "19",
            "entry": "[19] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pages 2551\u20132559, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muhammad%20Ghifary%2C%20W.Bastiaan%20Kleijn%20Zhang%2C%20Mengjie%20Balduzzi%2C%20David%20Domain%20generalization%20for%20object%20recognition%20with%20multi-task%20autoencoders%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muhammad%20Ghifary%2C%20W.Bastiaan%20Kleijn%20Zhang%2C%20Mengjie%20Balduzzi%2C%20David%20Domain%20generalization%20for%20object%20recognition%20with%20multi-task%20autoencoders%202015"
        },
        {
            "id": "20",
            "entry": "[20] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 513\u2013520, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Domain%20adaptation%20for%20large-scale%20sentiment%20classification%3A%20A%20deep%20learning%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Domain%20adaptation%20for%20large-scale%20sentiment%20classification%3A%20A%20deep%20learning%20approach%202011"
        },
        {
            "id": "21",
            "entry": "[21] Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In ICML (1), pages 222\u2013230, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20Boqing%20Grauman%2C%20Kristen%20Sha%2C%20Fei%20Connecting%20the%20dots%20with%20landmarks%3A%20Discriminatively%20learning%20domain-invariant%20features%20for%20unsupervised%20domain%20adaptation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20Boqing%20Grauman%2C%20Kristen%20Sha%2C%20Fei%20Connecting%20the%20dots%20with%20landmarks%3A%20Discriminatively%20learning%20domain-invariant%20features%20for%20unsupervised%20domain%20adaptation%202013"
        },
        {
            "id": "22",
            "entry": "[22] Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Unsupervised adaptation across domain shifts by generating intermediate data representations. IEEE transactions on pattern analysis and machine intelligence, 36(11):2288\u20132302, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Unsupervised%20adaptation%20across%20domain%20shifts%20by%20generating%20intermediate%20data%20representations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Unsupervised%20adaptation%20across%20domain%20shifts%20by%20generating%20intermediate%20data%20representations%202014"
        },
        {
            "id": "23",
            "entry": "[23] Judy Hoffman, Brian Kulis, Trevor Darrell, and Kate Saenko. Discovering latent domains for multisource domain adaptation. In Computer Vision\u2013ECCV 2012, pages 702\u2013715.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Judy%20Kulis%2C%20Brian%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Discovering%20latent%20domains%20for%20multisource%20domain%20adaptation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Judy%20Kulis%2C%20Brian%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Discovering%20latent%20domains%20for%20multisource%20domain%20adaptation%202012"
        },
        {
            "id": "24",
            "entry": "[24] Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Multiple-source adaptation for regression problems. arXiv preprint arXiv:1711.05037, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.05037"
        },
        {
            "id": "25",
            "entry": "[25] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03213"
        },
        {
            "id": "26",
            "entry": "[26] I-Hong Jhuo, Dong Liu, DT Lee, and Shih-Fu Chang. Robust visual domain adaptation with low-rank reconstruction. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2168\u20132175. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jhuo%2C%20I.-Hong%20Dong%20Liu%2C%20D.T.Lee%20Chang%2C%20Shih-Fu%20Robust%20visual%20domain%20adaptation%20with%20low-rank%20reconstruction%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jhuo%2C%20I.-Hong%20Dong%20Liu%2C%20D.T.Lee%20Chang%2C%20Shih-Fu%20Robust%20visual%20domain%20adaptation%20with%20low-rank%20reconstruction%202012"
        },
        {
            "id": "27",
            "entry": "[27] Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 180\u2013191. VLDB Endowment, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kifer%2C%20Daniel%20Ben-David%2C%20Shai%20Gehrke%2C%20Johannes%20Detecting%20change%20in%20data%20streams%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kifer%2C%20Daniel%20Ben-David%2C%20Shai%20Gehrke%2C%20Johannes%20Detecting%20change%20in%20data%20streams%202004"
        },
        {
            "id": "28",
            "entry": "[28] Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47(5):1902\u20131914, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20Vladimir%20Rademacher%20penalties%20and%20structural%20risk%20minimization%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koltchinskii%2C%20Vladimir%20Rademacher%20penalties%20and%20structural%20risk%20minimization%202001"
        },
        {
            "id": "29",
            "entry": "[29] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "30",
            "entry": "[30] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pages 97\u2013105, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Mingsheng%20Cao%2C%20Yue%20Wang%2C%20Jianmin%20Jordan%2C%20Michael%20Learning%20transferable%20features%20with%20deep%20adaptation%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Mingsheng%20Cao%2C%20Yue%20Wang%2C%20Jianmin%20Jordan%2C%20Michael%20Learning%20transferable%20features%20with%20deep%20adaptation%20networks%202015"
        },
        {
            "id": "31",
            "entry": "[31] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair autoencoder. arXiv preprint arXiv:1511.00830, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.00830"
        },
        {
            "id": "32",
            "entry": "[32] Yishay Mansour and Mariano Schain. Robust domain adaptation. In ISAIM, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Schain%2C%20Mariano%20Robust%20domain%20adaptation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Schain%2C%20Mariano%20Robust%20domain%20adaptation%202012"
        },
        {
            "id": "33",
            "entry": "[33] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009.",
            "arxiv_url": "https://arxiv.org/pdf/0902.3430"
        },
        {
            "id": "34",
            "entry": "[34] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. In Advances in neural information processing systems, pages 1041\u20131048, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%20with%20multiple%20sources%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Domain%20adaptation%20with%20multiple%20sources%202009"
        },
        {
            "id": "35",
            "entry": "[35] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and the renyi divergence. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 367\u2013374. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Multiple%20source%20adaptation%20and%20the%20renyi%20divergence%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Multiple%20source%20adaptation%20and%20the%20renyi%20divergence%202009"
        },
        {
            "id": "36",
            "entry": "[36] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Talwalkar%2C%20Ameet%20Foundations%20of%20machine%20learning%202012"
        },
        {
            "id": "37",
            "entry": "[37] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "38",
            "entry": "[38] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Sinno%20Jialin%20Pan%20and%20Qiang%20Yang.%20A%20survey%20on%20transfer%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Sinno%20Jialin%20Pan%20and%20Qiang%20Yang.%20A%20survey%20on%20transfer%20learning%202010"
        },
        {
            "id": "39",
            "entry": "[39] Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adaptation. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pei%2C%20Zhongyi%20Cao%2C%20Zhangjie%20Long%2C%20Mingsheng%20Wang%2C%20Jianmin%20Multi-adversarial%20domain%20adaptation%202018"
        },
        {
            "id": "40",
            "entry": "[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "41",
            "entry": "[41] Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb. Learning from simulated and unsupervised images through adversarial training. arXiv preprint arXiv:1612.07828, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.07828"
        },
        {
            "id": "42",
            "entry": "[42] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. arXiv preprint arXiv:1802.08735, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08735"
        },
        {
            "id": "43",
            "entry": "[43] Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, and Jieping Ye. A two-stage weighting framework for multi-source domain adaptation. In Advances in neural information processing systems, pages 505\u2013513, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Qian%20Chattopadhyay%2C%20Rita%20Panchanathan%2C%20Sethuraman%20Ye%2C%20Jieping%20A%20two-stage%20weighting%20framework%20for%20multi-source%20domain%20adaptation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Qian%20Chattopadhyay%2C%20Rita%20Panchanathan%2C%20Sethuraman%20Ye%2C%20Jieping%20A%20two-stage%20weighting%20framework%20for%20multi-source%20domain%20adaptation%202011"
        },
        {
            "id": "44",
            "entry": "[44] Yuta Tsuboi, Hisashi Kashima, Shohei Hido, Steffen Bickel, and Masashi Sugiyama. Direct density ratio estimation for large-scale covariate shift adaptation. Journal of Information Processing, 17:138\u2013155, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsuboi%2C%20Yuta%20Kashima%2C%20Hisashi%20Hido%2C%20Shohei%20Bickel%2C%20Steffen%20Direct%20density%20ratio%20estimation%20for%20large-scale%20covariate%20shift%20adaptation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsuboi%2C%20Yuta%20Kashima%2C%20Hisashi%20Hido%2C%20Shohei%20Bickel%2C%20Steffen%20Direct%20density%20ratio%20estimation%20for%20large-scale%20covariate%20shift%20adaptation%202009"
        },
        {
            "id": "45",
            "entry": "[45] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4068\u20134076, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tzeng%2C%20Eric%20Hoffman%2C%20Judy%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Simultaneous%20deep%20transfer%20across%20domains%20and%20tasks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tzeng%2C%20Eric%20Hoffman%2C%20Judy%20Darrell%2C%20Trevor%20Saenko%2C%20Kate%20Simultaneous%20deep%20transfer%20across%20domains%20and%20tasks%202015"
        },
        {
            "id": "46",
            "entry": "[46] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. arXiv preprint arXiv:1702.05464, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05464"
        },
        {
            "id": "47",
            "entry": "[47] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Bengio%2C%20Yoshua%20Manzagol%2C%20Pierre-Antoine%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "48",
            "entry": "[48] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371\u20133408, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010"
        },
        {
            "id": "49",
            "entry": "[49] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391\u2013423, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012"
        },
        {
            "id": "50",
            "entry": "[50] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "51",
            "entry": "[51] Kun Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation: A causal view. In AAAI, pages 3150\u20133157, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Kun%20Gong%2C%20Mingming%20Scholkopf%2C%20Bernhard%20Multi-source%20domain%20adaptation%3A%20A%20causal%20view%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Kun%20Gong%2C%20Mingming%20Scholkopf%2C%20Bernhard%20Multi-source%20domain%20adaptation%3A%20A%20causal%20view%202015"
        },
        {
            "id": "52",
            "entry": "[52] Shanghang Zhang, Guanhang Wu, Joao P Costeira, and Jose MF Moura. Understanding traffic density from large-scale web camera data. arXiv preprint arXiv:1703.05868, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1703.05868"
        }
    ]
}
