{
    "filename": "7371-dropmax-adaptive-variational-softmax.pdf",
    "metadata": {
        "title": "DropMax: Adaptive Variational Softmax",
        "author": "Hae Beom Lee, Juho Lee, Saehoon Kim, Eunho Yang, Sung Ju Hwang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7371-dropmax-adaptive-variational-softmax.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover, the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification."
    },
    "keywords": [
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        }
    ],
    "highlights": [
        "Deep learning models have shown impressive performances on classification tasks [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "We propose a novel variant of softmax classifier that achieves improved accuracy over the regular softmax function by leveraging the popular dropout regularization, which we refer to as DropMax",
        "We propose a variational inference framework to adaptively learn the dropout probability of non-target classes for each input, s.t. our stochastic classifier considers non-target classes confused with the true class of each instance more often than others",
        "We proposed a stochastic version of a softmax function, DropMax, that randomly drops non-target classes at each iteration of the training step",
        "We further proposed to learn the class dropout probabilities based on the input, such that it can consider the discrimination of each instance against more confusing classes"
    ],
    "key_statements": [
        "Deep learning models have shown impressive performances on classification tasks [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "We propose a novel variant of softmax classifier that achieves improved accuracy over the regular softmax function by leveraging the popular dropout regularization, which we refer to as DropMax",
        "We propose a variational inference framework to adaptively learn the dropout probability of non-target classes for each input, s.t. our stochastic classifier considers non-target classes confused with the true class of each instance more often than others",
        "We proposed a stochastic version of a softmax function, DropMax, that randomly drops non-target classes at each iteration of the training step",
        "We further proposed to learn the class dropout probabilities based on the input, such that it can consider the discrimination of each instance against more confusing classes"
    ],
    "summary": [
        "Deep learning models have shown impressive performances on classification tasks [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "We extend our classifier to learn the probability of dropping non-target classes for each input instance, such that the stochastic classifier can consider",
        "We propose a novel stochastic softmax function, DropMax, that randomly drops non-target classes when computing the class probability for each input instance.",
        "We propose a variational inference framework to adaptively learn the dropout probability of non-target classes for each input, s.t. our stochastic classifier considers non-target classes confused with the true class of each instance more often than others.",
        "Adaptive dropout [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] learns input dependent posterior at test time by overlaying binary belief network on hidden layers.",
        "6) Deterministic DropMax. This model is the same with Deterministic Attention, except that it is trained in a supervised manner with true class labels.",
        "8) DropMax. Our adaptive stochastic softmax, where each class is dropped out with input dependent probabilities trained from the data.",
        "Deterministic DropMax with supervised learning of attention mechanism improves the performance over the base soft classifier, which suggests that such combination of a multi-class and multi-label classifier could be somewhat beneficial.",
        "The gating function of DropMax is optimally regularized to make a crude selection of candidate classes via the proposed variational inference framework, and shows consistent and significant improvements over the baselines across all datasets.",
        "It suggests that retain probability implicitly learns correlations between classes, since it is modeled as an input dependent distribution.",
        "Since DropMax is a Bayesian inference framework, we can obtain predictive uncertainty from MC sampling in Figure 4(f), even when probabilistic modeling on intermediate layers is difficult.",
        "We proposed a stochastic version of a softmax function, DropMax, that randomly drops non-target classes at each iteration of the training step.",
        "We further proposed to learn the class dropout probabilities based on the input, such that it can consider the discrimination of each instance against more confusing classes.",
        "We cast this as a Bayesian learning problem and present how to optimize the parameters through variational inference, while proposing a novel regularizer to more exactly estimate the true posterior.",
        "We validate our model on multiple public datasets for classification, on which our model consistently obtains significant performance improvements over the base softmax classifier and its variants, achieving especially high accuracy on datasets for finegrained classification.",
        "We plan to further investigate the source of generalization improvements with DropMax, besides increased stability of gradients (Appendix B)"
    ],
    "headline": "We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. Tensorflow: Large-scale Machine Learning on Heterogeneous Distributed Systems. arXiv:1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "2",
            "entry": "[2] J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20J.%20Frey%2C%20B.%20Adaptive%20dropout%20for%20training%20deep%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20J.%20Frey%2C%20B.%20Adaptive%20dropout%20for%20training%20deep%20neural%20networks%202013"
        },
        {
            "id": "3",
            "entry": "[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015"
        },
        {
            "id": "4",
            "entry": "[4] X. Bouthillier, K. Konda, P. Vincent, and R. Memisevic. Dropout as data augmentation. ArXiv e-prints, June 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bouthillier%2C%20X.%20Konda%2C%20K.%20Vincent%2C%20P.%20R.%20Memisevic.%20Dropout%20as%20data%20augmentation.%20ArXiv%20e-prints%202015-06"
        },
        {
            "id": "5",
            "entry": "[5] A. de Br\u00e9bisson and P. Vincent. An exploration of softmax alternatives belonging to the spherical loss family. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Br%C3%A9bisson%2C%20A.%20Vincent%2C%20P.%20An%20exploration%20of%20softmax%20alternatives%20belonging%20to%20the%20spherical%20loss%20family%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Br%C3%A9bisson%2C%20A.%20Vincent%2C%20P.%20An%20exploration%20of%20softmax%20alternatives%20belonging%20to%20the%20spherical%20loss%20family%202016"
        },
        {
            "id": "6",
            "entry": "[6] Y. Gal and Z. Ghahramani. Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference. ArXiv e-prints, June 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20Bayesian%20Convolutional%20Neural%20Networks%20with%20Bernoulli%20Approximate%20Variational%20Inference.%20ArXiv%20e-prints%202015-06"
        },
        {
            "id": "7",
            "entry": "[7] Y. Gal and Z. Ghahramani. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20A%20Theoretically%20Grounded%20Application%20of%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20A%20Theoretically%20Grounded%20Application%20of%202016"
        },
        {
            "id": "8",
            "entry": "[8] Y. Gal and Z. Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20Dropout%20as%20a%20Bayesian%20Approximation%3A%20Representing%20Model%20Uncertainty%20in%20Deep%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Y.%20Ghahramani%2C%20Z.%20Dropout%20as%20a%20Bayesian%20Approximation%3A%20Representing%20Model%20Uncertainty%20in%20Deep%20Learning%202016"
        },
        {
            "id": "9",
            "entry": "[9] Y. Gal, J. Hron, and A. Kendall. Concrete Dropout. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%20Gal%20J%20Hron%20and%20A%20Kendall%20Concrete%20Dropout%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%20Gal%20J%20Hron%20and%20A%20Kendall%20Concrete%20Dropout%20In%20NIPS%202017"
        },
        {
            "id": "10",
            "entry": "[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202016"
        },
        {
            "id": "11",
            "entry": "[11] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20van%20der%20Maaten%2C%20L.%20Weinberger%2C%20K.Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20van%20der%20Maaten%2C%20L.%20Weinberger%2C%20K.Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "12",
            "entry": "[12] S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On Using Very Large Target Vocabulary for Neural Machine Translation. In ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jean%2C%20S.%20Cho%2C%20K.%20Memisevic%2C%20R.%20Bengio%2C%20Y.%20On%20Using%20Very%20Large%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jean%2C%20S.%20Cho%2C%20K.%20Memisevic%2C%20R.%20Bengio%2C%20Y.%20On%20Using%20Very%20Large%202015"
        },
        {
            "id": "13",
            "entry": "[13] A. Kendall and Y. Gal. What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20Deep%20Learning%20for%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kendall%2C%20A.%20Gal%2C%20Y.%20What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20Deep%20Learning%20for%202017"
        },
        {
            "id": "14",
            "entry": "[14] D. P. Kingma, T. Salimans, and M. Welling. Variational Dropout and the Local Reparameterization Trick. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%20P%20Kingma%20T%20Salimans%20and%20M%20Welling%20Variational%20Dropout%20and%20the%20Local%20Reparameterization%20Trick%20In%20NIPS%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%20P%20Kingma%20T%20Salimans%20and%20M%20Welling%20Variational%20Dropout%20and%20the%20Local%20Reparameterization%20Trick%20In%20NIPS%202015"
        },
        {
            "id": "15",
            "entry": "[15] D. P. Kingma and M. Welling. Auto encoding variational bayes. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto%20encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto%20encoding%20variational%20bayes%202014"
        },
        {
            "id": "16",
            "entry": "[16] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "17",
            "entry": "[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20Classification%20with%20Deep%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20Classification%20with%20Deep%202012"
        },
        {
            "id": "18",
            "entry": "[18] C. Lampert, H. Nickisch, and S. Harmeling. Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampert%2C%20C.%20Nickisch%2C%20H.%20Harmeling%2C%20S.%20Learning%20to%20Detect%20Unseen%20Object%20Classes%20by%20Between-Class%20Attribute%20Transfer%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampert%2C%20C.%20Nickisch%2C%20H.%20Harmeling%2C%20S.%20Learning%20to%20Detect%20Unseen%20Object%20Classes%20by%20Between-Class%20Attribute%20Transfer%202009"
        },
        {
            "id": "19",
            "entry": "[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "20",
            "entry": "[20] M.-T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and W. Zaremba. Addressing the Rare Word Problem in Neural Machine Translation. In ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MT%20Luong%20I%20Sutskever%20Q%20V%20Le%20O%20Vinyals%20and%20W%20Zaremba%20Addressing%20the%20Rare%20Word%20Problem%20in%20Neural%20Machine%20Translation%20In%20ACL%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MT%20Luong%20I%20Sutskever%20Q%20V%20Le%20O%20Vinyals%20and%20W%20Zaremba%20Addressing%20the%20Rare%20Word%20Problem%20in%20Neural%20Machine%20Translation%20In%20ACL%202015"
        },
        {
            "id": "21",
            "entry": "[21] C. J. Maddison, A. Mnih, and Y. Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20J%20Maddison%20A%20Mnih%20and%20Y%20Whye%20Teh%20The%20Concrete%20Distribution%20A%20Continuous%20Relaxation%20of%20Discrete%20Random%20Variables%20In%20ICLR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20J%20Maddison%20A%20Mnih%20and%20Y%20Whye%20Teh%20The%20Concrete%20Distribution%20A%20Continuous%20Relaxation%20of%20Discrete%20Random%20Variables%20In%20ICLR%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. F. T. Martins and R. Fernandez Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martins%2C%20A.F.T.%20Astudillo%2C%20R.Fernandez%20From%20Softmax%20to%20Sparsemax%3A%20A%20Sparse%20Model%20of%20Attention%20and%20Multi-Label%20Classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martins%2C%20A.F.T.%20Astudillo%2C%20R.Fernandez%20From%20Softmax%20to%20Sparsemax%3A%20A%20Sparse%20Model%20of%20Attention%20and%20Multi-Label%20Classification%202016"
        },
        {
            "id": "23",
            "entry": "[23] D. Molchanov, A. Ashukha, and D. Vetrov. Variational Dropout Sparsifies Deep Neural Networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%20Molchanov%20A%20Ashukha%20and%20D%20Vetrov%20Variational%20Dropout%20Sparsifies%20Deep%20Neural%20Networks%20In%20ICML%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%20Molchanov%20A%20Ashukha%20and%20D%20Vetrov%20Variational%20Dropout%20Sparsifies%20Deep%20Neural%20Networks%20In%20ICML%202017"
        },
        {
            "id": "24",
            "entry": "[24] K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep conditional generative models. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sohn%2C%20K.%20Lee%2C%20H.%20Yan%2C%20X.%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sohn%2C%20K.%20Lee%2C%20H.%20Yan%2C%20X.%20Learning%20structured%20output%20representation%20using%20deep%20conditional%20generative%20models%202015"
        },
        {
            "id": "25",
            "entry": "[25] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "26",
            "entry": "[26] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical report, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wah%2C%20C.%20Branson%2C%20S.%20Welinder%2C%20P.%20Perona%2C%20P.%20The%20Caltech-UCSD%20Birds-200-2011%20Dataset%202011"
        },
        {
            "id": "27",
            "entry": "[27] S. Wang and C. Manning. Fast dropout training. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20S.%20Manning%2C%20C.%20Fast%20dropout%20training%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20S.%20Manning%2C%20C.%20Fast%20dropout%20training%202013"
        },
        {
            "id": "28",
            "entry": "[28] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20Xu%20J%20Ba%20R%20Kiros%20K%20Cho%20A%20Courville%20R%20Salakhutdinov%20R%20Zemel%20and%20Y%20Bengio%20Show%20Attend%20and%20Tell%20Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention%20In%20ICML%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20Xu%20J%20Ba%20R%20Kiros%20K%20Cho%20A%20Courville%20R%20Salakhutdinov%20R%20Zemel%20and%20Y%20Bengio%20Show%20Attend%20and%20Tell%20Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention%20In%20ICML%202015"
        }
    ]
}
