{
    "filename": "8247-discretely-relaxing-continuous-variables-for-tractable-variational-inference.pdf",
    "metadata": {
        "date": 2018,
        "title": "Discretely Relaxing Continuous Variables for tractable Variational Inference",
        "author": "Trefor W. Evans University of Toronto trefor.evans@mail.utoronto.ca",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8247-discretely-relaxing-continuous-variables-for-tractable-variational-inference.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed \"DIRECT\" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient estimates), eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition, our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. The DIRECT approach is not practical for all likelihoods, however, we identify a popular model structure which is practical, and demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered in the numerical studies require over 102352 log-likelihood evaluations, we train on datasets with over two-million points in just seconds."
    },
    "keywords": [
        {
            "term": "generalized linear models",
            "url": "https://en.wikipedia.org/wiki/generalized_linear_models"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "DIRECT",
            "url": "https://en.wikipedia.org/wiki/DIRECT"
        },
        {
            "term": "ELBO",
            "url": "https://en.wikipedia.org/wiki/ELBO"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "root mean squared error",
            "url": "https://en.wikipedia.org/wiki/root_mean_squared_error"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "low precision",
            "url": "https://en.wikipedia.org/wiki/low_precision"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Hardware restrictions posed by mobile devices make Bayesian inference particularly ill-suited for on-board machine learning",
        "We have demonstrated that through the use of Kronecker matrix algebra, the evidence lower bound of a discretely relaxed model can be efficiently and exactly computed even when this computation requires significantly more log-likelihood evaluations than the number of atoms in the known universe",
        "Through this ability to exactly perform evidence lower bound computations we achieve unbiased, zero-variance gradient estimates using automatic differentiation which we show significantly outperforms competing Monte Carlo alternatives that admit high-variance gradient estimates",
        "We demonstrate that the computational complexity of evidence lower bound computations is independent of the quantity of training data using the DIRECT method, making the proposed approaches amenable to big data applications",
        "We show that we can again use Kronecker matrix algebra to exactly compute the statistical moments of the parameterized predictive posterior distribution, unlike competing techniques which rely on Monte Carlo sampling",
        "We discuss and demonstrate novel extensions to the DIRECT method that allow efficient computation of a lower bound of the evidence lower bound, and we demonstrate how an unfactorized variational distribution can be used by introducing a manageable level of stochasticity into the gradients"
    ],
    "key_statements": [
        "Hardware restrictions posed by mobile devices make Bayesian inference particularly ill-suited for on-board machine learning",
        "We make this comparison with continuous variables by discretely relaxing continuous priors using a discrete prior with a finite support set that contains much of the structure and information as its continuous analogue. Using this discretized prior we show that we can make use of Kronecker matrix algebra for efficient and exact evidence lower bound computations",
        "At inference time, we can exactly compute the statistical moments of the parameterized predictive posterior distribution, unlike competing techniques which rely on Monte Carlo sampling",
        "The technique uses Monte Carlo estimates of the gradient of the evidence lower bound (ELBO) which is maximized during the training procedure. While this approach has been employed successfully for many large-scale models, we find that discretely relaxing continuous latent variable priors can improve training and inference performance when using our proposed DIRECT technique which computes the evidence lower bound exactly",
        "The DIRECT method allows us to efficiently and exactly compute the evidence lower bound which has several advantages over existing stochastic variational inference techniques for discrete latent variable models such as, zerovariance gradient estimates, the ability to use a super-linearly convergent quasi-Newton optimizer, and the per-iteration complexity is independent of training set size",
        "It can be shown that the evidence lower bound can be exactly computed in Op ms pb{ q4 q for a deep Bayesian neural network with layers, where we assume a quadratic activation function and an equal distribution of discrete latent variables between network layers. This complexity enables scalable Bayesian inference for models of moderate depth, and like we found for the regression generalized linear models model of section 3.1, computational complexity is independent of the quantity of training data, making this approach ideal for large datasets",
        "In table 2 of the supplement, we present results for a DIRECT mixture model that uses the evidence lower bound lower bound presented in Theorem 3",
        "We have shown that by discretely relaxing continuous priors, variational inference can be performed accurately and efficiently using our DIRECT method",
        "We have demonstrated that through the use of Kronecker matrix algebra, the evidence lower bound of a discretely relaxed model can be efficiently and exactly computed even when this computation requires significantly more log-likelihood evaluations than the number of atoms in the known universe",
        "Through this ability to exactly perform evidence lower bound computations we achieve unbiased, zero-variance gradient estimates using automatic differentiation which we show significantly outperforms competing Monte Carlo alternatives that admit high-variance gradient estimates",
        "We demonstrate that the computational complexity of evidence lower bound computations is independent of the quantity of training data using the DIRECT method, making the proposed approaches amenable to big data applications",
        "We show that we can again use Kronecker matrix algebra to exactly compute the statistical moments of the parameterized predictive posterior distribution, unlike competing techniques which rely on Monte Carlo sampling",
        "We discuss and demonstrate how posterior samples can be sparse and can be represented as quantized integer values to enable efficient inference which is particularly powerful on hardware limited devices, or if energy efficiency is a major concern",
        "We discuss and demonstrate novel extensions to the DIRECT method that allow efficient computation of a lower bound of the evidence lower bound, and we demonstrate how an unfactorized variational distribution can be used by introducing a manageable level of stochasticity into the gradients"
    ],
    "summary": [
        "Hardware restrictions posed by mobile devices make Bayesian inference particularly ill-suited for on-board machine learning.",
        "While this approach has been employed successfully for many large-scale models, we find that discretely relaxing continuous latent variable priors can improve training and inference performance when using our proposed DIRECT technique which computes the ELBO exactly.",
        "The DIRECT method allows us to efficiently and exactly compute the ELBO which has several advantages over existing SVI techniques for discrete latent variable models such as, zerovariance gradient estimates, the ability to use a super-linearly convergent quasi-Newton optimizer, and the per-iteration complexity is independent of training set size.",
        "The following result demonstrates how the ELBO can be exactly and efficiently computed, assuming the factorized prior and variational distributions over w discussed earlier.",
        "In the interest of generalizing the technique, we outline a likelihood, a prior, and a variational distribution that does not admit a compact representation of the ELBO and discuss several ways the DIRECT method can still be used to efficiently compute, or lower bound the ELBO.",
        "While q can be expressed as a compact sum of Kronecker product vectors, log q is more challenging to compute than in the mean-field case, the following result demonstrates how we can lower-bound the term involving log q in the ELBO).",
        "As discussed in section 2, we cannot reparameterize because of the discrete latent variable priors considered, we can directly compare the optimization performance of the proposed techniques with the REINFORCE gradient estimator [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "The right two models use a discrete relaxation of a Gaussian prior (DIRECT) with support at 15 discrete values, allowing storage of each latent variable sample as a vector of 4-bit quantized integers.",
        "We have demonstrated that through the use of Kronecker matrix algebra, the ELBO of a discretely relaxed model can be efficiently and exactly computed even when this computation requires significantly more log-likelihood evaluations than the number of atoms in the known universe.",
        "We show that we can again use Kronecker matrix algebra to exactly compute the statistical moments of the parameterized predictive posterior distribution, unlike competing techniques which rely on Monte Carlo sampling.",
        "We illustrate the DIRECT approach on several popular models such as mean-field variational inference for generalized linear models and deep Bayesian neural networks for regression.",
        "We discuss and demonstrate novel extensions to the DIRECT method that allow efficient computation of a lower bound of the ELBO, and we demonstrate how an unfactorized variational distribution can be used by introducing a manageable level of stochasticity into the gradients.",
        "We discuss and demonstrate how posterior samples can be sparse and can be represented as quantized integer values to enable efficient inference which is particularly powerful on hardware limited devices, or if energy efficiency is a major concern"
    ],
    "headline": "We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Thrun, W. Burgard, and D. Fox. Probabilistic robotics. MIT press, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20S.%20Burgard%2C%20W.%20Fox%2C%20D.%20Probabilistic%20robotics%202005"
        },
        {
            "id": "2",
            "entry": "[2] J. Bradshaw, A. G. d. G. Matthews, and Z. Ghahramani. Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks. Tech. rep. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bradshaw%2C%20J.%20d.%20G.%20Matthews%2C%20A.G.%20Ghahramani%2C%20Z.%20Adversarial%20Examples%2C%20Uncertainty%2C%20and%20Transfer%20Testing%20Robustness%20in%20Gaussian%20Process%20Hybrid%20Deep%20Networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] C. Louizos, K. Ullrich, and M. Welling. \u201cBayesian compression for deep learning\u201d. In: Advances in Neural Information Processing Systems. 2017, pp. 3288\u20133298.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20C.%20Ullrich%2C%20K.%20Welling%2C%20M.%20Bayesian%20compression%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20C.%20Ullrich%2C%20K.%20Welling%2C%20M.%20Bayesian%20compression%20for%20deep%20learning%202017"
        },
        {
            "id": "4",
            "entry": "[4] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. \u201cAn introduction to variational methods for graphical models\u201d. In: Machine learning 37.2 (1999), pp. 183\u2013233.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.I.%20Ghahramani%2C%20Z.%20Jaakkola%2C%20T.S.%20Saul%2C%20L.K.%20%E2%80%9CAn%20introduction%20to%20variational%20methods%20for%20graphical%20models%E2%80%9D.%20In%3A%20Machine%20learning%2037.2%201999"
        },
        {
            "id": "5",
            "entry": "[5] M. J. Wainwright and M. I. Jordan. \u201cGraphical models, exponential families, and variational inference\u201d. In: Foundations and Trends in Machine Learning 1.1\u20132 (2008), pp. 1\u2013305.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008"
        },
        {
            "id": "6",
            "entry": "[6] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. \u201cStochastic variational inference\u201d. In: The Journal of Machine Learning Research 14.1 (2013), pp. 1303\u20131347.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Wang%2C%20C.%20Paisley%2C%20J.%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "7",
            "entry": "[7] R. Ranganath, S. Gerrish, and D. Blei. \u201cBlack box variational inference\u201d. In: Artificial Intelligence and Statistics. 2014, pp. 814\u2013822.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Gerrish%2C%20S.%20Blei%2C%20D.%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "8",
            "entry": "[8] A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei. \u201cAutomatic differentiation variational inference\u201d. In: The Journal of Machine Learning Research 18.1 (2017), pp. 430\u2013 474.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kucukelbir%2C%20A.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Gelman%2C%20A.%20Automatic%20differentiation%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kucukelbir%2C%20A.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Gelman%2C%20A.%20Automatic%20differentiation%20variational%20inference%202017"
        },
        {
            "id": "9",
            "entry": "[9] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. \u201cVariational inference: A review for statisticians\u201d. In: Journal of the American Statistical Association 112.518 (2017), pp. 859\u2013877.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20D.M.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20D.M.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "10",
            "entry": "[10] D. P. Kingma and M. Welling. \u201cAuto-encoding variational Bayes\u201d. In: arXiv preprint arXiv:1312.6114 (2013).",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "11",
            "entry": "[11] R. J. Williams. \u201cSimple statistical gradient-following algorithms for connectionist reinforcement learning\u201d. In: Reinforcement Learning. 1992, pp. 5\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "12",
            "entry": "[12] M. C. Fu. \u201cGradient estimation\u201d. In: Handbooks in operations research and management science 13 (2006), pp. 575\u2013616.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20M.C.%20%E2%80%9CGradient%20estimation%E2%80%9D.%20In%3A%20Handbooks%20in%20operations%20research%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20M.C.%20%E2%80%9CGradient%20estimation%E2%80%9D.%20In%3A%20Handbooks%20in%20operations%20research%202006"
        },
        {
            "id": "13",
            "entry": "[13] P. W. Glynn. \u201cLikelihood ratio gradient estimation for stochastic systems\u201d. In: Communications of the ACM 33.10 (1990), pp. 75\u201384.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glynn%2C%20P.W.%20Likelihood%20ratio%20gradient%20estimation%20for%20stochastic%20systems%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glynn%2C%20P.W.%20Likelihood%20ratio%20gradient%20estimation%20for%20stochastic%20systems%201990"
        },
        {
            "id": "14",
            "entry": "[14] E. Jang, S. Gu, and B. Poole. \u201cCategorical reparameterization with Gumbel-softmax\u201d. In: arXiv preprint arXiv:1611.01144 (2016).",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "15",
            "entry": "[15] C. J. Maddison, A. Mnih, and Y. W. Teh. \u201cThe concrete distribution: A continuous relaxation of discrete random variables\u201d. In: arXiv preprint arXiv:1611.00712 (2016).",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "16",
            "entry": "[16] G. Tucker, A. Mnih, C. J. Maddison, J. Lawson, and J. Sohl-Dickstein. \u201cREBAR: Lowvariance, unbiased gradient estimates for discrete latent variable models\u201d. In: Advances in Neural Information Processing Systems. 2017, pp. 2627\u20132636.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tucker%2C%20G.%20Mnih%2C%20A.%20Maddison%2C%20C.J.%20Lawson%2C%20J.%20REBAR%3A%20Lowvariance%2C%20unbiased%20gradient%20estimates%20for%20discrete%20latent%20variable%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tucker%2C%20G.%20Mnih%2C%20A.%20Maddison%2C%20C.J.%20Lawson%2C%20J.%20REBAR%3A%20Lowvariance%2C%20unbiased%20gradient%20estimates%20for%20discrete%20latent%20variable%20models%202017"
        },
        {
            "id": "17",
            "entry": "[17] W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud. \u201cBackpropagation through the void: Optimizing control variates for black-box gradient estimation\u201d. In: International Conference on Learning Representations. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grathwohl%2C%20W.%20Choi%2C%20D.%20Wu%2C%20Y.%20Roeder%2C%20G.%20Backpropagation%20through%20the%20void%3A%20Optimizing%20control%20variates%20for%20black-box%20gradient%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grathwohl%2C%20W.%20Choi%2C%20D.%20Wu%2C%20Y.%20Roeder%2C%20G.%20Backpropagation%20through%20the%20void%3A%20Optimizing%20control%20variates%20for%20black-box%20gradient%20estimation%202017"
        },
        {
            "id": "18",
            "entry": "[18] C. F. Van Loan. \u201cThe ubiquitous Kronecker product\u201d. In: Journal of Computational and Applied Mathematics 123.1 (2000), pp. 85\u2013100.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loan%2C%20C.F.Van%20The%20ubiquitous%20Kronecker%20product%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loan%2C%20C.F.Van%20The%20ubiquitous%20Kronecker%20product%202000"
        },
        {
            "id": "19",
            "entry": "[19] R. A. Horn and C. R. Johnson. Topics in Matrix analysis. Cambridge university press, 1994, p. 208.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horn%2C%20R.A.%20Johnson%2C%20C.R.%20Topics%20in%20Matrix%20analysis%201994"
        },
        {
            "id": "20",
            "entry": "[20] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing. \u201cDeep kernel learning\u201d. In: Artificial Intelligence and Statistics. 2016, pp. 370\u2013378.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.G.%20Hu%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20Deep%20kernel%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.G.%20Hu%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20Deep%20kernel%20learning%202016"
        },
        {
            "id": "21",
            "entry": "[21] W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen. \u201cCompressing neural networks with the hashing trick\u201d. In: International Conference on Machine Learning. 2015, pp. 2285\u20132294.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20W.%20Wilson%2C%20J.%20Tyree%2C%20S.%20Weinberger%2C%20K.%20Compressing%20neural%20networks%20with%20the%20hashing%20trick%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20W.%20Wilson%2C%20J.%20Tyree%2C%20S.%20Weinberger%2C%20K.%20Compressing%20neural%20networks%20with%20the%20hashing%20trick%202015"
        },
        {
            "id": "22",
            "entry": "[22] Y. Gong, L. Liu, M. Yang, and L. Bourdev. \u201cCompressing deep convolutional networks using vector quantization\u201d. In: arXiv preprint arXiv:1412.6115 (2014).",
            "arxiv_url": "https://arxiv.org/pdf/1412.6115"
        },
        {
            "id": "23",
            "entry": "[23] S. Han, H. Mao, and W. J. Dally. \u201cDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\u201d. In: arXiv preprint arXiv:1510.00149 (2015).",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "24",
            "entry": "[24] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen. \u201cIncremental network quantization: Towards lossless cnns with low-precision weights\u201d. In: arXiv preprint arXiv:1702.03044 (2017).",
            "arxiv_url": "https://arxiv.org/pdf/1702.03044"
        },
        {
            "id": "25",
            "entry": "[25] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. \u201cBinarized neural networks\u201d. In: Advances in neural information processing systems. 2016, pp. 4107\u20134115.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hubara%2C%20I.%20Courbariaux%2C%20M.%20Soudry%2C%20D.%20El-Yaniv%2C%20R.%20%E2%80%9CBinarized%20neural%20networks%E2%80%9D.%20In%3A%20Advances%20in%20neural%20information%20processing%20systems%202016"
        },
        {
            "id": "26",
            "entry": "[26] F. Li, B. Zhang, and B. Liu. \u201cTernary weight networks\u201d. In: arXiv preprint arXiv:1605.04711 (2016).",
            "arxiv_url": "https://arxiv.org/pdf/1605.04711"
        },
        {
            "id": "27",
            "entry": "[27] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. \u201cXnor-net: Imagenet classification using binary convolutional neural networks\u201d. In: European Conference on Computer Vision. Springer. 2016, pp. 525\u2013542.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20M.%20Ordonez%2C%20V.%20Redmon%2C%20J.%20Farhadi%2C%20A.%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rastegari%2C%20M.%20Ordonez%2C%20V.%20Redmon%2C%20J.%20Farhadi%2C%20A.%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "28",
            "entry": "[28] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko. \u201cQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\u201d. In: arXiv preprint arXiv:1712.05877 (2017).",
            "arxiv_url": "https://arxiv.org/pdf/1712.05877"
        },
        {
            "id": "29",
            "entry": "[29] P. Lazarsfeld and N. Henry. Latent structure analysis. Houghton Mifflin Company, Boston, Massachusetts, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazarsfeld%2C%20P.%20Henry%2C%20N.%20Latent%20structure%20analysis%201968"
        },
        {
            "id": "30",
            "entry": "[30] L. A. Goodman. \u201cExploratory latent structure analysis using both identifiable and unidentifiable models\u201d. In: Biometrika 61.2 (1974), pp. 215\u2013231.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodman%2C%20L.A.%20Exploratory%20latent%20structure%20analysis%20using%20both%20identifiable%20and%20unidentifiable%20models%201974"
        },
        {
            "id": "31",
            "entry": "[31] A. Rahimi and B. Recht. \u201cRandom features for large-scale kernel machines\u201d. In: Advances in Neural Information Processing Systems. 2007, pp. 1177\u20131184.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "32",
            "entry": "[32] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. \u201cA limited memory algorithm for bound constrained optimization\u201d. In: SIAM Journal on Scientific Computing 16.5 (1995), pp. 1190\u20131208.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20R.H.%20Lu%2C%20P.%20Nocedal%2C%20J.%20Zhu%2C%20C.%20A%20limited%20memory%20algorithm%20for%20bound%20constrained%20optimization%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20R.H.%20Lu%2C%20P.%20Nocedal%2C%20J.%20Zhu%2C%20C.%20A%20limited%20memory%20algorithm%20for%20bound%20constrained%20optimization%201995"
        },
        {
            "id": "33",
            "entry": "[33] M. Abadi et al. \u201cTensorFlow: A System for Large-Scale Machine Learning.\u201d In: OSDI. Vol. 16. 2016, pp. 265\u2013283.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20TensorFlow%3A%20A%20System%20for%20Large-Scale%20Machine%20Learning.%202016"
        },
        {
            "id": "34",
            "entry": "[34] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.M.%20Pattern%20Recognition%20and%20Machine%20Learning%202006"
        },
        {
            "id": "35",
            "entry": "[35] F. Nielsen and K. Sun. \u201cGuaranteed bounds on the Kullback-Leibler divergence of univariate mixtures using piecewise log-sum-exp inequalities\u201d. In: arXiv:1606.05850 (2016).",
            "arxiv_url": "https://arxiv.org/pdf/1606.05850"
        },
        {
            "id": "36",
            "entry": "[36] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "37",
            "entry": "[37] D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. \u201cEdward: A library for probabilistic modeling, inference, and criticism\u201d. In: arXiv preprint arXiv:1610.09787 (2016). ",
            "arxiv_url": "https://arxiv.org/pdf/1610.09787"
        }
    ]
}
