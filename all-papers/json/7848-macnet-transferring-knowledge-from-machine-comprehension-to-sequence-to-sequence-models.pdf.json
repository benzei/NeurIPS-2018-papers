{
    "filename": "7848-macnet-transferring-knowledge-from-machine-comprehension-to-sequence-to-sequence-models.pdf",
    "metadata": {
        "title": "MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models",
        "author": "Boyuan Pan, Yazheng Yang, Hao Li, Zhou Zhao, Yueting Zhuang, Deng Cai, Xiaofei He",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7848-macnet-transferring-knowledge-from-machine-comprehension-to-sequence-to-sequence-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Machine Comprehension (MC) is one of the core problems in natural language processing, requiring both understanding of the natural language and knowledge about the world. Rapid progress has been made since the release of several benchmark datasets, and recently the state-of-the-art models even surpass human performance on the well-known SQuAD evaluation. In this paper, we transfer knowledge learned from machine comprehension to the sequence-to-sequence tasks to deepen the understanding of the text. We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models. Experiments on neural machine translation (NMT) and abstractive text summarization show that our proposed framework can significantly improve the performance of the baseline models, and our method for the abstractive text summarization achieves the state-of-the-art results on the Gigaword dataset."
    },
    "keywords": [
        {
            "term": "ICLR",
            "url": "https://en.wikipedia.org/wiki/ICLR"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "NIPS",
            "url": "https://en.wikipedia.org/wiki/NIPS"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "text summarization",
            "url": "https://en.wikipedia.org/wiki/text_summarization"
        },
        {
            "term": "EMNLP",
            "url": "https://en.wikipedia.org/wiki/EMNLP"
        },
        {
            "term": "Recurrent Neural Network",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "Exact Match",
            "url": "https://en.wikipedia.org/wiki/Exact_Match"
        }
    ],
    "highlights": [
        "Machine comprehension (MC) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language understanding",
        "Machine comprehension requires to encode words from the passage and the question firstly, many methods [<a class=\"ref-link\" id=\"cSeo_et+al_2017_a\" href=\"#rSeo_et+al_2017_a\">Seo et al, 2017</a>; <a class=\"ref-link\" id=\"cWang_et+al_2016_a\" href=\"#rWang_et+al_2016_a\">Wang et al, 2016</a>; <a class=\"ref-link\" id=\"cXiong_et+al_2018_a\" href=\"#rXiong_et+al_2018_a\">Xiong et al, 2018</a>] employ attention mechanism with an Recurrent Neural Network-based modeling layer to capture the interaction among the passage words conditioned on the question and use an MLP classifier or pointer networks [<a class=\"ref-link\" id=\"cVinyals_et+al_2015_a\" href=\"#rVinyals_et+al_2015_a\">Vinyals et al, 2015</a>] to predict the answer span",
        "Similar to the neural machine translation task, the encoding layer contributes most of the improvement, while the modeling layer has stable gains in each evaluations",
        "We propose MacNet, which is a supplementary framework for the sequence-to-sequence tasks",
        "The experimental evaluation shows that our method significantly improves the performance of the baseline models on several benchmark datasets for different NLP tasks",
        "We hope this work can encourage further research into the transfer learning of multi-layer neural networks, and the future works involve the choice of other transfer learning sources and the transfer learning between different domains such as NLP, CV, etc"
    ],
    "key_statements": [
        "Machine comprehension (MC) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language understanding",
        "Machine comprehension requires to encode words from the passage and the question firstly, many methods [<a class=\"ref-link\" id=\"cSeo_et+al_2017_a\" href=\"#rSeo_et+al_2017_a\">Seo et al, 2017</a>; <a class=\"ref-link\" id=\"cWang_et+al_2016_a\" href=\"#rWang_et+al_2016_a\">Wang et al, 2016</a>; <a class=\"ref-link\" id=\"cXiong_et+al_2018_a\" href=\"#rXiong_et+al_2018_a\">Xiong et al, 2018</a>] employ attention mechanism with an Recurrent Neural Network-based modeling layer to capture the interaction among the passage words conditioned on the question and use an MLP classifier or pointer networks [<a class=\"ref-link\" id=\"cVinyals_et+al_2015_a\" href=\"#rVinyals_et+al_2015_a\">Vinyals et al, 2015</a>] to predict the answer span",
        "We propose MacNet, a machine comprehension augmented encoder-decoder supplementary architecture that can be applied to a variety of sequence generation tasks",
        "Unlike previous works that only focus on the encoding part or unsupervised knowledge source, we extract multiple layers of the neural networks from the machine comprehension model and insert them into the sequence-to-sequence model",
        "Similar to the neural machine translation task, the encoding layer contributes most of the improvement, while the modeling layer has stable gains in each evaluations",
        "We propose MacNet, which is a supplementary framework for the sequence-to-sequence tasks",
        "The experimental evaluation shows that our method significantly improves the performance of the baseline models on several benchmark datasets for different NLP tasks",
        "We hope this work can encourage further research into the transfer learning of multi-layer neural networks, and the future works involve the choice of other transfer learning sources and the transfer learning between different domains such as NLP, CV, etc"
    ],
    "summary": [
        "Machine comprehension (MC) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language understanding.",
        "Inspired by the recent success of the approaches for the machine comprehension tasks, we focus on exploring whether MC knowledge can further help the attention-based seq2seq models deeply comprehend the text.",
        "Machine comprehension requires to encode words from the passage and the question firstly, many methods [<a class=\"ref-link\" id=\"cSeo_et+al_2017_a\" href=\"#rSeo_et+al_2017_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2017_a\" href=\"#rSeo_et+al_2017_a\">Seo et al, 2017</a></a>; <a class=\"ref-link\" id=\"cWang_et+al_2016_a\" href=\"#rWang_et+al_2016_a\"><a class=\"ref-link\" id=\"cWang_et+al_2016_a\" href=\"#rWang_et+al_2016_a\">Wang et al, 2016</a></a>; <a class=\"ref-link\" id=\"cXiong_et+al_2018_a\" href=\"#rXiong_et+al_2018_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2018_a\" href=\"#rXiong_et+al_2018_a\">Xiong et al, 2018</a></a>] employ attention mechanism with an RNN-based modeling layer to capture the interaction among the passage words conditioned on the question and use an MLP classifier or pointer networks [<a class=\"ref-link\" id=\"cVinyals_et+al_2015_a\" href=\"#rVinyals_et+al_2015_a\">Vinyals et al, 2015</a>] to predict the answer span.",
        "We propose MacNet, a machine comprehension augmented encoder-decoder supplementary architecture that can be applied to a variety of sequence generation tasks.",
        "We transfer the knowledge from the machine comprehension model to the attention-based Neural Machine Translation (NMT) model.",
        "These models encode the input sequence as a set of vector representations using a recurrent neural network (RNN).",
        "McCann et al [2017] propose to transfer the pre-trained encoder from the neural machine translation (NMT) to the text classification and question answering tasks.",
        "Unlike previous works that only focus on the encoding part or unsupervised knowledge source, we extract multiple layers of the neural networks from the machine comprehension model and insert them into the sequence-to-sequence model.",
        "The state-of-the-art MC models are various in structures, but many popular works are essentially the combination of the encoding layer, the attention mechanism with and an RNN-based modeling layer and the output layer[<a class=\"ref-link\" id=\"cWang_et+al_2016_a\" href=\"#rWang_et+al_2016_a\"><a class=\"ref-link\" id=\"cWang_et+al_2016_a\" href=\"#rWang_et+al_2016_a\">Wang et al, 2016</a></a>; <a class=\"ref-link\" id=\"cSeo_et+al_2017_a\" href=\"#rSeo_et+al_2017_a\"><a class=\"ref-link\" id=\"cSeo_et+al_2017_a\" href=\"#rSeo_et+al_2017_a\">Seo et al, 2017</a></a>; <a class=\"ref-link\" id=\"cPan_et+al_2017_a\" href=\"#rPan_et+al_2017_a\">Pan et al, 2017</a>; <a class=\"ref-link\" id=\"cXiong_et+al_2018_a\" href=\"#rXiong_et+al_2018_a\"><a class=\"ref-link\" id=\"cXiong_et+al_2018_a\" href=\"#rXiong_et+al_2018_a\">Xiong et al, 2018</a></a>].",
        "In our MacNet, we send the attention vector at into the modeling layer of the pre-trained MC model in the equation (3) to deeply capture the interaction of the source and the target states: rt = Gmodel",
        "The modeling layer helps deeply understand the interaction of the contextual information of the output sequence, which is different from the encoding layer whose inputs are independent source sentences.",
        "We first evaluate our method on the neural machine translation (NMT) task, which requires to encode a source language sentence and predict a target language sentence.",
        "We use the Pointer-Generator Networks[See et al, 2017] as our baseline model, which applies the encoder-decoder architecture and is one of the state-of-the-art models for the text summarization.",
        "The experimental evaluation shows that our method significantly improves the performance of the baseline models on several benchmark datasets for different NLP tasks.",
        "We hope this work can encourage further research into the transfer learning of multi-layer neural networks, and the future works involve the choice of other transfer learning sources and the transfer learning between different domains such as NLP, CV, etc"
    ],
    "headline": "We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "Bahdanau, D.; Cho, K.; Bengio, Y.; et al. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate.%20In%20ICLR%202015"
        },
        {
            "id": "Chan_et+al_2016_a",
            "entry": "Chan, W.; Jaitly, N.; Le, Q.; and Vinyals, O. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%20W%20Jaitly%20N%20Le%20Q%20and%20Vinyals%20O%202016"
        },
        {
            "id": "Listen_2016_a",
            "entry": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, 4960\u20134964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Listen%20attend%20and%20spell%3A%20A%20neural%20network%20for%20large%20vocabulary%20conversational%20speech%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Listen%20attend%20and%20spell%3A%20A%20neural%20network%20for%20large%20vocabulary%20conversational%20speech%20recognition%202016"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In EMNLP, 1724\u20131734.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20K.%20van%20Merrienboer%2C%20B.%20Gulcehre%2C%20C.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20K.%20van%20Merrienboer%2C%20B.%20Gulcehre%2C%20C.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "Collobert_et+al_2011_a",
            "entry": "Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K.; and Kuksa, P. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12(Aug):2493\u20132537.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collobert%2C%20R.%20Weston%2C%20J.%20Bottou%2C%20L.%20Karlen%2C%20M.%20Natural%20language%20processing%20%28almost%29%20from%20scratch%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collobert%2C%20R.%20Weston%2C%20J.%20Bottou%2C%20L.%20Karlen%2C%20M.%20Natural%20language%20processing%20%28almost%29%20from%20scratch%202011"
        },
        {
            "id": "Cui_et+al_2017_a",
            "entry": "Cui, Y.; Chen, Z.; Wei, S.; Wang, S.; Liu, T.; and Hu, G. 2017. Attention-over-attention neural networks for reading comprehension. In ACL, volume 1, 593\u2013602.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cui%2C%20Y.%20Chen%2C%20Z.%20Wei%2C%20S.%20Wang%2C%20S.%20Attention-over-attention%20neural%20networks%20for%20reading%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cui%2C%20Y.%20Chen%2C%20Z.%20Wei%2C%20S.%20Wang%2C%20S.%20Attention-over-attention%20neural%20networks%20for%20reading%20comprehension%202017"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "Duchi, J.; Hazan, E.; Singer, Y.; et al. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12(Jul):2121\u20132159.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin, Y. N. 2017. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03122"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "Glorot, X.; Bordes, A.; Bengio, Y.; et al. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, 513\u2013520.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Domain%20adaptation%20for%20large-scale%20sentiment%20classification%3A%20A%20deep%20learning%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Domain%20adaptation%20for%20large-scale%20sentiment%20classification%3A%20A%20deep%20learning%20approach%202011"
        },
        {
            "id": "Guo_et+al_2018_a",
            "entry": "Guo, H.; Pasunuru, R.; and Bansal, M. 2018. Soft layer-specific multi-task summarization with entailment and question generation. arXiv preprint arXiv:1805.11004.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11004"
        },
        {
            "id": "Hermann_et+al_2015_a",
            "entry": "Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to read and comprehend. In NIPS, 1693\u20131701.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hermann%20K%20M%20Kocisky%20T%20Grefenstette%20E%20Espeholt%20L%20Kay%20W%20Suleyman%20M%20and%20Blunsom%20P%202015%20Teaching%20machines%20to%20read%20and%20comprehend%20In%20NIPS%2016931701",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hermann%20K%20M%20Kocisky%20T%20Grefenstette%20E%20Espeholt%20L%20Kay%20W%20Suleyman%20M%20and%20Blunsom%20P%202015%20Teaching%20machines%20to%20read%20and%20comprehend%20In%20NIPS%2016931701"
        },
        {
            "id": "Hu_et+al_2017_a",
            "entry": "Hu, M.; Peng, Y.; Qiu, X.; et al. 2017. Reinforced mnemonic reader for machine comprehension. CoRR, abs/1705.02798.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02798"
        },
        {
            "id": "Joshi_et+al_2017_a",
            "entry": "Joshi, M.; Choi, E.; Weld, D.; and Zettlemoyer, L. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joshi%2C%20M.%20Choi%2C%20E.%20Weld%2C%20D.%20Zettlemoyer%2C%20L.%20Triviaqa%3A%20A%20large%20scale%20distantly%20supervised%20challenge%20dataset%20for%20reading%20comprehension.%20In%20ACL%202017"
        },
        {
            "id": "Kim_2014_a",
            "entry": "Kim, Y. 2014. Convolutional neural networks for sentence classification. In EMNLP, 1746\u20131751.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Li, J.; Xiong, D.; Tu, Z.; Zhu, M.; Zhang, M.; and Zhou, G. 2017. Modeling source syntax for neural machine translation. In ACL, volume 1, 688\u2013697.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Xiong%2C%20D.%20Tu%2C%20Z.%20Zhu%2C%20M.%20Modeling%20source%20syntax%20for%20neural%20machine%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Xiong%2C%20D.%20Tu%2C%20Z.%20Zhu%2C%20M.%20Modeling%20source%20syntax%20for%20neural%20machine%20translation%202017"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Dollar, P. 2017. Focal loss for dense object detection. In ICCV, 2980\u20132988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20T.-Y.%20Goyal%2C%20P.%20Girshick%2C%20R.%20He%2C%20K.%20Focal%20loss%20for%20dense%20object%20detection.%20In%20ICCV%2C%202980%E2%80%932988%202017"
        },
        {
            "id": "Lin_2004_a",
            "entry": "Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20C.-Y.%20Rouge%3A%20A%20package%20for%20automatic%20evaluation%20of%20summaries.%20Text%20Summarization%20Branches%20Out%202004"
        },
        {
            "id": "Luong_et+al_2015_a",
            "entry": "Luong, T.; Pham, H.; Manning, C. D.; et al. 2015. Effective approaches to attention-based neural machine translation. In EMNLP, 1412\u20131421.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20T.%20Pham%2C%20H.%20Manning%2C%20C.D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation.%20In%20EMNLP%2C%201412%E2%80%931421%202015"
        },
        {
            "id": "Mccann_et+al_2017_a",
            "entry": "McCann, B.; Bradbury, J.; Xiong, C.; and Socher, R. 2017. Learned in translation: Contextualized word vectors. In NIPS, 6297\u20136308.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCann%20B%20Bradbury%20J%20Xiong%20C%20and%20Socher%20R%202017%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%2062976308",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCann%20B%20Bradbury%20J%20Xiong%20C%20and%20Socher%20R%202017%20Learned%20in%20translation%20Contextualized%20word%20vectors%20In%20NIPS%2062976308"
        },
        {
            "id": "Min_et+al_2017_a",
            "entry": "Min, S.; Seo, M.; and Hajishirzi, H. 2017. Question answering through transfer learning from large fine-grained supervision data. In ACL.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Min%2C%20S.%20Seo%2C%20M.%20Hajishirzi%2C%20H.%20Question%20answering%20through%20transfer%20learning%20from%20large%20fine-grained%20supervision%20data.%20In%20ACL%202017"
        },
        {
            "id": "Nallapati_et+al_2016_a",
            "entry": "Nallapati, R.; Zhou, B.; dos Santos, C.; Gulcehre, C.; and Xiang, B. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, 280\u2013290.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nallapati%2C%20R.%20Zhou%2C%20B.%20dos%20Santos%2C%20C.%20Gulcehre%2C%20C.%20Abstractive%20text%20summarization%20using%20sequence-to-sequence%20rnns%20and%20beyond%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nallapati%2C%20R.%20Zhou%2C%20B.%20dos%20Santos%2C%20C.%20Gulcehre%2C%20C.%20Abstractive%20text%20summarization%20using%20sequence-to-sequence%20rnns%20and%20beyond%202016"
        },
        {
            "id": "Nallapati_et+al_2017_a",
            "entry": "Nallapati, R.; Zhai, F.; Zhou, B.; et al. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In AAAI, 3075\u20133081.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nallapati%2C%20R.%20Zhai%2C%20F.%20Zhou%2C%20B.%20Summarunner%3A%20A%20recurrent%20neural%20network%20based%20sequence%20model%20for%20extractive%20summarization%20of%20documents.%20In%20AAAI%2C%203075%E2%80%933081%202017"
        },
        {
            "id": "Pan_et+al_2017_a",
            "entry": "Pan, B.; Li, H.; Zhao, Z.; Cao, B.; Cai, D.; and He, X. 2017. Memen: Multi-layer embedding with memory networks for machine comprehension. arXiv preprint arXiv:1707.09098.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09098"
        },
        {
            "id": "Pan_et+al_2018_a",
            "entry": "Pan, B.; Yang, Y.; Zhao, Z.; Zhuang, Y.; Cai, D.; and He, X. 2018. Discourse marker augmented network with reinforcement learning for natural language inference. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, 989\u2013999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20B.%20Yang%2C%20Y.%20Zhao%2C%20Z.%20Zhuang%2C%20Y.%20Discourse%20marker%20augmented%20network%20with%20reinforcement%20learning%20for%20natural%20language%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20B.%20Yang%2C%20Y.%20Zhao%2C%20Z.%20Zhuang%2C%20Y.%20Discourse%20marker%20augmented%20network%20with%20reinforcement%20learning%20for%20natural%20language%20inference%202018"
        },
        {
            "id": "Papineni_et+al_2002_a",
            "entry": "Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL, 311\u2013318. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.-J.%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.-J.%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "Paulus_2017_a",
            "entry": "Paulus, R.; , C.; Socher, R.; et al. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.",
            "arxiv_url": "https://arxiv.org/pdf/1705.04304"
        },
        {
            "id": "Pennington_et+al_2014_a",
            "entry": "Pennington, J.; Socher, R.; Manning, C. D.; et al. 2014. Glove: Global vectors for word representation. In EMNLP, 1532\u20131543.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "Rajpurkar_et+al_2016_a",
            "entry": "Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. Squad: 100,000+ questions for machine comprehension of text. In EMNLP, 2383\u20132392.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajpurkar%2C%20P.%20Zhang%2C%20J.%20Lopyrev%2C%20K.%20Liang%2C%20P.%20Squad%3A%20100%2C000%2B%20questions%20for%20machine%20comprehension%20of%20text.%20In%20EMNLP%2C%202383%E2%80%932392%202016"
        },
        {
            "id": "Rush_et+al_2015_a",
            "entry": "Rush, A. M.; Chopra, S.; Weston, J.; et al. 2015. A neural attention model for abstractive sentence summarization. In EMNLP, 379\u2013389.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rush%2C%20A.M.%20Chopra%2C%20S.%20Weston%2C%20J.%20A%20neural%20attention%20model%20for%20abstractive%20sentence%20summarization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rush%2C%20A.M.%20Chopra%2C%20S.%20Weston%2C%20J.%20A%20neural%20attention%20model%20for%20abstractive%20sentence%20summarization%202015"
        },
        {
            "id": "See_et+al_2017_a",
            "entry": "See, A.; Liu, P. J.; Manning, C. D.; et al. 2017. Get to the point: Summarization with pointer-generator networks. In ACL, volume 1, 1073\u20131083.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=See%2C%20A.%20Liu%2C%20P.J.%20Manning%2C%20C.D.%20Get%20to%20the%20point%3A%20Summarization%20with%20pointer-generator%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=See%2C%20A.%20Liu%2C%20P.J.%20Manning%2C%20C.D.%20Get%20to%20the%20point%3A%20Summarization%20with%20pointer-generator%20networks%202017"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "Sennrich, R.; Haddow, B.; Birch, A.; et al. 2016. Neural machine translation of rare words with subword units. In ACL.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20R.%20Haddow%2C%20B.%20Birch%2C%20A.%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units.%20In%20ACL%202016"
        },
        {
            "id": "Seo_et+al_2017_a",
            "entry": "Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2017. Bidirectional attention flow for machine comprehension. In ICLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seo%2C%20M.%20Kembhavi%2C%20A.%20Farhadi%2C%20A.%20Hajishirzi%2C%20H.%20Bidirectional%20attention%20flow%20for%20machine%20comprehension.%20In%20ICLR%202017"
        },
        {
            "id": "Shen_et+al_2017_a",
            "entry": "Shen, Y.; Huang, P.-S.; Gao, J.; and Chen, W. 2017. Reasonet: Learning to stop reading in machine comprehension. In KDD, 1047\u20131055. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Y.%20Huang%2C%20P.-S.%20Gao%2C%20J.%20Chen%2C%20W.%20Reasonet%3A%20Learning%20to%20stop%20reading%20in%20machine%20comprehension%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Y.%20Huang%2C%20P.-S.%20Gao%2C%20J.%20Chen%2C%20W.%20Reasonet%3A%20Learning%20to%20stop%20reading%20in%20machine%20comprehension%202017"
        },
        {
            "id": "Shi_et+al_2016_a",
            "entry": "Shi, X.; Padhi, I.; Knight, K.; et al. 2016. Does string-based neural mt learn source syntax? In ACL, 1526\u20131534.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20X.%20Padhi%2C%20I.%20Knight%2C%20K.%20Does%20string-based%20neural%20mt%20learn%20source%20syntax%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20X.%20Padhi%2C%20I.%20Knight%2C%20K.%20Does%20string-based%20neural%20mt%20learn%20source%20syntax%3F%202016"
        },
        {
            "id": "Sutskever_et+al_2014_a",
            "entry": "Sutskever, I.; Vinyals, O.; Le, Q. V.; et al. 2014. Sequence to sequence learning with neural networks. In NIPS, 3104\u20133112.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20I.%20Vinyals%2C%20O.%20Le%2C%20Q.V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks.%20In%20NIPS%2C%203104%E2%80%933112%202014"
        },
        {
            "id": "Vinyals_et+al_2015_a",
            "entry": "Vinyals, O.; Fortunato, M.; Jaitly, N.; et al. 2015. Pointer networks. In NIPS, 2692\u20132700.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%20O%20Fortunato%20M%20Jaitly%20N%20et%20al%202015%20Pointer%20networks%20In%20NIPS%2026922700",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%20O%20Fortunato%20M%20Jaitly%20N%20et%20al%202015%20Pointer%20networks%20In%20NIPS%2026922700"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Wang, Z.; Mi, H.; Hamza, W.; and Florian, R. 2016. Multi-perspective context matching for machine comprehension. arXiv preprint arXiv:1612.04211.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04211"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Wang, W.; Yang, N.; Wei, F.; Chang, B.; and Zhou, M. 2017. Gated self-matching networks for reading comprehension and question answering. In ACL. Weissenborn, D.; Wiese, G.; Seiffe, L.; et al. 2017. Making neural qa as simple as possible but not simpler. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20W.%20Yang%2C%20N.%20Wei%2C%20F.%20Chang%2C%20B.%20Gated%20self-matching%20networks%20for%20reading%20comprehension%20and%20question%20answering%202017"
        },
        {
            "id": "Williams_et+al_2017_a",
            "entry": "Williams, J. D.; Asadi, K.; Zweig, G.; et al. 2017. Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. In ACL, volume 1, 665\u2013677.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20J.D.%20Asadi%2C%20K.%20Zweig%2C%20G.%20Hybrid%20code%20networks%3A%20practical%20and%20efficient%20end-to-end%20dialog%20control%20with%20supervised%20and%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20J.D.%20Asadi%2C%20K.%20Zweig%2C%20G.%20Hybrid%20code%20networks%3A%20practical%20and%20efficient%20end-to-end%20dialog%20control%20with%20supervised%20and%20reinforcement%20learning%202017"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; Klingner, J.; Shah, A.; Johnson, M.; Liu, X.; \u0141ukasz Kaiser; Gouws, S.; Kato, Y.; Kudo, T.; Kazawa, H.; Stevens, K.; Kurian, G.; Patil, N.; Wang, W.; Young, C.; Smith, J.; Riesa, J.; Rudnick, A.; Vinyals, O.; Corrado, G.; Hughes, M.; and Dean, J. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR abs/1609.08144.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "Xia_et+al_2017_a",
            "entry": "Xia, Y.; Tian, F.; Wu, L.; Lin, J.; Qin, T.; Yu, N.; and Liu, T.-Y. 2017. Deliberation networks: Sequence generation beyond one-pass decoding. In NIPS, 1782\u20131792.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xia%20Y%20Tian%20F%20Wu%20L%20Lin%20J%20Qin%20T%20Yu%20N%20and%20Liu%20TY%202017%20Deliberation%20networks%20Sequence%20generation%20beyond%20onepass%20decoding%20In%20NIPS%2017821792"
        },
        {
            "id": "Xiong_et+al_2017_a",
            "entry": "Xiong, C.; Zhong, V.; Socher, R.; et al. 2017. Dynamic coattention networks for question answering. ICLR 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiong%2C%20C.%20Zhong%2C%20V.%20Socher%2C%20R.%20Dynamic%20coattention%20networks%20for%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiong%2C%20C.%20Zhong%2C%20V.%20Socher%2C%20R.%20Dynamic%20coattention%20networks%20for%20question%20answering%202017"
        },
        {
            "id": "Xiong_et+al_2018_a",
            "entry": "Xiong, C.; Zhong, V.; Socher, R.; et al. 2018. DCN+: Mixed objective and deep residual coattention for question answering. In ICLR. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; and Bengio, Y. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiong%2C%20C.%20Zhong%2C%20V.%20Socher%2C%20R.%20DCN%2B%3A%20Mixed%20objective%20and%20deep%20residual%20coattention%20for%20question%20answering%202018"
        },
        {
            "id": "Zeiler_2012_a",
            "entry": "Zeiler, M. D. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        },
        {
            "id": "Zhou_et+al_2017_a",
            "entry": "Zhou, Q.; Yang, N.; Wei, F.; and Zhou, M. 2017. Selective encoding for abstractive sentence summarization. In ACL, 1095\u20131104.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Q.%20Yang%2C%20N.%20Wei%2C%20F.%20Zhou%2C%20M.%20Selective%20encoding%20for%20abstractive%20sentence%20summarization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Q.%20Yang%2C%20N.%20Wei%2C%20F.%20Zhou%2C%20M.%20Selective%20encoding%20for%20abstractive%20sentence%20summarization%202017"
        }
    ]
}
