{
    "filename": "7520-active-learning-for-non-parametric-regression-using-purely-random-trees.pdf",
    "metadata": {
        "title": "Active Learning for Non-Parametric Regression Using Purely Random Trees",
        "author": "Jack Goetz, Ambuj Tewari, Paul Zimmerman",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7520-active-learning-for-non-parametric-regression-using-purely-random-trees.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Active learning is the task of using labelled data to select additional points to label, with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods were previously needed to obtain theoretically superior performance. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free, consistent and minimax optimal for Lipschitz functions."
    },
    "keywords": [
        {
            "term": "computational chemistry",
            "url": "https://en.wikipedia.org/wiki/computational_chemistry"
        },
        {
            "term": "chemical reaction",
            "url": "https://en.wikipedia.org/wiki/chemical_reaction"
        },
        {
            "term": "data point",
            "url": "https://en.wikipedia.org/wiki/data_point"
        },
        {
            "term": "activation energy",
            "url": "https://en.wikipedia.org/wiki/activation_energy"
        }
    ],
    "highlights": [
        "In this paper we study active learning for regression in the pool setting",
        "Our primary motivation comes from computational chemistry, where chemical properties of interest can be computed by solving approximations to the Schr\u00f6dinger equation",
        "One key property to chemists, the rate of chemical reaction, can be quantified via the activation energy, which controls the rate of reaction as a function of temperature [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "We examine performance on the Wine quality data set from UCI and a data set of activation energies of Claisen rearrangement reactions (Cl)",
        "One direction would be extending theory to ensembles of trees, or developing tools to deal with high dimensions. Another possibility is to exploit the online nature of Mondrian Trees to develop a parallel theory for streaming based active learning"
    ],
    "key_statements": [
        "In this paper we study active learning for regression in the pool setting",
        "Our primary motivation comes from computational chemistry, where chemical properties of interest can be computed by solving approximations to the Schr\u00f6dinger equation",
        "One key property to chemists, the rate of chemical reaction, can be quantified via the activation energy, which controls the rate of reaction as a function of temperature [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "We examine performance on the Wine quality data set from UCI and a data set of activation energies of Claisen rearrangement reactions (Cl)",
        "One direction would be extending theory to ensembles of trees, or developing tools to deal with high dimensions. Another possibility is to exploit the online nature of Mondrian Trees to develop a parallel theory for streaming based active learning"
    ],
    "summary": [
        "In this paper we study active learning for regression in the pool setting.",
        "Second we propose an active learning scheme where we first sample passively to estimate the required statistics, and use those estimates to approximate the oracle algorithm.",
        "We begin by describing the pool based active learning setting, as well as introducing purely random and Mondrian trees.",
        "We first describe a simple family of querying algorithms for a fixed purely random tree I which are not active.",
        "Algorithm 1: Generic \"oracle\" querying algorithm Input: Leaves of our tree I, pool of data points {Xi}m i=1, label budget n and joint distribution pX,Y Output: The set of points to label foreach Ik \u2208 I do",
        "For a fixed tree structure I, under any sampling distribution generated by Algorithm 1 we have the following bias-variance decomposition of our risk: E \u2212 f (X))2 = E \u2212 f (X))2 + E \u2212 fI(X))2 .",
        "For a fixed tree structure I, under any sampling distribution generated by Algorithm 1 we have that the variance error term on the leaf Ik is: E \u2212 fI (X))2|X \u2208 Ik",
        "For a fixed tree structure I, the risk from any randomized version of Algorithm 1 is greater than the risk from sampling according to pX\u2217 unless P (n\u22171, ..., n\u2217K ) = 1.",
        "Given fixed leaf errors a1, ..., aK we can calculate the additional risk generated by using \u03c3Y2,k in our optimal algorithm instead of the true \u03c3Y2,k",
        "Mondrian Trees trained using random sampling are minimax optimal for Lipschitz regression functions when the sequence of lifetime parameters satisfy \u03bbn n1/(d+2) and Var(Y ) < \u221e [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>].",
        "Mondrian Trees with random sampling are weakly universally consistent under the same lifetime sequence and variance assumption.",
        "We analyze the consequences of using these estimates, and show that in the case when Y are normal, our trees are Mondrian Trees, and our Stage 1 samples in each leaf, our active algorithm is eventually near optimal with high probability.",
        "More formally let n(1),k be the number of samples from leaf k during Stage 1, and let M(1),n = min(n(1),1, ..., n(1),K ),n implicitly depends on both the tree structure and version of algorithm 1 used in Stage 1).",
        "We compare the performance of selecting points to label using random sampling, our active algorithm, and a naive uncertainty sampling version of our active algorithm, where each leaf nk is proportional its variance.",
        "It may be possible to extend the ideas here to non tree based active learning for regression"
    ],
    "headline": "In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Attenberg, J. and Provost, F. (2011). Inactive learning?: difficulties employing active learning in practice. ACM SIGKDD Explorations Newsletter, 12(2):36\u201341.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Attenberg%2C%20J.%20Provost%2C%20F.%20Inactive%20learning%3F%3A%20difficulties%20employing%20active%20learning%20in%20practice%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Attenberg%2C%20J.%20Provost%2C%20F.%20Inactive%20learning%3F%3A%20difficulties%20employing%20active%20learning%20in%20practice%202011"
        },
        {
            "id": "2",
            "entry": "[2] Awasthi, P., Balcan, M. F., and Long, P. M. (2014). The power of localization for efficiently learning linear separators with noise. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 449\u2013458. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Awasthi%2C%20P.%20Balcan%2C%20M.F.%20Long%2C%20P.M.%20The%20power%20of%20localization%20for%20efficiently%20learning%20linear%20separators%20with%20noise%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Awasthi%2C%20P.%20Balcan%2C%20M.F.%20Long%2C%20P.M.%20The%20power%20of%20localization%20for%20efficiently%20learning%20linear%20separators%20with%20noise%202014"
        },
        {
            "id": "3",
            "entry": "[3] Balcan, M.-F., Beygelzimer, A., and Langford, J. (2009). Agnostic active learning. Journal of Computer and System Sciences, 75(1):78\u201389.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.-F.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.-F.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202009"
        },
        {
            "id": "4",
            "entry": "[4] Breiman, L. (2000). Some infinity theory for predictor ensembles. Technical report, Technical Report 579, Statistics Dept. UCB.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20L.%20Some%20infinity%20theory%20for%20predictor%20ensembles%202000"
        },
        {
            "id": "5",
            "entry": "[5] Breiman, L. (2017). Classification and regression trees. Routledge.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20L.%20Classification%20and%20regression%20trees%202017"
        },
        {
            "id": "6",
            "entry": "[6] Bull, A. D. (2013). Spatially-adaptive sensing in nonparametric regression. The Annals of Statistics, 41(1):41\u201362.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bull%2C%20A.D.%20Spatially-adaptive%20sensing%20in%20nonparametric%20regression%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bull%2C%20A.D.%20Spatially-adaptive%20sensing%20in%20nonparametric%20regression%202013"
        },
        {
            "id": "7",
            "entry": "[7] Chaudhuri, K., Jain, P., and Natarajan, N. (2017). Active heteroscedastic regression. In International Conference on Machine Learning, pages 694\u2013702.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20K.%20Jain%2C%20P.%20Natarajan%2C%20N.%20Active%20heteroscedastic%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20K.%20Jain%2C%20P.%20Natarajan%2C%20N.%20Active%20heteroscedastic%20regression%202017"
        },
        {
            "id": "8",
            "entry": "[8] Chaudhuri, K., Kakade, S. M., Netrapalli, P., and Sanghavi, S. (2015). Convergence rates of active learning for maximum likelihood estimation. In Advances in Neural Information Processing Systems, pages 1090\u20131098.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20K.%20Kakade%2C%20S.M.%20Netrapalli%2C%20P.%20Sanghavi%2C%20S.%20Convergence%20rates%20of%20active%20learning%20for%20maximum%20likelihood%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20K.%20Kakade%2C%20S.M.%20Netrapalli%2C%20P.%20Sanghavi%2C%20S.%20Convergence%20rates%20of%20active%20learning%20for%20maximum%20likelihood%20estimation%202015"
        },
        {
            "id": "9",
            "entry": "[9] Cramer, C. J. (2013). Essentials of computational chemistry: theories and models. John Wiley & Sons.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cramer%2C%20C.J.%20Essentials%20of%20computational%20chemistry%3A%20theories%20and%20models%202013"
        },
        {
            "id": "10",
            "entry": "[10] Dasgupta, S., Hsu, D. J., and Monteleoni, C. (2008). A general agnostic active learning algorithm. In Advances in neural information processing systems, pages 353\u2013360.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20S.%20Hsu%2C%20D.J.%20Monteleoni%2C%20C.%20A%20general%20agnostic%20active%20learning%20algorithm%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20S.%20Hsu%2C%20D.J.%20Monteleoni%2C%20C.%20A%20general%20agnostic%20active%20learning%20algorithm%202008"
        },
        {
            "id": "11",
            "entry": "[11] Efromovich, S. (2008). Optimal sequential design in a controlled non-parametric regression. Scandinavian Journal of Statistics, 35(2):266\u2013285.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Efromovich%2C%20S.%20Optimal%20sequential%20design%20in%20a%20controlled%20non-parametric%20regression%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Efromovich%2C%20S.%20Optimal%20sequential%20design%20in%20a%20controlled%20non-parametric%20regression%202008"
        },
        {
            "id": "12",
            "entry": "[12] Genuer, R. (2012). Variance reduction in purely random forests. Journal of Nonparametric Statistics, 24(3):543\u2013562.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genuer%2C%20R.%20Variance%20reduction%20in%20purely%20random%20forests%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genuer%2C%20R.%20Variance%20reduction%20in%20purely%20random%20forests%202012"
        },
        {
            "id": "13",
            "entry": "[13] Golovin, D. and Krause, A. (2011). Adaptive submodularity: Theory and applications in active learning and stochastic optimization. Journal of Artificial Intelligence Research, 42:427\u2013486.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golovin%2C%20D.%20Krause%2C%20A.%20Adaptive%20submodularity%3A%20Theory%20and%20applications%20in%20active%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golovin%2C%20D.%20Krause%2C%20A.%20Adaptive%20submodularity%3A%20Theory%20and%20applications%20in%20active%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "14",
            "entry": "[14] Gy\u00f6rfi, L., Kohler, M., Krzyzak, A., and Walk, H. (2006). A distribution-free theory of nonparametric regression. Springer Science & Business Media.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gy%C3%B6rfi%2C%20L.%20Kohler%2C%20M.%20Krzyzak%2C%20A.%20Walk%2C%20H.%20A%20distribution-free%20theory%20of%20nonparametric%20regression%202006"
        },
        {
            "id": "15",
            "entry": "[15] Hanneke, S. and Yang, L. (2015). Minimax analysis of active learning. Journal of Machine Learning Research, 16(12):3487\u20133602.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanneke%2C%20S.%20Yang%2C%20L.%20Minimax%20analysis%20of%20active%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanneke%2C%20S.%20Yang%2C%20L.%20Minimax%20analysis%20of%20active%20learning%202015"
        },
        {
            "id": "16",
            "entry": "[16] Hoang, T. N., Low, B. K. H., Jaillet, P., and Kankanhalli, M. (2014). Nonmyopic -bayesoptimal active learning of gaussian processes. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 739\u2013747.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoang%2C%20T.N.%20Low%2C%20B.K.H.%20Jaillet%2C%20P.%20Kankanhalli%2C%20M.%20Nonmyopic%20-bayesoptimal%20active%20learning%20of%20gaussian%20processes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoang%2C%20T.N.%20Low%2C%20B.K.H.%20Jaillet%2C%20P.%20Kankanhalli%2C%20M.%20Nonmyopic%20-bayesoptimal%20active%20learning%20of%20gaussian%20processes%202014"
        },
        {
            "id": "17",
            "entry": "[17] Lakshminarayanan, B., Roy, D. M., and Teh, Y. W. (2014). Mondrian forests: Efficient online random forests. In Advances in neural information processing systems, pages 3140\u20133148.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20B.%20Roy%2C%20D.M.%20Teh%2C%20Y.W.%20Mondrian%20forests%3A%20Efficient%20online%20random%20forests%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshminarayanan%2C%20B.%20Roy%2C%20D.M.%20Teh%2C%20Y.W.%20Mondrian%20forests%3A%20Efficient%20online%20random%20forests%202014"
        },
        {
            "id": "18",
            "entry": "[18] Liu, H., Ong, Y.-S., and Cai, J. (2017). A survey of adaptive sampling for global metamodeling in support of simulation-based complex engineering design. Structural and Multidisciplinary Optimization, pages 1\u201324.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20H.%20Ong%2C%20Y.-S.%20Cai%2C%20J.%20A%20survey%20of%20adaptive%20sampling%20for%20global%20metamodeling%20in%20support%20of%20simulation-based%20complex%20engineering%20design%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20H.%20Ong%2C%20Y.-S.%20Cai%2C%20J.%20A%20survey%20of%20adaptive%20sampling%20for%20global%20metamodeling%20in%20support%20of%20simulation-based%20complex%20engineering%20design%202017"
        },
        {
            "id": "19",
            "entry": "[19] Mourtada, J., Ga\u00efffas, S., and Scornet, E. (2017). Universal consistency and minimax rates for online mondrian forests. In Advances in Neural Information Processing Systems, pages 3761\u2013 3770.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mourtada%2C%20J.%20Ga%C3%AFffas%2C%20S.%20Scornet%2C%20E.%20Universal%20consistency%20and%20minimax%20rates%20for%20online%20mondrian%20forests%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mourtada%2C%20J.%20Ga%C3%AFffas%2C%20S.%20Scornet%2C%20E.%20Universal%20consistency%20and%20minimax%20rates%20for%20online%20mondrian%20forests%202017"
        },
        {
            "id": "20",
            "entry": "[20] Mourtada, J., Ga\u00efffas, S., and Scornet, E. (2018). Minimax optimal rates for mondrian trees and forests. arXiv preprint arXiv:1803.05784.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05784"
        },
        {
            "id": "21",
            "entry": "[21] Sabato, S. and Munos, R. (2014). Active regression by stratification. In Advances in Neural Information Processing Systems, pages 469\u2013477.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sabato%2C%20S.%20Munos%2C%20R.%20Active%20regression%20by%20stratification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sabato%2C%20S.%20Munos%2C%20R.%20Active%20regression%20by%20stratification%202014"
        },
        {
            "id": "22",
            "entry": "[22] Sourati, J., Akcakaya, M., Leen, T. K., Erdogmus, D., and Dy, J. G. (2017). Asymptotic analysis of objectives based on fisher information in active learning. Journal of Machine Learning Research, 18(34):1\u201341.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sourati%2C%20J.%20Akcakaya%2C%20M.%20Leen%2C%20T.K.%20Erdogmus%2C%20D.%20Asymptotic%20analysis%20of%20objectives%20based%20on%20fisher%20information%20in%20active%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sourati%2C%20J.%20Akcakaya%2C%20M.%20Leen%2C%20T.K.%20Erdogmus%2C%20D.%20Asymptotic%20analysis%20of%20objectives%20based%20on%20fisher%20information%20in%20active%20learning%202017"
        },
        {
            "id": "23",
            "entry": "[23] Willett, R., Nowak, R., and Castro, R. M. (2006). Faster rates in regression via active learning. In Advances in Neural Information Processing Systems, pages 179\u2013186. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Willett%2C%20R.%20Nowak%2C%20R.%20Castro%2C%20R.M.%20Faster%20rates%20in%20regression%20via%20active%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Willett%2C%20R.%20Nowak%2C%20R.%20Castro%2C%20R.M.%20Faster%20rates%20in%20regression%20via%20active%20learning%202006"
        }
    ]
}
