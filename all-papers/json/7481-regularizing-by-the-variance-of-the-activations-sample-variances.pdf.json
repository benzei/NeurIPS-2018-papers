{
    "filename": "7481-regularizing-by-the-variance-of-the-activations-sample-variances.pdf",
    "metadata": {
        "title": "Regularizing by the Variance of the Activations' Sample-Variances",
        "author": "Etai Littwin, Lior Wolf",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7481-regularizing-by-the-variance-of-the-activations-sample-variances.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Normalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations, we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. As we prove, this encourages the activations to be distributed around a few distinct modes. We also show that if the inputs are from a mixture of two Gaussians, the new loss would either join the two together, or separate between them optimally in the LDA sense, depending on the prior probabilities. Finally, we are able to link the new regularization term to the batchnorm method, which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        }
    ],
    "highlights": [
        "We propose a novel regularization technique that is applied before the activation of all neurons in the neural network",
        "The new regularization term encourages the distribution of the individual activations to have a few distinct modes",
        "We provide a theoretical link between the variance-based regularization term and the resulting peaked activation distributions, which we observe experimentally, see Fig. 1",
        "We provide experimental evidence that the new term leads to improved accuracy and can replace, during training, normalization techniques such as the batch-norm technique",
        "Since variance constancy loss is a regularization term and not a normalization mechanism, and since the statistics of sample moments is well understood, the new method could be compatible with a wider variety of optimization methods in comparison to bachnorm"
    ],
    "key_statements": [
        "We propose a novel regularization technique that is applied before the activation of all neurons in the neural network",
        "The new regularization term encourages the distribution of the individual activations to have a few distinct modes",
        "We provide a theoretical link between the variance-based regularization term and the resulting peaked activation distributions, which we observe experimentally, see Fig. 1",
        "We provide experimental evidence that the new term leads to improved accuracy and can replace, during training, normalization techniques such as the batch-norm technique",
        "In order to avoid searching over a wide range of hyper-parameters, we optimize for this term during training and allow each neuron to adapt to a different level of variance.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montreal, Canada.\n2 The Variance Constancy Loss",
        "We demonstrate the effectiveness of variance constancy loss regularization on several benchmark datasets, comparing with competitive baselines",
        "The gap between variance constancy loss and batchnorm is larger for SELU and the lowest for ReLU, which is consistent with the results in Tab. 2.\n4 Related Work",
        "Since variance constancy loss is a regularization term and not a normalization mechanism, and since the statistics of sample moments is well understood, the new method could be compatible with a wider variety of optimization methods in comparison to bachnorm",
        "The first is the observation that while variance constancy loss shows good results with the ReLU activations on the UCI experiences, in image experiments the combination of the two underperforms when compared to ReLU with batchnorm"
    ],
    "summary": [
        "We propose a novel regularization technique that is applied before the activation of all neurons in the neural network.",
        "The new regularization term encourages the distribution of the individual activations to have a few distinct modes.",
        "We provide a theoretical link between the variance-based regularization term and the resulting peaked activation distributions, which we observe experimentally, see Fig. 1.",
        "This regularization can be seen as an additional unsupervised clustering loss per unit, since it is minimized by clustering its input to two modes.",
        "For a Gaussian distribution with mean \u03bc and variance \u03c32, the non-centered fourth and second moments are given by: m4 = \u03bc4 + 6\u03bc2\u03c32 + 3\u03c34, m2 = \u03c32 + \u03bc2 (8)",
        "Sample statistics of each mini-batch are calculated, and used for normalization of the activations.",
        "The gap between VCL and batchnorm is larger for SELU and the lowest for ReLU, which is consistent with the results in Tab. 2.",
        "As a loss-based method, does not employ batch statistics at test time.",
        "We perform our experiments with only a few samples for the VCL loss.",
        "Considering one of the modes as a baseline activation, our work can be viewed as related to sparsity regularization methods, including L1 regularization [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] and its local or selective application [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] and structural sparsification methods [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] that modify the architecture by pruning some of the connections.",
        "Our method is related to variational methods such as the variational autoencoder [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], which employs a regularization term that enforces a certain distribution on some of the activations.",
        "In contrast to such work, our method employs the regularization term everywhere, the multi-modal structure is soft, and the number of modes is not enforced (Thm. 2, and the fact that multi-peak distributions with more than 2 peaks have low Kurtosis).",
        "VCL employs small subsets of the mini-batch and seems to perform as well or better than batchnorm on the standard benchmarks tested.",
        "Since VCL is a regularization term and not a normalization mechanism, and since the statistics of sample moments is well understood, the new method could be compatible with a wider variety of optimization methods in comparison to bachnorm.",
        "Compared to other loss terms, VCL shapes the activation distribution in one of several phases, according to the input statistics.",
        "The first is the observation that while VCL shows good results with the ReLU activations on the UCI experiences, in image experiments the combination of the two underperforms when compared to ReLU with batchnorm."
    ],
    "headline": "This encourages the activations to be distributed around a few distinct modes",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "2",
            "entry": "[2] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "3",
            "entry": "[3] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "4",
            "entry": "[4] Donald Estep, Axel Malqvist, and Simon Tavener. Error estimation and adaptive computation for elliptic problems with randomly perturbed data. 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Estep%2C%20Donald%20Malqvist%2C%20Axel%20Tavener%2C%20Simon%20Error%20estimation%20and%20adaptive%20computation%20for%20elliptic%20problems%20with%20randomly%20perturbed%20data%202006"
        },
        {
            "id": "5",
            "entry": "[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "6",
            "entry": "[6] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: Train 1, get M for free. arXiv preprint arXiv:1704.00109, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00109"
        },
        {
            "id": "7",
            "entry": "[7] Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batchnormalized models. In Advances in Neural Information Processing Systems, pages 1942\u20131950, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Batch%20renormalization%3A%20Towards%20reducing%20minibatch%20dependence%20in%20batchnormalized%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Batch%20renormalization%3A%20Towards%20reducing%20minibatch%20dependence%20in%20batchnormalized%20models%202017"
        },
        {
            "id": "8",
            "entry": "[8] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "9",
            "entry": "[9] Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146\u20132153. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jarrett%2C%20Kevin%20Kavukcuoglu%2C%20Koray%20LeCun%2C%20Yann%20What%20is%20the%20best%20multi-stage%20architecture%20for%20object%20recognition%3F%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jarrett%2C%20Kevin%20Kavukcuoglu%2C%20Koray%20LeCun%2C%20Yann%20What%20is%20the%20best%20multi-stage%20architecture%20for%20object%20recognition%3F%202009"
        },
        {
            "id": "10",
            "entry": "[10] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "11",
            "entry": "[11] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Selfnormalizing neural networks. In Advances in Neural Information Processing Systems, pages 972\u2013981, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klambauer%2C%20Gunter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Selfnormalizing%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klambauer%2C%20Gunter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Selfnormalizing%20neural%20networks%202017"
        },
        {
            "id": "12",
            "entry": "[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "13",
            "entry": "[13] Siwei Lyu and Eero P Simoncelli. Nonlinear image representation using divisive normalization. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lyu%2C%20Siwei%20Simoncelli%2C%20Eero%20P.%20Nonlinear%20image%20representation%20using%20divisive%20normalization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lyu%2C%20Siwei%20Simoncelli%2C%20Eero%20P.%20Nonlinear%20image%20representation%20using%20divisive%20normalization%202008"
        },
        {
            "id": "14",
            "entry": "[14] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, pages 506\u2013516, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20Sylvestre-Alvise%20Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Learning%20multiple%20visual%20domains%20with%20residual%20adapters%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20Sylvestre-Alvise%20Bilen%2C%20Hakan%20Vedaldi%2C%20Andrea%20Learning%20multiple%20visual%20domains%20with%20residual%20adapters%202017"
        },
        {
            "id": "15",
            "entry": "[15] Jason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.02200"
        },
        {
            "id": "16",
            "entry": "[16] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large scale visual recognition challenge. CoRR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202014"
        },
        {
            "id": "17",
            "entry": "[17] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "18",
            "entry": "[18] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901\u2013909, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "19",
            "entry": "[19] Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regularization for deep neural networks. Neurocomputing, 241:81\u201389, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scardapane%2C%20Simone%20Comminiello%2C%20Danilo%20Hussain%2C%20Amir%20Uncini%2C%20Aurelio%20Group%20sparse%20regularization%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scardapane%2C%20Simone%20Comminiello%2C%20Danilo%20Hussain%2C%20Amir%20Uncini%2C%20Aurelio%20Group%20sparse%20regularization%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "20",
            "entry": "[20] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "21",
            "entry": "[21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "22",
            "entry": "[22] D Ulyanov, A Vedaldi, and V Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "23",
            "entry": "[23] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pages 2074\u2013 2082, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "24",
            "entry": "[24] Yuxin Wu and Kaiming He. Group normalization. arXiv preprint arXiv:1803.08494, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08494"
        },
        {
            "id": "25",
            "entry": "[25] Jaehong Yoon and Sung Ju Hwang. Combined group and exclusive sparsity for deep neural networks. In International Conference on Machine Learning, pages 3958\u20133966, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yoon%2C%20Jaehong%20Hwang%2C%20Sung%20Ju%20Combined%20group%20and%20exclusive%20sparsity%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yoon%2C%20Jaehong%20Hwang%2C%20Sung%20Ju%20Combined%20group%20and%20exclusive%20sparsity%20for%20deep%20neural%20networks%202017"
        }
    ]
}
