{
    "filename": "8052-constructing-unrestricted-adversarial-examples-with-generative-models.pdf",
    "metadata": {
        "title": "Constructing Unrestricted Adversarial Examples with Generative Models",
        "author": "Yang Song, Rui Shu, Nate Kushman, Stefano Ermon",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8052-constructing-unrestricted-adversarial-examples-with-generative-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose a new class of adversarial examples that are synthesized entirely from scratch using a conditional generative model, without being restricted to norm-bounded perturbations. We first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that these new kind of adversarial images, which we call Generative Adversarial Examples, are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that generative adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "Mechanical Turk",
            "url": "https://en.wikipedia.org/wiki/Mechanical_Turk"
        },
        {
            "term": "data point",
            "url": "https://en.wikipedia.org/wiki/data_point"
        },
        {
            "term": "matrix norm",
            "url": "https://en.wikipedia.org/wiki/matrix_norm"
        }
    ],
    "highlights": [
        "We review recent work on adversarial examples, defense methods, and conditional generative models",
        "We explore two attacks derived from variants of Auxiliary Classifier Generative Adversarial Network",
        "The results indicate that the transferability of generative adversarial examples can be generally enhanced with noise-augmentation",
        "In contrast to our generative adversarial examples where images are generated from scratch, these attacking methods craft malicious inputs based on a given test dataset using a limited set of image manipulations",
        "This new kind of adversarial examples undermines current defenses, which are designed for perturbation-based attacks",
        "Generative adversarial examples are able to transfer to other classifiers trained using the same dataset"
    ],
    "key_statements": [
        "We review recent work on adversarial examples, defense methods, and conditional generative models",
        "We introduce a more general attack mechanism, where adversarial examples are generated entirely from scratch instead of perturbing an existing data point by a small amount",
        "Adversarial examples can be crafted for many domains, we focus on image classification tasks in the rest of this paper, and will use the words \u201cexamples\u201d and \u201cimages\u201d interchangeably",
        "We explore two attacks derived from variants of Auxiliary Classifier Generative Adversarial Network",
        "In order to produce generative adversarial examples, we propose finding the appropriate z by minimizing a loss function L that is carefully designed to produce high quality generative adversarial examples",
        "For some of our experiments, we want to investigate whether generative adversarial examples are more similar to existing images in the dataset, compared to perturbation-based attacks",
        "In Fig. 3 and Fig. 4, we show samples and detailed success rates of generative adversarial attacks without noise-augmentation",
        "Setup To test how our generative adversarial examples transfer to other architectures, we use all of the generative adversarial examples we created to target Madry Network [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] on MNIST for the results in Section 4.3, and filter out invalid ones using the majority vote of a set of human annotators",
        "Results We show in Tab. 3 that generative adversarial examples exhibit moderate transferability to different classifiers, which means they can be threatening in a black-box scenario as well",
        "The results indicate that the transferability of generative adversarial examples can be generally enhanced with noise-augmentation",
        "We provide a theoretical analysis in Appendix A under relatively strong assumptions to argue that most generative adversarial examples produced by our method should be legitimate.\n6 Related work",
        "In contrast to our generative adversarial examples where images are generated from scratch, these attacking methods craft malicious inputs based on a given test dataset using a limited set of image manipulations",
        "Similar to what we have shown for traditional adversarial examples, we can view these attacking methods as special instances of our generative adversarial attack framework by choosing a suitable generative model",
        "As with our generative adversarial examples, fooling images are not restricted to small norm-bounded perturbations",
        "We explore a new threat model and propose a more general form of adversarial attacks",
        "This new kind of adversarial examples undermines current defenses, which are designed for perturbation-based attacks",
        "Generative adversarial examples are able to transfer to other classifiers trained using the same dataset"
    ],
    "summary": [
        "We review recent work on adversarial examples, defense methods, and conditional generative models.",
        "We train an Auxiliary Classifier Generative Adversarial Network (AC-GAN [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]) to model the set of legitimate images for each class.",
        "One interesting observation is that traditional perturbation-based adversarial examples can be obtained as a special case of our noise-augmented attack, by choosing a suitable g\u2713(z, y) instead of the AC-GAN generator.",
        "For some of our experiments, we want to investigate whether generative adversarial examples are more similar to existing images in the dataset, compared to perturbation-based attacks.",
        "These defenses can provide a theoretically verified certificate that a training example cannot be classified incorrectly by any perterbation-based attack with a perturbation size less than a given \u270f.",
        "Setup For each source class, we use our method to produce 1000 untargeted generative adversarial examples without noise-augmentation.",
        "Results We found that with perturbation-based examples [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], annotators can correctly identify adversarial images with a 92.9% success rate.",
        "Setup We produce 100 generative adversarial examples for each pair of source and target classes and ask human annotators to label them.",
        "Our generative adversarial examples can successfully fool this defense with more than an 84% success rate on all datasets.",
        "In Fig. 3 and Fig. 4, we show samples and detailed success rates of generative adversarial attacks without noise-augmentation.",
        "Setup To test how our generative adversarial examples transfer to other architectures, we use all of the generative adversarial examples we created to target Madry Network [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] on MNIST for the results in Section 4.3, and filter out invalid ones using the majority vote of a set of human annotators.",
        "Suppose g(z, y) 2 Rn is an ideal generative model that can always produce legitimate images of class end-to-end score function y 2 {0, 1} for s(g(z, y)) can any z 2 Rm, and assume for be approximated by aslg)(\u21e1z0w, yg|)z)",
        "In contrast to our generative adversarial examples where images are generated from scratch, these attacking methods craft malicious inputs based on a given test dataset using a limited set of image manipulations.",
        "Fooling images consist of noise or patterns that do not necessarily look realistic but are predicted to be in one of the known classes with high confidence.",
        "As with our generative adversarial examples, fooling images are not restricted to small norm-bounded perturbations.",
        "This new kind of adversarial examples undermines current defenses, which are designed for perturbation-based attacks.",
        "A contest [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] has recently been launched on unrestricted adversarial examples"
    ],
    "headline": "We propose a new class of adversarial examples that are synthesized entirely from scratch using a conditional generative model, without being restricted to norm-bounded perturbations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "2",
            "entry": "[2] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "3",
            "entry": "[3] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "4",
            "entry": "[4] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust Physical-World Attacks on Deep Learning Visual Classification. In Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eykholt%2C%20Kevin%20Evtimov%2C%20Ivan%20Fernandes%2C%20Earlence%20Li%2C%20Bo%20Robust%20Physical-World%20Attacks%20on%20Deep%20Learning%20Visual%20Classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eykholt%2C%20Kevin%20Evtimov%2C%20Ivan%20Fernandes%2C%20Earlence%20Li%2C%20Bo%20Robust%20Physical-World%20Attacks%20on%20Deep%20Learning%20Visual%20Classification%202018"
        },
        {
            "id": "5",
            "entry": "[5] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In International Conference on Computer Vision. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Zhou%2C%20Yuyin%20Adversarial%20examples%20for%20semantic%20segmentation%20and%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Zhou%2C%20Yuyin%20Adversarial%20examples%20for%20semantic%20segmentation%20and%20object%20detection%202017"
        },
        {
            "id": "6",
            "entry": "[6] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. Hidden voice commands. In USENIX Security Symposium, pages 513\u2013530, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%202016"
        },
        {
            "id": "7",
            "entry": "[7] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and Wenyuan Xu. Dolphinattack: Inaudible voice commands. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 103\u2013117. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Guoming%20Yan%2C%20Chen%20Ji%2C%20Xiaoyu%20Zhang%2C%20Tianchen%20Dolphinattack%3A%20Inaudible%20voice%20commands%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Guoming%20Yan%2C%20Chen%20Ji%2C%20Xiaoyu%20Zhang%2C%20Tianchen%20Dolphinattack%3A%20Inaudible%20voice%20commands%202017"
        },
        {
            "id": "8",
            "entry": "[8] Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Fooling deep structured prediction models. arXiv preprint arXiv:1707.05373, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.05373"
        },
        {
            "id": "9",
            "entry": "[9] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01236"
        },
        {
            "id": "10",
            "entry": "[10] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "11",
            "entry": "[11] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10571"
        },
        {
            "id": "12",
            "entry": "[12] Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.5068"
        },
        {
            "id": "13",
            "entry": "[13] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Yang%20Kim%2C%20Taesup%20Nowozin%2C%20Sebastian%20Ermon%2C%20Stefano%20Pixeldefend%3A%20Leveraging%20generative%20models%20to%20understand%20and%20defend%20against%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Yang%20Kim%2C%20Taesup%20Nowozin%2C%20Sebastian%20Ermon%2C%20Stefano%20Pixeldefend%3A%20Leveraging%20generative%20models%20to%20understand%20and%20defend%20against%20adversarial%20examples%202018"
        },
        {
            "id": "14",
            "entry": "[14] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samangouei%2C%20Pouya%20Kabkab%2C%20Maya%20Chellappa%2C%20Rama%20Defense-gan%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%202018"
        },
        {
            "id": "15",
            "entry": "[15] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pages 854\u2013863, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017"
        },
        {
            "id": "16",
            "entry": "[16] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghunathan%2C%20Aditi%20Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Certified%20defenses%20against%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghunathan%2C%20Aditi%20Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Certified%20defenses%20against%20adversarial%20examples%202018"
        },
        {
            "id": "17",
            "entry": "[17] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "18",
            "entry": "[18] Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09665"
        },
        {
            "id": "19",
            "entry": "[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "20",
            "entry": "[20] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. In International Conference on Machine Learning, pages 2642\u20132651, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20gans%202017"
        },
        {
            "id": "21",
            "entry": "[21] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "22",
            "entry": "[22] Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon\u2019s mechanical turk: A new source of inexpensive, yet high-quality, data? Perspectives on psychological science, 6(1):3\u20135, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buhrmester%2C%20Michael%20Kwang%2C%20Tracy%20Gosling%2C%20Samuel%20D.%20Amazon%E2%80%99s%20mechanical%20turk%3A%20A%20new%20source%20of%20inexpensive%2C%20yet%20high-quality%2C%20data%3F%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buhrmester%2C%20Michael%20Kwang%2C%20Tracy%20Gosling%2C%20Samuel%20D.%20Amazon%E2%80%99s%20mechanical%20turk%3A%20A%20new%20source%20of%20inexpensive%2C%20yet%20high-quality%2C%20data%3F%202011"
        },
        {
            "id": "23",
            "entry": "[23] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541\u2013551, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Boser%2C%20Bernhard%20Denker%2C%20John%20S.%20Henderson%2C%20Donnie%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Boser%2C%20Bernhard%20Denker%2C%20John%20S.%20Henderson%2C%20Donnie%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201989"
        },
        {
            "id": "24",
            "entry": "[24] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "25",
            "entry": "[25] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "26",
            "entry": "[26] Jorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computation, 35(151):773\u2013782, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nocedal%2C%20Jorge%20Updating%20quasi-newton%20matrices%20with%20limited%20storage%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nocedal%2C%20Jorge%20Updating%20quasi-newton%20matrices%20with%20limited%20storage%201980"
        },
        {
            "id": "27",
            "entry": "[27] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20Seyed%20Mohsen%20Moosavi%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "28",
            "entry": "[28] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pages 372\u2013387. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "29",
            "entry": "[29] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39\u201357. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "30",
            "entry": "[30] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yanpei%20Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Yanpei%20Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%202017"
        },
        {
            "id": "31",
            "entry": "[31] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "32",
            "entry": "[32] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "33",
            "entry": "[33] Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Combining maximum likelihood and adversarial learning in generative models. In AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grover%2C%20Aditya%20Dhar%2C%20Manik%20Ermon%2C%20Stefano%20Flow-gan%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grover%2C%20Aditya%20Dhar%2C%20Manik%20Ermon%2C%20Stefano%20Flow-gan%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018"
        },
        {
            "id": "34",
            "entry": "[34] Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. Multi-agent generative adversarial imitation learning. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Jiaming%20Ren%2C%20Hongyu%20Sadigh%2C%20Dorsa%20Ermon%2C%20Stefano%20Multi-agent%20generative%20adversarial%20imitation%20learning%202018"
        },
        {
            "id": "35",
            "entry": "[35] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "36",
            "entry": "[36] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "37",
            "entry": "[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "38",
            "entry": "[38] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Mnist adversarial examples challenge, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Mnist%20adversarial%20examples%20challenge%202017"
        },
        {
            "id": "39",
            "entry": "[39] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "40",
            "entry": "[40] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 1528\u20131540. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "41",
            "entry": "[41] Alhussein Fawzi and Pascal Frossard. Measuring the effect of nuisance variables on classifiers. In British Machine Vision Conference (BMVC), number EPFL-CONF-220613, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Measuring%20the%20effect%20of%20nuisance%20variables%20on%20classifiers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Measuring%20the%20effect%20of%20nuisance%20variables%20on%20classifiers%202016"
        },
        {
            "id": "42",
            "entry": "[42] Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. arXiv preprint arXiv:1804.00499, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00499"
        },
        {
            "id": "43",
            "entry": "[43] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Zhengli%20Dua%2C%20Dheeru%20Singh%2C%20Sameer%20Generating%20natural%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Zhengli%20Dua%2C%20Dheeru%20Singh%2C%20Sameer%20Generating%20natural%20adversarial%20examples%202018"
        },
        {
            "id": "44",
            "entry": "[44] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427\u2013436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015"
        },
        {
            "id": "45",
            "entry": "[45] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02610"
        },
        {
            "id": "46",
            "entry": "[46] Hyrum S Anderson, Jonathan Woodbridge, and Bobby Filar. Deepdga: Adversarially-tuned domain generation and detection. In Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security, pages 13\u201321. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Hyrum%20S.%20Woodbridge%2C%20Jonathan%20Filar%2C%20Bobby%20Deepdga%3A%20Adversarially-tuned%20domain%20generation%20and%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Hyrum%20S.%20Woodbridge%2C%20Jonathan%20Filar%2C%20Bobby%20Deepdga%3A%20Adversarially-tuned%20domain%20generation%20and%20detection%202016"
        },
        {
            "id": "47",
            "entry": "[47] Shumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adversarial examples. arXiv preprint arXiv:1703.09387, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09387"
        },
        {
            "id": "48",
            "entry": "[48] Hyeungill Lee, Sungyeob Han, and Jungwoo Lee. Generative adversarial trainer: Defense to adversarial perturbations with gan. arXiv preprint arXiv:1705.03387, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03387"
        },
        {
            "id": "49",
            "entry": "[49] Tom B Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, and Ian Goodfellow. Unrestricted adversarial examples. arXiv preprint arXiv:1809.08352, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.08352"
        },
        {
            "id": "50",
            "entry": "[50] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227\u2013244, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shimodaira%2C%20Hidetoshi%20Improving%20predictive%20inference%20under%20covariate%20shift%20by%20weighting%20the%20log-likelihood%20function%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shimodaira%2C%20Hidetoshi%20Improving%20predictive%20inference%20under%20covariate%20shift%20by%20weighting%20the%20log-likelihood%20function%202000"
        },
        {
            "id": "51",
            "entry": "[51] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20The%20nature%20of%20statistical%20learning%20theory%202013"
        },
        {
            "id": "52",
            "entry": "[52] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T approach to unsupervised domain adaptation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shu%2C%20Rui%20Bui%2C%20Hung%20H.%20Narui%2C%20Hirokazu%20Ermon%2C%20Stefano%20A%20DIRT-T%20approach%20to%20unsupervised%20domain%20adaptation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shu%2C%20Rui%20Bui%2C%20Hung%20H.%20Narui%2C%20Hirokazu%20Ermon%2C%20Stefano%20A%20DIRT-T%20approach%20to%20unsupervised%20domain%20adaptation%202018"
        },
        {
            "id": "53",
            "entry": "[53] Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tao%2C%20Terence%20Topics%20in%20random%20matrix%20theory%2C%20volume%20132%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tao%2C%20Terence%20Topics%20in%20random%20matrix%20theory%2C%20volume%20132%202012"
        },
        {
            "id": "54",
            "entry": "[54] Phillippe Rigollet. High-dimensional statistics. Lecture notes for course 18S997, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20Phillippe%20High-dimensional%20statistics.%20Lecture%20notes%20for%20course%2018S997%202015"
        }
    ]
}
