{
    "filename": "7428-hybrid-knowledge-routed-modules-for-large-scale-object-detection.pdf",
    "metadata": {
        "title": "Hybrid Knowledge Routed Modules for Large-scale Object Detection",
        "author": "ChenHan Jiang, Hang Xu, Xiaodan Liang, Liang Lin",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7428-hybrid-knowledge-routed-modules-for-large-scale-object-detection.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The dominant object detection approaches treat the recognition of each region separately and overlook crucial semantic correlations between objects in one scene. This paradigm leads to substantial performance drop when facing heavy long-tail problems, where very few samples are available for rare classes and plenty of confusing categories exists. We exploit diverse human commonsense knowledge for reasoning over large-scale object categories and reaching semantic coherency within one image. Particularly, we present Hybrid Knowledge Routed Modules (HKRM) that incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge (e.g. shared attributes, relationships) about concepts; and an implicit knowledge module that depicts some implicit constraints (e.g. common spatial layouts). By functioning over a region-to-region graph, both modules can be individualized and adapted to coordinate with visual patterns in each image, guided by specific knowledge forms. HKRM are light-weight, general-purpose and extensible by easily incorporating multiple knowledge to endow any detection networks the ability of global semantic reasoning. Experiments on large-scale object detection benchmarks show HKRM obtains around 34.5% improvement on VisualGenome (1000 categories) and 30.4% on ADE in terms of mAP. Codes and trained model can be found in https://github.com/chanyn/HKRM."
    },
    "keywords": [
        {
            "term": "long tail",
            "url": "https://en.wikipedia.org/wiki/long_tail"
        },
        {
            "term": "mean Average Precision",
            "url": "https://en.wikipedia.org/wiki/mean_Average_Precision"
        },
        {
            "term": "Natural Science Foundation of China",
            "url": "https://en.wikipedia.org/wiki/Natural_Science_Foundation_of_China"
        },
        {
            "term": "object detection",
            "url": "https://en.wikipedia.org/wiki/object_detection"
        },
        {
            "term": "substantial performance",
            "url": "https://en.wikipedia.org/wiki/substantial_performance"
        }
    ],
    "highlights": [
        "The most state-of-the-art object detection methods [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] follow the region-based paradigm, which treats the classification and boundingbox regression of each region proposal separately",
        "The long-tail problem is very common, where very few samples exist for rare classes, such as pepperoni and bagel",
        "We propose Hybrid Knowledge Routed Modules (HKRM) to incorporate multiple semantic reasoning routed by two major kinds of knowledge forms: an explicit knowledge module that exploits structure constraints that are summarized with linguistic knowledge, and an implicit knowledge module to encode some implicit commonsense constraints over object",
        "We present two novel general knowledge modules in Hybrid Knowledge Routed Modules",
        "The second one can implicitly learn some knowledge without explicit definitions or being summarized by human",
        "The experiment and analysis indicated Hybrid Knowledge Routed Modules can alleviate the problems of large-scale object detection"
    ],
    "key_statements": [
        "The most state-of-the-art object detection methods [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] follow the region-based paradigm, which treats the classification and boundingbox regression of each region proposal separately",
        "The long-tail problem is very common, where very few samples exist for rare classes, such as pepperoni and bagel",
        "We propose Hybrid Knowledge Routed Modules (HKRM) to incorporate multiple semantic reasoning routed by two major kinds of knowledge forms: an explicit knowledge module that exploits structure constraints that are summarized with linguistic knowledge, and an implicit knowledge module to encode some implicit commonsense constraints over object",
        "Similar to region-to-region graph used in explicit knowledge module, we learn specific edge weights {e} of each graph Gm, m = 1, . . . , M for all-region proposal pairs, following Eqn 1",
        "To compare with the state-of-art knowledge-enhanced methods, we implement Hybrid Knowledge Routed Modules on PASCAL VOC and MS COCO datasets with only 20/80 categories in Table 2",
        "We present two novel general knowledge modules in Hybrid Knowledge Routed Modules",
        "The second one can implicitly learn some knowledge without explicit definitions or being summarized by human",
        "The experiment and analysis indicated Hybrid Knowledge Routed Modules can alleviate the problems of large-scale object detection"
    ],
    "summary": [
        "The most state-of-the-art object detection methods [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] follow the region-based paradigm, which treats the classification and boundingbox regression of each region proposal separately.",
        "Given diverse object appearances and correlations in each image, personalized edge connections with respect to each knowledge form should be adaptive for different regions.",
        "Instead of building category-to-category graph [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], each knowledge module in HKRM learns adaptive context connections for each pair of regions by regarding a specific prior knowledge graph as external supervisions, rather than fixing the connections.",
        "Our HKRM enables sharing visual features among certain regions with similar attributes, pairwise relationship or spatial relationship.",
        "Implicit knowledge module region region edge share feature dog path woman pants",
        "The goal of this paper is to develop general modules for incorporating knowledge to facilitate largescale object detection with global reasoning.",
        "As shown in Figure 3, this module updates edge connections between each pair of region graph nodes in G , supervised by a mapping of the ground truth from a class-to-class knowledge graph Q.",
        "Similar to region-to-region graph used in explicit knowledge module, we learn specific edge weights {e} of each graph Gm, m = 1, .",
        "In terms of our explicit attribute and relationship knowledge module upon region proposals, we use the final conv5 for 128 regions after avg-pool (D= 2048) as our module inputs.",
        "To compare with the state-of-art knowledge-enhanced methods, we implement HKRM on PASCAL VOC and MS COCO datasets with only 20/80 categories in Table 2.",
        "For PASCAL VOC, our HKRM performs 1.1% better than the baseline Faster RCNN, and outperforms Spatial Memory Network [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "Sharing visual feature according to both attribute and relationship knowledge can boost the performance of object detection.",
        "The proposed HKRM achieves the global reasoning over regions via one-time propagation over all graph edges and nodes.",
        "To better understand the underlying feature representations that our HKRM learn for graph reasoning, we record the output fa and g (Ea = Eg = 512) from the explicit attribute module and implicit spatial module and its corresponding real labels from each region of 8000 VG1000 images.",
        "This speaks well our explicit knowledge module successfully incorporates the prior attribute knowledge and leads to interpretable feature learning.",
        "We can add experiments using the word embedding knowledge in the explicit module and the latest new Open Images Dataset which consists about 600 categories."
    ],
    "headline": "We present Hybrid Knowledge Routed Modules  that incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge  about concepts; and an implicit knowledge module that depicts some implicit constraints ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-embedding for attribute-based classification. In CVPR, 2013. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akata%2C%20Z.%20Perronnin%2C%20F.%20Harchaoui%2C%20Z.%20Schmid%2C%20C.%20Label-embedding%20for%20attribute-based%20classification.%20In%20CVPR%202013"
        },
        {
            "id": "2",
            "entry": "[2] J. Almaz\u00e1n, A. Gordo, A. Forn\u00e9s, and E. Valveny. Word spotting and recognition with embedded attributes. IEEE transactions on pattern analysis and machine intelligence, 36(12):2552\u20132566, 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J.%20Almaz%C3%A1n%2C%20A.%20Gordo%2C%20A.%20Forn%C3%A9s%20Valveny%2C%20E.%20Word%20spotting%20and%20recognition%20with%20embedded%20attributes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J.%20Almaz%C3%A1n%2C%20A.%20Gordo%2C%20A.%20Forn%C3%A9s%20Valveny%2C%20E.%20Word%20spotting%20and%20recognition%20with%20embedded%20attributes%202014"
        },
        {
            "id": "3",
            "entry": "[3] I. Biederman, R. J. Mezzanotte, and J. C. Rabinowitz. Scene perception: Detecting and judging objects undergoing relational violations. Cognitive psychology, 14(2):143\u2013177, 1982. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biederman%2C%20I.%20Mezzanotte%2C%20R.J.%20Rabinowitz%2C%20J.C.%20Scene%20perception%3A%20Detecting%20and%20judging%20objects%20undergoing%20relational%20violations%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biederman%2C%20I.%20Mezzanotte%2C%20R.J.%20Rabinowitz%2C%20J.C.%20Scene%20perception%3A%20Detecting%20and%20judging%20objects%20undergoing%20relational%20violations%201982"
        },
        {
            "id": "4",
            "entry": "[4] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In CVPR, 2018. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Z.%20Vasconcelos%2C%20N.%20Cascade%20r-cnn%3A%20Delving%20into%20high%20quality%20object%20detection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Z.%20Vasconcelos%2C%20N.%20Cascade%20r-cnn%3A%20Delving%20into%20high%20quality%20object%20detection%202018"
        },
        {
            "id": "5",
            "entry": "[5] X. Chen and A. Gupta. Spatial memory for context reasoning in object detection. In ICCV, 2017. 2, 3, 7, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Gupta%2C%20A.%20Spatial%20memory%20for%20context%20reasoning%20in%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Gupta%2C%20A.%20Spatial%20memory%20for%20context%20reasoning%20in%20object%20detection%202017"
        },
        {
            "id": "6",
            "entry": "[6] X. Chen, L.-J. Li, L. Fei-Fei, and A. Gupta. Iterative visual reasoning beyond convolutions. In CVPR, 2018. 2, 3, 5, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Li%2C%20L.-J.%20Fei-Fei%2C%20L.%20Gupta%2C%20A.%20Iterative%20visual%20reasoning%20beyond%20convolutions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Li%2C%20L.-J.%20Fei-Fei%2C%20L.%20Gupta%2C%20A.%20Iterative%20visual%20reasoning%20beyond%20convolutions%202018"
        },
        {
            "id": "7",
            "entry": "[7] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships with deep relational networks. In CVPR, 2017. 2, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20B.%20Zhang%2C%20Y.%20Lin%2C%20D.%20Detecting%20visual%20relationships%20with%20deep%20relational%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20B.%20Zhang%2C%20Y.%20Lin%2C%20D.%20Detecting%20visual%20relationships%20with%20deep%20relational%20networks%202017"
        },
        {
            "id": "8",
            "entry": "[8] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. In NIPS, 2016. 1, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20J.%20Li%2C%20Y.%20He%2C%20K.%20Sun%2C%20J.%20R-fcn%3A%20Object%20detection%20via%20region-based%20fully%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20J.%20Li%2C%20Y.%20He%2C%20K.%20Sun%2C%20J.%20R-fcn%3A%20Object%20detection%20via%20region-based%20fully%20convolutional%20networks%202016"
        },
        {
            "id": "9",
            "entry": "[9] J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy, S. Bengio, Y. Li, H. Neven, and H. Adam. Large-scale object classification using label relation graphs. In ECCV, 2014. 2, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Ding%2C%20N.%20Jia%2C%20Y.%20Frome%2C%20A.%20Large-scale%20object%20classification%20using%20label%20relation%20graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Ding%2C%20N.%20Jia%2C%20Y.%20Frome%2C%20A.%20Large-scale%20object%20classification%20using%20label%20relation%20graphs%202014"
        },
        {
            "id": "10",
            "entry": "[10] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303\u2013338, June 2010. 3, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Everingham%2C%20M.%20Gool%2C%20L.Van%20Williams%2C%20C.K.I.%20Winn%2C%20J.%20The%20pascal%20visual%20object%20classes%20%28voc%29%20challenge%202010-06-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Everingham%2C%20M.%20Gool%2C%20L.Van%20Williams%2C%20C.K.I.%20Winn%2C%20J.%20The%20pascal%20visual%20object%20classes%20%28voc%29%20challenge%202010-06-03"
        },
        {
            "id": "11",
            "entry": "[11] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In CVPR, 2009. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farhadi%2C%20A.%20Endres%2C%20I.%20Hoiem%2C%20D.%20Forsyth%2C%20D.%20Describing%20objects%20by%20their%20attributes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farhadi%2C%20A.%20Endres%2C%20I.%20Hoiem%2C%20D.%20Forsyth%2C%20D.%20Describing%20objects%20by%20their%20attributes%202009"
        },
        {
            "id": "12",
            "entry": "[12] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9):1627\u20131645, 2010. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Felzenszwalb%2C%20P.F.%20Girshick%2C%20R.B.%20McAllester%2C%20D.%20Ramanan%2C%20D.%20Object%20detection%20with%20discriminatively%20trained%20part-based%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Felzenszwalb%2C%20P.F.%20Girshick%2C%20R.B.%20McAllester%2C%20D.%20Ramanan%2C%20D.%20Object%20detection%20with%20discriminatively%20trained%20part-based%20models%202010"
        },
        {
            "id": "13",
            "entry": "[13] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep visual-semantic embedding model. In NIPS, 2013. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frome%2C%20A.%20Corrado%2C%20G.S.%20Shlens%2C%20J.%20Bengio%2C%20S.%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model.%20In%20NIPS%202013"
        },
        {
            "id": "14",
            "entry": "[14] C. Galleguillos, A. Rabinovich, and S. Belongie. Object categorization using co-occurrence, location and appearance. In CVPR, 2008. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Galleguillos%2C%20C.%20Rabinovich%2C%20A.%20Belongie%2C%20S.%20Object%20categorization%20using%20co-occurrence%2C%20location%20and%20appearance%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Galleguillos%2C%20C.%20Rabinovich%2C%20A.%20Belongie%2C%20S.%20Object%20categorization%20using%20co-occurrence%2C%20location%20and%20appearance%202008"
        },
        {
            "id": "15",
            "entry": "[15] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In ICLR, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garcia%2C%20V.%20Bruna%2C%20J.%20Few-shot%20learning%20with%20graph%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garcia%2C%20V.%20Bruna%2C%20J.%20Few-shot%20learning%20with%20graph%20neural%20networks%202018"
        },
        {
            "id": "16",
            "entry": "[16] S. Gould, T. Gao, and D. Koller. Region-based segmentation and object detection. In Advances in Neural Information Processing Systems 22, 2009. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gould%2C%20S.%20Gao%2C%20T.%20Koller%2C%20D.%20Region-based%20segmentation%20and%20object%20detection%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gould%2C%20S.%20Gao%2C%20T.%20Koller%2C%20D.%20Region-based%20segmentation%20and%20object%20detection%202009"
        },
        {
            "id": "17",
            "entry": "[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 3, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "18",
            "entry": "[18] J. Hoffman, S. Guadarrama, E. S. Tzeng, R. Hu, J. Donahue, R. Girshick, T. Darrell, and K. Saenko. Lsda: Large scale detection through adaptation. In NIPS, 2014. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20J.%20Guadarrama%2C%20S.%20Tzeng%2C%20E.S.%20Hu%2C%20R.%20Lsda%3A%20Large%20scale%20detection%20through%20adaptation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20J.%20Guadarrama%2C%20S.%20Tzeng%2C%20E.S.%20Hu%2C%20R.%20Lsda%3A%20Large%20scale%20detection%20through%20adaptation%202014"
        },
        {
            "id": "19",
            "entry": "[19] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks for object detection. In CVPR, 2018. 2, 5, 7, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20H.%20Gu%2C%20J.%20Zhang%2C%20Z.%20Dai%2C%20J.%20Relation%20networks%20for%20object%20detection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20H.%20Gu%2C%20J.%20Zhang%2C%20Z.%20Dai%2C%20J.%20Relation%20networks%20for%20object%20detection%202018"
        },
        {
            "id": "20",
            "entry": "[20] R. Hu, P. Doll\u00e1r, K. He, T. Darrell, and R. Girshick. Learning to segment every thing. In CVPR, 2018. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Learning%20to%20segment%20every%20thing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Learning%20to%20segment%20every%20thing%202018"
        },
        {
            "id": "21",
            "entry": "[21] D. Jayaraman and K. Grauman. Zero-shot recognition with unreliable attributes. In NIPS, 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jayaraman%2C%20D.%20Grauman%2C%20K.%20Zero-shot%20recognition%20with%20unreliable%20attributes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jayaraman%2C%20D.%20Grauman%2C%20K.%20Zero-shot%20recognition%20with%20unreliable%20attributes%202014"
        },
        {
            "id": "22",
            "entry": "[22] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017. 2, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20T.N.%20Welling%2C%20M.%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20T.N.%20Welling%2C%20M.%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "23",
            "entry": "[23] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 2016. 1, 2, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishna%2C%20R.%20Zhu%2C%20Y.%20Groth%2C%20O.%20Johnson%2C%20J.%20Visual%20genome%3A%20Connecting%20language%20and%20vision%20using%20crowdsourced%20dense%20image%20annotations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishna%2C%20R.%20Zhu%2C%20Y.%20Groth%2C%20O.%20Johnson%2C%20J.%20Visual%20genome%3A%20Connecting%20language%20and%20vision%20using%20crowdsourced%20dense%20image%20annotations%202016"
        },
        {
            "id": "24",
            "entry": "[24] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In CVPR, 2009. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampert%2C%20C.H.%20Nickisch%2C%20H.%20Harmeling%2C%20S.%20Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampert%2C%20C.H.%20Nickisch%2C%20H.%20Harmeling%2C%20S.%20Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer%202009"
        },
        {
            "id": "25",
            "entry": "[25] J. Li, Y. Wei, X. Liang, J. Dong, T. Xu, J. Feng, and S. Yan. Attentive contexts for object detection. IEEE Transactions on Multimedia, 19(5):944\u2013954, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Wei%2C%20Y.%20Liang%2C%20X.%20Dong%2C%20J.%20Attentive%20contexts%20for%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Wei%2C%20Y.%20Liang%2C%20X.%20Dong%2C%20J.%20Attentive%20contexts%20for%20object%20detection%202017"
        },
        {
            "id": "26",
            "entry": "[26] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In ICLR, 2016. 2, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Tarlow%2C%20D.%20Brockschmidt%2C%20M.%20Zemel%2C%20R.%20Gated%20graph%20sequence%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Tarlow%2C%20D.%20Brockschmidt%2C%20M.%20Zemel%2C%20R.%20Gated%20graph%20sequence%20neural%20networks%202016"
        },
        {
            "id": "27",
            "entry": "[27] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun. Light-head r-cnn: In defense of two-stage object detector. In CVPR, 2017. 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Z%20Li%20C%20Peng%20G%20Yu%20X%20Zhang%20Y%20Deng%20and%20J%20Sun%20Lighthead%20rcnn%20In%20defense%20of%20twostage%20object%20detector%20In%20CVPR%202017%206%207",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Z%20Li%20C%20Peng%20G%20Yu%20X%20Zhang%20Y%20Deng%20and%20J%20Sun%20Lighthead%20rcnn%20In%20defense%20of%20twostage%20object%20detector%20In%20CVPR%202017%206%207"
        },
        {
            "id": "28",
            "entry": "[28] T.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feature%20pyramid%20networks%20for%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feature%20pyramid%20networks%20for%20object%20detection%202017"
        },
        {
            "id": "29",
            "entry": "[29] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 3, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Doll%C3%A1r%20and%20C%20L%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%202014%203%206",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TY%20Lin%20M%20Maire%20S%20Belongie%20J%20Hays%20P%20Perona%20D%20Ramanan%20P%20Doll%C3%A1r%20and%20C%20L%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%202014%203%206"
        },
        {
            "id": "30",
            "entry": "[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=W%20Liu%20D%20Anguelov%20D%20Erhan%20C%20Szegedy%20S%20Reed%20CY%20Fu%20and%20A%20C%20Berg%20Ssd%20Single%20shot%20multibox%20detector%20In%20ECCV%202016%203",
            "oa_query": "https://api.scholarcy.com/oa_version?query=W%20Liu%20D%20Anguelov%20D%20Erhan%20C%20Szegedy%20S%20Reed%20CY%20Fu%20and%20A%20C%20Berg%20Ssd%20Single%20shot%20multibox%20detector%20In%20ECCV%202016%203"
        },
        {
            "id": "31",
            "entry": "[31] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "32",
            "entry": "[32] J. Mao, X. Wei, Y. Yang, J. Wang, Z. Huang, and A. L. Yuille. Learning like a child: Fast novel visual concept learning from sentence descriptions of images. In ICCV, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20J.%20Wei%2C%20X.%20Yang%2C%20Y.%20Wang%2C%20J.%20Learning%20like%20a%20child%3A%20Fast%20novel%20visual%20concept%20learning%20from%20sentence%20descriptions%20of%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20J.%20Wei%2C%20X.%20Yang%2C%20Y.%20Wang%2C%20J.%20Learning%20like%20a%20child%3A%20Fast%20novel%20visual%20concept%20learning%20from%20sentence%20descriptions%20of%20images%202015"
        },
        {
            "id": "33",
            "entry": "[33] K. Marino, R. Salakhutdinov, and A. Gupta. The more you know: Using knowledge graphs for image classification. In CVPR, 2017. 2, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marino%2C%20K.%20Salakhutdinov%2C%20R.%20Gupta%2C%20A.%20The%20more%20you%20know%3A%20Using%20knowledge%20graphs%20for%20image%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marino%2C%20K.%20Salakhutdinov%2C%20R.%20Gupta%2C%20A.%20The%20more%20you%20know%3A%20Using%20knowledge%20graphs%20for%20image%20classification%202017"
        },
        {
            "id": "34",
            "entry": "[34] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric learning for large scale image classification: Generalizing to new classes at near-zero cost. In Computer Vision\u2013ECCV 2012, 2012. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mensink%2C%20T.%20Verbeek%2C%20J.%20Perronnin%2C%20F.%20Csurka%2C%20G.%20Metric%20learning%20for%20large%20scale%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mensink%2C%20T.%20Verbeek%2C%20J.%20Perronnin%2C%20F.%20Csurka%2C%20G.%20Metric%20learning%20for%20large%20scale%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost%202012"
        },
        {
            "id": "35",
            "entry": "[35] G. A. Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u2013 41, 1995. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miller%2C%20G.A.%20Wordnet%3A%20a%20lexical%20database%20for%20english%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miller%2C%20G.A.%20Wordnet%3A%20a%20lexical%20database%20for%20english%201995"
        },
        {
            "id": "36",
            "entry": "[36] I. Misra, A. Gupta, and M. Hebert. From red wine to red tomato: Composition with context. In CVPR, 2017. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20I.%20Gupta%2C%20A.%20Hebert%2C%20M.%20From%20red%20wine%20to%20red%20tomato%3A%20Composition%20with%20context%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20I.%20Gupta%2C%20A.%20Hebert%2C%20M.%20From%20red%20wine%20to%20red%20tomato%3A%20Composition%20with%20context%202017"
        },
        {
            "id": "37",
            "entry": "[37] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mottaghi%2C%20R.%20Chen%2C%20X.%20Liu%2C%20X.%20Cho%2C%20N.-G.%20The%20role%20of%20context%20for%20object%20detection%20and%20semantic%20segmentation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mottaghi%2C%20R.%20Chen%2C%20X.%20Liu%2C%20X.%20Cho%2C%20N.-G.%20The%20role%20of%20context%20for%20object%20detection%20and%20semantic%20segmentation%202014"
        },
        {
            "id": "38",
            "entry": "[38] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML, pages 2014\u20132023, 2016. 2, 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niepert%2C%20M.%20Ahmed%2C%20M.%20Kutzkov%2C%20K.%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niepert%2C%20M.%20Ahmed%2C%20M.%20Kutzkov%2C%20K.%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014"
        },
        {
            "id": "39",
            "entry": "[39] D. Parikh and K. Grauman. Relative attributes. In ICCV, 2011. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parikh%2C%20D.%20Grauman%2C%20K.%20Relative%20attributes%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parikh%2C%20D.%20Grauman%2C%20K.%20Relative%20attributes%202011"
        },
        {
            "id": "40",
            "entry": "[40] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. In NIPS Workshop, 2017. 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Paszke%20S%20Gross%20S%20Chintala%20G%20Chanan%20E%20Yang%20Z%20DeVito%20Z%20Lin%20A%20Desmaison%20L%20Antiga%20and%20A%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPS%20Workshop%202017%207",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Paszke%20S%20Gross%20S%20Chintala%20G%20Chanan%20E%20Yang%20Z%20DeVito%20Z%20Lin%20A%20Desmaison%20L%20Antiga%20and%20A%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPS%20Workshop%202017%207"
        },
        {
            "id": "41",
            "entry": "[41] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. In CVPR, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Redmon%2C%20J.%20Divvala%2C%20S.%20Girshick%2C%20R.%20Farhadi%2C%20A.%20You%20only%20look%20once%3A%20Unified%2C%20real-time%20object%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Redmon%2C%20J.%20Divvala%2C%20S.%20Girshick%2C%20R.%20Farhadi%2C%20A.%20You%20only%20look%20once%3A%20Unified%2C%20real-time%20object%20detection%202016"
        },
        {
            "id": "42",
            "entry": "[42] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep representations of fine-grained visual descriptions. In CVPR, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Akata%2C%20Z.%20Lee%2C%20H.%20Schiele%2C%20B.%20Learning%20deep%20representations%20of%20fine-grained%20visual%20descriptions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Akata%2C%20Z.%20Lee%2C%20H.%20Schiele%2C%20B.%20Learning%20deep%20representations%20of%20fine-grained%20visual%20descriptions%202016"
        },
        {
            "id": "43",
            "entry": "[43] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, 2015. 1, 2, 3, 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20S.%20He%2C%20K.%20Girshick%2C%20R.%20Sun%2C%20J.%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20S.%20He%2C%20K.%20Girshick%2C%20R.%20Sun%2C%20J.%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015"
        },
        {
            "id": "44",
            "entry": "[44] M. Rohrbach, S. Ebert, and B. Schiele. Transfer learning in a transductive setting. In NIPS, 2013. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rohrbach%2C%20M.%20Ebert%2C%20S.%20Schiele%2C%20B.%20Transfer%20learning%20in%20a%20transductive%20setting%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rohrbach%2C%20M.%20Ebert%2C%20S.%20Schiele%2C%20B.%20Transfer%20learning%20in%20a%20transductive%20setting%202013"
        },
        {
            "id": "45",
            "entry": "[45] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015. 6, 7",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "46",
            "entry": "[46] R. Salakhutdinov, A. Torralba, and J. Tenenbaum. Learning to share visual appearance for multiclass object detection. In CVPR, 2011. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Torralba%2C%20A.%20Tenenbaum%2C%20J.%20Learning%20to%20share%20visual%20appearance%20for%20multiclass%20object%20detection%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Torralba%2C%20A.%20Tenenbaum%2C%20J.%20Learning%20to%20share%20visual%20appearance%20for%20multiclass%20object%20detection%202011"
        },
        {
            "id": "47",
            "entry": "[47] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "48",
            "entry": "[48] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional net. In ICLR Workshop, 2015. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015"
        },
        {
            "id": "49",
            "entry": "[49] A. Torralba, K. P. Murphy, and W. T. Freeman. Sharing features: efficient boosting procedures for multiclass object detection. In CVPR, 2004. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Torralba%2C%20A.%20Murphy%2C%20K.P.%20Freeman%2C%20W.T.%20Sharing%20features%3A%20efficient%20boosting%20procedures%20for%20multiclass%20object%20detection%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Torralba%2C%20A.%20Murphy%2C%20K.P.%20Freeman%2C%20W.T.%20Sharing%20features%3A%20efficient%20boosting%20procedures%20for%20multiclass%20object%20detection%202004"
        },
        {
            "id": "50",
            "entry": "[50] A. Torralba, K. P. Murphy, W. T. Freeman, and M. A. Rubin. Context-based vision system for place and object recognition. In ICCV, 2003. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Torralba%2C%20A.%20Murphy%2C%20K.P.%20Freeman%2C%20W.T.%20Rubin%2C%20M.A.%20Context-based%20vision%20system%20for%20place%20and%20object%20recognition%202003"
        },
        {
            "id": "51",
            "entry": "[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In NIPS, 2017. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20L%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017%205",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20L%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%202017%205"
        },
        {
            "id": "52",
            "entry": "[52] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In CVPR, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Girshick%2C%20R.%20Gupta%2C%20A.%20He%2C%20K.%20Non-local%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Girshick%2C%20R.%20Gupta%2C%20A.%20He%2C%20K.%20Non-local%20neural%20networks%202018"
        },
        {
            "id": "53",
            "entry": "[53] X. Wang, Y. Ye, and A. Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In CVPR, 2018. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20Ye%2C%20Y.%20Gupta%2C%20A.%20Zero-shot%20recognition%20via%20semantic%20embeddings%20and%20knowledge%20graphs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20Ye%2C%20Y.%20Gupta%2C%20A.%20Zero-shot%20recognition%20via%20semantic%20embeddings%20and%20knowledge%20graphs%202018"
        },
        {
            "id": "54",
            "entry": "[54] Q. Wu, P. Wang, C. Shen, A. Dick, and A. van den Hengel. Ask me anything: Free-form visual question answering based on knowledge from external sources. In CVPR, 2016. 3",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Q.%20Wang%2C%20P.%20Shen%2C%20C.%20Dick%2C%20A.%20Ask%20me%20anything%3A%20Free-form%20visual%20question%20answering%20based%20on%20knowledge%20from%20external%20sources%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Q.%20Wang%2C%20P.%20Shen%2C%20C.%20Dick%2C%20A.%20Ask%20me%20anything%3A%20Free-form%20visual%20question%20answering%20based%20on%20knowledge%20from%20external%20sources%202016"
        },
        {
            "id": "55",
            "entry": "[55] J. Yang, J. Lu, D. Batra, and D. Parikh. A faster pytorch implementation of faster r-cnn. https://github.com/jwyang/faster-rcnn.pytorch, 2017.7",
            "url": "https://github.com/jwyang/faster-rcnn.pytorch"
        },
        {
            "id": "56",
            "entry": "[56] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. 2, 6 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Zhao%2C%20H.%20Puig%2C%20X.%20Fidler%2C%20S.%20Scene%20parsing%20through%20ade20k%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20B.%20Zhao%2C%20H.%20Puig%2C%20X.%20Fidler%2C%20S.%20Scene%20parsing%20through%20ade20k%20dataset%202017"
        }
    ]
}
