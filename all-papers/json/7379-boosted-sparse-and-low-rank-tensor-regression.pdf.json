{
    "filename": "7379-boosted-sparse-and-low-rank-tensor-regression.pdf",
    "metadata": {
        "title": "Boosted Sparse and Low-Rank Tensor Regression",
        "author": "Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, Fei Wang",
        "date": 2009,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7379-boosted-sparse-and-low-rank-tensor-regression.pdf"
        },
        "abstract": "We propose a sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse. This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions. We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor regression problems. To make the computation efficient and scalable, for the unit-rank tensor regression, we propose a stagewise estimation procedure to efficiently trace out its entire solution path. We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression. The superior performance of our approach is demonstrated on various real-world and synthetic examples."
    },
    "keywords": [
        {
            "term": "Magnetic Resonance Imaging",
            "url": "https://en.wikipedia.org/wiki/Magnetic_Resonance_Imaging"
        },
        {
            "term": "feature selection",
            "url": "https://en.wikipedia.org/wiki/feature_selection"
        },
        {
            "term": "regression model",
            "url": "https://en.wikipedia.org/wiki/regression_model"
        },
        {
            "term": "Montreal Cognitive Assessment",
            "url": "https://en.wikipedia.org/wiki/Montreal_Cognitive_Assessment"
        },
        {
            "term": "Elastic Net",
            "url": "https://en.wikipedia.org/wiki/Elastic_Net"
        }
    ],
    "highlights": [
        "Regression analysis is commonly used for modeling the relationship between a predictor vector x \u2208 RI and a scalar response y",
        "To make the solution process efficient for the Stagewise Unit-Rank Tensor Factorization problem, we propose a boosted/stagewise estimation procedure to efficiently trace out its entire solution path",
        "We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression",
        "Inspired by the biconvex structure of (7) and the stagewise algorithm for LASSO [<a class=\"ref-link\" id=\"cZhao_2007_a\" href=\"#rZhao_2007_a\">Zhao and Yu, 2007</a>, <a class=\"ref-link\" id=\"cVaughan_et+al_2017_a\" href=\"#rVaughan_et+al_2017_a\">Vaughan et al, 2017</a>], we develop a fast stagewise unit-rank tensor factorization (SURF) algorithm to trace out the entire solution paths of (7) in a single run",
        "We evaluate the effectiveness and efficiency of our method Stagewise Unit-Rank Tensor Factorization through numerical experiments on both synthetic and real data, and compare with various state-of-the-art regression methods, including LASSO, Elastic Net (ENet), Regularized multilinear regression and selection (Remurs) [<a class=\"ref-link\" id=\"cSong_2017_a\" href=\"#rSong_2017_a\">Song and Lu, 2017</a>], optimal CP-rank Tensor Ridge Regression [<a class=\"ref-link\" id=\"cGuo_et+al_2012_a\" href=\"#rGuo_et+al_2012_a\">Guo et al, 2012</a>], Generalized Linear Tensor Regression Model (GLTRM) [<a class=\"ref-link\" id=\"cZhou_et+al_2013_a\" href=\"#rZhou_et+al_2013_a\">Zhou et al, 2013</a>], and a variant of our method with Alternating Convex Search (ACS) estimation",
        "We use three metrics to evaluate the performance: root mean squared prediction error (RMSE), which describes the deviation between the ground truth of the response and the predicted values in out-of-sample testing; sparsity of coefficients (SC), which is the same as the S% defined in the synthetic data analysis; and CPU execution time"
    ],
    "key_statements": [
        "Regression analysis is commonly used for modeling the relationship between a predictor vector x \u2208 RI and a scalar response y",
        "The sum of squared loss with 1-norm regularization leads to the celebrated LASSO approach [<a class=\"ref-link\" id=\"cTibshirani_1996_a\" href=\"#rTibshirani_1996_a\">Tibshirani, 1996</a>], which performs sparse estimation of w and has implicit feature selection embedded therein",
        "We propose a sparse and low-rank tensor regression model in which the unit-rank tensors from the CP decomposition of the coefficient tensor are assumed to be sparse",
        "This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions",
        "We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor factorization/regression problems (SURF) in the form of\n1 min WM",
        "To make the solution process efficient for the Stagewise Unit-Rank Tensor Factorization problem, we propose a boosted/stagewise estimation procedure to efficiently trace out its entire solution path",
        "We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression",
        "Inspired by the biconvex structure of (7) and the stagewise algorithm for LASSO [<a class=\"ref-link\" id=\"cZhao_2007_a\" href=\"#rZhao_2007_a\">Zhao and Yu, 2007</a>, <a class=\"ref-link\" id=\"cVaughan_et+al_2017_a\" href=\"#rVaughan_et+al_2017_a\">Vaughan et al, 2017</a>], we develop a fast stagewise unit-rank tensor factorization (SURF) algorithm to trace out the entire solution paths of (7) in a single run",
        "Due to the biconvex or multi-convex structure of our objective function, it turns out that efficient stagewise estimation remains possible: the only catch is that, when we determine which coefficient to update at each iteration, we always get N competing proposals from N different tensor modes, rather than just one proposal in case of LASSO",
        "Lemma 3 shows that the backward step always performs coordinate descent update of fixed size , each time along the steepest coordinate direction within the current active set, until the descent becomes impossible subject to a tolerance level \u03be",
        "We evaluate the effectiveness and efficiency of our method Stagewise Unit-Rank Tensor Factorization through numerical experiments on both synthetic and real data, and compare with various state-of-the-art regression methods, including LASSO, Elastic Net (ENet), Regularized multilinear regression and selection (Remurs) [<a class=\"ref-link\" id=\"cSong_2017_a\" href=\"#rSong_2017_a\">Song and Lu, 2017</a>], optimal CP-rank Tensor Ridge Regression [<a class=\"ref-link\" id=\"cGuo_et+al_2012_a\" href=\"#rGuo_et+al_2012_a\">Guo et al, 2012</a>], Generalized Linear Tensor Regression Model (GLTRM) [<a class=\"ref-link\" id=\"cZhou_et+al_2013_a\" href=\"#rZhou_et+al_2013_a\">Zhou et al, 2013</a>], and a variant of our method with Alternating Convex Search (ACS) estimation",
        "We examine the performance of our method on a real medical image dataset, including both DTI and Magnetic Resonance Imaging images, obtained from the Parkinson\u2019s progression markers initiative",
        "We use three metrics to evaluate the performance: root mean squared prediction error (RMSE), which describes the deviation between the ground truth of the response and the predicted values in out-of-sample testing; sparsity of coefficients (SC), which is the same as the S% defined in the synthetic data analysis; and CPU execution time"
    ],
    "summary": [
        "Regression analysis is commonly used for modeling the relationship between a predictor vector x \u2208 RI and a scalar response y.",
        "We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor factorization/regression problems (SURF) in the form of",
        "This allows us to kill multiple birds with one stone: by pursuing the elementwise sparsity of the unit-rank coefficient tensor with only one tuning parameter \u03bb, solving (6) can produce a set of sparse factor coefficients wr(n) for n = 1, \u00b7 \u00b7 \u00b7 , N simultaneously.",
        "With this sequential pursue strategy, the general problem boils down to a set of sparse unit-rank estimation problems, for which we develop a novel stagewise/boosting algorithm.",
        "Inspired by the biconvex structure of (7) and the stagewise algorithm for LASSO [<a class=\"ref-link\" id=\"cZhao_2007_a\" href=\"#rZhao_2007_a\">Zhao and Yu, 2007</a>, <a class=\"ref-link\" id=\"cVaughan_et+al_2017_a\" href=\"#rVaughan_et+al_2017_a\">Vaughan et al, 2017</a>], we develop a fast stagewise unit-rank tensor factorization (SURF) algorithm to trace out the entire solution paths of (7) in a single run.",
        "Due to the biconvex or multi-convex structure of our objective function, it turns out that efficient stagewise estimation remains possible: the only catch is that, when we determine which coefficient to update at each iteration, we always get N competing proposals from N different tensor modes, rather than just one proposal in case of LASSO.",
        "Lemma 3 shows that the backward step always performs coordinate descent update of fixed size , each time along the steepest coordinate direction within the current active set, until the descent becomes impossible subject to a tolerance level \u03be.",
        "We evaluate the effectiveness and efficiency of our method SURF through numerical experiments on both synthetic and real data, and compare with various state-of-the-art regression methods, including LASSO, Elastic Net (ENet), Regularized multilinear regression and selection (Remurs) [<a class=\"ref-link\" id=\"cSong_2017_a\" href=\"#rSong_2017_a\">Song and Lu, 2017</a>], optimal CP-rank Tensor Ridge Regression [<a class=\"ref-link\" id=\"cGuo_et+al_2012_a\" href=\"#rGuo_et+al_2012_a\">Guo et al, 2012</a>], Generalized Linear Tensor Regression Model (GLTRM) [<a class=\"ref-link\" id=\"cZhou_et+al_2013_a\" href=\"#rZhou_et+al_2013_a\">Zhou et al, 2013</a>], and a variant of our method with Alternating Convex Search (ACS) estimation.",
        "We first use the synthetic data to examine the performance of our method in different scenarios, with varying step sizes, sparsity level, number of features as well as sample size.",
        "SURF achieves better predictions than other tensor methods, indicating the effectiveness of structured sparsity in unit-rank tensor decomposition itself.",
        "We use three metrics to evaluate the performance: root mean squared prediction error (RMSE), which describes the deviation between the ground truth of the response and the predicted values in out-of-sample testing; sparsity of coefficients (SC), which is the same as the S% defined in the synthetic data analysis; and CPU execution time."
    ],
    "headline": "We propose a sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse",
    "reference_links": [
        {
            "id": "Hastie_et+al_2009_a",
            "entry": "Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Friedman%2C%20Jerome%20The%20elements%20of%20statistical%20learning%202009"
        },
        {
            "id": "Tibshirani_1996_a",
            "entry": "Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B, pages 267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "Yu_2016_a",
            "entry": "Rose Yu and Yan Liu. Learning from multiway data: Simple and efficient tensor regression. In International Conference on Machine Learning, pages 373\u2013381, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Rose%20Liu%2C%20Yan%20Learning%20from%20multiway%20data%3A%20Simple%20and%20efficient%20tensor%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Rose%20Liu%2C%20Yan%20Learning%20from%20multiway%20data%3A%20Simple%20and%20efficient%20tensor%20regression%202016"
        },
        {
            "id": "Zhou_et+al_2013_a",
            "entry": "Hua Zhou, Lexin Li, and Hongtu Zhu. Tensor regression with applications in neuroimaging data analysis. Journal of the American Statistical Association, 108(502):540\u2013552, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Hua%20Li%2C%20Lexin%20Zhu%2C%20Hongtu%20Tensor%20regression%20with%20applications%20in%20neuroimaging%20data%20analysis%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Hua%20Li%2C%20Lexin%20Zhu%2C%20Hongtu%20Tensor%20regression%20with%20applications%20in%20neuroimaging%20data%20analysis%202013"
        },
        {
            "id": "Su_et+al_2012_a",
            "entry": "Ya Su, Xinbo Gao, Xuelong Li, and Dacheng Tao. Multivariate multilinear regression. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 42(6):1560\u20131573, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20Ya%20Gao%2C%20Xinbo%20Li%2C%20Xuelong%20Tao%2C%20Dacheng%20Multivariate%20multilinear%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Su%2C%20Ya%20Gao%2C%20Xinbo%20Li%2C%20Xuelong%20Tao%2C%20Dacheng%20Multivariate%20multilinear%20regression%202012"
        },
        {
            "id": "Guo_et+al_2012_a",
            "entry": "Weiwei Guo, Irene Kotsia, and Ioannis Patras. Tensor learning for regression. IEEE Transactions on Image Processing, 21(2):816\u2013827, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Weiwei%20Kotsia%2C%20Irene%20Patras%2C%20Ioannis%20Tensor%20learning%20for%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Weiwei%20Kotsia%2C%20Irene%20Patras%2C%20Ioannis%20Tensor%20learning%20for%20regression%202012"
        },
        {
            "id": "Wang_et+al_2014_a",
            "entry": "Fei Wang, Ping Zhang, Buyue Qian, Xiang Wang, and Ian Davidson. Clinical risk prediction with multilinear sparse logistic regression. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 145\u2013154. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Fei%20Zhang%2C%20Ping%20Qian%2C%20Buyue%20Wang%2C%20Xiang%20Clinical%20risk%20prediction%20with%20multilinear%20sparse%20logistic%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Fei%20Zhang%2C%20Ping%20Qian%2C%20Buyue%20Wang%2C%20Xiang%20Clinical%20risk%20prediction%20with%20multilinear%20sparse%20logistic%20regression%202014"
        },
        {
            "id": "Zou_2005_a",
            "entry": "Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B, 67(2):301\u2013320, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005"
        },
        {
            "id": "Signoretto_et+al_2014_a",
            "entry": "Marco Signoretto, Quoc Tran Dinh, Lieven De Lathauwer, and Johan AK Suykens. Learning with tensors: a framework based on convex optimization and spectral regularization. Machine Learning, 94(3):303\u2013351, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Signoretto%2C%20Marco%20Dinh%2C%20Quoc%20Tran%20Lathauwer%2C%20Lieven%20De%20Suykens%2C%20Johan%20A.K.%20Learning%20with%20tensors%3A%20a%20framework%20based%20on%20convex%20optimization%20and%20spectral%20regularization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Signoretto%2C%20Marco%20Dinh%2C%20Quoc%20Tran%20Lathauwer%2C%20Lieven%20De%20Suykens%2C%20Johan%20A.K.%20Learning%20with%20tensors%3A%20a%20framework%20based%20on%20convex%20optimization%20and%20spectral%20regularization%202014"
        },
        {
            "id": "Song_2017_a",
            "entry": "Xiaonan Song and Haiping Lu. Multilinear regression for embedded feature selection with application to fmri analysis. In AAAI, pages 2562\u20132568, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Xiaonan%20Lu%2C%20Haiping%20Multilinear%20regression%20for%20embedded%20feature%20selection%20with%20application%20to%20fmri%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Xiaonan%20Lu%2C%20Haiping%20Multilinear%20regression%20for%20embedded%20feature%20selection%20with%20application%20to%20fmri%20analysis%202017"
        },
        {
            "id": "Bengua_et+al_2017_a",
            "entry": "Johann A Bengua, Ho N Phien, Hoang Duong Tuan, and Minh N Do. Efficient tensor completion for color image and video recovery: Low-rank tensor train. IEEE Transactions on Image Processing, 26(5):2466\u20132479, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengua%2C%20Johann%20A.%20Phien%2C%20Ho%20N.%20Tuan%2C%20Hoang%20Duong%20Do%2C%20Minh%20N.%20Efficient%20tensor%20completion%20for%20color%20image%20and%20video%20recovery%3A%20Low-rank%20tensor%20train%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengua%2C%20Johann%20A.%20Phien%2C%20Ho%20N.%20Tuan%2C%20Hoang%20Duong%20Do%2C%20Minh%20N.%20Efficient%20tensor%20completion%20for%20color%20image%20and%20video%20recovery%3A%20Low-rank%20tensor%20train%202017"
        },
        {
            "id": "Kolda_2009_a",
            "entry": "Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM Review, 51(3):455\u2013500, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20decompositions%20and%20applications%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20decompositions%20and%20applications%202009"
        },
        {
            "id": "Chen_et+al_2012_a",
            "entry": "Kun Chen, Kung-Sik Chan, and Nils Chr Stenseth. Reduced rank stochastic regression with a sparse singular value decomposition. Journal of the Royal Statistical Society: Series B, 74(2):203\u2013221, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Kun%20Chan%2C%20Kung-Sik%20Stenseth%2C%20Nils%20Chr%20Reduced%20rank%20stochastic%20regression%20with%20a%20sparse%20singular%20value%20decomposition%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Kun%20Chan%2C%20Kung-Sik%20Stenseth%2C%20Nils%20Chr%20Reduced%20rank%20stochastic%20regression%20with%20a%20sparse%20singular%20value%20decomposition%202012"
        },
        {
            "id": "Mishra_et+al_2017_a",
            "entry": "Aditya Mishra, Dipak K. Dey, and Kun Chen. Sequential co-sparse factor regression. Journal of Computational and Graphical Statistics, pages 814\u2013825, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20Aditya%20Dey%2C%20Dipak%20K.%20Chen%2C%20Kun%20Sequential%20co-sparse%20factor%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20Aditya%20Dey%2C%20Dipak%20K.%20Chen%2C%20Kun%20Sequential%20co-sparse%20factor%20regression%202017"
        },
        {
            "id": "Phan_et+al_2015_a",
            "entry": "Anh-Huy Phan, Petr Tichavsky, and Andrzej Cichocki. Tensor deflation for candecomp/parafac\u2014part i: Alternating subspace update algorithm. IEEE Transactions on Signal Processing, 63(22):5924\u20135938, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Phan%2C%20Anh-Huy%20Tichavsky%2C%20Petr%20Cichocki%2C%20Andrzej%20Tensor%20deflation%20for%20candecomp/parafac%E2%80%94part%20i%3A%20Alternating%20subspace%20update%20algorithm%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Phan%2C%20Anh-Huy%20Tichavsky%2C%20Petr%20Cichocki%2C%20Andrzej%20Tensor%20deflation%20for%20candecomp/parafac%E2%80%94part%20i%3A%20Alternating%20subspace%20update%20algorithm%202015"
        },
        {
            "id": "Minasian_et+al_2014_a",
            "entry": "Arin Minasian, Shahram ShahbazPanahi, and Raviraj S Adve. Energy harvesting cooperative communication systems. IEEE Transactions on Wireless Communications, 13(11):6118\u20136131, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minasian%2C%20Arin%20ShahbazPanahi%2C%20Shahram%20Adve%2C%20Raviraj%20S.%20Energy%20harvesting%20cooperative%20communication%20systems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minasian%2C%20Arin%20ShahbazPanahi%2C%20Shahram%20Adve%2C%20Raviraj%20S.%20Energy%20harvesting%20cooperative%20communication%20systems%202014"
        },
        {
            "id": "Zhao_2007_a",
            "entry": "Peng Zhao and Bin Yu. Stagewise lasso. Journal of Machine Learning Research, 8(Dec):2701\u20132726, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Peng%20Yu%2C%20Bin%20Stagewise%20lasso%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Peng%20Yu%2C%20Bin%20Stagewise%20lasso%202007"
        },
        {
            "id": "Vaughan_et+al_2017_a",
            "entry": "Gregory Vaughan, Robert Aseltine, Kun Chen, and Jun Yan. Stagewise generalized estimation equations with grouped variables. Biometrics, 73:1332\u20131342, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaughan%2C%20Gregory%20Aseltine%2C%20Robert%20Chen%2C%20Kun%20Yan%2C%20Jun%20Stagewise%20generalized%20estimation%20equations%20with%20grouped%20variables%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaughan%2C%20Gregory%20Aseltine%2C%20Robert%20Chen%2C%20Kun%20Yan%2C%20Jun%20Stagewise%20generalized%20estimation%20equations%20with%20grouped%20variables%202017"
        },
        {
            "id": "Da_et+al_2015_a",
            "entry": "Alex Pereira da Silva, Pierre Comon, and Andre Lima Ferrer de Almeida. Rank-1 tensor approximation methods and application to deflation. arXiv preprint arXiv:1508.05273, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.05273"
        },
        {
            "id": "Friedman_et+al_2010_a",
            "entry": "Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20Jerome%20Hastie%2C%20Trevor%20Tibshirani%2C%20Rob%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20Jerome%20Hastie%2C%20Trevor%20Tibshirani%2C%20Rob%20Regularization%20paths%20for%20generalized%20linear%20models%20via%20coordinate%20descent%202010"
        },
        {
            "id": "Kittipat_et+al_2014_a",
            "entry": "Kittipat Kampa, S Mehta, Chun-An Chou, Wanpracha Art Chaovalitwongse, and Thomas J Grabowski. Sparse optimization in feature selection: application in neuroimaging. Journal of Global Optimization, 59(2-3): 439\u2013457, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kittipat%20Kampa%2C%20S.Mehta%20Chou%2C%20Chun-An%20Chaovalitwongse%2C%20Wanpracha%20Art%20Grabowski%2C%20Thomas%20J.%20Sparse%20optimization%20in%20feature%20selection%3A%20application%20in%20neuroimaging%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kittipat%20Kampa%2C%20S.Mehta%20Chou%2C%20Chun-An%20Chaovalitwongse%2C%20Wanpracha%20Art%20Grabowski%2C%20Thomas%20J.%20Sparse%20optimization%20in%20feature%20selection%3A%20application%20in%20neuroimaging%202014"
        }
    ]
}
