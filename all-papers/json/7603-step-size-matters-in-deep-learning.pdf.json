{
    "filename": "7603-step-size-matters-in-deep-learning.pdf",
    "metadata": {
        "title": "Step Size Matters in Deep Learning",
        "author": "Kamil Nar, Shankar Sastry",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7603-step-size-matters-in-deep-learning.pdf"
        },
        "abstract": "Training a neural network with the gradient descent algorithm gives rise to a discrete-time nonlinear dynamical system. Consequently, behaviors that are typically observed in these systems emerge during training, such as convergence to an orbit but not to a fixed point or dependence of convergence on the initialization. Step size of the algorithm plays a critical role in these behaviors: it determines the subset of the local optima that the algorithm can converge to, and it specifies the magnitude of the oscillations if the algorithm converges to an orbit. To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm. The results provide an explanation for several phenomena observed in practice, including the deterioration in the training error with increased depth, the hardness of estimating linear mappings with large singular values, and the distinct performance of deep residual networks."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "hybrid system",
            "url": "https://en.wikipedia.org/wiki/hybrid_system"
        },
        {
            "term": "lyapunov stability",
            "url": "https://en.wikipedia.org/wiki/lyapunov_stability"
        }
    ],
    "highlights": [
        "We study the gradient descent algorithm as a discrete-time dynamical system during training deep neural networks, and we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm",
        "We show that if the gradient descent algorithm can converge to a solution with a large step size, the function estimated by the network must have small singular values, and the estimated function must have a small Lipschitz constant.\n3",
        "Given a fixed step size \u03b4, the gradient descent algorithm can converge to only a subset of the local optima, and there are always some solutions that the algorithm cannot converge to independent of the initialization",
        "The training error may not reflect the oscillations in the dynamics, or when a stochastic optimization method is used, the oscillations in the training error might be wrongly attributed to the stochasticity of the algorithm",
        "We showed that the step size of the gradient descent algorithm influences the dynamics of the algorithm substantially"
    ],
    "key_statements": [
        "We study the gradient descent algorithm as a discrete-time dynamical system during training deep neural networks, and we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm",
        "The state x[k] converges to the origin if the initial state satisfies x[0]L\u22122 < (2/L\u03b4) and x[k] diverges if x[0]L\u22122 > (2/L\u03b4). These three examples demonstrate: 1. the convergence of training error does not imply the convergence of the algorithm to a local optimum or a saddle point, 2. the step size determines the magnitude of the oscillations if the algorithm converges to an orbit but not to a fixed point, 3. the step size restricts the set of local optima that the algorithm can converge to, 4. the step size influences the convergence of the algorithm differently for each initialization",
        "We study the gradient descent algorithm as a discrete-time dynamical system during training deep neural networks, and we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm",
        "We show that if the gradient descent algorithm can converge to a solution with a large step size, the function estimated by the network must have small singular values, and the estimated function must have a small Lipschitz constant.\n3",
        "We show that symmetric positive definite matrices can be estimated with a deep linear network by initializing the weight matrices as the identity, and this initialization allows the use of the largest step size",
        "We obtain upper bounds on the step size to be used, and we show that the step size restricts the set of local optima that the algorithm can converge to",
        "Given a fixed step size \u03b4, the gradient descent algorithm can converge to only a subset of the local optima, and there are always some solutions that the algorithm cannot converge to independent of the initialization",
        "For the minimization problem in Theorem 1, the gradient descent algorithm with random initialization can converge to a global optimum only if the step size \u03b4 satisfies \u03b4\u2264",
        "The smoothness parameter of the training cost function is directly related to the largest step size that can be used, and to the Lyapunov stability of the gradient descent algorithm",
        "If the matrix to be estimated is symmetric and positive definite, the algorithm can converge to a solution with step sizes close to (4), which requires a specific initialization of the weight parameters",
        "In Section 2, we analyzed the relationship between the step size of the gradient descent algorithm and the solutions that can be obtained by training deep linear networks",
        "The training error may not reflect the oscillations in the dynamics, or when a stochastic optimization method is used, the oscillations in the training error might be wrongly attributed to the stochasticity of the algorithm",
        "We showed that the step size of the gradient descent algorithm influences the dynamics of the algorithm substantially",
        "In Corollary 2 and Theorem 4, we showed that the step size required for convergence to a specific solution depends on the solution itself"
    ],
    "summary": [
        "We study the gradient descent algorithm as a discrete-time dynamical system during training deep neural networks, and we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm.",
        "We analyze the Lyapunov stability of the gradient descent algorithm on deep linear networks and find different upper bounds on the step size that enable convergence to each solution.",
        "We show that if the gradient descent algorithm can converge to a solution with a large step size, the function estimated by the network must have small singular values, and the estimated function must have a small Lipschitz constant.",
        "Multiple local optima attain the same training cost for deep linear networks, the dynamics of the gradient descent algorithm exhibits distinct behaviors around these points.",
        "The gradient descent algorithm with random initialization can converge to a solution {Wj}j\u2208[L] only if the step size \u03b4 satisfies \u03b4\u2264",
        "Given a fixed step size \u03b4, the gradient descent algorithm can converge to only a subset of the local optima, and there are always some solutions that the algorithm cannot converge to independent of the initialization.",
        "For the minimization problem in Theorem 1, the gradient descent algorithm with random initialization can converge to a global optimum only if the step size \u03b4 satisfies \u03b4\u2264",
        "The smoothness parameter of the training cost function is directly related to the largest step size that can be used, and to the Lyapunov stability of the gradient descent algorithm.",
        "If the matrix to be estimated is symmetric and positive definite, the algorithm can converge to a solution with step sizes close to (4), which requires a specific initialization of the weight parameters.",
        "From the analysis of symmetric matrices, we observe that the step size required for convergence to a global optimum is largest when the singular vector of R corresponding to its largest singular value is amplified or attenuated at each layer of the network.",
        "In Section 2, we analyzed the relationship between the step size of the gradient descent algorithm and the solutions that can be obtained by training deep linear networks.",
        "The following theorem, for example, provides an upper bound on the step size for the convergence of the algorithm when the network has two layers and ReLU activations."
    ],
    "headline": "To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, Vol. 2, pp. 53\u201358, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20P.%20Hornik%2C%20K.%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20P.%20Hornik%2C%20K.%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989"
        },
        {
            "id": "2",
            "entry": "[2] N. E. Barabanov and D. V. Prokhorov. Stability analysis of discrete-time recurrent neural networks. IEEE Transactions on Neural Networks, Vol. 13, No. 2, pp. 292\u2013303, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barabanov%2C%20N.E.%20Prokhorov%2C%20D.V.%20Stability%20analysis%20of%20discrete-time%20recurrent%20neural%20networks%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barabanov%2C%20N.E.%20Prokhorov%2C%20D.V.%20Stability%20analysis%20of%20discrete-time%20recurrent%20neural%20networks%202002"
        },
        {
            "id": "3",
            "entry": "[3] P. L. Bartlett, S. Evans, and P. Long. Representing smooth functions as compositions of nearidentity functions with implications for deep network optimization. arXiv:1804.05012 [cs.LG], 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05012"
        },
        {
            "id": "4",
            "entry": "[4] P. L. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] P. L. Bartlett, D. P. Helmbold, and P. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. arXiv:1802.06093 [cs.LG], 2018b.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06093"
        },
        {
            "id": "6",
            "entry": "[6] C. Daniel, J. Taylor, and S. Nowozin. Learning step size controllers for robust neural network training. In AAAI Conference on Artificial Intelligence, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20C.%20Taylor%2C%20J.%20Nowozin%2C%20S.%20Learning%20step%20size%20controllers%20for%20robust%20neural%20network%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%2C%20C.%20Taylor%2C%20J.%20Nowozin%2C%20S.%20Learning%20step%20size%20controllers%20for%20robust%20neural%20network%20training%202016"
        },
        {
            "id": "7",
            "entry": "[7] S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pp. 6152\u20136160, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20S.%20Woodworth%2C%20B.E.%20Bhojanapalli%2C%20S.%20Neyshabur%2C%20B.%20Implicit%20regularization%20in%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20S.%20Woodworth%2C%20B.E.%20Bhojanapalli%2C%20S.%20Neyshabur%2C%20B.%20Implicit%20regularization%20in%20matrix%20factorization%202017"
        },
        {
            "id": "8",
            "entry": "[8] M. Hardt and T. Ma. Identity matters in deep learning. arXiv:1611.04231 [cs.LG], 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04231"
        },
        {
            "id": "9",
            "entry": "[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "10",
            "entry": "[10] R. A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, Vol. 1, pp. 295\u2013307, 1988. Processing Systems, pp. 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacobs%2C%20R.A.%20Increased%20rates%20of%20convergence%20through%20learning%20rate%20adaptation%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacobs%2C%20R.A.%20Increased%20rates%20of%20convergence%20through%20learning%20rate%20adaptation%201988"
        },
        {
            "id": "12",
            "entry": "[12] H. K. Khalil. Nonlinear Systems, 3rd Edition. Prentice Hall, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khalil%2C%20H.K.%20Nonlinear%20Systems"
        },
        {
            "id": "13",
            "entry": "[13] G. D. Magoulas, M. N. Vrahatis and G. S. Androulakis. Effective backpropagation training with variable stepsize. Neural Networks, Vol. 10, No. 1, pp. 69\u201382, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magoulas%2C%20G.D.%20Vrahatis%2C%20M.N.%20Androulakis%2C%20G.S.%20Effective%20backpropagation%20training%20with%20variable%20stepsize%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Magoulas%2C%20G.D.%20Vrahatis%2C%20M.N.%20Androulakis%2C%20G.S.%20Effective%20backpropagation%20training%20with%20variable%20stepsize%201997"
        },
        {
            "id": "14",
            "entry": "[14] K. Matsuoka. Stability conditions for nonlinear continuous neural networks with asymmetric connection weights. Neural Networks, Vol. 5, No. 3, pp. 495\u2013500, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matsuoka%2C%20K.%20Stability%20conditions%20for%20nonlinear%20continuous%20neural%20networks%20with%20asymmetric%20connection%20weights%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matsuoka%2C%20K.%20Stability%20conditions%20for%20nonlinear%20continuous%20neural%20networks%20with%20asymmetric%20connection%20weights%201992"
        },
        {
            "id": "15",
            "entry": "[15] A. N. Michel, J. A. Farrell, and W. Porod. Stability results for neural networks. In Neural Information Processing Systems, pp. 554\u2013563, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Michel%2C%20A.N.%20Farrell%2C%20J.A.%20Porod%2C%20W.%20Stability%20results%20for%20neural%20networks%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Michel%2C%20A.N.%20Farrell%2C%20J.A.%20Porod%2C%20W.%20Stability%20results%20for%20neural%20networks%201988"
        },
        {
            "id": "16",
            "entry": "[16] K. Nar and S. S. Sastry. Residual Networks: Lyapunov stability and convex decomposition. arXiv:1803.08203 [cs.LG], 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08203"
        },
        {
            "id": "17",
            "entry": "[17] B. Neyshabur, R. Tomioka, R. Salakhutdinov, and N. Srebro. Geometry of optimization and implicit regularization in deep learning. arXiv:1705.03071 [cs.LG], 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03071"
        },
        {
            "id": "18",
            "entry": "[18] S. Sastry. Nonlinear Systems: Analysis, Stability, and Control. Springer: New York, NY, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sastry%2C%20S.%20Nonlinear%20Systems%3A%20Analysis%2C%20Stability%2C%20and%20Control%201999"
        },
        {
            "id": "19",
            "entry": "[19] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120 [cs.NE], 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6120"
        },
        {
            "id": "20",
            "entry": "[20] M. Rolinek and G. Martius. L4: Practical loss-based stepsize adaptation for deep learning. arXiv:1802.05074 [cs.LG], 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05074"
        }
    ]
}
