{
    "filename": "8194-maximizing-acquisition-functions-for-bayesian-optimization.pdf",
    "metadata": {
        "title": "Maximizing acquisition functions for Bayesian optimization",
        "author": "James Wilson, Frank Hutter, Marc Deisenroth",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8194-maximizing-acquisition-functions-for-bayesian-optimization.pdf"
        },
        "abstract": "Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes\u2019 decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, highdimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose characteristics not only facilitate but justify use of greedy approaches for their maximization."
    },
    "keywords": [
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Monte_Carlo"
        },
        {
            "term": "global optimization",
            "url": "https://en.wikipedia.org/wiki/global_optimization"
        },
        {
            "term": "Bayesian optimization",
            "url": "https://en.wikipedia.org/wiki/Bayesian_optimization"
        }
    ],
    "highlights": [
        "Bayesian optimization (BO) is a powerful framework for tackling complicated global optimization problems [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>]",
        "To help reconcile theory and practice, we present two modern approaches for addressing Bayesian optimization\u2019s inner optimization problem that exploit key properties of acquisition functions and their estimators",
        "Inner optimization problem Maximizing acquisition functions plays a crucial role in Bayesian optimization as the process through which abstract machinery yields concrete actions",
        "Bayesian optimization relies upon an array of powerful tools, such as surrogate models and acquisition functions, and all of these tools are sharpened by strong usage practices",
        "We extend these practices by demonstrating that Monte Carlo acquisition functions provide unbiased gradient estimates that can be exploited when optimizing them",
        "We show that many of the same acquisition functions form a family of submodular acquisition funcionts that can be efficient optimized using greedy maximization"
    ],
    "key_statements": [
        "Bayesian optimization (BO) is a powerful framework for tackling complicated global optimization problems [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>]",
        "Challenges associated with maximizing L greatly impede our ability to live up to this standard. This inner optimization problem is often treated as a black-box unto itself",
        "To help reconcile theory and practice, we present two modern approaches for addressing Bayesian optimization\u2019s inner optimization problem that exploit key properties of acquisition functions and their estimators",
        "Inner optimization problem Maximizing acquisition functions plays a crucial role in Bayesian optimization as the process through which abstract machinery yields concrete actions",
        "We assessed the efficacy of gradient-based and submodular strategies for maximizing acquisition function in two primary settings: \u201csynthetic\u201d, where task f was drawn from a known Gaussian process prior prior, and \u201cblack-box\u201d, where f \u2019s nature is unknown to the optimizer",
        "Acquisition functions We focused on parallel Monte Carlo acquisition functions Lm, particularly EI and Upper Confidence Bound",
        "Bayesian optimization relies upon an array of powerful tools, such as surrogate models and acquisition functions, and all of these tools are sharpened by strong usage practices",
        "We extend these practices by demonstrating that Monte Carlo acquisition functions provide unbiased gradient estimates that can be exploited when optimizing them",
        "We show that many of the same acquisition functions form a family of submodular acquisition funcionts that can be efficient optimized using greedy maximization"
    ],
    "summary": [
        "Bayesian optimization (BO) is a powerful framework for tackling complicated global optimization problems [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>].",
        "One first specifies a belief over possible explanations for f using a probabilistic surrogate model and combines this belief with an acquisition function L to convey the expected utility for evaluating a set of queries X.",
        "To help reconcile theory and practice, we present two modern approaches for addressing BO\u2019s inner optimization problem that exploit key properties of acquisition functions and their estimators.",
        "Bayesian optimization relies on both a surrogate model M and an acquisition function L to define a strategy for efficiently maximizing a black-box function f .",
        "Inner optimization problem Maximizing acquisition functions plays a crucial role in BO as the process through which abstract machinery yields concrete actions.",
        "The authors use these results to prove that (2) is an unbiased gradient estimator for the parallel Expected Improvement (q-EI) acquisition function [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>].",
        "Greedy maximization is a popular approach for selecting near-optimal sets of queries X to be evaluated in parallel [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>].",
        "We show that the family of MM acquisition functions are inherently SM, thereby guaranteeing that greedy maximization thereof produces near-optimal choices X at each step of BO\u2019s outer-loop.3 We begin by removing some unnecessary complexity: 1.",
        "This result establishes the MM family as a class of SM functions, providing strong theoretical justification for greedy approaches to solving BO\u2019s inner-optimization problem.",
        "Incremental form So far, we have discussed greedy maximizers that select new points x by optimizing the joint acquisition L(X\u222a{x }; D) = Ey\u222a{y }|D [ (y \u222a {y })] originally defined in (1).",
        "We assessed the efficacy of gradient-based and submodular strategies for maximizing acquisition function in two primary settings: \u201csynthetic\u201d, where task f was drawn from a known GP prior, and \u201cblack-box\u201d, where f \u2019s nature is unknown to the optimizer.",
        "At each outer-loop iteration, an \u201cinner budget\u201d was defined as the average time taken to simultaneously evaluate N acquisition values given equivalent conditions.",
        "These results clearly demonstrate that both gradient-based and submodular approaches to query optimization lead to reliable and, often, substantial improvement in outer-loop performance.",
        "We extend these practices by demonstrating that Monte Carlo acquisition functions provide unbiased gradient estimates that can be exploited when optimizing them.",
        "We show that many of the same acquisition functions form a family of submodular acquisition funcionts that can be efficient optimized using greedy maximization.",
        "By tackling the inner optimization problem, our contributions directly benefit the theory and practice of Bayesian optimization"
    ],
    "headline": "We identify a common family of acquisition functions, including EI and Upper Confidence Bound, whose characteristics not only facilitate but justify use of greedy approaches for their maximization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J. Azimi, A. Fern, and X.Z. Fern. Batch Bayesian optimization via simulation matching. In Advances in Neural Information Processing Systems, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azimi%2C%20J.%20Fern%2C%20A.%20Fern%2C%20X.Z.%20Batch%20Bayesian%20optimization%20via%20simulation%20matching%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azimi%2C%20J.%20Fern%2C%20A.%20Fern%2C%20X.Z.%20Batch%20Bayesian%20optimization%20via%20simulation%20matching%202010"
        },
        {
            "id": "2",
            "entry": "[2] F. Bach. Learning with submodular functions: A convex optimization perspective. Foundations and Trends R in Machine Learning, 6(2-3), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013"
        },
        {
            "id": "3",
            "entry": "[3] S. Bansal, R. Calandra, T. Xiao, S. Levine, and C.J. Tomlin. Goal-driven dynamics learning via Bayesian optimization. arXiv preprint arXiv:1703.09260, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09260"
        },
        {
            "id": "4",
            "entry": "[4] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20J.%20Bengio%2C%20Y.%20Random%20search%20for%20hyper-parameter%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20J.%20Bengio%2C%20Y.%20Random%20search%20for%20hyper-parameter%20optimization%202012"
        },
        {
            "id": "5",
            "entry": "[5] O. Bousquet and L. Bottou. The tradeoffs of large scale learning. In Advances in Neural Information Processing Systems, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20O.%20Bottou%2C%20L.%20The%20tradeoffs%20of%20large%20scale%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20O.%20Bottou%2C%20L.%20The%20tradeoffs%20of%20large%20scale%20learning%202008"
        },
        {
            "id": "6",
            "entry": "[6] R. Calandra, A. Seyfarth, J. Peters, and M.P. Deisenroth. Bayesian optimization for learning gaits under uncertainty. Annals of Mathematics and Artificial Intelligence, 76(1-2), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calandra%2C%20R.%20Seyfarth%2C%20A.%20Peters%2C%20J.%20Deisenroth%2C%20M.P.%20Bayesian%20optimization%20for%20learning%20gaits%20under%20uncertainty%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calandra%2C%20R.%20Seyfarth%2C%20A.%20Peters%2C%20J.%20Deisenroth%2C%20M.P.%20Bayesian%20optimization%20for%20learning%20gaits%20under%20uncertainty%202016"
        },
        {
            "id": "7",
            "entry": "[7] X. Cao. Convergence of parameter sensitivity estimates in a stochastic experiment. IEEE Transactions on Automatic Control, 30(9), 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20X.%20Convergence%20of%20parameter%20sensitivity%20estimates%20in%20a%20stochastic%20experiment%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20X.%20Convergence%20of%20parameter%20sensitivity%20estimates%20in%20a%20stochastic%20experiment%201985"
        },
        {
            "id": "8",
            "entry": "[8] Y. Chen and A. Krause. Near-optimal batch mode active learning and adaptive submodular optimization.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Krause%2C%20A.%20Near-optimal%20batch%20mode%20active%20learning%20and%20adaptive%20submodular%20optimization"
        },
        {
            "id": "9",
            "entry": "[9] C. Chevalier and D. Ginsbourger. Fast computation of the multi-points expected improvement with applications in batch selection. In International Conference on Learning and Intelligent Optimization, 2013. Springer Science & Business Media, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chevalier%2C%20C.%20Ginsbourger%2C%20D.%20Fast%20computation%20of%20the%20multi-points%20expected%20improvement%20with%20applications%20in%20batch%20selection%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chevalier%2C%20C.%20Ginsbourger%2C%20D.%20Fast%20computation%20of%20the%20multi-points%20expected%20improvement%20with%20applications%20in%20batch%20selection%202013"
        },
        {
            "id": "11",
            "entry": "[11] E. Contal, D. Buffoni, A. Robicquet, and N. Vayatis. Parallel Gaussian process optimization with upper confidence bound and pure exploration. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Contal%2C%20E.%20Buffoni%2C%20D.%20Robicquet%2C%20A.%20Vayatis%2C%20N.%20Parallel%20Gaussian%20process%20optimization%20with%20upper%20confidence%20bound%20and%20pure%20exploration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Contal%2C%20E.%20Buffoni%2C%20D.%20Robicquet%2C%20A.%20Vayatis%2C%20N.%20Parallel%20Gaussian%20process%20optimization%20with%20upper%20confidence%20bound%20and%20pure%20exploration%202013"
        },
        {
            "id": "12",
            "entry": "[12] J.P. Cunningham, P. Hennig, and S. Lacoste-Julien. Gaussian probabilities and expectation propagation. arXiv preprint arXiv:1111.6832, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1111.6832"
        },
        {
            "id": "13",
            "entry": "[13] M.H. DeGroot. Optimal statistical decisions, volume 82. John Wiley & Sons, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=DeGroot%2C%20M.H.%20Optimal%20statistical%20decisions%2C%20volume%2082%202005"
        },
        {
            "id": "14",
            "entry": "[14] T. Desautels, A. Krause, and J.W. Burdick. Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. Journal of Machine Learning Research, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Desautels%2C%20T.%20Krause%2C%20A.%20Burdick%2C%20J.W.%20Parallelizing%20exploration-exploitation%20tradeoffs%20in%20Gaussian%20process%20bandit%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Desautels%2C%20T.%20Krause%2C%20A.%20Burdick%2C%20J.W.%20Parallelizing%20exploration-exploitation%20tradeoffs%20in%20Gaussian%20process%20bandit%20optimization%202014"
        },
        {
            "id": "15",
            "entry": "[15] S. Falkner, A. Klein, and F. Hutter. BOHB: Robust and efficient hyperparameter optimization at scale. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Falkner%2C%20S.%20Klein%2C%20A.%20Hutter%2C%20F.%20BOHB%3A%20Robust%20and%20efficient%20hyperparameter%20optimization%20at%20scale%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Falkner%2C%20S.%20Klein%2C%20A.%20Hutter%2C%20F.%20BOHB%3A%20Robust%20and%20efficient%20hyperparameter%20optimization%20at%20scale%202018"
        },
        {
            "id": "16",
            "entry": "[16] P.I. Frazier and J. Wang. Bayesian optimization for materials design. In Information Science for Materials Discovery and Design. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frazier%2C%20P.I.%20Wang%2C%20J.%20Bayesian%20optimization%20for%20materials%20design.%20In%20Information%20Science%20for%20Materials%20Discovery%20and%20Design%202016"
        },
        {
            "id": "17",
            "entry": "[17] H.I. Gassmann, I. De\u00e1k, and T. Sz\u00e1ntai. Computing multivariate normal probabilities: A new look. Journal of Computational and Graphical Statistics, 11(4), 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gassmann%2C%20H.I.%20De%C3%A1k%2C%20I.%20Sz%C3%A1ntai%2C%20T.%20Computing%20multivariate%20normal%20probabilities%3A%20A%20new%20look%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gassmann%2C%20H.I.%20De%C3%A1k%2C%20I.%20Sz%C3%A1ntai%2C%20T.%20Computing%20multivariate%20normal%20probabilities%3A%20A%20new%20look%202002"
        },
        {
            "id": "18",
            "entry": "[18] M.A. Gelbart, J. Snoek, and R.P. Adams. Bayesian optimization with unknown constraints. arXiv preprint arXiv:1403.5607, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1403.5607"
        },
        {
            "id": "19",
            "entry": "[19] A. Genz. Numerical computation of multivariate normal probabilities. Journal of Computational and Graphical Statistics, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genz%2C%20A.%20Numerical%20computation%20of%20multivariate%20normal%20probabilities%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genz%2C%20A.%20Numerical%20computation%20of%20multivariate%20normal%20probabilities%201992"
        },
        {
            "id": "20",
            "entry": "[20] A. Genz. Numerical computation of rectangular bivariate and trivariate normal and t probabilities. Statistics and Computing, 14(3), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genz%2C%20A.%20Numerical%20computation%20of%20rectangular%20bivariate%20and%20trivariate%20normal%20and%20t%20probabilities%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genz%2C%20A.%20Numerical%20computation%20of%20rectangular%20bivariate%20and%20trivariate%20normal%20and%20t%20probabilities%202004"
        },
        {
            "id": "21",
            "entry": "[21] D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization, chapter 6.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%20Ginsbourger%20R%20Le%20Riche%20and%20L%20Carraro%20Kriging%20is%20wellsuited%20to%20parallelize%20optimization%20chapter%206"
        },
        {
            "id": "22",
            "entry": "[22] D. Ginsbourger, J. Janusevskis, and R. Le Riche. Dealing with asynchronicity in parallel Gaussian process based global optimization. In International Conference of the ERCIM WG on Computing & Statistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ginsbourger%2C%20D.%20Janusevskis%2C%20J.%20Riche%2C%20R.Le%20Dealing%20with%20asynchronicity%20in%20parallel%20Gaussian%20process%20based%20global%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ginsbourger%2C%20D.%20Janusevskis%2C%20J.%20Riche%2C%20R.Le%20Dealing%20with%20asynchronicity%20in%20parallel%20Gaussian%20process%20based%20global%20optimization%202011"
        },
        {
            "id": "23",
            "entry": "[23] P. Glasserman. Performance continuity and differentiability in Monte Carlo optimization. In Simulation Conference Proceedings, 1988 Winter. IEEE, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glasserman%2C%20P.%20Performance%20continuity%20and%20differentiability%20in%20Monte%20Carlo%20optimization%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glasserman%2C%20P.%20Performance%20continuity%20and%20differentiability%20in%20Monte%20Carlo%20optimization%201988"
        },
        {
            "id": "24",
            "entry": "[24] N. Hansen. The CMA evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.00772"
        },
        {
            "id": "25",
            "entry": "[25] P. Hennig and C. Schuler. Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hennig%2C%20P.%20Schuler%2C%20C.%20Entropy%20search%20for%20information-efficient%20global%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hennig%2C%20P.%20Schuler%2C%20C.%20Entropy%20search%20for%20information-efficient%20global%20optimization%202012"
        },
        {
            "id": "26",
            "entry": "[26] J. Hern\u00e1ndez-Lobato, M. Hoffman, and Z. Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20J.%20Hoffman%2C%20M.%20Ghahramani%2C%20Z.%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20J.%20Hoffman%2C%20M.%20Ghahramani%2C%20Z.%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014"
        },
        {
            "id": "27",
            "entry": "[27] F. Hutter, H.H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In International Conference on Learning and Intelligent Optimization. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hutter%2C%20F.%20Hoos%2C%20H.H.%20Leyton-Brown%2C%20K.%20Sequential%20model-based%20optimization%20for%20general%20algorithm%20configuration%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hutter%2C%20F.%20Hoos%2C%20H.H.%20Leyton-Brown%2C%20K.%20Sequential%20model-based%20optimization%20for%20general%20algorithm%20configuration%202011"
        },
        {
            "id": "28",
            "entry": "[28] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with Gumbel-Softmax. arXiv preprint arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "29",
            "entry": "[29] D. Jones, M. Schonlau, and W. Welch. Efficient global optimization of expensive black box functions. Journal of Global Optimization, 13:455\u2013492, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jones%2C%20D.%20Schonlau%2C%20M.%20Welch%2C%20W.%20Efficient%20global%20optimization%20of%20expensive%20black%20box%20functions%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jones%2C%20D.%20Schonlau%2C%20M.%20Welch%2C%20W.%20Efficient%20global%20optimization%20of%20expensive%20black%20box%20functions%201998"
        },
        {
            "id": "30",
            "entry": "[30] T. Kathuria, A. Deshpande, and P. Kohli. Batched Gaussian process bandit optimization via determinantal point processes. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kathuria%2C%20T.%20Deshpande%2C%20A.%20Kohli%2C%20P.%20Batched%20Gaussian%20process%20bandit%20optimization%20via%20determinantal%20point%20processes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kathuria%2C%20T.%20Deshpande%2C%20A.%20Kohli%2C%20P.%20Batched%20Gaussian%20process%20bandit%20optimization%20via%20determinantal%20point%20processes%202016"
        },
        {
            "id": "31",
            "entry": "[31] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "32",
            "entry": "[32] D.P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "33",
            "entry": "[33] A. Krause and D. Golovin. Submodular function maximization, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20A.%20Golovin%2C%20D.%20Submodular%20function%20maximization%202014"
        },
        {
            "id": "34",
            "entry": "[34] H.J. Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering, 86(1), 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kushner%2C%20H.J.%20A%20new%20method%20of%20locating%20the%20maximum%20point%20of%20an%20arbitrary%20multipeak%20curve%20in%20the%20presence%20of%20noise",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kushner%2C%20H.J.%20A%20new%20method%20of%20locating%20the%20maximum%20point%20of%20an%20arbitrary%20multipeak%20curve%20in%20the%20presence%20of%20noise"
        },
        {
            "id": "35",
            "entry": "[35] C.J. Maddison, A. Mnih, and Y.W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "36",
            "entry": "[36] R. Martinez-Cantin. Bayesopt: A Bayesian optimization library for nonlinear optimization, experimental design and bandits. Journal of Machine Learning Research, 15(1), 2014. Techniques. 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martinez-Cantin%2C%20R.%20Bayesopt%3A%20A%20Bayesian%20optimization%20library%20for%20nonlinear%20optimization%2C%20experimental%20design%20and%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martinez-Cantin%2C%20R.%20Bayesopt%3A%20A%20Bayesian%20optimization%20library%20for%20nonlinear%20optimization%2C%20experimental%20design%20and%20bandits%202014"
        },
        {
            "id": "Conference._1975_a",
            "entry": "Conference. Springer, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Conference%201975"
        },
        {
            "id": "Journal_1994_a",
            "entry": "Journal of Global Optimization, 4(4), 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Journal%20of%20Global%20Optimization%2044%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Journal%20of%20Global%20Optimization%2044%201994"
        },
        {
            "id": "40",
            "entry": "[40] G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. An analysis of approximations for maximizing submodular set functions\u2014I. Mathematical Programming, 14(1), 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Fisher%2C%20M.L.%20An%20analysis%20of%20approximations%20for%20maximizing%20submodular%20set%20functions%E2%80%94I%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Fisher%2C%20M.L.%20An%20analysis%20of%20approximations%20for%20maximizing%20submodular%20set%20functions%E2%80%94I%201978"
        },
        {
            "id": "41",
            "entry": "[41] M.A. Osborne, R. Garnett, and S.J. Roberts. Gaussian processes for global optimization. In International Conference on Learning and Intelligent Optimization, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osborne%2C%20M.A.%20Garnett%2C%20R.%20Roberts%2C%20S.J.%20Gaussian%20processes%20for%20global%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osborne%2C%20M.A.%20Garnett%2C%20R.%20Roberts%2C%20S.J.%20Gaussian%20processes%20for%20global%20optimization%202009"
        },
        {
            "id": "42",
            "entry": "[42] D.J. Rezende, M. Shakir, and D. Wierstra. Stochastic backpropagation and variational inference in deep latent Gaussian models. In International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Shakir%2C%20M.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20variational%20inference%20in%20deep%20latent%20Gaussian%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Shakir%2C%20M.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20variational%20inference%20in%20deep%20latent%20Gaussian%20models%202014"
        },
        {
            "id": "43",
            "entry": "[43] A. Shah and Z. Ghahramani. Parallel predictive entropy search for batch global optimization of expensive objective functions. In Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shah%2C%20A.%20Ghahramani%2C%20Z.%20Parallel%20predictive%20entropy%20search%20for%20batch%20global%20optimization%20of%20expensive%20objective%20functions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shah%2C%20A.%20Ghahramani%2C%20Z.%20Parallel%20predictive%20entropy%20search%20for%20batch%20global%20optimization%20of%20expensive%20objective%20functions%202015"
        },
        {
            "id": "44",
            "entry": "[44] B. Shahriari, K. Swersky, Z. Wang, R.P. Adams, and N. de Freitas. Taking the human out of the loop: A Review of Bayesian Optimization. Proceedings of the IEEE, (1), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shahriari%2C%20B.%20Swersky%2C%20K.%20Wang%2C%20Z.%20Adams%2C%20R.P.%20Taking%20the%20human%20out%20of%20the%20loop%3A%20A%20Review%20of%20Bayesian%20Optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shahriari%2C%20B.%20Swersky%2C%20K.%20Wang%2C%20Z.%20Adams%2C%20R.P.%20Taking%20the%20human%20out%20of%20the%20loop%3A%20A%20Review%20of%20Bayesian%20Optimization%202016"
        },
        {
            "id": "45",
            "entry": "[45] J. Snoek, H. Larochelle, and R.P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "46",
            "entry": "[46] J.T. Springenberg, A. Klein, S. Falkner, and F. Hutter. Bayesian optimization with robust Bayesian neural networks. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20J.T.%20Klein%2C%20A.%20Falkner%2C%20S.%20Hutter%2C%20F.%20Bayesian%20optimization%20with%20robust%20Bayesian%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20J.T.%20Klein%2C%20A.%20Falkner%2C%20S.%20Hutter%2C%20F.%20Bayesian%20optimization%20with%20robust%20Bayesian%20neural%20networks%202016"
        },
        {
            "id": "47",
            "entry": "[47] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In International Conference on Machine Learning, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srinivas%2C%20N.%20Krause%2C%20A.%20Kakade%2C%20S.%20Seeger%2C%20M.%20Gaussian%20process%20optimization%20in%20the%20bandit%20setting%3A%20No%20regret%20and%20experimental%20design%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srinivas%2C%20N.%20Krause%2C%20A.%20Kakade%2C%20S.%20Seeger%2C%20M.%20Gaussian%20process%20optimization%20in%20the%20bandit%20setting%3A%20No%20regret%20and%20experimental%20design%202010"
        },
        {
            "id": "48",
            "entry": "[48] K. Swersky, J. Snoek, and R.P. Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Swersky%2C%20K.%20Snoek%2C%20J.%20Adams%2C%20R.P.%20Multi-task%20Bayesian%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Swersky%2C%20K.%20Snoek%2C%20J.%20Adams%2C%20R.P.%20Multi-task%20Bayesian%20optimization%202013"
        },
        {
            "id": "49",
            "entry": "[49] T. Ueno, T.D. Rhone, Z. Hou, T. Mizoguchi, and K. Tsuda. Combo: An efficient Bayesian optimization library for materials science. Materials discovery, 4, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ueno%2C%20T.%20Rhone%2C%20T.D.%20Hou%2C%20Z.%20Mizoguchi%2C%20T.%20Combo%3A%20An%20efficient%20Bayesian%20optimization%20library%20for%20materials%20science%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ueno%2C%20T.%20Rhone%2C%20T.D.%20Hou%2C%20Z.%20Mizoguchi%2C%20T.%20Combo%3A%20An%20efficient%20Bayesian%20optimization%20library%20for%20materials%20science%202016"
        },
        {
            "id": "50",
            "entry": "[50] F. Viana and R. Haftka. Surrogate-based optimization with parallel simulations using the probability of improvement. In AIAA/ISSMO Multidisciplinary Analysis Optimization Conference, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Viana%2C%20F.%20Haftka%2C%20R.%20Surrogate-based%20optimization%20with%20parallel%20simulations%20using%20the%20probability%20of%20improvement%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Viana%2C%20F.%20Haftka%2C%20R.%20Surrogate-based%20optimization%20with%20parallel%20simulations%20using%20the%20probability%20of%20improvement%202010"
        },
        {
            "id": "51",
            "entry": "[51] J. Wang, S.C. Clark, E. Liu, and P.I. Frazier. Parallel Bayesian global optimization of expensive functions. arXiv preprint arXiv:1602.05149, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.05149"
        },
        {
            "id": "52",
            "entry": "[52] Z. Wang and S. Jegelka. Max-value entropy search for efficient Bayesian optimization. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Jegelka%2C%20S.%20Max-value%20entropy%20search%20for%20efficient%20Bayesian%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Jegelka%2C%20S.%20Max-value%20entropy%20search%20for%20efficient%20Bayesian%20optimization%202017"
        },
        {
            "id": "53",
            "entry": "[53] J. Wu and P.I. Frazier. The parallel Knowledge Gradient method for batch Bayesian optimization. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20J.%20Frazier%2C%20P.I.%20The%20parallel%20Knowledge%20Gradient%20method%20for%20batch%20Bayesian%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20J.%20Frazier%2C%20P.I.%20The%20parallel%20Knowledge%20Gradient%20method%20for%20batch%20Bayesian%20optimization%202016"
        },
        {
            "id": "54",
            "entry": "[54] Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with gradients. In Advances in Neural Information Processing Systems, pages 5267\u20135278, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jian%20Poloczek%2C%20Matthias%20Wilson%2C%20Andrew%20G.%20Frazier%2C%20Peter%20Bayesian%20optimization%20with%20gradients%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jian%20Poloczek%2C%20Matthias%20Wilson%2C%20Andrew%20G.%20Frazier%2C%20Peter%20Bayesian%20optimization%20with%20gradients%202017"
        }
    ]
}
