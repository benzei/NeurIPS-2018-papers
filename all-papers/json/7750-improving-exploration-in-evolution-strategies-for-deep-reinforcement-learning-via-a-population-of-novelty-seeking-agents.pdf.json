{
    "filename": "7750-improving-exploration-in-evolution-strategies-for-deep-reinforcement-learning-via-a-population-of-novelty-seeking-agents.pdf",
    "metadata": {
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents",
        "author": "Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, Jeff Clune",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7750-improving-exploration-in-evolution-strategies-for-deep-reinforcement-learning-via-a-population-of-novelty-seeking-agents.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "evolution strategy",
            "url": "https://en.wikipedia.org/wiki/evolution_strategy"
        },
        {
            "term": "local optima",
            "url": "https://en.wikipedia.org/wiki/local_optima"
        },
        {
            "term": "Natural Evolution Strategies",
            "url": "https://en.wikipedia.org/wiki/Natural_Evolution_Strategies"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "Evolution strategies",
            "url": "https://en.wikipedia.org/wiki/Evolution_strategies"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "highlights": [
        "Evolution Strategies<br/><br/>Evolution strategies (ES) are a class of black box optimization algorithms inspired by natural evolution [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]: At every iteration, a population of parameter vectors is perturbed and, optionally, recombined via crossover",
        "For the first time, we study how these two types of algorithms can be hybridized with Evolution strategies to scale them to deep neural networks and tackle hard, high-dimensional deep reinforcement learning problems, without sacrificing the speed/scalability benefits of Evolution strategies",
        "Our experiments confirm that Novelty Search-Evolution strategies and two simple versions of quality diversity-Evolution strategies (NSR-Evolution strategies and NSRA-Evolution strategies) avoid local optima encountered by Evolution strategies and achieve higher performance on tasks ranging from simulated robots learning to walk around a deceptive trap to the high-dimensional pixel-to-action task of playing Atari games",
        "Evolution strategies was recently shown to be capable of training deep neural networks that can solve challenging, high-dimensional reinforcement learning tasks [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "To the best of our knowledge, this paper reports the first attempt at augmenting Evolution strategies to perform directed exploration in high-dimensional environments",
        "The latter scenario will likely hold for most challenging, real-world domains that machine learning practitioners will wish to tackle in the future"
    ],
    "key_statements": [
        "Evolution Strategies<br/><br/>Evolution strategies (ES) are a class of black box optimization algorithms inspired by natural evolution [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]: At every iteration, a population of parameter vectors is perturbed and, optionally, recombined via crossover",
        "For the first time, we study how these two types of algorithms can be hybridized with Evolution strategies to scale them to deep neural networks and tackle hard, high-dimensional deep reinforcement learning problems, without sacrificing the speed/scalability benefits of Evolution strategies",
        "We investigate algorithms that balance exploration and exploitation, specifically novel instances of quality diversity algorithms, which seek to produce a set of solutions that are both novel and high-performing [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>\u2013<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "We investigate adding Novelty Search and quality diversity to Evolution strategies only; in future work, we will investigate how they might be hybridized with Q-learning and policy gradient methods",
        "Our experiments confirm that Novelty Search-Evolution strategies and two simple versions of quality diversity-Evolution strategies (NSR-Evolution strategies and NSRA-Evolution strategies) avoid local optima encountered by Evolution strategies and achieve higher performance on tasks ranging from simulated robots learning to walk around a deceptive trap to the high-dimensional pixel-to-action task of playing Atari games",
        "Many algorithms in the Evolution strategies class differ in their representation of the population and methods of recombination; the algorithms subsequently referred to in this work belong to the class of Natural Evolution Strategies (NES) [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "Optimizing a linear combination of novelty and reward was previously explored in Cuccu and Gomez [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and Cuccu et al [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], but not with large neural networks on high-dimensional problems",
        "We explore a further extension of NSR-Evolution strategies called NSRAdapt-Evolution strategies (NSRA-Evolution strategies), which takes advantage of the opportunity to dynamically weight the priority given to the performance gradient f (\u2713ti,m) vs. the novelty gradient N (\u2713ti,m, A) by intelligently adapting a weighting parameter w during training",
        "We report the median reward across 5 independent runs of the best policy found in each run",
        "We focus on learning with a pre-defined, informative behavior characterization and leave the task of jointly learning a policy and latent representation of states for future work",
        "In this work we found it beneficial to keep novelty unnormalized, but further investigation into different behavior characterization designs could yield additional improvements",
        "Because NSR-Evolution strategies combines exploration with reward maximization, it is able to avoid local optima encountered by Evolution strategies while learning to play the game well",
        "Evolution strategies was recently shown to be capable of training deep neural networks that can solve challenging, high-dimensional reinforcement learning tasks [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "To the best of our knowledge, this paper reports the first attempt at augmenting Evolution strategies to perform directed exploration in high-dimensional environments",
        "The latter scenario will likely hold for most challenging, real-world domains that machine learning practitioners will wish to tackle in the future",
        "We introduce the NSRA-Evolution strategies algorithm, which attempts to invest in exploration only when necessary",
        "Our work shows that Evolution strategies is a rich and unexploited parallel path for deep reinforcement learning research"
    ],
    "summary": [
        "Evolution Strategies<br/><br/>Evolution strategies (ES) are a class of black box optimization algorithms inspired by natural evolution [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]: At every iteration, a population of parameter vectors is perturbed and, optionally, recombined via crossover.",
        "Our experiments confirm that NS-ES and two simple versions of QD-ES (NSR-ES and NSRA-ES) avoid local optima encountered by ES and achieve higher performance on tasks ranging from simulated robots learning to walk around a deceptive trap to the high-dimensional pixel-to-action task of playing Atari games.",
        "The algorithm encourages different behaviors by computing the novelty of the current policy with respect to previously generated policies and encourages the population distribution to move towards areas of parameter space with high novelty.",
        "After selecting an individual m from the meta-population, we compute the gradient of expected novelty with respect to m\u2019s current parameter vector, \u2713tm, and perform an update step :",
        "We train a variant of NS-ES, which we call NSR-ES, that combines the reward (\u201cfitness\") and novelty calculated for a given set of policy parameters \u2713.",
        "The algorithm follows the approximated gradient in parameter-space towards policies that both exhibit novel behaviors and achieve high rewards.",
        "Optimizing a linear combination of novelty and reward was previously explored in Cuccu and Gomez [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and Cuccu et al [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], but not with large neural networks on high-dimensional problems.",
        "NSR-ES and NSRA-ES experiments test the effectiveness of combining exploration and reward pressures on this difficult continuous control problem.",
        "Once stuck in the local optimum, the algorithm continually increases its pressure for novelty, allowing it to escape the deceptive trap and achieve much higher rewards than NS-ES (p < 0.01) and NSR-ES (p < 0.01).",
        "To demonstrate the effectiveness of NS-ES, NSR-ES, and NSRA-ES for local optima avoidance and directed exploration, we tested on 12 different games with varying levels of complexity, as defined by the taxonomy in Bellemare et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "Because NSR-ES combines exploration with reward maximization, it is able to avoid local optima encountered by ES while learning to play the game well.",
        "It\u2019s superior performance validates NSRA-ES as the best among the evolutionary algorithms considered and suggests that using an adaptive weighting between novelty and reward is a promising direction for future research.",
        "The Atari results illustrate that NS is an effective mechanism for encouraging directed exploration, given an appropriate behavior characterization, for complex, high-dimensional control tasks.",
        "Single-agent exploration, more systematically and on more domains, (2) investigating the best way to combine the merits of all of these options, and (3) hybridizing holistic and/or population-based exploration with other algorithms that work well on deep RL problems, such as policy gradients and DQN."
    ],
    "headline": "We show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search  and quality diversity  algorithms, can be hybridized with Evolution strategies to improve its performance on sparse or deceptive deep reinforcement learning tasks, while retaining scalability",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "2",
            "entry": "[2] Gunar E. Liepins and Michael D. Vose. Deceptiveness and genetic algorithm dynamics. Technical Report CONF-9007175-1, Oak Ridge National Lab., TN (USA); Tennessee Univ., Knoxville, TN (USA), 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liepins%2C%20Gunar%20E.%20Vose%2C%20Michael%20D.%20Deceptiveness%20and%20genetic%20algorithm%20dynamics%201990"
        },
        {
            "id": "3",
            "entry": "[3] Joel Lehman and Kenneth O. Stanley. Novelty search and the problem with objectives. In Genetic Programming Theory and Practice IX (GPTP 2011), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Novelty%20search%20and%20the%20problem%20with%20objectives%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Novelty%20search%20and%20the%20problem%20with%20objectives%202011"
        },
        {
            "id": "4",
            "entry": "[4] Kenji Kawaguchi. Deep learning without poor local minima. In NIPS, pages 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "5",
            "entry": "[5] Yann Dauphin, Razvan Pascanu, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. ArXiv e-prints, abs/1406.2572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.2572"
        },
        {
            "id": "6",
            "entry": "[6] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "7",
            "entry": "[7] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "8",
            "entry": "[8] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "9",
            "entry": "[9] J\u00fcrgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010). IEEE Transactions on Autonomous Mental Development, 2(3):230\u2013247, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%C3%BCrgen%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%2023230247%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%C3%BCrgen%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%2023230247%202010"
        },
        {
            "id": "10",
            "entry": "[10] Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in Neurorobotics, 1:6, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20a%20typology%20of%20computational%20approaches%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20a%20typology%20of%20computational%20approaches%202009"
        },
        {
            "id": "11",
            "entry": "[11] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In NIPS, pages 2750\u20132759, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "12",
            "entry": "[12] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In NIPS, pages 1471\u20131479, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "13",
            "entry": "[13] Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and R\u00e9mi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01310"
        },
        {
            "id": "14",
            "entry": "[14] Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00814"
        },
        {
            "id": "15",
            "entry": "[15] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In NIPS, pages 1109\u20131117, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houthooft%2C%20Rein%20Chen%2C%20Xi%20Duan%2C%20Yan%20Schulman%2C%20John%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Houthooft%2C%20Rein%20Chen%2C%20Xi%20Duan%2C%20Yan%20Schulman%2C%20John%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016"
        },
        {
            "id": "16",
            "entry": "[16] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. arXiv preprint arXiv:1705.05363, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.05363"
        },
        {
            "id": "17",
            "entry": "[17] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret. Robots that can adapt like animals. Nature, 521:503\u2013507, 2015. doi: 10.1038/nature14422.",
            "crossref": "https://dx.doi.org/10.1038/nature14422",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1038/nature14422"
        },
        {
            "id": "18",
            "entry": "[18] Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.04909"
        },
        {
            "id": "19",
            "entry": "[19] Justin K Pugh, Lisa B. Soros, and Kenneth O. Stanley. Quality diversity: A new frontier for evolutionary computation. 3(40), 2016. ISSN 2296-9144.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pugh%2C%20Justin%20K.%20Soros%2C%20Lisa%20B.%20Stanley%2C%20Kenneth%20O.%20Quality%20diversity%3A%20A%20new%20frontier%20for%20evolutionary%20computation.%203%2840%29%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pugh%2C%20Justin%20K.%20Soros%2C%20Lisa%20B.%20Stanley%2C%20Kenneth%20O.%20Quality%20diversity%3A%20A%20new%20frontier%20for%20evolutionary%20computation.%203%2840%29%202016"
        },
        {
            "id": "20",
            "entry": "[20] Joel Lehman and Kenneth O. Stanley. Evolving a diversity of virtual creatures through novelty search and local competition. In GECCO \u201911: Proceedings of the 13th annual conference on Genetic and evolutionary computation, pages 211\u2013218, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Evolving%20a%20diversity%20of%20virtual%20creatures%20through%20novelty%20search%20and%20local%20competition.%20In%20GECCO%E2%80%99%2011%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Evolving%20a%20diversity%20of%20virtual%20creatures%20through%20novelty%20search%20and%20local%20competition.%20In%20GECCO%E2%80%99%2011%202011"
        },
        {
            "id": "21",
            "entry": "[21] Roby Velez and Jeff Clune. Novelty search creates robots with general skills for exploration. In Proceedings of the 2014 Conference on Genetic and Evolutionary Computation, GECCO \u201914, pages 737\u2013744, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Velez%2C%20Roby%20Clune%2C%20Jeff%20Novelty%20search%20creates%20robots%20with%20general%20skills%20for%20exploration%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Velez%2C%20Roby%20Clune%2C%20Jeff%20Novelty%20search%20creates%20robots%20with%20general%20skills%20for%20exploration%202014"
        },
        {
            "id": "22",
            "entry": "[22] Joost Huizinga, Jean-Baptiste Mouret, and Jeff Clune. Does aligning phenotypic and genotypic modularity improve the evolution of neural networks? In Proceedings of the 2016 on Genetic and Evolutionary Computation Conference (GECCO), pages 125\u2013132, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huizinga%2C%20Joost%20Mouret%2C%20Jean-Baptiste%20Clune%2C%20Jeff%20Does%20aligning%20phenotypic%20and%20genotypic%20modularity%20improve%20the%20evolution%20of%20neural%20networks%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huizinga%2C%20Joost%20Mouret%2C%20Jean-Baptiste%20Clune%2C%20Jeff%20Does%20aligning%20phenotypic%20and%20genotypic%20modularity%20improve%20the%20evolution%20of%20neural%20networks%3F%202016"
        },
        {
            "id": "23",
            "entry": "[23] Ingo Rechenberg. Evolutionsstrategien. In Simulationsmethoden in der Medizin und Biologie, pages 83\u2013114. 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ingo%20Rechenberg%20Evolutionsstrategien%20In%20Simulationsmethoden%20in%20der%20Medizin%20und%20Biologie%20pages%2083114%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ingo%20Rechenberg%20Evolutionsstrategien%20In%20Simulationsmethoden%20in%20der%20Medizin%20und%20Biologie%20pages%2083114%201978"
        },
        {
            "id": "24",
            "entry": "[24] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "25",
            "entry": "[25] Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In Evolutionary Computation, 2008., pages 3381\u20133387, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wierstra%2C%20Daan%20Schaul%2C%20Tom%20Peters%2C%20Jan%20Schmidhuber%2C%20Juergen%20Natural%20evolution%20strategies%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wierstra%2C%20Daan%20Schaul%2C%20Tom%20Peters%2C%20Jan%20Schmidhuber%2C%20Juergen%20Natural%20evolution%20strategies%202008"
        },
        {
            "id": "26",
            "entry": "[26] Frank Sehnke, Christian Osendorfer, Thomas R\u00fcckstie\u00df, Alex Graves, Jan Peters, and J\u00fcrgen Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551\u2013559, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parameter-exploring%20policy%20gradients%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parameter-exploring%20policy%20gradients%202010"
        },
        {
            "id": "27",
            "entry": "[27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "28",
            "entry": "[28] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "29",
            "entry": "[29] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.10295"
        },
        {
            "id": "30",
            "entry": "[30] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "31",
            "entry": "[31] Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. arXiv preprint arXiv:1704.02399, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02399"
        },
        {
            "id": "32",
            "entry": "[32] Giuseppe Cuccu and Faustino Gomez. When novelty is not enough. In European Conference on the Applications of Evolutionary Computation, pages 234\u2013243.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuccu%2C%20Giuseppe%20Gomez%2C%20Faustino%20When%20novelty%20is%20not%20enough",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuccu%2C%20Giuseppe%20Gomez%2C%20Faustino%20When%20novelty%20is%20not%20enough"
        },
        {
            "id": "33",
            "entry": "[33] Giuseppe Cuccu, Faustino Gomez, and Tobias Glasmachers. Novelty-based restarts for evolution strategies. In Evolutionary Computation (CEC), 2011 IEEE Congress on, pages 158\u2013163. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuccu%2C%20Giuseppe%20Gomez%2C%20Faustino%20Glasmachers%2C%20Tobias%20Novelty-based%20restarts%20for%20evolution%20strategies%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuccu%2C%20Giuseppe%20Gomez%2C%20Faustino%20Glasmachers%2C%20Tobias%20Novelty-based%20restarts%20for%20evolution%20strategies%202011"
        },
        {
            "id": "34",
            "entry": "[34] Nikolaus Hansen, Sibylle D M\u00fcller, and Petros Koumoutsakos. Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). Evolutionary computation, 11(1):1\u201318, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20Nikolaus%20M%C3%BCller%2C%20Sibylle%20D.%20Koumoutsakos%2C%20Petros%20Reducing%20the%20time%20complexity%20of%20the%20derandomized%20evolution%20strategy%20with%20covariance%20matrix%20adaptation%20%28cma-es%29.%20Evolutionary%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20Nikolaus%20M%C3%BCller%2C%20Sibylle%20D.%20Koumoutsakos%2C%20Petros%20Reducing%20the%20time%20complexity%20of%20the%20derandomized%20evolution%20strategy%20with%20covariance%20matrix%20adaptation%20%28cma-es%29.%20Evolutionary%202003"
        },
        {
            "id": "35",
            "entry": "[35] Justin K Pugh, Lisa B Soros, Paul A Szerlip, and Kenneth O Stanley. Confronting the challenge of quality diversity. In Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation (GECCO), pages 967\u2013974, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pugh%2C%20Justin%20K.%20Soros%2C%20Lisa%20B.%20Szerlip%2C%20Paul%20A.%20Stanley%2C%20Kenneth%20O.%20Confronting%20the%20challenge%20of%20quality%20diversity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pugh%2C%20Justin%20K.%20Soros%2C%20Lisa%20B.%20Szerlip%2C%20Paul%20A.%20Stanley%2C%20Kenneth%20O.%20Confronting%20the%20challenge%20of%20quality%20diversity%202015"
        },
        {
            "id": "36",
            "entry": "[36] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greg%20Brockman%20Vicki%20Cheung%20Ludwig%20Pettersson%20Jonas%20Schneider%20John%20Schulman%20Jie%20Tang%20and%20Wojciech%20Zaremba%20Openai%20gym%202016"
        },
        {
            "id": "37",
            "entry": "[37] Yavar Naddaf. Game-independent ai agents for playing atari 2600 console games. 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naddaf%2C%20Yavar%20Game-independent%20ai%20agents%20for%20playing%20atari%202600%20console%20games%202010"
        },
        {
            "id": "38",
            "entry": "[38] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In The International Joint Conference on Neural Networks, pages 1\u20138. IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20Deep%20auto-encoder%20neural%20networks%20in%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lange%2C%20Sascha%20Riedmiller%2C%20Martin%20Deep%20auto-encoder%20neural%20networks%20in%20reinforcement%20learning%202010"
        },
        {
            "id": "39",
            "entry": "[39] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "40",
            "entry": "[40] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In NIPS, pages 4790\u20134798, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016"
        },
        {
            "id": "41",
            "entry": "[41] Christopher Stanton and Jeff Clune. Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime. PloS one, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanton%2C%20Christopher%20Clune%2C%20Jeff%20Curiosity%20search%3A%20producing%20generalists%20by%20encouraging%20individuals%20to%20continually%20explore%20and%20acquire%20skills%20throughout%20their%20lifetime%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanton%2C%20Christopher%20Clune%2C%20Jeff%20Curiosity%20search%3A%20producing%20generalists%20by%20encouraging%20individuals%20to%20continually%20explore%20and%20acquire%20skills%20throughout%20their%20lifetime%202016"
        },
        {
            "id": "42",
            "entry": "[42] Phillip Paquette. Super mario bros. in openai gym, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paquette%2C%20Phillip%20Super%20mario%20bros.%20in%20openai%20gym%202016"
        },
        {
            "id": "43",
            "entry": "[43] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Pascanu%2C%20Razvan%20Rabinowitz%2C%20Neil%20Veness%2C%20Joel%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017"
        },
        {
            "id": "44",
            "entry": "[44] Roby Velez and Jeff Clune. Diffusion-based neuromodulation can eliminate catastrophic forgetting in simple neural networks. arXiv preprint arXiv:1705.07241, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07241"
        },
        {
            "id": "45",
            "entry": "[45] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4): 128\u2013135, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=French%2C%20Robert%20M.%20Catastrophic%20forgetting%20in%20connectionist%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=French%2C%20Robert%20M.%20Catastrophic%20forgetting%20in%20connectionist%20networks%201999"
        },
        {
            "id": "46",
            "entry": "[46] Antoine Cully and Jean-Baptiste Mouret. Behavioral repertoire learning in robotics. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 175\u2013182. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cully%2C%20Antoine%20Mouret%2C%20Jean-Baptiste%20Behavioral%20repertoire%20learning%20in%20robotics%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cully%2C%20Antoine%20Mouret%2C%20Jean-Baptiste%20Behavioral%20repertoire%20learning%20in%20robotics%202013"
        },
        {
            "id": "47",
            "entry": "[47] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06295"
        },
        {
            "id": "48",
            "entry": "[48] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09846"
        },
        {
            "id": "49",
            "entry": "[49] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural networks. arXiv preprint arXiv:1703.00548, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00548"
        },
        {
            "id": "50",
            "entry": "[50] Jorge Gomes, Pedro Mariano, and Anders Lyhne Christensen. Systematic derivation of behaviour characterisations in evolutionary robotics. arXiv preprint arXiv:1407.0577, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.0577"
        },
        {
            "id": "51",
            "entry": "[51] Elliot Meyerson, Joel Lehman, and Risto Miikkulainen. Learning behavior characterizations for novelty search. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2016), Denver, Colorado, 2016. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meyerson%2C%20Elliot%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Learning%20behavior%20characterizations%20for%20novelty%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meyerson%2C%20Elliot%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Learning%20behavior%20characterizations%20for%20novelty%20search%202016"
        },
        {
            "id": "52",
            "entry": "[52] Joel Lehman and Kenneth O. Stanley. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary Computation, 19(2):189\u2013223, 2011. URL http://www.mitpressjournals.org/doi/pdf/10.1162/EVCO_a_00025.",
            "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/EVCO_a_00025",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20Joel%20Stanley%2C%20Kenneth%20O.%20Abandoning%20objectives%3A%20Evolution%20through%20the%20search%20for%20novelty%20alone%202011"
        },
        {
            "id": "53",
            "entry": "[53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "54",
            "entry": "[54] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "55",
            "entry": "[55] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. ",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        }
    ]
}
