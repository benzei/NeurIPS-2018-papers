{
    "filename": "7364-large-margin-deep-networks-for-classification.pdf",
    "metadata": {
        "title": "Large Margin Deep Networks for Classification",
        "author": "Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, Samy Bengio",
        "date": 1995,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7364-large-margin-deep-networks-for-classification.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a formulation of deep learning that aims at producing a large margin classifier. The notion of margin, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classification and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any lp norm (p \u2265 1) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classification loss functions. Specifically, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques such as weight decay, dropout, and batch norm. 2"
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "decision boundary",
            "url": "https://en.wikipedia.org/wiki/decision_boundary"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        },
        {
            "term": "weight decay",
            "url": "https://en.wikipedia.org/wiki/weight_decay"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "The large margin principle has played a key role in the course of machine learning history, producing remarkable theoretical and empirical results for classification (<a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\"><a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\">Vapnik, 1995</a></a>) and regression problems (<a class=\"ref-link\" id=\"cDrucker_et+al_1997_a\" href=\"#rDrucker_et+al_1997_a\"><a class=\"ref-link\" id=\"cDrucker_et+al_1997_a\" href=\"#rDrucker_et+al_1997_a\">Drucker et al, 1997</a></a>)",
        "We empirically evaluate our loss function on deep networks across different applications, datasets and model architectures",
        "We have presented a new loss function inspired by the theory of large margin that is amenable to deep network training",
        "This new loss is flexible and can establish a large margin that can be defined on input, hidden or output layers, and using l\u221e, l1, and l2 distance definitions",
        "The formulation is independent of network architecture and input domain and is complementary to other regularization techniques such as weight decay and dropout",
        "Our empirical results show the benefit of margin at the hidden layers of a network"
    ],
    "key_statements": [
        "The large margin principle has played a key role in the course of machine learning history, producing remarkable theoretical and empirical results for classification (<a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\"><a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\">Vapnik, 1995</a></a>) and regression problems (<a class=\"ref-link\" id=\"cDrucker_et+al_1997_a\" href=\"#rDrucker_et+al_1997_a\"><a class=\"ref-link\" id=\"cDrucker_et+al_1997_a\" href=\"#rDrucker_et+al_1997_a\">Drucker et al, 1997</a></a>)",
        "We empirically evaluate our loss function on deep networks across different applications, datasets and model architectures",
        "We show that the proposed loss function consistently outperforms baseline models trained with conventional losses, e.g. for adversarial perturbation, we outperform common baselines by up to 21% on MNIST, 14% on CIFAR-10 and 11% on Imagenet.\n2 Related Work",
        "Their setup and optimization are specific to the adversarial robustness scenario, whereas we consider generalization and noisy labels; their resulting loss function is computationally expensive and possibly difficult to scale to large problems such as Imagenet",
        "We show the white-box performance of the method from (<a class=\"ref-link\" id=\"cMadry_et+al_2017_a\" href=\"#rMadry_et+al_2017_a\">Madry et al, 2017</a>) 5, which is an algorithm specifically designed for adversarial defenses against Fast Gradient Sign Method attacks",
        "We have presented a new loss function inspired by the theory of large margin that is amenable to deep network training",
        "This new loss is flexible and can establish a large margin that can be defined on input, hidden or output layers, and using l\u221e, l1, and l2 distance definitions",
        "The formulation is independent of network architecture and input domain and is complementary to other regularization techniques such as weight decay and dropout",
        "Our empirical results show the benefit of margin at the hidden layers of a network"
    ],
    "summary": [
        "The large margin principle has played a key role in the course of machine learning history, producing remarkable theoretical and empirical results for classification (<a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\"><a class=\"ref-link\" id=\"cVapnik_1995_a\" href=\"#rVapnik_1995_a\">Vapnik, 1995</a></a>) and regression problems (<a class=\"ref-link\" id=\"cDrucker_et+al_1997_a\" href=\"#rDrucker_et+al_1997_a\"><a class=\"ref-link\" id=\"cDrucker_et+al_1997_a\" href=\"#rDrucker_et+al_1997_a\">Drucker et al, 1997</a></a>).",
        "Their setup and optimization are specific to the adversarial robustness scenario, whereas we consider generalization and noisy labels; their resulting loss function is computationally expensive and possibly difficult to scale to large problems such as Imagenet.",
        "We develop a novel margin-based loss function that uses this distance metric at multiple hidden layers, and show benefits for a wide range of problems.",
        "The first baseline model uses a cross-entropy loss function, trained with stochastic gradient descent optimization with momentum and learning rate decay.",
        "The large margin model has the same architecture as the baseline, but we use our new loss function in formulation (10).",
        "The cross-entropy and margin models trained on the 55, 000 sample training set achieves a test accuracy of 99.4% and the hinge loss model achieve 99.2%.",
        "The margin l2 model achieves a evaluation accuracy of 96.4% at 80% label noise, compared to 93.9% for cross-entropy.",
        "The all-layer l\u221e-margin model outperforms cross-entropy by around 3.7% in the smallest training set of 68 samples.",
        "It is seen that the margin models are robust against black-box attacks, significantly outperforming cross-entropy.",
        "We saw no benefit for the generalization or noisy label tasks from adversarial training - showing that this type of data augmentation provides very specific robustness.",
        "We achieve a baseline accuracy of around 90% for the following 5 models: cross-entropy, hinge, margin l\u221e, l1 and l26",
        "4.3.3 Adversarial Perturbations Fig. 5 shows the performance of cross-entropy and margin models for IFGSM attacks, for both white-box and black box scenarios.",
        "The l1 and l\u221e margin models perform well for both sets of attacks, giving a clear boost over cross-entropy.",
        "4.4 Imagenet We tested our l1 margin model against cross-entropy for a full-scale Imagenet model based on the Inception architecture (<a class=\"ref-link\" id=\"cSzegedy_et+al_2016_a\" href=\"#rSzegedy_et+al_2016_a\">Szegedy et al, 2016</a>), with data augmentation.",
        "We see that the margin model consistently outperforms cross-entropy for black and white box FGSM and IFGSM attacks.",
        "Note that our FGSM accuracy numbers on the cross-entropy model are quite close to that achieved in (<a class=\"ref-link\" id=\"cKurakin_et+al_2016_a\" href=\"#rKurakin_et+al_2016_a\">Kurakin et al, 2016</a>) (Table 2, top row); note that we use a wider range of in our experiments.",
        "We have presented a new loss function inspired by the theory of large margin that is amenable to deep network training.",
        "This new loss is flexible and can establish a large margin that can be defined on input, hidden or output layers, and using l\u221e, l1, and l2 distance definitions."
    ],
    "headline": "We present a formulation of deep learning that aims at producing a large margin classifier",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Abadi, Mart\u00edn, Barham, Paul, Chen, Jianmin, Chen, Zhifeng, Davis, Andy, Dean, Jeffrey, Devin, Matthieu, Ghemawat, Sanjay, Irving, Geoffrey, Isard, Michael, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pp. 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Athalye_2017_a",
            "entry": "Athalye, Anish and Sutskever, Ilya. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "Bousquet_2002_a",
            "entry": "Bousquet, Olivier and Elisseeff, Andr\u00e9. Stability and generalization. Journal of machine learning research, 2(Mar):499\u2013526, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andr%C3%A9%20Stability%20and%20generalization%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andr%C3%A9%20Stability%20and%20generalization%202002"
        },
        {
            "id": "Boyd_2004_a",
            "entry": "Boyd, Stephen and Vandenberghe, Lieven. Convex optimization. 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Convex%20optimization%202004"
        },
        {
            "id": "Cisse_et+al_2017_a",
            "entry": "Cisse, Moustapha, Bojanowski, Piotr, Grave, Edouard, Dauphin, Yann, and Usunier, Nicolas. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pp. 854\u2013863, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017"
        },
        {
            "id": "Cortes_1995_a",
            "entry": "Cortes, Corinna and Vapnik, Vladimir. Support-vector networks. Machine learning, 20(3):273\u2013297, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Vapnik%2C%20Vladimir%20Support-vector%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Vapnik%2C%20Vladimir%20Support-vector%20networks%201995"
        },
        {
            "id": "Drucker_et+al_1997_a",
            "entry": "Drucker, Harris, Burges, Chris J. C., Kaufman, Linda, Smola, Alex, and Vapnik, Vladimir. Support vector regression machines. In NIPS, pp. 155\u2013161. MIT Press, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drucker%2C%20Harris%20Burges%2C%20Chris%20J.C.%20Kaufman%2C%20Linda%20Smola%2C%20Alex%20Support%20vector%20regression%20machines%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drucker%2C%20Harris%20Burges%2C%20Chris%20J.C.%20Kaufman%2C%20Linda%20Smola%2C%20Alex%20Support%20vector%20regression%20machines%201997"
        },
        {
            "id": "Gal_et+al_2017_a",
            "entry": "Gal, Yarin, Islam, Riashat, and Ghahramani, Zoubin. Deep bayesian active learning with image data. arXiv preprint arXiv:1703.02910, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.02910"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Guo_et+al_2017_a",
            "entry": "Guo, Chuan, Rana, Mayank, Ciss\u00e9, Moustapha, and van der Maaten, Laurens. Countering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00117"
        },
        {
            "id": "Hein_2017_a",
            "entry": "Hein, Matthias and Andriushchenko, Maksym. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems, pp. 2266\u20132276, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "Hosseini_et+al_2017_a",
            "entry": "Hosseini, Hossein, Xiao, Baicen, and Poovendran, Radha. Google\u2019s cloud vision api is not robust to noise. arXiv preprint arXiv:1704.05051, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05051"
        },
        {
            "id": "Huang_et+al_2015_a",
            "entry": "Huang, Ruitong, Xu, Bing, Schuurmans, Dale, and Szepesv\u00e1ri, Csaba. Learning with a strong adversary. arXiv preprint arXiv:1511.03034, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.03034"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial Machine Learning at Scale. ArXiv e-prints, November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20A.%20Goodfellow%2C%20I.%20Bengio%2C%20S.%20Adversarial%20Machine%20Learning%20at%20Scale.%20ArXiv%20e-prints%202016-11"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "LeCun, Yann, Bottou, L\u00e9on, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "Liu_et+al_2016_a",
            "entry": "Liu, Weiyang, Wen, Yandong, Yu, Zhiding, and Yang, Meng. Large-margin softmax loss for convolutional neural networks. In ICML, pp. 507\u2013516, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Madry, Aleksander, Makelov, Aleksandar, Schmidt, Ludwig, Tsipras, Dimitris, and Vladu, Adrian. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Matyasko_2017_a",
            "entry": "Matyasko, Alexander and Chau, Lap-Pui. Margin maximization for robust classification using deep learning. In Neural Networks (IJCNN), 2017 International Joint Conference on, pp. 300\u2013307. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matyasko%2C%20Alexander%20Chau%2C%20Lap-Pui%20Margin%20maximization%20for%20robust%20classification%20using%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matyasko%2C%20Alexander%20Chau%2C%20Lap-Pui%20Margin%20maximization%20for%20robust%20classification%20using%20deep%20learning%202017"
        },
        {
            "id": "Moosavi-Dezfooli_et+al_2016_a",
            "entry": "Moosavi-Dezfooli, Seyed-Mohsen, Fawzi, Alhussein, Fawzi, Omar, and Frossard, Pascal. Universal adversarial perturbations. arXiv preprint arXiv:1610.08401, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.08401"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Papernot, Nicolas, McDaniel, Patrick, Goodfellow, Ian, Jha, Somesh, Celik, Z Berkay, and Swami, Ananthram. Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02697"
        },
        {
            "id": "Papernot_et+al_2017_a",
            "entry": "Papernot, Nicolas, McDaniel, Patrick, Goodfellow, Ian, Jha, Somesh, Celik, Z Berkay, and Swami, Ananthram. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Jha%2C%20Somesh%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Jha%2C%20Somesh%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "Rasmus_et+al_2015_a",
            "entry": "Rasmus, Antti, Berglund, Mathias, Honkala, Mikko, Valpola, Harri, and Raiko, Tapani. Semisupervised learning with ladder networks. In Advances in Neural Information Processing Systems, pp. 3546\u20133554, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semisupervised%20learning%20with%20ladder%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semisupervised%20learning%20with%20ladder%20networks%202015"
        },
        {
            "id": "Reed_et+al_2014_a",
            "entry": "Reed, Scott, Lee, Honglak, Anguelov, Dragomir, Szegedy, Christian, Erhan, Dumitru, and Rabinovich, Andrew. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6596"
        },
        {
            "id": "Sharif_et+al_2016_a",
            "entry": "Sharif, Mahmood, Bhagavatula, Sruti, Bauer, Lujo, and Reiter, Michael K. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 1528\u20131540. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "Sokolic_et+al_2016_a",
            "entry": "Sokolic, Jure, Giryes, Raja, Sapiro, Guillermo, and Rodrigues, Miguel R. D. Robust large margin deep neural networks. CoRR, abs/1605.08254, 2016. URL http://arxiv.org/abs/1605.08254.",
            "url": "http://arxiv.org/abs/1605.08254",
            "arxiv_url": "https://arxiv.org/pdf/1605.08254"
        },
        {
            "id": "Soudry_et+al_2017_a",
            "entry": "Soudry, Daniel, Hoffer, Elad, and Srebro, Nathan. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10345"
        },
        {
            "id": "Sukhbaatar_et+al_2014_a",
            "entry": "Sukhbaatar, Sainbayar, Bruna, Joan, Paluri, Manohar, Bourdev, Lubomir, and Fergus, Rob. Training convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.2080"
        },
        {
            "id": "Sun_et+al_2015_a",
            "entry": "Sun, Shizhao, Chen, Wei, Wang, Liwei, and Liu, Tie-Yan. Large margin deep neural networks: Theory and algorithms. CoRR, abs/1506.05232, 2015. URL http://arxiv.org/abs/1506.05232.",
            "url": "http://arxiv.org/abs/1506.05232",
            "arxiv_url": "https://arxiv.org/pdf/1506.05232"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian J., and Fergus, Rob. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL http://arxiv.org/abs/1312.6199.",
            "url": "http://arxiv.org/abs/1312.6199",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "Szegedy_et+al_2016_a",
            "entry": "Szegedy, Christian, Vanhoucke, Vincent, Ioffe, Sergey, Shlens, Jon, and Wojna, Zbigniew. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "Tieleman_2012_a",
            "entry": "Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20Tijmen%20Hinton%2C%20Geoffrey%20Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "Vapnik_1995_a",
            "entry": "Vapnik, Vladimir N. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc., New York, NY, USA, 1995. ISBN 0-387-94559-8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20N.%20The%20Nature%20of%20Statistical%20Learning%20Theory%201995"
        },
        {
            "id": "Vinyals_et+al_2016_a",
            "entry": "Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630\u20133638, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Zagoruyko, Sergey and Komodakis, Nikos. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        }
    ]
}
