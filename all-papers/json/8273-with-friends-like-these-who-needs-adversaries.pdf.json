{
    "filename": "8273-with-friends-like-these-who-needs-adversaries.pdf",
    "metadata": {
        "title": "With Friends Like These, Who Needs Adversaries?",
        "author": "Saumya Jetley, Nicholas Lord, Philip Torr",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8273-with-friends-like-these-who-needs-adversaries.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The vulnerability of deep image classification networks to adversarial attack is now well known, but less well understood. Via a novel experimental analysis, we illustrate some facts about deep convolutional networks for image classification that shed new light on their behaviour and how it connects to the problem of adversaries. In short, the celebrated performance of these networks and their vulnerability to adversarial attack are simply two sides of the same coin: the input image-space directions along which the networks are most vulnerable to attack are the same directions which they use to achieve their classification performance in the first place. We develop this result in two main steps. The first uncovers the fact that classes tend to be associated with specific image-space directions. This is shown by an examination of the class-score outputs of nets as functions of 1D movements along these directions. This provides a novel perspective on the existence of universal adversarial perturbations. The second is a clear demonstration of the tight coupling between classification performance and vulnerability to adversarial attack within the spaces spanned by these directions. Thus, our analysis resolves the apparent contradiction between accuracy and vulnerability. It provides a new perspective on much of the prior art and reveals profound implications for efforts to construct neural nets that are both accurate and robust to adversarial attack.2"
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        }
    ],
    "highlights": [
        "Those studying deep networks find themselves forced to confront an apparent paradox",
        "A common understanding of the issue can be stated as follows: \u201cWhile deep networks have proven their ability to distinguish between their target classes so as to generalise over unseen natural variations, they curiously possess an Achilles heel which must be defended.\u201d efforts to formulate attacks and counteracting defences of networks have led to a dedicated competition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and a body of literature already too vast to summarise in total",
        "Our work examines the nature of Fc as a function of movement s in specific image-space directions dj starting from randomly sampled natural images in, for a variety of classification deep convolutional networks",
        "We illustrate a close relationship between these directions and class identity: many such directions effectively encode the extent to which the net believes that a particular target class is or is not present",
        "As it stands, the predictive power and adversarial vulnerability of studied nets are intertwined owing to the fact that they base their classification decisions on rather simplistic responses to components of the input images in specific directions, irrespective of whether the source of those components is natural or adversarial",
        "We believe that for any scheme to be truly effective against the problem of adversarial vulnerability, it must lead to a fundamentally more insightful use of features than presently occurs"
    ],
    "key_statements": [
        "Those studying deep networks find themselves forced to confront an apparent paradox",
        "A common understanding of the issue can be stated as follows: \u201cWhile deep networks have proven their ability to distinguish between their target classes so as to generalise over unseen natural variations, they curiously possess an Achilles heel which must be defended.\u201d efforts to formulate attacks and counteracting defences of networks have led to a dedicated competition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and a body of literature already too vast to summarise in total",
        "Our work examines the nature of Fc as a function of movement s in specific image-space directions dj starting from randomly sampled natural images in, for a variety of classification deep convolutional networks",
        "We illustrate a close relationship between these directions and class identity: many such directions effectively encode the extent to which the net believes that a particular target class is or is not present",
        "As it stands, the predictive power and adversarial vulnerability of studied nets are intertwined owing to the fact that they base their classification decisions on rather simplistic responses to components of the input images in specific directions, irrespective of whether the source of those components is natural or adversarial",
        "We believe that for any scheme to be truly effective against the problem of adversarial vulnerability, it must lead to a fundamentally more insightful use of features than presently occurs"
    ],
    "summary": [
        "Those studying deep networks find themselves forced to confront an apparent paradox.",
        "Our work examines the nature of Fc as a function of movement s in specific image-space directions dj starting from randomly sampled natural images in, for a variety of classification DCNs. With this novel analysis, we uncover three noteworthy observations about these functions that relate directly to the phenomenon of adversarial vulnerability in these nets, all of which are on display in Fig. 1.",
        "They illustrate in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] why a hypothetical network which possessed this property would theoretically be predicted to be vulnerable to universal adversaries, and note that the analysis suggests a direct construction method for such adversaries as an alternative to the original randomised iterative approach of [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]: they can be constructed as random vectors in the subspace of shared high-curvature dimensions.",
        "The analysis begins as in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], with the extraction of the principal directions and principal curvatures of the classifier\u2019s image-space class decision boundaries.",
        "To confirm the above hypothesis regarding the relative importance of different image-space directions for classification, we plot the training and test accuracies of a sample of nets as a function of the subspace onto which their input images are projected.",
        "Since it is the high-curvature directions that are largely responsible for determining the nets\u2019 classification decisions, the nets should be vulnerable to adversarial attack along precisely these directions.",
        "While Fig. 3 displays the magnitudes of components of pre-computed adversarial perturbations in different subspaces, we design a variation on the analysis to illustrate how effective an efficient attack method (DeepFool) is when confined to the respective subspaces.",
        "Any attempt to mitigate adversarial vulnerability by discarding these directions, either by compressing the input data [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] or by suppressing specific components of image representations at intermediate network layers [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], must effect a loss in the classification accuracy.",
        "Our framework anticipates the fact that the nets must remain just as vulnerable to attack along the remaining directions that continue to determine classification decisions, given that the corresponding class-score functions, which possess the properties discussed earlier, remain unchanged.",
        "The resulting singular vectors span the entire space of classification/adversarial directions of the corresponding resampling network, as can be seen from the accuracy values in the rightmost subplot of Fig. 4.",
        "As it stands, the predictive power and adversarial vulnerability of studied nets are intertwined owing to the fact that they base their classification decisions on rather simplistic responses to components of the input images in specific directions, irrespective of whether the source of those components is natural or adversarial.",
        "Those features will continue to be the nets\u2019 own worst adversaries"
    ],
    "headline": "We develop this result in two main steps",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] NIPS: 2017 competition on adversarial attacks and defenses. https://www.kaggle.com/nips-2017-adversarial-learning-competition (2017)accessed:2018-03-12.",
            "url": "https://www.kaggle.com/nips-2017-adversarial-learning-competition"
        },
        {
            "id": "2",
            "entry": "[2] Moosavi-Dezfooli*, S.M., Fawzi*, A., Fawzi, O., Frossard, P., Soatto, S.: Robustness of classifiers to universal perturbations: A geometric perspective. In: International Conference on Learning Representations. (2018)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2A%2C%20S.M.%20Fawzi%2A%2C%20A.%20Fawzi%2C%20O.%20Frossard%2C%20P.%20S.%3A%20Robustness%20of%20classifiers%20to%20universal%20perturbations%3A%20A%20geometric%20perspective%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2A%2C%20S.M.%20Fawzi%2A%2C%20A.%20Fawzi%2C%20O.%20Frossard%2C%20P.%20S.%3A%20Robustness%20of%20classifiers%20to%20universal%20perturbations%3A%20A%20geometric%20perspective%202018"
        },
        {
            "id": "3",
            "entry": "[3] Lin, M., Chen, Q., Yan, S.: Network in network. International Conference on Learning Representations (2013)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20M.%20Chen%2C%20Q.%20Yan%20S.%3A%20Network%20in%20network%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20M.%20Chen%2C%20Q.%20Yan%20S.%3A%20Network%20in%20network%202013"
        },
        {
            "id": "4",
            "entry": "[4] Moosavi-Dezfooli*, S.M., Fawzi*, A., Fawzi, O., Frossard, P.: Universal adversarial perturbations. In: Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, IEEE (2017) 86\u201394",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2A%2C%20S.M.%20Fawzi%2A%2C%20A.%20Fawzi%2C%20O.%20Frossard%20P.%3A%20Universal%20adversarial%20perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2A%2C%20S.M.%20Fawzi%2A%2C%20A.%20Fawzi%2C%20O.%20Frossard%20P.%3A%20Universal%20adversarial%20perturbations%202017"
        },
        {
            "id": "5",
            "entry": "[5] Gao, J., Wang, B., Lin, Z., Xu, W., Qi, Y.: Deepcloak: Masking deep neural network models for robustness against adversarial samples. In: International Conference on Learning Representations. (2017)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20J.%20Wang%2C%20B.%20Lin%2C%20Z.%20Xu%2C%20W.%20Y.%3A%20Deepcloak%3A%20Masking%20deep%20neural%20network%20models%20for%20robustness%20against%20adversarial%20samples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20J.%20Wang%2C%20B.%20Lin%2C%20Z.%20Xu%2C%20W.%20Y.%3A%20Deepcloak%3A%20Masking%20deep%20neural%20network%20models%20for%20robustness%20against%20adversarial%20samples%202017"
        },
        {
            "id": "6",
            "entry": "[6] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R.: Intriguing properties of neural networks. In: International Conference on Learning Representations. (2014)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Zaremba%2C%20W.%20Sutskever%2C%20I.%20Bruna%2C%20J.%20R.%3A%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Zaremba%2C%20W.%20Sutskever%2C%20I.%20Bruna%2C%20J.%20R.%3A%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "7",
            "entry": "[7] Goodfellow, I., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. In: International Conference on Learning Representations. (2015)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Shlens%2C%20J.%20Szegedy%20C.%3A%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Shlens%2C%20J.%20Szegedy%20C.%3A%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "8",
            "entry": "[8] Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P.: Deepfool: a simple and accurate method to fool deep neural networks. In: Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Number EPFL-CONF-218057 (2016)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.M.%20Fawzi%2C%20A.%20Frossard%20P.%3A%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20S.M.%20Fawzi%2C%20A.%20Frossard%20P.%3A%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "9",
            "entry": "[9] Sabour*, S., Cao*, Y., Faghri, F., Fleet, D.J.: Adversarial manipulation of deep representations. In: International Conference on Learning Representations. (2016)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sabour%2A%2C%20S.%20Cao%2A%2C%20Y.%20Faghri%2C%20F.%20Fleet%2C%20D.%20J.%3A%20Adversarial%20manipulation%20of%20deep%20representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sabour%2A%2C%20S.%20Cao%2A%2C%20Y.%20Faghri%2C%20F.%20Fleet%2C%20D.%20J.%3A%20Adversarial%20manipulation%20of%20deep%20representations%202016"
        },
        {
            "id": "10",
            "entry": "[10] Nguyen, A., Yosinski, J., Clune, J.: Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2015) 427\u2013436",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20A.%20Yosinski%2C%20J.%20Clune%20J.%3A%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20A.%20Yosinski%2C%20J.%20Clune%20J.%3A%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015"
        },
        {
            "id": "11",
            "entry": "[11] Stanley, K.O.: Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines 8(2) (2007) 131\u2013162",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20K.%20O.%3A%20Compositional%20pattern%20producing%20networks%3A%20A%20novel%20abstraction%20of%20development.%20Genetic%20programming%20and%20evolvable%20machines%208%282%29%202007"
        },
        {
            "id": "12",
            "entry": "[12] Wang, B., Gao, J., Qi, Y.: A theoretical framework for robustness of (deep) classifiers under adversarial noise. arXiv preprint arXiv:1612.00334 (2016)",
            "arxiv_url": "https://arxiv.org/pdf/1612.00334"
        },
        {
            "id": "13",
            "entry": "[13] Maharaj, A.V.: Improving the adversarial robustness of convnets by reduction of input dimensionality (2015)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maharaj%2C%20A.%20V.%3A%20Improving%20the%20adversarial%20robustness%20of%20convnets%20by%20reduction%20of%20input%20dimensionality%202015"
        },
        {
            "id": "14",
            "entry": "[14] Das, N., Shanbhogue, M., Chen, S., Hohman, F., Chen, L., Kounavis, M.E., Chau, D.H.: Keeping the bad guys out: Protecting and vaccinating deep learning with JPEG compression. CoRR abs/1705.02900 (2017)",
            "arxiv_url": "https://arxiv.org/pdf/1705.02900"
        },
        {
            "id": "15",
            "entry": "[15] Xie, C., Wang, J., Zhang, Z., Ren, Z., Yuille, A.L.: Mitigating adversarial effects through randomization. CoRR abs/1711.01991 (2017)",
            "arxiv_url": "https://arxiv.org/pdf/1711.01991"
        },
        {
            "id": "16",
            "entry": "[16] Tanay, T., Griffin, L.: A boundary tilting persepective on the phenomenon of adversarial examples. arXiv preprint arXiv:1608.07690 (2016)",
            "arxiv_url": "https://arxiv.org/pdf/1608.07690"
        },
        {
            "id": "17",
            "entry": "[17] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. International Conference on Learning Representations (2018)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20A.%3A%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20A.%3A%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "18",
            "entry": "[18] Lu, J., Issaranon, T., Forsyth, D.: Safetynet: Detecting and rejecting adversarial examples robustly. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2017) 446\u2013454",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20J.%20Issaranon%2C%20T.%20Forsyth%20D.%3A%20Safetynet%3A%20Detecting%20and%20rejecting%20adversarial%20examples%20robustly%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20J.%20Issaranon%2C%20T.%20Forsyth%20D.%3A%20Safetynet%3A%20Detecting%20and%20rejecting%20adversarial%20examples%20robustly%202017"
        },
        {
            "id": "19",
            "entry": "[19] Metzen, J.H., Genewein, T., Fischer, V., Bischoff, B.: On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267 (2017)",
            "arxiv_url": "https://arxiv.org/pdf/1702.04267"
        },
        {
            "id": "20",
            "entry": "[20] Zhang, H., Cisse, M., Dauphin, Y., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. In: International Conference on Learning Representations. (2018)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Cisse%2C%20M.%20Dauphin%2C%20Y.%20Lopez-Paz%20D.%3A%20mixup%3A%20Beyond%20empirical%20risk%20minimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Cisse%2C%20M.%20Dauphin%2C%20Y.%20Lopez-Paz%20D.%3A%20mixup%3A%20Beyond%20empirical%20risk%20minimization%202018"
        },
        {
            "id": "21",
            "entry": "[21] Fawzi*, A., Moosavi-Dezfooli*, S.M., Frossard, P.: Robustness of classifiers: from adversarial to random noise. In: Advances in Neural Information Processing Systems. (2016) 1632\u20131640",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2A%2C%20A.%20Moosavi-Dezfooli%2A%2C%20S.M.%20Frossard%20P.%3A%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2A%2C%20A.%20Moosavi-Dezfooli%2A%2C%20S.M.%20Frossard%20P.%3A%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016"
        },
        {
            "id": "22",
            "entry": "[22] Fawzi*, A., Moosavi-Dezfooli*, S.M., Frossard, P., Soatto, S.: Classification regions of deep neural networks. arXiv preprint arXiv:1705.09552 (2017)",
            "arxiv_url": "https://arxiv.org/pdf/1705.09552"
        },
        {
            "id": "23",
            "entry": "[23] Fawzi, A., Moosavi-Dezfooli, S.M., Frossard, P.: The robustness of deep networks: A geometrical perspective. IEEE Signal Processing Magazine 34(6) (2017) 50\u201362",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20A.%20Moosavi-Dezfooli%2C%20S.M.%20Frossard%20P.%3A%20The%20robustness%20of%20deep%20networks%3A%20A%20geometrical%20perspective%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20A.%20Moosavi-Dezfooli%2C%20S.M.%20Frossard%20P.%3A%20The%20robustness%20of%20deep%20networks%3A%20A%20geometrical%20perspective%202017"
        },
        {
            "id": "24",
            "entry": "[24] Simonyan, K., Vedald, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013)",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "25",
            "entry": "[25] Russakovsky*, O., Deng*, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115(3) (2015) 211\u2013252 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2A%2C%20O.%20Deng%2A%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20L.%3A%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2A%2C%20O.%20Deng%2A%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20L.%3A%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015"
        }
    ]
}
