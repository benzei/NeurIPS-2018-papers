{
    "filename": "7995-scalar-posterior-sampling-with-applications.pdf",
    "metadata": {
        "title": "Scalar Posterior Sampling with Applications",
        "author": "Georgios Theocharous, Zheng Wen, Yasin Abbasi, Nikos Vlassis",
        "date": 1933,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7995-scalar-posterior-sampling-with-applications.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose a practical non-episodic PSRL algorithm that unlike recent state-of-theart PSRL algorithms uses a deterministic, model-independent episode switching schedule. Our algorithm termed deterministic schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space complexity. We prove a Bayesian regret bound under mild assumptions. Our result is more generally applicable to multiple parameters and continuous state action problems. We compare our algorithm with state-of-the-art PSRL algorithms on standard discrete and continuous problems from the literature. Finally, we show how the assumptions of our algorithm satisfy a sensible parametrization for a large class of problems in sequential recommendations."
    },
    "keywords": [
        {
            "term": "PSRL",
            "url": "https://en.wikipedia.org/wiki/PSRL"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "van roy",
            "url": "https://en.wikipedia.org/wiki/Van_Roy"
        },
        {
            "term": "thompson sampling",
            "url": "https://en.wikipedia.org/wiki/thompson_sampling"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "Thompson sampling [Thompson, 1933], or posterior sampling for reinforcement learning (PSRL), is a conceptually simple approach to deal with unknown Markov decision process [<a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\"><a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\"><a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\">Strens, 2000</a></a></a>; <a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\">Osband et al, 2013</a></a></a></a></a>]",
        "Thompson sampling [Thompson, 1933], or posterior sampling for reinforcement learning (PSRL), is a conceptually simple approach to deal with unknown Markov decision process [<a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\">Strens, 2000</a>; <a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\">Osband et al, 2013</a>]",
        "We proposed a practical general case posterior sampling for Reinforcement learning algorithm, called deterministic schedule posterior sampling for Reinforcement learning with provable guarantees",
        "Our result is more generally applicable to continuous state-action problems; when dynamics of the system is parametrized by a scalar, our regret is independent of the number of states",
        "We demonstrated empirically how the algorithm outperforms state-of-the-art posterior sampling for Reinforcement learning algorithms",
        "We showed how the assumptions satisfy a sensible parametrization for a large class of problems in sequential recommendations"
    ],
    "key_statements": [
        "Thompson sampling [Thompson, 1933], or posterior sampling for reinforcement learning (PSRL), is a conceptually simple approach to deal with unknown Markov decision process [<a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\"><a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\"><a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\">Strens, 2000</a></a></a>; <a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\">Osband et al, 2013</a></a></a></a></a>]",
        "Thompson sampling [Thompson, 1933], or posterior sampling for reinforcement learning (PSRL), is a conceptually simple approach to deal with unknown Markov decision process [<a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\">Strens, 2000</a>; <a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\">Osband et al, 2013</a>]",
        "posterior sampling for Reinforcement learning begins with a prior distribution over the Markov decision process model parameters and typically works in episodes",
        "At the start of each episode, an Markov decision process model is sampled from the posterior belief and the agent follows the policy that is optimal for that sampled Markov decision process until the end of the episode",
        "Despite the elegant regret bounds for the general case posterior sampling for Reinforcement learning algorithms developed in [<a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\">Ouyang et al, 2017b</a>; <a class=\"ref-link\" id=\"cAgrawal_2017_a\" href=\"#rAgrawal_2017_a\">Agrawal and Jia, 2017</a>], both of them focus on tabular reinforcement learning and are sample inefficient for many practical problems with exponentially large or even continuous state/action spaces",
        "We prove an bound for deterministic schedule posterior sampling for Reinforcement learning, assuming we can model every Markov decision process with a single smooth parameter",
        "The Bayes regret analysis of posterior sampling for Reinforcement learning relies on the key observation that at each stopping time \u2327 the true Markov decision process model \u2713\u21e4 and the sampled model \u2713e\u2327 are identically distributed [<a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\">Ouyang et al, 2017b</a>]",
        "We proposed a practical general case posterior sampling for Reinforcement learning algorithm, called deterministic schedule posterior sampling for Reinforcement learning with provable guarantees",
        "Our result is more generally applicable to continuous state-action problems; when dynamics of the system is parametrized by a scalar, our regret is independent of the number of states",
        "We demonstrated empirically how the algorithm outperforms state-of-the-art posterior sampling for Reinforcement learning algorithms",
        "We showed how the assumptions satisfy a sensible parametrization for a large class of problems in sequential recommendations"
    ],
    "summary": [
        "Thompson sampling [Thompson, 1933], or posterior sampling for reinforcement learning (PSRL), is a conceptually simple approach to deal with unknown MDPs [<a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\"><a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\"><a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\">Strens, 2000</a></a></a>; <a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\">Osband et al, 2013</a></a></a></a></a>].",
        "A recent general case PSRL algorithm with Bayes regret analysis was proposed in [<a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\">Ouyang et al, 2017b</a></a></a></a>].",
        "At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters.",
        "Both of the above two recent state-of-the-art algorithms [<a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\">Ouyang et al, 2017b</a></a></a></a>; <a class=\"ref-link\" id=\"cAgrawal_2017_a\" href=\"#rAgrawal_2017_a\"><a class=\"ref-link\" id=\"cAgrawal_2017_a\" href=\"#rAgrawal_2017_a\">Agrawal and Jia, 2017</a></a>], do not use generalization, in that they learn separate parameters for each state-action pair.",
        "Despite the elegant regret bounds for the general case PSRL algorithms developed in [<a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\">Ouyang et al, 2017b</a></a></a></a>; <a class=\"ref-link\" id=\"cAgrawal_2017_a\" href=\"#rAgrawal_2017_a\"><a class=\"ref-link\" id=\"cAgrawal_2017_a\" href=\"#rAgrawal_2017_a\">Agrawal and Jia, 2017</a></a>], both of them focus on tabular reinforcement learning and are sample inefficient for many practical problems with exponentially large or even continuous state/action spaces.",
        "In the rest of the paper we will describe the DS-PSRL algorithm, and derive a state-of-the-art Bayes regret analysis.",
        "We will show how the assumptions of our algorithm satisfy a sensible parametrization for a large class of problems in sequential recommendations.",
        "The Bayes regret analysis of PSRL relies on the key observation that at each stopping time \u2327 the true MDP model \u2713\u21e4 and the sampled model \u2713e\u2327 are identically distributed [<a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\"><a class=\"ref-link\" id=\"cOuyang_et+al_2017_b\" href=\"#rOuyang_et+al_2017_b\">Ouyang et al, 2017b</a></a></a></a>].",
        "We propose a PSRL algorithm with a deterministic policy update schedule, shown in Figure 1.",
        "Because the algorithm takes the optimal action with respect to parameter \u2713et and at is the action at time t, the right-hand side of the above equation is minimized and h i",
        "We assumed the true model of the world was \u2713\u21e4 = \u27132 and that the agent starts in the left-most state.",
        "In a final experiment we tested the ability of DS-PSRL algorithm in continuous state and action domains.",
        "We compared DS-PSRL with t-mod-1 and a recent TSDE algorithm for learning-based control of unknown linear systems with Thompson Sampling Ouyang et al [2017a].",
        "When there are multiple model one can use online algorithms, such as posterior sampling for Reinforcement learning (PSRL) to identify the best model for a new user [<a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\"><a class=\"ref-link\" id=\"cStrens_2000_a\" href=\"#rStrens_2000_a\">Strens, 2000</a></a>; <a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\"><a class=\"ref-link\" id=\"cOsband_et+al_2013_a\" href=\"#rOsband_et+al_2013_a\">Osband et al, 2013</a></a>].",
        "Our result is more generally applicable to continuous state-action problems; when dynamics of the system is parametrized by a scalar, our regret is independent of the number of states.",
        "The algorithm provides for generalization, and uses a deterministic policy switching schedule of logarithmic order, which is independent from the true model of the world.",
        "We showed how the assumptions satisfy a sensible parametrization for a large class of problems in sequential recommendations"
    ],
    "headline": "We propose a practical non-episodic posterior sampling for Reinforcement learning algorithm that unlike recent state-of-theart posterior sampling for Reinforcement learning algorithms uses a deterministic, model-independent episode switching schedule",
    "reference_links": [
        {
            "id": "Abbasi-Yadkori_2011_a",
            "entry": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret bounds for the adaptive control of linear quadratic systems. In COLT, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Regret%20bounds%20for%20the%20adaptive%20control%20of%20linear%20quadratic%20systems%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Regret%20bounds%20for%20the%20adaptive%20control%20of%20linear%20quadratic%20systems%202011"
        },
        {
            "id": "Abbasi-Yadkori_2015_a",
            "entry": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Bayesian optimal control of smoothly parameterized systems. In UAI, pages 1\u201311, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Bayesian%20optimal%20control%20of%20smoothly%20parameterized%20systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Bayesian%20optimal%20control%20of%20smoothly%20parameterized%20systems%202015"
        },
        {
            "id": "Agrawal_2017_a",
            "entry": "Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret bounds. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017"
        },
        {
            "id": "Bartlett_2009_a",
            "entry": "Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In UAI, pages 35\u201342, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Tewari%2C%20Ambuj%20Regal%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20MDPs%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Tewari%2C%20Ambuj%20Regal%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20MDPs%202009"
        },
        {
            "id": "Bertsekas_1995_a",
            "entry": "Dimitri P Bertsekas. Dynamic programming and optimal control, volume 2. Athena Scientific Belmont, MA, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Dynamic%20programming%20and%20optimal%20control%2C%20volume%202.%20Athena%20Scientific%201995"
        },
        {
            "id": "Ronen_2002_a",
            "entry": "Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213\u2013231, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Ronen%20I%20Brafman%20and%20Moshe%20Tennenholtz.%20R-max-a%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Ronen%20I%20Brafman%20and%20Moshe%20Tennenholtz.%20R-max-a%20general%20polynomial%20time%20algorithm%20for%20nearoptimal%20reinforcement%20learning%202002"
        },
        {
            "id": "Gopalan_2015_a",
            "entry": "Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized Markov decision processes. In COLT, pages 861\u2013898, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopalan%2C%20Aditya%20Mannor%2C%20Shie%20Thompson%20sampling%20for%20learning%20parameterized%20Markov%20decision%20processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopalan%2C%20Aditya%20Mannor%2C%20Shie%20Thompson%20sampling%20for%20learning%20parameterized%20Markov%20decision%20processes%202015"
        },
        {
            "id": "Jaksch_et+al_2010_a",
            "entry": "Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563\u20131600, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010"
        },
        {
            "id": "Osband_2014_a",
            "entry": "Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. In NIPS, pages 1466\u20131474, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Model-based%20reinforcement%20learning%20and%20the%20eluder%20dimension%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Model-based%20reinforcement%20learning%20and%20the%20eluder%20dimension%202014"
        },
        {
            "id": "Osband_2016_a",
            "entry": "Ian Osband and Benjamin Van Roy. Posterior sampling for reinforcement learning without episodes. arXiv preprint arXiv:1608.02731, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.02731"
        },
        {
            "id": "Osband_et+al_2013_a",
            "entry": "Ian Osband, Dan Russo, and Benjamin Van Roy. (More) efficient reinforcement learning via posterior sampling. In NIPS, pages 3003\u20133011, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Russo%2C%20Dan%20Roy%2C%20Benjamin%20Van%20%28More%29%20efficient%20reinforcement%20learning%20via%20posterior%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Russo%2C%20Dan%20Roy%2C%20Benjamin%20Van%20%28More%29%20efficient%20reinforcement%20learning%20via%20posterior%20sampling%202013"
        },
        {
            "id": "Ouyang_et+al_2017_a",
            "entry": "Yi Ouyang, Mukul Gagrani, and Rahul Jain. Learning-based control of unknown linear systems with thompson sampling. arXiv preprint arXiv:1709.04047, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04047"
        },
        {
            "id": "Ouyang_et+al_2017_b",
            "entry": "Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning unknown Markov decision processes: A thompson sampling approach. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ouyang%2C%20Yi%20Gagrani%2C%20Mukul%20Nayyar%2C%20Ashutosh%20Jain%2C%20Rahul%20Learning%20unknown%20Markov%20decision%20processes%3A%20A%20thompson%20sampling%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ouyang%2C%20Yi%20Gagrani%2C%20Mukul%20Nayyar%2C%20Ashutosh%20Jain%2C%20Rahul%20Learning%20unknown%20Markov%20decision%20processes%3A%20A%20thompson%20sampling%20approach%202017"
        },
        {
            "id": "Strehl_2008_a",
            "entry": "Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309 \u2013 1331, 2008. Learning Theory 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20Alexander%20L.%20Littman%2C%20Michael%20L.%20An%20analysis%20of%20model-based%20interval%20estimation%20for%20markov%20decision%20processes%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20Alexander%20L.%20Littman%2C%20Michael%20L.%20An%20analysis%20of%20model-based%20interval%20estimation%20for%20markov%20decision%20processes%202008"
        },
        {
            "id": "Strens_2000_a",
            "entry": "Malcolm Strens. A Bayesian framework for reinforcement learning. In ICML, pages 943\u2013950, 2000. Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 135. MIT",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strens%2C%20Malcolm%20A%20Bayesian%20framework%20for%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strens%2C%20Malcolm%20A%20Bayesian%20framework%20for%20reinforcement%20learning%202000"
        },
        {
            "id": "Cambridge_1998_a",
            "entry": "Press Cambridge, 1998. Georgios Theocharous, Nikos Vlassis, and Zheng Wen. An interactive points of interest guidance system. In IUI Companion, pages 49\u201352. ACM, 2017. William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25:285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cambridge%2C%20Press%20Georgios%20Theocharous%2C%20Nikos%20Vlassis%2C%20and%20Zheng%20Wen.%20An%20interactive%20points%20of%20interest%20guidance%20system%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cambridge%2C%20Press%20Georgios%20Theocharous%2C%20Nikos%20Vlassis%2C%20and%20Zheng%20Wen.%20An%20interactive%20points%20of%20interest%20guidance%20system%201998"
        }
    ]
}
