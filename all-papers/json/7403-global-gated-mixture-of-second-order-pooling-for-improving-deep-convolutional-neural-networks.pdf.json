{
    "filename": "7403-global-gated-mixture-of-second-order-pooling-for-improving-deep-convolutional-neural-networks.pdf",
    "metadata": {
        "title": "Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks",
        "author": "Qilong Wang, Zilin Gao, Jiangtao Xie, Wangmeng Zuo, Peihua Li",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7403-global-gated-mixture-of-second-order-pooling-for-improving-deep-convolutional-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In most of existing deep convolutional neural networks (CNNs) for classification, global average (first-order) pooling (GAP) has become a standard module to summarize activations of the last convolution layer as final representation for prediction. Recent researches show integration of higher-order pooling (HOP) methods clearly improves performance of deep CNNs. However, both GAP and existing HOP methods assume unimodal distributions, which cannot fully capture statistics of convolutional activations, limiting representation ability of deep CNNs, especially for samples with complex contents. To overcome the above limitation, this paper proposes a global Gated Mixture of Second-order Pooling (GM-SOP) method to further improve representation ability of deep CNNs. To this end, we introduce a sparsity-constrained gating mechanism and propose a novel parametric SOP as component of mixture model. Given a bank of SOP candidates, our method can adaptively choose Top-K(K > 1) candidates for each input sample through the sparsity-constrained gating module, and performs weighted sum of outputs of K selected candidates as representation of the sample. The proposed GM-SOP can flexibly accommodate a large number of personalized SOP candidates in an efficient way, leading to richer representations. The deep networks with our GM-SOP can be end-to-end trained, having potential to characterize complex, multi-modal distributions. The proposed method is evaluated on two large scale image benchmarks (i.e., downsampled ImageNet-1K and Places365), and experimental results show our GM-SOP is superior to its counterparts and achieves very competitive performance. The source code will be available at http://www.peihuali.org/GM-SOP."
    },
    "keywords": [
        {
            "term": "eigenvalue decomposition",
            "url": "https://en.wikipedia.org/wiki/eigenvalue_decomposition"
        },
        {
            "term": "unimodal distribution",
            "url": "https://en.wikipedia.org/wiki/unimodal_distribution"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "maximum likelihood estimation",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimation"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "deep convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/deep_convolutional_neural_network"
        },
        {
            "term": "component model",
            "url": "https://en.wikipedia.org/wiki/component_model"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        }
    ],
    "highlights": [
        "Deep convolutional neural networks (CNNs) have achieved great success in a variety of computer vision tasks, especially image classification [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "Based on the parametric square-root normalized second-order pooling, we propose a global Gated Mixture of Second-order Pooling (GM-second-order pooling), 4We reasonably introduce a group of trainable parameters into square-root normalized second-order pooling, and use the recently proposed fast iterative algorithm [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] to speed up matrix square-root normalization on GPU",
        "Motivated by success of matrix square-root normalized second-order pooling (SR-second-order pooling) in deep convolutional neural networks architectures [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], we propose a parametric square-root normalized second-order pooling as component models of our Gated Mixture of Second-order Pooling",
        "GM-global average pooling-16-8 and Gated Mixture of Second-order Pooling-16-8 are superior to ResNet-18-512d and square-root normalized second-order pooling by a large margin, respectively",
        "This paper proposes a novel Gated Mixture of Second-order Pooling method for improving deep convolutional neural networks, whose core is a trainable gated mixture of parametric second-order pooling model for summarizing the outputs of the last convolution layer as image representation",
        "The Gated Mixture of Second-order Pooling can be flexibly integrated into deep convolutional neural networks in an end-to-end manner"
    ],
    "key_statements": [
        "Deep convolutional neural networks (CNNs) have achieved great success in a variety of computer vision tasks, especially image classification [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "This paper proposes a parametric square-root normalized second-order pooling method4, which enables candidate component models to be individually trained with negligible additional cost",
        "Based on the parametric square-root normalized second-order pooling, we propose a global Gated Mixture of Second-order Pooling (GM-second-order pooling), 4We reasonably introduce a group of trainable parameters into square-root normalized second-order pooling, and use the recently proposed fast iterative algorithm [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] to speed up matrix square-root normalization on GPU",
        "Motivated by success of matrix square-root normalized second-order pooling (SR-second-order pooling) in deep convolutional neural networks architectures [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], we propose a parametric square-root normalized second-order pooling as component models of our Gated Mixture of Second-order Pooling",
        "We evaluate them using gated mixture of first-order global average pooling with ResNet-18 on downsampled ImageNet-1K, and the decided optimal parameters are directly adopted to Gated Mixture of Second-order Pooling",
        "By using the same augmentation strategy in [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], GM-global average pooling-16-8 achieves over 2% gains in Top-1 error, which uses much less parameters to get simliar results with wide residual network-36-2",
        "Note that Gated Mixture of Second-order Pooling with wide residual network-36-2 obtains the similar result with wide residual network-36-5 [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] using one half parameters.\n3.4",
        "The inference is performed on single center crop, and we report results on validation set for comparison",
        "The results of different methods are given in Table 4, from it we can see that our Gated Mixture of Second-order Pooling-16-8 achieves the best result and significantly outperforms ResNet-18-512d and ResNet-18-8256d, further demonstrating the effectiveness of Gated Mixture of Second-order Pooling",
        "GM-global average pooling-16-8 and Gated Mixture of Second-order Pooling-16-8 are superior to ResNet-18-512d and square-root normalized second-order pooling by a large margin, respectively",
        "It indicates the idea of gated mixture model is helpful for improving representation ability of deep convolutional neural networks",
        "This paper proposes a novel Gated Mixture of Second-order Pooling method for improving deep convolutional neural networks, whose core is a trainable gated mixture of parametric second-order pooling model for summarizing the outputs of the last convolution layer as image representation",
        "The Gated Mixture of Second-order Pooling can be flexibly integrated into deep convolutional neural networks in an end-to-end manner"
    ],
    "summary": [
        "Deep convolutional neural networks (CNNs) have achieved great success in a variety of computer vision tasks, especially image classification [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "Our gated mixture model can accommodate efficiently a large number of CMs because only K ones are trained and used to generate representation given a sample.",
        "We first describe a general idea of gated mixture model, and propose a parametric matrix squareroot normalized second-order pooling (SR-SOP) method as our component model.",
        "SR-SOP in Eq (5) benefits from some merits and achieves promising performance, it is a parameter-free model, which can not be trained as personalized CMs. covariance \u03a3 in Eq (5) is calculated based on assumption that X is sampled from a Gaussian distribution, which may not always hold true.",
        "Fast Iterative Algorithm The computation of matrix square root in our parametric SR-SOP depends on EIG, which is limited supported on GPU, slowing down training speed of the whole network.",
        "We evaluate them using gated mixture of first-order GAP with ResNet-18 on downsampled ImageNet-1K, and the decided optimal parameters are directly adopted to GM-SOP.",
        "All ResNet-50 based methods are trained within {50, 15, 10} epochs, and initial learning rates with decay rate of 0.1 are set to 0.1 and 0.075 for our GM-SOP and remaining ones, respectively.",
        "These results verify our methods effectively improve existing deep CNNs. Our GM-SOP with ResNet-18 clearly outperforms WRN-36-1 and WRN-36-2, the latter ones adopt more sophisticated data augmentation.",
        "It indicates the idea of gated mixture model is helpful for improving representation ability of deep CNNs. Note that parametric SR-SOP for non-trivial gains over original SR-SOP, showing a more general approach for image modeling is meaningful and useful for improving performance.",
        "The SG-MoE is proposed as a general purpose component in a recurrent model [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], while our GM-SOP is proposed as a global modeling step to improve representation ability of deep CNNs. This work is related to those methods integrating single HOP into deep CNNs [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>].",
        "This paper proposes a novel GM-SOP method for improving deep CNNs, whose core is a trainable gated mixture of parametric second-order pooling model for summarizing the outputs of the last convolution layer as image representation.",
        "Compared with popular GAP and existing HOP methods only considering unimodal distributions, our GM-SOP can make better use of statistical information inherent in convolutional activations, leading better representation ability and higher accuracy.",
        "The experimental results on two large-scale image benchmarks demonstrate the gated mixture model is helpful to improve classification performance of deep CNNs, and our GM-SOP method clearly outperforms its counterparts with affordable costs.",
        "We will experiment with standard-size ImageNet dataset, and extend GM-SOP to other tasks, such as video classification and semantic segmentation"
    ],
    "headline": "To overcome the above limitation, this paper proposes a global Gated Mixture of Second-order Pooling  method to further improve representation ability of deep convolutional neural networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic. NetVLAD: CNN architecture for weakly supervised place recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arandjelovic%2C%20R.%20Gronat%2C%20P.%20Torii%2C%20A.%20Pajdla%2C%20T.%20NetVLAD%3A%20CNN%20architecture%20for%20weakly%20supervised%20place%20recognition%202016"
        },
        {
            "id": "2",
            "entry": "[2] V. Arsigny, P. Fillard, X. Pennec, and N. Ayache. Fast and simple calculus on tensors in the Log-Euclidean framework. In MICCAI, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arsigny%2C%20V.%20Fillard%2C%20P.%20Pennec%2C%20X.%20Ayache%2C%20N.%20Fast%20and%20simple%20calculus%20on%20tensors%20in%20the%20Log-Euclidean%20framework%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arsigny%2C%20V.%20Fillard%2C%20P.%20Pennec%2C%20X.%20Ayache%2C%20N.%20Fast%20and%20simple%20calculus%20on%20tensors%20in%20the%20Log-Euclidean%20framework%202005"
        },
        {
            "id": "3",
            "entry": "[3] O. Arslan. Convergence behavior of an iterative reweighting algorithm to compute multivariate M-estimates for location and scatter. Journal of Statistical Planning and Inference, 118:115\u2013128, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arslan%2C%20O.%20Convergence%20behavior%20of%20an%20iterative%20reweighting%20algorithm%20to%20compute%20multivariate%20M-estimates%20for%20location%20and%20scatter%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arslan%2C%20O.%20Convergence%20behavior%20of%20an%20iterative%20reweighting%20algorithm%20to%20compute%20multivariate%20M-estimates%20for%20location%20and%20scatter%202004"
        },
        {
            "id": "4",
            "entry": "[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Free-form region description with second-order pooling. IEEE TPAMI, 37(6):1177\u20131189, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20J.%20Caseiro%2C%20R.%20Batista%2C%20J.%20Sminchisescu%2C%20C.%20Free-form%20region%20description%20with%20second-order%20pooling%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20J.%20Caseiro%2C%20R.%20Batista%2C%20J.%20Sminchisescu%2C%20C.%20Free-form%20region%20description%20with%20second-order%20pooling%202015"
        },
        {
            "id": "5",
            "entry": "[5] Y. Chen, J. Li, H. Xiao, X. Jin, S. Yan, and J. Feng. Dual path networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%20Chen%20J%20Li%20H%20Xiao%20X%20Jin%20S%20Yan%20and%20J%20Feng%20Dual%20path%20networks%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%20Chen%20J%20Li%20H%20Xiao%20X%20Jin%20S%20Yan%20and%20J%20Feng%20Dual%20path%20networks%20In%20NIPS%202017"
        },
        {
            "id": "6",
            "entry": "[6] P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of ImageNet as an alternative to the CIFAR datasets. arXiv, abs/1707.08819, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08819"
        },
        {
            "id": "7",
            "entry": "[7] M. Cimpoi, S. Maji, and A. Vedaldi. Deep filter banks for texture recognition and segmentation. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cimpoi%2C%20M.%20Maji%2C%20S.%20Vedaldi%2C%20A.%20Deep%20filter%20banks%20for%20texture%20recognition%20and%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cimpoi%2C%20M.%20Maji%2C%20S.%20Vedaldi%2C%20A.%20Deep%20filter%20banks%20for%20texture%20recognition%20and%20segmentation%202015"
        },
        {
            "id": "8",
            "entry": "[8] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie. Kernel pooling for convolutional neural networks.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cui%2C%20Y.%20Zhou%2C%20F.%20Wang%2C%20J.%20Liu%2C%20X.%20Kernel%20pooling%20for%20convolutional%20neural%20networks"
        },
        {
            "id": "9",
            "entry": "[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "10",
            "entry": "[10] D. L. Donoho, M. Gavish, and I. M. Johnstone. Optimal shrinkage of eigenvalues in the spiked covariance model. arXiv, 1311.0851, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1311.0851"
        },
        {
            "id": "11",
            "entry": "[11] L. Dryden, A. Koloydenko, and D. Zhou. Non-Euclidean statistics for covariance matrices, with applications to diffusion tensor imaging. The Annals of Applied Statistics, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dryden%2C%20L.%20Koloydenko%2C%20A.%20Zhou%2C%20D.%20Non-Euclidean%20statistics%20for%20covariance%20matrices%2C%20with%20applications%20to%20diffusion%20tensor%20imaging%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dryden%2C%20L.%20Koloydenko%2C%20A.%20Zhou%2C%20D.%20Non-Euclidean%20statistics%20for%20covariance%20matrices%2C%20with%20applications%20to%20diffusion%20tensor%20imaging%202009"
        },
        {
            "id": "12",
            "entry": "[12] B. S. Everitt and D. J. Hand. Finite mixture distributions. Springer Press, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Everitt%2C%20B.S.%20Hand%2C%20D.J.%20Finite%20mixture%20distributions%201981"
        },
        {
            "id": "13",
            "entry": "[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "14",
            "entry": "[14] N. J. Higham. Functions of Matrices: Theory and Computation. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Higham%2C%20N.J.%20Functions%20of%20Matrices%3A%20Theory%20and%20Computation.%20Society%20for%20Industrial%20and%20Applied%20Mathematics%202008"
        },
        {
            "id": "15",
            "entry": "[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "16",
            "entry": "[16] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20J.%20Shen%2C%20L.%20Sun%2C%20G.%20Squeeze-and-excitation%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20J.%20Shen%2C%20L.%20Sun%2C%20G.%20Squeeze-and-excitation%20networks%202018"
        },
        {
            "id": "17",
            "entry": "[17] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20van%20der%20Maaten%2C%20L.%20Weinberger%2C%20K.Q.%20Densely%20connected%20convolutional%20networks"
        },
        {
            "id": "18",
            "entry": "[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "19",
            "entry": "[19] C. Ionescu, O. Vantzos, and C. Sminchisescu. Matrix backpropagation for deep networks with structured layers. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ionescu%2C%20C.%20Vantzos%2C%20O.%20Sminchisescu%2C%20C.%20Matrix%20backpropagation%20for%20deep%20networks%20with%20structured%20layers%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ionescu%2C%20C.%20Vantzos%2C%20O.%20Sminchisescu%2C%20C.%20Matrix%20backpropagation%20for%20deep%20networks%20with%20structured%20layers%202015"
        },
        {
            "id": "20",
            "entry": "[20] C. Ionescu, O. Vantzos, and C. Sminchisescu. Training deep networks with structured layers by matrix backpropagation. arXiv, abs/1509.07838, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.07838"
        },
        {
            "id": "21",
            "entry": "[21] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3(1):79\u201387, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacobs%2C%20R.A.%20Jordan%2C%20M.I.%20Nowlan%2C%20S.J.%20Hinton%2C%20G.E.%20Adaptive%20mixtures%20of%20local%20experts%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacobs%2C%20R.A.%20Jordan%2C%20M.I.%20Nowlan%2C%20S.J.%20Hinton%2C%20G.E.%20Adaptive%20mixtures%20of%20local%20experts%201991"
        },
        {
            "id": "22",
            "entry": "[22] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6(2):181\u2013214, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.I.%20Jacobs%2C%20R.A.%20Hierarchical%20mixtures%20of%20experts%20and%20the%20EM%20algorithm%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20M.I.%20Jacobs%2C%20R.A.%20Hierarchical%20mixtures%20of%20experts%20and%20the%20EM%20algorithm%201994"
        },
        {
            "id": "23",
            "entry": "[23] P. Koniusz, F. Yan, P. Gosselin, and K. Mikolajczyk. Higher-order occurrence pooling for bags-of-words: Visual concept detection. IEEE TPAMI, 39(2):313\u2013326, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koniusz%2C%20P.%20Yan%2C%20F.%20Gosselin%2C%20P.%20Mikolajczyk%2C%20K.%20Higher-order%20occurrence%20pooling%20for%20bags-of-words%3A%20Visual%20concept%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koniusz%2C%20P.%20Yan%2C%20F.%20Gosselin%2C%20P.%20Mikolajczyk%2C%20K.%20Higher-order%20occurrence%20pooling%20for%20bags-of-words%3A%20Visual%20concept%20detection%202017"
        },
        {
            "id": "24",
            "entry": "[24] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "25",
            "entry": "[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "26",
            "entry": "[26] P. Li, J. Xie, Q. Wang, and Z. Gao. Towards faster training of global covariance pooling networks by iterative matrix square root normalization. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20P.%20Xie%2C%20J.%20Wang%2C%20Q.%20Gao%2C%20Z.%20Towards%20faster%20training%20of%20global%20covariance%20pooling%20networks%20by%20iterative%20matrix%20square%20root%20normalization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20P.%20Xie%2C%20J.%20Wang%2C%20Q.%20Gao%2C%20Z.%20Towards%20faster%20training%20of%20global%20covariance%20pooling%20networks%20by%20iterative%20matrix%20square%20root%20normalization%202018"
        },
        {
            "id": "27",
            "entry": "[27] P. Li, J. Xie, Q. Wang, and W. Zuo. Is second-order information helpful for large-scale visual recognition?",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20P.%20Xie%2C%20J.%20Wang%2C%20Q.%20Zuo%2C%20W.%20Is%20second-order%20information%20helpful%20for%20large-scale%20visual%20recognition%3F"
        },
        {
            "id": "28",
            "entry": "[28] Y. Li, M. Dixit, and N. Vasconcelos. Deep scene image classification with the MFAFVNet. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Dixit%2C%20M.%20Vasconcelos%2C%20N.%20Deep%20scene%20image%20classification%20with%20the%20MFAFVNet%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Dixit%2C%20M.%20Vasconcelos%2C%20N.%20Deep%20scene%20image%20classification%20with%20the%20MFAFVNet%202017"
        },
        {
            "id": "29",
            "entry": "[29] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN models for fine-grained visual recognition. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20T.-Y.%20RoyChowdhury%2C%20A.%20Maji%2C%20S.%20Bilinear%20CNN%20models%20for%20fine-grained%20visual%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20T.-Y.%20RoyChowdhury%2C%20A.%20Maji%2C%20S.%20Bilinear%20CNN%20models%20for%20fine-grained%20visual%20recognition"
        },
        {
            "id": "30",
            "entry": "[30] G. McLachlan and D. Peel. Finite Mixture Models. Wiley Press, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McLachlan%2C%20G.%20Peel%2C%20D.%20Finite%20Mixture%20Models%202005"
        },
        {
            "id": "31",
            "entry": "[31] M. T. Nguyen, W. Liu, E. Perez, R. G. Baraniuk, and A. B. Patel. Semi-supervised learning with the deep rendering mixture model. arXiv, abs/1612.01942, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01942"
        },
        {
            "id": "32",
            "entry": "[32] F. Pascal, L. Bombrun, J. Tourneret, and Y. Berthoumieu. Parameter estimation for multivariate generalized Gaussian distributions. IEEE TSP, 61(23):5960\u20135971, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascal%2C%20F.%20Bombrun%2C%20L.%20Tourneret%2C%20J.%20Berthoumieu%2C%20Y.%20Parameter%20estimation%20for%20multivariate%20generalized%20Gaussian%20distributions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascal%2C%20F.%20Bombrun%2C%20L.%20Tourneret%2C%20J.%20Berthoumieu%2C%20Y.%20Parameter%20estimation%20for%20multivariate%20generalized%20Gaussian%20distributions%202013"
        },
        {
            "id": "33",
            "entry": "[33] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shazeer%2C%20N.%20Mirhoseini%2C%20A.%20Maziarz%2C%20K.%20Davis%2C%20A.%20Outrageously%20large%20neural%20networks%3A%20The%20sparsely-gated%20mixture-of-experts%20layer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shazeer%2C%20N.%20Mirhoseini%2C%20A.%20Maziarz%2C%20K.%20Davis%2C%20A.%20Outrageously%20large%20neural%20networks%3A%20The%20sparsely-gated%20mixture-of-experts%20layer%202017"
        },
        {
            "id": "34",
            "entry": "[34] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition"
        },
        {
            "id": "35",
            "entry": "[35] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "36",
            "entry": "[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "37",
            "entry": "[37] A. Vedaldi and K. Lenc. MatConvNet \u2013 Convolutional Neural Networks for MATLAB. In ACM MM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Vedaldi%20and%20K%20Lenc%20MatConvNet%20%20Convolutional%20Neural%20Networks%20for%20MATLAB%20In%20ACM%20MM%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Vedaldi%20and%20K%20Lenc%20MatConvNet%20%20Convolutional%20Neural%20Networks%20for%20MATLAB%20In%20ACM%20MM%202015"
        },
        {
            "id": "38",
            "entry": "[38] C. Viroli and G. J. McLachlan. Deep Gaussian mixture models. arXiv, abs/1711.06929, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06929"
        },
        {
            "id": "39",
            "entry": "[39] Q. Wang, P. Li, and L. Zhang. G2DeNet: Global Gaussian distribution embedding network and its application to visual recognition. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Q.%20Li%2C%20P.%20Zhang%2C%20L.%20G2DeNet%3A%20Global%20Gaussian%20distribution%20embedding%20network%20and%20its%20application%20to%20visual%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Q.%20Li%2C%20P.%20Zhang%2C%20L.%20G2DeNet%3A%20Global%20Gaussian%20distribution%20embedding%20network%20and%20its%20application%20to%20visual%20recognition%202017"
        },
        {
            "id": "40",
            "entry": "[40] Q. Wang, P. Li, W. Zuo, and L. Zhang. RAID-G: Robust estimation of approximate infinite dimensional Gaussian with application to material recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Q.%20Li%2C%20P.%20Zuo%2C%20W.%20Zhang%2C%20L.%20RAID-G%3A%20Robust%20estimation%20of%20approximate%20infinite%20dimensional%20Gaussian%20with%20application%20to%20material%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Q.%20Li%2C%20P.%20Zuo%2C%20W.%20Zhang%2C%20L.%20RAID-G%3A%20Robust%20estimation%20of%20approximate%20infinite%20dimensional%20Gaussian%20with%20application%20to%20material%20recognition%202016"
        },
        {
            "id": "41",
            "entry": "[41] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE TPAMI, 31(2):210\u2013227, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wright%2C%20J.%20Yang%2C%20A.Y.%20Ganesh%2C%20A.%20Sastry%2C%20S.S.%20Robust%20face%20recognition%20via%20sparse%20representation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wright%2C%20J.%20Yang%2C%20A.Y.%20Ganesh%2C%20A.%20Sastry%2C%20S.S.%20Robust%20face%20recognition%20via%20sparse%20representation%202009"
        },
        {
            "id": "42",
            "entry": "[42] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Wide%20residual%20networks%202016"
        },
        {
            "id": "43",
            "entry": "[43] T. Zhang, A. Wiesel, and M. S. Greco. Multivariate generalized Gaussian distribution: Convexity and graphical models. IEEE TSP, 61(16):4141\u20134148, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20T.%20Wiesel%2C%20A.%20Greco%2C%20M.S.%20Multivariate%20generalized%20Gaussian%20distribution%3A%20Convexity%20and%20graphical%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20T.%20Wiesel%2C%20A.%20Greco%2C%20M.S.%20Multivariate%20generalized%20Gaussian%20distribution%3A%20Convexity%20and%20graphical%20models%202013"
        },
        {
            "id": "44",
            "entry": "[44] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image database for scene recognition. IEEE TPAMI, 40(6):1452\u20131464, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Lapedriza%2C%20A.%20Khosla%2C%20A.%20Oliva%2C%20A.%20Places%3A%20A%2010%20million%20image%20database%20for%20scene%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20B.%20Lapedriza%2C%20A.%20Khosla%2C%20A.%20Oliva%2C%20A.%20Places%3A%20A%2010%20million%20image%20database%20for%20scene%20recognition%202017"
        }
    ]
}
