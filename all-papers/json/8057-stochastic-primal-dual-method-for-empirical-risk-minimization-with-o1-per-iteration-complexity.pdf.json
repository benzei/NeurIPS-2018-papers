{
    "filename": "8057-stochastic-primal-dual-method-for-empirical-risk-minimization-with-o1-per-iteration-complexity.pdf",
    "metadata": {
        "title": "Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity",
        "author": "Conghui Tan, Tong Zhang, Shiqian Ma, Ji Liu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8057-stochastic-primal-dual-method-for-empirical-risk-minimization-with-o1-per-iteration-complexity.pdf"
        },
        "abstract": "Regularized empirical risk minimization problem with linear predictor appears frequently in machine learning. In this paper, we propose a new stochastic primaldual method to solve this class of problems. Different from existing methods, our proposed methods only require O(1) operations in each iteration. We also develop a variance-reduction variant of the algorithm that converges linearly. Numerical experiments suggest that our methods are faster than existing ones such as proximal SGD, SVRG and SAGA on high-dimensional problems."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "feasible set",
            "url": "https://en.wikipedia.org/wiki/feasible_set"
        },
        {
            "term": "SVRG",
            "url": "https://en.wikipedia.org/wiki/SVRG"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "It is well known that proximal stochastic gradient descent method converges at a sub-linear rate [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] even for strongly convex problems, due to non-diminishing variance of the stochastic gradients",
        "By incorporating the variance reduction technique, we develop a variant of SPD1, named SPD1 with Variance Reduction, that converges linearly for strongly convex problems",
        "Algorithm 1 can be viewed as a primal-dual version of stochastic gradient descent (SGD) on this finite-sum problem, which samples a pair of induces and only utilizes the corresponding summand to do updates in each iteration",
        "The theoretical complexity of SPD1 with Variance Reduction is same as SVRG when d \u2265 n, we empirically found that SPD1 with Variance Reduction is significantly faster than SVRG in high-dimensional problems, by allowing much larger step sizes than the ones in (9) suggested by theory",
        "We developed two stochastic primal-dual algorithms, named SPD1 and SPD1 with Variance Reduction for solving regularized empirical risk minimization problems",
        "We proved that the overall convergence property of SPD1 and SPD1 with Variance Reduction resembles proximal stochastic gradient descent method and SVRG respectively under certain condition, and empirically showed that they are faster than existing methods such as proximal stochastic gradient descent method, SVRG and SAGA in high-dimensional settings"
    ],
    "key_statements": [
        "It is well known that proximal stochastic gradient descent method converges at a sub-linear rate [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] even for strongly convex problems, due to non-diminishing variance of the stochastic gradients",
        "By incorporating the variance reduction technique, we develop a variant of SPD1, named SPD1 with Variance Reduction, that converges linearly for strongly convex problems",
        "Our numerical tests indicate that our methods are faster than both proximal stochastic gradient descent method and SVRG on high-dimensional problems, even in single-machine setting",
        "Algorithm 1 can be viewed as a primal-dual version of stochastic gradient descent (SGD) on this finite-sum problem, which samples a pair of induces and only utilizes the corresponding summand to do updates in each iteration",
        "Their algorithm needs to use the full-dimensional gradient in each iteration, so the per-iteration cost is much higher than SPD1.\n3 SPD1 with Variance Reduction",
        "Comparing with the classical convergence rate result of proximal stochastic gradient descent method, we note that the convergence rate of SPD1 not only depends on \u03bc, but depends on the dual strong convexity parameter \u03b3",
        "The theoretical complexity of SPD1 with Variance Reduction is same as SVRG when d \u2265 n, we empirically found that SPD1 with Variance Reduction is significantly faster than SVRG in high-dimensional problems, by allowing much larger step sizes than the ones in (9) suggested by theory",
        "We developed two stochastic primal-dual algorithms, named SPD1 and SPD1 with Variance Reduction for solving regularized empirical risk minimization problems",
        "We proved that the overall convergence property of SPD1 and SPD1 with Variance Reduction resembles proximal stochastic gradient descent method and SVRG respectively under certain condition, and empirically showed that they are faster than existing methods such as proximal stochastic gradient descent method, SVRG and SAGA in high-dimensional settings"
    ],
    "summary": [
        "It is well known that PSGD converges at a sub-linear rate [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] even for strongly convex problems, due to non-diminishing variance of the stochastic gradients.",
        "Our numerical tests indicate that our methods are faster than both PSGD and SVRG on high-dimensional problems, even in single-machine setting.",
        "We need to point out that their algorithms still need to sample the full vector ai to compute the directional gradient, and the per-iteration cost is still O(d).",
        "2 Stochastic Primal-Dual Method with O(1) Per-Iteration Cost",
        "Our algorithm solves the following equivalent primal-dual reformulation of (1): min max F (x, y)",
        "In each iteration of the algorithm, only one coordinate of x and one coordinate of y are updated by using a randomly sampled data entry aitjt .",
        "Algorithm 1 Stochastic Primal-Dual Method with O(1) Per-Iteration Cost (SPD1)",
        "Algorithm 1 can be viewed as a primal-dual version of stochastic gradient descent (SGD) on this finite-sum problem, which samples a pair of induces and only utilizes the corresponding summand to do updates in each iteration.",
        "One can view SPD1 as a combination of randomized coordinate descent method and stochastic gradient descent method applied to the primal-dual reformulation (5).",
        "Their algorithm needs to use the full-dimensional gradient in each iteration, so the per-iteration cost is much higher than SPD1.",
        "Comparing with the classical convergence rate result of PSGD, we note that the convergence rate of SPD1 not only depends on \u03bc, but depends on the dual strong convexity parameter \u03b3.",
        "This is reasonable because SPD1 has stochastic updates for both primal and dual variables, and this is why \u03b3 > 0 is necessary for ensuring the O convergence rate.",
        "We compare our SPD1 and SPD1-VR with some standard stochastic algorithms for solving (1), including PSGD, SVRG and SAGA.",
        "These results on real datasets further confirm that our proposed methods, especially SPD1-VR, are faster than existing algorithms on high-dimensional problems.",
        "We developed two stochastic primal-dual algorithms, named SPD1 and SPD1-VR for solving regularized empirical risk minimization problems.",
        "Our proposed algorithms have a brand-new updating style, which only need to use one coordinate of one sampled data in each iteration.",
        "We proved that the overall convergence property of SPD1 and SPD1-VR resembles PSGD and SVRG respectively under certain condition, and empirically showed that they are faster than existing methods such as PSGD, SVRG and SAGA in high-dimensional settings.",
        "We believe that our new methods have great potential to be used in large-scale distributed optimization applications"
    ],
    "headline": "We propose a new stochastic primaldual method to solve this class of problems",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International Conference on Machine Learning, pages 699\u2013707, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "2",
            "entry": "[2] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of mathematical imaging and vision, 40(1):120\u2013145, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20Antonin%20Pock%2C%20Thomas%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20Antonin%20Pock%2C%20Thomas%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011"
        },
        {
            "id": "3",
            "entry": "[3] Cong Dang and Guanghui Lan. Randomized first-order methods for saddle point optimization. https://arxiv.org/abs/1409.8625, 2014.",
            "url": "https://arxiv.org/abs/1409.8625",
            "arxiv_url": "https://arxiv.org/pdf/1409.8625"
        },
        {
            "id": "4",
            "entry": "[4] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "5",
            "entry": "[5] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "6",
            "entry": "[6] Jakub Konecny, Zheng Qu, and Peter Richt\u00e1rik. Semi-stochastic coordinate descent. Optimization Methods and Software, 32(5):993\u20131005, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konecny%2C%20Jakub%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Semi-stochastic%20coordinate%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konecny%2C%20Jakub%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Semi-stochastic%20coordinate%20descent%202017"
        },
        {
            "id": "7",
            "entry": "[7] GM Korpelevich. The extragradient method for finding saddle points and other problems. Matecon, 12:747\u2013756, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Korpelevich%2C%20G.M.%20The%20extragradient%20method%20for%20finding%20saddle%20points%20and%20other%20problems%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Korpelevich%2C%20G.M.%20The%20extragradient%20method%20for%20finding%20saddle%20points%20and%20other%20problems%201976"
        },
        {
            "id": "8",
            "entry": "[8] Arkadi Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadi%20Prox-method%20with%20rate%20of%20convergence%20o%281/t%29%20for%20variational%20inequalities%20with%20lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20Arkadi%20Prox-method%20with%20rate%20of%20convergence%20o%281/t%29%20for%20variational%20inequalities%20with%20lipschitz%20continuous%20monotone%20operators%20and%20smooth%20convex-concave%20saddle%20point%20problems%202004"
        },
        {
            "id": "9",
            "entry": "[9] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574\u2013 1609, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%2C%20Guanghui%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%2C%20Guanghui%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "10",
            "entry": "[10] Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Applied Optimization. Kluwer Academic Publishers, Boston, MA, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course.%20Applied%20Optimization%202004"
        },
        {
            "id": "11",
            "entry": "[11] Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Efficiency%20of%20coordinate%20descent%20methods%20on%20huge-scale%20optimization%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Efficiency%20of%20coordinate%20descent%20methods%20on%20huge-scale%20optimization%20problems%202012"
        },
        {
            "id": "12",
            "entry": "[12] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddlepoint problems. In Advances in Neural Information Processing Systems, pages 1416\u20131424, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Palaniappan%2C%20Balamurugan%20Bach%2C%20Francis%20Stochastic%20variance%20reduction%20methods%20for%20saddlepoint%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Palaniappan%2C%20Balamurugan%20Bach%2C%20Francis%20Stochastic%20variance%20reduction%20methods%20for%20saddlepoint%20problems%202016"
        },
        {
            "id": "13",
            "entry": "[13] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "14",
            "entry": "[14] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In International Conference on Machine Learning, pages 64\u201372, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014"
        },
        {
            "id": "15",
            "entry": "[15] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International Conference on Machine Learning, pages 71\u201379, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Zhang%2C%20Tong%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%3A%20Convergence%20results%20and%20optimal%20averaging%20schemes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Zhang%2C%20Tong%20Stochastic%20gradient%20descent%20for%20non-smooth%20optimization%3A%20Convergence%20results%20and%20optimal%20averaging%20schemes%202013"
        },
        {
            "id": "16",
            "entry": "[16] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014"
        },
        {
            "id": "17",
            "entry": "[17] Adams Wei Yu, Qihang Lin, and Tianbao Yang. Doubly stochastic primal-dual coordinate method for empirical risk minimization and bilinear saddle-point problem. arXiv preprint arXiv:1508.03390, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.03390"
        },
        {
            "id": "18",
            "entry": "[18] Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. The Journal of Machine Learning Research, 18(1):2939\u20132980, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Xiao%2C%20Lin%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Xiao%2C%20Lin%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202017"
        }
    ]
}
