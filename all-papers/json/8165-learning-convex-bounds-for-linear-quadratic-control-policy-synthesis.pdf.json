{
    "filename": "8165-learning-convex-bounds-for-linear-quadratic-control-policy-synthesis.pdf",
    "metadata": {
        "title": "Learning convex bounds for linear quadratic control policy synthesis",
        "author": "Jack Umenberger, Thomas B. Sch\u00f6n",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8165-learning-convex-bounds-for-linear-quadratic-control-policy-synthesis.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a number of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "dynamical system",
            "url": "https://en.wikipedia.org/wiki/dynamical_system"
        },
        {
            "term": "optimal control",
            "url": "https://en.wikipedia.org/wiki/optimal_control"
        },
        {
            "term": "robust control",
            "url": "https://en.wikipedia.org/wiki/robust_control"
        },
        {
            "term": "Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Monte_Carlo"
        },
        {
            "term": "Swedish Foundation for Strategic Research",
            "url": "https://en.wikipedia.org/wiki/Swedish_Foundation_for_Strategic_Research"
        },
        {
            "term": "linear matrix inequality",
            "url": "https://en.wikipedia.org/wiki/linear_matrix_inequality"
        },
        {
            "term": "discrete time",
            "url": "https://en.wikipedia.org/wiki/discrete_time"
        },
        {
            "term": "system identification",
            "url": "https://en.wikipedia.org/wiki/system_identification"
        }
    ],
    "highlights": [
        "Decision making for dynamical systems in the presence of uncertainty is a problem of great prevalence and importance, as well as considerable difficulty, especially when knowledge of the dynamics is available only via limited observations of system behavior",
        "The data-driven search for a control policy to maximize the expected reward attained by a stochastic dynamic process is known as reinforcement learning (RL) [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]",
        "The problem we address in this paper lies at the intersection of reinforcement learning and robust control, and can be summarized as follows: given observations from an unknown dynamical system, we seek a policy to optimize the expected cost, subject to certain robust stability guarantees",
        "Of particular relevance to the present paper is [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which provides sample complexity bounds on the performance of robust control synthesis for the infinite horizon linear quadratic regulator problem, when the true system is not known",
        "To circumvent both of these issues, we propose the following Monte Carlo (MC) approximation of Jc(K), c",
        "{exAilsst,iBngls}m:e=thaordgsm:An,oBmPinaNr=l:1sPtanTt=d1a|rxdrtLQARxutrsi1ng the nominal model from r\n1|2; worst-case: optimize for worst-case model (95% confidence) s.t. robust stability constraints, i.e., the method of [17, \u00a75.2]; H2/H1: enforce stability constraint from [17, \u00a75.2], but optimize performance for the nominal model {Als, Bls}; proposed method(s): CL: the common Lyapunov relaxation of 8; (v) proposed: the method proposed in this paper, i.e., Algorithm 1; additional new methods: alternate-r: initialize with the H2/H1 solution, and apply the iterative optimization method proposed in Section 4.2, cf"
    ],
    "key_statements": [
        "Decision making for dynamical systems in the presence of uncertainty is a problem of great prevalence and importance, as well as considerable difficulty, especially when knowledge of the dynamics is available only via limited observations of system behavior",
        "The data-driven search for a control policy to maximize the expected reward attained by a stochastic dynamic process is known as reinforcement learning (RL) [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]",
        "The problem we address in this paper lies at the intersection of reinforcement learning and robust control, and can be summarized as follows: given observations from an unknown dynamical system, we seek a policy to optimize the expected cost, subject to certain robust stability guarantees",
        "Of particular relevance to the present paper is [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which provides sample complexity bounds on the performance of robust control synthesis for the infinite horizon linear quadratic regulator problem, when the true system is not known",
        "To circumvent both of these issues, we propose the following Monte Carlo (MC) approximation of Jc(K), c",
        "{exAilsst,iBngls}m:e=thaordgsm:An,oBmPinaNr=l:1sPtanTt=d1a|rxdrtLQARxutrsi1ng the nominal model from r\n1|2; worst-case: optimize for worst-case model (95% confidence) s.t. robust stability constraints, i.e., the method of [17, \u00a75.2]; H2/H1: enforce stability constraint from [17, \u00a75.2], but optimize performance for the nominal model {Als, Bls}; proposed method(s): CL: the common Lyapunov relaxation of 8; (v) proposed: the method proposed in this paper, i.e., Algorithm 1; additional new methods: alternate-r: initialize with the H2/H1 solution, and apply the iterative optimization method proposed in Section 4.2, cf"
    ],
    "summary": [
        "Decision making for dynamical systems in the presence of uncertainty is a problem of great prevalence and importance, as well as considerable difficulty, especially when knowledge of the dynamics is available only via limited observations of system behavior.",
        "The problem we address in this paper lies at the intersection of reinforcement learning and robust control, and can be summarized as follows: given observations from an unknown dynamical system, we seek a policy to optimize the expected cost, subject to certain robust stability guarantees.",
        "Of particular relevance to the present paper is [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which provides sample complexity bounds on the performance of robust control synthesis for the infinite horizon LQR problem, when the true system is not known.",
        "Such bounds necessarily consider the worst-case model, given the observed data, where as we are concerned with expected cost over the posterior distribution of models.",
        "Of particular relevance to the present paper is the so-called \u2018scenario approach\u2019 to control: robustness requirements lead to semi-infinite convex programs, which are approximated by sampling a finite number of constraints, cf.",
        "Optimization objective Given observed data and, possibly, prior knowledge of the system, we have the posterior distribution over the model parameters denoted \u21e1(\u2713) := p(A, B, \u21e7|D), in place of the true parameters \u2713tr.",
        "As an alternative to (2) we may consider a cost function like Jc(K) := \u21e5c J(K|\u2713)\u21e1(\u2713)d\u2713, where \u21e5c denotes a c % confidence region of the parameter space w.r.t. the posterior \u21e1.",
        "Iterative algorithm To improve upon the common Lyapunov solution given by (8), we can solve a sequence of convex optimization problems: (k+1)",
        "We have considered the performance component of the robust control problem, namely minimization of the expected cost; we address the robust stability requirement.",
        "If closed-loop stability of each sampled model is verified with a common Lyapunov function, the policy stabilizes the convex hull of the sampled systems: Theorem 4.2.",
        "We observe empirically that Algorithm 1 returns policies that very nearly stabilize the entire region \u21e5c, despite a very modest number of samples M relative to the dimension of the parameter space, cf.",
        "A number of recent papers have investigated sampling based approaches to stability verification of control policies, e.g., [<a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].",
        "1|2; worst-case: optimize for worst-case model (95% confidence) s.t. robust stability constraints, i.e., the method of [17, \u00a75.2]; H2/H1: enforce stability constraint from [17, \u00a75.2], but optimize performance for the nominal model {Als, Bls}; proposed method(s): CL: the common Lyapunov relaxation of 8; (v) proposed: the method proposed in this paper, i.e., Algorithm 1; additional new methods: alternate-r: initialize with the H2/H1 solution, and apply the iterative optimization method proposed in Section 4.2, cf."
    ],
    "headline": "We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. Abbeel and A. Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In Proceedings of the 22nd international conference on Machine learning, pages 1\u20138. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Exploration%20and%20apprenticeship%20learning%20in%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Exploration%20and%20apprenticeship%20learning%20in%20reinforcement%20learning%202005"
        },
        {
            "id": "2",
            "entry": "[2] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man\u00e9. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "3",
            "entry": "[3] A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin. Provably safe and robust learning-based model predictive control. Automatica, 49(5):1216\u20131226, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aswani%2C%20A.%20Gonzalez%2C%20H.%20Sastry%2C%20S.S.%20Tomlin%2C%20C.%20Provably%20safe%20and%20robust%20learning-based%20model%20predictive%20control%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aswani%2C%20A.%20Gonzalez%2C%20H.%20Sastry%2C%20S.S.%20Tomlin%2C%20C.%20Provably%20safe%20and%20robust%20learning-based%20model%20predictive%20control%202013"
        },
        {
            "id": "4",
            "entry": "[4] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learning with stability guarantees. In Advances in Neural Information Processing Systems (NIPS). 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berkenkamp%2C%20F.%20Turchetta%2C%20M.%20Schoellig%2C%20A.%20Krause%2C%20A.%20Safe%20model-based%20reinforcement%20learning%20with%20stability%20guarantees%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berkenkamp%2C%20F.%20Turchetta%2C%20M.%20Schoellig%2C%20A.%20Krause%2C%20A.%20Safe%20model-based%20reinforcement%20learning%20with%20stability%20guarantees%202017"
        },
        {
            "id": "5",
            "entry": "[5] D. P. Bertsekas. Dynamic programming and optimal control. Belmont, MA: Athena Scientific, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Dynamic%20programming%20and%20optimal%20control%201995"
        },
        {
            "id": "6",
            "entry": "[6] R. Bobiti and M. Lazar. A sampling approach to finding lyapunov functions for nonlinear discrete-time systems. In Control Conference (ECC), 2016 European, pages 561\u2013566. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bobiti%2C%20R.%20Lazar%2C%20M.%20A%20sampling%20approach%20to%20finding%20lyapunov%20functions%20for%20nonlinear%20discrete-time%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bobiti%2C%20R.%20Lazar%2C%20M.%20A%20sampling%20approach%20to%20finding%20lyapunov%20functions%20for%20nonlinear%20discrete-time%20systems%202016"
        },
        {
            "id": "7",
            "entry": "[7] R. Boczar, N. Matni, and B. Recht. Finite-data performance guarantees for the output-feedback control of an unknown system. arXiv preprint arXiv:1803.09186, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09186"
        },
        {
            "id": "8",
            "entry": "[8] J. B. Burl. Linear optimal control: H2 and H1 methods. Addison-Wesley Longman Publishing Co., Inc., 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burl%2C%20J.B.%20Linear%20optimal%20control%3A%20H2%20and%20H1%20methods%201998"
        },
        {
            "id": "9",
            "entry": "[9] G. C. Calafiore and M. C. Campi. Uncertain convex programs: randomized solutions and confidence levels. Mathematical Programming, 102(1):25\u201346, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calafiore%2C%20G.C.%20Campi%2C%20M.C.%20Uncertain%20convex%20programs%3A%20randomized%20solutions%20and%20confidence%20levels%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calafiore%2C%20G.C.%20Campi%2C%20M.C.%20Uncertain%20convex%20programs%3A%20randomized%20solutions%20and%20confidence%20levels%202005"
        },
        {
            "id": "10",
            "entry": "[10] G. C. Calafiore and M. C. Campi. The scenario approach to robust control design. IEEE Transactions on Automatic Control, 51(5):742\u2013753, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calafiore%2C%20G.C.%20Campi%2C%20M.C.%20The%20scenario%20approach%20to%20robust%20control%20design%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calafiore%2C%20G.C.%20Campi%2C%20M.C.%20The%20scenario%20approach%20to%20robust%20control%20design%202006"
        },
        {
            "id": "11",
            "entry": "[11] G. C. Calafiore, F. Dabbene, and R. Tempo. Research on probabilistic methods for control system design. Automatica, 47(7):1279\u20131293, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calafiore%2C%20G.C.%20Dabbene%2C%20F.%20Tempo%2C%20R.%20Research%20on%20probabilistic%20methods%20for%20control%20system%20design%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calafiore%2C%20G.C.%20Dabbene%2C%20F.%20Tempo%2C%20R.%20Research%20on%20probabilistic%20methods%20for%20control%20system%20design%202011"
        },
        {
            "id": "12",
            "entry": "[12] C. K. Carter and R. Kohn. On gibbs sampling for state space models. Biometrika, 81(3):541\u2013553, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carter%2C%20C.K.%20Kohn%2C%20R.%20On%20gibbs%20sampling%20for%20state%20space%20models%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carter%2C%20C.K.%20Kohn%2C%20R.%20On%20gibbs%20sampling%20for%20state%20space%20models%201994"
        },
        {
            "id": "13",
            "entry": "[13] J. Chen and G. Gu. Control-oriented system identification: an H1 approach, volume 19. WileyInterscience, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20J.%20Gu%2C%20G.%20Control-oriented%20system%20identification%3A%20an%202000"
        },
        {
            "id": "14",
            "entry": "[14] J. Chen and C. N. Nett. The Caratheodory-Fejer problem and H2/H1 identification: a time domain approach. In Proceedings of the 32nd IEEE Conference on Decision and Control (CDC), pages 68\u201373, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20J.%20Nett%2C%20C.N.%20The%20Caratheodory-Fejer%20problem%20and%20H2/H1%20identification%3A%20a%20time%20domain%20approach%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20J.%20Nett%2C%20C.N.%20The%20Caratheodory-Fejer%20problem%20and%20H2/H1%20identification%3A%20a%20time%20domain%20approach%201993"
        },
        {
            "id": "15",
            "entry": "[15] S. H. Cheung and J. L. Beck. Bayesian model updating using hybrid Monte Carlo simulation with application to structural dynamic models with many uncertain parameters. Journal of engineering mechanics, 135(4):243\u2013255, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheung%2C%20S.H.%20Beck%2C%20J.L.%20Bayesian%20model%20updating%20using%20hybrid%20Monte%20Carlo%20simulation%20with%20application%20to%20structural%20dynamic%20models%20with%20many%20uncertain%20parameters%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheung%2C%20S.H.%20Beck%2C%20J.L.%20Bayesian%20model%20updating%20using%20hybrid%20Monte%20Carlo%20simulation%20with%20application%20to%20structural%20dynamic%20models%20with%20many%20uncertain%20parameters%202009"
        },
        {
            "id": "16",
            "entry": "[16] J. Ching, M. Muto, and J. Beck. Bayesian linear structural model updating using Gibbs sampler with modal data. In Proceedings of the 9th International Conference on Structural Safety and Reliability, pages 2609\u20132616. Millpress, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ching%2C%20J.%20Muto%2C%20M.%20Beck%2C%20J.%20Bayesian%20linear%20structural%20model%20updating%20using%20Gibbs%20sampler%20with%20modal%20data%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ching%2C%20J.%20Muto%2C%20M.%20Beck%2C%20J.%20Bayesian%20linear%20structural%20model%20updating%20using%20Gibbs%20sampler%20with%20modal%20data%202005"
        },
        {
            "id": "17",
            "entry": "[17] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the sample complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.01688"
        },
        {
            "id": "18",
            "entry": "[18] M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465\u2013472, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.%20Rasmussen%2C%20C.E.%20Pilco%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.%20Rasmussen%2C%20C.E.%20Pilco%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011"
        },
        {
            "id": "19",
            "entry": "[19] S. Depeweg, J. M. Hern\u00e1ndez-Lobato, F. Doshi-Velez, and S. Udluft. Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning. arXiv:1710.07283, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07283"
        },
        {
            "id": "20",
            "entry": "[20] J. Doyle, K. Zhou, K. Glover, and B. Bodenheimer. Mixed H2 and H1 performance objectives. II. Optimal control. IEEE Transactions on Automatic Control, 39(8):1575\u20131587, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doyle%2C%20J.%20Zhou%2C%20K.%20Glover%2C%20K.%20Bodenheimer%2C%20B.%20Mixed%20H2%20and%20H1%20performance%20objectives%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doyle%2C%20J.%20Zhou%2C%20K.%20Glover%2C%20K.%20Bodenheimer%2C%20B.%20Mixed%20H2%20and%20H1%20performance%20objectives%201994"
        },
        {
            "id": "21",
            "entry": "[21] J. Garc\u0131a and F. Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garc%C4%B1a%2C%20J.%20Fern%C3%A1ndez%2C%20F.%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garc%C4%B1a%2C%20J.%20Fern%C3%A1ndez%2C%20F.%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015"
        },
        {
            "id": "22",
            "entry": "[22] P. Geibel and F. Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints. Journal of Artificial Intelligence Research, 24:81\u2013108, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geibel%2C%20P.%20Wysotzki%2C%20F.%20Risk-sensitive%20reinforcement%20learning%20applied%20to%20control%20under%20constraints%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geibel%2C%20P.%20Wysotzki%2C%20F.%20Risk-sensitive%20reinforcement%20learning%20applied%20to%20control%20under%20constraints%202005"
        },
        {
            "id": "23",
            "entry": "[23] S. Gu, T. Lillicrap, Z. Ghahramani, R. E. Turner, and S. Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20S.%20Lillicrap%2C%20T.%20Ghahramani%2C%20Z.%20Turner%2C%20R.E.%20Q-prop%3A%20Sample-efficient%20policy%20gradient%20with%20an%20off-policy%20critic%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20S.%20Lillicrap%2C%20T.%20Ghahramani%2C%20Z.%20Turner%2C%20R.E.%20Q-prop%3A%20Sample-efficient%20policy%20gradient%20with%20an%20off-policy%20critic%202017"
        },
        {
            "id": "24",
            "entry": "[24] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep Q-learning with model-based acceleration. In International Conference on Machine Learning, pages 2829\u20132838, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20S.%20Lillicrap%2C%20T.%20Sutskever%2C%20I.%20Levine%2C%20S.%20Continuous%20deep%20Q-learning%20with%20model-based%20acceleration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20S.%20Lillicrap%2C%20T.%20Sutskever%2C%20I.%20Levine%2C%20S.%20Continuous%20deep%20Q-learning%20with%20model-based%20acceleration%202016"
        },
        {
            "id": "25",
            "entry": "[25] W. M. Haddad, D. S. Bernstein, and D. Mustafa. Mixed-norm H2/H1 regulation and estimation: The discrete-time case. Systems & Control Letters, 16(4):235\u2013247, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haddad%2C%20W.M.%20Bernstein%2C%20D.S.%20Mustafa%2C%20D.%20Mixed-norm%20H2/H1%20regulation%20and%20estimation%3A%20The%20discrete-time%20case%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haddad%2C%20W.M.%20Bernstein%2C%20D.S.%20Mustafa%2C%20D.%20Mixed-norm%20H2/H1%20regulation%20and%20estimation%3A%20The%20discrete-time%20case%201991"
        },
        {
            "id": "26",
            "entry": "[26] A. J. Helmicki, C. A. Jacobson, and C. N. Nett. Control oriented system identification: a worstcase/deterministic approach in H1. IEEE Transactions on Automatic control, 36(10):1163\u20131176, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Helmicki%2C%20A.J.%20Jacobson%2C%20C.A.%20Nett%2C%20C.N.%20Control%20oriented%20system%20identification%3A%20a%20worstcase/deterministic%20approach%20in%20H1%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Helmicki%2C%20A.J.%20Jacobson%2C%20C.A.%20Nett%2C%20C.N.%20Control%20oriented%20system%20identification%3A%20a%20worstcase/deterministic%20approach%20in%20H1%201991"
        },
        {
            "id": "27",
            "entry": "[27] H. K. Khalil. Noninear systems. Prentice-Hall, New Jersey, 2(5):5\u20131, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khalil%2C%20H.K.%20Noninear%20systems%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khalil%2C%20H.K.%20Noninear%20systems%201996"
        },
        {
            "id": "28",
            "entry": "[28] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of machine learning research, 4(Dec):1107\u20131149, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lagoudakis%2C%20M.G.%20Parr%2C%20R.%20Least-squares%20policy%20iteration%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lagoudakis%2C%20M.G.%20Parr%2C%20R.%20Least-squares%20policy%20iteration%202003"
        },
        {
            "id": "29",
            "entry": "[29] K. Lange, D. R. Hunter, and I. Yang. Optimization transfer using surrogate objective functions. Journal of computational and graphical statistics, 9(1):1\u201320, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lange%2C%20K.%20Hunter%2C%20D.R.%20Yang%2C%20I.%20Optimization%20transfer%20using%20surrogate%20objective%20functions%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lange%2C%20K.%20Hunter%2C%20D.R.%20Yang%2C%20I.%20Optimization%20transfer%20using%20surrogate%20objective%20functions%202000"
        },
        {
            "id": "30",
            "entry": "[30] Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approximation with application to system identification. SIAM J. Matrix Analysis and Applications, 31(3):1235\u20131256, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Z.%20Vandenberghe%2C%20L.%20Interior-point%20method%20for%20nuclear%20norm%20approximation%20with%20application%20to%20system%20identification%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Z.%20Vandenberghe%2C%20L.%20Interior-point%20method%20for%20nuclear%20norm%20approximation%20with%20application%20to%20system%20identification%202009"
        },
        {
            "id": "31",
            "entry": "[31] O. Mihatsch and R. Neuneier. Risk-sensitive reinforcement learning. Machine learning, 49(2-3):267\u2013290, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mihatsch%2C%20O.%20Neuneier%2C%20R.%20Risk-sensitive%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mihatsch%2C%20O.%20Neuneier%2C%20R.%20Risk-sensitive%20reinforcement%20learning%202002"
        },
        {
            "id": "32",
            "entry": "[32] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "33",
            "entry": "[33] B. Ninness and S. Henriksen. Bayesian system identification via Markov chain Monte Carlo techniques. Automatica, 46(1):40\u201351, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ninness%2C%20B.%20Henriksen%2C%20S.%20Bayesian%20system%20identification%20via%20Markov%20chain%20Monte%20Carlo%20techniques%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ninness%2C%20B.%20Henriksen%2C%20S.%20Bayesian%20system%20identification%20via%20Markov%20chain%20Monte%20Carlo%20techniques%202010"
        },
        {
            "id": "34",
            "entry": "[34] Oliveira, Maur\u00edcio de. MAE 280B: Linear Control Design., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oliveira%20Maur%C3%ADcio%20de%20MAE%20280B%20Linear%20Control%20Design%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oliveira%20Maur%C3%ADcio%20de%20MAE%20280B%20Linear%20Control%20Design%202009"
        },
        {
            "id": "35",
            "entry": "[35] C. J. Ostafew, A. P. Schoellig, and T. D. Barfoot. Robust Constrained Learning-based NMPC enabling reliable mobile robot path tracking. The International Journal of Robotics Research, 35(13):1547\u20131563, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ostafew%2C%20C.J.%20Schoellig%2C%20A.P.%20Barfoot%2C%20T.D.%20Robust%20Constrained%20Learning-based%20NMPC%20enabling%20reliable%20mobile%20robot%20path%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ostafew%2C%20C.J.%20Schoellig%2C%20A.P.%20Barfoot%2C%20T.D.%20Robust%20Constrained%20Learning-based%20NMPC%20enabling%20reliable%20mobile%20robot%20path%20tracking%202016"
        },
        {
            "id": "36",
            "entry": "[36] I. R. Petersen, M. R. James, and P. Dupuis. Minimax optimal control of stochastic uncertain systems with relative entropy constraints. IEEE Transactions on Automatic Control, 45(3):398\u2013412, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petersen%2C%20I.R.%20James%2C%20M.R.%20Dupuis%2C%20P.%20Minimax%20optimal%20control%20of%20stochastic%20uncertain%20systems%20with%20relative%20entropy%20constraints%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petersen%2C%20I.R.%20James%2C%20M.R.%20Dupuis%2C%20P.%20Minimax%20optimal%20control%20of%20stochastic%20uncertain%20systems%20with%20relative%20entropy%20constraints%202000"
        },
        {
            "id": "37",
            "entry": "[37] K. B. Petersen and M. S. Pedersen. The Matrix Cookbook., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20B%20Petersen%20and%20M%20S%20Pedersen%20The%20Matrix%20Cookbook%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20B%20Petersen%20and%20M%20S%20Pedersen%20The%20Matrix%20Cookbook%202012"
        },
        {
            "id": "38",
            "entry": "[38] I. Postlethwaite, M. C. Turner, and G. Herrmann. Robust control applications. Annual Reviews in Control, 31(1):27\u201339, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Postlethwaite%2C%20I.%20Turner%2C%20M.C.%20Herrmann%2C%20G.%20Robust%20control%20applications%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Postlethwaite%2C%20I.%20Turner%2C%20M.C.%20Herrmann%2C%20G.%20Robust%20control%20applications%202007"
        },
        {
            "id": "40",
            "entry": "[40] I. M. Ross, R. J. Proulx, and M. Karpenko. Unscented optimal control for space flight. In 24th International Symposium on Space Flight Dynamics, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20I.M.%20Proulx%2C%20R.J.%20Karpenko%2C%20M.%20Unscented%20optimal%20control%20for%20space%20flight%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20I.M.%20Proulx%2C%20R.J.%20Karpenko%2C%20M.%20Unscented%20optimal%20control%20for%20space%20flight%202014"
        },
        {
            "id": "41",
            "entry": "[41] I. M. Ross, R. J. Proulx, M. Karpenko, and Q. Gong. Riemann\u2013stieltjes optimal control problems for uncertain dynamic systems. Journal of Guidance, Control, and Dynamics, 38(7):1251\u20131263, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20I.M.%20Proulx%2C%20R.J.%20Karpenko%2C%20M.%20Gong%2C%20Q.%20Riemann%E2%80%93stieltjes%20optimal%20control%20problems%20for%20uncertain%20dynamic%20systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20I.M.%20Proulx%2C%20R.J.%20Karpenko%2C%20M.%20Gong%2C%20Q.%20Riemann%E2%80%93stieltjes%20optimal%20control%20problems%20for%20uncertain%20dynamic%20systems%202015"
        },
        {
            "id": "42",
            "entry": "[42] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "43",
            "entry": "[43] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "44",
            "entry": "[44] M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Learning without mixing: Towards a sharp analysis of linear system identification. arXiv preprint arXiv:1802.08334, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08334"
        },
        {
            "id": "45",
            "entry": "[45] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "46",
            "entry": "[46] R. Tempo, G. Calafiore, and F. Dabbene. Randomized algorithms for analysis and control of uncertain systems: with applications. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tempo%2C%20R.%20Calafiore%2C%20G.%20Dabbene%2C%20F.%20Randomized%20algorithms%20for%20analysis%20and%20control%20of%20uncertain%20systems%3A%20with%20applications%202012"
        },
        {
            "id": "47",
            "entry": "[47] S. Tu, R. Boczar, A. Packard, and B. Recht. Non-asymptotic analysis of robust control from coarse-grained identification. arXiv preprint arXiv:1707.04791, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04791"
        },
        {
            "id": "48",
            "entry": "[48] S. Tu and B. Recht. Least-squares temporal difference learning for the linear quadratic regulator. arXiv preprint arXiv:1712.08642, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08642"
        },
        {
            "id": "49",
            "entry": "[49] F. Vaida. Parameter convergence for EM and MM algorithms. Statistica Sinica, pages 831\u2013840, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaida%2C%20F.%20Parameter%20convergence%20for%20EM%20and%20MM%20algorithms%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaida%2C%20F.%20Parameter%20convergence%20for%20EM%20and%20MM%20algorithms%202005"
        },
        {
            "id": "50",
            "entry": "[50] L. Vandenberghe and S. Boyd. Semidefinite programming. SIAM review, 38(1):49\u201395, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vandenberghe%2C%20L.%20Boyd%2C%20S.%20Semidefinite%20programming%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vandenberghe%2C%20L.%20Boyd%2C%20S.%20Semidefinite%20programming%201996"
        },
        {
            "id": "51",
            "entry": "[51] M. Vidyasagar. Statistical learning theory and randomized algorithms for control. IEEE Control Systems, 18(6):69\u201385, 1998. Automatica, 37(10):1515\u20131528, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vidyasagar%2C%20M.%20Statistical%20learning%20theory%20and%20randomized%20algorithms%20for%20control%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vidyasagar%2C%20M.%20Statistical%20learning%20theory%20and%20randomized%20algorithms%20for%20control%201998"
        },
        {
            "id": "53",
            "entry": "[53] M. Vidyasagar. A theory of learning and generalization. Springer-Verlag New York, Inc., 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vidyasagar%2C%20M.%20A%20theory%20of%20learning%20and%20generalization%202002"
        },
        {
            "id": "54",
            "entry": "[54] J. Vinogradska, B. Bischoff, D. Nguyen-Tuong, A. Romer, H. Schmidt, and J. Peters. Stability of controllers for gaussian process forward models. In International Conference on Machine Learning, pages 545\u2013554, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinogradska%2C%20J.%20Bischoff%2C%20B.%20Nguyen-Tuong%2C%20D.%20Romer%2C%20A.%20Stability%20of%20controllers%20for%20gaussian%20process%20forward%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinogradska%2C%20J.%20Bischoff%2C%20B.%20Nguyen-Tuong%2C%20D.%20Romer%2C%20A.%20Stability%20of%20controllers%20for%20gaussian%20process%20forward%20models%202016"
        },
        {
            "id": "55",
            "entry": "[55] Y.-S. Wang, N. Matni, and J. C. Doyle. A system level approach to controller synthesis. arXiv preprint arXiv:1610.04815, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04815"
        },
        {
            "id": "56",
            "entry": "[56] A. Wills, T. B. Sch\u00f6n, F. Lindsten, and B. Ninness. Estimation of linear systems using a Gibbs sampler. IFAC Proceedings Volumes, 45(16):203\u2013208, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wills%2C%20A.%20Sch%C3%B6n%2C%20T.B.%20Lindsten%2C%20F.%20Ninness%2C%20B.%20Estimation%20of%20linear%20systems%20using%20a%20Gibbs%20sampler%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wills%2C%20A.%20Sch%C3%B6n%2C%20T.B.%20Lindsten%2C%20F.%20Ninness%2C%20B.%20Estimation%20of%20linear%20systems%20using%20a%20Gibbs%20sampler%202012"
        },
        {
            "id": "57",
            "entry": "[57] K. Zhou and J. C. Doyle. Essentials of robust control, volume 104. Prentice Hall Upper Saddle River, NJ, 1998. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20K.%20Doyle%2C%20J.C.%20Essentials%20of%20robust%20control%2C%20volume%20104%201998"
        }
    ]
}
