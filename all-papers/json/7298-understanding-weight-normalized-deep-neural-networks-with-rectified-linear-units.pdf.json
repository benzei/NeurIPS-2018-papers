{
    "filename": "7298-understanding-weight-normalized-deep-neural-networks-with-rectified-linear-units.pdf",
    "metadata": {
        "title": "Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units",
        "author": "Yixi Xu, Xiao Wang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7298-understanding-weight-normalized-deep-neural-networks-with-rectified-linear-units.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper presents a general framework for norm-based capacity control for Lp,q weight normalized deep neural networks. We establish the upper bound on the Rademacher complexities of this family. With an Lp,q normalization where q \u2264 p\u2217 and 1/p+1/p\u2217 = 1, we discuss properties of a width-independent capacity control, which only depends on the depth by a square root term. We further analyze the approximation properties of Lp,q weight normalized deep neural networks. In particular, for an L1,\u221e weight normalized network, the approximation error can be controlled by the L1 norm of the output layer, and the corresponding generalization error only depends on the architecture by the square root of the depth."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "approximation error",
            "url": "https://en.wikipedia.org/wiki/approximation_error"
        },
        {
            "term": "image recognition",
            "url": "https://en.wikipedia.org/wiki/image_recognition"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        }
    ],
    "highlights": [
        "Deep neural networks (DNNs) have demonstrated an amazing performance in solving many complex artificial intelligence tasks such as object recognition and identification, text understanding and translation, question answering, and more [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "There is a central question waiting for an answer: Can we bound the capacity of fully connected deep neural networks with bias neurons by weight normalization alone, which has the least dependence on the architecture?",
        "We focus on networks with rectified linear units (ReLU) and study a more general weight normalized deep neural network (WN-deep neural networks), which includes all layer-wise Lp,q weight normalizations",
        "While fixing the architecture of neural networks, these k+1 works imply that Wi \u2217 is sufficient to control the Rademacher complexity of the function class i=1 represented by these deep neural networks, where \u00b7 \u2217 is the spectral norm in [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], the L1,\u221e norm in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], the L1,\u221e/L2,2 norm in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and the Lp,q norm in [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] for any p \u2208 [1, \u221e), q \u2208 [1, \u221e]",
        "We present a general framework for capacity control on WN-deep neural networks",
        "We provide a satisfying answer for the central question: we obtain the generalization bounds for L1,\u221e WN-deep neural networks that grows with depth by a square root term while getting the approximation error controlled"
    ],
    "key_statements": [
        "Deep neural networks (DNNs) have demonstrated an amazing performance in solving many complex artificial intelligence tasks such as object recognition and identification, text understanding and translation, question answering, and more [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "There is a central question waiting for an answer: Can we bound the capacity of fully connected deep neural networks with bias neurons by weight normalization alone, which has the least dependence on the architecture?",
        "We focus on networks with rectified linear units (ReLU) and study a more general weight normalized deep neural network (WN-deep neural networks), which includes all layer-wise Lp,q weight normalizations",
        "We establish the upper bound on the Rademacher complexities of this family and study the theoretical properties of WN-deep neural networks in terms of the approximation error",
        "We examine the approximation error of WN-deep neural networks",
        "While fixing the architecture of neural networks, these k+1 works imply that Wi \u2217 is sufficient to control the Rademacher complexity of the function class i=1 represented by these deep neural networks, where \u00b7 \u2217 is the spectral norm in [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], the L1,\u221e norm in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], the L1,\u221e/L2,2 norm in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and the Lp,q norm in [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] for any p \u2208 [1, \u221e), q \u2208 [1, \u221e]",
        "An Lp,q WN-deep neural networks by a normalization constant c \u2265 1 with k hidden layers is defined by a set of k + 1 affine transformations T1 : Rd0 \u2192 Rd1 , T2 : Rd1 \u2192 Rd2 , \u00b7 \u00b7 \u00b7 , Tk+1 : Rdk \u2192 Rdk+1 and the rectified linear units activation, where Ti(u) = V Ti (1, uT )T , Vi \u2208 R\u00d7di and",
        "We present a general framework for capacity control on WN-deep neural networks",
        "We provide a satisfying answer for the central question: we obtain the generalization bounds for L1,\u221e WN-deep neural networks that grows with depth by a square root term while getting the approximation error controlled"
    ],
    "summary": [
        "Deep neural networks (DNNs) have demonstrated an amazing performance in solving many complex artificial intelligence tasks such as object recognition and identification, text understanding and translation, question answering, and more [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "To the best of our knowledge, this is the first theoretical result for the fully connected DNNs including a bias neuron for each hidden layer in terms of generalization.",
        "Claim 1 shows the failure of current norm-based constraints on fully connected neural networks with the bias neuron in each hidden layer.",
        "While fixing the architecture of neural networks, these k+1 works imply that Wi \u2217 is sufficient to control the Rademacher complexity of the function class i=1 represented by these DNNs, where \u00b7 \u2217 is the spectral norm in [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], the L1,\u221e norm in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], the L1,\u221e/L2,2 norm in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and the Lp,q norm in [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] for any p \u2208 [1, \u221e), q \u2208 [1, \u221e].",
        "The Lp,q WN-DNNs. An Lp,q WN-DNN by a normalization constant c \u2265 1 with k hidden layers is defined by a set of k + 1 affine transformations T1 : Rd0 \u2192 Rd1 , T2 : Rd1 \u2192 Rd2 , \u00b7 \u00b7 \u00b7 , Tk+1 : Rdk \u2192 Rdk+1 and the ReLU activation, where Ti(u) = V Ti (1, uT )T , Vi \u2208 R\u00d7di and",
        "Define Npk,,qd,c,co as the collection of all functions that could be represented by an Lp,q WN-DNN with the normalization constant c satisfying: (a) The number of neurons in the ith hidden layer is di for i = 1, 2, \u00b7 \u00b7 \u00b7 , k.",
        "[<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] provides the up-to-date Rademacher complexity bounds of both L1,\u221e and L2,2 norm-constrained fully connected DNNs without bias neurons.",
        "It is not straightforward to extend their results to fully connected DNNs with a bias neuron in each hidden layer.",
        "[<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] is the most recent work on the Rademacher complexities of the L1,\u221e and L2,2 norm-constrained fully connected DNNs without bias neurons.",
        "As summarized in Table 1, these comparisons suggest that the inclusion of a bias neuron in each hidden layer might lead to extra de\u221apendence of generalization bounds on\u221athe depth especially when c is small.",
        "Based on Lemma 1, we establish that a WN-DNN is able to approximate any Lipschitz-continuous function arbitrarily well by loosing the constraint for the norm of the output layer and either widening or deepening the neural network at the same time.",
        "Assume that the loss function is 1-Lipschitz continuous, the dependence of the corresponding generalization bound on the architecture for each Npk,,qd,wkid1k/q,2co defined above are summarized as follows:",
        "Besides the extension to convolutional neural networks, we are working on the design of effective algorithms for L1,\u221e WN-DNNs"
    ],
    "headline": "This paper presents a general framework for norm-based capacity control for Lp,q weight normalized deep neural networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20network%20learning%3A%20Theoretical%20foundations%202009"
        },
        {
            "id": "2",
            "entry": "[2] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Raman%20Basu%2C%20Amitabh%20Mianjy%2C%20Poorya%20Mukherjee%2C%20Anirbit%20Understanding%20deep%20neural%20networks%20with%20rectified%20linear%20units%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Raman%20Basu%2C%20Amitabh%20Mianjy%2C%20Poorya%20Mukherjee%2C%20Anirbit%20Understanding%20deep%20neural%20networks%20with%20rectified%20linear%20units%202018"
        },
        {
            "id": "3",
            "entry": "[3] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1\u201353, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE transactions on Information Theory, 44(2):525\u2013536, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20The%20sample%20complexity%20of%20pattern%20classification%20with%20neural%20networks%3A%20the%20size%20of%20the%20weights%20is%20more%20important%20than%20the%20size%20of%20the%20network%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20The%20sample%20complexity%20of%20pattern%20classification%20with%20neural%20networks%3A%20the%20size%20of%20the%20weights%20is%20more%20important%20than%20the%20size%20of%20the%20network%201998"
        },
        {
            "id": "5",
            "entry": "[5] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "6",
            "entry": "[6] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "7",
            "entry": "[7] St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Olivier Bousquet. Concentration inequalities. In Summer School on Machine Learning, pages 208\u2013240, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boucheron%2C%20St%C3%A9phane%20Lugosi%2C%20G%C3%A1bor%20Bousquet%2C%20Olivier%20Concentration%20inequalities%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boucheron%2C%20St%C3%A9phane%20Lugosi%2C%20G%C3%A1bor%20Bousquet%2C%20Olivier%20Concentration%20inequalities%202003"
        },
        {
            "id": "8",
            "entry": "[8] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303\u2013314, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20George%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function.%20Mathematics%20of%20control%2C%20signals%20and%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20George%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function.%20Mathematics%20of%20control%2C%20signals%20and%201989"
        },
        {
            "id": "9",
            "entry": "[9] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pages 907\u2013940, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016"
        },
        {
            "id": "10",
            "entry": "[10] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Proceedings of the 31st Conference On Learning Theory, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018"
        },
        {
            "id": "11",
            "entry": "[11] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Deep%20learning%2C%20volume%201%202016"
        },
        {
            "id": "12",
            "entry": "[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "13",
            "entry": "[13] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251\u2013257, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hornik%2C%20Kurt%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hornik%2C%20Kurt%20Approximation%20capabilities%20of%20multilayer%20feedforward%20networks%201991"
        },
        {
            "id": "14",
            "entry": "[14] Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems, pages 793\u2013800, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202009"
        },
        {
            "id": "15",
            "entry": "[15] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20Michel%20Talagrand%2C%20Michel%20Probability%20in%20Banach%20Spaces%3A%20isoperimetry%20and%20processes%202013"
        },
        {
            "id": "16",
            "entry": "[16] Shiyu Liang and R Srikant. Why deep neural networks for function approximation? In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Shiyu%20Srikant%2C%20R.%20Why%20deep%20neural%20networks%20for%20function%20approximation%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Shiyu%20Srikant%2C%20R.%20Why%20deep%20neural%20networks%20for%20function%20approximation%3F%202017"
        },
        {
            "id": "17",
            "entry": "[17] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Talwalkar%2C%20Ameet%20Foundations%20of%20machine%20learning%202012"
        },
        {
            "id": "18",
            "entry": "[18] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018"
        },
        {
            "id": "19",
            "entry": "[19] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pages 1376\u20131401, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Tomioka%2C%20Ryota%20Srebro%2C%20Nathan%20Norm-based%20capacity%20control%20in%20neural%20networks%202015"
        },
        {
            "id": "20",
            "entry": "[20] Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143\u2013195, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinkus%2C%20Allan%20Approximation%20theory%20of%20the%20mlp%20model%20in%20neural%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinkus%2C%20Allan%20Approximation%20theory%20of%20the%20mlp%20model%20in%20neural%20networks%201999"
        },
        {
            "id": "21",
            "entry": "[21] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2979\u20132987, International Convention Centre, Sydney, Australia, 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Depth-width%20tradeoffs%20in%20approximating%20natural%20functions%20with%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20Depth-width%20tradeoffs%20in%20approximating%20natural%20functions%20with%20neural%20networks%202017"
        },
        {
            "id": "22",
            "entry": "[22] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901\u2013909, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "23",
            "entry": "[23] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Ben-David%2C%20Shai%20Understanding%20machine%20learning%3A%20From%20theory%20to%20algorithms%202014"
        },
        {
            "id": "24",
            "entry": "[24] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of online learning algorithms. Machine Learning, 69(2-3):115\u2013142, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20A%20primal-dual%20perspective%20of%20online%20learning%20algorithms%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20A%20primal-dual%20perspective%20of%20online%20learning%20algorithms%202007"
        },
        {
            "id": "25",
            "entry": "[25] Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural networks: A theoretical view. In AAAI, pages 2066\u20132072, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Shizhao%20Chen%2C%20Wei%20Wang%2C%20Liwei%20Xiaoguang%20Liu%2C%20and%20Tie-Yan%20Liu.%20On%20the%20depth%20of%20deep%20neural%20networks%3A%20A%20theoretical%20view%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Shizhao%20Chen%2C%20Wei%20Wang%2C%20Liwei%20Xiaoguang%20Liu%2C%20and%20Tie-Yan%20Liu.%20On%20the%20depth%20of%20deep%20neural%20networks%3A%20A%20theoretical%20view%202016"
        },
        {
            "id": "26",
            "entry": "[26] Matus Telgarsky. Benefits of depth in neural networks. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 1517\u20131539, Columbia University, New York, New York, USA, 2016. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Telgarsky%2C%20Matus%20Benefits%20of%20depth%20in%20neural%20networks.%20In%2029th%20Annual%20Conference%20on%20Learning%20Theory%2C%20volume%2049%20of%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Telgarsky%2C%20Matus%20Benefits%20of%20depth%20in%20neural%20networks.%20In%2029th%20Annual%20Conference%20on%20Learning%20Theory%2C%20volume%2049%20of%202016"
        },
        {
            "id": "27",
            "entry": "[27] Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:103\u2013114, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yarotsky%2C%20Dmitry%20Error%20bounds%20for%20approximations%20with%20deep%20relu%20networks%202017"
        },
        {
            "id": "28",
            "entry": "[28] Tong Zhang. Statistical analysis of some multi-category large margin classification methods. Journal of Machine Learning Research, 5:1225\u20131251, 2004. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Statistical%20analysis%20of%20some%20multi-category%20large%20margin%20classification%20methods%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Statistical%20analysis%20of%20some%20multi-category%20large%20margin%20classification%20methods%202004"
        }
    ]
}
