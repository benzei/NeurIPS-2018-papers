{
    "filename": "7708-hunting-for-discriminatory-proxies-in-linear-regression-models.pdf",
    "metadata": {
        "title": "Hunting for Discriminatory Proxies in Linear Regression Models",
        "author": "Samuel Yeom, Anupam Datta, Matt Fredrikson",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7708-hunting-for-discriminatory-proxies-in-linear-regression-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models, and characterizes a model\u2019s constituent behavior that: 1) correlates closely with a protected random variable, and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a \u201cbusiness necessity\u201d. Finally, we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes, demonstrating that proxies shed useful light on the causes of discriminatory behavior in models."
    },
    "keywords": [
        {
            "term": "supreme court",
            "url": "https://en.wikipedia.org/wiki/supreme_court"
        },
        {
            "term": "linear regression model",
            "url": "https://en.wikipedia.org/wiki/linear_regression_model"
        },
        {
            "term": "COMPAS",
            "url": "https://en.wikipedia.org/wiki/COMPAS"
        },
        {
            "term": "disparate impact",
            "url": "https://en.wikipedia.org/wiki/disparate_impact"
        }
    ],
    "highlights": [
        "The use of machine learning in domains like insurance [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], criminal justice [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and child welfare [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] raises concerns about fairness, as decisions based on model predictions may discriminate on the basis of demographic attributes like race and gender",
        "The U.S has recognized the doctrine of disparate impact since 1971, when the Supreme Court held in Griggs v",
        "Duke Power Co. [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] that the Duke Power Company had discriminated against its black employees by requiring a high-school diploma for promotion when the diploma had little to do with competence in the new job",
        "Because disparate impact is not always forbidden, we extend our definition to account for an exempt input variable whose use for a particular problem is justified",
        "The creator of the Strategic Subject List model claims that the model avoids variables that could lead to discrimination [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], and if",
        "Developing learning rules that account for proxy use, leading to models without proxies above specified thresholds, is an intriguing direction with direct potential for impact on practical scenarios"
    ],
    "key_statements": [
        "The use of machine learning in domains like insurance [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], criminal justice [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and child welfare [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] raises concerns about fairness, as decisions based on model predictions may discriminate on the basis of demographic attributes like race and gender",
        "These concerns are driven by highprofile examples of models that appear to have discriminatory effect, ranging from gender bias in job advertisements [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] to racial bias in same-day delivery services [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] and predictive policing [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "The U.S has recognized the doctrine of disparate impact since 1971, when the Supreme Court held in Griggs v",
        "Duke Power Co. [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] that the Duke Power Company had discriminated against its black employees by requiring a high-school diploma for promotion when the diploma had little to do with competence in the new job",
        "These regulations pose a challenge for machine learning models, which may give discriminatory predictions as an unintentional side effect of misconfiguration or biased training data",
        "It is important to find a workable standard for detecting discriminatory behavior in models",
        "Because disparate impact is not always forbidden, we extend our definition to account for an exempt input variable whose use for a particular problem is justified",
        "We show that slight modifications to our detection algorithm allow us to effectively \u201cignore\u201d proxies based on the exempt variable (Section 4)",
        "We find that the algorithm, despite taking little time to run, accurately identifies parts of the model that are the most problematic in terms of disparate impact",
        "We show in Section 2.5 that proxy use is a stronger notion of fairness than demographic parity, of which the four-fifths rule is a relaxation",
        "As we show in the rest of this paper, the two-metric-based approach of Datta et al leads to an efficient proxy detection algorithm.\n2 Proxy Use",
        "We present a definition of proxy use that is suited to linear regression models",
        "We present Definition 1 as the notion of subprogram that we use to define proxy use in the setting of linear regression",
        "Having defined a component as the equivalent of a subprogram in a linear regression model, we formalize the association and influence conditions given by Datta et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "In Section 5, we evaluate how these algorithms perform on real-world data",
        "The creator of the Strategic Subject List model claims that the model avoids variables that could lead to discrimination [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], and if",
        "Our algorithms pinpoint components of the model that are the most problematic in terms of disparate impact, and we find that the exemption policy discussed in Section 4 removes the appropriate proxies from the Strategic Subject List model",
        "We briefly describe the dataset and present the experimental results, demonstrating how the identified proxies can provide evidence of discriminatory behavior in models",
        "We found very strong proxies for race in the model trained with the Communities and Crimes dataset",
        "Developing learning rules that account for proxy use, leading to models without proxies above specified thresholds, is an intriguing direction with direct potential for impact on practical scenarios"
    ],
    "summary": [
        "The use of machine learning in domains like insurance [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], criminal justice [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and child welfare [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] raises concerns about fairness, as decisions based on model predictions may discriminate on the basis of demographic attributes like race and gender.",
        "We define a notion of proxy use (Section 2) for linear regression models, and show how it can be used to inform considerations of fairness and discrimination.",
        "Because disparate impact is not always forbidden, we extend our definition to account for an exempt input variable whose use for a particular problem is justified.",
        "We present a definition of proxy use that is suited to linear regression models.",
        "Having defined a component as the equivalent of a subprogram in a linear regression model, we formalize the association and influence conditions given by Datta et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "Equation 3 shows that our association measure is related to demographic parity in regression models.",
        "The linear regression model Y = \u03b21X1 + \u00b7 \u00b7 \u00b7 + \u03b2nXn contains a proxy if and only if there exists a solution to Problem 1 with s \u2208 {\u22121, 1} such that InflY (P ) \u2265 \u03b4.",
        "Theorem 1 guarantees the correctness of the following proxy detection algorithm: Run Problem 1 with s = 1 and s = \u22121, and compute the association and influence of the resulting solutions.",
        "The model contains a proxy if and only if any of the solutions passes both the association and the influence thresholds.",
        "If the linear regression model Y = \u03b21X1 + \u00b7 \u00b7 \u00b7 + \u03b2nXn contains a proxy, there exists a solution to Problem 2 with s \u2208 {\u22121, 1} such that cT \u03b1 \u2265 (\u03b4 Var(Y ))0.5.",
        "Let P be a proxy component of a linear regression model, and let X1 be the exempt variable.",
        "Our algorithms pinpoint components of the model that are the most problematic in terms of disparate impact, and we find that the exemption policy discussed in Section 4 removes the appropriate proxies from the SSL model.",
        "A cursory analysis suggested that the variables used in these proxies are not justifiable correlates of race, so an exemption policy may not suffice to \u201cexplain away\u201d the discriminatory behavior of the model.",
        "We have formalized the notion of proxy discrimination in linear regression models and presented an efficient proxy detection algorithm.",
        "We account for the case where the use of one variable is justified, and extending this result to multiple exempt variables is valuable future work that would enable better handling of models like C&C that take many closely related input variables.",
        "Developing learning rules that account for proxy use, leading to models without proxies above specified thresholds, is an intriguing direction with direct potential for impact on practical scenarios"
    ],
    "headline": "We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a \u201cbusiness necessity\u201d",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Philip Adler, Casey Falk, Sorelle A Friedler, Tionney Nix, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. Auditing black-box models for indirect influence. Knowledge and Information Systems, 54(1):95\u2013122, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adler%2C%20Philip%20Falk%2C%20Casey%20Friedler%2C%20Sorelle%20A.%20Nix%2C%20Tionney%20Auditing%20black-box%20models%20for%20indirect%20influence%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adler%2C%20Philip%20Falk%2C%20Casey%20Friedler%2C%20Sorelle%20A.%20Nix%2C%20Tionney%20Auditing%20black-box%20models%20for%20indirect%20influence%202018"
        },
        {
            "id": "2",
            "entry": "[2] Martin S Andersen, Joachim Dahl, and Lieven Vandenberghe. CVXOPT: Python software for convex optimization. http://cvxopt.org.",
            "url": "http://cvxopt.org"
        },
        {
            "id": "3",
            "entry": "[3] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias: There\u2019s software used across the country to predict future criminals. and it\u2019s biased against blacks. ProPublica, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angwin%2C%20Julia%20Larson%2C%20Jeff%20Mattu%2C%20Surya%20Kirchner%2C%20Lauren%20Machine%20bias%3A%20There%E2%80%99s%20software%20used%20across%20the%20country%20to%20predict%20future%20criminals.%20and%20it%E2%80%99s%20biased%20against%20blacks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angwin%2C%20Julia%20Larson%2C%20Jeff%20Mattu%2C%20Surya%20Kirchner%2C%20Lauren%20Machine%20bias%3A%20There%E2%80%99s%20software%20used%20across%20the%20country%20to%20predict%20future%20criminals.%20and%20it%E2%80%99s%20biased%20against%20blacks%202016"
        },
        {
            "id": "4",
            "entry": "[4] Jeff Asher and Rob Arthur. Inside the algorithm that tries to predict gun violence in Chicago. The New York Times, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asher%2C%20Jeff%20Arthur%2C%20Rob%20Inside%20the%20algorithm%20that%20tries%20to%20predict%20gun%20violence%20in%20Chicago%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asher%2C%20Jeff%20Arthur%2C%20Rob%20Inside%20the%20algorithm%20that%20tries%20to%20predict%20gun%20violence%20in%20Chicago%202017"
        },
        {
            "id": "5",
            "entry": "[5] JC Barnes, Kevin M Beaver, and J Mitchell Miller. Estimating the effect of gang membership on nonviolent and violent delinquency: A counterfactual analysis. Aggressive behavior, 36(6):437\u2013451, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barnes%2C%20J.C.%20Beaver%2C%20Kevin%20M.%20Miller%2C%20J.Mitchell%20Estimating%20the%20effect%20of%20gang%20membership%20on%20nonviolent%20and%20violent%20delinquency%3A%20A%20counterfactual%20analysis%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barnes%2C%20J.C.%20Beaver%2C%20Kevin%20M.%20Miller%2C%20J.Mitchell%20Estimating%20the%20effect%20of%20gang%20membership%20on%20nonviolent%20and%20violent%20delinquency%3A%20A%20counterfactual%20analysis%202010"
        },
        {
            "id": "6",
            "entry": "[6] Solon Barocas and Andrew D Selbst. Big data\u2019s disparate impact. California Law Review, 104:671\u2013732, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barocas%2C%20Solon%20Selbst%2C%20Andrew%20D.%20Big%20data%E2%80%99s%20disparate%20impact%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barocas%2C%20Solon%20Selbst%2C%20Andrew%20D.%20Big%20data%E2%80%99s%20disparate%20impact%202016"
        },
        {
            "id": "7",
            "entry": "[7] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Vandenberghe%2C%20Lieven%20Convex%20Optimization%202004"
        },
        {
            "id": "8",
            "entry": "[8] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5(2):153\u2013163, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017"
        },
        {
            "id": "9",
            "entry": "[9] City of Chicago. Strategic Subject List. https://data.cityofchicago.org/ Public-Safety/Strategic-Subject-List/4aki-r3np, 2017.",
            "url": "https://data.cityofchicago.org/Public-Safety/Strategic-Subject-List/4aki-r3np"
        },
        {
            "id": "10",
            "entry": "[10] Amit Datta, Michael Carl Tschantz, and Anupam Datta. Automated experiments on ad privacy settings. Privacy Enhancing Technologies, 2015(1):92\u2013112, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Datta%2C%20Amit%20Tschantz%2C%20Michael%20Carl%20Datta%2C%20Anupam%20Automated%20experiments%20on%20ad%20privacy%20settings%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Datta%2C%20Amit%20Tschantz%2C%20Michael%20Carl%20Datta%2C%20Anupam%20Automated%20experiments%20on%20ad%20privacy%20settings%202015"
        },
        {
            "id": "11",
            "entry": "[11] Anupam Datta, Matt Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen. Proxy discrimination in data-driven systems. arXiv preprint arXiv:1707.08120, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08120"
        },
        {
            "id": "12",
            "entry": "[12] Anupam Datta, Matt Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen. Use privacy in data-driven systems: Theory and experiments with machine learnt programs. In ACM SIGSAC Conference on Computer and Communications Security, pages 1193\u20131210, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Datta%2C%20Anupam%20Fredrikson%2C%20Matt%20Ko%2C%20Gihyuk%20Mardziel%2C%20Piotr%20Use%20privacy%20in%20data-driven%20systems%3A%20Theory%20and%20experiments%20with%20machine%20learnt%20programs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Datta%2C%20Anupam%20Fredrikson%2C%20Matt%20Ko%2C%20Gihyuk%20Mardziel%2C%20Piotr%20Use%20privacy%20in%20data-driven%20systems%3A%20Theory%20and%20experiments%20with%20machine%20learnt%20programs%202017"
        },
        {
            "id": "13",
            "entry": "[13] Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In IEEE Symposium on Security and Privacy, pages 598\u2013617, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Datta%2C%20Anupam%20Sen%2C%20Shayak%20Zick%2C%20Yair%20Algorithmic%20transparency%20via%20quantitative%20input%20influence%3A%20Theory%20and%20experiments%20with%20learning%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Datta%2C%20Anupam%20Sen%2C%20Shayak%20Zick%2C%20Yair%20Algorithmic%20transparency%20via%20quantitative%20input%20influence%3A%20Theory%20and%20experiments%20with%20learning%20systems%202016"
        },
        {
            "id": "14",
            "entry": "[14] William Dieterich, Christina Mendoza, and Tim Brennan. COMPAS risk scales: Demonstrating accuracy equity and predictive parity. http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf, 2016.",
            "url": "http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf"
        },
        {
            "id": "15",
            "entry": "[15] Dheeru Dua and Efi Karra Taniskidou. UCI machine learning repository. https://archive.ics.uci.edu/ml, 2017.",
            "url": "https://archive.ics.uci.edu/ml"
        },
        {
            "id": "16",
            "entry": "[16] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Innovations in Theoretical Computer Science, pages 214\u2013226, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "17",
            "entry": "[17] Equal Employment Opportunities Commission. Uniform guidelines on employee selection procedures. 29 CFR Part 1607, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Equal%20Employment%20Opportunities%20Commission.%20Uniform%20guidelines%20on%20employee%20selection%20procedures.%2029%20CFR%20Part%201607%201978"
        },
        {
            "id": "18",
            "entry": "[18] Equivant. Practitioner\u2019s guide to COMPAS core. http://www.equivant.com/assets/img/content/Practitioners_Guide_COMPASCore_121917.pdf, 2017.",
            "url": "http://www.equivant.com/assets/img/content/Practitioners_Guide_COMPASCore_121917.pdf"
        },
        {
            "id": "19",
            "entry": "[19] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259\u2013268, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20A.%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20A.%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015"
        },
        {
            "id": "20",
            "entry": "[20] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems, pages 3315\u20133323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016"
        },
        {
            "id": "21",
            "entry": "[21] David Ingold and Spencer Soper. Amazon doesn\u2019t consider the race of its customers. Should it? Bloomberg, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ingold%2C%20David%20Soper%2C%20Spencer%20Amazon%20doesn%E2%80%99t%20consider%20the%20race%20of%20its%20customers.%20Should%20it%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ingold%2C%20David%20Soper%2C%20Spencer%20Amazon%20doesn%E2%80%99t%20consider%20the%20race%20of%20its%20customers.%20Should%20it%3F%202016"
        },
        {
            "id": "22",
            "entry": "[22] Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Scholkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing Systems, pages 656\u2013666, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kilbertus%2C%20Niki%20Carulla%2C%20Mateo%20Rojas%20Parascandolo%2C%20Giambattista%20Hardt%2C%20Moritz%20Avoiding%20discrimination%20through%20causal%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kilbertus%2C%20Niki%20Carulla%2C%20Mateo%20Rojas%20Parascandolo%2C%20Giambattista%20Hardt%2C%20Moritz%20Avoiding%20discrimination%20through%20causal%20reasoning%202017"
        },
        {
            "id": "23",
            "entry": "[23] Bernard Marr. How AI and machine learning are used to transform the insurance industry. Forbes, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marr%2C%20Bernard%20How%20AI%20and%20machine%20learning%20are%20used%20to%20transform%20the%20insurance%20industry%202017"
        },
        {
            "id": "24",
            "entry": "[24] Michael Redmond and Alok Baveja. A data-driven software tool for enabling cooperative information sharing among police departments. European Journal of Operational Research, 141(3):660\u2013678, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Redmond%2C%20Michael%20Baveja%2C%20Alok%20A%20data-driven%20software%20tool%20for%20enabling%20cooperative%20information%20sharing%20among%20police%20departments%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Redmond%2C%20Michael%20Baveja%2C%20Alok%20A%20data-driven%20software%20tool%20for%20enabling%20cooperative%20information%20sharing%20among%20police%20departments%202002"
        },
        {
            "id": "25",
            "entry": "[25] Supreme Court of the United States. Griggs v. Duke Power Co. 401 U.S. 424, 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Supreme%20Court%20of%20the%20United%20States.%20Griggs%20v.%20Duke%20Power%20Co.%20401%20U.%20S%201971"
        },
        {
            "id": "26",
            "entry": "[26] Supreme Court of the United States. Connecticut v. Teal. 457 U.S. 440, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Supreme%20Court%20of%20the%20United%20States.%20Connecticut%20v%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Supreme%20Court%20of%20the%20United%20States.%20Connecticut%20v%201982"
        },
        {
            "id": "27",
            "entry": "[27] Supreme Court of the United States. Ricci v. DeStefano. 557 U.S. 557, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Supreme%20Court%20of%20the%20United%20States.%20Ricci%20v%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Supreme%20Court%20of%20the%20United%20States.%20Ricci%20v%202009"
        },
        {
            "id": "28",
            "entry": "[28] Rhema Vaithianathan, Emily Putnam-Hornstein, Nan Jiang, Parma Nand, and Tim Maloney. Developing predictive models to support child maltreatment hotline screening decisions: Allegheny County methodology and implementation. https://www.alleghenycountyanalytics.us/wp-content/uploads/2018/02/ DevelopingPredictiveRiskModels-package_011618.pdf, 2017.",
            "url": "https://www.alleghenycountyanalytics.us/wp-content/uploads/2018/02/DevelopingPredictiveRiskModels-package_011618.pdf"
        },
        {
            "id": "29",
            "entry": "[29] Samuel Yeom, Anupam Datta, and Matt Fredrikson. Hunting for discriminatory proxies in linear regression models. arXiv preprint arXiv:1810.07155, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.07155"
        },
        {
            "id": "30",
            "entry": "[30] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pages 962\u2013970, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rogriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rogriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017"
        }
    ]
}
