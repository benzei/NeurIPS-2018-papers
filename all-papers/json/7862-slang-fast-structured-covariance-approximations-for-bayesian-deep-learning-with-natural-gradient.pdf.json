{
    "filename": "7862-slang-fast-structured-covariance-approximations-for-bayesian-deep-learning-with-natural-gradient.pdf",
    "metadata": {
        "title": "SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient",
        "author": "Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, Mohammad Emtiyaz Khan",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7862-slang-fast-structured-covariance-approximations-for-bayesian-deep-learning-with-natural-gradient.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Uncertainty estimation in large deep-learning models is a computationally challenging task, where it is difficult to form even a Gaussian approximation to the posterior distribution. In such situations, existing methods usually resort to a diagonal approximation of the covariance matrix despite the fact that these matrices are known to give poor uncertainty estimates. To address this issue, we propose a new stochastic, low-rank, approximate natural-gradient (SLANG) method for variational inference in large deep models. Our method estimates a \u201cdiagonal plus low-rank\u201d structure based solely on back-propagated gradients of the network log-likelihood. This requires strictly less gradient computations than methods that compute the gradient of the whole variational objective. Empirical evaluations on standard benchmarks confirm that SLANG enables faster and more accurate estimation of uncertainty than mean-field methods, and performs comparably to state-of-the-art methods."
    },
    "keywords": [
        {
            "term": "covariance matrix",
            "url": "https://en.wikipedia.org/wiki/covariance_matrix"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "bayesian neural network",
            "url": "https://en.wikipedia.org/wiki/bayesian_neural_network"
        },
        {
            "term": "Variational inference",
            "url": "https://en.wikipedia.org/wiki/Variational_inference"
        }
    ],
    "highlights": [
        "Deep learning has had enormous recent success in fields such as speech recognition and computer vision",
        "Uncertainty estimates are important for physicians who use automated diagnosis systems to choose effective and safe treatment options",
        "It is infeasible to form a Gaussian approximation to the posterior distribution",
        "We address this issue by estimating a Gaussian approximation that uses a covariance with low-rank plus diagonal structure",
        "We proposed an approximate natural-gradient algorithm to estimate the structured covariance matrix",
        "Called SLANG, relies only on the back-propagated gradients to estimate the covariance structure, which is a desirable feature when working with deep models"
    ],
    "key_statements": [
        "Deep learning has had enormous recent success in fields such as speech recognition and computer vision",
        "Uncertainty estimates are important for physicians who use automated diagnosis systems to choose effective and safe treatment options",
        "We propose a new variational inference method to estimate Gaussian approximations with a diagonal plus low-rank covariance structure",
        "We tried an alternative method where Ut+1 is learned using an exponential moving-average of the eigendecompositions of G (\u03b8). This previous iteration of SLANG is discussed in Appendix B, where we show that it gives worse results than the SLANG update",
        "We use L-BFGS [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] to compute the optimal full-Gaussian variational approximation that minimizes the evidence lower bound using the method described in Marlin et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "It is infeasible to form a Gaussian approximation to the posterior distribution",
        "We address this issue by estimating a Gaussian approximation that uses a covariance with low-rank plus diagonal structure",
        "We proposed an approximate natural-gradient algorithm to estimate the structured covariance matrix",
        "Called SLANG, relies only on the back-propagated gradients to estimate the covariance structure, which is a desirable feature when working with deep models",
        "We found that comparing the quality of covariance approximations is a nontrivial task for deep neural networks"
    ],
    "summary": [
        "Deep learning has had enormous recent success in fields such as speech recognition and computer vision.",
        "We propose a new variational inference method to estimate Gaussian approximations with a diagonal plus low-rank covariance structure.",
        "We call our method stochastic low-rank approximate naturalgradient (SLANG).",
        "Our empirical comparisons demonstrate the improvements obtained over mean-field methods and show that SLANG gives comparable results to the state-of-the-art on standard benchmarks.",
        "A variety of methods have been proposed based on mean-field approximations.",
        "Our idea is to estimate a low-rank plus diagonal approximation of \u03a3 that reduces the computational cost while preserving some off-diagonal covariance structure.",
        "3 Stochastic, Low-rank, Approximate Natural-Gradient (SLANG) Method",
        "The above update uses a stochastic, low-rank covariance estimate to approximate natural-gradient updates, which is why we use the name SLANG.",
        "When L = D, Ut+1Ut+1 is full rank and SLANG is identical to the approximate natural-gradient update (2).",
        "Our goal is to show experimental results to support the following claims: (1) SLANG gives reasonable posterior approximations, and (2) SLANG performs well on standard benchmarks for Bayesian neural networks.",
        "We use L-BFGS [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] to compute the optimal full-Gaussian variational approximation that minimizes the ELBO using the method described in Marlin et al [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "We compute the optimal mean-field Gaussian approximation and refer to it as \"MF Exact\".",
        "The figure on the left compares covariance approximations obtained with SLANG to the Full-Gaussian Exact method.",
        "We compare negative ELBO, test log-loss using cross-entropy, and symmetric KL-divergence between the approximations and the Full-Gaussian Exact method.",
        "We find that SLANG with L = 1 nearly always produces better approximations than the mean-field methods.",
        "We compare these three SLANG methods to Mean-Field Hessian and Full-Gaussian Hessian.",
        "An example for Bayesian Neural Networks on a synthetic regression dataset is given in Appendix F.1, where we illustrate the quality of SLANG\u2019s posterior covariance.",
        "Similar to the Bayesian logistic regression experiment, SLANG converges much faster than the mean-field method.",
        "We address this issue by estimating a Gaussian approximation that uses a covariance with low-rank plus diagonal structure.",
        "We proposed an approximate natural-gradient algorithm to estimate the structured covariance matrix.",
        "Called SLANG, relies only on the back-propagated gradients to estimate the covariance structure, which is a desirable feature when working with deep models.",
        "Our method converges faster than the mean-field methods and can sometimes converge as fast as VI methods that use a full-Gaussian approximation.",
        "SLANG is based on a natural-gradient method that employs the empirical Fisher approximation [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]."
    ],
    "headline": "We propose a new stochastic, low-rank, approximate natural-gradient  method for variational inference in large deep models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] Sungjin Ahn, Anoop Korattikara Balan, and Max Welling. Bayesian posterior sampling via stochastic gradient fisher scoring. In Proceedings of the 29th International Conference on Machine Learning, ICML, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahn%2C%20Sungjin%20Balan%2C%20Anoop%20Korattikara%20Welling%2C%20Max%20Bayesian%20posterior%20sampling%20via%20stochastic%20gradient%20fisher%20scoring%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahn%2C%20Sungjin%20Balan%2C%20Anoop%20Korattikara%20Welling%2C%20Max%20Bayesian%20posterior%20sampling%20via%20stochastic%20gradient%20fisher%20scoring%202012"
        },
        {
            "id": "3",
            "entry": "[3] Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251\u2013276, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20Shun-ichi%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "4",
            "entry": "[4] Sivaram Ambikasaran and Michael O\u2019Neil. Fast symmetric factorization of hierarchical matrices with applications. CoRR, abs/1405.0223, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1405.0223"
        },
        {
            "id": "5",
            "entry": "[5] Anoop Korattikara Balan, Vivek Rathod, Kevin P. Murphy, and Max Welling. Bayesian dark knowledge. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, pages 3438\u20133446, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balan%2C%20Anoop%20Korattikara%20Rathod%2C%20Vivek%20Murphy%2C%20Kevin%20P.%20Welling%2C%20Max%20Bayesian%20dark%20knowledge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balan%2C%20Anoop%20Korattikara%20Rathod%2C%20Vivek%20Murphy%2C%20Kevin%20P.%20Welling%2C%20Max%20Bayesian%20dark%20knowledge%202015"
        },
        {
            "id": "6",
            "entry": "[6] David Barber and Christopher M. Bishop. Ensemble learning for multi-layer networks. In Advances in Neural Information Processing Systems 10, pages 395\u2013401, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barber%2C%20David%20Bishop%2C%20Christopher%20M.%20Ensemble%20learning%20for%20multi-layer%20networks%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barber%2C%20David%20Bishop%2C%20Christopher%20M.%20Ensemble%20learning%20for%20multi-layer%20networks%201997"
        },
        {
            "id": "7",
            "entry": "[7] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. CoRR, abs/1505.05424, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05424"
        },
        {
            "id": "8",
            "entry": "[8] Edward Challis and David Barber. Concave gaussian variational approximations for inference in large-scale bayesian linear models. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS, pages 199\u2013207, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Challis%2C%20Edward%20Barber%2C%20David%20Concave%20gaussian%20variational%20approximations%20for%20inference%20in%20large-scale%20bayesian%20linear%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Challis%2C%20Edward%20Barber%2C%20David%20Concave%20gaussian%20variational%20approximations%20for%20inference%20in%20large-scale%20bayesian%20linear%20models%202011"
        },
        {
            "id": "9",
            "entry": "[9] Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In Proceedings of the 31th International Conference on Machine Learning, ICML, pages 1683\u20131691, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Fox%2C%20Emily%20B.%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Tianqi%20Fox%2C%20Emily%20B.%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014"
        },
        {
            "id": "10",
            "entry": "[10] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 1050\u20131059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "11",
            "entry": "[11] Ian J. Goodfellow. Efficient per-example gradient computations. CoRR, abs/1510.01799, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.01799"
        },
        {
            "id": "12",
            "entry": "[12] Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems., pages 2348\u20132356, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Practical%20variational%20inference%20for%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Practical%20variational%20inference%20for%20neural%20networks%202011"
        },
        {
            "id": "13",
            "entry": "[13] Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217\u2013288, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Halko%2C%20Nathan%20Martinsson%2C%20Per-Gunnar%20Tropp%2C%20Joel%20A.%20Finding%20structure%20with%20randomness%3A%20Probabilistic%20algorithms%20for%20constructing%20approximate%20matrix%20decompositions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Halko%2C%20Nathan%20Martinsson%2C%20Per-Gunnar%20Tropp%2C%20Joel%20A.%20Finding%20structure%20with%20randomness%3A%20Probabilistic%20algorithms%20for%20constructing%20approximate%20matrix%20decompositions%202011"
        },
        {
            "id": "14",
            "entry": "[14] Jos\u00e9 Miguel Hern\u00e1ndez-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In Proceedings of the 32nd International Conference on Machine Learning, ICML, pages 1861\u20131869, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Adams%2C%20Ryan%20P.%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20bayesian%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Adams%2C%20Ryan%20P.%20Probabilistic%20backpropagation%20for%20scalable%20learning%20of%20bayesian%20neural%20networks%202015"
        },
        {
            "id": "15",
            "entry": "[15] Matthew D. Hoffman, David M. Blei, Chong Wang, and John William Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20William%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20William%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "16",
            "entry": "[16] Tommi Jaakkola and Michael Jordan. A variational approach to Bayesian logistic regression problems and their extensions. In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, AISTATS, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaakkola%2C%20Tommi%20Jordan%2C%20Michael%20A%20variational%20approach%20to%20Bayesian%20logistic%20regression%20problems%20and%20their%20extensions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaakkola%2C%20Tommi%20Jordan%2C%20Michael%20A%20variational%20approach%20to%20Bayesian%20logistic%20regression%20problems%20and%20their%20extensions%201997"
        },
        {
            "id": "17",
            "entry": "[17] Mohammad Emtiyaz Khan and Wu Lin. Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS, pages 878\u2013887, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20Mohammad%20Emtiyaz%20Lin%2C%20Wu%20Conjugate-computation%20variational%20inference%3A%20Converting%20variational%20inference%20in%20non-conjugate%20models%20to%20inferences%20in%20conjugate%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20Mohammad%20Emtiyaz%20Lin%2C%20Wu%20Conjugate-computation%20variational%20inference%3A%20Converting%20variational%20inference%20in%20non-conjugate%20models%20to%20inferences%20in%20conjugate%20models%202017"
        },
        {
            "id": "18",
            "entry": "[18] Mohammad Emtiyaz Khan and Didrik Nielsen. Fast yet simple natural-gradient descent for variational inference in complex models. CoRR, abs/1807.04489, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04489"
        },
        {
            "id": "19",
            "entry": "[19] Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable Bayesian deep learning by weight-perturbation in Adam. In Proceedings of the 35th International Conference on Machine Learning, pages 2611\u20132620, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20Mohammad%20Emtiyaz%20Nielsen%2C%20Didrik%20Tangkaratt%2C%20Voot%20Lin%2C%20Wu%20Fast%20and%20scalable%20Bayesian%20deep%20learning%20by%20weight-perturbation%20in%20Adam%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20Mohammad%20Emtiyaz%20Nielsen%2C%20Didrik%20Tangkaratt%2C%20Voot%20Lin%2C%20Wu%20Fast%20and%20scalable%20Bayesian%20deep%20learning%20by%20weight-perturbation%20in%20Adam%202018"
        },
        {
            "id": "20",
            "entry": "[20] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "21",
            "entry": "[21] Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix gaussian posteriors. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 1708\u20131716, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20gaussian%20posteriors%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louizos%2C%20Christos%20Welling%2C%20Max%20Structured%20and%20efficient%20variational%20deep%20learning%20with%20matrix%20gaussian%20posteriors%202016"
        },
        {
            "id": "22",
            "entry": "[22] Benjamin M. Marlin, Mohammad Emtiyaz Khan, and Kevin P. Murphy. Piecewise bounds for estimating bernoulli-logistic latent gaussian models. In Proceedings of the 28th International Conference on Machine Learning, ICML, pages 633\u2013640, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marlin%2C%20Benjamin%20M.%20Khan%2C%20Mohammad%20Emtiyaz%20Murphy%2C%20Kevin%20P.%20Piecewise%20bounds%20for%20estimating%20bernoulli-logistic%20latent%20gaussian%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marlin%2C%20Benjamin%20M.%20Khan%2C%20Mohammad%20Emtiyaz%20Murphy%2C%20Kevin%20P.%20Piecewise%20bounds%20for%20estimating%20bernoulli-logistic%20latent%20gaussian%20models%202011"
        },
        {
            "id": "23",
            "entry": "[23] James Martens. New perspectives on the natural gradient method. CoRR, abs/1412.1193, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.1193"
        },
        {
            "id": "24",
            "entry": "[24] Andrew C. Miller, Nicholas J. Foti, and Ryan P. Adams. Variational boosting: Iteratively refining posterior approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML, pages 2420\u20132429, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miller%2C%20Andrew%20C.%20Foti%2C%20Nicholas%20J.%20Adams%2C%20Ryan%20P.%20Variational%20boosting%3A%20Iteratively%20refining%20posterior%20approximations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miller%2C%20Andrew%20C.%20Foti%2C%20Nicholas%20J.%20Adams%2C%20Ryan%20P.%20Variational%20boosting%3A%20Iteratively%20refining%20posterior%20approximations%202017"
        },
        {
            "id": "25",
            "entry": "[25] Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary gaussian process classification. Journal of Machine Learning Research, 9(Oct):2035\u20132078, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickisch%2C%20Hannes%20Rasmussen%2C%20Carl%20Edward%20Approximations%20for%20binary%20gaussian%20process%20classification%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickisch%2C%20Hannes%20Rasmussen%2C%20Carl%20Edward%20Approximations%20for%20binary%20gaussian%20process%20classification%202008"
        },
        {
            "id": "26",
            "entry": "[26] Victor M.-H. Ong, David J. Nott, and Michael S. Smith. Gaussian variational approximation with a factor covariance structure. Journal of Computational and Graphical Statistics, 27(3):465\u2013478, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ong%2C%20Victor%20M.-H.%20Nott%2C%20David%20J.%20Smith%2C%20Michael%20S.%20Gaussian%20variational%20approximation%20with%20a%20factor%20covariance%20structure%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ong%2C%20Victor%20M.-H.%20Nott%2C%20David%20J.%20Smith%2C%20Michael%20S.%20Gaussian%20variational%20approximation%20with%20a%20factor%20covariance%20structure%202018"
        },
        {
            "id": "27",
            "entry": "[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In Autodiff Workshop, during the Annual Conference on Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20Autodiff%20Workshop%20during%20the%20Annual%20Conference%20on%20Neural%20Information%20Processing%20Systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20Autodiff%20Workshop%20during%20the%20Annual%20Conference%20on%20Neural%20Information%20Processing%20Systems%202017"
        },
        {
            "id": "28",
            "entry": "[28] Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, AISTATS, pages 814\u2013822, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20M.%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20M.%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "29",
            "entry": "[29] Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ritter%2C%20Hippolyt%20Botev%2C%20Aleksandar%20Barber%2C%20David%20A%20scalable%20laplace%20approximation%20for%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ritter%2C%20Hippolyt%20Botev%2C%20Aleksandar%20Barber%2C%20David%20A%20scalable%20laplace%20approximation%20for%20neural%20networks%202018"
        },
        {
            "id": "30",
            "entry": "[30] Matthias W. Seeger. Bayesian model selection for support vector machines, gaussian processes and other kernel classifiers. In Advances in Neural Information Processing Systems 12, pages 603\u2013609, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20Matthias%20W.%20Bayesian%20model%20selection%20for%20support%20vector%20machines%2C%20gaussian%20processes%20and%20other%20kernel%20classifiers%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20Matthias%20W.%20Bayesian%20model%20selection%20for%20support%20vector%20machines%2C%20gaussian%20processes%20and%20other%20kernel%20classifiers%201999"
        },
        {
            "id": "31",
            "entry": "[31] Matthias W. Seeger. Gaussian covariance and scalable variational inference. In Proceedings of the 27th International Conference on Machine Learning, ICML, pages 967\u2013974, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20Matthias%20W.%20Gaussian%20covariance%20and%20scalable%20variational%20inference%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20Matthias%20W.%20Gaussian%20covariance%20and%20scalable%20variational%20inference%202010"
        },
        {
            "id": "32",
            "entry": "[32] Umut Simsekli, Roland Badeau, A. Taylan Cemgil, and Ga\u00ebl Richard. Stochastic quasi-newton langevin monte carlo. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 642\u2013651, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simsekli%2C%20Umut%20Roland%20Badeau%2C%20A.Taylan%20Cemgil%20Richard%2C%20Ga%C3%ABl%20Stochastic%20quasi-newton%20langevin%20monte%20carlo%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simsekli%2C%20Umut%20Roland%20Badeau%2C%20A.Taylan%20Cemgil%20Richard%2C%20Ga%C3%ABl%20Stochastic%20quasi-newton%20langevin%20monte%20carlo%202016"
        },
        {
            "id": "33",
            "entry": "[33] Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertainty in bayesian neural networks. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS, pages 1283\u20131292, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Shengyang%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structured%20weight%20uncertainty%20in%20bayesian%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Shengyang%20Chen%2C%20Changyou%20Carin%2C%20Lawrence%20Learning%20structured%20weight%20uncertainty%20in%20bayesian%20neural%20networks%202017"
        },
        {
            "id": "34",
            "entry": "[34] Linda S. L. Tan and David J. Nott. Gaussian variational approximation with sparse precision matrices. Statistics and Computing, 28(2):259\u2013275, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tan%2C%20Linda%20S.L.%20Nott%2C%20David%20J.%20Gaussian%20variational%20approximation%20with%20sparse%20precision%20matrices%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tan%2C%20Linda%20S.L.%20Nott%2C%20David%20J.%20Gaussian%20variational%20approximation%20with%20sparse%20precision%20matrices%202018"
        },
        {
            "id": "35",
            "entry": "[35] Michalis K. Titsias and Miguel L\u00e1zaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In Proceedings of the 31th International Conference on Machine Learning, ICML, pages 1971\u20131979, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20Michalis%20K.%20L%C3%A1zaro-Gredilla%2C%20Miguel%20Doubly%20stochastic%20variational%20bayes%20for%20non-conjugate%20inference%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20Michalis%20K.%20L%C3%A1zaro-Gredilla%2C%20Miguel%20Doubly%20stochastic%20variational%20bayes%20for%20non-conjugate%20inference%201971"
        },
        {
            "id": "36",
            "entry": "[36] Richard E. Turner and Maneesh Sahani. Two problems with variational expectation maximisation for time-series models. In D. Barber, T. Cemgil, and S. Chiappa, editors, Bayesian Time series models, chapter 5, pages 109\u2013130. Cambridge University Press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richard%2C%20E.%20Turner%20and%20Maneesh%20Sahani.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richard%2C%20E.%20Turner%20and%20Maneesh%20Sahani.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011"
        },
        {
            "id": "37",
            "entry": "[37] Stephen J. Wright and J Nocedal. Numerical optimization. Springer New York, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wright%2C%20Stephen%20J.%20Nocedal%2C%20J.%20Numerical%20optimization%201999"
        },
        {
            "id": "38",
            "entry": "[38] Guodong Zhang, Shengyang Sun, David K. Duvenaud, and Roger B. Grosse. Noisy natural gradient as variational inference. In Proceedings of the 35th International Conference on Machine Learning, ICML, pages 5847\u20135856, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Guodong%20Sun%2C%20Shengyang%20Duvenaud%2C%20David%20K.%20Grosse%2C%20Roger%20B.%20Noisy%20natural%20gradient%20as%20variational%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Guodong%20Sun%2C%20Shengyang%20Duvenaud%2C%20David%20K.%20Grosse%2C%20Roger%20B.%20Noisy%20natural%20gradient%20as%20variational%20inference%202018"
        }
    ]
}
