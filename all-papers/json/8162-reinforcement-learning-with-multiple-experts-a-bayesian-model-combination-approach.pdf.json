{
    "filename": "8162-reinforcement-learning-with-multiple-experts-a-bayesian-model-combination-approach.pdf",
    "metadata": {
        "title": "Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach",
        "author": "Michael Gimelfarb, Scott Sanner, Chi-Guhn Lee",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8162-reinforcement-learning-with-multiple-experts-a-bayesian-model-combination-approach.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Potential based reward shaping is a powerful technique for accelerating convergence of reinforcement learning algorithms. Typically, such information includes an estimate of the optimal value function and is often provided by a human expert or other sources of domain knowledge. However, this information is often biased or inaccurate and can mislead many reinforcement learning algorithms. In this paper, we apply Bayesian Model Combination with multiple experts in a way that learns to trust a good combination of experts as training progresses. This approach is both computationally efficient and general, and is shown numerically to improve convergence across discrete and continuous domains and different reinforcement learning algorithms."
    },
    "keywords": [
        {
            "term": "decision maker",
            "url": "https://en.wikipedia.org/wiki/decision_maker"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "temporal difference learning",
            "url": "https://en.wikipedia.org/wiki/temporal_difference_learning"
        },
        {
            "term": "domain knowledge",
            "url": "https://en.wikipedia.org/wiki/domain_knowledge"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Bayesian model averaging",
            "url": "https://en.wikipedia.org/wiki/Bayesian_model_averaging"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "Potential-based reward shaping incorporates prior domain knowledge in the form of additional rewards provided during training to speed up convergence of reinforcement learning algorithms, without changing the optimal policies (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al [1999</a></a></a></a>])",
        "Potential-based reward shaping incorporates prior domain knowledge in the form of additional rewards provided during training to speed up convergence of reinforcement learning algorithms, without changing the optimal policies (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al [1999</a>])",
        "The decision maker is presented with multiple sources of expert advice in the form of potential-based reward functions, some of which can be misleading and should not be trusted",
        "In Section 3.3 we show how our approach can be incorporated into any reinforcement learning algorithm, preserving the asymptotically optimal policy without incurring additional runtime complexity",
        "The decision-making framework used throughout this paper focuses on the Markov decision process (MDP) (<a class=\"ref-link\" id=\"cBertsekas_2004_a\" href=\"#rBertsekas_2004_a\">Bertsekas and Shreve [2004</a>])",
        "In order to validate the effectiveness of our proposed algorithm, we apply it to a Gridworld problem with subgoals and the classical CartPole problem"
    ],
    "key_statements": [
        "Potential-based reward shaping incorporates prior domain knowledge in the form of additional rewards provided during training to speed up convergence of reinforcement learning algorithms, without changing the optimal policies (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al [1999</a></a></a></a>])",
        "Potential-based reward shaping incorporates prior domain knowledge in the form of additional rewards provided during training to speed up convergence of reinforcement learning algorithms, without changing the optimal policies (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al [1999</a>])",
        "The decision maker is presented with multiple sources of expert advice in the form of potential-based reward functions, some of which can be misleading and should not be trusted",
        "In Section 3.3 we show how our approach can be incorporated into any reinforcement learning algorithm, preserving the asymptotically optimal policy without incurring additional runtime complexity",
        "The decision-making framework used throughout this paper focuses on the Markov decision process (MDP) (<a class=\"ref-link\" id=\"cBertsekas_2004_a\" href=\"#rBertsekas_2004_a\">Bertsekas and Shreve [2004</a>])",
        "In order to validate the effectiveness of our proposed algorithm, we apply it to a Gridworld problem with subgoals and the classical CartPole problem",
        "The performance obtained from each expert and the model combination approach are illustrated in Figure 1 for Gridworld and 2 for CartPole, and the learned expert weights are illustrated in Figure 3"
    ],
    "summary": [
        "Potential-based reward shaping incorporates prior domain knowledge in the form of additional rewards provided during training to speed up convergence of reinforcement learning algorithms, without changing the optimal policies (<a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\"><a class=\"ref-link\" id=\"cNg_et+al_1999_a\" href=\"#rNg_et+al_1999_a\">Ng et al [1999</a></a></a></a>]).",
        "In Section 3.3 we show how our approach can be incorporated into any reinforcement learning algorithm, preserving the asymptotically optimal policy without incurring additional runtime complexity.",
        "Our approach can work in on-line settings, in general reinforcement learning algorithms with minimal assumptions, and with value function approximation.",
        "Given a new estimate of the expected future returns Rt at time t after taking action at in state st according to some policy \u03bc, Q is updated as follows",
        "In the general setting of Bayesian model combination, we interpret Q-values for each state-action pair qs,a as random variables, and maintain a set of past return observations D and a multivariate posterior probability distribution P (q|D) over Q-values.",
        "Combining (17) and (18) with the general solution to the moment matching problem (16) yields the new projected posterior Dir. This leads to a very efficient O(N ) algorithm for posterior updates given in Algorithm 1.",
        "Following the Bayesian Q-learning framework (<a class=\"ref-link\" id=\"cDearden_et+al_1998_a\" href=\"#rDearden_et+al_1998_a\">Dearden et al [1998</a>]), we model Q-values for each state-action pair as independent Gaussian distributed random variables.",
        "Since \u03c1t in (20) is a potential-based reward shaping function, it would not change the asymptotically optimal policy.",
        "The performance obtained from each expert and the model combination approach are illustrated in Figure 1 for Gridworld and 2 for CartPole, and the learned expert weights are illustrated in Figure 3.",
        "We followed the Bayesian model combination approach and assigned posterior probabilities to distributions over experts.",
        "We showed that the total expected return is a linear combination of individual expert predictions, weighted by the posterior beliefs assigned to them.",
        "We solved the issue of tractability by projecting the true posterior distribution onto the Dirichlet family using moment matching, and specialized our analysis to Bayesian Q-learning.",
        "Our approach followed the potential-based reward shaping framework and does not change the optimal policies.",
        "Further extensions and generalizations of this work could include rigorous theoretical analysis of posterior convergence under certain conditions on the reward shaping functions.",
        "It is possible to extend our analysis to state/action-dependent weightings of experts, at the cost of higher space complexity; this could be useful in situations where the most suitable potential function changes in different regions of the state space.",
        "We showed that our proposed method accelerated the learning phase when solving discrete and continuous domains using different learning algorithms"
    ],
    "headline": "In Section 3.1, we show that the total return can be written as a linear combination of individual return 32nd Conference on Neural Information Processing Systems , Montr\u00e9al, Canada",
    "reference_links": [
        {
            "id": "Asmuth_et+al_2009_a",
            "entry": "J. Asmuth, L. Li, M. L. Littman, A. Nouri, and D. Wingate. A bayesian sampling approach to exploration in reinforcement learning. In UAI, pages 19\u201326. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asmuth%2C%20J.%20Li%2C%20L.%20Littman%2C%20M.L.%20Nouri%2C%20A.%20A%20bayesian%20sampling%20approach%20to%20exploration%20in%20reinforcement%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asmuth%2C%20J.%20Li%2C%20L.%20Littman%2C%20M.L.%20Nouri%2C%20A.%20A%20bayesian%20sampling%20approach%20to%20exploration%20in%20reinforcement%20learning%202009"
        },
        {
            "id": "Bertsekas_2004_a",
            "entry": "D. P. Bertsekas and S. Shreve. Stochastic optimal control: the discrete-time case. Athena Scientific, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Shreve%2C%20S.%20Stochastic%20optimal%20control%3A%20the%20discrete-time%20case%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20D.P.%20Shreve%2C%20S.%20Stochastic%20optimal%20control%3A%20the%20discrete-time%20case%202004"
        },
        {
            "id": "Brockman_et+al_2016_a",
            "entry": "G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "Brys_et+al_2015_a",
            "entry": "T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and A. Now\u00e9. Reinforcement learning from demonstration through shaping. In IJCAI, pages 3352\u20133358, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brys%2C%20T.%20Harutyunyan%2C%20A.%20Suay%2C%20H.B.%20Chernova%2C%20S.%20Reinforcement%20learning%20from%20demonstration%20through%20shaping%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brys%2C%20T.%20Harutyunyan%2C%20A.%20Suay%2C%20H.B.%20Chernova%2C%20S.%20Reinforcement%20learning%20from%20demonstration%20through%20shaping%202015"
        },
        {
            "id": "Christiano_et+al_2017_a",
            "entry": "P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In NIPS, pages 4302\u20134310, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christiano%2C%20P.F.%20Leike%2C%20J.%20Brown%2C%20T.%20Martic%2C%20M.%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christiano%2C%20P.F.%20Leike%2C%20J.%20Brown%2C%20T.%20Martic%2C%20M.%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017"
        },
        {
            "id": "Dearden_et+al_1998_a",
            "entry": "R. Dearden, N. Friedman, and S. Russell. Bayesian q-learning. In AAAI/IAAI, pages 761\u2013768, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dearden%2C%20R.%20Friedman%2C%20N.%20Russell%2C%20S.%20Bayesian%20q-learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dearden%2C%20R.%20Friedman%2C%20N.%20Russell%2C%20S.%20Bayesian%20q-learning%201998"
        },
        {
            "id": "Devlin_2011_a",
            "entry": "S. Devlin and D. Kudenko. Theoretical considerations of potential-based reward shaping for multi-agent systems. In AAMAS, pages 225\u2013232. International Foundation for Autonomous Agents and Multiagent Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devlin%2C%20S.%20Kudenko%2C%20D.%20Theoretical%20considerations%20of%20potential-based%20reward%20shaping%20for%20multi-agent%20systems%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devlin%2C%20S.%20Kudenko%2C%20D.%20Theoretical%20considerations%20of%20potential-based%20reward%20shaping%20for%20multi-agent%20systems%202011"
        },
        {
            "id": "Devlin_2012_a",
            "entry": "S. Devlin and D. Kudenko. Dynamic potential-based reward shaping. In AAMAS, pages 433\u2013440. International Foundation for Autonomous Agents and Multiagent Systems, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devlin%2C%20S.%20Kudenko%2C%20D.%20Dynamic%20potential-based%20reward%20shaping%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devlin%2C%20S.%20Kudenko%2C%20D.%20Dynamic%20potential-based%20reward%20shaping%202012"
        },
        {
            "id": "Downey_2010_a",
            "entry": "C. Downey and S. Sanner. Temporal difference bayesian model averaging: A bayesian perspective on adapting lambda. In ICML, pages 311\u2013318, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Downey%2C%20C.%20Sanner%2C%20S.%20Temporal%20difference%20bayesian%20model%20averaging%3A%20A%20bayesian%20perspective%20on%20adapting%20lambda%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Downey%2C%20C.%20Sanner%2C%20S.%20Temporal%20difference%20bayesian%20model%20averaging%3A%20A%20bayesian%20perspective%20on%20adapting%20lambda%202010"
        },
        {
            "id": "Eck_et+al_2013_a",
            "entry": "A. Eck, L.-K. Soh, S. Devlin, and D. Kudenko. Potential-based reward shaping for pomdps. In AAMAS, pages 1123\u20131124. International Foundation for Autonomous Agents and Multiagent Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eck%2C%20A.%20Soh%2C%20L.-K.%20Devlin%2C%20S.%20Kudenko%2C%20D.%20Potential-based%20reward%20shaping%20for%20pomdps%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eck%2C%20A.%20Soh%2C%20L.-K.%20Devlin%2C%20S.%20Kudenko%2C%20D.%20Potential-based%20reward%20shaping%20for%20pomdps%202013"
        },
        {
            "id": "Geva_1993_a",
            "entry": "S. Geva and J. Sitte. A cartpole experiment benchmark for trainable controllers. IEEE Control Systems, 13(5): 40\u201351, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geva%2C%20S.%20Sitte%2C%20J.%20A%20cartpole%20experiment%20benchmark%20for%20trainable%20controllers%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geva%2C%20S.%20Sitte%2C%20J.%20A%20cartpole%20experiment%20benchmark%20for%20trainable%20controllers%201993"
        },
        {
            "id": "Griffith_et+al_2013_a",
            "entry": "S. Griffith, K. Subramanian, J. Scholz, C. L. Isbell, and A. L. Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. In NIPS, pages 2625\u20132633, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffith%2C%20S.%20Subramanian%2C%20K.%20Scholz%2C%20J.%20Isbell%2C%20C.L.%20Thomaz.%20Policy%20shaping%3A%20Integrating%20human%20feedback%20with%20reinforcement%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffith%2C%20S.%20Subramanian%2C%20K.%20Scholz%2C%20J.%20Isbell%2C%20C.L.%20Thomaz.%20Policy%20shaping%3A%20Integrating%20human%20feedback%20with%20reinforcement%20learning%202013"
        },
        {
            "id": "Grzes_2017_a",
            "entry": "M. Grzes. Reward shaping in episodic reinforcement learning. In AAMAS, pages 565\u2013573. International Foundation for Autonomous Agents and Multiagent Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grzes%2C%20M.%20Reward%20shaping%20in%20episodic%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grzes%2C%20M.%20Reward%20shaping%20in%20episodic%20reinforcement%20learning%202017"
        },
        {
            "id": "Grzes_2009_a",
            "entry": "M. Grzes and D. Kudenko. Learning shaping rewards in model-based reinforcement learning. In Proc. AAMAS 2009 Workshop on Adaptive Learning Agents, volume 115, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grzes%2C%20M.%20Kudenko%2C%20D.%20Learning%20shaping%20rewards%20in%20model-based%20reinforcement%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grzes%2C%20M.%20Kudenko%2C%20D.%20Learning%20shaping%20rewards%20in%20model-based%20reinforcement%20learning%202009"
        },
        {
            "id": "Kudenko_2010_a",
            "entry": "M. Grzesand D. Kudenko. Online learning of shaping rewards in reinforcement learning. Neural Networks, 23(4):541 \u2013 550, 2010. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2010.01.001. The 18th International Conference on Artificial Neural Networks, ICANN 2008.",
            "crossref": "https://dx.doi.org/10.1016/j.neunet.2010.01.001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.neunet.2010.01.001"
        },
        {
            "id": "Harutyunyan_et+al_2015_a",
            "entry": "A. Harutyunyan, T. Brys, P. Vrancx, and A. Now\u00e9. Shaping mario with human advice. In AAMAS, pages 1913\u20131914. International Foundation for Autonomous Agents and Multiagent Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harutyunyan%2C%20A.%20Brys%2C%20T.%20Vrancx%2C%20P.%20Now%C3%A9%2C%20A.%20Shaping%20mario%20with%20human%20advice%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harutyunyan%2C%20A.%20Brys%2C%20T.%20Vrancx%2C%20P.%20Now%C3%A9%2C%20A.%20Shaping%20mario%20with%20human%20advice%202015"
        },
        {
            "id": "Hsu_2016_a",
            "entry": "W.-S. Hsu and P. Poupart. Online bayesian moment matching for topic modeling with unknown number of topics. In NIPS, pages 4536\u20134544, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20W.-S.%20P.%20Poupart.%20Online%20bayesian%20moment%20matching%20for%20topic%20modeling%20with%20unknown%20number%20of%20topics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20W.-S.%20P.%20Poupart.%20Online%20bayesian%20moment%20matching%20for%20topic%20modeling%20with%20unknown%20number%20of%20topics%202016"
        },
        {
            "id": "Konidaris_2006_a",
            "entry": "G. Konidaris and A. Barto. Autonomous shaping: Knowledge transfer in reinforcement learning. In ICML, pages 489\u2013496. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20G.%20Barto%2C%20A.%20Autonomous%20shaping%3A%20Knowledge%20transfer%20in%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20G.%20Barto%2C%20A.%20Autonomous%20shaping%3A%20Knowledge%20transfer%20in%20reinforcement%20learning%202006"
        },
        {
            "id": "Lantz_2013_a",
            "entry": "B. Lantz. Machine learning with R. Packt Publishing Ltd, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lantz%2C%20B.%20Machine%20learning%20with%20R%202013"
        },
        {
            "id": "Li_2017_a",
            "entry": "Y. Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07274"
        },
        {
            "id": "Maclin_et+al_2005_a",
            "entry": "R. Maclin, J. Shavlik, L. Torrey, T. Walker, and E. Wild. Giving advice about preferred actions to reinforcement learners via knowledge-based kernel regression. In AAAI, pages 819\u2013824, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclin%2C%20R.%20Shavlik%2C%20J.%20Torrey%2C%20L.%20Walker%2C%20T.%20Giving%20advice%20about%20preferred%20actions%20to%20reinforcement%20learners%20via%20knowledge-based%20kernel%20regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maclin%2C%20R.%20Shavlik%2C%20J.%20Torrey%2C%20L.%20Walker%2C%20T.%20Giving%20advice%20about%20preferred%20actions%20to%20reinforcement%20learners%20via%20knowledge-based%20kernel%20regression%202005"
        },
        {
            "id": "Marom_2018_a",
            "entry": "O. Marom and B. S. Rosman. Belief reward shaping in reinforcement learning. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marom%2C%20O.%20Rosman%2C%20B.S.%20Belief%20reward%20shaping%20in%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marom%2C%20O.%20Rosman%2C%20B.S.%20Belief%20reward%20shaping%20in%20reinforcement%20learning%202018"
        },
        {
            "id": "Minka_2000_a",
            "entry": "T. P. Minka. Bayesian model averaging is not model combination. Available electronically at http://www.stat.cmu.edu/minka/papers/bma.html, pages 1\u20132, 2000.",
            "url": "http://www.stat.cmu.edu/minka/papers/bma.html"
        },
        {
            "id": "Mnih_et+al_2013_a",
            "entry": "V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pages 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20A.Y.%20Harada%2C%20D.%20Russell%2C%20S.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20A.Y.%20Harada%2C%20D.%20Russell%2C%20S.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Omar_2016_a",
            "entry": "F. Omar. Online bayesian learning in probabilistic graphical models using moment matching with applications. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omar%2C%20F.%20Online%20bayesian%20learning%20in%20probabilistic%20graphical%20models%20using%20moment%20matching%20with%20applications%202016"
        },
        {
            "id": "O_2004_a",
            "entry": "A. O\u2019Hagan. Bayesian statistics: principles and benefits. Frontis, pages 31\u201345, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Hagan%2C%20A.%20Bayesian%20statistics%3A%20principles%20and%20benefits%202004"
        },
        {
            "id": "Philipp_2017_a",
            "entry": "P. Philipp and A. Rettinger. Reinforcement learning for multi-step expert advice. In AAMAS, pages 962\u2013971. International Foundation for Autonomous Agents and Multiagent Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Philipp%2C%20P.%20Rettinger%2C%20A.%20Reinforcement%20learning%20for%20multi-step%20expert%20advice%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Philipp%2C%20P.%20Rettinger%2C%20A.%20Reinforcement%20learning%20for%20multi-step%20expert%20advice%202017"
        },
        {
            "id": "Randl_1998_a",
            "entry": "J. Randl\u00f8v and P. Alstr\u00f8m. Learning to drive a bicycle using reinforcement learning and shaping. In ICML, volume 98, pages 463\u2013471, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Randl%C3%B8v%2C%20J.%20Alstr%C3%B8m%2C%20P.%20Learning%20to%20drive%20a%20bicycle%20using%20reinforcement%20learning%20and%20shaping%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Randl%C3%B8v%2C%20J.%20Alstr%C3%B8m%2C%20P.%20Learning%20to%20drive%20a%20bicycle%20using%20reinforcement%20learning%20and%20shaping%201998"
        },
        {
            "id": "Suay_et+al_2016_a",
            "entry": "H. B. Suay, T. Brys, M. E. Taylor, and S. Chernova. Learning from demonstration for shaping through inverse reinforcement learning. In AAMAS, pages 429\u2013437. International Foundation for Autonomous Agents and Multiagent Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suay%2C%20H.B.%20Brys%2C%20T.%20Taylor%2C%20M.E.%20Chernova%2C%20S.%20Learning%20from%20demonstration%20for%20shaping%20through%20inverse%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suay%2C%20H.B.%20Brys%2C%20T.%20Taylor%2C%20M.E.%20Chernova%2C%20S.%20Learning%20from%20demonstration%20for%20shaping%20through%20inverse%20reinforcement%20learning%202016"
        },
        {
            "id": "Sutton_2018_a",
            "entry": "R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 2018. M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. JMLR, 10(Jul): 1633\u20131685, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%202018"
        },
        {
            "id": "J_2007_a",
            "entry": "JMLR, 8(Sep):2125\u20132167, 2007. A. C. Tenorio-Gonzalez, E. F. Morales, and L. Villase\u00f1or-Pineda. Dynamic reward shaping: training a robot by voice. In Ibero-American Conference on Artificial Intelligence, pages 483\u2013492.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J.M.L.R.%20Dynamic%20reward%20shaping%3A%20training%20a%20robot%20by%20voice%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J.M.L.R.%20Dynamic%20reward%20shaping%3A%20training%20a%20robot%20by%20voice%202007"
        },
        {
            "id": "Springer_2003_a",
            "entry": "Springer, 2010. W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933. E. Wiewiora. Potential-based shaping and q-value initialization are equivalent. JAIR, 19:205\u2013208, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%2C%202010.%20W.%20R.%20Thompson%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springer%2C%202010.%20W.%20R.%20Thompson%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%202003"
        },
        {
            "id": "ICML,_2003_a",
            "entry": "ICML, pages 792\u2013799, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ICML%20pages%20792799%202003"
        }
    ]
}
