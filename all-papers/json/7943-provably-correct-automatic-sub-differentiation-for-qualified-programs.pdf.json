{
    "filename": "7943-provably-correct-automatic-sub-differentiation-for-qualified-programs.pdf",
    "metadata": {
        "title": "Provably Correct Automatic Sub-Differentiation for Qualified Programs",
        "author": "Sham M. Kakade, Jason D. Lee",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7943-provably-correct-automatic-sub-differentiation-for-qualified-programs.pdf"
        },
        "journal": "Strassen",
        "abstract": "The Cheap Gradient Principle [<a class=\"ref-link\" id=\"cGriewank_2008_a\" href=\"#rGriewank_2008_a\">Griewank and Walther, 2008</a>] \u2014 the computational cost of computing the gradient of a scalar-valued function is nearly the same (often within a factor of 5) as that of simply computing the function itself \u2014 is of central importance in optimization; it allows us to quickly obtain (high dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing subderivatives: widely used ML libraries, including TensorFlow and PyTorch, do not correctly compute (generalized) subderivatives even on simple examples. This work considers the question: is there a Cheap Subgradient Principle? Our main result shows that, under certain restrictions on our library of nonsmooth functions (standard in nonlinear programming), provably correct generalized subderivatives can be computed at a computational cost that is within a (dimension-free) factor of 6 of the cost of computing the scalar function itself."
    },
    "keywords": [
        {
            "term": "piecewise polynomial",
            "url": "https://en.wikipedia.org/wiki/piecewise_polynomial"
        },
        {
            "term": "computational cost",
            "url": "https://en.wikipedia.org/wiki/computational_cost"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "partial derivative",
            "url": "https://en.wikipedia.org/wiki/partial_derivative"
        }
    ],
    "highlights": [
        "The widespread implementation of Automatic Differentiation (AD) methods [<a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\"><a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\">Baydin et al, 2015</a></a>] has had a transformative effect on applied machine learning; these methods have eased the difficulty for practitioners, across a range of disciplines, to learn sophisticated machine learning models",
        "The widespread implementation of Automatic Differentiation (AD) methods [<a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\">Baydin et al, 2015</a>] has had a transformative effect on applied machine learning; these methods have eased the difficulty for practitioners, across a range of disciplines, to learn sophisticated machine learning models",
        "The paradigm is: one p q \u00d1 writes a program to compute the function of interest, say a scalar function f x : Rd R, p q and a correctly implemented Automatic Differentiation method will return both f x and all d of its partial derivatives when provided with x as an input",
        "Notable differences in this work is that our assumptions make strong connections to nonlinear programming [<a class=\"ref-link\" id=\"cAbadie_1967_a\" href=\"#rAbadie_1967_a\">Abadie, 1967</a>, <a class=\"ref-link\" id=\"cPeterson_1973_a\" href=\"#rPeterson_1973_a\">Peterson, 1973</a>, <a class=\"ref-link\" id=\"cGould_1971_a\" href=\"#rGould_1971_a\">Gould and Tolle, 1971</a>], which help in characterizing when the linearization approach is informative, and we provide a key technical result showing a certain chain rule holds for randomized algorithms",
        "Overloading the Library Functions: It is not difficult to see that piecewise univariate functions can be implemented in our library",
        "Let \u03c3 : R R be a unip\u00a1Vqpqp Vq variate piecewise polynomial, meaning that the domain R is partitioned into a set of k intervals"
    ],
    "key_statements": [
        "The widespread implementation of Automatic Differentiation (AD) methods [<a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\"><a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\">Baydin et al, 2015</a></a>] has had a transformative effect on applied machine learning; these methods have eased the difficulty for practitioners, across a range of disciplines, to learn sophisticated machine learning models",
        "The widespread implementation of Automatic Differentiation (AD) methods [<a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\">Baydin et al, 2015</a>] has had a transformative effect on applied machine learning; these methods have eased the difficulty for practitioners, across a range of disciplines, to learn sophisticated machine learning models",
        "The paradigm is: one p q \u00d1 writes a program to compute the function of interest, say a scalar function f x : Rd R, p q and a correctly implemented Automatic Differentiation method will return both f x and all d of its partial derivatives when provided with x as an input",
        "Notable differences in this work is that our assumptions make strong connections to nonlinear programming [<a class=\"ref-link\" id=\"cAbadie_1967_a\" href=\"#rAbadie_1967_a\">Abadie, 1967</a>, <a class=\"ref-link\" id=\"cPeterson_1973_a\" href=\"#rPeterson_1973_a\">Peterson, 1973</a>, <a class=\"ref-link\" id=\"cGould_1971_a\" href=\"#rGould_1971_a\">Gould and Tolle, 1971</a>], which help in characterizing when the linearization approach is informative, and we provide a key technical result showing a certain chain rule holds for randomized algorithms",
        "Our contributions: Our main result provides \u2014 under a natural set of assumptions widely used in nonlinear programming \u2014 a provably correct Automatic Subdifferentiation procedure, p q which given some x, computes both the functional value f x and a d dimensional subdifferential p q \u0080 f p q u1, . . . ud f x , with a computational cost that is a factor of at most 6 times that of computing p q the scalar function f x itself",
        "To specify how our nonsmooth functions are implemented, we extend the computational model to allow for branching, using (a restricted version3 of) the Blum-Shub-Smale model of computation [<a class=\"ref-link\" id=\"cBlum_et+al_1988_a\" href=\"#rBlum_et+al_1988_a\">Blum et al, 1988</a>]",
        "In Example 3.4, we show that our algorithm does correctly compute the gradient on this function",
        "We provide a provably correct construction of this overloaded library",
        "We examine how h is overloaded based on the implementation in Algorithm 7",
        "Overloading the Library Functions: It is not difficult to see that piecewise univariate functions can be implemented in our library",
        "Let \u03c3 : R R be a unip\u00a1Vqpqp Vq variate piecewise polynomial, meaning that the domain R is partitioned into a set of k intervals",
        "For p\u00a4q \u00a1 the abs function, it should be acceptable for an Automatic Differentiation method to provide a subgradient near to 1 for \u00a1 a small input 0 due to roundoff error; it would undesirable for numerical error to lead vastly different gradients than those that arise from any nearby problem"
    ],
    "summary": [
        "The widespread implementation of Automatic Differentiation (AD) methods [<a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\"><a class=\"ref-link\" id=\"cBaydin_et+al_2015_a\" href=\"#rBaydin_et+al_2015_a\">Baydin et al, 2015</a></a>] has had a transformative effect on applied machine learning; these methods have eased the difficulty for practitioners, across a range of disciplines, to learn sophisticated machine learning models.",
        "The paradigm is: one p q \u00d1 writes a program to compute the function of interest, say a scalar function f x : Rd R, p q and a correctly implemented AD method will return both f x and all d of its partial derivatives when provided with x as an input.",
        "One of the first ideas along this line of thought is due to [A.Griewank, 1995], which shows how to compute directional derivatives of nonsmooth functions through following a \u201cbranch\u201d the program would take on an input.",
        "Our contributions: Our main result provides \u2014 under a natural set of assumptions widely used in nonlinear programming \u2014 a provably correct Automatic Subdifferentiation procedure, p q which given some x, computes both the functional value f x and a d dimensional subdifferential p q \u0080 f p q u1, .",
        "This theorem is often more general: the reverse mode correctly returns the derivatives even with a richer family of smooth functions in our library L, often with a constant factor cost increase as well [<a class=\"ref-link\" id=\"cGriewank_1989_a\" href=\"#rGriewank_1989_a\">Griewank, 1989</a>].",
        "(A Cheap Subgradient Principle) Assume that our program for f x , in Algorithm 1, is allowed to use nonsmooth functions from our library L.",
        "R sp q By Assumption 3.2 and Theorem 3.2, ASD gk xparentspkq, p q x9 parents k returns u fr s fr sp q fr sp q gk; p q x9 parents k and this limiting total derivate satisfies the chain rule xk; v x fxk fxparentspkq t g; v x u.",
        "The following lemma shows that we can provide a method to correctly overload the library, which we use in Algorithm 6.",
        "(Correct Library Overloading) Assume g satisfies the constraint qualificap q \u00b0tion conditions in Assumption 3.1, Suppose the corresponding representation is gxpqpqpqr sp q z\u0080t\u00a11,1uT ISz x pz x , On an arbitrary input x and v, Algorithm 7 returns g x , D g; v x ,",
        "An important step would be in extending our computational model to allow the incorporation of provably correct automatic subdifferentiation libraries for linear algebra libraries.",
        "For p\u00a4q \u00a1 the abs function, it should be acceptable for an AD method to provide a subgradient near to 1 for \u00a1 a small input 0 due to roundoff error; it would undesirable for numerical error to lead vastly different gradients than those that arise from any nearby problem."
    ],
    "headline": "Notable differences in this work is that our assumptions make strong connections to nonlinear programming , which help in characterizing when the linearization approach is informative, and we provide a key technical result showing a certain chain rule holds for randomized algorithms",
    "reference_links": [
        {
            "id": "Abadi_et+al_2015_a",
            "entry": "M. Abadi, A. Agarwal, P. Barham, E. Brevdo, et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "Abadie_1967_a",
            "entry": "J. Abadie. On the kuhn tucker theorem. Nonlinear Programming, pages 19\u201336, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadie%2C%20J.%20On%20the%20kuhn%20tucker%20theorem%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadie%2C%20J.%20On%20the%20kuhn%20tucker%20theorem%201967"
        },
        {
            "id": "Griewank_1994_a",
            "entry": "A.Griewank. Automatic directional differentiation of nonsmooth composite functions. In R.Durier, editor, Recent developments in Optimization / Seventh French-German Conference on Optimization, Dijon 1994, pages 155\u2013169. Springer Verlag, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20A.%20Automatic%20directional%20differentiation%20of%20nonsmooth%20composite%20functions%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griewank%2C%20A.%20Automatic%20directional%20differentiation%20of%20nonsmooth%20composite%20functions%201994"
        },
        {
            "id": "Baur_1983_a",
            "entry": "Walter Baur and Volker Strassen. The complexity of partial derivatives. Theoretical Computer Science, 22:317\u2013330, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baur%2C%20Walter%20Strassen%2C%20Volker%20The%20complexity%20of%20partial%20derivatives%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baur%2C%20Walter%20Strassen%2C%20Volker%20The%20complexity%20of%20partial%20derivatives%201983"
        },
        {
            "id": "Baydin_et+al_2015_a",
            "entry": "Atilim Gunes Baydin, Barak A. Pearlmutter, and Alexey Radul. Automatic differentiation in machine learning: a survey. CoRR, abs/1502.05767, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05767"
        },
        {
            "id": "Blum_et+al_1988_a",
            "entry": "Lenore Blum, Mike Shub, and Steve Smale. On a theory of computation over the real numbers; NP completeness, recursive functions and universal machines (extended abstract). In FOCS, pages 387\u2013397. IEEE Computer Society, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Lenore%20Shub%2C%20Mike%20Smale%2C%20Steve%20On%20a%20theory%20of%20computation%20over%20the%20real%20numbers%3B%20NP%20completeness%2C%20recursive%20functions%20and%20universal%20machines%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Lenore%20Shub%2C%20Mike%20Smale%2C%20Steve%20On%20a%20theory%20of%20computation%20over%20the%20real%20numbers%3B%20NP%20completeness%2C%20recursive%20functions%20and%20universal%20machines%201988"
        },
        {
            "id": "Clarke_1975_a",
            "entry": "F.H. Clarke. Generalized gradients and applications. Trans. Amer. Math. Soc., 205:247\u2013262, Apr. 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarke%2C%20F.H.%20Generalized%20gradients%20and%20applications%201975-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarke%2C%20F.H.%20Generalized%20gradients%20and%20applications%201975-04"
        },
        {
            "id": "Clarke_et+al_2008_a",
            "entry": "F.H. Clarke, Y.S. Ledyaev, R.J. Stern, and P.R. Wolenski. Nonsmooth analysis and control theory, volume 178. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarke%2C%20F.H.%20Ledyaev%2C%20Y.S.%20Stern%2C%20R.J.%20Wolenski%2C%20P.R.%20Nonsmooth%20analysis%20and%20control%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarke%2C%20F.H.%20Ledyaev%2C%20Y.S.%20Stern%2C%20R.J.%20Wolenski%2C%20P.R.%20Nonsmooth%20analysis%20and%20control%202008"
        },
        {
            "id": "Demmel_1997_a",
            "entry": "J. W. Demmel. Applied numerical linear algebra. Society for Industrial Mathematics, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Demmel%2C%20J.W.%20Applied%20numerical%20linear%20algebra%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Demmel%2C%20J.W.%20Applied%20numerical%20linear%20algebra%201997"
        },
        {
            "id": "Fiege_et+al_2017_a",
            "entry": "Sabrina Fiege, Andrea Walther, Kshitij Kulshreshtha, and Andreas Griewank. Algorithmic differentiation for piecewise smooth functions: a case study for robust optimization. pages 1\u201316, 06 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fiege%2C%20Sabrina%20Walther%2C%20Andrea%20Kulshreshtha%2C%20Kshitij%20Griewank%2C%20Andreas%20Algorithmic%20differentiation%20for%20piecewise%20smooth%20functions%3A%20a%20case%20study%20for%20robust%20optimization%202017"
        },
        {
            "id": "Gould_1971_a",
            "entry": "F.J. Gould and J.W. Tolle. A necessary and sufficient qualification for constrained optimization. 20, 03 1971.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gould%2C%20F.J.%20Tolle%2C%20J.W.%20A%20necessary%20and%20sufficient%20qualification%20for%20constrained%20optimization%201971"
        },
        {
            "id": "Griewank_1989_a",
            "entry": "Andreas Griewank. On automatic differentiation. In Mathematical Programming: Recent Developments and Applications, pages 83\u2013108. Kluwer Academic Publishers, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20On%20automatic%20differentiation%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griewank%2C%20Andreas%20On%20automatic%20differentiation%201989"
        },
        {
            "id": "Griewank_2012_a",
            "entry": "Andreas Griewank. Who invented the reverse mode of differentiation? Optimization Stories, Documenta Matematica, Extra Volume ISMP (2012):389\u2013400, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20Who%20invented%20the%20reverse%20mode%20of%20differentiation%3F%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griewank%2C%20Andreas%20Who%20invented%20the%20reverse%20mode%20of%20differentiation%3F%202012"
        },
        {
            "id": "Griewank_2013_a",
            "entry": "Andreas Griewank. On stable picewise linearization and generalized differentiation. Optimization Methods and Software, 28(6):1139\u20131178, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20On%20stable%20picewise%20linearization%20and%20generalized%20differentiation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griewank%2C%20Andreas%20On%20stable%20picewise%20linearization%20and%20generalized%20differentiation%202013"
        },
        {
            "id": "Griewank_2014_a",
            "entry": "Andreas Griewank. On Automatic Differentiation and Algorithmic Linearization. Pesquisa Operacional, 34:621 \u2013 645, 12 2014. ISSN 0101-7438.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20On%20Automatic%20Differentiation%20and%20Algorithmic%20Linearization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griewank%2C%20Andreas%20On%20Automatic%20Differentiation%20and%20Algorithmic%20Linearization%202014"
        },
        {
            "id": "Griewank_2008_a",
            "entry": "Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, second edition, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20Walther%2C%20Andrea%20Evaluating%20Derivatives%3A%20Principles%20and%20Techniques%20of%20Algorithmic%20Differentiation.%20Society%20for%20Industrial%20and%20Applied%20Mathematics%202008"
        },
        {
            "id": "Khan_2013_a",
            "entry": "K. A. Khan and P.I. Barton. Evaluating an element of the clarke generalized jacobian of a composite piecewise differentiable function. ACM Trans. Math. Softw., 39:23:1, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20K.A.%20Barton%2C%20P.I.%20Evaluating%20an%20element%20of%20the%20clarke%20generalized%20jacobian%20of%20a%20composite%20piecewise%20differentiable%20function%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20K.A.%20Barton%2C%20P.I.%20Evaluating%20an%20element%20of%20the%20clarke%20generalized%20jacobian%20of%20a%20composite%20piecewise%20differentiable%20function%202013"
        },
        {
            "id": "Khan_2015_a",
            "entry": "K. A. Khan and P.I. Barton. A vector forward mode of automatic differentiation for generalized derivative evaluation. Optim. Method Softw., 30:1185, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20K.A.%20Barton%2C%20P.I.%20A%20vector%20forward%20mode%20of%20automatic%20differentiation%20for%20generalized%20derivative%20evaluation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20K.A.%20Barton%2C%20P.I.%20A%20vector%20forward%20mode%20of%20automatic%20differentiation%20for%20generalized%20derivative%20evaluation%202015"
        },
        {
            "id": "Khan_2017_a",
            "entry": "Kamil A. Khan. Branch-locking ad techniques for nonsmooth composite functions and nonsmooth implicit functions. Optimization Methods and Software, 0(0):1\u201329, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20Kamil%20A.%20Branch-locking%20ad%20techniques%20for%20nonsmooth%20composite%20functions%20and%20nonsmooth%20implicit%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20Kamil%20A.%20Branch-locking%20ad%20techniques%20for%20nonsmooth%20composite%20functions%20and%20nonsmooth%20implicit%20functions%202017"
        },
        {
            "id": "Klatte_2002_a",
            "entry": "D. Klatte and B. Kummer. Nonsmooth equations in optimization, volume 60 of Nonconvex Optimization and its Applications. Kluwer Academic Publishers, Dordrecht, 2002. ISBN 1-4020-0550-4.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klatte%2C%20D.%20Kummer%2C%20B.%20Nonsmooth%20equations%20in%20optimization%2C%20volume%2060%20of%20Nonconvex%20Optimization%20and%20its%20Applications%202002"
        },
        {
            "id": "Maclaurin_et+al_2015_a",
            "entry": "Dougal Maclaurin, David Duvenaud, Matthew Johnson, and Ryan P. Adams. Autograd: Reversemode differentiation of native Python, 2015. URL http://github.com/HIPS/autograd.",
            "url": "http://github.com/HIPS/autograd",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Johnson%2C%20Matthew%20Adams%2C%20Ryan%20P.%20Autograd%3A%20Reversemode%20differentiation%20of%20native%202015"
        },
        {
            "id": "Mordukhovich_2006_a",
            "entry": "B. S. Mordukhovich. Variational Analysis and Generalized Differentiation II: Applications. Springer Berlin Heidelberg, 2006. ISBN 9783540312468. URL https://books.google.com/books?id=lmdmY75lrokC.",
            "url": "https://books.google.com/books?id=lmdmY75lrokC"
        },
        {
            "id": "Morgenstern_1985_a",
            "entry": "Jacques Morgenstern. How to compute fast a function and all its derivatives: a variation on the theorem of baur-strassen. In SIGA, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morgenstern%2C%20Jacques%20How%20to%20compute%20fast%20a%20function%20and%20all%20its%20derivatives%3A%20a%20variation%20on%20the%20theorem%20of%20baur-strassen%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morgenstern%2C%20Jacques%20How%20to%20compute%20fast%20a%20function%20and%20all%20its%20derivatives%3A%20a%20variation%20on%20the%20theorem%20of%20baur-strassen%201985"
        },
        {
            "id": "Nesterov_2005_a",
            "entry": "Yurii Nesterov. Lexicographic differentiation of nonsmooth functions. Math. Program., 104(2-3): 669\u2013700, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Lexicographic%20differentiation%20of%20nonsmooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Lexicographic%20differentiation%20of%20nonsmooth%20functions%202005"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Paszke%20S%20Gross%20S%20Chintala%20G%20Chanan%20E%20Yang%20Z%20DeVito%20Z%20Lin%20A%20Desmaison%20L%20Antiga%20and%20A%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Paszke%20S%20Gross%20S%20Chintala%20G%20Chanan%20E%20Yang%20Z%20DeVito%20Z%20Lin%20A%20Desmaison%20L%20Antiga%20and%20A%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "Peterson_1973_a",
            "entry": "David W. Peterson. A review of constraint qualifications in finite-dimensional spaces. SIAM Review, 15(3):639\u2013654, 1973.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peterson%2C%20David%20W.%20A%20review%20of%20constraint%20qualifications%20in%20finite-dimensional%20spaces%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peterson%2C%20David%20W.%20A%20review%20of%20constraint%20qualifications%20in%20finite-dimensional%20spaces%201973"
        },
        {
            "id": "Rumelhart_et+al_1986_a",
            "entry": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Learning Internal Representations by Error Propagation, pages 318\u2013362. MIT Press, Cambridge, MA, USA, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Parallel%20distributed%20processing%3A%20Explorations%20in%20the%20microstructure%20of%20cognition%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Parallel%20distributed%20processing%3A%20Explorations%20in%20the%20microstructure%20of%20cognition%201986"
        },
        {
            "id": "Seeger_et+al_2017_a",
            "entry": "Matthias W. Seeger, Asmus Hetzel, Zhenwen Dai, and Neil D. Lawrence. Auto-differentiating linear algebra. CoRR, abs/1710.08717, 2017. URL http://arxiv.org/abs/1710.08717.",
            "url": "http://arxiv.org/abs/1710.08717",
            "arxiv_url": "https://arxiv.org/pdf/1710.08717"
        },
        {
            "id": "Shapiro_1990_a",
            "entry": "A. Shapiro. On concepts of directional differentiability. Journal of Optimization Theory and Applications, 66(3):477\u2013487, Sep 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapiro%2C%20A.%20On%20concepts%20of%20directional%20differentiability%201990-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shapiro%2C%20A.%20On%20concepts%20of%20directional%20differentiability%201990-09"
        },
        {
            "id": "Trefethen_1997_a",
            "entry": "Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. SIAM, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lloyd%20N%20Trefethen%20and%20David%20Bau%20III%20Numerical%20linear%20algebra%20volume%2050%20SIAM%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lloyd%20N%20Trefethen%20and%20David%20Bau%20III%20Numerical%20linear%20algebra%20volume%2050%20SIAM%201997"
        }
    ]
}
