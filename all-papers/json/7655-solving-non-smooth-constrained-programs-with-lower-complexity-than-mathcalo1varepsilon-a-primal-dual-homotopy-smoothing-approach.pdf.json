{
    "filename": "7655-solving-non-smooth-constrained-programs-with-lower-complexity-than-mathcalo1varepsilon-a-primal-dual-homotopy-smoothing-approach.pdf",
    "metadata": {
        "date": 2018,
        "title": "Solving Non-smooth Constrained Programs with Lower Complexity than O(1/\"): A Primal-Dual",
        "author": "Homotopy Smoothing Approach",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7655-solving-non-smooth-constrained-programs-with-lower-complexity-than-mathcalo1varepsilon-a-primal-dual-homotopy-smoothing-approach.pdf"
        },
        "abstract": "We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex. The best known iteration complexity solving such a non-smooth problem is O(\" 1). In this paper, we show that by leveraging a local error bound condition on the dual function, the proposed algorithm can achieve a better primal convergence time of O \" 2/(2+ ) log2(\" 1) , where 2 (0, 1] is a local error bound parameter. As an example application of the general algorithm, we show that the distributed geometric median problem, which can be formulated as a constrained convex program, has its dual function non-smooth but satisfying the aforementioned local error bound condition with = 1/2, therefore enjoying a convergence time of O \" 4/5 log2(\" 1) . This result improves upon the O(\" 1) convergence time bound achieved by existing distributed optimization algorithms. Simulation experiments also demonstrate the performance of our proposed algorithm."
    },
    "keywords": [
        {
            "term": "convex programming",
            "url": "https://en.wikipedia.org/wiki/convex_programming"
        },
        {
            "term": "proximal gradient",
            "url": "https://en.wikipedia.org/wiki/proximal_gradient"
        },
        {
            "term": "dual function",
            "url": "https://en.wikipedia.org/wiki/dual_function"
        },
        {
            "term": "linear convergence",
            "url": "https://en.wikipedia.org/wiki/linear_convergence"
        },
        {
            "term": "convex program",
            "url": "https://en.wikipedia.org/wiki/convex_program"
        },
        {
            "term": "statistical estimation",
            "url": "https://en.wikipedia.org/wiki/statistical_estimation"
        },
        {
            "term": "ADMM",
            "url": "https://en.wikipedia.org/wiki/ADMM"
        },
        {
            "term": "convex optimization problem",
            "url": "https://en.wikipedia.org/wiki/convex_optimization_problem"
        },
        {
            "term": "frank wolfe",
            "url": "https://en.wikipedia.org/wiki/Frank_Wolfe"
        },
        {
            "term": "geometric median",
            "url": "https://en.wikipedia.org/wiki/geometric_median"
        },
        {
            "term": "convergence time",
            "url": "https://en.wikipedia.org/wiki/convergence_time"
        }
    ],
    "highlights": [
        "Several works seek to achieve a better convergence time than O(1/\") under weaker assumptions than Lipschitz gradient and strong convexity of the dual function",
        "Method solving (1-2), which achieves a convergence time of O(1/\" 1+3\u232b ), where \u232b is the modulus of Holder continuity on the gradient of the dual function of the formulation (1-2).1",
        "The work <a class=\"ref-link\" id=\"cYu_2018_a\" href=\"#rYu_2018_a\">Yu and Neely (2018</a>) shows that when the dual function has Lipschitz continuous gradient and satisfies a locally quadratic property (i.e. a local error bound with = 1/2, see Definition 2.1 for details), which is weaker than strong convexity, one can still obtain a linear convergence with a dual subgradient algorithm",
        "Contributions: In the current work, we show a multi-stage homotopy smoothing method enjoys a primal convergence time O \" 2/(2+ ) log2(\" 1) solving (1-2) when the dual function satisfies a local error bound condition with 2 (0, 1]",
        "We show theoretically that such a problem, when formulated as (1-2), has its dual function non-smooth but locally quadratic.\n3The result in <a class=\"ref-link\" id=\"cXu_et+al_2017_a\" href=\"#rXu_et+al_2017_a\">Xu et al (2017</a>) heavily depends on the assumption that the subgradient of (\u00b7) is defined everywhere over the set \u23261 and uniformly bound by some constant \u21e2, which excludes the choice of indicator functions necessary to deal with constraints in the ADMM framework",
        "The first step is to derive a primal convergence time bound for Algorithm 1, which involves the location information of the initial Lagrange multiplier at the beginning of this stage"
    ],
    "key_statements": [
        "Several works seek to achieve a better convergence time than O(1/\") under weaker assumptions than Lipschitz gradient and strong convexity of the dual function",
        "Method solving (1-2), which achieves a convergence time of O(1/\" 1+3\u232b ), where \u232b is the modulus of Holder continuity on the gradient of the dual function of the formulation (1-2).1",
        "The work <a class=\"ref-link\" id=\"cYu_2018_a\" href=\"#rYu_2018_a\">Yu and Neely (2018</a>) shows that when the dual function has Lipschitz continuous gradient and satisfies a locally quadratic property (i.e. a local error bound with = 1/2, see Definition 2.1 for details), which is weaker than strong convexity, one can still obtain a linear convergence with a dual subgradient algorithm",
        "Contributions: In the current work, we show a multi-stage homotopy smoothing method enjoys a primal convergence time O \" 2/(2+ ) log2(\" 1) solving (1-2) when the dual function satisfies a local error bound condition with 2 (0, 1]",
        "We show theoretically that such a problem, when formulated as (1-2), has its dual function non-smooth but locally quadratic.\n3The result in <a class=\"ref-link\" id=\"cXu_et+al_2017_a\" href=\"#rXu_et+al_2017_a\">Xu et al (2017</a>) heavily depends on the assumption that the subgradient of (\u00b7) is defined everywhere over the set \u23261 and uniformly bound by some constant \u21e2, which excludes the choice of indicator functions necessary to deal with constraints in the ADMM framework",
        "The first step is to derive a primal convergence time bound for Algorithm 1, which involves the location information of the initial Lagrange multiplier at the beginning of this stage",
        "To see how this bound leads to an improved convergence time when running in multiple rounds, suppose the computation from the last round gives a e that is close enough to the optimal set \u21e4\u21e4, ke\u21e4 ek would be small"
    ],
    "summary": [
        "Several works seek to achieve a better convergence time than O(1/\") under weaker assumptions than Lipschitz gradient and strong convexity of the dual function.",
        "Method solving (1-2), which achieves a convergence time of O(1/\" 1+3\u232b ), where \u232b is the modulus of Holder continuity on the gradient of the dual function of the formulation (1-2).1 On the other hand, the work <a class=\"ref-link\" id=\"cYu_2018_a\" href=\"#rYu_2018_a\">Yu and Neely (2018</a>) shows that when the dual function has Lipschitz continuous gradient and satisfies a locally quadratic property (i.e. a local error bound with = 1/2, see Definition 2.1 for details), which is weaker than strong convexity, one can still obtain a linear convergence with a dual subgradient algorithm.",
        "They show that when the function (x) satisfies a local error bound with parameter 2 (0, 1], such a combination gives an improved convergence time of O/\"1 ) minimizing the unconstrained problem (3).",
        "The work <a class=\"ref-link\" id=\"cXu_et+al_2017_a\" href=\"#rXu_et+al_2017_a\">Xu et al (2017</a>) shows that the homotopy method can be combined with ADMM to achieve a faster convergence solving problems of the form min f (x) + (Ax b), x2\u23261 where \u23261 is a closed convex set, f, are both convex functions with f (x) + (Ax b) satisfying the local error bound, and the proximal operator of (\u00b7) can be computed.",
        "Contributions: In the current work, we show a multi-stage homotopy smoothing method enjoys a primal convergence time O \" 2/(2+ ) log2(\" 1) solving (1-2) when the dual function satisfies a local error bound condition with 2 (0, 1].",
        "The first step is to derive a primal convergence time bound for Algorithm 1, which involves the location information of the initial Lagrange multiplier at the beginning of this stage.",
        "To see how this bound leads to an improved convergence time when running in multiple rounds, suppose the computation from the last round gives a e that is close enough to the optimal set \u21e4\u21e4, ke\u21e4 ek would be small.",
        "We will show that it can be written in the form of problem (1-2), has its Lagrange dual function locally quadratic and optimal Lagrange multiplier unique up to the null space of A, thereby satisfying Assumption 2.1.",
        "Note that in the second step, we obtain the primal update x = [x1T , \u00b7 \u00b7 \u00b7 , xnT ] 2 Rnd by solving the following problem: x = argmaxx:kxi bik"
    ],
    "headline": "We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex",
    "reference_links": [
        {
            "id": "Beck_et+al_2014_a",
            "entry": "Beck, A., A. Nedic, A. Ozdaglar, and M. Teboulle (2014). An o(1/k) gradient method for network resource allocation problems. IEEE Transactions on Control of Network Systems 1(1), 64\u201373.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20A.%20Nedic%2C%20A.%20Ozdaglar%2C%20A.%20M.%20An%20o%281/k%29%20gradient%20method%20for%20network%20resource%20allocation%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20A.%20Nedic%2C%20A.%20Ozdaglar%2C%20A.%20M.%20An%20o%281/k%29%20gradient%20method%20for%20network%20resource%20allocation%20problems%202014"
        },
        {
            "id": "Bertsekas_1999_a",
            "entry": "Bertsekas, D. P. (1999). Nonlinear programming. Athena Scientific Belmont.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Nonlinear%20programming.%20Athena%20Scientific%20Belmont%201999"
        },
        {
            "id": "Bertsekas_2009_a",
            "entry": "Bertsekas, D. P. (2009). Convex optimization theory. Athena Scientific Belmont.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Convex%20optimization%20theory.%20Athena%20Scientific%20Belmont%202009"
        },
        {
            "id": "Boyd_et+al_2004_a",
            "entry": "Boyd, S., P. Diaconis, and L. Xiao (2004). Fastest mixing markov chain on a graph. SIAM",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Diaconis%2C%20P.%20L.%20Fastest%20mixing%20markov%20chain%20on%20a%20graph%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20S.%20Diaconis%2C%20P.%20L.%20Fastest%20mixing%20markov%20chain%20on%20a%20graph%202004"
        },
        {
            "id": "Burke_1996_a",
            "entry": "Burke, J. V. and P. Tseng (1996). A unified analysis of Hoffman\u2019s bound via Fenchel duality. SIAM",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burke%2C%20J.V.%20P.%20A%20unified%20analysis%20of%20Hoffman%E2%80%99s%20bound%20via%20Fenchel%20duality%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burke%2C%20J.V.%20P.%20A%20unified%20analysis%20of%20Hoffman%E2%80%99s%20bound%20via%20Fenchel%20duality%201996"
        },
        {
            "id": "Journal_0000_a",
            "entry": "Journal on Optimization 6(2), 265\u2013282.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Journal%20on",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Journal%20on"
        },
        {
            "id": "Cohen_et+al_2016_a",
            "entry": "Cohen, M. B., Y. T. Lee, G. Miller, J. Pachocki, and A. Sidford (2016). Geometric median in nearly linear time. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pp. 9\u201321.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20M.B.%20Lee%2C%20Y.T.%20Miller%2C%20G.%20Pachocki%2C%20J.%20Geometric%20median%20in%20nearly%20linear%20time%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20M.B.%20Lee%2C%20Y.T.%20Miller%2C%20G.%20Pachocki%2C%20J.%20Geometric%20median%20in%20nearly%20linear%20time%202016"
        },
        {
            "id": "Deng_et+al_2017_a",
            "entry": "Deng, W., M.-J. Lai, Z. Peng, and W. Yin (2017). Parallel multi-block ADMM with o(1/k) convergence. Journal of Scientific Computing 71(2), 712\u2013736.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20W.%20Lai%2C%20M.-J.%20Peng%2C%20Z.%20W.%20Parallel%20multi-block%20ADMM%20with%20o%281/k%29%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20W.%20Lai%2C%20M.-J.%20Peng%2C%20Z.%20W.%20Parallel%20multi-block%20ADMM%20with%20o%281/k%29%20convergence%202017"
        },
        {
            "id": "Deng_2016_a",
            "entry": "Deng, W. and W. Yin (2016). On the global and linear convergence of the generalized alternating direction method of multipliers. Journal of Scientific Computing 66(3), 889\u2013916.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20W.%20W.%20On%20the%20global%20and%20linear%20convergence%20of%20the%20generalized%20alternating%20direction%20method%20of%20multipliers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20W.%20W.%20On%20the%20global%20and%20linear%20convergence%20of%20the%20generalized%20alternating%20direction%20method%20of%20multipliers%202016"
        },
        {
            "id": "Duchi_et+al_2014_a",
            "entry": "Duchi, J. C., M. I. Jordan, M. J. Wainwright, and Y. Zhang (2014). Optimality guarantees for distributed statistical estimation. arXiv preprint arXiv:1405.0782.",
            "arxiv_url": "https://arxiv.org/pdf/1405.0782"
        },
        {
            "id": "Gidel_et+al_2018_a",
            "entry": "Gidel, G., F. Pedregosa, and S. Lacoste-Julien (2018). Frank-Wolfe splitting via augmented Lagrangian method. arXiv preprint arXiv:1804.03176.",
            "arxiv_url": "https://arxiv.org/pdf/1804.03176"
        },
        {
            "id": "Grant_et+al_2008_a",
            "entry": "Grant, M., S. Boyd, and Y. Ye (2008). CVX: Matlab software for disciplined convex programming.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grant%2C%20M.%20Boyd%2C%20S.%20Y.%20CVX%3A%20Matlab%20software%20for%20disciplined%20convex%20programming%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grant%2C%20M.%20Boyd%2C%20S.%20Y.%20CVX%3A%20Matlab%20software%20for%20disciplined%20convex%20programming%202008"
        },
        {
            "id": "Han_et+al_2015_a",
            "entry": "Han, D., D. Sun, and L. Zhang (2015). Linear rate convergence of the alternating direction method of multipliers for convex composite quadratic and semi-definite programming. arXiv preprint arXiv:1508.02134.",
            "arxiv_url": "https://arxiv.org/pdf/1508.02134"
        },
        {
            "id": "Lan_2013_a",
            "entry": "Lan, G. and R. D. Monteiro (2013). Iteration-complexity of first-order penalty methods for convex programming. Mathematical Programming 138(1-2), 115\u2013139.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20G.%20D%2C%20R.%20Iteration-complexity%20of%20first-order%20penalty%20methods%20for%20convex%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20G.%20D%2C%20R.%20Iteration-complexity%20of%20first-order%20penalty%20methods%20for%20convex%20programming%202013"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Li, J., G. Chen, Z. Dong, and Z. Wu (2016). A fast dual proximal-gradient method for separable convex optimization with linear coupled constraints. Computational Optimization and Applications 64(3), 671\u2013697.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Chen%2C%20G.%20Dong%2C%20Z.%20Z.%20A%20fast%20dual%20proximal-gradient%20method%20for%20separable%20convex%20optimization%20with%20linear%20coupled%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Chen%2C%20G.%20Dong%2C%20Z.%20Z.%20A%20fast%20dual%20proximal-gradient%20method%20for%20separable%20convex%20optimization%20with%20linear%20coupled%20constraints%202016"
        },
        {
            "id": "Ling_2010_a",
            "entry": "Ling, Q. and Z. Tian (2010). Decentralized sparse signal recovery for compressive sleeping wireless sensor networks. IEEE Transactions on Signal Processing 58(7), 3816\u20133827.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ling%2C%20Q.%20Z.%20Decentralized%20sparse%20signal%20recovery%20for%20compressive%20sleeping%20wireless%20sensor%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ling%2C%20Q.%20Z.%20Decentralized%20sparse%20signal%20recovery%20for%20compressive%20sleeping%20wireless%20sensor%20networks%202010"
        },
        {
            "id": "Luo_1994_a",
            "entry": "Luo, X.-D. and Z.-Q. Luo (1994). Extension of hoffman\u2019s error bound to polynomial systems. SIAM Journal on Optimization 4(2), 383\u2013392.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20X.-D.%20Z.-Q.%20Extension%20of%20hoffman%E2%80%99s%20error%20bound%20to%20polynomial%20systems%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20X.-D.%20Z.-Q.%20Extension%20of%20hoffman%E2%80%99s%20error%20bound%20to%20polynomial%20systems%201994"
        },
        {
            "id": "Minsker_et+al_2014_a",
            "entry": "Minsker, S., S. Srivastava, L. Lin, and D. B. Dunson (2014). Robust and scalable bayes via a median of subset posterior measures. arXiv preprint arXiv:1403.2660.",
            "arxiv_url": "https://arxiv.org/pdf/1403.2660"
        },
        {
            "id": "Minsker_2017_a",
            "entry": "Minsker, S. and N. Strawn (2017). Distributed statistical estimation and rates of convergence in normal approximation. arXiv preprint arXiv:1704.02658.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02658"
        },
        {
            "id": "Motzkin_1952_a",
            "entry": "Motzkin, T. (1952). Contributions to the theory of linear inequalities. D.R. Fulkerson (Transl.) (Santa Monica: RAND Corporation). RAND Corporation Translation 22.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Motzkin%2C%20T.%20Contributions%20to%20the%20theory%20of%20linear%20inequalities%201952"
        },
        {
            "id": "Necoara_2008_a",
            "entry": "Necoara, I. and J. A. Suykens (2008). Application of a smoothing technique to decomposition in convex optimization. IEEE Transactions on Automatic control 53(11), 2674\u20132679.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Necoara%2C%20I.%20A%2C%20J.%20Application%20of%20a%20smoothing%20technique%20to%20decomposition%20in%20convex%20optimization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Necoara%2C%20I.%20A%2C%20J.%20Application%20of%20a%20smoothing%20technique%20to%20decomposition%20in%20convex%20optimization%202008"
        },
        {
            "id": "Nedic_2009_a",
            "entry": "Nedic, A. and A. Ozdaglar (2009). Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control 54(1), 48\u201361.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20A.%20A.%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20A.%20A.%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009"
        },
        {
            "id": "Nesterov_2005_a",
            "entry": "Nesterov, Y. (2005). Smooth minimization of non-smooth functions. Mathematical Programming 103(1), 127\u2013152.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Smooth%20minimization%20of%20non-smooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Smooth%20minimization%20of%20non-smooth%20functions%202005"
        },
        {
            "id": "Nesterov_2015_a",
            "entry": "Nesterov, Y. (2015a). Complexity bounds for primal-dual methods minimizing the model of objective function. Mathematical Programming, 1\u201320.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Complexity%20bounds%20for%20primal-dual%20methods%20minimizing%20the%20model%20of%20objective%20function%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Complexity%20bounds%20for%20primal-dual%20methods%20minimizing%20the%20model%20of%20objective%20function%202015"
        },
        {
            "id": "Nesterov_2015_b",
            "entry": "Nesterov, Y. (2015b). Universal gradient methods for convex optimization problems. Mathematical Programming 152(1-2), 381\u2013404.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Universal%20gradient%20methods%20for%20convex%20optimization%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Universal%20gradient%20methods%20for%20convex%20optimization%20problems%202015"
        },
        {
            "id": "Osborne_et+al_2000_a",
            "entry": "Osborne, M. R., B. Presnell, and B. A. Turlach (2000). A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis 20(3), 389\u2013403.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osborne%2C%20M.R.%20Presnell%2C%20B.%20A%2C%20B.%20A%20new%20approach%20to%20variable%20selection%20in%20least%20squares%20problems%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osborne%2C%20M.R.%20Presnell%2C%20B.%20A%2C%20B.%20A%20new%20approach%20to%20variable%20selection%20in%20least%20squares%20problems%202000"
        },
        {
            "id": "Pang_1997_a",
            "entry": "Pang, J.-S. (1997, Oct). Error bounds in mathematical programming. Mathematical Programming 79(1), 299\u2013332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pang%2C%20J.-S.%20Error%20bounds%20in%20mathematical%20programming%201997-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20J.-S.%20Error%20bounds%20in%20mathematical%20programming%201997-10"
        },
        {
            "id": "Parrilo_2003_a",
            "entry": "Parrilo, P. A. and B. Sturmfels (2003). Minimizing polynomial functions. Algorithmic and quantitative real algebraic geometry, DIMACS Series in Discrete Mathematics and Theoretical Computer Science 60, 83\u201399.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parrilo%2C%20P.A.%20B.%20Minimizing%20polynomial%20functions.%20Algorithmic%20and%20quantitative%20real%20algebraic%20geometry%2C%20DIMACS%20Series%20in%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parrilo%2C%20P.A.%20B.%20Minimizing%20polynomial%20functions.%20Algorithmic%20and%20quantitative%20real%20algebraic%20geometry%2C%20DIMACS%20Series%20in%202003"
        },
        {
            "id": "Shi_et+al_2015_a",
            "entry": "Shi, W., Q. Ling, G. Wu, and W. Yin (2015). A proximal gradient algorithm for decentralized composite optimization. IEEE Transactions on Signal Processing 63(22), 6013\u20136023.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20W.%20Ling%2C%20Q.%20Wu%2C%20G.%20W.%20A%20proximal%20gradient%20algorithm%20for%20decentralized%20composite%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20W.%20Ling%2C%20Q.%20Wu%2C%20G.%20W.%20A%20proximal%20gradient%20algorithm%20for%20decentralized%20composite%20optimization%202015"
        },
        {
            "id": "Shi_et+al_2014_a",
            "entry": "Shi, W., Q. Ling, K. Yuan, G. Wu, and W. Yin (2014). On the linear convergence of the admm in decentralized consensus optimization. IEEE Trans. Signal Processing 62(7), 1750\u20131761.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20W.%20Ling%2C%20Q.%20Yuan%2C%20K.%20Wu%2C%20G.%20On%20the%20linear%20convergence%20of%20the%20admm%20in%20decentralized%20consensus%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20W.%20Ling%2C%20Q.%20Yuan%2C%20K.%20Wu%2C%20G.%20On%20the%20linear%20convergence%20of%20the%20admm%20in%20decentralized%20consensus%20optimization%202014"
        },
        {
            "id": "Tran-Dinh_et+al_2018_a",
            "entry": "Tran-Dinh, Q., O. Fercoq, and V. Cevher (2018). A smooth primal-dual optimization framework for nonsmooth composite convex minimization. SIAM Journal on Optimization 28(1), 96\u2013134.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran-Dinh%2C%20Q.%20Fercoq%2C%20O.%20V.%20A%20smooth%20primal-dual%20optimization%20framework%20for%20nonsmooth%20composite%20convex%20minimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran-Dinh%2C%20Q.%20Fercoq%2C%20O.%20V.%20A%20smooth%20primal-dual%20optimization%20framework%20for%20nonsmooth%20composite%20convex%20minimization%202018"
        },
        {
            "id": "Tseng_2010_a",
            "entry": "Tseng, P. (2010). Approximation accuracy, gradient methods, and error bound for structured convex optimization. Mathematical Programming 125(2), 263\u2013295.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tseng%2C%20P.%20Approximation%20accuracy%2C%20gradient%20methods%2C%20and%20error%20bound%20for%20structured%20convex%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tseng%2C%20P.%20Approximation%20accuracy%2C%20gradient%20methods%2C%20and%20error%20bound%20for%20structured%20convex%20optimization%202010"
        },
        {
            "id": "Wang_1994_a",
            "entry": "Wang, T. and J.-S. Pang (1994). Global error bounds for convex quadratic inequality systems. Optimization 31(1), 1\u201312.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20T.%20J.-S.%20Global%20error%20bounds%20for%20convex%20quadratic%20inequality%20systems%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20T.%20J.-S.%20Global%20error%20bounds%20for%20convex%20quadratic%20inequality%20systems%201994"
        },
        {
            "id": "Wei_2018_a",
            "entry": "Wei, X. and M. J. Neely (2018). Primal-dual Frank-Wolfe for constrained stochastic programs with convex and non-convex objectives. arXiv preprint arXiv:1806.00709.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00709"
        },
        {
            "id": "Wei_et+al_2015_a",
            "entry": "Wei, X., H. Yu, and M. J. Neely (2015). A probabilistic sample path convergence time analysis of drift-plus-penalty algorithm for stochastic optimization. arXiv preprint arXiv:1510.02973.",
            "arxiv_url": "https://arxiv.org/pdf/1510.02973"
        },
        {
            "id": "Weiszfeld_2009_a",
            "entry": "Weiszfeld, E. and F. Plastria (2009). On the point for which the sum of the distances to n given points is minimum. Annals of Operations Research 167(1), 7\u201341.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weiszfeld%2C%20E.%20F.%20On%20the%20point%20for%20which%20the%20sum%20of%20the%20distances%20to%20n%20given%20points%20is%20minimum%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weiszfeld%2C%20E.%20F.%20On%20the%20point%20for%20which%20the%20sum%20of%20the%20distances%20to%20n%20given%20points%20is%20minimum%202009"
        },
        {
            "id": "Xiao_2013_a",
            "entry": "Xiao, L. and T. Zhang (2013). A proximal-gradient homotopy method for the sparse least-squares problem. SIAM Journal on Optimization 23(2), 1062\u20131091.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20L.%20T.%20A%20proximal-gradient%20homotopy%20method%20for%20the%20sparse%20least-squares%20problem%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20L.%20T.%20A%20proximal-gradient%20homotopy%20method%20for%20the%20sparse%20least-squares%20problem%202013"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Xu, Y., M. Liu, Q. Lin, and T. Yang (2017). ADMM without a fixed penalty parameter: Faster convergence with new adaptive penalization. In Advances in Neural Information Processing Systems, pp. 1267\u20131277.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Y.%20Liu%2C%20M.%20Lin%2C%20Q.%20T.%20ADMM%20without%20a%20fixed%20penalty%20parameter%3A%20Faster%20convergence%20with%20new%20adaptive%20penalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Y.%20Liu%2C%20M.%20Lin%2C%20Q.%20T.%20ADMM%20without%20a%20fixed%20penalty%20parameter%3A%20Faster%20convergence%20with%20new%20adaptive%20penalization%202017"
        },
        {
            "id": "Xu_et+al_2016_a",
            "entry": "Xu, Y., Y. Yan, Q. Lin, and T. Yang (2016). Homotopy smoothing for non-smooth problems with lower complexity than o(1/\u270f). In Advances In Neural Information Processing Systems, pp. 1208\u20131216.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Y.%20Yan%2C%20Y.%20Lin%2C%20Q.%20T.%20Homotopy%20smoothing%20for%20non-smooth%20problems%20with%20lower%20complexity%20than%20o%281/%E2%9C%8F%29%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Y.%20Yan%2C%20Y.%20Lin%2C%20Q.%20T.%20Homotopy%20smoothing%20for%20non-smooth%20problems%20with%20lower%20complexity%20than%20o%281/%E2%9C%8F%29%202016"
        },
        {
            "id": "Xue_1997_a",
            "entry": "Xue, G. and Y. Ye (1997). An efficient algorithm for minimizing a sum of euclidean norms with applications. SIAM Journal on Optimization 7(4), 1017\u20131036.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xue%2C%20G.%20Y.%20An%20efficient%20algorithm%20for%20minimizing%20a%20sum%20of%20euclidean%20norms%20with%20applications%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xue%2C%20G.%20Y.%20An%20efficient%20algorithm%20for%20minimizing%20a%20sum%20of%20euclidean%20norms%20with%20applications%201997"
        },
        {
            "id": "Yang_2015_a",
            "entry": "Yang, T. and Q. Lin (2015). Rsg: Beating subgradient method without smoothness and strong convexity. arXiv preprint arXiv:1512.03107.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03107"
        },
        {
            "id": "Yin_et+al_2018_a",
            "entry": "Yin, D., Y. Chen, K. Ramchandran, and P. Bartlett (2018). Byzantine-robust distributed learning: Towards optimal statistical rates. arXiv preprint arXiv:1803.01498.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01498"
        },
        {
            "id": "Yu_2017_a",
            "entry": "Yu, H. and M. J. Neely (2017a). A new backpressure algorithm for joint rate control and routing with vanishing utility optimality gaps and finite queue lengths. In INFOCOM 2017-IEEE Conference on Computer Communications, IEEE, pp. 1\u20139. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20H.%20J%2C%20M.%20A%20new%20backpressure%20algorithm%20for%20joint%20rate%20control%20and%20routing%20with%20vanishing%20utility%20optimality%20gaps%20and%20finite%20queue%20lengths%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20H.%20J%2C%20M.%20A%20new%20backpressure%20algorithm%20for%20joint%20rate%20control%20and%20routing%20with%20vanishing%20utility%20optimality%20gaps%20and%20finite%20queue%20lengths%202017"
        },
        {
            "id": "Yu_2017_b",
            "entry": "Yu, H. and M. J. Neely (2017b). A simple parallel algorithm with an o(1/t) convergence rate for general convex programs. SIAM Journal on Optimization 27(2), 759\u2013783.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20H.%20J%2C%20M.%20A%20simple%20parallel%20algorithm%20with%20an%20o%281/t%29%20convergence%20rate%20for%20general%20convex%20programs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20H.%20J%2C%20M.%20A%20simple%20parallel%20algorithm%20with%20an%20o%281/t%29%20convergence%20rate%20for%20general%20convex%20programs%202017"
        },
        {
            "id": "Yu_2018_a",
            "entry": "Yu, H. and M. J. Neely (2018). On the convergence time of dual subgradient methods for strongly convex programs. IEEE Transactions on Automatic Control.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20H.%20J%2C%20M.%20On%20the%20convergence%20time%20of%20dual%20subgradient%20methods%20for%20strongly%20convex%20programs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20H.%20J%2C%20M.%20On%20the%20convergence%20time%20of%20dual%20subgradient%20methods%20for%20strongly%20convex%20programs%202018"
        },
        {
            "id": "Yuan_et+al_2016_a",
            "entry": "Yuan, K., Q. Ling, and W. Yin (2016). On the convergence of decentralized gradient descent. SIAM Journal on Optimization 26(3), 1835\u20131854.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20K.%20Ling%2C%20Q.%20W.%20On%20the%20convergence%20of%20decentralized%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20K.%20Ling%2C%20Q.%20W.%20On%20the%20convergence%20of%20decentralized%20gradient%20descent%202016"
        },
        {
            "id": "Yurtsever_et+al_2015_a",
            "entry": "Yurtsever, A., Q. T. Dinh, and V. Cevher (2015). A universal primal-dual convex optimization framework. In Advances in Neural Information Processing Systems, pp. 3150\u20133158.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yurtsever%2C%20A.%20Dinh%2C%20Q.T.%20V.%20A%20universal%20primal-dual%20convex%20optimization%20framework%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yurtsever%2C%20A.%20Dinh%2C%20Q.T.%20V.%20A%20universal%20primal-dual%20convex%20optimization%20framework%202015"
        },
        {
            "id": "Yurtsever_et+al_2018_a",
            "entry": "Yurtsever, A., O. Fercoq, F. Locatello, and V. Cevher (2018). A conditional gradient framework for composite convex minimization with applications to semidefinite programming. arXiv preprint arXiv:1804.08544.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08544"
        }
    ]
}
