{
    "filename": "7814-critical-initialisation-for-deep-signal-propagation-in-noisy-rectifier-neural-networks.pdf",
    "metadata": {
        "title": "Critical initialisation for deep signal propagation in noisy rectifier neural networks",
        "author": "Arnu Pretorius, Elan van Biljon, Steve Kroon, Herman Kamper",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7814-critical-initialisation-for-deep-signal-propagation-in-noisy-rectifier-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Stochastic regularisation is an important weapon in the arsenal of a deep learning practitioner. However, despite recent theoretical advances, our understanding of how noise influences signal propagation in deep neural networks remains limited. By extending recent work based on mean field theory, we develop a new framework for signal propagation in stochastic regularised neural networks. Our noisy signal propagation theory can incorporate several common noise distributions, including additive and multiplicative Gaussian noise as well as dropout. We use this framework to investigate initialisation strategies for noisy ReLU networks. We show that no critical initialisation strategy exists using additive noise, with signal propagation exploding regardless of the selected noise distribution. For multiplicative noise (e.g. dropout), we identify alternative critical initialisation strategies that depend on the second moment of the noise distribution. Simulations and experiments on real-world data confirm that our proposed initialisation is able to stably propagate signals in deep networks, while using an initialisation disregarding noise fails to do so. Furthermore, we analyse correlation dynamics between inputs. Stronger noise regularisation is shown to reduce the depth to which discriminatory information about the inputs to a noisy ReLU network is able to propagate, even when initialised at criticality. We support our theoretical predictions for these trainable depths with simulations, as well as with experiments on MNIST and CIFAR-10.\u2021"
    },
    "keywords": [
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "mean field theory",
            "url": "https://en.wikipedia.org/wiki/mean_field_theory"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "additive noise",
            "url": "https://en.wikipedia.org/wiki/additive_noise"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "multiplicative noise",
            "url": "https://en.wikipedia.org/wiki/multiplicative_noise"
        }
    ],
    "highlights": [
        "Over the last few years, advances in network design strategies have made it easier to train large networks and have helped to reduce overfitting",
        "This paper focuses on the effect of noise on signal propagation in deep neural networks",
        "We ask: How much are current weight initialisation strategies affected by noise-induced regularisation in terms of their ability to initialise at a critical point for stable signal propagation? Using our derived theory, we investigate this question specifically for rectified linear unit (ReLU) networks",
        "We investigated depth scales for Rectified Linear Unit networks with dropout initialised at criticality: we trained 100 networks on MNIST and CIFAR-10 for 200 epochs using SGD and a learning rate of 10\u22123 with dropout rates ranging from 0.1 to 1 for varying depths",
        "By developing a general framework to study signal propagation in noisy neural networks, we were able to show how different stochastic regularisation strategies may impact the flow of information in a deep network",
        "Focusing specifically on Rectified Linear Unit networks, we derived novel critical initialisation strategies for multiplicative noise distributions and showed that no such critical initialisations exist for commonly used additive noise distributions"
    ],
    "key_statements": [
        "Over the last few years, advances in network design strategies have made it easier to train large networks and have helped to reduce overfitting",
        "This paper focuses on the effect of noise on signal propagation in deep neural networks",
        "We ask: How much are current weight initialisation strategies affected by noise-induced regularisation in terms of their ability to initialise at a critical point for stable signal propagation? Using our derived theory, we investigate this question specifically for rectified linear unit (ReLU) networks",
        "We investigated depth scales for Rectified Linear Unit networks with dropout initialised at criticality: we trained 100 networks on MNIST and CIFAR-10 for 200 epochs using SGD and a learning rate of 10\u22123 with dropout rates ranging from 0.1 to 1 for varying depths",
        "By developing a general framework to study signal propagation in noisy neural networks, we were able to show how different stochastic regularisation strategies may impact the flow of information in a deep network",
        "Focusing specifically on Rectified Linear Unit networks, we derived novel critical initialisation strategies for multiplicative noise distributions and showed that no such critical initialisations exist for commonly used additive noise distributions"
    ],
    "summary": [
        "Over the last few years, advances in network design strategies have made it easier to train large networks and have helped to reduce overfitting.",
        "The application of noise regularisation directly limits the trainable depth of critically initialised ReLU networks.",
        "Table 1: Critical point initialisation for noisy ReLU networks.",
        "We note that similar results can be derived for other rectifying activation functions; for example, for multiplicative noise the critical initialisation for parametric ReLU (PReLU)",
        "To see the effect of initialising on or off the critical point for ReLU networks, Figure 3 compares the predicted versus simulated variance dynamics for different initialisation schemes.",
        "For schemes not initialising at criticality, the variance map in (a) no longer lies on the identity line and as a result the forward propagating signal in (b) either explodes, or vanishes.",
        "Random deep ReLU networks lose discriminatory information about their inputs as the depth of the network increases, even when initialised at criticality.",
        "The exponential rate assumption underlying the derivation of (8) is supported in Figure 5, where for different noise types and levels, we plot |cl \u2212 c\u2217| as a function of depth on a log-scale, with corresponding linear fits and (c)).",
        "(8) to actual depth scales obtained through simulation and (d)), as a function of noise and observe a good fit for non-zero noise levels.4 We find that noise limits the depth at which critically initialised ReLU networks are expected to perform well through training.",
        "Depth scales fit to the training loss on MNIST for networks initialised at criticality for dropout rates p = 0.1 to p = 1.",
        "As the depth of the network increases, any initialisation strategy that does not factor in the effects of noise, will cause the forward propagating signal to become increasingly unstable.",
        "We sent inputs from MNIST and CIFAR-10 through ReLU networks using dropout at varying depths and for different initialisations of the network.",
        "Due to the loss of correlation information between inputs as a function of noise and network depth, we expect noisy ReLU networks not to be able to perform well beyond certain depths.",
        "Our theory predicts that the statistics of the input should remain within a stable range during the forward pass and enable reliable signal propagation for noise regularised deep ReLU networks.",
        "Our initialisations ensure that individual input statistics are preserved, we further analysed the correlation dynamics between inputs and found the following: at large depths inputs become predictably correlated with each other based on the amount of noise injected into the network."
    ],
    "headline": "By extending recent work based on mean field theory, we develop a new framework for signal propagation in stochastic regularised neural networks",
    "reference_links": [
        {
            "id": "Glorot_2010_a",
            "entry": "X. Glorot and Y. Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in Proceedings of the International Conference on Artificial Intelligence and Statistics, 2010, pp. 249\u2013256.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%2C%202010"
        },
        {
            "id": "Saxe_et+al_2014_a",
            "entry": "A. M. Saxe, J. L. McClelland, and S. Ganguli, \u201cExact solutions to the nonlinear dynamics of learning in deep linear neural networks,\u201d Proceedings of the International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20A.M.%20McClelland%2C%20J.L.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20A.M.%20McClelland%2C%20J.L.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%2C%202014"
        },
        {
            "id": "Sussillo_2014_a",
            "entry": "D. Sussillo and L. Abbott, \u201cRandom walk initialization for training very deep feedforward networks,\u201d arXiv preprint arXiv:1412.6558, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6558"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDelving deep into rectifiers: Surpassing human-level performance on ImageNet classification,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026\u20131034.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20ImageNet%20classification%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20ImageNet%20classification%2C%202015"
        },
        {
            "id": "Mishkin_2016_a",
            "entry": "D. Mishkin and J. Matas, \u201cAll you need is a good init,\u201d Proceedings of International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishkin%2C%20D.%20Matas%2C%20J.%20All%20you%20need%20is%20a%20good%20init%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishkin%2C%20D.%20Matas%2C%20J.%20All%20you%20need%20is%20a%20good%20init%2C%202016"
        },
        {
            "id": "Glorot_et+al_2011_a",
            "entry": "X. Glorot, A. Bordes, and Y. Bengio, \u201cDeep sparse rectifier neural networks,\u201d in Proceedings of the International Conference on Artificial Intelligence and Statistics, 2011, pp. 315\u2013323.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%2C%202011"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting.\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting.%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.E.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting.%202014"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classification with deep convolutional neural networks,\u201d in Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%2C%202012"
        },
        {
            "id": "Dahl_et+al_2013_a",
            "entry": "G. E. Dahl, T. N. Sainath, and G. E. Hinton, \u201cImproving deep neural networks for LVCSR using rectified linear units and dropout,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2013, pp. 8609\u20138613.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dahl%2C%20G.E.%20Sainath%2C%20T.N.%20Hinton%2C%20G.E.%20Improving%20deep%20neural%20networks%20for%20LVCSR%20using%20rectified%20linear%20units%20and%20dropout%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dahl%2C%20G.E.%20Sainath%2C%20T.N.%20Hinton%2C%20G.E.%20Improving%20deep%20neural%20networks%20for%20LVCSR%20using%20rectified%20linear%20units%20and%20dropout%2C%202013"
        },
        {
            "id": "Poole_et+al_2016_a",
            "entry": "B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli, \u201cExponential expressivity in deep neural networks through transient chaos,\u201d in Advances in Neural Information Processing Systems, 2016, pp. 3360\u20133368.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poole%2C%20B.%20Lahiri%2C%20S.%20Raghu%2C%20M.%20Sohl-Dickstein%2C%20J.%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poole%2C%20B.%20Lahiri%2C%20S.%20Raghu%2C%20M.%20Sohl-Dickstein%2C%20J.%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%2C%202016"
        },
        {
            "id": "Schoenholz_et+al_2017_a",
            "entry": "S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein, \u201cDeep information propagation,\u201d Proceedings of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schoenholz%2C%20S.S.%20Gilmer%2C%20J.%20Ganguli%2C%20S.%20Sohl-Dickstein%2C%20J.%20Deep%20information%20propagation%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schoenholz%2C%20S.S.%20Gilmer%2C%20J.%20Ganguli%2C%20S.%20Sohl-Dickstein%2C%20J.%20Deep%20information%20propagation%2C%202017"
        },
        {
            "id": "Yang_2017_a",
            "entry": "G. Yang and S. Schoenholz, \u201cMean field residual networks: On the edge of chaos,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 7103\u20137114.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20G.%20Schoenholz%2C%20S.%20Mean%20field%20residual%20networks%3A%20On%20the%20edge%20of%20chaos%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20G.%20Schoenholz%2C%20S.%20Mean%20field%20residual%20networks%3A%20On%20the%20edge%20of%20chaos%2C%202017"
        },
        {
            "id": "Xiao_et+al_2018_a",
            "entry": "L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington, \u201cDynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks,\u201d Proceedings of the International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20L.%20Bahri%2C%20Y.%20Sohl-Dickstein%2C%20J.%20Schoenholz%2C%20S.S.%20Dynamical%20isometry%20and%20a%20mean%20field%20theory%20of%20CNNs%3A%20How%20to%20train%2010%2C000-layer%20vanilla%20convolutional%20neural%20networks%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20L.%20Bahri%2C%20Y.%20Sohl-Dickstein%2C%20J.%20Schoenholz%2C%20S.S.%20Dynamical%20isometry%20and%20a%20mean%20field%20theory%20of%20CNNs%3A%20How%20to%20train%2010%2C000-layer%20vanilla%20convolutional%20neural%20networks%2C%202018"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "M. Chen, J. Pennington, and S. S. Schoenholz, \u201cDynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks,\u201d Proceedings of the International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20M.%20Pennington%2C%20J.%20Schoenholz%2C%20S.S.%20Dynamical%20isometry%20and%20a%20mean%20field%20theory%20of%20RNNs%3A%20Gating%20enables%20signal%20propagation%20in%20recurrent%20neural%20networks%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20M.%20Pennington%2C%20J.%20Schoenholz%2C%20S.S.%20Dynamical%20isometry%20and%20a%20mean%20field%20theory%20of%20RNNs%3A%20Gating%20enables%20signal%20propagation%20in%20recurrent%20neural%20networks%2C%202018"
        },
        {
            "id": "Hayou_et+al_2018_a",
            "entry": "S. Hayou, A. Doucet, and J. Rousseau, \u201cOn the selection of initialization and activation function for deep neural networks,\u201d arXiv preprint arXiv:1805.08266, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08266"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        }
    ]
}
