{
    "filename": "8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf",
    "metadata": {
        "title": "Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation",
        "author": "Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, Tie-Yan Liu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Neural Machine Translation (NMT) has achieved remarkable progress with the quick evolvement of model structures. In this paper, we propose the concept of layer-wise coordination for NMT, which explicitly coordinates the learning of hidden representations of the encoder and decoder together layer by layer, gradually from low level to high level. Specifically, we design a layer-wise attention and mixed attention mechanism, and further share the parameters of each layer between the encoder and decoder to regularize and coordinate the learning. Experiments show that combined with the state-of-the-art Transformer model, layer-wise coordination achieves improvements on three IWSLT and two WMT translation tasks. More specifically, our method achieves 34.43 and 29.01 BLEU score on WMT16 English-Romanian and WMT14 English-German tasks, outperforming the Transformer baseline."
    },
    "keywords": [
        {
            "term": "statistical machine translation",
            "url": "https://en.wikipedia.org/wiki/statistical_machine_translation"
        },
        {
            "term": "conditional probability",
            "url": "https://en.wikipedia.org/wiki/conditional_probability"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        }
    ],
    "highlights": [
        "The decoder can leverage more fine-grained source information when generating target tokens, instead of only using high-level representations outputted by the encoder in the previous model structure",
        "Given a bilingual sentence pair (x, y), an Neural Machine Translation model learns its parameter \u03b8 by maximizing the log-likelihood P (y|x; \u03b8), which is usually decomposed into the product of the conditional probability of each target word: P (y|x; \u03b8) =",
        "We evaluate our proposed model on the five translation tasks and compare with the previous models that are under the typical encoder-decoder framework",
        "We compare with the Transformer baseline, which is trained with the tensor2tensor codes [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]",
        "We compare with the results of other Recurrent Neural Networks/CNN-based models reported in the previous works"
    ],
    "key_statements": [
        "The decoder can leverage more fine-grained source information when generating target tokens, instead of only using high-level representations outputted by the encoder in the previous model structure",
        "Given a bilingual sentence pair (x, y), an Neural Machine Translation model learns its parameter \u03b8 by maximizing the log-likelihood P (y|x; \u03b8), which is usually decomposed into the product of the conditional probability of each target word: P (y|x; \u03b8) =",
        "We evaluate our proposed model on the five translation tasks and compare with the previous models that are under the typical encoder-decoder framework",
        "We compare with the Transformer baseline, which is trained with the tensor2tensor codes [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]",
        "We compare with the results of other Recurrent Neural Networks/CNN-based models reported in the previous works"
    ],
    "summary": [
        "The decoder can leverage more fine-grained source information when generating target tokens, instead of only using high-level representations outputted by the encoder in the previous model structure.",
        "Through layer-wise coordination and parameter sharing, we ensure that the hidden representations in two corresponding layers of the encoder and decoder are in the same semantic level.",
        "Layer-wise coordination can be applied to any encoder-decoder based models, including RNN, CNN and Transformer.",
        "While the decoder of Transformer uses a separate encoder-decoder attention module to extract information from the source sentence and a self-attention module to extract information from previous target tokens, we merge the two attentions into one, which is called as mixed attention, to coordinate the learning between source and target.",
        "We share the parameters of attention and feed-forward layer between the encoder and decoder, in order to ensure the outputs of the corresponding layers of the encoder and decoder are in the same semantic level, and enhance layer-wise coordination.",
        "Mixed Attention In order to coordinate the learning of source and target tokens in each layer, the decoder of our model uses a mixed attention for a target token to extract cross-position information from both the source and preceding target tokens.",
        "Transformer nearly translates the meaning of the source sentence adequately, the result suffers from fluency compared to our model.",
        "As the typical encoder-decoder framework, uses two attentions to extract information from source and target separately and it extracts the source information from the last layer of hidden representations of the encoder, and the low layer of the decoder may not extract this high-level representation precisely.",
        "This kind of cases show the advantages of our layer-wise coordination learning over the typical encoder-decoder based models.",
        "Our mixed attention is designed to better coordinate the learning of encoder and decoder by extracting cross-position information from both the source and preceding target tokens in one and the same attention function.",
        "We improved existing NMT models through layer-wise coordination of the encoder and decoder.",
        "Our method aligns the i-th layer of the encoder to the i-th layer of the decoder and coordinates the learning of the hidden representations of source and target sentences layer by layer, by sharing the parameters of the aligned layers.",
        "Experiments on several translation tasks demonstrated our proposed model outperforms the Transformer baseline as well as other RNN/CNN-based encoderdecoder models.",
        "We will study how to leverage layer-wise coordination to train deeper NMT models"
    ],
    "headline": "We propose the concept of layer-wise coordination for Neural Machine Translation, which explicitly coordinates the learning of hidden representations of the encoder and decoder together layer by layer, gradually from low level to high level",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An actor-critic algorithm for sequence prediction. 5th International Conference on Learning Representations, ICLR, 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Brakel%2C%20P.%20Xu%2C%20K.%20Goyal%2C%20A.%20An%20actor-critic%20algorithm%20for%20sequence%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Brakel%2C%20P.%20Xu%2C%20K.%20Goyal%2C%20A.%20An%20actor-critic%20algorithm%20for%20sequence%20prediction%202017"
        },
        {
            "id": "2",
            "entry": "[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. ICLR 2015, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate"
        },
        {
            "id": "3",
            "entry": "[3] M. Cettolo, J. Niehues, S. St\u00fcker, L. Bentivogli, and M. Federico. Report on the 11th iwslt evaluation campaign. In Proceedings of the 11th IWSLT, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Cettolo%20J%20Niehues%20S%20St%C3%BCker%20L%20Bentivogli%20and%20M%20Federico%20Report%20on%20the%2011th%20iwslt%20evaluation%20campaign%20In%20Proceedings%20of%20the%2011th%20IWSLT%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Cettolo%20J%20Niehues%20S%20St%C3%BCker%20L%20Bentivogli%20and%20M%20Federico%20Report%20on%20the%2011th%20iwslt%20evaluation%20campaign%20In%20Proceedings%20of%20the%2011th%20IWSLT%202014"
        },
        {
            "id": "4",
            "entry": "[4] J. Cheng, L. Dong, and M. Lapata. Long short-term memory-networks for machine reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 551\u2013561, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20J.%20Dong%2C%20L.%20Lapata%2C%20M.%20Long%20short-term%20memory-networks%20for%20machine%20reading%202016-11-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20J.%20Dong%2C%20L.%20Lapata%2C%20M.%20Long%20short-term%20memory-networks%20for%20machine%20reading%202016-11-01"
        },
        {
            "id": "5",
            "entry": "[5] K. Cho, B. van Merrienboer, \u00c7. G\u00fcl\u00e7ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1724\u20131734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20K.%20van%20Merrienboer%2C%20B.%20G%C3%BCl%C3%A7ehre%2C%20%C3%87.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014-10-25",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20K.%20van%20Merrienboer%2C%20B.%20G%C3%BCl%C3%A7ehre%2C%20%C3%87.%20Bahdanau%2C%20D.%20Learning%20phrase%20representations%20using%20RNN%20encoder-decoder%20for%20statistical%20machine%20translation%202014-10-25"
        },
        {
            "id": "6",
            "entry": "[6] Y. Ding, Y. Liu, H. Luan, and M. Sun. Visualizing and understanding neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1150\u20131159, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Y.%20Liu%2C%20Y.%20Luan%2C%20H.%20Sun%2C%20M.%20Visualizing%20and%20understanding%20neural%20machine%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Y.%20Liu%2C%20Y.%20Luan%2C%20H.%20Sun%2C%20M.%20Visualizing%20and%20understanding%20neural%20machine%20translation%202017"
        },
        {
            "id": "7",
            "entry": "[7] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1243\u20131252, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20J.%20Auli%2C%20M.%20Grangier%2C%20D.%20Yarats%2C%20D.%20Convolutional%20sequence%20to%20sequence%20learning%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20J.%20Auli%2C%20M.%20Grangier%2C%20D.%20Yarats%2C%20D.%20Convolutional%20sequence%20to%20sequence%20learning%202017-08"
        },
        {
            "id": "8",
            "entry": "[8] J. Gu, J. Bradbury, C. Xiong, V. O. K. Li, and R. Socher. Non-autoregressive neural machine translation. CoRR, abs/1711.02281, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02281"
        },
        {
            "id": "9",
            "entry": "[9] H. Hassan, A. Aue, C. Chen, V. Chowdhary, J. Clark, C. Federmann, X. Huang, M. JunczysDowmunt, W. Lewis, M. Li, S. Liu, T. Liu, R. Luo, A. Menezes, T. Qin, F. Seide, X. Tan, F. Tian, L. Wu, S. Wu, Y. Xia, D. Zhang, Z. Zhang, and M. Zhou. Achieving human parity on automatic chinese to english news translation. CoRR, abs/1803.05567, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05567"
        },
        {
            "id": "10",
            "entry": "[10] B. Hu, Z. Lu, H. Li, and Q. Chen. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2042\u20132050, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20B.%20Lu%2C%20Z.%20Li%2C%20H.%20Chen%2C%20Q.%20Convolutional%20neural%20network%20architectures%20for%20matching%20natural%20language%20sentences%202014-12-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20B.%20Lu%2C%20Z.%20Li%2C%20H.%20Chen%2C%20Q.%20Convolutional%20neural%20network%20architectures%20for%20matching%20natural%20language%20sentences%202014-12-08"
        },
        {
            "id": "11",
            "entry": "[11] P.-S. Huang, C. Wang, D. Zhou, and L. Deng. Neural phrase-based machine translation. arXiv preprint arXiv:1706.05565, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05565"
        },
        {
            "id": "12",
            "entry": "[12] N. Kalchbrenner, L. Espeholt, K. Simonyan, A. van den Oord, A. Graves, and K. Kavukcuoglu. Neural machine translation in linear time. CoRR, abs/1610.10099, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.10099"
        },
        {
            "id": "13",
            "entry": "[13] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "14",
            "entry": "[14] P. Koehn. Statistical machine translation. Draft of chapter, 13, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koehn%2C%20P.%20Statistical%20machine%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koehn%2C%20P.%20Statistical%20machine%20translation%202017"
        },
        {
            "id": "15",
            "entry": "[15] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio. A structured self-attentive sentence embedding. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Z.%20Feng%2C%20M.%20dos%20Santos%2C%20C.N.%20Yu%2C%20M.%20A%20structured%20self-attentive%20sentence%20embedding%202017"
        },
        {
            "id": "16",
            "entry": "[16] Z. Lu and H. Li. A deep architecture for matching short texts. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 1367\u20131375, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Z.%20Li%2C%20H.%20A%20deep%20architecture%20for%20matching%20short%20texts%202013-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Z.%20Li%2C%20H.%20A%20deep%20architecture%20for%20matching%20short%20texts%202013-12-05"
        },
        {
            "id": "17",
            "entry": "[17] M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. In EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20M.-T.%20Pham%2C%20H.%20Manning%2C%20C.D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20M.-T.%20Pham%2C%20H.%20Manning%2C%20C.D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015"
        },
        {
            "id": "18",
            "entry": "[18] T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1412\u20131421, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20T.%20Pham%2C%20H.%20Manning%2C%20C.D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015-09-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20T.%20Pham%2C%20H.%20Manning%2C%20C.D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015-09-17"
        },
        {
            "id": "19",
            "entry": "[19] L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, and X. Cheng. Text matching as image recognition. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 2793\u20132799, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pang%2C%20L.%20Lan%2C%20Y.%20Guo%2C%20J.%20Xu%2C%20J.%20Text%20matching%20as%20image%20recognition%202016-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20L.%20Lan%2C%20Y.%20Guo%2C%20J.%20Xu%2C%20J.%20Text%20matching%20as%20image%20recognition%202016-02"
        },
        {
            "id": "20",
            "entry": "[20] K. Papineni, S. Roukos, T. Ward, and W. Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA., pages 311\u2013318, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002-07-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20K.%20Roukos%2C%20S.%20Ward%2C%20T.%20Zhu%2C%20W.%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002-07-06"
        },
        {
            "id": "21",
            "entry": "[21] A. P. Parikh, O. T\u00e4ckstr\u00f6m, D. Das, and J. Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2249\u20132255, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parikh%2C%20A.P.%20T%C3%A4ckstr%C3%B6m%2C%20O.%20Das%2C%20D.%20Uszkoreit%2C%20J.%20A%20decomposable%20attention%20model%20for%20natural%20language%20inference%202016-11-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parikh%2C%20A.P.%20T%C3%A4ckstr%C3%B6m%2C%20O.%20Das%2C%20D.%20Uszkoreit%2C%20J.%20A%20decomposable%20attention%20model%20for%20natural%20language%20inference%202016-11-01"
        },
        {
            "id": "22",
            "entry": "[22] R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. CoRR, abs/1705.04304, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.04304"
        },
        {
            "id": "23",
            "entry": "[23] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. CoRR, abs/1511.06732, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06732"
        },
        {
            "id": "24",
            "entry": "[24] R. Sennrich, B. Haddow, and A. Birch. Edinburgh neural machine translation systems for WMT 16. In Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany, pages 371\u2013376, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20R.%20Haddow%2C%20B.%20Birch%2C%20A.%20Edinburgh%20neural%20machine%20translation%20systems%20for%20WMT%2016%202016-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20R.%20Haddow%2C%20B.%20Birch%2C%20A.%20Edinburgh%20neural%20machine%20translation%20systems%20for%20WMT%2016%202016-08"
        },
        {
            "id": "25",
            "entry": "[25] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20R.%20Haddow%2C%20B.%20Birch%2C%20A.%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20R.%20Haddow%2C%20B.%20Birch%2C%20A.%20Neural%20machine%20translation%20of%20rare%20words%20with%20subword%20units%202016-08"
        },
        {
            "id": "26",
            "entry": "[26] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.06538"
        },
        {
            "id": "27",
            "entry": "[27] Y. Shen, X. Tan, D. He, T. Qin, and T. Liu. Dense information flow for neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1294\u20131303, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Y.%20Tan%2C%20X.%20He%2C%20D.%20Qin%2C%20T.%20Dense%20information%20flow%20for%20neural%20machine%20translation%202018-06-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Y.%20Tan%2C%20X.%20He%2C%20D.%20Qin%2C%20T.%20Dense%20information%20flow%20for%20neural%20machine%20translation%202018-06-01"
        },
        {
            "id": "28",
            "entry": "[28] K. Song, X. Tan, D. He, J. Lu, T. Qin, and T. Liu. Double path networks for sequence to sequence learning. In Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August 20-26, 2018, pages 3064\u20133074, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20K.%20Tan%2C%20X.%20He%2C%20D.%20Lu%2C%20J.%20Double%20path%20networks%20for%20sequence%20to%20sequence%20learning%202018-08-20",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20K.%20Tan%2C%20X.%20He%2C%20D.%20Lu%2C%20J.%20Double%20path%20networks%20for%20sequence%20to%20sequence%20learning%202018-08-20"
        },
        {
            "id": "29",
            "entry": "[29] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20I.%20Vinyals%2C%20O.%20Le%2C%20Q.V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014-12-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20I.%20Vinyals%2C%20O.%20Le%2C%20Q.V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014-12-08"
        },
        {
            "id": "30",
            "entry": "[30] Z. Tu, Y. Liu, Z. Lu, X. Liu, and H. Li. Context gates for neural machine translation. TACL, 5:87\u201399, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tu%2C%20Z.%20Liu%2C%20Y.%20Lu%2C%20Z.%20Liu%2C%20X.%20Context%20gates%20for%20neural%20machine%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tu%2C%20Z.%20Liu%2C%20Y.%20Lu%2C%20Z.%20Liu%2C%20X.%20Context%20gates%20for%20neural%20machine%20translation%202017"
        },
        {
            "id": "31",
            "entry": "[31] Z. Tu, Y. Liu, L. Shang, X. Liu, and H. Li. Neural machine translation with reconstruction. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., pages 3097\u20133103, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tu%2C%20Z.%20Liu%2C%20Y.%20Shang%2C%20L.%20Liu%2C%20X.%20Neural%20machine%20translation%20with%20reconstruction%202017-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tu%2C%20Z.%20Liu%2C%20Y.%20Shang%2C%20L.%20Liu%2C%20X.%20Neural%20machine%20translation%20with%20reconstruction%202017-02"
        },
        {
            "id": "32",
            "entry": "[32] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07416"
        },
        {
            "id": "33",
            "entry": "[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaswani%2C%20A.%20Shazeer%2C%20N.%20Parmar%2C%20N.%20Uszkoreit%2C%20J.%20Attention%20is%20all%20you%20need%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaswani%2C%20A.%20Shazeer%2C%20N.%20Parmar%2C%20N.%20Uszkoreit%2C%20J.%20Attention%20is%20all%20you%20need%202017-12"
        },
        {
            "id": "34",
            "entry": "[34] Y. Wang, Y. Xia, L. Zhao, J. Bian, T. Qin, G. Liu, and Liu. Dual transfer learning for neural machine translation with marginal distribution regularization. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Xia%2C%20Y.%20Zhao%2C%20L.%20Bian%2C%20J.%20Dual%20transfer%20learning%20for%20neural%20machine%20translation%20with%20marginal%20distribution%20regularization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Xia%2C%20Y.%20Zhao%2C%20L.%20Bian%2C%20J.%20Dual%20transfer%20learning%20for%20neural%20machine%20translation%20with%20marginal%20distribution%20regularization%202018"
        },
        {
            "id": "35",
            "entry": "[35] L. Wu, X. Tan, D. He, F. Tian, T. Qin, J. Lai, and T. Liu. Beyond error propagation in neural machine translation: Characteristics of language also matter. In EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20L.%20Tan%2C%20X.%20He%2C%20D.%20Tian%2C%20F.%20Beyond%20error%20propagation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20L.%20Tan%2C%20X.%20He%2C%20D.%20Tian%2C%20F.%20Beyond%20error%20propagation%202018"
        },
        {
            "id": "36",
            "entry": "[36] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "37",
            "entry": "[37] Y. Xia, X. Tan, F. Tian, T. Qin, N. Yu, and T. Liu. Model-level dual learning. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, pages 5379\u20135388, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xia%2C%20Y.%20Tan%2C%20X.%20Tian%2C%20F.%20Qin%2C%20T.%20Model-level%20dual%20learning%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xia%2C%20Y.%20Tan%2C%20X.%20Tian%2C%20F.%20Qin%2C%20T.%20Model-level%20dual%20learning%202018-07"
        }
    ]
}
