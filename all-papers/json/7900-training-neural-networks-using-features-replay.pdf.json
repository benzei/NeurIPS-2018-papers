{
    "filename": "7900-training-neural-networks-using-features-replay.pdf",
    "metadata": {
        "title": "Training Neural Networks Using Features Replay",
        "author": "Zhouyuan Huo, Bin Gu, Heng Huang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7900-training-neural-networks-using-features-replay.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network. The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources. Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep. To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that, we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves faster convergence, lower memory consumption, and better generalization error than compared methods."
    },
    "keywords": [
        {
            "term": "generalization error",
            "url": "https://en.wikipedia.org/wiki/generalization_error"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "low memory",
            "url": "https://en.wikipedia.org/wiki/low_memory"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "convex problem",
            "url": "https://en.wikipedia.org/wiki/convex_problem"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/deep_convolutional_neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "critical point",
            "url": "https://en.wikipedia.org/wiki/critical_point"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "As long as the loss functions are differentiable, we can compute the gradients of the networks using backpropagation algorithm [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "We propose a novel parallel-objective formulation for the objective function of the neural networks in Section 3",
        "We provide the theoretical analysis in Section 4 and prove that the proposed method is guaranteed to converge to critical points for the non-convex problem under certain conditions",
        "We proposed a novel parallel-objective formulation for the objective function of the neural network and broke the backward locking using a new features replay algorithm",
        "Our theoretical contributions include analyzing the convergence property of the proposed method and proving that our new algorithm is guaranteed to converge to critical points for the non-convex problem under certain conditions",
        "We conducted experiments with deep convolutional neural networks on two image classification datasets, and all experimental results verify that the proposed method can achieve faster convergence, lower memory consumption, and better generalization error than compared methods"
    ],
    "key_statements": [
        "As long as the loss functions are differentiable, we can compute the gradients of the networks using backpropagation algorithm [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "We propose a novel parallel-objective formulation for the objective function of the neural networks in Section 3",
        "We provide the theoretical analysis in Section 4 and prove that the proposed method is guaranteed to converge to critical points for the non-convex problem under certain conditions",
        "By using stochastic gradient descent, the weights of the network are updated in the direction of their negative gradients of the loss function following: wlt+1 = wlt \u2212 \u03b3t \u00b7 glt for all l \u2208 {1, 2, ..., L}",
        "We solve this problem by building a connection between the gradients of Algorithm 1 and stochastic gradient descent in Assumption 1, and prove that the proposed method is guaranteed to converge to critical points for the non-convex problem (1)",
        "We proposed a novel parallel-objective formulation for the objective function of the neural network and broke the backward locking using a new features replay algorithm",
        "Our theoretical contributions include analyzing the convergence property of the proposed method and proving that our new algorithm is guaranteed to converge to critical points for the non-convex problem under certain conditions",
        "We conducted experiments with deep convolutional neural networks on two image classification datasets, and all experimental results verify that the proposed method can achieve faster convergence, lower memory consumption, and better generalization error than compared methods"
    ],
    "summary": [
        "As long as the loss functions are differentiable, we can compute the gradients of the networks using backpropagation algorithm [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>].",
        "The backpropagation algorithm requires two passes of the neural network, the forward pass to compute activations and the backward pass to compute gradients.",
        "In [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], the authors proposed to remove the backward locking by employing the decoupled neural interface to approximate error gradients (Figure 1 DNI).",
        "The backpropagation algorithm [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] is utilized to compute the gradients for the neural networks.",
        "To break the dependencies between modules in the backward pass, we propose to compute the gradients of the modules using immediate features from different timestamps.",
        "We break the backward locking in the backpropagation algorithm because the error gradient variable \u03b4kt can be determined at the previous iteration t \u2212 1 such that all modules are independent of each other at iteration t.",
        "We solve this problem by building a connection between the gradients of Algorithm 1 and stochastic gradient descent in Assumption 1, and prove that the proposed method is guaranteed to converge to critical points for the non-convex problem (1).",
        "Algorithm can guarantee the convergence to critical points for the non-convex problem, as long as the diminishing stepsizes satisfy the requirements in [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] such that: T \u22121 lim \u03b3t = \u221e and",
        "According to Theorem 2, we can prove that Algorithm 1 guarantees convergence to critical points for the non-convex problem: lim E",
        "Assumption 1 is satisfied such that Algorithm 1 is guaranteed to converge to the critical points for the non-convex problem.",
        "FR 6.03 27.34 4.97 23.10 4.91 23.61 think it is related to the variation Table 2: Best testing error rates (%) of the compared methods on of the sufficient descent constant CIFAR-10 and CIFAR-100 datasets.",
        "We proposed a novel parallel-objective formulation for the objective function of the neural network and broke the backward locking using a new features replay algorithm.",
        "Our theoretical contributions include analyzing the convergence property of the proposed method and proving that our new algorithm is guaranteed to converge to critical points for the non-convex problem under certain conditions.",
        "We conducted experiments with deep convolutional neural networks on two image classification datasets, and all experimental results verify that the proposed method can achieve faster convergence, lower memory consumption, and better generalization error than compared methods."
    ],
    "headline": "To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] David Balduzzi, Hastagiri Vanchinathan, and Joachim M Buhmann. Kickback cuts backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks. In AAAI, pages 485\u2013491, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balduzzi%2C%20David%20Vanchinathan%2C%20Hastagiri%20Buhmann%2C%20Joachim%20M.%20Kickback%20cuts%20backprop%E2%80%99s%20red-tape%3A%20Biologically%20plausible%20credit%20assignment%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balduzzi%2C%20David%20Vanchinathan%2C%20Hastagiri%20Buhmann%2C%20Joachim%20M.%20Kickback%20cuts%20backprop%E2%80%99s%20red-tape%3A%20Biologically%20plausible%20credit%20assignment%20in%20neural%20networks%202015"
        },
        {
            "id": "2",
            "entry": "[2] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R in Machine Learning, 2(1):1\u2013127, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Learning%20deep%20architectures%20for%20ai%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Learning%20deep%20architectures%20for%20ai%202009"
        },
        {
            "id": "3",
            "entry": "[3] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. arXiv preprint arXiv:1606.04838, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04838"
        },
        {
            "id": "4",
            "entry": "[4] Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In Artificial Intelligence and Statistics, pages 10\u201319, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira-Perpinan%2C%20Miguel%20Wang%2C%20Weiran%20Distributed%20optimization%20of%20deeply%20nested%20systems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira-Perpinan%2C%20Miguel%20Wang%2C%20Weiran%20Distributed%20optimization%20of%20deeply%20nested%20systems%202014"
        },
        {
            "id": "5",
            "entry": "[5] Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting distributed synchronous sgd. arXiv preprint arXiv:1604.00981, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.00981"
        },
        {
            "id": "6",
            "entry": "[6] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "7",
            "entry": "[7] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pages 907\u2013940, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eldan%2C%20Ronen%20Shamir%2C%20Ohad%20The%20power%20of%20depth%20for%20feedforward%20neural%20networks%202016"
        },
        {
            "id": "8",
            "entry": "[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "9",
            "entry": "[9] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6a overview of mini\u2013 batch gradient descent. Coursera Lecture slides https://class.coursera.org/neuralnets-2012001/lecture,[Online, 2012.",
            "url": "https://class.coursera.org/neuralnets-2012001/lecture,[Online"
        },
        {
            "id": "10",
            "entry": "[10] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06993"
        },
        {
            "id": "11",
            "entry": "[11] Zhouyuan Huo, Bin Gu, Qian Yang, and Heng Huang. Decoupled parallel backpropagation with convergence guarantee. arXiv preprint arXiv:1804.10574, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.10574"
        },
        {
            "id": "12",
            "entry": "[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "13",
            "entry": "[13] Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.05343"
        },
        {
            "id": "14",
            "entry": "[14] Justin Johnson. Benchmarks for popular cnn models. https://github.com/jcjohnson/cnn-benchmarks, 2017.",
            "url": "https://github.com/jcjohnson/cnn-benchmarks"
        },
        {
            "id": "15",
            "entry": "[15] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1404.2188"
        },
        {
            "id": "16",
            "entry": "[16] Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1408.5882"
        },
        {
            "id": "17",
            "entry": "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "18",
            "entry": "[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "19",
            "entry": "[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "20",
            "entry": "[20] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015"
        },
        {
            "id": "21",
            "entry": "[21] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "22",
            "entry": "[22] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.4400"
        },
        {
            "id": "23",
            "entry": "[23] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "24",
            "entry": "[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "25",
            "entry": "[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "26",
            "entry": "[26] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20linear%20units%20improve%20restricted%20boltzmann%20machines%202010"
        },
        {
            "id": "27",
            "entry": "[27] Arild N\u00f8kland. Direct feedback alignment provides learning in deep neural networks. In Advances in Neural Information Processing Systems, pages 1037\u20131045, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=N%C3%B8kland%2C%20Arild%20Direct%20feedback%20alignment%20provides%20learning%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=N%C3%B8kland%2C%20Arild%20Direct%20feedback%20alignment%20provides%20learning%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "28",
            "entry": "[28] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "29",
            "entry": "[29] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method.%20The%20annals%20of%20mathematical%20statistics%201951"
        },
        {
            "id": "30",
            "entry": "[30] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20representations%20by%20back-propagating%20errors%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20representations%20by%20back-propagating%20errors%201988"
        },
        {
            "id": "31",
            "entry": "[31] Cicero D Santos and Bianca Zadrozny. Learning character-level representations for part-ofspeech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818\u20131826, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santos%2C%20Cicero%20D.%20Zadrozny%2C%20Bianca%20Learning%20character-level%20representations%20for%20part-ofspeech%20tagging%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santos%2C%20Cicero%20D.%20Zadrozny%2C%20Bianca%20Learning%20character-level%20representations%20for%20part-ofspeech%20tagging%202014"
        },
        {
            "id": "32",
            "entry": "[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "33",
            "entry": "[33] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "34",
            "entry": "[34] Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training neural networks without gradients: A scalable admm approach. In International Conference on Machine Learning, pages 2722\u20132731, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Gavin%20Burmeister%2C%20Ryan%20Xu%2C%20Zheng%20Singh%2C%20Bharat%20Training%20neural%20networks%20without%20gradients%3A%20A%20scalable%20admm%20approach%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Gavin%20Burmeister%2C%20Ryan%20Xu%2C%20Zheng%20Singh%2C%20Bharat%20Training%20neural%20networks%20without%20gradients%3A%20A%20scalable%20admm%20approach%202016"
        },
        {
            "id": "35",
            "entry": "[35] Matus Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.04485"
        },
        {
            "id": "36",
            "entry": "[36] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649\u2013657, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Xiang%20Zhao%2C%20Junbo%20LeCun%2C%20Yann%20Character-level%20convolutional%20networks%20for%20text%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Xiang%20Zhao%2C%20Junbo%20LeCun%2C%20Yann%20Character-level%20convolutional%20networks%20for%20text%20classification%202015"
        }
    ]
}
