{
    "filename": "7749-adversarially-robust-generalization-requires-more-data.pdf",
    "metadata": {
        "title": "Adversarially Robust Generalization Requires More Data",
        "author": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander Madry",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high \u201cstandard\u201d accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of \u201cstandard\u201d learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity."
    },
    "keywords": [
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "high confidence",
            "url": "https://en.wikipedia.org/wiki/high_confidence"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Modern machine learning models achieve high accuracy on a broad range of datasets, yet can be misled by small perturbations of their input",
        "Researchers have demonstrated the phenomenon under the name of adversarial examples in image classification [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], question answering [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], voice recognition [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>, <a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>], and other domains",
        "The vulnerability of neural networks to adversarial perturbations has recently been a source of much discussion and is still poorly understood",
        "Different works have argued that this vulnerability stems from their discontinuous nature [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], their linear nature [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], or is a result of high-dimensional geometry and independent of the model class [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "We show that for a natural data distribution, the model class we train does not matter and a standard linear classifier achieves optimal robustness",
        "For other data models, our results demonstrate that non-linearities are indispensable to learn from few samples. This dichotomy provides evidence that defenses against adversarial examples need to be tailored to the specific dataset and may be more complicated than a single, broad approach"
    ],
    "key_statements": [
        "Modern machine learning models achieve high accuracy on a broad range of datasets, yet can be misled by small perturbations of their input",
        "Researchers have demonstrated the phenomenon under the name of adversarial examples in image classification [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], question answering [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], voice recognition [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>, <a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>], and other domains",
        "Our results stand in stark contrast: we show that generalization can, in simple models, be significantly easier than robustness when sample complexity enters the picture",
        "The vulnerability of neural networks to adversarial perturbations has recently been a source of much discussion and is still poorly understood",
        "Different works have argued that this vulnerability stems from their discontinuous nature [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], their linear nature [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], or is a result of high-dimensional geometry and independent of the model class [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "We show that for a natural data distribution, the model class we train does not matter and a standard linear classifier achieves optimal robustness",
        "For other data models, our results demonstrate that non-linearities are indispensable to learn from few samples. This dichotomy provides evidence that defenses against adversarial examples need to be tailored to the specific dataset and may be more complicated than a single, broad approach"
    ],
    "summary": [
        "Modern machine learning models achieve high accuracy on a broad range of datasets, yet can be misled by small perturbations of their input.",
        "Already in a simple data model, adversarial examples provably occur for any learning approach, even when the classifier already achieves high standard accuracy.",
        "The results show that on MNIST, robust optimization is able to learn a model with around 90% adversarial accuracy and a relatively small gap between training and test error.",
        "The fact that we can establish a separation between standard and robust generalization already in our Gaussian data model is evidence that the existence of adversarial examples for neural",
        "The theorem shows that it is possible to learn an l\u221e \u03b5 -robust classifier in the Gaussian model as long as \u03b5 is bounded by a small constant and we have a large number of samples.",
        "As in the Gaussian model, we first calibrate the distribution so that we can learn a classifier with good standard accuracy from a single sample.4 The following theorem is a direct consequence of the fact that bounded random variables exhibit sub-Gaussian concentration.",
        "Our experiments in Section 3 show that an explicit thresholding layer significantly improves the sample complexity of training a robust neural network on MNIST.",
        "Motivated by our theoretical study of the Bernoulli model, we investigate whether thresholding can improve the sample complexity of robust generalization against an l\u221e adversary on MNIST.",
        "As predicted by our theory, the networks achieve good adversarially robust generalization with significantly fewer samples when thresholding filters are added.",
        "Note that adding a simple thresholding layer directly yields nearly state-of-the-art robustness against moderately strong adversaries (\u03b5 = 0.1), without any other modifications to the model architecture or training algorithm.",
        "In contrast to our work, the authors give theoretical guarantees for a specific classification algorithm, and do not see a separation in sample complexity between robust and regular generalization.",
        "Our results stand in stark contrast: we show that generalization can, in simple models, be significantly easier than robustness when sample complexity enters the picture.",
        "We show that for a natural data distribution, the model class we train does not matter and a standard linear classifier achieves optimal robustness.",
        "Understanding the interactions between robustness, classifier model, and data distribution from the perspective of generalization is an important direction for future work.",
        "Our Gaussian lower bound implies that if an algorithm works for all settings of the unknown parameter \u03b8\u22c6, achieving strong l\u221e-robustness requires a sample complexity increase that is polynomial in the dimension."
    ],
    "headline": "To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Tensor flow models repository. https://www.tensorflow.org/tutorials/layers, 2017.",
            "url": "https://www.tensorflow.org/tutorials/layers"
        },
        {
            "id": "2",
            "entry": "[2] Anurag Arnab, Ondrej Miksik, and Philip H. S. Torr. On the robustness of semantic segmentation models to adversarial attacks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. URL http://arxiv.org/abs/1711.09856.",
            "url": "http://arxiv.org/abs/1711.09856",
            "arxiv_url": "https://arxiv.org/pdf/1711.09856"
        },
        {
            "id": "3",
            "entry": "[3] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning (ICML), 2018. URL https://arxiv.org/abs/1802.00420.",
            "url": "https://arxiv.org/abs/1802.00420",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "4",
            "entry": "[4] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction attacks. In International Conference on Machine Learning and Data Mining (MLDM), 2017. URL https://arxiv.org/abs/1701.04143.",
            "url": "https://arxiv.org/abs/1701.04143",
            "arxiv_url": "https://arxiv.org/pdf/1701.04143"
        },
        {
            "id": "5",
            "entry": "[5] Aur\u00e9lien Bellet and Amaury Habrard. Robustness and generalization for metric learning. Neurocomputing, 2015. URL https://arxiv.org/abs/1209.1086.",
            "url": "https://arxiv.org/abs/1209.1086",
            "arxiv_url": "https://arxiv.org/pdf/1209.1086"
        },
        {
            "id": "6",
            "entry": "[6] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-Tal%2C%20Aharon%20Ghaoui%2C%20Laurent%20El%20Nemirovski%2C%20Arkadi%20Robust%20optimization%202009"
        },
        {
            "id": "7",
            "entry": "[7] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 2018. URL https://arxiv.org/abs/1712.03141.",
            "url": "https://arxiv.org/abs/1712.03141",
            "arxiv_url": "https://arxiv.org/pdf/1712.03141"
        },
        {
            "id": "8",
            "entry": "[8] St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boucheron%2C%20St%C3%A9phane%20Lugosi%2C%20G%C3%A1bor%20Massart%2C%20Pascal%20Concentration%20Inequalities%3A%20A%20Nonasymptotic%20Theory%20of%20Independence%202013"
        },
        {
            "id": "9",
            "entry": "[9] Nader H. Bshouty, Nadav Eiron, and Eyal Kushilevitz. PAC learning with nasty noise. In Algorithmic Learning Theory (ALT), 1999. URL https://link.springer.com/chapter/10.1007/3-540-46769-6_17.",
            "url": "https://link.springer.com/chapter/10.1007/3-540-46769-6_17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bshouty%2C%20Nader%20H.%20Eiron%2C%20Nadav%20Kushilevitz%2C%20Eyal%20PAC%20learning%20with%20nasty%20noise%201999"
        },
        {
            "id": "10",
            "entry": "[10] Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Defensive%20distillation%20is%20not%20robust%20to%20adversarial%20examples%202016"
        },
        {
            "id": "11",
            "entry": "[11] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Symposium on Security and Privacy (SP), 2016. URL http://arxiv.org/abs/1608.04644.",
            "url": "http://arxiv.org/abs/1608.04644",
            "arxiv_url": "https://arxiv.org/pdf/1608.04644"
        },
        {
            "id": "12",
            "entry": "[12] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speechto-text. In Security and Privacy Workshops (SPW), 2018. URL https://arxiv.org/abs/1801.01944.",
            "url": "https://arxiv.org/abs/1801.01944",
            "arxiv_url": "https://arxiv.org/pdf/1801.01944"
        },
        {
            "id": "13",
            "entry": "[13] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. Hidden voice commands. In USENIX Security Symposium, 2016. URL https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/carlini.",
            "url": "https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/carlini",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%202016"
        },
        {
            "id": "14",
            "entry": "[14] Moustapha M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Fooling deep structured visual and speech recognition models with adversarial examples. In Neural Information Processing Systems (NIPS), 2017. URL https://arxiv.org/abs/1707.05373.",
            "url": "https://arxiv.org/abs/1707.05373",
            "arxiv_url": "https://arxiv.org/pdf/1707.05373"
        },
        {
            "id": "15",
            "entry": "[15] Ekin D. Cubuk Cubuk, Barret Zoph, Samuel S. Schoenholz, and Quoc V. Le. Intriguing properties of adversarial examples. arXiv, 2017. URL https://arxiv.org/abs/1711.02846.",
            "url": "https://arxiv.org/abs/1711.02846",
            "arxiv_url": "https://arxiv.org/pdf/1711.02846"
        },
        {
            "id": "16",
            "entry": "[16] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Adversarial classification. In International Conference on Knowledge Discovery and Data Mining (KDD), 2004. URL http://doi.acm.org/10.1145/1014052.1014066.",
            "url": "http://doi.acm.org/10.1145/1014052.1014066",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalvi%2C%20Nilesh%20Domingos%2C%20Pedro%20Mausam%2C%20Sumit%20Sanghai%20Verma%2C%20Deepak%20Adversarial%20classification%202004"
        },
        {
            "id": "17",
            "entry": "[17] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation suffice: Fooling CNNs with simple transformations. arXiv, 2017. URL https://arxiv.org/abs/1712.02779.",
            "url": "https://arxiv.org/abs/1712.02779",
            "arxiv_url": "https://arxiv.org/pdf/1712.02779"
        },
        {
            "id": "18",
            "entry": "[18] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In Neural Information Processing Systems (NIPS), 2016. URL https://arxiv.org/abs/1608.08967.",
            "url": "https://arxiv.org/abs/1608.08967",
            "arxiv_url": "https://arxiv.org/pdf/1608.08967"
        },
        {
            "id": "19",
            "entry": "[19] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv, 2018. URL https://arxiv.org/abs/1802.08686.",
            "url": "https://arxiv.org/abs/1802.08686",
            "arxiv_url": "https://arxiv.org/pdf/1802.08686"
        },
        {
            "id": "20",
            "entry": "[20] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv, 2018. URL https://arxiv.org/abs/1801.02774.",
            "url": "https://arxiv.org/abs/1801.02774",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "21",
            "entry": "[21] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv, 2014. URL http://arxiv.org/abs/1412.6572.",
            "url": "http://arxiv.org/abs/1412.6572",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "22",
            "entry": "[22] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick D. McDaniel. Adversarial perturbations against deep neural networks for malware classification. In European Symposium on Research in Computer Security (ESORICS), 2016. URL http://arxiv.org/abs/1606.04435.",
            "url": "http://arxiv.org/abs/1606.04435",
            "arxiv_url": "https://arxiv.org/pdf/1606.04435"
        },
        {
            "id": "23",
            "entry": "[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL https://arxiv.org/abs/1512.03385.",
            "url": "https://arxiv.org/abs/1512.03385",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "24",
            "entry": "[24] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example defenses: Ensembles of weak defenses are not strong. In USENIX Workshop on Offensive Technologies, 2017. URL https://arxiv.org/abs/1706.04701.",
            "url": "https://arxiv.org/abs/1706.04701",
            "arxiv_url": "https://arxiv.org/pdf/1706.04701"
        },
        {
            "id": "25",
            "entry": "[25] Alex Huang, Abdullah Al-Dujaili, Erik Hemberg, and Una-May O\u2019Reilly. Adversarial deep learning for robust detection of binary encoded malware. In Security and Privacy Workshops (SPW), 2018. URL https://arxiv.org/abs/1801.02950.",
            "url": "https://arxiv.org/abs/1801.02950",
            "arxiv_url": "https://arxiv.org/pdf/1801.02950"
        },
        {
            "id": "26",
            "entry": "[26] Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. In International Conference on Learning Representations (ICLR), 2017. URL https://arxiv.org/abs/1702.02284.",
            "url": "https://arxiv.org/abs/1702.02284",
            "arxiv_url": "https://arxiv.org/pdf/1702.02284"
        },
        {
            "id": "27",
            "entry": "[27] Peter J. Huber. Robust Statistics. Wiley, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huber%2C%20Peter%20J.%20Robust%20Statistics%201981"
        },
        {
            "id": "28",
            "entry": "[28] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017. URL https://arxiv.org/abs/1707.07328.",
            "url": "https://arxiv.org/abs/1707.07328",
            "arxiv_url": "https://arxiv.org/pdf/1707.07328"
        },
        {
            "id": "29",
            "entry": "[29] Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on Computing, 1993. URL http://dx.doi.org/10.1137/0222052.",
            "crossref": "https://dx.doi.org/10.1137/0222052",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/0222052"
        },
        {
            "id": "30",
            "entry": "[30] Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efficient agnostic learning. Machine Learning, 1994. URL https://doi.org/10.1023/A:1022615600103.",
            "crossref": "https://dx.doi.org/10.1023/A:1022615600103",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1023/A%3A1022615600103"
        },
        {
            "id": "31",
            "entry": "[31] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1711.00851.",
            "url": "https://arxiv.org/abs/1711.00851",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "32",
            "entry": "[32] Jernej Kos, Ian Fischer, and Dawn Song. Adversarial examples for generative models. In Security and Privacy Workshops (SPW), 2018. URL http://arxiv.org/abs/1702.06832.",
            "url": "http://arxiv.org/abs/1702.06832",
            "arxiv_url": "https://arxiv.org/pdf/1702.06832"
        },
        {
            "id": "33",
            "entry": "[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009. URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.",
            "url": "https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf"
        },
        {
            "id": "34",
            "entry": "[34] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of handwritten digits. Website, 1998. URL http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Cortes%2C%20Corinna%20Burges%2C%20Christopher%20J.C.%20The%20mnist%20database%20of%20handwritten%20digits%201998"
        },
        {
            "id": "35",
            "entry": "[35] Daniel Lowd and Christopher Meek. Adversarial learning. In International Conference on Knowledge Discovery in Data Mining (KDD), 2005. URL http://doi.acm.org/10.1145/1081870.1081950.",
            "url": "http://doi.acm.org/10.1145/1081870.1081950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowd%2C%20Daniel%20Meek%2C%20Christopher%20Adversarial%20learning%202005"
        },
        {
            "id": "36",
            "entry": "[36] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1706.06083.",
            "url": "https://arxiv.org/abs/1706.06083",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "37",
            "entry": "[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL https://arxiv.org/abs/1511.04599.",
            "url": "https://arxiv.org/abs/1511.04599",
            "arxiv_url": "https://arxiv.org/pdf/1511.04599"
        },
        {
            "id": "38",
            "entry": "[38] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. URL https://arxiv.org/abs/1610.08401.",
            "url": "https://arxiv.org/abs/1610.08401",
            "arxiv_url": "https://arxiv.org/pdf/1610.08401"
        },
        {
            "id": "39",
            "entry": "[39] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations for deep networks. In Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017. URL http://arxiv.org/abs/1612.06299.",
            "url": "http://arxiv.org/abs/1612.06299",
            "arxiv_url": "https://arxiv.org/pdf/1612.06299"
        },
        {
            "id": "40",
            "entry": "[40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. URL http://ufldl.stanford.edu/housenumbers/.",
            "url": "http://ufldl.stanford.edu/housenumbers/",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "41",
            "entry": "[41] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Symposium on Security and Privacy (SP), 2016. URL https://arxiv.org/abs/1511.04508.",
            "url": "https://arxiv.org/abs/1511.04508",
            "arxiv_url": "https://arxiv.org/pdf/1511.04508"
        },
        {
            "id": "42",
            "entry": "[42] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In European Symposium on Security and Privacy (EuroS&P), 2016. URL https://arxiv.org/abs/1511.07528.",
            "url": "https://arxiv.org/abs/1511.07528",
            "arxiv_url": "https://arxiv.org/pdf/1511.07528"
        },
        {
            "id": "43",
            "entry": "[43] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of security and privacy in machine learning. In European Symposium on Security and Privacy (EuroS&P), 2018. URL https://arxiv.org/abs/1611.03814.",
            "url": "https://arxiv.org/abs/1611.03814",
            "arxiv_url": "https://arxiv.org/pdf/1611.03814"
        },
        {
            "id": "44",
            "entry": "[44] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1801.09344.",
            "url": "https://arxiv.org/abs/1801.09344",
            "arxiv_url": "https://arxiv.org/pdf/1801.09344"
        },
        {
            "id": "45",
            "entry": "[45] Phillippe Rigollet and Jan-Christian H\u00fctter. High-dimensional statistics. Lecture notes, 2017. URL http://www-math.mit.edu/~rigollet/PDFs/RigNotes17.pdf.",
            "url": "http://www-math.mit.edu/~rigollet/PDFs/RigNotes17.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rigollet%2C%20Phillippe%20H%C3%BCtter%2C%20Jan-Christian%20High-dimensional%20statistics%202017"
        },
        {
            "id": "46",
            "entry": "[46] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. URL http://www.cs.huji.ac.il/~shais/ UnderstandingMachineLearning/.",
            "url": "http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/"
        },
        {
            "id": "47",
            "entry": "[47] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Conference on Computer and Communications Security (CCS), 2016. URL http://doi.acm.org/10.1145/2976749.2978392.",
            "url": "http://doi.acm.org/10.1145/2976749.2978392",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "48",
            "entry": "[48] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1710.10571.",
            "url": "https://arxiv.org/abs/1710.10571",
            "arxiv_url": "https://arxiv.org/pdf/1710.10571"
        },
        {
            "id": "49",
            "entry": "[49] Liwei Song and Prateek Mittal. Inaudible voice commands. In Conference on Computer and Communications Security (CCS), 2017. URL http://arxiv.org/abs/1708.07238.",
            "url": "http://arxiv.org/abs/1708.07238",
            "arxiv_url": "https://arxiv.org/pdf/1708.07238"
        },
        {
            "id": "50",
            "entry": "[50] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural networks. arXiv, 2017. URL http://arxiv.org/abs/1710.08864.",
            "url": "http://arxiv.org/abs/1710.08864",
            "arxiv_url": "https://arxiv.org/pdf/1710.08864"
        },
        {
            "id": "51",
            "entry": "[51] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014. URL http://arxiv.org/abs/1312.6199.",
            "url": "http://arxiv.org/abs/1312.6199",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "52",
            "entry": "[52] Florian Tram\u00e8r, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D. McDaniel. The space of transferable adversarial examples. arXiv, 2017. URL http://arxiv.org/abs/1704.03453.",
            "url": "http://arxiv.org/abs/1704.03453",
            "arxiv_url": "https://arxiv.org/pdf/1704.03453"
        },
        {
            "id": "53",
            "entry": "[53] Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick D. McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations (ICLR), 2018. URL http://arxiv.org/abs/1705.07204.",
            "url": "http://arxiv.org/abs/1705.07204",
            "arxiv_url": "https://arxiv.org/pdf/1705.07204"
        },
        {
            "id": "54",
            "entry": "[54] Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of Mathematics, 1945.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wald%2C%20Abraham%20Statistical%20decision%20functions%20which%20minimize%20the%20maximum%20risk%201945",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wald%2C%20Abraham%20Statistical%20decision%20functions%20which%20minimize%20the%20maximum%20risk%201945"
        },
        {
            "id": "55",
            "entry": "[55] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors to adversarial examples. In International Conference on Machine Learning (ICML), 2018. URL http://proceedings.mlr.press/v80/wang18c/wang18c.pdf.",
            "url": "http://proceedings.mlr.press/v80/wang18c/wang18c.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yizhen%20Jha%2C%20Somesh%20Chaudhuri%2C%20Kamalika%20Analyzing%20the%20robustness%20of%20nearest%20neighbors%20to%20adversarial%20examples%202018"
        },
        {
            "id": "56",
            "entry": "[56] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed adversarial examples. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1801.02612.",
            "url": "https://arxiv.org/abs/1801.02612",
            "arxiv_url": "https://arxiv.org/pdf/1801.02612"
        },
        {
            "id": "57",
            "entry": "[57] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 2012. URL https://arxiv.org/abs/1005.2243.",
            "url": "https://arxiv.org/abs/1005.2243",
            "arxiv_url": "https://arxiv.org/pdf/1005.2243"
        },
        {
            "id": "58",
            "entry": "[58] Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. Journal of Machine Learning Research (JMLR), 2009. URL http://www.jmlr.org/papers/v10/xu09b.html.",
            "url": "http://www.jmlr.org/papers/v10/xu09b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huan%20Caramanis%2C%20Constantine%20Mannor%2C%20Shie%20Robustness%20and%20regularization%20of%20support%20vector%20machines%202009"
        },
        {
            "id": "59",
            "entry": "[59] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. In Network and Distributed System Security Symposium (NDSS), 2017. URL https://arxiv.org/abs/1704.01155.",
            "url": "https://arxiv.org/abs/1704.01155",
            "arxiv_url": "https://arxiv.org/pdf/1704.01155"
        },
        {
            "id": "60",
            "entry": "[60] Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darell, and Dawn Song. Can you fool AI with adversarial examples on a visual Turing test? arXiv, 2017. URL http://arxiv.org/abs/1709.08693.",
            "url": "http://arxiv.org/abs/1709.08693",
            "arxiv_url": "https://arxiv.org/pdf/1709.08693"
        },
        {
            "id": "61",
            "entry": "[61] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference (BMVC), 2016. URL http://arxiv.org/abs/1605.07146.",
            "url": "http://arxiv.org/abs/1605.07146",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "62",
            "entry": "[62] Guoming Zhang, Chen Yan, Xiaoyu Ji, Taimin Zhang, Tianchen Zhang, and Wenyuan Xu. Dolphinatack: Inaudible voice commands. In Conference on Computer and Communications Security (CCS), 2017. URL http://arxiv.org/abs/1708.09537. ",
            "url": "http://arxiv.org/abs/1708.09537",
            "arxiv_url": "https://arxiv.org/pdf/1708.09537"
        }
    ]
}
