{
    "filename": "7350-are-gans-created-equal-a-large-scale-study.pdf",
    "metadata": {
        "title": "Are GANs Created Equal? A Large-Scale Study",
        "author": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7350-are-gans-created-equal-a-large-scale-study.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]."
    },
    "keywords": [
        {
            "term": "empirical evidence",
            "url": "https://en.wikipedia.org/wiki/empirical_evidence"
        },
        {
            "term": "log likelihood",
            "url": "https://en.wikipedia.org/wiki/log_likelihood"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "highlights": [
        "There are several ongoing challenges in the study of Generative adversarial networks, including their convergence and generalization properties [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], and optimization stability [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We focus on two sets of evaluation metrics: (i) The Fr\u00e9chet Inception Distance, and precision, recall and F1",
        "We provide empirical evidence that Fr\u00e9chet Inception Distance is a reasonable metric due to its robustness with respect to mode dropping and encoding network choices",
        "We propose to compare distributions of the minimum achivable Fr\u00e9chet Inception Distance for a fixed computational budget",
        "Empirical evidence presented herein imply that algorithmic differences in state-of-the-art Generative adversarial networks become less relevant, as the computational budget increases",
        "Notwithstanding the limitations discussed in Section 7, this work strongly suggests that future Generative adversarial networks research should be more experimentally systematic and model comparison should be performed on neutral ground"
    ],
    "key_statements": [
        "There are several ongoing challenges in the study of Generative adversarial networks, including their convergence and generalization properties [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], and optimization stability [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "Our main contributions: (1) We provide a fair and comprehensive comparison of the state-of-the-art Generative adversarial networks, and empirically demonstrate that nearly all of them can reach similar values of Fr\u00e9chet Inception Distance, given a high enough computational budget",
        "(2) We provide strong empirical evidence2 that to compare Generative adversarial networks it is necessary to report a summary of distribution of results, rather than the best result achieved, due to the randomness of the optimization process and model instability",
        "The classic approach towards evaluating generative models is based on model likelihood which is often intractable",
        "The key drawback of the proposed approach is the assumption of the Gaussian observation model which carries over all issues of kernel density estimation in highdimensional spaces",
        "We provide a thorough empirical analysis of Fr\u00e9chet Inception Distance in Section 5",
        "Our focus is on providing a fair assessment of the current state-of-the-art Generative adversarial networks using Fr\u00e9chet Inception Distance, as well as precision and recall, and verifying the robustness of these models in a large-scale empirical evaluation.\n2Reproducing these experiments requires approximately 6.85",
        "In this work we focus on unconditional generative adversarial networks",
        "Evaluation metrics in Section 5: Fr\u00e9chet Inception Distance, which can be computed on all data sets, and precision, recall, and F1, which we can compute for the proposed tasks",
        "We explore how the results vary depending on the budget k, where k is the number of hyperparameter settings for a fixed model",
        "While we present the results which were obtained by a random search, we have investigated sequential Bayesian optimization, which resulted in comparable results",
        "We propose an approximation to precision and recall for Generative adversarial networks and how that it can be used to quantify the degree of overfitting",
        "We evaluate the bias and variance of Fr\u00e9chet Inception Distance on four data sets from the Generative adversarial networks literature",
        "We estimate the sensitivity to the choice of the encoding network by computing Fr\u00e9chet Inception Distance using the 4096 dimensional FC7 layer of the VGG network trained on ImageNet",
        "We propose a simple and effective data set for evaluating generative models",
        "We argue that it is critical to be able to increase the complexity of the task in a relatively smooth and controlled fashion",
        "FASHION MNIST, 40 on CELEBA and 100 on CIFAR. These data sets are a popular choice for generative modeling, range from simple to medium complexity, which makes it possible to run many experiments as well as getting decent results",
        "We present the sensitivity of models to the hyper-parameters in Figure 4 and the best Fr\u00e9chet Inception Distance achieved by each model in Table 2",
        "We report the narrow hyperparameter ranges in Appendix A",
        "We focus on two sets of evaluation metrics: (i) The Fr\u00e9chet Inception Distance, and precision, recall and F1",
        "We provide empirical evidence that Fr\u00e9chet Inception Distance is a reasonable metric due to its robustness with respect to mode dropping and encoding network choices",
        "We propose to compare distributions of the minimum achivable Fr\u00e9chet Inception Distance for a fixed computational budget",
        "Empirical evidence presented herein imply that algorithmic differences in state-of-the-art Generative adversarial networks become less relevant, as the computational budget increases",
        "As discussed in Section 4, many dimensions have to be taken into account for model comparison, and this work only explores a subset of the options",
        "Notwithstanding the limitations discussed in Section 7, this work strongly suggests that future Generative adversarial networks research should be more experimentally systematic and model comparison should be performed on neutral ground"
    ],
    "summary": [
        "There are several ongoing challenges in the study of GANs, including their convergence and generalization properties [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], and optimization stability [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "The authors focus on IS and consider a smaller subset of GANs. In contrast, our focus is on providing a fair assessment of the current state-of-the-art GANs using FID, as well as precision and recall, and verifying the robustness of these models in a large-scale empirical evaluation.",
        "Evaluation metrics in Section 5: FID, which can be computed on all data sets, and precision, recall, and F1, which we can compute for the proposed tasks.",
        "Even when the metric is fixed, a given algorithm can achieve very different scores, when varying the architecture, hyperparameters, random initialization, or the data set.",
        "We optimize the hyperparameters for each model and data set by performing a random search.",
        "We observe results similar to Table 1 which is expected as both training and testing data sets are sampled from the same distribution.",
        "We estimate the sensitivity to the choice of the encoding network by computing FID using the 4096 dimensional FC7 layer of the VGG network trained on ImageNet. Figure 1 shows the resulting distribution.",
        "We propose a simple and effective data set for evaluating generative models.",
        "These data sets are a popular choice for generative modeling, range from simple to medium complexity, which makes it possible to run many experiments as well as getting decent results.",
        "We consider this setting in which we allow only 50 samples from a set of narrow ranges, which were selected based on the wide hyperparameter search on the FASHION-MNIST data set.",
        "To test this hypothesis we re-train the best models from the limited hyperparameter range considered for the previous section, while changing the initial weights of the generator and discriminator networks.",
        "We perform a search over the wide range of hyperparameters and compute precision and recall by considering n = 1024 samples.",
        "In this study we use one neural network architecture which suffices to achieve good results in terms of FID on all considered data sets.",
        "From the classic machine learning point of view, a major drawback of FID is that it cannot detect overfitting to the training data set \u2013 an algorithm that outputs only the training examples would have an excellent score.",
        "In this paper we have started a discussion on how to neutrally and fairly compare GANs. We focus on two sets of evaluation metrics: (i) The Fr\u00e9chet Inception Distance, and precision, recall and F1.",
        "Notwithstanding the limitations discussed in Section 7, this work strongly suggests that future GAN research should be more experimentally systematic and model comparison should be performed on neutral ground"
    ],
    "headline": "We find that most models can reach similar scores with enough hyperparameter optimization and random restarts",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mart%C3%ADn%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mart%C3%ADn%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "2",
            "entry": "[2] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (GANs). In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Liang%2C%20Yingyu%20Ma%2C%20Tengyu%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29%202017"
        },
        {
            "id": "3",
            "entry": "[3] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Risteski%2C%20Andrej%20Zhang%2C%20Yi%20Do%20GANs%20learn%20the%20distribution%3F%20some%20theory%20and%20empirics%202018"
        },
        {
            "id": "4",
            "entry": "[4] Philip Bachman and Doina Precup. Variational generative stochastic networks with collaborative shaping. In International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bachman%2C%20Philip%20Precup%2C%20Doina%20Variational%20generative%20stochastic%20networks%20with%20collaborative%20shaping%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bachman%2C%20Philip%20Precup%2C%20Doina%20Variational%20generative%20stochastic%20networks%20with%20collaborative%20shaping%202015"
        },
        {
            "id": "5",
            "entry": "[5] David Berthelot, Tom Schumm, and Luke Metz. BEGAN: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "6",
            "entry": "[6] Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "7",
            "entry": "[7] William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018"
        },
        {
            "id": "8",
            "entry": "[8] Holly E Gerhard, Felix A Wichmann, and Matthias Bethge. How sensitive is the human visual system to the local statistics of natural images? PLoS computational biology, 9(1), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerhard%2C%20Holly%20E.%20Wichmann%2C%20Felix%20A.%20Bethge%2C%20Matthias%20How%20sensitive%20is%20the%20human%20visual%20system%20to%20the%20local%20statistics%20of%20natural%20images%3F%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gerhard%2C%20Holly%20E.%20Wichmann%2C%20Felix%20A.%20Bethge%2C%20Matthias%20How%20sensitive%20is%20the%20human%20visual%20system%20to%20the%20local%20statistics%20of%20natural%20images%3F%202013"
        },
        {
            "id": "9",
            "entry": "[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "10",
            "entry": "[10] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein gans. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20gans%202017"
        },
        {
            "id": "11",
            "entry": "[11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20Nash%20equilibrium%202017"
        },
        {
            "id": "12",
            "entry": "[12] Ferenc Husz\u00e1r. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05101"
        },
        {
            "id": "13",
            "entry": "[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "14",
            "entry": "[14] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "15",
            "entry": "[15] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of GANs. arXiv preprint arXiv:1705.07215, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07215"
        },
        {
            "id": "16",
            "entry": "[16] Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The GAN Landscape: Losses, architectures, regularization, and normalization. arXiv preprint arXiv:1807.04720, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04720"
        },
        {
            "id": "17",
            "entry": "[17] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Understanding%20deep%20image%20representations%20by%20inverting%20them%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Understanding%20deep%20image%20representations%20by%20inverting%20them%202015"
        },
        {
            "id": "18",
            "entry": "[18] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "19",
            "entry": "[19] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20Lars%20Nowozin%2C%20Sebastian%20Geiger%2C%20Andreas%20The%20numerics%20of%20GANs%202017"
        },
        {
            "id": "20",
            "entry": "[20] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "21",
            "entry": "[21] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20GANs%202017"
        },
        {
            "id": "22",
            "entry": "[22] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "23",
            "entry": "[23] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sajjadi%2C%20Mehdi%20S.M.%20Bachem%2C%20Olivier%20Lucic%2C%20Mario%20Bousquet%2C%20Olivier%20Assessing%20generative%20models%20via%20precision%20and%20recall%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sajjadi%2C%20Mehdi%20S.M.%20Bachem%2C%20Olivier%20Lucic%2C%20Mario%20Bousquet%2C%20Olivier%20Assessing%20generative%20models%20via%20precision%20and%20recall%202018"
        },
        {
            "id": "24",
            "entry": "[24] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "25",
            "entry": "[25] Lucas Theis, A\u00e4ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.01844"
        },
        {
            "id": "26",
            "entry": "[26] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017"
        },
        {
            "id": "27",
            "entry": "[27] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. International Conference on Computer Vision (ICCV), 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Han%20Xu%2C%20Tao%20Li%2C%20Hongsheng%20Zhang%2C%20Shaoting%20Xiaolei%20Huang%2C%20Xiaogang%20Wang%2C%20and%20Dimitris%20Metaxas.%20Stackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%202017"
        }
    ]
}
