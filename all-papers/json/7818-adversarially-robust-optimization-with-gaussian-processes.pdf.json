{
    "filename": "7818-adversarially-robust-optimization-with-gaussian-processes.pdf",
    "metadata": {
        "title": "Adversarially Robust Optimization with Gaussian Processes",
        "author": "Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, Volkan Cevher",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7818-adversarially-robust-optimization-with-gaussian-processes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we consider the problem of Gaussian process (GP) optimization with an added robustness requirement: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation. This problem is motivated by settings in which the underlying functions during optimization and implementation stages are different, or when one is interested in finding an entire region of good inputs rather than only a single point. We show that standard GP optimization algorithms do not exhibit the desired robustness properties, and provide a novel confidence-bound based algorithm STABLEOPT for this purpose. We rigorously establish the required number of samples for STABLEOPT to find a near-optimal point, and we complement this guarantee with an algorithm-independent lower bound. We experimentally demonstrate several potential applications of interest using real-world data sets, and we show that STABLEOPT consistently succeeds in finding a stable maximizer where several baseline methods fail."
    },
    "keywords": [
        {
            "term": "Swiss National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/Swiss_National_Science_Foundation"
        },
        {
            "term": "process optimization",
            "url": "https://en.wikipedia.org/wiki/process_optimization"
        },
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "recommender system",
            "url": "https://en.wikipedia.org/wiki/recommender_system"
        },
        {
            "term": "robust optimization",
            "url": "https://en.wikipedia.org/wiki/robust_optimization"
        },
        {
            "term": "Gaussian process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_process"
        },
        {
            "term": "bayesian optimization",
            "url": "https://en.wikipedia.org/wiki/bayesian_optimization"
        }
    ],
    "highlights": [
        "Gaussian processes (GP) provide a powerful means for sequentially optimizing a black-box function f that is costly to evaluate and for which noisy point evaluations are available",
        "The optimization is often performed via simulations, creating a mismatch between the assumed function and the true one; in hyperparameter tuning, the function is typically mismatched due to limited training data; in recommendation systems and several other applications, the underlying function is inherently time-varying, so the returned solution may become increasingly stale over time; the list goes on",
        "We address these considerations by studying the Gaussian process optimization problem with an additional requirement of adversarial robustness: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation",
        "We have introduced and studied a variant of Gaussian process optimization in which one requires stability/robustness to an adversarial perturbation",
        "We demonstrated the failures of existing algorithms, and provided a new algorithm STABLEOPT that overcomes these limitations, with rigorous guarantees",
        "We showed that our framework naturally applies to several interesting max-min optimization formulations, and we demonstrated significant improvements over some natural baselines in the experimental examples"
    ],
    "key_statements": [
        "Gaussian processes (GP) provide a powerful means for sequentially optimizing a black-box function f that is costly to evaluate and for which noisy point evaluations are available. This approach has successfully been applied to numerous applications, including robotics [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], hyperparameter tuning [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], recommender systems [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>], environmental monitoring [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>], and more",
        "The optimization is often performed via simulations, creating a mismatch between the assumed function and the true one; in hyperparameter tuning, the function is typically mismatched due to limited training data; in recommendation systems and several other applications, the underlying function is inherently time-varying, so the returned solution may become increasingly stale over time; the list goes on",
        "We address these considerations by studying the Gaussian process optimization problem with an additional requirement of adversarial robustness: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation",
        "We provide a novel theoretical analysis characterizing the number of samples required for STABLEOPT to attain a near-optimal robust solution, and we complement this with an algorithm-independent lower bound",
        "While we focused on the non-Bayesian Reproducing Kernel Hilbert Space setting, the proof can be adapted to handle the Bayesian optimization (BO) setting in which f \u21e0 Gaussian process(0, k); see Section 4 for further discussion",
        "While the above problem formulation seeks robustness within an \u270f-ball corresponding to the distance function d(\u00b7, \u00b7), our algorithm and theory apply to a variety of seemingly distinct settings",
        "Two observations are in order: (i) All the baselines are heuristic approaches for our problem, and they do not have guarantees in terms of near-optimal stability; We do not compare against other standard Gaussian process optimization methods, as their performance is comparable to that of Gaussian process-UCB; in particular, they suffer from exactly the same pitfalls described at the end of Section 2",
        "We have introduced and studied a variant of Gaussian process optimization in which one requires stability/robustness to an adversarial perturbation",
        "We demonstrated the failures of existing algorithms, and provided a new algorithm STABLEOPT that overcomes these limitations, with rigorous guarantees",
        "We showed that our framework naturally applies to several interesting max-min optimization formulations, and we demonstrated significant improvements over some natural baselines in the experimental examples",
        "Similar considerations are an ongoing topic of debate in understanding the parameter space rather than the hyperparmeter space, e.g., see [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]"
    ],
    "summary": [
        "Gaussian processes (GP) provide a powerful means for sequentially optimizing a black-box function f that is costly to evaluate and for which noisy point evaluations are available.",
        "Such as [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>], have considered robust optimization problems of the following form: For a given set of objectives {f1, .",
        "We provide a novel theoretical analysis characterizing the number of samples required for STABLEOPT to attain a near-optimal robust solution, and we complement this with an algorithm-independent lower bound.",
        "The following theorem bounds the performance of STABLEOPT under a suitable choice of the recommended point x(T ).",
        "Establishing lower bounds under general kernels and input domains is an open problem even in the non-robust setting.",
        "The following theorem focuses on a more specific setting than the upper bound: We let the input domain be [0, 1]p for some dimension p, and we focus on the SE and Mat\u00e9rn kernels.",
        "The proof is based on constructing a finite subset of \u201cdifficult\u201d functions in Fk(B) and applying lower bounding techniques from the multi-armed bandit literature, making use of several auxiliary results from the non-robust setting [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>].",
        "While the above problem formulation seeks robustness within an \u270f-ball corresponding to the distance function d(\u00b7, \u00b7), our algorithm and theory apply to a variety of seemingly distinct settings.",
        "A robust optimization formulation in this setting is to seek x that solves max min f (x, \u2713).",
        "For the given estimate \u2713 \u0304, the main steps of STABLEOPT in this setting are xt 2 arg max x2D",
        ", Gk} denote the groups that partition the input space, the robust optimization problem is given by max min f (x), (24)",
        "We consider a natural generalization of GP-UCB in which, at each time step, the sampled and reported point are both given by xt = arg max min ucbt 1(x + ).",
        "Two observations are in order: (i) All the baselines are heuristic approaches for our problem, and they do not have guarantees in terms of near-optimal stability; We do not compare against other standard GP optimization methods, as their performance is comparable to that of GP-UCB; in particular, they suffer from exactly the same pitfalls described at the end of Section 2.",
        "We consider a twist on this problem in which there is uncertainty regarding the precise target location, so one seeks a set of input parameters that is robust against a number of different potential locations.",
        "To (26), the robust optimization problem for a given user j is max gj(G), (31)",
        "Similar considerations are an ongoing topic of debate in understanding the parameter space rather than the hyperparmeter space, e.g., see [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]"
    ],
    "headline": "We show that standard Gaussian process optimization algorithms do not exhibit the desired robustness properties, and provide a novel confidence-bound based algorithm STABLEOPT for this purpose",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. Technical report, http://www.dklevine.com/archive/refs4462.pdf, 1998.",
            "url": "http://www.dklevine.com/archive/refs4462.pdf"
        },
        {
            "id": "2",
            "entry": "[2] Justin J. Beland and Prasanth B. Nair. Bayesian optimization under uncertainty. NIPS BayesOpt 2017 workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beland%2C%20Justin%20J.%20Nair%2C%20Prasanth%20B.%20Bayesian%20optimization%20under%20uncertainty%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beland%2C%20Justin%20J.%20Nair%2C%20Prasanth%20B.%20Bayesian%20optimization%20under%20uncertainty%202017"
        },
        {
            "id": "3",
            "entry": "[3] Dimitris Bertsimas, Omid Nohadani, and Kwong Meng Teo. Nonconvex robust optimization for problems with constraints. INFORMS Journal on Computing, 22(1):44\u201358, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Nohadani%2C%20Omid%20Teo%2C%20Kwong%20Meng%20Nonconvex%20robust%20optimization%20for%20problems%20with%20constraints%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsimas%2C%20Dimitris%20Nohadani%2C%20Omid%20Teo%2C%20Kwong%20Meng%20Nonconvex%20robust%20optimization%20for%20problems%20with%20constraints%202010"
        },
        {
            "id": "4",
            "entry": "[4] Dimitris Bertsimas, Omid Nohadani, and Kwong Meng Teo. Robust optimization for unconstrained simulation-based problems. Operations Research, 58(1):161\u2013178, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Nohadani%2C%20Omid%20Teo%2C%20Kwong%20Meng%20Robust%20optimization%20for%20unconstrained%20simulation-based%20problems%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsimas%2C%20Dimitris%20Nohadani%2C%20Omid%20Teo%2C%20Kwong%20Meng%20Robust%20optimization%20for%20unconstrained%20simulation-based%20problems%202010"
        },
        {
            "id": "5",
            "entry": "[5] Ilija Bogunovic, Slobodan Mitrovic, Jonathan Scarlett, and Volkan Cevher. Robust submodular maximization: A non-uniform partitioning approach. In International Conference on Machine Learning (ICML), pages 508\u2013516, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bogunovic%2C%20Ilija%20Mitrovic%2C%20Slobodan%20Scarlett%2C%20Jonathan%20Cevher%2C%20Volkan%20Robust%20submodular%20maximization%3A%20A%20non-uniform%20partitioning%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bogunovic%2C%20Ilija%20Mitrovic%2C%20Slobodan%20Scarlett%2C%20Jonathan%20Cevher%2C%20Volkan%20Robust%20submodular%20maximization%3A%20A%20non-uniform%20partitioning%20approach%202017"
        },
        {
            "id": "6",
            "entry": "[6] Ilija Bogunovic, Jonathan Scarlett, and Volkan Cevher. Time-varying Gaussian process bandit optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bogunovic%2C%20Ilija%20Scarlett%2C%20Jonathan%20Cevher%2C%20Volkan%20Time-varying%20Gaussian%20process%20bandit%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bogunovic%2C%20Ilija%20Scarlett%2C%20Jonathan%20Cevher%2C%20Volkan%20Time-varying%20Gaussian%20process%20bandit%20optimization%202016"
        },
        {
            "id": "7",
            "entry": "[7] Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher. Truncated variance reduction: A unified approach to Bayesian optimization and level-set estimation. In Advances in Neural Information Processing Systems (NIPS), pages 1507\u20131515, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bogunovic%2C%20Ilija%20Scarlett%2C%20Jonathan%20Krause%2C%20Andreas%20Cevher%2C%20Volkan%20Truncated%20variance%20reduction%3A%20A%20unified%20approach%20to%20Bayesian%20optimization%20and%20level-set%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bogunovic%2C%20Ilija%20Scarlett%2C%20Jonathan%20Krause%2C%20Andreas%20Cevher%2C%20Volkan%20Truncated%20variance%20reduction%3A%20A%20unified%20approach%20to%20Bayesian%20optimization%20and%20level-set%20estimation%202016"
        },
        {
            "id": "8",
            "entry": "[8] Ilija Bogunovic, Junyao Zhao, and Volkan Cevher. Robust maximization of non-submodular objectives. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 890\u2013899, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bogunovic%2C%20Ilija%20Zhao%2C%20Junyao%20Cevher%2C%20Volkan%20Robust%20maximization%20of%20non-submodular%20objectives%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bogunovic%2C%20Ilija%20Zhao%2C%20Junyao%20Cevher%2C%20Volkan%20Robust%20maximization%20of%20non-submodular%20objectives%202018"
        },
        {
            "id": "9",
            "entry": "[9] Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non-convex objectives. In Advances in Neural Information Processing Systems (NIPS), pages 4708\u20134717, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Robert%20S.%20Lucier%2C%20Brendan%20Singer%2C%20Yaron%20Syrgkanis%2C%20Vasilis%20Robust%20optimization%20for%20non-convex%20objectives%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Robert%20S.%20Lucier%2C%20Brendan%20Singer%2C%20Yaron%20Syrgkanis%2C%20Vasilis%20Robust%20optimization%20for%20non-convex%20objectives%202017"
        },
        {
            "id": "10",
            "entry": "[10] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International Conference on Machine Learning (ICML), pages 844\u2013853, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chowdhury%2C%20Sayak%20Ray%20Gopalan%2C%20Aditya%20On%20kernelized%20multi-armed%20bandits%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chowdhury%2C%20Sayak%20Ray%20Gopalan%2C%20Aditya%20On%20kernelized%20multi-armed%20bandits%202017"
        },
        {
            "id": "11",
            "entry": "[11] Emile Contal, David Buffoni, Alexandre Robicquet, and Nicolas Vayatis. Parallel Gaussian process optimization with upper confidence bound and pure exploration. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 225\u2013240.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Contal%2C%20Emile%20Buffoni%2C%20David%20Robicquet%2C%20Alexandre%20Vayatis%2C%20Nicolas%20Parallel%20Gaussian%20process%20optimization%20with%20upper%20confidence%20bound%20and%20pure%20exploration",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Contal%2C%20Emile%20Buffoni%2C%20David%20Robicquet%2C%20Alexandre%20Vayatis%2C%20Nicolas%20Parallel%20Gaussian%20process%20optimization%20with%20upper%20confidence%20bound%20and%20pure%20exploration"
        },
        {
            "id": "12",
            "entry": "[12] Thomas Desautels, Andreas Krause, and Joel W Burdick. Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. Journal of Machine Learning Research, 15(1):3873\u20133923, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Desautels%2C%20Thomas%20Krause%2C%20Andreas%20Burdick%2C%20Joel%20W.%20Parallelizing%20exploration-exploitation%20tradeoffs%20in%20Gaussian%20process%20bandit%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Desautels%2C%20Thomas%20Krause%2C%20Andreas%20Burdick%2C%20Joel%20W.%20Parallelizing%20exploration-exploitation%20tradeoffs%20in%20Gaussian%20process%20bandit%20optimization%202014"
        },
        {
            "id": "13",
            "entry": "[13] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017"
        },
        {
            "id": "14",
            "entry": "[14] Javier Gonz\u00e1lez, Zhenwen Dai, Philipp Hennig, and Neil Lawrence. Batch Bayesian optimization via local penalization. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 648\u2013657, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gonz%C3%A1lez%2C%20Javier%20Dai%2C%20Zhenwen%20Hennig%2C%20Philipp%20Lawrence%2C%20Neil%20Batch%20Bayesian%20optimization%20via%20local%20penalization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gonz%C3%A1lez%2C%20Javier%20Dai%2C%20Zhenwen%20Hennig%2C%20Philipp%20Lawrence%2C%20Neil%20Batch%20Bayesian%20optimization%20via%20local%20penalization%202016"
        },
        {
            "id": "15",
            "entry": "[15] Alkis Gotovos, Nathalie Casati, Gregory Hitz, and Andreas Krause. Active learning for level set estimation. In International Joint Conference on Artificial Intelligence (IJCAI), pages 1344\u20131350, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gotovos%2C%20Alkis%20Casati%2C%20Nathalie%20Hitz%2C%20Gregory%20Krause%2C%20Andreas%20Active%20learning%20for%20level%20set%20estimation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gotovos%2C%20Alkis%20Casati%2C%20Nathalie%20Hitz%2C%20Gregory%20Krause%2C%20Andreas%20Active%20learning%20for%20level%20set%20estimation%202013"
        },
        {
            "id": "16",
            "entry": "[16] Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 13(Jun):1809\u20131837, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hennig%2C%20Philipp%20Schuler%2C%20Christian%20J.%20Entropy%20search%20for%20information-efficient%20global%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hennig%2C%20Philipp%20Schuler%2C%20Christian%20J.%20Entropy%20search%20for%20information-efficient%20global%20optimization%202012"
        },
        {
            "id": "17",
            "entry": "[17] Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In Advances in Neural Information Processing Systems (NIPS), pages 918\u2013926, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Hoffman%2C%20Matthew%20W.%20Ghahramani%2C%20Zoubin%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Hoffman%2C%20Matthew%20W.%20Ghahramani%2C%20Zoubin%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014"
        },
        {
            "id": "18",
            "entry": "[18] Kirthevasan Kandasamy, Jeff Schneider, and Barnab\u00e1s P\u00f3czos. High dimensional Bayesian optimisation and bandits via additive models. In International Conference on Machine Learning (ICML), pages 295\u2013304, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kandasamy%2C%20Kirthevasan%20Schneider%2C%20Jeff%20P%C3%B3czos%2C%20Barnab%C3%A1s%20High%20dimensional%20Bayesian%20optimisation%20and%20bandits%20via%20additive%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kandasamy%2C%20Kirthevasan%20Schneider%2C%20Jeff%20P%C3%B3czos%2C%20Barnab%C3%A1s%20High%20dimensional%20Bayesian%20optimisation%20and%20bandits%20via%20additive%20models%202015"
        },
        {
            "id": "19",
            "entry": "[19] Andreas Krause, H Brendan McMahan, Carlos Guestrin, and Anupam Gupta. Robust submodular observation selection. Journal of Machine Learning Research, 9(Dec):2761\u20132801, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%20Krause%2C%20H.Brendan%20McMahan%20Guestrin%2C%20Carlos%20Gupta%2C%20Anupam%20Robust%20submodular%20observation%20selection%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%20Krause%2C%20H.Brendan%20McMahan%20Guestrin%2C%20Carlos%20Gupta%2C%20Anupam%20Robust%20submodular%20observation%20selection%202008"
        },
        {
            "id": "20",
            "entry": "[20] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems (NIPS), pages 2447\u20132455, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20Andreas%20Ong%2C%20Cheng%20S.%20Contextual%20Gaussian%20process%20bandit%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krause%2C%20Andreas%20Ong%2C%20Cheng%20S.%20Contextual%20Gaussian%20process%20bandit%20optimization%202011"
        },
        {
            "id": "21",
            "entry": "[21] Daniel J Lizotte, Tao Wang, Michael H Bowling, and Dale Schuurmans. Automatic gait optimization with Gaussian process regression. In International Joint Conference on Artificial Intelligence (IJCAI), pages 944\u2013949, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lizotte%2C%20Daniel%20J.%20Wang%2C%20Tao%20Bowling%2C%20Michael%20H.%20Schuurmans%2C%20Dale%20Automatic%20gait%20optimization%20with%20Gaussian%20process%20regression%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lizotte%2C%20Daniel%20J.%20Wang%2C%20Tao%20Bowling%2C%20Michael%20H.%20Schuurmans%2C%20Dale%20Automatic%20gait%20optimization%20with%20Gaussian%20process%20regression%202007"
        },
        {
            "id": "22",
            "entry": "[22] Ruben Martinez-Cantin, Kevin Tee, and Michael McCourt. Practical Bayesian optimization in the presence of outliers. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martinez-Cantin%2C%20Ruben%20Tee%2C%20Kevin%20McCourt%2C%20Michael%20Practical%20Bayesian%20optimization%20in%20the%20presence%20of%20outliers%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martinez-Cantin%2C%20Ruben%20Tee%2C%20Kevin%20McCourt%2C%20Michael%20Practical%20Bayesian%20optimization%20in%20the%20presence%20of%20outliers%202018"
        },
        {
            "id": "23",
            "entry": "[23] J. Nogueira, R. Martinez-Cantin, A. Bernardino, and L. Jamone. Unscented Bayesian optimization for safe robot grasping. In IEEE/RSJ Int. Conf. Intel. Robots and Systems (IROS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nogueira%2C%20J.%20Martinez-Cantin%2C%20R.%20Bernardino%2C%20A.%20Jamone%2C%20L.%20Unscented%20Bayesian%20optimization%20for%20safe%20robot%20grasping%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nogueira%2C%20J.%20Martinez-Cantin%2C%20R.%20Bernardino%2C%20A.%20Jamone%2C%20L.%20Unscented%20Bayesian%20optimization%20for%20safe%20robot%20grasping%202016"
        },
        {
            "id": "24",
            "entry": "[24] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20Carl%20Edward%20Williams%2C%20Christopher%20K.I.%20Gaussian%20processes%20for%20machine%20learning%2C%20volume%201%202006"
        },
        {
            "id": "25",
            "entry": "[25] Paul Rolland, Jonathan Scarlett, Ilija Bogunovic, and Volkan Cevher. High-dimensional Bayesian optimization via additive models with overlapping groups. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 298\u2013307, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rolland%2C%20Paul%20Scarlett%2C%20Jonathan%20Bogunovic%2C%20Ilija%20Cevher%2C%20Volkan%20High-dimensional%20Bayesian%20optimization%20via%20additive%20models%20with%20overlapping%20groups%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rolland%2C%20Paul%20Scarlett%2C%20Jonathan%20Bogunovic%2C%20Ilija%20Cevher%2C%20Volkan%20High-dimensional%20Bayesian%20optimization%20via%20additive%20models%20with%20overlapping%20groups%202018"
        },
        {
            "id": "26",
            "entry": "[26] Binxin Ru, Michael Osborne, and Mark McLeod. Fast information-theoretic Bayesian optimisation. arXiv preprint arXiv:1711.00673, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00673"
        },
        {
            "id": "27",
            "entry": "[27] Jonathan Scarlett, Ilijia Bogunovic, and Volkan Cevher. Lower bounds on regret for noisy Gaussian process bandit optimization. In Conference on Learning Theory (COLT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarlett%2C%20Jonathan%20Bogunovic%2C%20Ilijia%20Cevher%2C%20Volkan%20Lower%20bounds%20on%20regret%20for%20noisy%20Gaussian%20process%20bandit%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarlett%2C%20Jonathan%20Bogunovic%2C%20Ilijia%20Cevher%2C%20Volkan%20Lower%20bounds%20on%20regret%20for%20noisy%20Gaussian%20process%20bandit%20optimization%202017"
        },
        {
            "id": "28",
            "entry": "[28] Shubhanshu Shekhar and Tara Javidi. Gaussian process bandits with adaptive discretization. arXiv preprint arXiv:1712.01447, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01447"
        },
        {
            "id": "29",
            "entry": "[29] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20Aman%20Namkoong%2C%20Hongseok%20Duchi%2C%20John%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%202018"
        },
        {
            "id": "30",
            "entry": "[30] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural information Processing Systems (NIPS), pages 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "31",
            "entry": "[31] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In International Conference on Machine Learning (ICML), pages 1015\u20131022, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srinivas%2C%20Niranjan%20Krause%2C%20Andreas%20Kakade%2C%20Sham%20M.%20Seeger%2C%20Matthias%20Gaussian%20process%20optimization%20in%20the%20bandit%20setting%3A%20No%20regret%20and%20experimental%20design%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srinivas%2C%20Niranjan%20Krause%2C%20Andreas%20Kakade%2C%20Sham%20M.%20Seeger%2C%20Matthias%20Gaussian%20process%20optimization%20in%20the%20bandit%20setting%3A%20No%20regret%20and%20experimental%20design%202010"
        },
        {
            "id": "32",
            "entry": "[32] Matthew Staib, Bryan Wilder, and Stefanie Jegelka. Distributionally robust submodular maximization. arXiv preprint arXiv:1802.05249, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05249"
        },
        {
            "id": "33",
            "entry": "[33] Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization with Gaussian processes. In International Conference on Machine Learning (ICML), pages 997\u20131005, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sui%2C%20Yanan%20Gotovos%2C%20Alkis%20Burdick%2C%20Joel%20Krause%2C%20Andreas%20Safe%20exploration%20for%20optimization%20with%20Gaussian%20processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sui%2C%20Yanan%20Gotovos%2C%20Alkis%20Burdick%2C%20Joel%20Krause%2C%20Andreas%20Safe%20exploration%20for%20optimization%20with%20Gaussian%20processes%202015"
        },
        {
            "id": "34",
            "entry": "[34] Hastagiri P Vanchinathan, Isidor Nikolic, Fabio De Bona, and Andreas Krause. Exploreexploit in top-n recommender systems via Gaussian processes. In Proceedings of the 8th ACM Conference on Recommender systems, pages 225\u2013232. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vanchinathan%2C%20Hastagiri%20P.%20Nikolic%2C%20Isidor%20Bona%2C%20Fabio%20De%20Krause%2C%20Andreas%20Exploreexploit%20in%20top-n%20recommender%20systems%20via%20Gaussian%20processes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vanchinathan%2C%20Hastagiri%20P.%20Nikolic%2C%20Isidor%20Bona%2C%20Fabio%20De%20Krause%2C%20Andreas%20Exploreexploit%20in%20top-n%20recommender%20systems%20via%20Gaussian%20processes%202014"
        },
        {
            "id": "35",
            "entry": "[35] Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization. In International Conference on Machine Learning (ICML), pages 3627\u20133635, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zi%20Jegelka%2C%20Stefanie%20Max-value%20entropy%20search%20for%20efficient%20Bayesian%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zi%20Jegelka%2C%20Stefanie%20Max-value%20entropy%20search%20for%20efficient%20Bayesian%20optimization%202017"
        },
        {
            "id": "36",
            "entry": "[36] Zi Wang, Chengtao Li, Stefanie Jegelka, and Pushmeet Kohli. Batched high-dimensional Bayesian optimization via structural kernel learning. In International Conference on Machine Learning (ICML), pages 3656\u20133664, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Zi%20Li%2C%20Chengtao%20Jegelka%2C%20Stefanie%20Kohli%2C%20Pushmeet%20Batched%20high-dimensional%20Bayesian%20optimization%20via%20structural%20kernel%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Zi%20Li%2C%20Chengtao%20Jegelka%2C%20Stefanie%20Kohli%2C%20Pushmeet%20Batched%20high-dimensional%20Bayesian%20optimization%20via%20structural%20kernel%20learning%202017"
        },
        {
            "id": "37",
            "entry": "[37] Bryan Wilder. Equilibrium computation for zero sum games with submodular structure. In Conference on Artificial Intelligence (AAAI), 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilder%2C%20Bryan%20Equilibrium%20computation%20for%20zero%20sum%20games%20with%20submodular%20structure%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilder%2C%20Bryan%20Equilibrium%20computation%20for%20zero%20sum%20games%20with%20submodular%20structure%202017"
        }
    ]
}
