{
    "filename": "8285-semidefinite-relaxations-for-certifying-robustness-to-adversarial-examples.pdf",
    "metadata": {
        "title": "Semidefinite relaxations for certifying robustness to adversarial examples",
        "author": "Aditi Raghunathan, Jacob Steinhardt, Percy S. Liang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8285-semidefinite-relaxations-for-certifying-robustness-to-adversarial-examples.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Despite their impressive performance on diverse tasks, neural networks fail catastrophically in the presence of adversarial inputs\u2014imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different foreign networks whose training objectives are agnostic to our proposed relaxation."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "linear program",
            "url": "https://en.wikipedia.org/wiki/linear_program"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "quadratically constrained quadratic program",
            "url": "https://en.wikipedia.org/wiki/quadratically_constrained_quadratic_program"
        }
    ],
    "highlights": [
        "Many state-of-the-art classifiers have been shown to fail catastrophically in the presence of small imperceptible but adversarial perturbations",
        "Since the discovery of such adversarial examples [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], numerous defenses have been proposed in attempt to build classifiers that are robust to adversarial examples",
        "We propose a new convex relaxation based on semidefinite programming (SDP) that is significantly tighter than previous relaxations based on linear programming (LP) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] and handles arbitrary number of layers",
        "We present the linear program relaxation for a neural network with one hidden layer, where the hidden layer activations z \u2208 Rm are related to the input x \u2208 Rd as z = ReLU(W x)",
        "We focused on fully connected feedforward networks for computational efficiency",
        "We are hopeful that we can scale up our semidefinite program to answer this question, perhaps borrowing ideas from work on highly scalable semidefinite program [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and explicitly exploiting the sparsity and structure induced by the convolutional neural networks architecture"
    ],
    "key_statements": [
        "Many state-of-the-art classifiers have been shown to fail catastrophically in the presence of small imperceptible but adversarial perturbations",
        "Since the discovery of such adversarial examples [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], numerous defenses have been proposed in attempt to build classifiers that are robust to adversarial examples",
        "We propose a new convex relaxation based on semidefinite programming (SDP) that is significantly tighter than previous relaxations based on linear programming (LP) [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] and handles arbitrary number of layers",
        "The above semidefinite program is a relaxation of the quadratically constrained quadratic program in 3 with fSDP \u2265 fQCQP, providing an upper bound on y(x,y) that could serve as a certificate of robustness",
        "We note that this semidefinite program relaxation is different from the one proposed in [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], which applies only to neural networks with one hidden layer",
        "We present the linear program relaxation for a neural network with one hidden layer, where the hidden layer activations z \u2208 Rm are related to the input x \u2208 Rd as z = ReLU(W x)",
        "To provide a comparison, we compute a lower bound on the adversarial error\u2014the error obtained by the Projected Gradient Descent attack",
        "Let xPGD be the adversarial example generated by the Projected Gradient Descent attack on clean input xwith true label y",
        "We focused on fully connected feedforward networks for computational efficiency",
        "We are hopeful that we can scale up our semidefinite program to answer this question, perhaps borrowing ideas from work on highly scalable semidefinite program [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and explicitly exploiting the sparsity and structure induced by the convolutional neural networks architecture",
        "Current work on certification of neural networks against adversarial examples has focused on perturbations bounded in some norm ball"
    ],
    "summary": [
        "Many state-of-the-art classifiers have been shown to fail catastrophically in the presence of small imperceptible but adversarial perturbations.",
        "A network f with L hidden layers is defined as follows: let x0 \u2208 Rd denote the input and x1, ...",
        "We first show how this QCQP can be relaxed to a semidefinite program (SDP) for networks with one hidden layer.",
        "The above SDP is a relaxation of the QCQP in 3 with fSDP \u2265 fQCQP, providing an upper bound on y(x,y) that could serve as a certificate of robustness.",
        "We note that this SDP relaxation is different from the one proposed in [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], which applies only to neural networks with one hidden layer.",
        "We present the LP relaxation for a neural network with one hidden layer, where the hidden layer activations z \u2208 Rm are related to the input x \u2208 Rd as z = ReLU(W x).",
        "L\u2264x\u2264u for the neural network is written by constructing the convex envelopes for each ReLU unit and optimizing over this set as follows: fLP = max c z s.t z \u2265 0, z \u2265 W x, (6) (Lower bound lines)",
        "The SDP relaxation to evaluate robustness for multi-layer networks is a straightforward generalization of the relaxation presented for one hidden layer in Section 3.1.",
        "For the multi-layer case, since the activations at layer i \u2212 1 act as input to the layer i, adding constraints that restrict P [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] will lead to a tighter relaxation for the overall objective.",
        "We use the two-layer network with 500 hidden nodes from [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], obtained by using an SDP-based bound on the gradient of the network as a regularizer.",
        "We use a two-layer network with 500 hidden nodes trained via the LP-based robust training procedure of [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "This uses the upper bound based on the LP relaxation discussed in Section 4.2 which forms the basis for several existing works on scalable certification [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "To provide a comparison, we compute a lower bound on the adversarial error\u2014the error obtained by the PGD attack.",
        "SDP-cert provides non-vacuous certificates for all networks considered.",
        "Let xPGD be the adversarial example generated by the PGD attack on clean input xwith true label y.",
        "As discussed in Section 5, we could consider a version of the SDP that does not include the constraints relating linear and quadratic terms at the intermediate layers of the network.",
        "Current work on certification of neural networks against adversarial examples has focused on perturbations bounded in some norm ball."
    ],
    "headline": "We propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS optimization: more tractable alternatives to sum of squares and semidefinite optimization. arXiv preprint arXiv:1706.02586, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02586"
        },
        {
            "id": "2",
            "entry": "[2] A. Athalye and I. Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "3",
            "entry": "[3] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "4",
            "entry": "[4] T. B. Brown, D. Man\u00e9, A. Roy, M. Abadi, and J. Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09665"
        },
        {
            "id": "5",
            "entry": "[5] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy, pages 39\u201357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "6",
            "entry": "[6] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In USENIX Security, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Mishra%2C%20P.%20Vaidya%2C%20T.%20Zhang%2C%20Y.%20Hidden%20voice%20commands%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Mishra%2C%20P.%20Vaidya%2C%20T.%20Zhang%2C%20Y.%20Hidden%20voice%20commands%202016"
        },
        {
            "id": "7",
            "entry": "[7] N. Carlini, G. Katz, C. Barrett, and D. L. Dill. Ground-truth adversarial examples. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Katz%2C%20G.%20Barrett%2C%20C.%20Dill%2C%20D.L.%20Ground-truth%20adversarial%20examples%202017"
        },
        {
            "id": "8",
            "entry": "[8] K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O\u2019Donoghue, J. Uesato, and P. Kohli. Training verified learners with learned verifiers. arXiv preprint arXiv:1805.10265, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10265"
        },
        {
            "id": "9",
            "entry": "[9] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli. A dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06567"
        },
        {
            "id": "10",
            "entry": "[10] R. Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis (ATVA), pages 269\u2013286, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ehlers%2C%20R.%20Formal%20verification%20of%20piece-wise%20linear%20feed-forward%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ehlers%2C%20R.%20Formal%20verification%20of%20piece-wise%20linear%20feed-forward%20neural%20networks%202017"
        },
        {
            "id": "11",
            "entry": "[11] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song. Robust physical-world attacks on machine learning models. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evtimov%2C%20I.%20Eykholt%2C%20K.%20Fernandes%2C%20E.%20Kohno%2C%20T.%20Robust%20physical-world%20attacks%20on%20machine%20learning%20models%202017"
        },
        {
            "id": "12",
            "entry": "[12] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "13",
            "entry": "[13] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems (NIPS), pages 2263\u20132273, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20M.%20Andriushchenko%2C%20M.%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20M.%20Andriushchenko%2C%20M.%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "14",
            "entry": "[14] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel. Adversarial attacks on neural network policies. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20S.%20Papernot%2C%20N.%20Goodfellow%2C%20I.%20Duan%2C%20Y.%20Adversarial%20attacks%20on%20neural%20network%20policies%202017"
        },
        {
            "id": "15",
            "entry": "[15] R. Jia and P. Liang. Adversarial examples for evaluating reading comprehension systems. In Empirical Methods in Natural Language Processing (EMNLP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20R.%20Liang%2C%20P.%20Adversarial%20examples%20for%20evaluating%20reading%20comprehension%20systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20R.%20Liang%2C%20P.%20Adversarial%20examples%20for%20evaluating%20reading%20comprehension%20systems%202017"
        },
        {
            "id": "16",
            "entry": "[16] G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. arXiv preprint arXiv:1702.01135, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.01135"
        },
        {
            "id": "17",
            "entry": "[17] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Towards proving the adversarial robustness of deep neural networks. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katz%2C%20G.%20Barrett%2C%20C.%20Dill%2C%20D.L.%20Julian%2C%20K.%20Towards%20proving%20the%20adversarial%20robustness%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "18",
            "entry": "[18] J. Z. Kolter and E. Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope (published at ICML 2018). arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "19",
            "entry": "[19] J. L\u00f6fberg. YALMIP: A toolbox for modeling and optimization in MATLAB. In CACSD, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L%C3%B6fberg%2C%20J.%20YALMIP%3A%20A%20toolbox%20for%20modeling%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L%C3%B6fberg%2C%20J.%20YALMIP%3A%20A%20toolbox%20for%20modeling%202004"
        },
        {
            "id": "20",
            "entry": "[20] J. Lu, H. Sibai, E. Fabry, and D. Forsyth. No need to worry about adversarial examples in object detection in autonomous vehicles. arXiv preprint arXiv:1707.03501, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03501"
        },
        {
            "id": "21",
            "entry": "[21] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "22",
            "entry": "[22] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy, pages 582\u2013597, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Wu%2C%20X.%20Jha%2C%20S.%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Wu%2C%20X.%20Jha%2C%20S.%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "23",
            "entry": "[23] A. Raghunathan, J. Steinhardt, and P. Liang. Certified defenses against adversarial examples. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghunathan%2C%20A.%20Steinhardt%2C%20J.%20Liang%2C%20P.%20Certified%20defenses%20against%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghunathan%2C%20A.%20Steinhardt%2C%20J.%20Liang%2C%20P.%20Certified%20defenses%20against%20adversarial%20examples%202018"
        },
        {
            "id": "24",
            "entry": "[24] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In ACM SIGSAC Conference on Computer and Communications Security, pages 1528\u20131540, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20M.%20Bhagavatula%2C%20S.%20Bauer%2C%20L.%20Reiter%2C%20M.K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20M.%20Bhagavatula%2C%20S.%20Bauer%2C%20L.%20Reiter%2C%20M.K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "25",
            "entry": "[25] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Zaremba%2C%20W.%20Sutskever%2C%20I.%20Bruna%2C%20J.%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Zaremba%2C%20W.%20Sutskever%2C%20I.%20Bruna%2C%20J.%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "26",
            "entry": "[26] V. Tjeng and R. Tedrake. Verifying neural networks with mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07356"
        },
        {
            "id": "27",
            "entry": "[27] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20R.%20Introduction%20to%20the%20non-asymptotic%20analysis%20of%20random%20matrices%202010"
        },
        {
            "id": "28",
            "entry": "[28] T. Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel. Towards fast computation of certified robustness for relu networks. arXiv preprint arXiv:1804.09699, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09699"
        },
        {
            "id": "29",
            "entry": "[29] E. Wong and J. Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wong%2C%20E.%20Kolter%2C%20J.Z.%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wong%2C%20E.%20Kolter%2C%20J.Z.%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%202018"
        },
        {
            "id": "30",
            "entry": "[30] E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses. arXiv preprint arXiv:1805.12514, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1805.12514"
        }
    ]
}
