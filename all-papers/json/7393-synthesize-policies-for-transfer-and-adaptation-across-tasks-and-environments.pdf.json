{
    "filename": "7393-synthesize-policies-for-transfer-and-adaptation-across-tasks-and-environments.pdf",
    "metadata": {
        "title": "Synthesize Policies for Transfer and Adaptation across Tasks and Environments",
        "author": "Hexiang Hu, Liyu Chen, Boqing Gong, Fei Sha",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7393-synthesize-policies-for-transfer-and-adaptation-across-tasks-and-environments.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments (\u03b5) and tasks (\u03c4 ), probably more importantly, by learning from only sparse (\u03b5, \u03c4 ) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from environment and task embeddings. Notably, one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GRIDWORLD and THOR, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (\u03b5, \u03c4 ) pairs after learning from only 40% of them."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "highlights": [
        "Remarkable progress has been made in reinforcement learning in the last few years [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "While deep reinforcement learning algorithms are capable of memorizing and entangling representations of tasks and environments [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], we propose a disentanglement objective such that the embeddings for the tasks and the environments can be extracted to maximize the efficacy of the synthesized policy",
        "Does reinforcement learning help transfer? Beyond imitation learning, we further study our Synthesizing Policies for reinforcement learning (RL) under the same transfer learning setting",
        "The results on the THOR simulator are shown in Table 3, where we report our approach as well as the top performing ones on GRIDWORLD",
        "We evaluate our approach for the challenging transfer scenarios in two simulators, GRIDWORLD and THOR, respectfully",
        "Empirical results verify that our method generalizes better across environments and tasks than several competing baselines"
    ],
    "key_statements": [
        "Remarkable progress has been made in reinforcement learning in the last few years [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "Being able to swiftly adapt to new environments and transfer skills to new tasks is crucial for the agents to act in real-world settings",
        "While deep reinforcement learning algorithms are capable of memorizing and entangling representations of tasks and environments [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], we propose a disentanglement objective such that the embeddings for the tasks and the environments can be extracted to maximize the efficacy of the synthesized policy",
        "We study how an agent learns to accomplish a variety of tasks in different environments",
        "We argue that a good embedding ought to be able to tell from which environment or task the trajectory is from",
        "We study its effectiveness on imitation learning and reinforcement learning",
        "We evaluate an agent\u2019s performance by the averaged success rate (AvgSR.) for accomplishing the tasks, limiting the maximum trajectory length to 300 steps",
        "There, the agent acts upon a new pair of environment and task, both of which it has encountered during training but not in the same (\u03b5, \u03c4 ) pair",
        "Does reinforcement learning help transfer? Beyond imitation learning, we further study our Synthesizing Policies for reinforcement learning (RL) under the same transfer learning setting",
        "We find that reinforcement learning fine-tuning improves the transfer performance for all the three algorithms",
        "The results on the THOR simulator are shown in Table 3, where we report our approach as well as the top performing ones on GRIDWORLD",
        "We evaluate our approach for the challenging transfer scenarios in two simulators, GRIDWORLD and THOR, respectfully",
        "Empirical results verify that our method generalizes better across environments and tasks than several competing baselines"
    ],
    "summary": [
        "Remarkable progress has been made in reinforcement learning in the last few years [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "Our main idea is to learn a meta rule to synthesize policies whenever the agent encounters new environments or tasks.",
        "On the training data from seen pairs of environments and tasks, our algorithm learns the embeddings as well as the policy basis.",
        "For new environments or tasks, the agent learns the corresponding embeddings only while it holds the policy basis fixed.",
        "While the most basic approach is to learn an optimal policy under each pair (\u03b5, \u03c4 ) of environment and task, we are interested in generalizing to all combinations in (E, T ), with interactive learning from a limited subset of (\u03b5, \u03c4 )",
        "Our main idea is to synthesize policies for the unseen pairs of environments and tasks.",
        "Our agent learns two sets of embeddings: one for the environments and the other for the tasks.",
        "Other model parameters include the basis \u0398, the embeddings e\u03b5 and e\u03c4 in the look-up tables respectively for the environments and the tasks, and the coefficient functions \u03b1k(\u00b7, \u00b7) and \u03b2k(\u00b7, \u00b7) for respectively synthesizing the policy and reward.",
        "As the defining coefficients \u03b1k are parameterized by a neural network whose inputs and parameters are both optimized, we need to impose additional structures such that the learned embeddings facilitate the transfer across environments or tasks.",
        "When there is a limited budget, our approach enables the agent to learn from 40% of all possible (\u03b5, \u03c4 ) pairs and yet generalize well across the tasks and environments.",
        "We investigate how effectively one can schedule transfer from seen environments and tasks to unseen ones, i.e., the settings 2 and 3 described in Section 1 and Figure 1(b) and (c).",
        "One is to transfer to pairs which cross the seen set P and unseen set Q \u2013 this corresponds to the setting 2 as the embeddings for either the unseen tasks or the unseen environments need to be learnt, but not both.",
        "The agent learns policies via learning embeddings for the tasks and environments of the unseen set Q and composing, as described in section 3.5.",
        "We consider the problem of learning to simultaneously transfer across both environments and tasks under the reinforcement learning framework and, more importantly, by learning from only sparse (\u03b5, \u03c4 ) pairs out of all the possible combinations.",
        "We present a novel approach that learns to synthesize policies from the disentangled embeddings of environments and tasks.",
        "Empirical results verify that our method generalizes better across environments and tasks than several competing baselines"
    ],
    "headline": "We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from environment and task embeddings",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy sketches. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20J.%20Klein%2C%20D.%20Levine%2C%20S.%20Modular%20multitask%20reinforcement%20learning%20with%20policy%20sketches%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20J.%20Klein%2C%20D.%20Levine%2C%20S.%20Modular%20multitask%20reinforcement%20learning%20with%20policy%20sketches%202017"
        },
        {
            "id": "2",
            "entry": "[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39\u201348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20J.%20Rohrbach%2C%20M.%20Darrell%2C%20T.%20Klein%2C%20D.%20Neural%20module%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20J.%20Rohrbach%2C%20M.%20Darrell%2C%20T.%20Klein%2C%20D.%20Neural%20module%20networks%202016"
        },
        {
            "id": "3",
            "entry": "[3] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, D. Silver, and H. P. van Hasselt. Successor features for transfer in reinforcement learning. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barreto%2C%20A.%20Dabney%2C%20W.%20Munos%2C%20R.%20Hunt%2C%20J.J.%20Successor%20features%20for%20transfer%20in%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barreto%2C%20A.%20Dabney%2C%20W.%20Munos%2C%20R.%20Hunt%2C%20J.J.%20Successor%20features%20for%20transfer%20in%20reinforcement%20learning%202017"
        },
        {
            "id": "4",
            "entry": "[4] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthesized classifiers for zero-shot learning. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5327\u20135336, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Changpinyo%20WL%20Chao%20B%20Gong%20and%20F%20Sha%20Synthesized%20classifiers%20for%20zeroshot%20learning%202016%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%20pages%2053275336%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%20Changpinyo%20WL%20Chao%20B%20Gong%20and%20F%20Sha%20Synthesized%20classifiers%20for%20zeroshot%20learning%202016%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20CVPR%20pages%2053275336%202016"
        },
        {
            "id": "5",
            "entry": "[5] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empirical study and analysis of generalized zero-shot learning for object recognition in the wild. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chao%2C%20W.-L.%20Changpinyo%2C%20S.%20Gong%2C%20B.%20Sha%2C%20F.%20An%20empirical%20study%20and%20analysis%20of%20generalized%20zero-shot%20learning%20for%20object%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chao%2C%20W.-L.%20Changpinyo%2C%20S.%20Gong%2C%20B.%20Sha%2C%20F.%20An%20empirical%20study%20and%20analysis%20of%20generalized%20zero-shot%20learning%20for%20object%20recognition%202016"
        },
        {
            "id": "6",
            "entry": "[6] P. Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5:613\u2013624, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20P.%20Improving%20generalization%20for%20temporal%20difference%20learning%3A%20The%20successor%20representation%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20P.%20Improving%20generalization%20for%20temporal%20difference%20learning%3A%20The%20successor%20representation%201993"
        },
        {
            "id": "7",
            "entry": "[7] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 2169\u20132176. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devin%2C%20C.%20Gupta%2C%20A.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devin%2C%20C.%20Gupta%2C%20A.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20Learning%20modular%20neural%20network%20policies%20for%20multi-task%20and%20multi-robot%20transfer%202017"
        },
        {
            "id": "8",
            "entry": "[8] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2066\u20132073, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20B.%20Shi%2C%20Y.%20Sha%2C%20F.%20Grauman%2C%20K.%20Geodesic%20flow%20kernel%20for%20unsupervised%20domain%20adaptation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20B.%20Shi%2C%20Y.%20Sha%2C%20F.%20Grauman%2C%20K.%20Geodesic%20flow%20kernel%20for%20unsupervised%20domain%20adaptation%202012"
        },
        {
            "id": "9",
            "entry": "[9] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "10",
            "entry": "[10] J. Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pages 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "11",
            "entry": "[11] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02284"
        },
        {
            "id": "12",
            "entry": "[12] M. Jaderberg, V. Mnih, W. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. CoRR, abs/1611.05397, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05397"
        },
        {
            "id": "13",
            "entry": "[13] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. Ai2-thor: An interactive 3d environment for visual ai. CoRR, abs/1712.05474, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05474"
        },
        {
            "id": "14",
            "entry": "[14] T. D. Kulkarni, A. Saeedi, S. Gautam, and S. Gershman. Deep successor reinforcement learning. CoRR, abs/1606.02396, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.02396"
        },
        {
            "id": "15",
            "entry": "[15] I. Misra, A. Gupta, and M. Hebert. From red wine to red tomato: Composition with context. CVPR 2017, pages 1160\u20131169, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20I.%20Gupta%2C%20A.%20Hebert%2C%20M.%20From%20red%20wine%20to%20red%20tomato%3A%20Composition%20with%20context%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20I.%20Gupta%2C%20A.%20Hebert%2C%20M.%20From%20red%20wine%20to%20red%20tomato%3A%20Composition%20with%20context%202017"
        },
        {
            "id": "16",
            "entry": "[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "17",
            "entry": "[17] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. arXiv preprint arXiv:1706.05064, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05064"
        },
        {
            "id": "18",
            "entry": "[18] E. Parisotto, J. L. Ba, and R. Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06342"
        },
        {
            "id": "19",
            "entry": "[19] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=T%20Schaul%20D%20Horgan%20K%20Gregor%20and%20D%20Silver%20Universal%20value%20function%20approximators%20In%20ICML%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=T%20Schaul%20D%20Horgan%20K%20Gregor%20and%20D%20Silver%20Universal%20value%20function%20approximators%20In%20ICML%202015"
        },
        {
            "id": "20",
            "entry": "[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "21",
            "entry": "[21] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01815"
        },
        {
            "id": "22",
            "entry": "[22] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 16:285\u2013286, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "23",
            "entry": "[23] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013211, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Precup%2C%20D.%20Singh%2C%20S.%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Precup%2C%20D.%20Singh%2C%20S.%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999"
        },
        {
            "id": "24",
            "entry": "[24] M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10:1633\u20131685, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20M.E.%20Stone%2C%20P.%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20M.E.%20Stone%2C%20P.%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009"
        },
        {
            "id": "25",
            "entry": "[25] Y. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems, pages 4499\u20134509, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Y.%20Bapst%2C%20V.%20Czarnecki%2C%20W.M.%20Quan%2C%20J.%20Distral%3A%20Robust%20multitask%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Y.%20Bapst%2C%20V.%20Czarnecki%2C%20W.M.%20Quan%2C%20J.%20Distral%3A%20Robust%20multitask%20reinforcement%20learning%202017"
        },
        {
            "id": "26",
            "entry": "[26] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani, H. K\u00fcttler, J. Agapiou, J. Schrittwieser, et al. Starcraft ii: a new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04782"
        },
        {
            "id": "27",
            "entry": "[27] A. Wilson, A. Fern, S. Ray, and P. Tadepalli. Multi-task reinforcement learning: a hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine learning, pages 1015\u20131022. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.%20Fern%2C%20A.%20Ray%2C%20S.%20Tadepalli%2C%20P.%20Multi-task%20reinforcement%20learning%3A%20a%20hierarchical%20bayesian%20approach%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.%20Fern%2C%20A.%20Ray%2C%20S.%20Tadepalli%2C%20P.%20Multi-task%20reinforcement%20learning%3A%20a%20hierarchical%20bayesian%20approach%202007"
        },
        {
            "id": "28",
            "entry": "[28] C. Zhang, O. Vinyals, R. Munos, and S. Bengio. A study on overfitting in deep reinforcement learning. arXiv preprint arXiv:1804.06893, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06893"
        },
        {
            "id": "29",
            "entry": "[29] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi, and A. Farhadi. Visual semantic planning using deep successor representations. In Proceedings of the IEEE International Conference on Computer Vision, volume 2, page 7, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Y.%20Gordon%2C%20D.%20Kolve%2C%20E.%20Fox%2C%20D.%20Visual%20semantic%20planning%20using%20deep%20successor%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Y.%20Gordon%2C%20D.%20Kolve%2C%20E.%20Fox%2C%20D.%20Visual%20semantic%20planning%20using%20deep%20successor%20representations%202017"
        }
    ]
}
