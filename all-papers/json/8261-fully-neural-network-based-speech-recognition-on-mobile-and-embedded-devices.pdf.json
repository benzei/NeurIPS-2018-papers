{
    "filename": "8261-fully-neural-network-based-speech-recognition-on-mobile-and-embedded-devices.pdf",
    "metadata": {
        "title": "Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices",
        "author": "Jinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin, Wonyong Sung",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8261-fully-neural-network-based-speech-recognition-on-mobile-and-embedded-devices.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Real-time automatic speech recognition (ASR) on mobile and embedded devices has been of great interests for many years. We present real-time speech recognition on smartphones or embedded systems by employing recurrent neural network (RNN) based acoustic models, RNN based language models, and beam-search decoding. The acoustic model is end-to-end trained with connectionist temporal classification (CTC) loss. The RNN implementation on embedded devices can suffer from excessive DRAM accesses because the parameter size of a neural network usually exceeds that of the cache memory and the parameters are used only once for each time step. To remedy this problem, we employ a multi-time step parallelization approach that computes multiple output samples at a time with the parameters fetched from the DRAM. Since the number of DRAM accesses can be reduced in proportion to the number of parallelization steps, we can achieve a high processing speed. However, conventional RNNs, such as long short-term memory (LSTM) or gated recurrent unit (GRU), do not permit multi-time step parallelization. We construct an acoustic model by combining simple recurrent units (SRUs) and depth-wise 1-dimensional convolution layers for multi-time step parallelization. Both the character and word piece models are developed for acoustic modeling, and the corresponding RNN based language models are used for beam search decoding. We achieve a competitive WER for WSJ corpus using the entire model size of around 15MB and achieve real-time speed using only a single core ARM without GPU or special hardware."
    },
    "keywords": [
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "internet of things",
            "url": "https://en.wikipedia.org/wiki/internet_of_things"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "automatic speech recognition",
            "url": "https://en.wikipedia.org/wiki/automatic_speech_recognition"
        },
        {
            "term": "hidden Markov model",
            "url": "https://en.wikipedia.org/wiki/hidden_Markov_model"
        },
        {
            "term": "real time",
            "url": "https://en.wikipedia.org/wiki/real_time"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "acoustic model",
            "url": "https://en.wikipedia.org/wiki/acoustic_model"
        },
        {
            "term": "DRAM",
            "url": "https://en.wikipedia.org/wiki/DRAM"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "long short term memory",
            "url": "https://en.wikipedia.org/wiki/long_short_term_memory"
        },
        {
            "term": "long short-term memory",
            "url": "https://en.wikipedia.org/wiki/long_short-term_memory"
        },
        {
            "term": "Convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/Convolutional_neural_networks"
        },
        {
            "term": "Wall Street Journal",
            "url": "https://en.wikipedia.org/wiki/Wall_Street_Journal"
        },
        {
            "term": "gated recurrent unit",
            "url": "https://en.wikipedia.org/wiki/gated_recurrent_unit"
        },
        {
            "term": "language models",
            "url": "https://en.wikipedia.org/wiki/language_models"
        },
        {
            "term": "connectionist temporal classification",
            "url": "https://en.wikipedia.org/wiki/connectionist_temporal_classification"
        },
        {
            "term": "word error rate",
            "url": "https://en.wikipedia.org/wiki/word_error_rate"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "single instruction multiple data",
            "url": "https://en.wikipedia.org/wiki/Single_Instruction_Multiple_Data"
        }
    ],
    "highlights": [
        "Neural network technology has greatly improved the accuracy of automatic speech recognition (ASR), and many applications are being developed for smartphones and intelligent personal assistants",
        "We reduce the overhead of DRAM accesses by executing multiple streams of recurrent neural networks language models at a time, where the stream size depends on the beam search width",
        "We develop two automatic speech recognition models; one employs character-based acoustic model operating at 20 msec frame rate, and the other uses the word piece based acoustic model that operates at the frame rate of 40 msec",
        "The character based model shows very high accuracy on Wall Street Journal corpus when combined with the hierarchical character language model",
        "This study can be applied to all single stream or small batch-size implementation of automatic speech recognition regardless of the platform, such as GPU or special-purpose hardware"
    ],
    "key_statements": [
        "Neural network technology has greatly improved the accuracy of automatic speech recognition (ASR), and many applications are being developed for smartphones and intelligent personal assistants",
        "The recurrent neural networks language models are based on long short-term memory or gated recurrent unit because multiple streams are executed concurrently for beam search decoding",
        "We reduce the overhead of DRAM accesses by executing multiple streams of recurrent neural networks language models at a time, where the stream size depends on the beam search width",
        "The target speech recognition system in this work consists of connectionist temporal classification-trained acoustic model, recurrent neural networks language models, and beam search decoder",
        "We present the acoustic model training results on character and word piece models",
        "The decoding is conducted with recurrent neural networks language models",
        "The word piece language models consists of two layers of 512-dimensional long short-term memory, which has the same structure as that of recurrent neural networks language models in Section 4.1",
        "We develop two automatic speech recognition models; one employs character-based acoustic model operating at 20 msec frame rate, and the other uses the word piece based acoustic model that operates at the frame rate of 40 msec",
        "The character based model shows very high accuracy on Wall Street Journal corpus when combined with the hierarchical character language model",
        "This study can be applied to all single stream or small batch-size implementation of automatic speech recognition regardless of the platform, such as GPU or special-purpose hardware"
    ],
    "summary": [
        "Neural network technology has greatly improved the accuracy of automatic speech recognition (ASR), and many applications are being developed for smartphones and intelligent personal assistants.",
        "Character and word piece based LMs are used for beam search decoding.",
        "The RNN LMs are based on LSTM or GRU because multiple streams are executed concurrently for beam search decoding.",
        "The model size of the proposed speech recognition system including CTC-AM and RNN LM is about 15 MB with 8-bit parameters which is far smaller than that of conventional HMM-based systems.",
        "The target speech recognition system in this work consists of CTC-trained AM, RNN LM, and beam search decoder.",
        "RNN based LMs show quite high performance when compared to statistical n-gram based LMs. When AM employs characters as the output unit, a dictionary or character-level LM (CLM) can be used for reducing the word error rate (WER) [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "We have developed a word piece model based ASR to reduce the complexity further by lowering the frame rate.",
        "We present the AM training results on character and word piece models.",
        "Table 1 shows the CER and WER performance of the RNN models trained with the WSJ SI-284 training set.",
        "Since the amount of data is critical, we further trained two selected models, i-SRU and LSTM, using all the available speaker independent data in the WSJ corpus to improve the WER.",
        "We trained the i-SRU with 1-D convolutional layer on the word piece model with a vocabulary size of 500.",
        "The word piece LM consists of two layers of 512-dimensional LSTM, which has the same structure as that of RNN LM in Section 4.1.",
        "The difference between WER and CER in the word piece models is much smaller when compared to that in character based models.",
        "Figure 2 shows the execution time estimate of the character and word piece models when the beam widths are 32, 64, and 128.",
        "The word piece model is more advantageous for real-time speech recognition.",
        "Real-time automatic speech recognition (ASR) on embedded CPUs is studied by integrating end-toend trained acoustic RNN, character or word piece language model RNN, and efficient decoding algorithm.",
        "To reduce the DRAM access overhead, we apply multi-frame parallel processing for the AM RNN, and develop high accuracy CTC-trained AM using simple recurrent units (SRUs) combined with 1-dimensional convolution at the input.",
        "We develop two ASR models; one employs character-based AM operating at 20 msec frame rate, and the other uses the word piece based AM that operates at the frame rate of 40 msec.",
        "This study can be applied to all single stream or small batch-size implementation of ASR regardless of the platform, such as GPU or special-purpose hardware"
    ],
    "headline": "We present real-time speech recognition on smartphones or embedded systems by employing recurrent neural network  based acoustic models, recurrent neural networks based language models, and beam-search decoding",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Mohamed%2C%20Abdel-rahman%20Hinton%2C%20Geoffrey%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "2",
            "entry": "[2] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in English and Mandarin. In International Conference on Machine Learning (ICML), pages 173\u2013182, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amodei%2C%20Dario%20Ananthanarayanan%2C%20Sundaram%20Anubhai%2C%20Rishita%20Bai%2C%20Jingliang%20Deep%20speech%202%3A%20End-to-end%20speech%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amodei%2C%20Dario%20Ananthanarayanan%2C%20Sundaram%20Anubhai%2C%20Rishita%20Bai%2C%20Jingliang%20Deep%20speech%202%3A%20End-to-end%20speech%20recognition%202016"
        },
        {
            "id": "3",
            "entry": "[3] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20William%20Jaitly%2C%20Navdeep%20Le%2C%20Quoc%20Listen%2C%20Oriol%20Vinyals%20attend%20and%20spell%3A%20A%20neural%20network%20for%20large%20vocabulary%20conversational%20speech%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20William%20Jaitly%2C%20Navdeep%20Le%2C%20Quoc%20Listen%2C%20Oriol%20Vinyals%20attend%20and%20spell%3A%20A%20neural%20network%20for%20large%20vocabulary%20conversational%20speech%20recognition%202016"
        },
        {
            "id": "4",
            "entry": "[4] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, et al. State-of-theart speech recognition with sequence-to-sequence models. In Acoustics, Speech and Signal Processing (ICASSP), 2018 IEEE International Conference on, pages 4774\u20134778. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chiu%2C%20Chung-Cheng%20Sainath%2C%20Tara%20N.%20Wu%2C%20Yonghui%20Prabhavalkar%2C%20Rohit%20State-of-theart%20speech%20recognition%20with%20sequence-to-sequence%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chiu%2C%20Chung-Cheng%20Sainath%2C%20Tara%20N.%20Wu%2C%20Yonghui%20Prabhavalkar%2C%20Rohit%20State-of-theart%20speech%20recognition%20with%20sequence-to-sequence%20models%202018"
        },
        {
            "id": "5",
            "entry": "[5] Kanishka Rao, Hasim Sak, and Rohit Prabhavalkar. Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer. In Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE, pages 193\u2013199. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rao%2C%20Kanishka%20Sak%2C%20Hasim%20Prabhavalkar%2C%20Rohit%20Exploring%20architectures%2C%20data%20and%20units%20for%20streaming%20end-to-end%20speech%20recognition%20with%20RNN-transducer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rao%2C%20Kanishka%20Sak%2C%20Hasim%20Prabhavalkar%2C%20Rohit%20Exploring%20architectures%2C%20data%20and%20units%20for%20streaming%20end-to-end%20speech%20recognition%20with%20RNN-transducer%202017"
        },
        {
            "id": "6",
            "entry": "[6] Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates, Yashesh Gaur Yi Li, Hairong Liu, Sanjeev Satheesh, Anuroop Sriram, and Zhenyao Zhu. Exploring neural transducers for endto-end speech recognition. In Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE, pages 206\u2013213. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battenberg%2C%20Eric%20Chen%2C%20Jitong%20Child%2C%20Rewon%20Coates%2C%20Adam%20Exploring%20neural%20transducers%20for%20endto-end%20speech%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battenberg%2C%20Eric%20Chen%2C%20Jitong%20Child%2C%20Rewon%20Coates%2C%20Adam%20Exploring%20neural%20transducers%20for%20endto-end%20speech%20recognition%202017"
        },
        {
            "id": "7",
            "entry": "[7] David Huggins-Daines, Mohit Kumar, Arthur Chan, Alan W Black, Mosur Ravishankar, and Alexander I Rudnicky. Pocketsphinx: A free, real-time continuous speech recognition system for hand-held devices. In Acoustics, Speech and Signal Processing (ICASSP), 2006 IEEE International Conference on. IEEE, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huggins-Daines%2C%20David%20Kumar%2C%20Mohit%20Chan%2C%20Arthur%20Black%2C%20Alan%20W.%20Pocketsphinx%3A%20A%20free%2C%20real-time%20continuous%20speech%20recognition%20system%20for%20hand-held%20devices%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huggins-Daines%2C%20David%20Kumar%2C%20Mohit%20Chan%2C%20Arthur%20Black%2C%20Alan%20W.%20Pocketsphinx%3A%20A%20free%2C%20real-time%20continuous%20speech%20recognition%20system%20for%20hand-held%20devices%202006"
        },
        {
            "id": "8",
            "entry": "[8] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "9",
            "entry": "[9] Kyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. Syntax, Semantics and Structure in Statistical Translation, page 103, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merri%C3%ABnboer%2C%20Bart%20Bahdanau%2C%20Dzmitry%20Bengio%2C%20Yoshua%20On%20the%20properties%20of%20neural%20machine%20translation%3A%20Encoder-decoder%20approaches%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merri%C3%ABnboer%2C%20Bart%20Bahdanau%2C%20Dzmitry%20Bengio%2C%20Yoshua%20On%20the%20properties%20of%20neural%20machine%20translation%3A%20Encoder-decoder%20approaches%202014"
        },
        {
            "id": "10",
            "entry": "[10] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bradbury%2C%20James%20Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Quasi-recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bradbury%2C%20James%20Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Quasi-recurrent%20neural%20networks%202017"
        },
        {
            "id": "11",
            "entry": "[11] Tao Lei and Yu Zhang. Training RNNs as fast as CNNs. arXiv preprint arXiv:1709.02755, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.02755"
        },
        {
            "id": "12",
            "entry": "[12] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%2C%20Eric%20Cundy%2C%20Chris%20Parallelizing%20linear%20recurrent%20neural%20nets%20over%20sequence%20length%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%2C%20Eric%20Cundy%2C%20Chris%20Parallelizing%20linear%20recurrent%20neural%20nets%20over%20sequence%20length%202018"
        },
        {
            "id": "13",
            "entry": "[13] David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. arXiv preprint arXiv:1602.02218, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02218"
        },
        {
            "id": "14",
            "entry": "[14] Yajie Miao, Mohammad Gowayyed, and Florian Metze. EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding. In Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on, pages 167\u2013174. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miao%2C%20Yajie%20Gowayyed%2C%20Mohammad%20Metze%2C%20Florian%20EESEN%3A%20End-to-end%20speech%20recognition%20using%20deep%20RNN%20models%20and%20WFST-based%20decoding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miao%2C%20Yajie%20Gowayyed%2C%20Mohammad%20Metze%2C%20Florian%20EESEN%3A%20End-to-end%20speech%20recognition%20using%20deep%20RNN%20models%20and%20WFST-based%20decoding%202015"
        },
        {
            "id": "15",
            "entry": "[15] Sharan Narang, Erich Elsen, Gregory Diamos, and Shubho Sengupta. Exploring sparsity in recurrent neural networks. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Narang%2C%20Sharan%20Elsen%2C%20Erich%20Diamos%2C%20Gregory%20Sengupta%2C%20Shubho%20Exploring%20sparsity%20in%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Narang%2C%20Sharan%20Elsen%2C%20Erich%20Diamos%2C%20Gregory%20Sengupta%2C%20Shubho%20Exploring%20sparsity%20in%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Chen%20Yao%2C%20Jianqiang%20Lin%2C%20Zhouchen%20Ou%2C%20Wenwu%20Alternating%20multi-bit%20quantization%20for%20recurrent%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Chen%20Yao%2C%20Jianqiang%20Lin%2C%20Zhouchen%20Ou%2C%20Wenwu%20Alternating%20multi-bit%20quantization%20for%20recurrent%20neural%20networks%202018"
        },
        {
            "id": "17",
            "entry": "[17] Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, and Ian McGraw. On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 5970\u20135974. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prabhavalkar%2C%20Rohit%20Alsharif%2C%20Ouais%20Bruguier%2C%20Antoine%20McGraw%2C%20Ian%20On%20the%20compression%20of%20recurrent%20neural%20networks%20with%20an%20application%20to%20LVCSR%20acoustic%20modeling%20for%20embedded%20speech%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prabhavalkar%2C%20Rohit%20Alsharif%2C%20Ouais%20Bruguier%2C%20Antoine%20McGraw%2C%20Ian%20On%20the%20compression%20of%20recurrent%20neural%20networks%20with%20an%20application%20to%20LVCSR%20acoustic%20modeling%20for%20embedded%20speech%20recognition%202016"
        },
        {
            "id": "18",
            "entry": "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1610.10099v2"
        },
        {
            "id": "19",
            "entry": "[19] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning (ICML), pages 1243\u20131252, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning (ICML), pages 933\u2013941, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Fan%2C%20Angela%20Auli%2C%20Michael%20Grangier%2C%20David%20Language%20modeling%20with%20gated%20convolutional%20networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Collobert. Letter-based speech recognition with gated ConvNets. arXiv preprint arXiv:1712.09444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09444"
        },
        {
            "id": "22",
            "entry": "[22] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "23",
            "entry": "[23] Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In International Conference on Machine Learning (ICML), pages 369\u2013376, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alex%20Graves%2C%20Santiago%20Fern%C3%A1ndez%2C%20Faustino%20Gomez%20Schmidhuber%2C%20J%C3%BCrgen%20Connectionist%20temporal%20classification%3A%20Labelling%20unsegmented%20sequence%20data%20with%20recurrent%20neural%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alex%20Graves%2C%20Santiago%20Fern%C3%A1ndez%2C%20Faustino%20Gomez%20Schmidhuber%2C%20J%C3%BCrgen%20Connectionist%20temporal%20classification%3A%20Labelling%20unsegmented%20sequence%20data%20with%20recurrent%20neural%20networks%202006"
        },
        {
            "id": "24",
            "entry": "[24] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Hasim Sak. Convolutional, long short-term memory, fully connected deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 4580\u20134584. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20Tara%20N.%20Vinyals%2C%20Oriol%20Senior%2C%20Andrew%20Sak%2C%20Hasim%20Convolutional%2C%20long%20short-term%20memory%2C%20fully%20connected%20deep%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20Tara%20N.%20Vinyals%2C%20Oriol%20Senior%2C%20Andrew%20Sak%2C%20Hasim%20Convolutional%2C%20long%20short-term%20memory%2C%20fully%20connected%20deep%20neural%20networks%202015"
        },
        {
            "id": "25",
            "entry": "[25] Kyuyeon Hwang and Wonyong Sung. Character-level incremental speech recognition with recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 5335\u20135339. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hwang%2C%20Kyuyeon%20Sung%2C%20Wonyong%20Character-level%20incremental%20speech%20recognition%20with%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hwang%2C%20Kyuyeon%20Sung%2C%20Wonyong%20Character-level%20incremental%20speech%20recognition%20with%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "26",
            "entry": "[26] Kyuyeon Hwang and Wonyong Sung. Character-level language modeling with hierarchical recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pages 5720\u20135724. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hwang%2C%20Kyuyeon%20Sung%2C%20Wonyong%20Character-level%20language%20modeling%20with%20hierarchical%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hwang%2C%20Kyuyeon%20Sung%2C%20Wonyong%20Character-level%20language%20modeling%20with%20hierarchical%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "27",
            "entry": "[27] Zhehuai Chen, Yimeng Zhuang, Yanmin Qian, Kai Yu, Zhehuai Chen, Yimeng Zhuang, Yanmin Qian, Kai Yu, Kai Yu, Yimeng Zhuang, et al. Phone synchronous speech recognition with CTC lattices. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 25(1): 90\u2013101, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Zhehuai%20Zhuang%2C%20Yimeng%20Qian%2C%20Yanmin%20Yu%2C%20Kai%20Phone%20synchronous%20speech%20recognition%20with%20CTC%20lattices%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Zhehuai%20Zhuang%2C%20Yimeng%20Qian%2C%20Yanmin%20Yu%2C%20Kai%20Phone%20synchronous%20speech%20recognition%20with%20CTC%20lattices%202017"
        },
        {
            "id": "28",
            "entry": "[28] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "29",
            "entry": "[29] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 1019\u2013 1027, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "30",
            "entry": "[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "31",
            "entry": "[31] Yingbo Zhou, Caiming Xiong, and Richard Socher. Improving end-to-end speech recognition with policy learning. In Acoustics, Speech and Signal Processing (ICASSP), 2018 IEEE International Conference on, pages 5819\u20135823. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Yingbo%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Improving%20end-to-end%20speech%20recognition%20with%20policy%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Yingbo%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Improving%20end-to-end%20speech%20recognition%20with%20policy%20learning%202018"
        },
        {
            "id": "32",
            "entry": "[32] Albert Zeyer, Kazuki Irie, Ralf Schl\u00fcter, and Hermann Ney. Improved training of end-to-end attention models for speech recognition. Interspeech, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeyer%2C%20Albert%20Irie%2C%20Kazuki%20Schl%C3%BCter%2C%20Ralf%20Ney%2C%20Hermann%20Improved%20training%20of%20end-to-end%20attention%20models%20for%20speech%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeyer%2C%20Albert%20Irie%2C%20Kazuki%20Schl%C3%BCter%2C%20Ralf%20Ney%2C%20Hermann%20Improved%20training%20of%20end-to-end%20attention%20models%20for%20speech%20recognition%202018"
        },
        {
            "id": "33",
            "entry": "[33] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an ASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panayotov%2C%20Vassil%20Chen%2C%20Guoguo%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20Librispeech%3A%20an%20ASR%20corpus%20based%20on%20public%20domain%20audio%20books%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panayotov%2C%20Vassil%20Chen%2C%20Guoguo%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20Librispeech%3A%20an%20ASR%20corpus%20based%20on%20public%20domain%20audio%20books%202015"
        },
        {
            "id": "34",
            "entry": "[34] Zhang Xianyi, Wang Qian, and Zaheer Chothia. Openblas. URL: http://xianyi.github.io/OpenBLAS.",
            "url": "http://xianyi.github.io/OpenBLAS"
        },
        {
            "id": "35",
            "entry": "[35] Gemmlowp: A small self-contained low-precision gemm library. URL: https://github.com/google/gemmlowp. ",
            "url": "https://github.com/google/gemmlowp"
        }
    ]
}
