{
    "filename": "7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks.pdf",
    "metadata": {
        "title": "Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks",
        "author": "Anirvan Sengupta, Mariano Tepper, Cengiz Pehlevan, Alexander Genkin, Dmitri Chklovskii",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks.pdf",
            "doi": "10.1101/338947"
        },
        "abstract": "Many neurons in the brain, such as place cells in the rodent hippocampus, have localized receptive fields, i.e., they respond to a small neighborhood of stimulus space. What is the functional significance of such representations and how can they arise? Here, we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs. Numerical simulations of such networks on standard datasets yield manifold-tiling localized receptive fields. More generally, we show analytically that, for data lying on symmetric manifolds, optimal solutions of objectives, from which similarity-preserving networks are derived, have localized receptive fields. Therefore, nonnegative similarity-preserving mapping (NSM) implemented by neural networks can model representations of continuous manifolds in the brain.",
        "date": 2018
    },
    "keywords": [
        {
            "term": "multidimensional scaling",
            "url": "https://en.wikipedia.org/wiki/multidimensional_scaling"
        },
        {
            "term": "receptive field",
            "url": "https://en.wikipedia.org/wiki/receptive_field"
        },
        {
            "term": "nonlinear dimensionality reduction",
            "url": "https://en.wikipedia.org/wiki/nonlinear_dimensionality_reduction"
        },
        {
            "term": "nonnegative matrix factorization",
            "url": "https://en.wikipedia.org/wiki/nonnegative_matrix_factorization"
        },
        {
            "term": "dimensionality reduction",
            "url": "https://en.wikipedia.org/wiki/dimensionality_reduction"
        },
        {
            "term": "place cell",
            "url": "https://en.wikipedia.org/wiki/place_cell"
        },
        {
            "term": "visual cortex",
            "url": "https://en.wikipedia.org/wiki/visual_cortex"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "A salient and unexplained feature of many neurons is that their receptive fields are localized in the parameter space they represent",
        "A hippocampus place cell is active in a particular spatial location [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], the response of a V1 neuron is localized in visual space and orientation [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], and the response of an auditory neuron is localized in the sound frequency space [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Activity influences receptive fields via modification, or learning, of synaptic weights which gate the activity of upstream neurons channeling sensory inputs",
        "We demonstrate that biologically plausible neural networks can learn manifold-tiling localized receptive fields from the upstream activity in an unsupervised fashion",
        "Analytical optimization of nonnegative similarity-preserving mapping objectives for input originating from symmetric manifolds. Derivation of biologically plausible nonnegative similarity-preserving mapping neural networks. Offline and online algorithms for manifold learning of arbitrary manifolds",
        "Neural networks implementing nonnegative similarity-preserving mapping algorithms use only biologically plausible local (Hebbian or anti-Hebbian) synaptic learning rules. These results add to the versatility of nonnegative similarity-preserving mapping networks previously shown to cluster data, learn sparse dictionaries and blindly separate sources [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], depending on the nature of input data"
    ],
    "key_statements": [
        "A salient and unexplained feature of many neurons is that their receptive fields are localized in the parameter space they represent",
        "A hippocampus place cell is active in a particular spatial location [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], the response of a V1 neuron is localized in visual space and orientation [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], and the response of an auditory neuron is localized in the sound frequency space [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Activity influences receptive fields via modification, or learning, of synaptic weights which gate the activity of upstream neurons channeling sensory inputs",
        "We demonstrate that biologically plausible neural networks can learn manifold-tiling localized receptive fields from the upstream activity in an unsupervised fashion",
        "Analytical optimization of nonnegative similarity-preserving mapping objectives for input originating from symmetric manifolds. Derivation of biologically plausible nonnegative similarity-preserving mapping neural networks. Offline and online algorithms for manifold learning of arbitrary manifolds",
        "In our case, the variable yat gives the activity of the a-th neuron as the t-th input vector is presented to the network, such a solution would define a receptive field of neuron, a, localized in the space of inputs",
        "While we do not have a closed-form solution for nonnegative similarity-preserving mapping-0 on a ring, we show that the optimal solution has localized receptive fields (see Supplementary Material, Sec",
        "We demonstrated, in the continuum limit, that the solutions to nonnegative similarity-preserving mapping objectives for data on symmetric manifolds possess localized receptive fields that tile these manifolds",
        "Neural networks implementing nonnegative similarity-preserving mapping algorithms use only biologically plausible local (Hebbian or anti-Hebbian) synaptic learning rules. These results add to the versatility of nonnegative similarity-preserving mapping networks previously shown to cluster data, learn sparse dictionaries and blindly separate sources [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], depending on the nature of input data"
    ],
    "summary": [
        "A salient and unexplained feature of many neurons is that their receptive fields are localized in the parameter space they represent.",
        "We demonstrate that biologically plausible neural networks can learn manifold-tiling localized receptive fields from the upstream activity in an unsupervised fashion.",
        "For inputs lying on a manifold, we derive an optimal offline solution and demonstrate analytically and numerically that the receptive fields are localized and tile the manifold, Fig. 1.",
        "From the same objective, we derive an online optimization algorithm which can be implemented by a biologically plausible neural network, Fig. 1.",
        "Analytical optimization of NSM objectives for input originating from symmetric manifolds.",
        "In Sec. 5, we derive online optimization algorithm and an NSM neural network.",
        "Just like Eq (2), NSM-1 preserves similarity of nearby input data samples while orthogonalizing output vectors of dissimilar input pairs.",
        "In our case, the variable yat gives the activity of the a-th neuron as the t-th input vector is presented to the network, such a solution would define a receptive field of neuron, a, localized in the space of inputs.",
        "We will derive such localized-receptive field solutions for inputs originating from symmetric manifolds.",
        "We demonstrated, in the continuum limit, that the solutions to NSM objectives for data on symmetric manifolds possess localized receptive fields that tile these manifolds.",
        "Our algorithms yield manifold-tiling localized receptive fields on real-world data.",
        "Recall that for the input data lying on a ring, optimization without a rank constraint yields truncated cosine solutions, see Eq (9).",
        "We show numerically that fixed-rank optimization yields the same solutions, Fig. 2: the computed matrix Y>Y is circulant, all receptive fields are equivalent to each other, are well approximated by truncated cosine and tile the manifold with overlap.",
        "For the input lying on a 2-sphere, we find numerically that localized solutions tile the manifold, Fig. 3.",
        "We show that objective functions approximately preserving similarity, along with nonnegativity constraint on the outputs, learn data manifolds.",
        "Neural networks implementing NSM algorithms use only biologically plausible local (Hebbian or anti-Hebbian) synaptic learning rules.",
        "These results add to the versatility of NSM networks previously shown to cluster data, learn sparse dictionaries and blindly separate sources [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], depending on the nature of input data.",
        "Unlike most existing algorithms, ours do not output low-dimensional vectors of embedding variables but rather high-dimensional vectors of assignment indices to centroids tiling the manifold, similar to radial basis function networks [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>]."
    ],
    "headline": "What is the functional significance of such representations and how can they arise? Here, we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] John O\u2019Keefe and Lynn Nadel. The hippocampus as a cognitive map. Oxford: Clarendon Press, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Keefe%2C%20John%20Nadel%2C%20Lynn%20The%20hippocampus%20as%20a%20cognitive%20map%201978"
        },
        {
            "id": "2",
            "entry": "[2] David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. The Journal of Physiology, 160(1):106\u2013154, 1962.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hubel%2C%20David%20H.%20Wiesel%2C%20Torsten%20N.%20Receptive%20fields%2C%20binocular%20interaction%20and%20functional%20architecture%20in%20the%20cat%E2%80%99s%20visual%20cortex%201962",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hubel%2C%20David%20H.%20Wiesel%2C%20Torsten%20N.%20Receptive%20fields%2C%20binocular%20interaction%20and%20functional%20architecture%20in%20the%20cat%E2%80%99s%20visual%20cortex%201962"
        },
        {
            "id": "3",
            "entry": "[3] Eric I Knudsen and Masakazu Konishi. Center-surround organization of auditory receptive fields in the owl. Science, 202(4369):778\u2013780, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Knudsen%2C%20Eric%20I.%20Konishi%2C%20Masakazu%20Center-surround%20organization%20of%20auditory%20receptive%20fields%20in%20the%20owl%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Knudsen%2C%20Eric%20I.%20Konishi%2C%20Masakazu%20Center-surround%20organization%20of%20auditory%20receptive%20fields%20in%20the%20owl%201978"
        },
        {
            "id": "4",
            "entry": "[4] Michael P Kilgard and Michael M Merzenich. Cortical map reorganization enabled by nucleus basalis activity. Science, 279(5357):1714\u20131718, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kilgard%2C%20Michael%20P.%20Merzenich%2C%20Michael%20M.%20Cortical%20map%20reorganization%20enabled%20by%20nucleus%20basalis%20activity%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kilgard%2C%20Michael%20P.%20Merzenich%2C%20Michael%20M.%20Cortical%20map%20reorganization%20enabled%20by%20nucleus%20basalis%20activity%201998"
        },
        {
            "id": "5",
            "entry": "[5] Daniel E Feldman and Michael Brecht. Map plasticity in somatosensory cortex. Science, 310(5749):810\u2013 815, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Daniel%20E.%20Brecht%2C%20Michael%20Map%20plasticity%20in%20somatosensory%20cortex%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20Daniel%20E.%20Brecht%2C%20Michael%20Map%20plasticity%20in%20somatosensory%20cortex%202005"
        },
        {
            "id": "6",
            "entry": "[6] Takao K Hensch. Critical period plasticity in local cortical circuits. Nature Reviews Neuroscience, 6(11):877, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensch%2C%20Takao%20K.%20Critical%20period%20plasticity%20in%20local%20cortical%20circuits%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensch%2C%20Takao%20K.%20Critical%20period%20plasticity%20in%20local%20cortical%20circuits%202005"
        },
        {
            "id": "7",
            "entry": "[7] Valentin Dragoi, Jitendra Sharma, and Mriganka Sur. Adaptation-induced plasticity of orientation tuning in adult visual cortex. Neuron, 28(1):287\u2013298, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dragoi%2C%20Valentin%20Sharma%2C%20Jitendra%20Sur%2C%20Mriganka%20Adaptation-induced%20plasticity%20of%20orientation%20tuning%20in%20adult%20visual%20cortex%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dragoi%2C%20Valentin%20Sharma%2C%20Jitendra%20Sur%2C%20Mriganka%20Adaptation-induced%20plasticity%20of%20orientation%20tuning%20in%20adult%20visual%20cortex%202000"
        },
        {
            "id": "8",
            "entry": "[8] Cengiz Pehlevan, Tao Hu, and Dmitri B Chklovskii. A Hebbian/anti-Hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data. Neural Computation, 27(7):1461\u20131495, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pehlevan%2C%20Cengiz%20Hu%2C%20Tao%20Chklovskii%2C%20Dmitri%20B.%20A%20Hebbian/anti-Hebbian%20neural%20network%20for%20linear%20subspace%20learning%3A%20A%20derivation%20from%20multidimensional%20scaling%20of%20streaming%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pehlevan%2C%20Cengiz%20Hu%2C%20Tao%20Chklovskii%2C%20Dmitri%20B.%20A%20Hebbian/anti-Hebbian%20neural%20network%20for%20linear%20subspace%20learning%3A%20A%20derivation%20from%20multidimensional%20scaling%20of%20streaming%20data%202015"
        },
        {
            "id": "9",
            "entry": "[9] Cengiz Pehlevan and Dmitri Chklovskii. A normative theory of adaptive dimensionality reduction in neural networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pehlevan%2C%20Cengiz%20Chklovskii%2C%20Dmitri%20A%20normative%20theory%20of%20adaptive%20dimensionality%20reduction%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pehlevan%2C%20Cengiz%20Chklovskii%2C%20Dmitri%20A%20normative%20theory%20of%20adaptive%20dimensionality%20reduction%20in%20neural%20networks%202015"
        },
        {
            "id": "10",
            "entry": "[10] Cengiz Pehlevan, Anirvan M Sengupta, and Dmitri B Chklovskii. Why do similarity matching objectives lead to Hebbian/anti-Hebbian networks? Neural Computation, 30(1):84\u2013124, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pehlevan%2C%20Cengiz%20Sengupta%2C%20Anirvan%20M.%20Chklovskii%2C%20Dmitri%20B.%20Why%20do%20similarity%20matching%20objectives%20lead%20to%20Hebbian/anti-Hebbian%20networks%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pehlevan%2C%20Cengiz%20Sengupta%2C%20Anirvan%20M.%20Chklovskii%2C%20Dmitri%20B.%20Why%20do%20similarity%20matching%20objectives%20lead%20to%20Hebbian/anti-Hebbian%20networks%3F%202018"
        },
        {
            "id": "11",
            "entry": "[11] Cengiz Pehlevan and Dmitri B Chklovskii. A Hebbian/anti-Hebbian network derived from online nonnegative matrix factorization can cluster and discover sparse features. In ACSSC, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pehlevan%2C%20Cengiz%20Chklovskii%2C%20Dmitri%20B.%20A%20Hebbian/anti-Hebbian%20network%20derived%20from%20online%20nonnegative%20matrix%20factorization%20can%20cluster%20and%20discover%20sparse%20features%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pehlevan%2C%20Cengiz%20Chklovskii%2C%20Dmitri%20B.%20A%20Hebbian/anti-Hebbian%20network%20derived%20from%20online%20nonnegative%20matrix%20factorization%20can%20cluster%20and%20discover%20sparse%20features%202014"
        },
        {
            "id": "12",
            "entry": "[12] Cengiz Pehlevan, Alex Genkin, and Dmitri B Chklovskii. A clustering neural network model of insect olfaction. In ACSSC, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pehlevan%2C%20Cengiz%20Genkin%2C%20Alex%20Chklovskii%2C%20Dmitri%20B.%20A%20clustering%20neural%20network%20model%20of%20insect%20olfaction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pehlevan%2C%20Cengiz%20Genkin%2C%20Alex%20Chklovskii%2C%20Dmitri%20B.%20A%20clustering%20neural%20network%20model%20of%20insect%20olfaction%202017"
        },
        {
            "id": "13",
            "entry": "[13] Christopher KI Williams. On a connection between kernel PCA and metric multidimensional scaling. In NIPS, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20K.I.%20On%20a%20connection%20between%20kernel%20PCA%20and%20metric%20multidimensional%20scaling%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Christopher%20K.I.%20On%20a%20connection%20between%20kernel%20PCA%20and%20metric%20multidimensional%20scaling%202001"
        },
        {
            "id": "14",
            "entry": "[14] Trevor F Cox and Michael AA Cox. Multidimensional scaling. CRC press, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cox%2C%20Trevor%20F.%20Cox%2C%20Michael%20A.A.%20Multidimensional%20scaling%202000"
        },
        {
            "id": "15",
            "entry": "[15] John M Bibby, John T Kent, and Kanti V Mardia. Multivariate analysis, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=John%20M%20Bibby%20John%20T%20Kent%20and%20Kanti%20V%20Mardia%20Multivariate%20analysis%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=John%20M%20Bibby%20John%20T%20Kent%20and%20Kanti%20V%20Mardia%20Multivariate%20analysis%201979"
        },
        {
            "id": "16",
            "entry": "[16] H Sebastian Seung and Jonathan Zung. A correlation game for unsupervised learning yields computational interpretations of Hebbian excitation, anti-Hebbian inhibition, and synapse elimination. arXiv preprint arXiv:1704.00646, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00646"
        },
        {
            "id": "17",
            "entry": "[17] Yanis Bahroun and Andrea Soltoggio. Online representation learning with single and multi-layer Hebbian networks for image classification. In ICANN, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahroun%2C%20Yanis%20Soltoggio%2C%20Andrea%20Online%20representation%20learning%20with%20single%20and%20multi-layer%20Hebbian%20networks%20for%20image%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahroun%2C%20Yanis%20Soltoggio%2C%20Andrea%20Online%20representation%20learning%20with%20single%20and%20multi-layer%20Hebbian%20networks%20for%20image%20classification%202017"
        },
        {
            "id": "18",
            "entry": "[18] Cengiz Pehlevan, Sreyas Mohan, and Dmitri B Chklovskii. Blind nonnegative source separation using biological neural networks. Neural Computation, 29(11):2925\u20132954, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pehlevan%2C%20Cengiz%20Mohan%2C%20Sreyas%20Chklovskii%2C%20Dmitri%20B.%20Blind%20nonnegative%20source%20separation%20using%20biological%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pehlevan%2C%20Cengiz%20Mohan%2C%20Sreyas%20Chklovskii%2C%20Dmitri%20B.%20Blind%20nonnegative%20source%20separation%20using%20biological%20neural%20networks%202017"
        },
        {
            "id": "19",
            "entry": "[19] Chris Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative matrix factorization and spectral clustering. In ICDM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Chris%20He%2C%20Xiaofeng%20Simon%2C%20Horst%20D.%20On%20the%20equivalence%20of%20nonnegative%20matrix%20factorization%20and%20spectral%20clustering%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Chris%20He%2C%20Xiaofeng%20Simon%2C%20Horst%20D.%20On%20the%20equivalence%20of%20nonnegative%20matrix%20factorization%20and%20spectral%20clustering%202005"
        },
        {
            "id": "20",
            "entry": "[20] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadsell%2C%20Raia%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Dimensionality%20reduction%20by%20learning%20an%20invariant%20mapping%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadsell%2C%20Raia%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Dimensionality%20reduction%20by%20learning%20an%20invariant%20mapping%202006"
        },
        {
            "id": "21",
            "entry": "[21] Rani Ben-Yishai, Ruth Lev Bar-Or, and Haim Sompolinsky. Theory of orientation tuning in visual cortex. Proceedings of the National Academy of Sciences, 92(9):3844\u20133848, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-Yishai%2C%20Rani%20Bar-Or%2C%20Ruth%20Lev%20Sompolinsky%2C%20Haim%20Theory%20of%20orientation%20tuning%20in%20visual%20cortex%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-Yishai%2C%20Rani%20Bar-Or%2C%20Ruth%20Lev%20Sompolinsky%2C%20Haim%20Theory%20of%20orientation%20tuning%20in%20visual%20cortex%201995"
        },
        {
            "id": "22",
            "entry": "[22] Abraham Berman and Naomi Shaked-Monderer. Completely positive matrices. World Scientific, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berman%2C%20Abraham%20Shaked-Monderer%2C%20Naomi%20Completely%20positive%20matrices%202003"
        },
        {
            "id": "23",
            "entry": "[23] Samuel Burer, Kurt M. Anstreicher, and Mirjam D\u00fcr. The difference between 5 \u21e5 5 doubly nonnegative and completely positive matrices. Linear Algebra and its Applications, 431(9):1539\u20131552, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burer%2C%20Samuel%20Anstreicher%2C%20Kurt%20M.%20D%C3%BCr%2C%20Mirjam%20The%20difference%20between%205%20%E2%87%A5%205%20doubly%20nonnegative%20and%20completely%20positive%20matrices%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burer%2C%20Samuel%20Anstreicher%2C%20Kurt%20M.%20D%C3%BCr%2C%20Mirjam%20The%20difference%20between%205%20%E2%87%A5%205%20doubly%20nonnegative%20and%20completely%20positive%20matrices%202009"
        },
        {
            "id": "24",
            "entry": "[24] Samuel Burer and Renato D.C. Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329\u2013357, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burer%2C%20Samuel%20Monteiro%2C%20Renato%20D.C.%20A%20nonlinear%20programming%20algorithm%20for%20solving%20semidefinite%20programs%20via%20low-rank%20factorization%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burer%2C%20Samuel%20Monteiro%2C%20Renato%20D.C.%20A%20nonlinear%20programming%20algorithm%20for%20solving%20semidefinite%20programs%20via%20low-rank%20factorization%202003"
        },
        {
            "id": "25",
            "entry": "[25] Arash A Amini and Elizaveta Levina. On semidefinite relaxations for the block model. arXiv preprint arXiv:1406.5647, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.5647"
        },
        {
            "id": "26",
            "entry": "[26] Pranjal Awasthi, Afonso S Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad Villar, and Rachel Ward. Relax, no need to round: Integrality of clustering formulations. In ITCS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Awasthi%2C%20Pranjal%20Bandeira%2C%20Afonso%20S.%20Charikar%2C%20Moses%20Krishnaswamy%2C%20Ravishankar%20Relax%2C%20no%20need%20to%20round%3A%20Integrality%20of%20clustering%20formulations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Awasthi%2C%20Pranjal%20Bandeira%2C%20Afonso%20S.%20Charikar%2C%20Moses%20Krishnaswamy%2C%20Ravishankar%20Relax%2C%20no%20need%20to%20round%3A%20Integrality%20of%20clustering%20formulations%202015"
        },
        {
            "id": "27",
            "entry": "[27] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semidefinite programming. SIAM Journal on Optimization, 18(1):186\u2013205, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Jiming%20Wei%2C%20Yu%20Approximating%20k-means-type%20clustering%20via%20semidefinite%20programming%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20Jiming%20Wei%2C%20Yu%20Approximating%20k-means-type%20clustering%20via%20semidefinite%20programming%202007"
        },
        {
            "id": "28",
            "entry": "[28] Mariano Tepper, Anirvan M Sengupta, and Dmitri Chklovskii. Clustering is semidefinitely not that hard: Nonnegative SDP for manifold disentangling. arXiv preprint arXiv:1706.06028, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06028"
        },
        {
            "id": "29",
            "entry": "[29] Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semidefinite programs. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boumal%2C%20Nicolas%20Voroninski%2C%20Vlad%20Bandeira%2C%20Afonso%20The%20non-convex%20Burer-Monteiro%20approach%20works%20on%20smooth%20semidefinite%20programs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boumal%2C%20Nicolas%20Voroninski%2C%20Vlad%20Bandeira%2C%20Afonso%20The%20non-convex%20Burer-Monteiro%20approach%20works%20on%20smooth%20semidefinite%20programs%202016"
        },
        {
            "id": "30",
            "entry": "[30] Killan Q. Weinberger and Lawrence K. Saul. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. AAAI, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20Killan%20Q.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20nonlinear%20dimensionality%20reduction%20by%20maximum%20variance%20unfolding%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20Killan%20Q.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20nonlinear%20dimensionality%20reduction%20by%20maximum%20variance%20unfolding%202006"
        },
        {
            "id": "31",
            "entry": "[31] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In NIPS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009"
        },
        {
            "id": "32",
            "entry": "[32] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500), 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roweis%2C%20Sam%20T.%20Saul%2C%20Lawrence%20K.%20Nonlinear%20dimensionality%20reduction%20by%20locally%20linear%20embedding%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roweis%2C%20Sam%20T.%20Saul%2C%20Lawrence%20K.%20Nonlinear%20dimensionality%20reduction%20by%20locally%20linear%20embedding%202000"
        },
        {
            "id": "33",
            "entry": "[33] Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500), 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tenenbaum%2C%20Joshua%20B.%20de%20Silva%2C%20Vin%20Langford%2C%20John%20C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tenenbaum%2C%20Joshua%20B.%20de%20Silva%2C%20Vin%20Langford%2C%20John%20C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000"
        },
        {
            "id": "34",
            "entry": "[34] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373\u20131396, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Laplacian%20eigenmaps%20for%20dimensionality%20reduction%20and%20data%20representation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Laplacian%20eigenmaps%20for%20dimensionality%20reduction%20and%20data%20representation%202003"
        },
        {
            "id": "35",
            "entry": "[35] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-SNE%202008"
        },
        {
            "id": "36",
            "entry": "[36] Kilian Q Weinberger and Lawrence K Saul. Unsupervised learning of image manifolds by semidefinite programming. International Journal of Computer Vision, 70(1):77\u201390, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20Kilian%20Q.%20Saul%2C%20Lawrence%20K.%20Unsupervised%20learning%20of%20image%20manifolds%20by%20semidefinite%20programming%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20Kilian%20Q.%20Saul%2C%20Lawrence%20K.%20Unsupervised%20learning%20of%20image%20manifolds%20by%20semidefinite%20programming%202006"
        },
        {
            "id": "37",
            "entry": "[37] David L Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591\u20135596, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Grimes%2C%20Carrie%20Hessian%20eigenmaps%3A%20Locally%20linear%20embedding%20techniques%20for%20high-dimensional%20data%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Grimes%2C%20Carrie%20Hessian%20eigenmaps%3A%20Locally%20linear%20embedding%20techniques%20for%20high-dimensional%20data%202003"
        },
        {
            "id": "38",
            "entry": "[38] David S Broomhead and David Lowe. Radial basis functions, multi-variable functional interpolation and adaptive networks. Technical report, Royal Signals and Radar Establishment Malvern (United Kingdom), 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Broomhead%2C%20David%20S.%20Lowe%2C%20David%20Radial%20basis%20functions%2C%20multi-variable%20functional%20interpolation%20and%20adaptive%20networks%201988"
        },
        {
            "id": "39",
            "entry": "[39] Matthew Brand. Charting a manifold. In NIPS, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brand%2C%20Matthew%20Charting%20a%20manifold%202003"
        },
        {
            "id": "40",
            "entry": "[40] Nikolaos Pitelis, Chris Russell, and Lourdes Agapito. Learning a manifold as an atlas. In CVPR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pitelis%2C%20Nikolaos%20Russell%2C%20Chris%20Agapito%2C%20Lourdes%20Learning%20a%20manifold%20as%20an%20atlas.%20In%20CVPR%202013"
        },
        {
            "id": "41",
            "entry": "[41] Sanjeev Arora and Andrej Risteski. Provable benefits of representation learning. arXiv preprint arXiv:1706.04601, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04601"
        },
        {
            "id": "42",
            "entry": "[42] Christine Bachoc, Dion C Gijswijt, Alexander Schrijver, and Frank Vallentin. Invariant semidefinite programs. In Handbook on semidefinite, conic and polynomial optimization, pages 219\u2013269.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bachoc%2C%20Christine%20Gijswijt%2C%20Dion%20C.%20Schrijver%2C%20Alexander%20Vallentin%2C%20Frank%20Invariant%20semidefinite%20programs",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bachoc%2C%20Christine%20Gijswijt%2C%20Dion%20C.%20Schrijver%2C%20Alexander%20Vallentin%2C%20Frank%20Invariant%20semidefinite%20programs"
        },
        {
            "id": "43",
            "entry": "[43] Nathan Jacobson. Basic algebra I. Courier Corporation, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nathan%20Jacobson%20Basic%20algebra%20I%20Courier%20Corporation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nathan%20Jacobson%20Basic%20algebra%20I%20Courier%20Corporation%202012"
        },
        {
            "id": "44",
            "entry": "[44] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olshausen%2C%20Bruno%20A.%20Field%2C%20David%20J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996"
        },
        {
            "id": "45",
            "entry": "[45] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In COLT, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Simple%2C%20Ankur%20Moitra%20and%20neural%20algorithms%20for%20sparse%20coding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Ma%2C%20Tengyu%20Simple%2C%20Ankur%20Moitra%20and%20neural%20algorithms%20for%20sparse%20coding%202015"
        }
    ]
}
