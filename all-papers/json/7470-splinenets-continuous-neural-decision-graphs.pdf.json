{
    "filename": "7470-splinenets-continuous-neural-decision-graphs.pdf",
    "metadata": {
        "title": "SplineNets: Continuous Neural Decision Graphs",
        "author": "Cem Keskin, Shahram Izadi",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7470-splinenets-continuous-neural-decision-graphs.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e., conditioned on the computational path). SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. In particular, we embed infinitely many function weights (e.g. filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter. Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight. We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks. Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet."
    },
    "keywords": [
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "There is a growing body of literature applying conditioning to convolutional neural networks, where only parts of the network are selectively active to save runtime complexity and compute",
        "We demonstrate how these novel networks dramatically reduce runtime complexity and computation cost beyond regular convolutional neural networks while maintaining or even increasing accuracy",
        "Our contributions in this paper are: (i) the embedding trick to enable smooth conditional branching, a novel neural decision graph that is uncountable and operates in continuous decision space, an architectural constraint to enforce a semantic relationship between latent parameters of functions in consecutive layers, a regularizer that maximizes mutual information to utilize and specialize splines, and (v) a new differentiable quantization method for estimating soft entropies",
        "The continuous position distribution P plays an important role in utilizing SplineNets properly, e.g. if all \u03c6i\u2019s that are dynamically generated by samples are the same, the model reduces to a regular convolutional neural networks",
        "We presented the concept of SplineNets, a novel and practical method for realizing conditional neural networks using embedded continuous manifolds",
        "Our results dramatically reduce runtime complexity and computation costs of convolutional neural networks while maintaining or even increasing accuracy"
    ],
    "key_statements": [
        "There is a growing body of literature applying conditioning to convolutional neural networks, where only parts of the network are selectively active to save runtime complexity and compute",
        "In this paper we present a new and practical approach for supporting conditional neural networks called SplineNets",
        "We demonstrate how these novel networks dramatically reduce runtime complexity and computation cost beyond regular convolutional neural networks while maintaining or even increasing accuracy",
        "We show that maximizing the mutual information between spline positions and labels solves both problems, and in the absence of labels, one can instead maximize the mutual information between spline positions and the input, in the style of InfoGANs [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "Our contributions in this paper are: (i) the embedding trick to enable smooth conditional branching, a novel neural decision graph that is uncountable and operates in continuous decision space, an architectural constraint to enforce a semantic relationship between latent parameters of functions in consecutive layers, a regularizer that maximizes mutual information to utilize and specialize splines, and (v) a new differentiable quantization method for estimating soft entropies",
        "Skipping non-parametric functions for convolutional neural networks and assuming sparse activations for decision graphs, we can describe their behavior with the graphical models shown in Figure 1",
        "The continuous position distribution P plays an important role in utilizing SplineNets properly, e.g. if all \u03c6i\u2019s that are dynamically generated by samples are the same, the model reduces to a regular convolutional neural networks",
        "We presented the concept of SplineNets, a novel and practical method for realizing conditional neural networks using embedded continuous manifolds",
        "Our results dramatically reduce runtime complexity and computation costs of convolutional neural networks while maintaining or even increasing accuracy"
    ],
    "summary": [
        "There is a growing body of literature applying conditioning to CNNs, where only parts of the network are selectively active to save runtime complexity and compute.",
        "This effectively makes the branching factor of the underlying decision graph uncountable, and indexes the infinitely many choices with a continuous position parameter.",
        "Our contributions in this paper are: (i) the embedding trick to enable smooth conditional branching, a novel neural decision graph that is uncountable and operates in continuous decision space, an architectural constraint to enforce a semantic relationship between latent parameters of functions in consecutive layers, a regularizer that maximizes mutual information to utilize and specialize splines, and (v) a new differentiable quantization method for estimating soft entropies.",
        "Such that Tji for each node j in layer i has the same form but different weights, a neural decision graph (NDG) can be described by the graphical model shown on the right.",
        "Decision parameter generation The splines S\u03b8i of the hierarchical SplineNets all have knots C\u03b8i,k that live in the same space as \u03b8i, which is RMi .",
        "The additional costs compared to a regular fully connected layer besides the projection is the generation of \u03c9 via the spline function S\u03c9i , which has a time complexity O, and a memory complexity of O(K\u03c9i M iM i+1), where K\u03c9i is the number of knots.",
        "The continuous position distribution P plays an important role in utilizing SplineNets properly, e.g. if all \u03c6i\u2019s that are dynamically generated by samples are the same, the model reduces to a regular CNN.",
        "We experimented with model type M \u2208 {D, H} (D is dynamic-only, H is hierarchical), decision type T \u2208 {C, D} (C is 1\u00d71 convolution, D is dot product), and the knot rank R \u2208 {3, 4}.",
        "Figure 8 shows how the accuracy, model size and runtime complexity for a single sample are affected when going from two to five knots.",
        "The most powerful SplineNet tested is H(5)-D-R3, with hierarchical, dense projections for each filter, and it matches the accuracy of ResNet-110 with almost half the number of FLOPS.",
        "We used SplineNets with the same parameters of LeNet-32, combined with the more powerful rank3 knots and dot product decisions.",
        "We presented the concept of SplineNets, a novel and practical method for realizing conditional neural networks using embedded continuous manifolds.",
        "Like other conditional computation techniques, SplineNets have the added benefit of allowing further efficiency gains through compression, for instance by pruning layers [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] in an energy-aware manner [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], by substitution of filters with smaller ones [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], or by quantizing [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] or binarizing [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] weights.",
        "While these methods are mostly orthogonal to our work, one should take care when sparsifying knots independently"
    ],
    "headline": "We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. CoRR, abs/1701.06538, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.06538"
        },
        {
            "id": "2",
            "entry": "[2] Yani Ioannou, Duncan P. Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew Brown, and Antonio Criminisi. Decision forests, convolutional networks and the models inbetween. CoRR, abs/1603.01250, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.01250"
        },
        {
            "id": "3",
            "entry": "[3] Chao Xiong, Xiaowei Zhao, Danhang Tang, Jayashree Karlekar, Shuicheng Yan, and Tae-Kyun Kim. Conditional convolutional neural network for modality-aware face recognition. In ICCV, pages 3667\u20133675. IEEE Computer Society, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiong%2C%20Chao%20Zhao%2C%20Xiaowei%20Tang%2C%20Danhang%20Karlekar%2C%20Jayashree%20Conditional%20convolutional%20neural%20network%20for%20modality-aware%20face%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiong%2C%20Chao%20Zhao%2C%20Xiaowei%20Tang%2C%20Danhang%20Karlekar%2C%20Jayashree%20Conditional%20convolutional%20neural%20network%20for%20modality-aware%20face%20recognition%202015"
        },
        {
            "id": "4",
            "entry": "[4] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Deep convolutional decision jungle for image classification. CoRR, abs/1706.02003, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02003"
        },
        {
            "id": "5",
            "entry": "[5] Ufuk Can Bicici, Cem Keskin, and Lale Akarun. Conditional information gain networks. In Pattern Recognition (ICPR), 2018 24th International Conference on. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bicici%2C%20Ufuk%20Can%20Keskin%2C%20Cem%20Akarun%2C%20Lale%20Conditional%20information%20gain%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bicici%2C%20Ufuk%20Can%20Keskin%2C%20Cem%20Akarun%2C%20Lale%20Conditional%20information%20gain%20networks%202018"
        },
        {
            "id": "6",
            "entry": "[6] Samuel Rota Bul and Peter Kontschieder. Neural decision forests for semantic image labelling. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 81\u201388, 06 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bul%2C%20Samuel%20Rota%20Kontschieder%2C%20Peter%20Neural%20decision%20forests%20for%20semantic%20image%20labelling%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bul%2C%20Samuel%20Rota%20Kontschieder%2C%20Peter%20Neural%20decision%20forests%20for%20semantic%20image%20labelling%202014"
        },
        {
            "id": "7",
            "entry": "[7] Ludovic Denoyer and Patrick Gallinari. Deep sequential neural network. CoRR, abs/1410.0510, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.0510"
        },
        {
            "id": "8",
            "entry": "[8] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo. Deep neural decision forests. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 4190\u20134194, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kontschieder%2C%20Peter%20Fiterau%2C%20Madalina%20Criminisi%2C%20Antonio%20Bulo%2C%20Samuel%20Rota%20Deep%20neural%20decision%20forests%202016-07-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kontschieder%2C%20Peter%20Fiterau%2C%20Madalina%20Criminisi%2C%20Antonio%20Bulo%2C%20Samuel%20Rota%20Deep%20neural%20decision%20forests%202016-07-09"
        },
        {
            "id": "9",
            "entry": "[9] Suhang Wang, Charu Aggarwal, and Huan Liu. Using a random forest to inspire a neural network and improving on it. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 1\u20139, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Suhang%20Aggarwal%2C%20Charu%20Liu%2C%20Huan%20Using%20a%20random%20forest%20to%20inspire%20a%20neural%20network%20and%20improving%20on%20it%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Suhang%20Aggarwal%2C%20Charu%20Liu%2C%20Huan%20Using%20a%20random%20forest%20to%20inspire%20a%20neural%20network%20and%20improving%20on%20it%202017"
        },
        {
            "id": "10",
            "entry": "[10] C. Murdock and F. D. l. Torre. Additive component analysis. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 673\u2013681, July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murdock%2C%20C.%20l.%20Torre%2C%20F.D.%20Additive%20component%20analysis%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murdock%2C%20C.%20l.%20Torre%2C%20F.D.%20Additive%20component%20analysis%202017-07"
        },
        {
            "id": "11",
            "entry": "[11] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. CoRR, abs/1609.09106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.09106"
        },
        {
            "id": "12",
            "entry": "[12] Bert De Brabandere, Xu Jia, Tinne Tuytelaars, and Luc Van Gool. Dynamic filter networks. CoRR, abs/1605.09673, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09673"
        },
        {
            "id": "13",
            "entry": "[13] Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Trans. Graph., 36(4):42:1\u201342:13, July 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holden%2C%20Daniel%20Komura%2C%20Taku%20Saito%2C%20Jun%20Phase-functioned%20neural%20networks%20for%20character%20control%202017-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holden%2C%20Daniel%20Komura%2C%20Taku%20Saito%2C%20Jun%20Phase-functioned%20neural%20networks%20for%20character%20control%202017-07"
        },
        {
            "id": "14",
            "entry": "[14] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. CoRR, abs/1506.02025, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02025"
        },
        {
            "id": "15",
            "entry": "[15] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "16",
            "entry": "[16] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "17",
            "entry": "[17] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03657"
        },
        {
            "id": "18",
            "entry": "[18] Jamie Shotton, Sebastian Nowozin, Toby Sharp, John Winn, Pushmeet Kohli, and Antonio Criminisi. Decision jungles: Compact and rich models for classification. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201913, pages 234\u2013242, USA, 2013. Curran Associates Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shotton%2C%20Jamie%20Nowozin%2C%20Sebastian%20Sharp%2C%20Toby%20Winn%2C%20John%20Decision%20jungles%3A%20Compact%20and%20rich%20models%20for%20classification%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shotton%2C%20Jamie%20Nowozin%2C%20Sebastian%20Sharp%2C%20Toby%20Winn%2C%20John%20Decision%20jungles%3A%20Compact%20and%20rich%20models%20for%20classification%202013"
        },
        {
            "id": "19",
            "entry": "[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "20",
            "entry": "[20] Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pages 2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lecun%2C%20Yann%20Bottou%2C%20Lon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lecun%2C%20Yann%20Bottou%2C%20Lon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "21",
            "entry": "[21] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1207.0580"
        },
        {
            "id": "22",
            "entry": "[22] Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C4%B1n%20Agarwal%2C%20Ashish%20Barham%2C%20Paul%20Brevdo%2C%20Eugene%20TensorFlow%3A%20Large-scale%20machine%20learning%20on%20heterogeneous%20systems%202015"
        },
        {
            "id": "23",
            "entry": "[23] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dynamic routing between capsules. CoRR, abs/1710.09829, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09829"
        },
        {
            "id": "24",
            "entry": "[24] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "25",
            "entry": "[25] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient transfer learning. CoRR, abs/1611.06440, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06440"
        },
        {
            "id": "26",
            "entry": "[26] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. CoRR, abs/1611.05128, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05128"
        },
        {
            "id": "27",
            "entry": "[27] Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "28",
            "entry": "[28] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. CoRR, abs/1512.06473, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.06473"
        },
        {
            "id": "29",
            "entry": "[29] Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefanet: Training low bitwidth convolutional neural networks with low bitwidth gradients. CoRR, abs/1606.06160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06160"
        },
        {
            "id": "30",
            "entry": "[30] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. CoRR, abs/1702.03044, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03044"
        },
        {
            "id": "31",
            "entry": "[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. CoRR, abs/1603.05279, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1603.05279"
        }
    ]
}
