{
    "filename": "7445-neural-guided-constraint-logic-programming-for-program-synthesis.pdf",
    "metadata": {
        "title": "Neural Guided Constraint Logic Programming for Program Synthesis",
        "author": "Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William Byrd, Matthew Might, Raquel Urtasun, Richard Zemel",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7445-neural-guided-constraint-logic-programming-for-program-synthesis.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren\u2019s internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems."
    },
    "keywords": [
        {
            "term": "program synthesis",
            "url": "https://en.wikipedia.org/wiki/program_synthesis"
        },
        {
            "term": "constraint logic programming",
            "url": "https://en.wikipedia.org/wiki/constraint_logic_programming"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "relational programming",
            "url": "https://en.wikipedia.org/wiki/relational_programming"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "Programming by example",
            "url": "https://en.wikipedia.org/wiki/Programming_by_example"
        },
        {
            "term": "programming language",
            "url": "https://en.wikipedia.org/wiki/programming_language"
        },
        {
            "term": "Directed Acyclic Graph",
            "url": "https://en.wikipedia.org/wiki/Directed_Acyclic_Graph"
        },
        {
            "term": "r x",
            "url": "https://en.wikipedia.org/wiki/R_X"
        }
    ],
    "highlights": [
        "The constraint logic programming language miniKanren uses the relational programming paradigm, where programmers write relations instead of functions",
        "The symbolic system we use is a constraint logic programming system called miniKanren1[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], chominiKanren ML Agent output program expands candidate sen for its ability to encode synthesis problems Figure 1: Neural Guided Synthesis Approach that are difficult to express in other systems.\n1The name \u201cKanren\u201d comes from the Japanese word for \u201crelation\u201d.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "We explore two models for scoring constraints: Recurrent Neural Network (RNN) and Graph Neural Network (GNN) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "Relations are a generalization of functions: a function f with n parameters can be expressed as a relation R with n + 1 parameters, e.g., (f x) = y implies (R x y)",
        "If at some point we find a fully specified P that satisfies all relevant constraints, P is a solution to the Programming by example problem",
        "We present our neural guided synthesis approach summarized in Figure 3"
    ],
    "key_statements": [
        "The constraint logic programming language miniKanren uses the relational programming paradigm, where programmers write relations instead of functions",
        "The symbolic system we use is a constraint logic programming system called miniKanren1[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], chominiKanren ML Agent output program expands candidate sen for its ability to encode synthesis problems Figure 1: Neural Guided Synthesis Approach that are difficult to express in other systems.\n1The name \u201cKanren\u201d comes from the Japanese word for \u201crelation\u201d.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "We explore two models for scoring constraints: Recurrent Neural Network (RNN) and Graph Neural Network (GNN) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "Relations are a generalization of functions: a function f with n parameters can be expressed as a relation R with n + 1 parameters, e.g., (f x) = y implies (R x y)",
        "If at some point we find a fully specified P that satisfies all relevant constraints, P is a solution to the Programming by example problem",
        "We present our neural guided synthesis approach summarized in Figure 3"
    ],
    "summary": [
        "The constraint logic programming language miniKanren uses the relational programming paradigm, where programmers write relations instead of functions.",
        "Works generally fall under three categories: differentiable programming [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], direct synthesis [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], and neural guided search [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "We take integrating with a symbolic system further: we use its internal representation as input to the neural model.",
        "The symbolic system we use is a constraint logic programming system called miniKanren1[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], chominiKanren ML Agent output program expands candidate sen for its ability to encode synthesis problems Figure 1: Neural Guided Synthesis Approach that are difficult to express in other systems.",
        "MiniKanren searches for a candidate program that satisfies the recursive constraints imposed by the input/output examples.",
        "Our model uses these constraints to score candidate programs and guide miniKanren\u2019s search.",
        "We contribute a novel form of neural guided synthesis, where we use a symbolic system\u2019s internal representations to solve an auxiliary problem of constraint scoring using neural embeddings.",
        "Later works like [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] introduce a \u201cRecursive-Reverse-Recursive Neural Network\u201d to generate a program tree conditioned on input/output embeddings.",
        "This section describes the constraint logic programming language miniKanren and its use for program synthesis.",
        "A miniKanren program internally represents a query as a constraint tree built out of conjunctions, disjunctions, and calls to relations.",
        "In Figure 3, we show portions of the constraint tree representing a PBE problem with two input/output pairs.",
        "MiniKanren represents the PBE problem in terms of a disjunction of candidate partial programs, and the constraints that must be satisfied for the partial program to be consistent with the examples.",
        "The use of graph or tree structure to represent programs [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] and constraints [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] is not unprecedented.",
        "We compare two miniKanren search strategies that do not use a neural guide, three of our neural-guided models, and RobustFill with a generous beam size.",
        "We programmatically generate training data by querying in miniKanren, where the program, inputs, and outputs are all unknown.",
        "We use 500 generated problems for training, each with 5 input/output examples.",
        "We compare against a baseline RNN model that does not take constraints as input: instead, it computes embeddings of the input, output, and the candidate partial program using an LSTM, scores the concatenated embeddings using a MLP.",
        "All three neural guided models performed better than symbolic methods in our tests, with the RNN+Constraints model solving all but one problem.",
        "We have built a neural guided synthesis model that works directly with miniKanren\u2019s constraint representations, and a transparent implementation of miniKanren available at https://github.com/xuexue/neuralkanren.",
        "These results indicate that our approach is a promising stepping stone towards more general computation"
    ],
    "headline": "We present a method for solving Programming By Example  problems by using a neural model to guide the search of a constraint logic programming system called miniKanren",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient descent. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neelakantan%2C%20Arvind%20Le%2C%20Quoc%20V.%20Sutskever%2C%20Ilya%20Neural%20programmer%3A%20Inducing%20latent%20programs%20with%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neelakantan%2C%20Arvind%20Le%2C%20Quoc%20V.%20Sutskever%2C%20Ilya%20Neural%20programmer%3A%20Inducing%20latent%20programs%20with%20gradient%20descent%202016"
        },
        {
            "id": "2",
            "entry": "[2] Scott Reed and Nando de Freitas. Neural programmer-interpreters. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%20Reed%20and%20Nando%20de%20Freitas%20Neural%20programmerinterpreters%20International%20Conference%20on%20Learning%20Representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scott%20Reed%20and%20Nando%20de%20Freitas%20Neural%20programmerinterpreters%20International%20Conference%20on%20Learning%20Representations%202016"
        },
        {
            "id": "3",
            "entry": "[3] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.5401"
        },
        {
            "id": "4",
            "entry": "[4] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. RobustFill: Neural program learning under noisy I/O. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 990\u2013998, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20RobustFill%3A%20Neural%20program%20learning%20under%20noisy%20I/O%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20RobustFill%3A%20Neural%20program%20learning%20under%20noisy%20I/O%202017-08"
        },
        {
            "id": "5",
            "entry": "[5] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20Emilio%20Mohamed%2C%20Abdel-rahman%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20Emilio%20Mohamed%2C%20Abdel-rahman%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202017"
        },
        {
            "id": "6",
            "entry": "[6] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balog%2C%20Matej%20Gaunt%2C%20Alexander%20L.%20Brockschmidt%2C%20Marc%20Nowozin%2C%20Sebastian%20Deepcoder%3A%20Learning%20to%20write%20programs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balog%2C%20Matej%20Gaunt%2C%20Alexander%20L.%20Brockschmidt%2C%20Marc%20Nowozin%2C%20Sebastian%20Deepcoder%3A%20Learning%20to%20write%20programs%202017"
        },
        {
            "id": "7",
            "entry": "[7] Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani. Neural-guided deductive search for real-time program synthesis from examples. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalyan%2C%20Ashwin%20Mohta%2C%20Abhishek%20Polozov%2C%20Oleksandr%20Batra%2C%20Dhruv%20Neural-guided%20deductive%20search%20for%20real-time%20program%20synthesis%20from%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalyan%2C%20Ashwin%20Mohta%2C%20Abhishek%20Polozov%2C%20Oleksandr%20Batra%2C%20Dhruv%20Neural-guided%20deductive%20search%20for%20real-time%20program%20synthesis%20from%20examples%202018"
        },
        {
            "id": "8",
            "entry": "[8] William E. Byrd and Daniel P. Friedman. From variadic functions to variadic relations. In Proceedings of the 2006 Scheme and Functional Programming Workshop, University of Chicago Technical Report TR-2006-06, pages 105\u2013117, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20William%20E.%20Friedman%2C%20Daniel%20P.%20From%20variadic%20functions%20to%20variadic%20relations%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20William%20E.%20Friedman%2C%20Daniel%20P.%20From%20variadic%20functions%20to%20variadic%20relations%202006"
        },
        {
            "id": "9",
            "entry": "[9] Jason Hemann and Daniel P. Friedman. \u03bckanren: A minimal functional core for relational programming. In Scheme and Functional Programming Workshop 2013, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hemann%2C%20Jason%20Friedman%2C%20Daniel%20P.%20%CE%BCkanren%3A%20A%20minimal%20functional%20core%20for%20relational%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hemann%2C%20Jason%20Friedman%2C%20Daniel%20P.%20%CE%BCkanren%3A%20A%20minimal%20functional%20core%20for%20relational%20programming%202013"
        },
        {
            "id": "10",
            "entry": "[10] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "11",
            "entry": "[11] John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from input-output examples. In ACM SIGPLAN Notices, volume 50, pages 229\u2013239. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feser%2C%20John%20K.%20Chaudhuri%2C%20Swarat%20Dillig%2C%20Isil%20Synthesizing%20data%20structure%20transformations%20from%20input-output%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feser%2C%20John%20K.%20Chaudhuri%2C%20Swarat%20Dillig%2C%20Isil%20Synthesizing%20data%20structure%20transformations%20from%20input-output%20examples%202015"
        },
        {
            "id": "12",
            "entry": "[12] Aws Albarghouthi, Sumit Gulwani, and Zachary Kincaid. Recursive program synthesis. In International Conference on Computer Aided Verification, pages 934\u2013950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Albarghouthi%2C%20Aws%20Gulwani%2C%20Sumit%20Kincaid%2C%20Zachary%20Recursive%20program%20synthesis",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Albarghouthi%2C%20Aws%20Gulwani%2C%20Sumit%20Kincaid%2C%20Zachary%20Recursive%20program%20synthesis"
        },
        {
            "id": "13",
            "entry": "[13] Peter-Michael Osera and Steve Zdancewic. Type-and-example-directed program synthesis. In ACM SIGPLAN Notices, volume 50, pages 619\u2013630. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osera%2C%20Peter-Michael%20Zdancewic%2C%20Steve%20Type-and-example-directed%20program%20synthesis%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osera%2C%20Peter-Michael%20Zdancewic%2C%20Steve%20Type-and-example-directed%20program%20synthesis%202015"
        },
        {
            "id": "14",
            "entry": "[14] Phillip D. Summers. A methodology for lisp program construction from examples. Journal of the ACM (JACM), 24(1):161\u2013175, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Summers%2C%20Phillip%20D.%20A%20methodology%20for%20lisp%20program%20construction%20from%20examples%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Summers%2C%20Phillip%20D.%20A%20methodology%20for%20lisp%20program%20construction%20from%20examples%201977"
        },
        {
            "id": "15",
            "entry": "[15] Alan W. Biermann. The inference of regular lisp programs from examples. IEEE transactions on Systems, Man, and Cybernetics, 8(8):585\u2013600, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biermann%2C%20Alan%20W.%20The%20inference%20of%20regular%20lisp%20programs%20from%20examples%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biermann%2C%20Alan%20W.%20The%20inference%20of%20regular%20lisp%20programs%20from%20examples%201978"
        },
        {
            "id": "16",
            "entry": "[16] William E. Byrd, Michael Ballantyne, Gregory Rosenblatt, and Matthew Might. A unified approach to solving seven programming problems (functional pearl). Proceedings of the ACM on Programming Languages, 1(ICFP):8, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20William%20E.%20Ballantyne%2C%20Michael%20Rosenblatt%2C%20Gregory%20Might%2C%20Matthew%20A%20unified%20approach%20to%20solving%20seven%20programming%20problems%20%28functional%20pearl%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20William%20E.%20Ballantyne%2C%20Michael%20Rosenblatt%2C%20Gregory%20Might%2C%20Matthew%20A%20unified%20approach%20to%20solving%20seven%20programming%20problems%20%28functional%20pearl%29%202017"
        },
        {
            "id": "17",
            "entry": "[17] Xinyun Chen, Chang Liu, and Dawn Song. Towards synthesizing complex programs from input-output examples. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Towards%20synthesizing%20complex%20programs%20from%20input-output%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Towards%20synthesizing%20complex%20programs%20from%20input-output%20examples%202018"
        },
        {
            "id": "18",
            "entry": "[18] Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. In ACM SIGPLAN Notices, volume 46, pages 317\u2013330. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulwani%2C%20Sumit%20Automating%20string%20processing%20in%20spreadsheets%20using%20input-output%20examples%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulwani%2C%20Sumit%20Automating%20string%20processing%20in%20spreadsheets%20using%20input-output%20examples%202011"
        },
        {
            "id": "19",
            "entry": "[19] Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Deep API programmer: Learning to program with APIs. arXiv preprint arXiv:1704.04327, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04327"
        },
        {
            "id": "20",
            "entry": "[20] Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction. arXiv preprint arXiv:1608.04428, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04428"
        },
        {
            "id": "21",
            "entry": "[21] Kevin Ellis, Armando Solar-Lezama, and Josh Tenenbaum. Sampling for bayesian program learning. In Advances in Neural Information Processing Systems, pages 1297\u20131305, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ellis%2C%20Kevin%20Solar-Lezama%2C%20Armando%20Tenenbaum%2C%20Josh%20Sampling%20for%20bayesian%20program%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ellis%2C%20Kevin%20Solar-Lezama%2C%20Armando%20Tenenbaum%2C%20Josh%20Sampling%20for%20bayesian%20program%20learning%202016"
        },
        {
            "id": "22",
            "entry": "[22] Tim Rockt\u00e4schel and Sebastian Riedel. End-to-end differentiable proving. In Advances in Neural Information Processing Systems, pages 3788\u20133800, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockt%C3%A4schel%2C%20Tim%20Riedel%2C%20Sebastian%20End-to-end%20differentiable%20proving%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rockt%C3%A4schel%2C%20Tim%20Riedel%2C%20Sebastian%20End-to-end%20differentiable%20proving%202017"
        },
        {
            "id": "23",
            "entry": "[23] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "24",
            "entry": "[24] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allamanis%2C%20Miltiadis%20Brockschmidt%2C%20Marc%20Khademi%2C%20Mahmoud%20Learning%20to%20represent%20programs%20with%20graphs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allamanis%2C%20Miltiadis%20Brockschmidt%2C%20Marc%20Khademi%2C%20Mahmoud%20Learning%20to%20represent%20programs%20with%20graphs%202018"
        },
        {
            "id": "25",
            "entry": "[25] Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Tree-to-tree%20neural%20networks%20for%20program%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xinyun%20Liu%2C%20Chang%20Song%2C%20Dawn%20Tree-to-tree%20neural%20networks%20for%20program%20translation%202018"
        },
        {
            "id": "26",
            "entry": "[26] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Selsam%2C%20Daniel%20Lamm%2C%20Matthew%20Bunz%2C%20Benedikt%20Liang%2C%20Percy%20Learning%20a%20sat%20solver%20from%20single-bit%20supervision%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Selsam%2C%20Daniel%20Lamm%2C%20Matthew%20Bunz%2C%20Benedikt%20Liang%2C%20Percy%20Learning%20a%20sat%20solver%20from%20single-bit%20supervision%202018"
        },
        {
            "id": "27",
            "entry": "[27] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016"
        },
        {
            "id": "28",
            "entry": "[28] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1171\u20131179, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Samy%20Vinyals%2C%20Oriol%20Jaitly%2C%20Navdeep%20Shazeer%2C%20Noam%20Scheduled%20sampling%20for%20sequence%20prediction%20with%20recurrent%20neural%20networks%202015"
        },
        {
            "id": "29",
            "entry": "[29] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaul%2C%20Tom%20Quan%2C%20John%20Antonoglou%2C%20Ioannis%20Silver%2C%20David%20Prioritized%20experience%20replay%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schaul%2C%20Tom%20Quan%2C%20John%20Antonoglou%2C%20Ioannis%20Silver%2C%20David%20Prioritized%20experience%20replay%202016"
        },
        {
            "id": "30",
            "entry": "[30] T. Tieleman and G. Hinton. Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5%E2%80%94RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5%E2%80%94RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "31",
            "entry": "[31] William E. Byrd, Eric Holk, and Daniel P. Friedman. miniKanren, live and untagged: Quine generation via relational interpreters (programming pearl). In Proceedings of the 2012 Annual Workshop on Scheme and Functional Programming, pages 8\u201329. ACM, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20William%20E.%20Holk%2C%20Eric%20miniKanren%2C%20Daniel%20P.Friedman%20live%20and%20untagged%3A%20Quine%20generation%20via%20relational%20interpreters%20%28programming%20pearl%29%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20William%20E.%20Holk%2C%20Eric%20miniKanren%2C%20Daniel%20P.Friedman%20live%20and%20untagged%3A%20Quine%20generation%20via%20relational%20interpreters%20%28programming%20pearl%29%202012"
        }
    ]
}
