{
    "filename": "7512-recurrent-world-models-facilitate-policy-evolution.pdf",
    "metadata": {
        "title": "Recurrent World Models Facilitate Policy Evolution",
        "date": 2018,
        "author": "David Ha Google Brain Tokyo, Japan hadavid@google.com",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatiotemporal representations. The world model\u2019s extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper: https://worldmodels.github.io"
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "predictive model",
            "url": "https://en.wikipedia.org/wiki/predictive_model"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "evolution strategy",
            "url": "https://en.wikipedia.org/wiki/evolution_strategy"
        },
        {
            "term": "Gaussian process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_process"
        },
        {
            "term": "world model",
            "url": "https://en.wikipedia.org/wiki/world_model"
        },
        {
            "term": "evolution strategies",
            "url": "https://en.wikipedia.org/wiki/evolution_strategies"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        }
    ],
    "highlights": [
        "Humans develop a mental model of the world based on what they are able to perceive with their limited senses, learning abstract representations of both spatial and temporal aspects of sensory inputs",
        "Since there are a mere 867 parameters inside the linear C, evolutionary algorithms such as CMA-evolution strategies are well suited for this optimization task.\n1We find this task interesting because it is not difficult to train an agent to wobble around randomly generated tracks and obtain a mediocre score, CarRacing-v0 defines solving as getting average reward of 900 over 100 consecutive trials, which means the agent can only afford very few driving mistakes.\n2Although in principle, we can train V and M together in an end-to-end manner, we found that training each separately is more practical, achieves satisfactory results, and does not require exhaustive hyperparameter tuning",
        "Using data collected from the environment, PILCO uses a Gaussian process (GP) model to learn the system dynamics, and uses this model to sample many trajectories in order to train a controller to perform a desired task, such as swinging up a pendulum",
        "We have demonstrated the possibility of training an agent to perform tasks entirely inside of its simulated latent space world",
        "Agents that are trained incrementally to simulate reality may prove to be useful for transferring policies back to the real world"
    ],
    "key_statements": [
        "Humans develop a mental model of the world based on what they are able to perceive with their limited senses, learning abstract representations of both spatial and temporal aspects of sensory inputs",
        "Since there are a mere 867 parameters inside the linear C, evolutionary algorithms such as CMA-evolution strategies are well suited for this optimization task.\n1We find this task interesting because it is not difficult to train an agent to wobble around randomly generated tracks and obtain a mediocre score, CarRacing-v0 defines solving as getting average reward of 900 over 100 consecutive trials, which means the agent can only afford very few driving mistakes.\n2Although in principle, we can train V and M together in an end-to-end manner, we found that training each separately is more practical, achieves satisfactory results, and does not require exhaustive hyperparameter tuning",
        "Using data collected from the environment, PILCO uses a Gaussian process (GP) model to learn the system dynamics, and uses this model to sample many trajectories in order to train a controller to perform a desired task, such as swinging up a pendulum",
        "We have demonstrated the possibility of training an agent to perform tasks entirely inside of its simulated latent space world",
        "We may not want to waste cycles training an agent in the actual environment, but instead train the agent as many times as we want inside its simulated environment",
        "Agents that are trained incrementally to simulate reality may prove to be useful for transferring policies back to the real world",
        "Our approach may complement sim2real approaches outlined in previous work [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]"
    ],
    "summary": [
        "Humans develop a mental model of the world based on what they are able to perceive with their limited senses, learning abstract representations of both spatial and temporal aspects of sensory inputs.",
        "We do not need the V model to encode any real pixel frames during the generation process, so our agent will only train entirely in a more efficient latent space environment.",
        "Recent work [<a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>] combines the model-based approach with traditional model-free RL training by first initializing the policy network with the learned policy, but must subsequently rely on model-free methods to fine-tune this policy in the actual environment.",
        "To make it more difficult for our C to exploit deficiencies of M, we chose to use the MDN-RNN as the dynamics model of the distribution of possible outcomes in the actual environment, rather than merely predicting a deterministic future.",
        "If we set the temperature parameter to a very low value of \u03c4 = 0.1, effectively training our C with an M that is almost identical to a deterministic LSTM, the monsters inside this generated environment fail to shoot fireballs, no matter what the agent does, due to mode collapse.",
        "Whatever policy learned inside of this generated environment will achieve a perfect score of 2100 most of the time, but will obviously fail when unleashed into the harsh reality of the actual world, underperforming even a random policy.",
        "Using data collected from the environment, PILCO uses a Gaussian process (GP) model to learn the system dynamics, and uses this model to sample many trajectories in order to train a controller to perform a desired task, such as swinging up a pendulum.",
        "To get around the difficulty of training a dynamical model to learn directly from high-dimensional pixel images, researchers explored using neural networks to first learn a compressed representation of the video frames.",
        "A more recent work [<a class=\"ref-link\" id=\"c83\" href=\"#r83\">83</a>] presented a unifying framework for building an RNN-based general problem solver that can learn a world model of its environment and learn to reason about the future using this model.",
        "Future work will incorporate an iterative training procedure [<a class=\"ref-link\" id=\"c83\" href=\"#r83\">83</a>], where our controller actively explores parts of the environment that is beneficial to improve its world model.",
        "While modern storage devices can store large amounts of historical data generated using an iterative training procedure, our LSTM [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]based world model may not be able to store all of the recorded information inside of its weight connections.",
        "Future work will explore replacing the VAE and MDN-RNN with higher capacity models [<a class=\"ref-link\" id=\"c89\" href=\"#r89\">89</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c93\" href=\"#r93\">93</a>, <a class=\"ref-link\" id=\"c97\" href=\"#r97\">97</a>, <a class=\"ref-link\" id=\"c98\" href=\"#r98\">98</a>], or incorporating an external memory module [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c107\" href=\"#r107\">107</a>], if we want our agent to learn to explore more complicated worlds."
    ],
    "headline": "We explore fully replacing an actual reinforcement learning environment with a generated one, training our agent\u2019s controller C only inside of the environment generated by its own internal world model M, and transfer this policy back into the actual environment",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Alvernaz and J. Togelius. Autoencoder-augmented neuroevolution for visual doom playing. In Computational Intelligence and Games (CIG), 2017 IEEE Conference on, pages 1\u20138. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alvernaz%2C%20S.%20Togelius%2C%20J.%20Autoencoder-augmented%20neuroevolution%20for%20visual%20doom%20playing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alvernaz%2C%20S.%20Togelius%2C%20J.%20Autoencoder-augmented%20neuroevolution%20for%20visual%20doom%20playing%202017"
        },
        {
            "id": "2",
            "entry": "[2] T. M. Bartol Jr, C. Bromer, J. Kinney, M. A. Chirillo, J. N. Bourne, K. M. Harris, and T. J. Sejnowski. Nanoconnectomic upper bound on the variability of synaptic plasticity. Elife, 4, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartol%2C%20Jr%2C%20T.M.%20Bromer%2C%20C.%20Kinney%2C%20J.%20Chirillo%2C%20M.A.%20Nanoconnectomic%20upper%20bound%20on%20the%20variability%20of%20synaptic%20plasticity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartol%2C%20Jr%2C%20T.M.%20Bromer%2C%20C.%20Kinney%2C%20J.%20Chirillo%2C%20M.A.%20Nanoconnectomic%20upper%20bound%20on%20the%20variability%20of%20synaptic%20plasticity%202015"
        },
        {
            "id": "3",
            "entry": "[3] C. M. Bishop. Neural networks for pattern recognition (chapter 6). Oxford university press, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.M.%20Neural%20networks%20for%20pattern%20recognition%20%28chapter%206%29%201995"
        },
        {
            "id": "4",
            "entry": "[4] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. Preprint arXiv:1709.07857, Sept. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.07857"
        },
        {
            "id": "5",
            "entry": "[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym. Preprint arXiv:1606.01540, June 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "6",
            "entry": "[6] S. Carter, D. Ha, I. Johnson, and C. Olah. Experiments in handwriting with a neural network. Distill, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carter%2C%20S.%20Ha%2C%20D.%20Johnson%2C%20I.%20Olah%2C%20C.%20Experiments%20in%20handwriting%20with%20a%20neural%20network%202016"
        },
        {
            "id": "7",
            "entry": "[7] L. Chang and D. Y. Tsao. The code for facial identity in the primate brain. Cell, 169(6):1013\u20131028, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20L.%20Tsao%2C%20D.Y.%20The%20code%20for%20facial%20identity%20in%20the%20primate%20brain%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20L.%20Tsao%2C%20D.Y.%20The%20code%20for%20facial%20identity%20in%20the%20primate%20brain%202017"
        },
        {
            "id": "8",
            "entry": "[8] S. Chiappa, S. Racaniere, D. Wierstra, and S. Mohamed. Recurrent environment simulators. Preprint arXiv:1704.02254, Apr. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02254"
        },
        {
            "id": "9",
            "entry": "[9] M. Consalvo. Cheating: Gaining Advantage in Videogames (Chapter 5). The MIT Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Consalvo%2C%20M.%20Cheating%3A%20Gaining%20Advantage%20in%20Videogames%20%28Chapter%205%29%202007"
        },
        {
            "id": "10",
            "entry": "[10] M. Deisenroth and C. E. Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465\u2013472, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.%20Rasmussen%2C%20C.E.%20PILCO%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.%20Rasmussen%2C%20C.E.%20PILCO%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011"
        },
        {
            "id": "11",
            "entry": "[11] E. L. Denton et al. Unsupervised learning of disentangled representations from video. In Advances in Neural Information Processing Systems, pages 4417\u20134426, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.L.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.L.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017"
        },
        {
            "id": "12",
            "entry": "[12] S. Depeweg, J. M. Hern\u00e1ndez-Lobato, F. Doshi-Velez, and S. Udluft. Learning and policy search in stochastic dynamical systems with bayesian neural networks. Preprint arXiv:1605.07127, May 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07127"
        },
        {
            "id": "13",
            "entry": "[13] A. Dosovitskiy and V. Koltun. Learning to act by predicting the future. Preprint arXiv:1611.01779, Nov. 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01779"
        },
        {
            "id": "14",
            "entry": "[14] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. Rusu, A. Pritzel, and D. Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. Preprint arXiv:1701.08734, Jan. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "15",
            "entry": "[15] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel. Deep spatial autoencoders for visuomotor learning. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512\u2013519. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Tan%2C%20X.Y.%20Duan%2C%20Y.%20Darrell%2C%20T.%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Tan%2C%20X.Y.%20Duan%2C%20Y.%20Darrell%2C%20T.%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016"
        },
        {
            "id": "16",
            "entry": "[16] R. M. French. Catastrophic interference in connectionist networks: Can it be predicted, can it be prevented? In J. D. Cowan, G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing Systems 6, pages 1176\u20131177. Morgan-Kaufmann, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=French%2C%20R.M.%20Catastrophic%20interference%20in%20connectionist%20networks%3A%20Can%20it%20be%20predicted%2C%20can%20it%20be%20prevented%3F%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=French%2C%20R.M.%20Catastrophic%20interference%20in%20connectionist%20networks%3A%20Can%20it%20be%20predicted%2C%20can%20it%20be%20prevented%3F%201994"
        },
        {
            "id": "17",
            "entry": "[17] Y. Gal, R. McAllister, and C. E. Rasmussen. Improving PILCO with bayesian neural network dynamics models. In Data-Efficient Machine Learning workshop, ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Y.%20McAllister%2C%20R.%20Rasmussen%2C%20C.E.%20Improving%20PILCO%20with%20bayesian%20neural%20network%20dynamics%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Y.%20McAllister%2C%20R.%20Rasmussen%2C%20C.E.%20Improving%20PILCO%20with%20bayesian%20neural%20network%20dynamics%20models%202016"
        },
        {
            "id": "18",
            "entry": "[18] J. Gauci and K. O. Stanley. Autonomous evolution of topographic regularities in artificial neural networks. Neural Computation, 22(7):1860\u20131898, July 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gauci%2C%20J.%20Stanley%2C%20K.O.%20Autonomous%20evolution%20of%20topographic%20regularities%20in%20artificial%20neural%20networks%202010-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gauci%2C%20J.%20Stanley%2C%20K.O.%20Autonomous%20evolution%20of%20topographic%20regularities%20in%20artificial%20neural%20networks%202010-07"
        },
        {
            "id": "19",
            "entry": "[19] M. Gemici, C. Hung, A. Santoro, G. Wayne, S. Mohamed, D. Rezende, D. Amos, and T. Lillicrap. Generative temporal models with memory. Preprint arXiv:1702.04649, Feb. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04649"
        },
        {
            "id": "20",
            "entry": "[20] F. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, 12(10):2451\u20132471, Oct. 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gers%2C%20F.%20Schmidhuber%2C%20J.%20Cummins%2C%20F.%20Learning%20to%20forget%3A%20Continual%20prediction%20with%20LSTM%202000-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gers%2C%20F.%20Schmidhuber%2C%20J.%20Cummins%2C%20F.%20Learning%20to%20forget%3A%20Continual%20prediction%20with%20LSTM%202000-10"
        },
        {
            "id": "21",
            "entry": "[21] F. Gomez and J. Schmidhuber. Co-evolving recurrent neurons learn deep memory POMDPs. Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation, pages 491\u2013498, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomez%2C%20F.%20Schmidhuber%2C%20J.%20Co-evolving%20recurrent%20neurons%20learn%20deep%20memory%20POMDPs%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomez%2C%20F.%20Schmidhuber%2C%20J.%20Co-evolving%20recurrent%20neurons%20learn%20deep%20memory%20POMDPs%202005"
        },
        {
            "id": "22",
            "entry": "[22] F. Gomez, J. Schmidhuber, and R. Miikkulainen. Accelerated neural evolution through cooperatively coevolved synapses. Journal of Machine Learning Research, 9:937\u2013965, June 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomez%2C%20F.%20Schmidhuber%2C%20J.%20Miikkulainen%2C%20R.%20Accelerated%20neural%20evolution%20through%20cooperatively%20coevolved%20synapses%202008-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomez%2C%20F.%20Schmidhuber%2C%20J.%20Miikkulainen%2C%20R.%20Accelerated%20neural%20evolution%20through%20cooperatively%20coevolved%20synapses%202008-06"
        },
        {
            "id": "23",
            "entry": "[23] J. Gottlieb, P.-Y. Oudeyer, M. Lopes, and A. Baranes. Information-seeking, curiosity, and attention: computational and neural mechanisms. Trends in cognitive sciences, 17(11):585\u2013593, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gottlieb%2C%20J.%20Oudeyer%2C%20P.-Y.%20Lopes%2C%20M.%20Baranes%2C%20A.%20Information-seeking%2C%20curiosity%2C%20and%20attention%3A%20computational%20and%20neural%20mechanisms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gottlieb%2C%20J.%20Oudeyer%2C%20P.-Y.%20Lopes%2C%20M.%20Baranes%2C%20A.%20Information-seeking%2C%20curiosity%2C%20and%20attention%3A%20computational%20and%20neural%20mechanisms%202013"
        },
        {
            "id": "24",
            "entry": "[24] A. Graves. Generating sequences with recurrent neural networks. Preprint arXiv:1308.0850, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.0850"
        },
        {
            "id": "25",
            "entry": "[25] A. Graves. Hallucination with recurrent neural networks. https://youtu.be/-yX1SYeDHbg, 2015.",
            "url": "https://youtu.be/-yX1SYeDHbg"
        },
        {
            "id": "26",
            "entry": "[26] D. Ha. Evolving stable strategies. http://blog.otoro.net/, 2017.",
            "url": "http://blog.otoro.net/"
        },
        {
            "id": "27",
            "entry": "[27] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%20Ha%20A%20Dai%20and%20Q%20V%20Le%20Hypernetworks%20In%20International%20Conference%20on%20Learning%20Representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%20Ha%20A%20Dai%20and%20Q%20V%20Le%20Hypernetworks%20In%20International%20Conference%20on%20Learning%20Representations%202017"
        },
        {
            "id": "28",
            "entry": "[28] D. Ha and D. Eck. A neural representation of sketch drawings. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ha%2C%20D.%20Eck%2C%20D.%20A%20neural%20representation%20of%20sketch%20drawings%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ha%2C%20D.%20Eck%2C%20D.%20A%20neural%20representation%20of%20sketch%20drawings%202018"
        },
        {
            "id": "29",
            "entry": "[29] N. Hansen. The CMA evolution strategy: A tutorial. Preprint arXiv:1604.00772, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.00772"
        },
        {
            "id": "30",
            "entry": "[30] N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159\u2013195, June 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20N.%20Ostermeier%2C%20A.%20Completely%20derandomized%20self-adaptation%20in%20evolution%20strategies%202001-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20N.%20Ostermeier%2C%20A.%20Completely%20derandomized%20self-adaptation%20in%20evolution%20strategies%202001-06"
        },
        {
            "id": "31",
            "entry": "[31] M. Hausknecht, J. Lehman, R. Miikkulainen, and P. Stone. A neuroevolution approach to general Atari game playing. IEEE Transactions on Computational Intelligence and AI in Games, 6(4):355\u2013366, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausknecht%2C%20M.%20Lehman%2C%20J.%20Miikkulainen%2C%20R.%20Stone%2C%20P.%20A%20neuroevolution%20approach%20to%20general%20Atari%20game%20playing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausknecht%2C%20M.%20Lehman%2C%20J.%20Miikkulainen%2C%20R.%20Stone%2C%20P.%20A%20neuroevolution%20approach%20to%20general%20Atari%20game%20playing%202014"
        },
        {
            "id": "32",
            "entry": "[32] D. Hein, S. Depeweg, M. Tokic, S. Udluft, A. Hentschel, T. Runkler, and V. Sterzing. A benchmark environment motivated by industrial control problems. Preprint arXiv:1709.09480, Sept. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.09480"
        },
        {
            "id": "33",
            "entry": "[33] I. Higgins, A. Pal, A. A. Rusu, L. Matthey, C. P. Burgess, A. Pritzel, M. Botvinick, C. Blundell, and A. Lerchner. DARLA: Improving zero-shot transfer in reinforcement learning. Preprint arXiv:1707.08475, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08475"
        },
        {
            "id": "34",
            "entry": "[34] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "35",
            "entry": "[35] J. H\u00fcnermann. Self-driving cars in the browser. http://janhuenermann.com/, 2017.",
            "url": "http://janhuenermann.com/"
        },
        {
            "id": "36",
            "entry": "[36] S. Jang, J. Min, and C. Lee. Reinforcement car racing with A3C. https://goo.gl/58SKBp, 2017.",
            "url": "https://goo.gl/58SKBp"
        },
        {
            "id": "37",
            "entry": "[37] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. Journal of AI research, 4:237\u2013285, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaelbling%2C%20L.P.%20Littman%2C%20M.L.%20Moore%2C%20A.W.%20Reinforcement%20learning%3A%20a%20survey%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaelbling%2C%20L.P.%20Littman%2C%20M.L.%20Moore%2C%20A.W.%20Reinforcement%20learning%3A%20a%20survey%201996"
        },
        {
            "id": "38",
            "entry": "[38] G. Keller, T. Bonhoeffer, and M. H\u00fcbener. Sensorimotor mismatch signals in primary visual cortex of the behaving mouse. Neuron, 74(5):809 \u2013 815, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keller%2C%20G.%20Bonhoeffer%2C%20T.%20H%C3%BCbener%2C%20M.%20Sensorimotor%20mismatch%20signals%20in%20primary%20visual%20cortex%20of%20the%20behaving%20mouse%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keller%2C%20G.%20Bonhoeffer%2C%20T.%20H%C3%BCbener%2C%20M.%20Sensorimotor%20mismatch%20signals%20in%20primary%20visual%20cortex%20of%20the%20behaving%20mouse%202012"
        },
        {
            "id": "39",
            "entry": "[39] H. J. Kelley. Gradient theory of optimal flight paths. ARS Journal, 30(10):947\u2013954, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelley%2C%20H.J.%20Gradient%20theory%20of%20optimal%20flight%20paths%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kelley%2C%20H.J.%20Gradient%20theory%20of%20optimal%20flight%20paths%201960"
        },
        {
            "id": "40",
            "entry": "[40] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaskowski. VizDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, pages 341\u2013348, Santorini, Greece, Sep 2016. IEEE. The best paper award.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kempka%2C%20M.%20Wydmuch%2C%20M.%20Runc%2C%20G.%20Toczek%2C%20J.%20VizDoom%3A%20A%20Doom-based%20AI%20research%20platform%20for%20visual%20reinforcement%20learning%202016-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kempka%2C%20M.%20Wydmuch%2C%20M.%20Runc%2C%20G.%20Toczek%2C%20J.%20VizDoom%3A%20A%20Doom-based%20AI%20research%20platform%20for%20visual%20reinforcement%20learning%202016-09"
        },
        {
            "id": "41",
            "entry": "[41] M. Khan and O. Elibol. Car racing using reinforcement learning. https://goo.gl/neSBSx, 2016.",
            "url": "https://goo.gl/neSBSx"
        },
        {
            "id": "42",
            "entry": "[42] D. Kingma and M. Welling. Auto-encoding variational bayes. Preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "43",
            "entry": "[43] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20J.%20Pascanu%2C%20R.%20Rabinowitz%2C%20N.%20Veness%2C%20J.%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20J.%20Pascanu%2C%20R.%20Rabinowitz%2C%20N.%20Veness%2C%20J.%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017"
        },
        {
            "id": "44",
            "entry": "[44] O. Klimov. CarRacing-v0. http://gym.openai.com/, 2016.",
            "url": "http://gym.openai.com/"
        },
        {
            "id": "45",
            "entry": "[45] J. Koutnik, G. Cuccu, J. Schmidhuber, and F. Gomez. Evolving large-scale neural networks for visionbased reinforcement learning. Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, pages 1061\u20131068, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koutnik%2C%20J.%20Cuccu%2C%20G.%20Schmidhuber%2C%20J.%20Gomez%2C%20F.%20Evolving%20large-scale%20neural%20networks%20for%20visionbased%20reinforcement%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koutnik%2C%20J.%20Cuccu%2C%20G.%20Schmidhuber%2C%20J.%20Gomez%2C%20F.%20Evolving%20large-scale%20neural%20networks%20for%20visionbased%20reinforcement%20learning%202013"
        },
        {
            "id": "46",
            "entry": "[46] B. Lau. Using Keras and deep deterministic policy gradient to play TORCS. https://yanpanlau.github.io/, 2016.",
            "url": "https://yanpanlau.github.io/"
        },
        {
            "id": "47",
            "entry": "[47] J. Lehman and K. Stanley. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary Computation, 19(2):189\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20J.%20Stanley%2C%20K.%20Abandoning%20objectives%3A%20Evolution%20through%20the%20search%20for%20novelty%20alone%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20J.%20Stanley%2C%20K.%20Abandoning%20objectives%3A%20Evolution%20through%20the%20search%20for%20novelty%20alone%202011"
        },
        {
            "id": "48",
            "entry": "[48] M. Leinweber, D. R. Ward, J. M. Sobczak, A. Attinger, and G. B. Keller. A sensorimotor circuit in mouse cortex for visual flow predictions. Neuron, 95(6):1420 \u2013 1432.e5, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leinweber%2C%20M.%20Ward%2C%20D.R.%20Sobczak%2C%20J.M.%20Attinger%2C%20A.%20A%20sensorimotor%20circuit%20in%20mouse%20cortex%20for%20visual%20flow%20predictions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leinweber%2C%20M.%20Ward%2C%20D.R.%20Sobczak%2C%20J.M.%20Attinger%2C%20A.%20A%20sensorimotor%20circuit%20in%20mouse%20cortex%20for%20visual%20flow%20predictions%202017"
        },
        {
            "id": "49",
            "entry": "[49] L. Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie Mellon University, Pittsburgh, January 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20L.%20Reinforcement%20Learning%20for%20Robots%20Using%20Neural%20Networks%201993-01"
        },
        {
            "id": "50",
            "entry": "[50] S. Linnainmaa. The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors. Master\u2019s thesis, Univ. Helsinki, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linnainmaa%2C%20S.%20The%20representation%20of%20the%20cumulative%20rounding%20error%20of%20an%20algorithm%20as%20a%20taylor%20expansion%20of%20the%20local%20rounding%20errors%201970"
        },
        {
            "id": "51",
            "entry": "[51] M. O. R. Matthew Guzdial, Boyang Li. Game engine learning from video. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 3707\u20133713, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guzdial%2C%20M.O.R.Matthew%20Li%2C%20Boyang%20Game%20engine%20learning%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guzdial%2C%20M.O.R.Matthew%20Li%2C%20Boyang%20Game%20engine%20learning%20from%20video%202017"
        },
        {
            "id": "52",
            "entry": "[52] G. W. Maus, J. Fischer, and D. Whitney. Motion-dependent representation of space in area MT+. Neuron, 78(3):554\u2013562, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maus%2C%20G.W.%20Fischer%2C%20J.%20Whitney%2C%20D.%20Motion-dependent%20representation%20of%20space%20in%20area%20MT%2B%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maus%2C%20G.W.%20Fischer%2C%20J.%20Whitney%2C%20D.%20Motion-dependent%20representation%20of%20space%20in%20area%20MT%2B%202013"
        },
        {
            "id": "53",
            "entry": "[53] R. McAllister and C. E. Rasmussen. Data-efficient reinforcement learning in continuous state-action Gaussian-POMDPs. In Advances in Neural Information Processing Systems, pages 2037\u20132046, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllister%2C%20R.%20Rasmussen%2C%20C.E.%20Data-efficient%20reinforcement%20learning%20in%20continuous%20state-action%20Gaussian-POMDPs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllister%2C%20R.%20Rasmussen%2C%20C.E.%20Data-efficient%20reinforcement%20learning%20in%20continuous%20state-action%20Gaussian-POMDPs%202017"
        },
        {
            "id": "54",
            "entry": "[54] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing Atari with deep reinforcement learning. Preprint arXiv:1312.5602, Dec. 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "55",
            "entry": "[55] D. Mobbs, C. C. Hagan, T. Dalgleish, B. Silston, and C. Pr\u00e9vost. The ecology of human fear: survival optimization and the nervous system. Frontiers in neuroscience, 9:55, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mobbs%2C%20D.%20Hagan%2C%20C.C.%20Dalgleish%2C%20T.%20Silston%2C%20B.%20The%20ecology%20of%20human%20fear%3A%20survival%20optimization%20and%20the%20nervous%20system%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mobbs%2C%20D.%20Hagan%2C%20C.C.%20Dalgleish%2C%20T.%20Silston%2C%20B.%20The%20ecology%20of%20human%20fear%3A%20survival%20optimization%20and%20the%20nervous%20system%202015"
        },
        {
            "id": "56",
            "entry": "[56] P. W. Munro. A dual back-propagation scheme for scalar reinforcement learning. Proceedings of the Ninth Annual Conference of the Cognitive Science Society, Seattle, WA, pages 165\u2013176, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munro%2C%20P.W.%20A%20dual%20back-propagation%20scheme%20for%20scalar%20reinforcement%20learning%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munro%2C%20P.W.%20A%20dual%20back-propagation%20scheme%20for%20scalar%20reinforcement%20learning%201987"
        },
        {
            "id": "57",
            "entry": "[57] A. Nagabandi, G. Kahn, R. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Preprint arXiv:1708.02596, Aug. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02596"
        },
        {
            "id": "58",
            "entry": "[58] N. Nguyen and B. Widrow. The truck backer-upper: An example of self learning in neural networks. In Proceedings of the International Joint Conference on Neural Networks, pages 357\u2013363. IEEE Press, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20N.%20Widrow%2C%20B.%20The%20truck%20backer-upper%3A%20An%20example%20of%20self%20learning%20in%20neural%20networks%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20N.%20Widrow%2C%20B.%20The%20truck%20backer-upper%3A%20An%20example%20of%20self%20learning%20in%20neural%20networks%201989"
        },
        {
            "id": "59",
            "entry": "[59] N. Nortmann, S. Rekauzke, S. Onat, P. K\u00f6nig, and D. Jancke. Primary visual cortex represents the difference between past and present. Cerebral Cortex, 25(6):1427\u20131440, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nortmann%2C%20N.%20Rekauzke%2C%20S.%20Onat%2C%20S.%20K%C3%B6nig%2C%20P.%20Primary%20visual%20cortex%20represents%20the%20difference%20between%20past%20and%20present%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nortmann%2C%20N.%20Rekauzke%2C%20S.%20Onat%2C%20S.%20K%C3%B6nig%2C%20P.%20Primary%20visual%20cortex%20represents%20the%20difference%20between%20past%20and%20present%202015"
        },
        {
            "id": "60",
            "entry": "[60] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems, pages 2863\u20132871, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20J.%20Guo%2C%20X.%20Lee%2C%20H.%20Lewis%2C%20R.L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20Atari%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20J.%20Guo%2C%20X.%20Lee%2C%20H.%20Lewis%2C%20R.L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20Atari%20games%202015"
        },
        {
            "id": "61",
            "entry": "[61] P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265\u2013286, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oudeyer%2C%20P.-Y.%20Kaplan%2C%20F.%20Hafner%2C%20V.V.%20Intrinsic%20motivation%20systems%20for%20autonomous%20mental%20development%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oudeyer%2C%20P.-Y.%20Kaplan%2C%20F.%20Hafner%2C%20V.V.%20Intrinsic%20motivation%20systems%20for%20autonomous%20mental%20development%202007"
        },
        {
            "id": "62",
            "entry": "[62] P. Paquette. DoomTakeCover-v0. https://gym.openai.com/, 2016.",
            "url": "https://gym.openai.com/"
        },
        {
            "id": "63",
            "entry": "[63] M. Parker and B. D. Bryant. Neurovisual control in the Quake II environment. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):44\u201354, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parker%2C%20M.%20Bryant%2C%20B.D.%20Neurovisual%20control%20in%20the%20Quake%20II%20environment%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parker%2C%20M.%20Bryant%2C%20B.D.%20Neurovisual%20control%20in%20the%20Quake%20II%20environment%202012"
        },
        {
            "id": "64",
            "entry": "[64] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction"
        },
        {
            "id": "65",
            "entry": "[65] H.-J. Pi, B. Hangya, D. Kvitsiani, J. I. Sanders, Z. J. Huang, and A. Kepecs. Cortical interneurons that specialize in disinhibitory control. Nature, 503(7477):521, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pi%2C%20H.-J.%20Hangya%2C%20B.%20Kvitsiani%2C%20D.%20Sanders%2C%20J.I.%20Cortical%20interneurons%20that%20specialize%20in%20disinhibitory%20control%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pi%2C%20H.-J.%20Hangya%2C%20B.%20Kvitsiani%2C%20D.%20Sanders%2C%20J.I.%20Cortical%20interneurons%20that%20specialize%20in%20disinhibitory%20control%202013"
        },
        {
            "id": "66",
            "entry": "[66] L. Prieur. Deep-Q Learning for racecar reinforcement learning problem. https://goo.gl/VpDqSw, 2017.",
            "url": "https://goo.gl/VpDqSw"
        },
        {
            "id": "67",
            "entry": "[67] R. Q. Quiroga, L. Reddy, G. Kreiman, C. Koch, and I. Fried. Invariant visual representation by single neurons in the human brain. Nature, 435(7045):1102, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quiroga%2C%20R.Q.%20Reddy%2C%20L.%20Kreiman%2C%20G.%20Koch%2C%20C.%20Invariant%20visual%20representation%20by%20single%20neurons%20in%20the%20human%20brain%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quiroga%2C%20R.Q.%20Reddy%2C%20L.%20Kreiman%2C%20G.%20Koch%2C%20C.%20Invariant%20visual%20representation%20by%20single%20neurons%20in%20the%20human%20brain%202005"
        },
        {
            "id": "68",
            "entry": "[68] S. Racani\u00e8re, T. Weber, D. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 5694\u20135705, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Racani%C3%A8re%2C%20S.%20Weber%2C%20T.%20Reichert%2C%20D.%20Buesing%2C%20L.%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Racani%C3%A8re%2C%20S.%20Weber%2C%20T.%20Reichert%2C%20D.%20Buesing%2C%20L.%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "69",
            "entry": "[69] R. M. Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 97 2:285\u2013308, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ratcliff%2C%20R.M.%20Connectionist%20models%20of%20recognition%20memory%3A%20constraints%20imposed%20by%20learning%20and%20forgetting%20functions%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ratcliff%2C%20R.M.%20Connectionist%20models%20of%20recognition%20memory%3A%20constraints%20imposed%20by%20learning%20and%20forgetting%20functions%201990"
        },
        {
            "id": "71",
            "entry": "[71] D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. Preprint arXiv:1401.4082, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "72",
            "entry": "[72] T. Robinson and F. Fallside. Dynamic reinforcement driven error propagation networks with application to game playing. In Proceedings of the 11th Conference of the Cognitive Science Society, Ann Arbor, pages 836\u2013843, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robinson%2C%20T.%20Fallside%2C%20F.%20Dynamic%20reinforcement%20driven%20error%20propagation%20networks%20with%20application%20to%20game%20playing%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robinson%2C%20T.%20Fallside%2C%20F.%20Dynamic%20reinforcement%20driven%20error%20propagation%20networks%20with%20application%20to%20game%20playing%201989"
        },
        {
            "id": "73",
            "entry": "[73] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. Preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "74",
            "entry": "[74] J. Schmidhuber. Making the world differentiable: On using supervised learning fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Technische Universit\u00e4t M\u00fcnchen Tech. Report: FKI-126-90, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Making%20the%20world%20differentiable%3A%20On%20using%20supervised%20learning%20fully%20recurrent%20neural%20networks%20for%20dynamic%20reinforcement%20learning%20and%20planning%20in%20non-stationary%20environments%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Making%20the%20world%20differentiable%3A%20On%20using%20supervised%20learning%20fully%20recurrent%20neural%20networks%20for%20dynamic%20reinforcement%20learning%20and%20planning%20in%20non-stationary%20environments%201990"
        },
        {
            "id": "75",
            "entry": "[75] J. Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In Neural Networks, 1990., 1990 IJCNN International Joint Conference on, pages 253\u2013258. IEEE, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20An%20on-line%20algorithm%20for%20dynamic%20reinforcement%20learning%20and%20planning%20in%20reactive%20environments%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20An%20on-line%20algorithm%20for%20dynamic%20reinforcement%20learning%20and%20planning%20in%20reactive%20environments%201990"
        },
        {
            "id": "76",
            "entry": "[76] J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pages 222\u2013227, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20A%20possibility%20for%20implementing%20curiosity%20and%20boredom%20in%20model-building%20neural%20controllers%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20A%20possibility%20for%20implementing%20curiosity%20and%20boredom%20in%20model-building%20neural%20controllers%201990"
        },
        {
            "id": "77",
            "entry": "[77] J. Schmidhuber. Curious model-building control systems. In Neural Networks, 1991. 1991 IEEE International Joint Conference on, pages 1458\u20131463. IEEE, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Curious%20model-building%20control%20systems%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Curious%20model-building%20control%20systems%201991"
        },
        {
            "id": "78",
            "entry": "[78] J. Schmidhuber. Reinforcement learning in markovian and non-markovian environments. In Advances in neural information processing systems, pages 500\u2013506, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Reinforcement%20learning%20in%20markovian%20and%20non-markovian%20environments%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Reinforcement%20learning%20in%20markovian%20and%20non-markovian%20environments%201991"
        },
        {
            "id": "79",
            "entry": "[79] J. Schmidhuber. Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234\u2013242, 1992. (Based on TR FKI-148-91, TUM, 1991).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Learning%20complex%2C%20extended%20sequences%20using%20the%20principle%20of%20history%20compression%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Learning%20complex%2C%20extended%20sequences%20using%20the%20principle%20of%20history%20compression%201992"
        },
        {
            "id": "80",
            "entry": "[80] J. Schmidhuber. Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Connection Science, 18(2):173\u2013187, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Developmental%20robotics%2C%20optimal%20artificial%20curiosity%2C%20creativity%2C%20music%2C%20and%20the%20fine%20arts%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Developmental%20robotics%2C%20optimal%20artificial%20curiosity%2C%20creativity%2C%20music%2C%20and%20the%20fine%20arts%202006"
        },
        {
            "id": "81",
            "entry": "[81] J. Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010). IEEE Transactions on Autonomous Mental Development, 2(3):230\u2013247, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%2023230247%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%2023230247%202010"
        },
        {
            "id": "82",
            "entry": "[82] J. Schmidhuber. Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. Frontiers in Psychology, 4:313, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Powerplay%3A%20Training%20an%20increasingly%20general%20problem%20solver%20by%20continually%20searching%20for%20the%20simplest%20still%20unsolvable%20problem%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Powerplay%3A%20Training%20an%20increasingly%20general%20problem%20solver%20by%20continually%20searching%20for%20the%20simplest%20still%20unsolvable%20problem%202013"
        },
        {
            "id": "83",
            "entry": "[83] J. Schmidhuber. On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. Preprint arXiv:1511.09249, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.09249"
        },
        {
            "id": "84",
            "entry": "[84] J. Schmidhuber. One big net for everything. Preprint arXiv:1802.08864, Feb. 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08864"
        },
        {
            "id": "85",
            "entry": "[85] J. Schmidhuber and R. Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 2(1-2):125\u2013134, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Huber%2C%20R.%20Learning%20to%20generate%20artificial%20fovea%20trajectories%20for%20target%20detection%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J.%20Huber%2C%20R.%20Learning%20to%20generate%20artificial%20fovea%20trajectories%20for%20target%20detection%201991"
        },
        {
            "id": "86",
            "entry": "[86] J. Schmidhuber, J. Storck, and S. Hochreiter. Reinforcement driven information acquisition in nondeterministic environments. Technical Report FKI--94, TUM Department of Informatics, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J.%20Storck%2C%20J.%20Hochreiter%2C%20S.%20Reinforcement%20driven%20information%20acquisition%20in%20nondeterministic%20environments%201994"
        },
        {
            "id": "87",
            "entry": "[87] H. Schwefel. Numerical Optimization of Computer Models. John Wiley and Sons, Inc., New York, NY, USA, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwefel%2C%20H.%20Numerical%20Optimization%20of%20Computer%20Models%201977"
        },
        {
            "id": "88",
            "entry": "[88] F. Sehnke, C. Osendorfer, T. R\u00fcckstie\u00df, A. Graves, J. Peters, and J. Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551\u2013559, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sehnke%2C%20F.%20Osendorfer%2C%20C.%20R%C3%BCckstie%C3%9F%2C%20T.%20Graves%2C%20A.%20Parameter-exploring%20policy%20gradients%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sehnke%2C%20F.%20Osendorfer%2C%20C.%20R%C3%BCckstie%C3%9F%2C%20T.%20Graves%2C%20A.%20Parameter-exploring%20policy%20gradients%202010"
        },
        {
            "id": "89",
            "entry": "[89] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shazeer%2C%20N.%20Mirhoseini%2C%20A.%20Maziarz%2C%20K.%20Davis%2C%20A.%20Outrageously%20large%20neural%20networks%3A%20The%20sparsely-gated%20mixture-of-experts%20layer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shazeer%2C%20N.%20Mirhoseini%2C%20A.%20Maziarz%2C%20K.%20Davis%2C%20A.%20Outrageously%20large%20neural%20networks%3A%20The%20sparsely-gated%20mixture-of-experts%20layer%202017"
        },
        {
            "id": "90",
            "entry": "[90] D. Silver, H. van Hasselt, M. Hessel, T. Schaul, A. Guez, T. Harley, G. Dulac-Arnold, D. Reichert, N. Rabinowitz, A. Barreto, and T. Degris. The predictron: End-to-end learning and planning. Preprint arXiv:1612.08810, Dec. 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08810"
        },
        {
            "id": "91",
            "entry": "[91] R. K. Srivastava, B. R. Steunebrink, and J. Schmidhuber. First experiments with powerplay. Neural Networks, 41:130\u2013136, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20R.K.%20Steunebrink%2C%20B.R.%20Schmidhuber%2C%20J.%20First%20experiments%20with%20powerplay%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20R.K.%20Steunebrink%2C%20B.R.%20Schmidhuber%2C%20J.%20First%20experiments%20with%20powerplay%202013"
        },
        {
            "id": "92",
            "entry": "[92] K. O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary computation, 10(2):99\u2013127, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20K.O.%20Miikkulainen%2C%20R.%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanley%2C%20K.O.%20Miikkulainen%2C%20R.%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202002"
        },
        {
            "id": "93",
            "entry": "[93] J. Suarez. Language modeling with recurrent highway hypernetworks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3269\u20133278. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suarez%2C%20J.%20Language%20modeling%20with%20recurrent%20highway%20hypernetworks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suarez%2C%20J.%20Language%20modeling%20with%20recurrent%20highway%20hypernetworks%202017"
        },
        {
            "id": "94",
            "entry": "[94] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. Preprint arXiv:1712.06567, Dec. 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06567"
        },
        {
            "id": "95",
            "entry": "[95] R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine Learning Proceedings 1990, pages 216\u2013224.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Integrated%20architectures%20for%20learning%2C%20planning%2C%20and%20reacting%20based%20on%20approximating%20dynamic%20programming%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Integrated%20architectures%20for%20learning%2C%20planning%2C%20and%20reacting%20based%20on%20approximating%20dynamic%20programming%201990"
        },
        {
            "id": "96",
            "entry": "[96] R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Introduction%20to%20Reinforcement%20Learning%201998"
        },
        {
            "id": "97",
            "entry": "[97] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. Preprint arXiv:1609.03499, Sept. 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "98",
            "entry": "[98] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "99",
            "entry": "[99] N. Wahlstr\u00f6m, T. B. Sch\u00f6n, and M. P. Desienroth. Learning deep dynamical models from image pixels. In 17th IFAC Symposium on System Identification (SYSID), October 19-21, Beijing, China, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wahlstr%C3%B6m%2C%20N.%20Sch%C3%B6n%2C%20T.B.%20Desienroth%2C%20M.P.%20Learning%20deep%20dynamical%20models%20from%20image%20pixels.%20In%2017th%20IFAC%20Symposium%20on%20System%20Identification%20%28SYSID%29%2C%20October%2019-21%202015"
        },
        {
            "id": "100",
            "entry": "[100] N. Wahlstr\u00f6m, T. Sch\u00f6n, and M. Deisenroth. From pixels to torques: Policy learning with deep dynamical models. Preprint arXiv:1502.02251, June 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.02251"
        },
        {
            "id": "101",
            "entry": "[101] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pages 2746\u20132754, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watter%2C%20M.%20Springenberg%2C%20J.%20Boedecker%2C%20J.%20Riedmiller%2C%20M.%20Embed%20to%20control%3A%20A%20locally%20linear%20latent%20dynamics%20model%20for%20control%20from%20raw%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watter%2C%20M.%20Springenberg%2C%20J.%20Boedecker%2C%20J.%20Riedmiller%2C%20M.%20Embed%20to%20control%3A%20A%20locally%20linear%20latent%20dynamics%20model%20for%20control%20from%20raw%20images%202015"
        },
        {
            "id": "102",
            "entry": "[102] N. Watters, A. Tacchetti, T. Weber, R. Pascanu, P. Battaglia, and D. Zoran. Visual interaction networks. Preprint arXiv:1706.01433, June 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01433"
        },
        {
            "id": "104",
            "entry": "[104] P. J. Werbos. Learning how the world works: Specifications for predictive networks in robots and brains. In Proceedings of IEEE International Conference on Systems, Man and Cybernetics, N.Y., 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20P.J.%20Learning%20how%20the%20world%20works%3A%20Specifications%20for%20predictive%20networks%20in%20robots%20and%20brains%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Werbos%2C%20P.J.%20Learning%20how%20the%20world%20works%3A%20Specifications%20for%20predictive%20networks%20in%20robots%20and%20brains%201987"
        },
        {
            "id": "105",
            "entry": "[105] P. J. Werbos. Neural networks for control and system identification. In Decision and Control, 1989., Proceedings of the 28th IEEE Conference on, pages 260\u2013265. IEEE, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20P.J.%20Neural%20networks%20for%20control%20and%20system%20identification%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Werbos%2C%20P.J.%20Neural%20networks%20for%20control%20and%20system%20identification%201989"
        },
        {
            "id": "106",
            "entry": "[106] M. Wiering and M. van Otterlo. Reinforcement Learning. Springer, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiering%2C%20M.%20van%20Otterlo%2C%20M.%20Reinforcement%20Learning%202012"
        },
        {
            "id": "107",
            "entry": "[107] Y. Wu, G. Wayne, A. Graves, and T. Lillicrap. The Kanerva machine: A generative distributed memory. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Wayne%2C%20G.%20Graves%2C%20A.%20Lillicrap%2C%20T.%20The%20Kanerva%20machine%3A%20A%20generative%20distributed%20memory%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Y.%20Wayne%2C%20G.%20Graves%2C%20A.%20Lillicrap%2C%20T.%20The%20Kanerva%20machine%3A%20A%20generative%20distributed%20memory%202018"
        }
    ]
}
