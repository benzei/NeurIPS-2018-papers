{
    "filename": "7333-learning-to-decompose-and-disentangle-representations-for-video-prediction.pdf",
    "metadata": {
        "title": "Learning to Decompose and Disentangle Representations for Video Prediction",
        "author": "Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F. Fei-Fei, Juan Carlos Niebles",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7333-learning-to-decompose-and-disentangle-representations-for-video-prediction.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the highdimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we intuitively would do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision."
    },
    "keywords": [
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "video frame",
            "url": "https://en.wikipedia.org/wiki/video_frame"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "bouncing balls",
            "url": "https://en.wikipedia.org/wiki/Bouncing_Ball"
        },
        {
            "term": "auto encoder",
            "url": "https://en.wikipedia.org/wiki/auto_encoder"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        }
    ],
    "highlights": [
        "Our goal is to build intelligent systems that are capable of visually predicting and forecasting what will happen in video sequences",
        "We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the video we aim to predict into components, and disentangle each component into low-dimensional temporal dynamics that are easy to predict",
        "With appropriately specified generative model on future frames, Disentangled Predictive Auto-Encoder is able to learn both the video decomposition and the component disentanglement that are effective for video prediction without any explicit supervision on these latent variables",
        "We presented Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a video prediction framework that explicitly decomposes and disentangles the video representation and reduces the complexity of future frame prediction",
        "With an appropriately specified structural model, Disentangled Predictive Auto-Encoder is able to learn both the video decomposition and disentanglement that are effective for video prediction without any explicit supervision on these latent variables",
        "We further show that Disentangled Predictive Auto-Encoder is able to achieve reliable prediction directly from the pixel on the Bouncing Balls dataset involving complex object interaction, and recover physical properties without explicit modeling the physical states"
    ],
    "key_statements": [
        "Our goal is to build intelligent systems that are capable of visually predicting and forecasting what will happen in video sequences",
        "We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the video we aim to predict into components, and disentangle each component into low-dimensional temporal dynamics that are easy to predict",
        "With appropriately specified generative model on future frames, Disentangled Predictive Auto-Encoder is able to learn both the video decomposition and the component disentanglement that are effective for video prediction without any explicit supervision on these latent variables",
        "We evaluate Disentangled Predictive Auto-Encoder on two datasets: Moving MNIST [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] and Bouncing Balls [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We show that Disentangled Predictive Auto-Encoder is able to learn to decompose videos in the Moving MNIST dataset into individual digits, and further disentangles each component into the digit\u2019s appearance and its spatial location which is much easier to predict (Figure 1)",
        "We further demonstrate that Disentangled Predictive Auto-Encoder can be applied to the Bouncing Balls dataset, which has been used mainly for approaches that have access to full physical states [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "We show how we learn the Disentangled Predictive Auto-Encoder by optimizing the evidence lower bound in Section 3.3.\n3.1",
        "By training this structural generative model of future frames, the hope is to learn to produce good decomposition and disentangled representations of the video that reduce the complexity of frame prediction.\n3.2",
        "We evaluate the importance of both the decomposition and disentanglement of the video representation for frame prediction on the widely used Moving MNIST dataset [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "We evaluate how Disentangled Predictive Auto-Encoder can be applied to videos involving more complex interactions between components on the Bouncing Balls dataset [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>]",
        "We show that Disentangled Predictive Auto-Encoder is able to learn the decomposition and disentanglement automatically without explicit supervision, which plays an important role in the accurate prediction of Disentangled Predictive Auto-Encoder",
        "Our baseline model predicts the balls\u2019 motion independently and fails to identify the collision, and the two balls overlap each other in the predicted video. This shows that Disentangled Predictive Auto-Encoder is able to capture the important dependencies between components when predicting the pose vectors",
        "We presented Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a video prediction framework that explicitly decomposes and disentangles the video representation and reduces the complexity of future frame prediction",
        "With an appropriately specified structural model, Disentangled Predictive Auto-Encoder is able to learn both the video decomposition and disentanglement that are effective for video prediction without any explicit supervision on these latent variables",
        "We further show that Disentangled Predictive Auto-Encoder is able to achieve reliable prediction directly from the pixel on the Bouncing Balls dataset involving complex object interaction, and recover physical properties without explicit modeling the physical states"
    ],
    "summary": [
        "Our goal is to build intelligent systems that are capable of visually predicting and forecasting what will happen in video sequences.",
        "With appropriately specified generative model on future frames, DDPAE is able to learn both the video decomposition and the component disentanglement that are effective for video prediction without any explicit supervision on these latent variables.",
        "We show that DDPAE is able to learn to decompose videos in the Moving MNIST dataset into individual digits, and further disentangles each component into the digit\u2019s appearance and its spatial location which is much easier to predict (Figure 1).",
        "Our formulation encourages the model to decompose the video into components with low-dimensional temporal dynamics in the disentangled representation.",
        "By training this structural generative model of future frames, the hope is to learn to produce good decomposition and disentangled representations of the video that reduce the complexity of frame prediction.",
        "We can combine the video decomposition with the variational approximation as q(z1i:T |x1:T ), which directly infers the latent representations of each component.",
        "As shown in Figure 2(a) and (b), given an input sequence x1:T , for each component our model infers an initial pose vector z0i,P and the transition variables \u03b21i:T , from which we can iteratively obtain zti,P at each time step.",
        "The key contribution of our DDPAE is to both decompose and disentangle the video representation to simplify the challenging frame prediction task.",
        "We evaluate the importance of both the decomposition and disentanglement of the video representation for frame prediction on the widely used Moving MNIST dataset [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>].",
        "On the other hand, greatly simplifies the inference of the latent variables and the decoder by both decomposition and disentanglement, resulting in better prediction.",
        "This is shown in the qualitative results in Figure 3, where DDPAE successfully separates the two digits into two components and only needs to predict the low-dimensional pose vectors.",
        "We presented Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a video prediction framework that explicitly decomposes and disentangles the video representation and reduces the complexity of future frame prediction.",
        "With an appropriately specified structural model, DDPAE is able to learn both the video decomposition and disentanglement that are effective for video prediction without any explicit supervision on these latent variables.",
        "We further show that DDPAE is able to achieve reliable prediction directly from the pixel on the Bouncing Balls dataset involving complex object interaction, and recover physical properties without explicit modeling the physical states."
    ],
    "headline": "We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder , a framework that combines structured probabilistic models and deep networks to automatically  decompose the high-dimensional video that we aim to predict into components, and  disentangle each component to have low-dimensional temporal dynamics that are easier to predict",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese. Social LSTM: Human trajectory prediction in crowded spaces. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alahi%2C%20A.%20Goel%2C%20K.%20Ramanathan%2C%20V.%20Robicquet%2C%20A.%20Social%20LSTM%3A%20Human%20trajectory%20prediction%20in%20crowded%20spaces%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alahi%2C%20A.%20Goel%2C%20K.%20Ramanathan%2C%20V.%20Robicquet%2C%20A.%20Social%20LSTM%3A%20Human%20trajectory%20prediction%20in%20crowded%20spaces%202016"
        },
        {
            "id": "2",
            "entry": "[2] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglia%2C%20P.%20Pascanu%2C%20R.%20Lai%2C%20M.%20Rezende%2C%20D.J.%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglia%2C%20P.%20Pascanu%2C%20R.%20Lai%2C%20M.%20Rezende%2C%20D.J.%20Interaction%20networks%20for%20learning%20about%20objects%2C%20relations%202016"
        },
        {
            "id": "3",
            "entry": "[3] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20M.B.%20Ullman%2C%20T.%20Torralba%2C%20A.%20Tenenbaum%2C%20J.B.%20A%20compositional%20object-based%20approach%20to%20learning%20physical%20dynamics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20M.B.%20Ullman%2C%20T.%20Torralba%2C%20A.%20Tenenbaum%2C%20J.B.%20A%20compositional%20object-based%20approach%20to%20learning%20physical%20dynamics%202017"
        },
        {
            "id": "4",
            "entry": "[4] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "5",
            "entry": "[5] B. De Brabandere, X. Jia, T. Tuytelaars, and L. V. Gool. Dynamic filter networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brabandere%2C%20B.De%20Jia%2C%20X.%20Tuytelaars%2C%20T.%20Gool%2C%20L.V.%20Dynamic%20filter%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brabandere%2C%20B.De%20Jia%2C%20X.%20Tuytelaars%2C%20T.%20Gool%2C%20L.V.%20Dynamic%20filter%20networks%202016"
        },
        {
            "id": "6",
            "entry": "[6] E. Denton and V. Birodkar. Unsupervised learning of disentangled representations from video. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.%20Birodkar%2C%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.%20Birodkar%2C%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017"
        },
        {
            "id": "7",
            "entry": "[7] S. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari, G. E. Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eslami%2C%20S.A.%20Heess%2C%20N.%20Weber%2C%20T.%20Tassa%2C%20Y.%20Attend%2C%20infer%2C%20repeat%3A%20Fast%20scene%20understanding%20with%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eslami%2C%20S.A.%20Heess%2C%20N.%20Weber%2C%20T.%20Tassa%2C%20Y.%20Attend%2C%20infer%2C%20repeat%3A%20Fast%20scene%20understanding%20with%20generative%20models%202016"
        },
        {
            "id": "8",
            "entry": "[8] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through video prediction. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Goodfellow%2C%20I.%20Levine%2C%20S.%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016"
        },
        {
            "id": "9",
            "entry": "[9] K. Fragkiadaki, P. Agrawal, S. Levine, and J. Malik. Learning visual predictive models of physics for playing billiards. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fragkiadaki%2C%20K.%20Agrawal%2C%20P.%20Levine%2C%20S.%20Malik%2C%20J.%20Learning%20visual%20predictive%20models%20of%20physics%20for%20playing%20billiards%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fragkiadaki%2C%20K.%20Agrawal%2C%20P.%20Levine%2C%20S.%20Malik%2C%20J.%20Learning%20visual%20predictive%20models%20of%20physics%20for%20playing%20billiards%202016"
        },
        {
            "id": "10",
            "entry": "[10] A. Ghosh, V. Kulharia, A. Mukerjee, V. Namboodiri, and M. Bansal. Contextual rnn-gans for abstract reasoning diagram generation. AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghosh%2C%20A.%20Kulharia%2C%20V.%20Mukerjee%2C%20A.%20Namboodiri%2C%20V.%20Contextual%20rnn-gans%20for%20abstract%20reasoning%20diagram%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghosh%2C%20A.%20Kulharia%2C%20V.%20Mukerjee%2C%20A.%20Namboodiri%2C%20V.%20Contextual%20rnn-gans%20for%20abstract%20reasoning%20diagram%20generation%202017"
        },
        {
            "id": "11",
            "entry": "[11] K. Greff, A. Rasmus, M. Berglund, T. Hao, H. Valpola, and J. Schmidhuber. Tagger: Deep unsupervised perceptual grouping. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greff%2C%20K.%20Rasmus%2C%20A.%20Berglund%2C%20M.%20Hao%2C%20T.%20Tagger%3A%20Deep%20unsupervised%20perceptual%20grouping%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greff%2C%20K.%20Rasmus%2C%20A.%20Berglund%2C%20M.%20Hao%2C%20T.%20Tagger%3A%20Deep%20unsupervised%20perceptual%20grouping%202016"
        },
        {
            "id": "12",
            "entry": "[12] K. Greff, S. van Steenkiste, and J. Schmidhuber. Neural expectation maximization. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20Greff%20S%20van%20Steenkiste%20and%20J%20Schmidhuber%20Neural%20expectation%20maximization%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20Greff%20S%20van%20Steenkiste%20and%20J%20Schmidhuber%20Neural%20expectation%20maximization%20In%20NIPS%202017"
        },
        {
            "id": "13",
            "entry": "[13] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.04623"
        },
        {
            "id": "14",
            "entry": "[14] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Spatial%20transformer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Spatial%20transformer%20networks%202015"
        },
        {
            "id": "15",
            "entry": "[15] N. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=N%20Kalchbrenner%20A%20v%20d%20Oord%20K%20Simonyan%20I%20Danihelka%20O%20Vinyals%20A%20Graves%20and%20K%20Kavukcuoglu%20Video%20pixel%20networks%20In%20ICML%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=N%20Kalchbrenner%20A%20v%20d%20Oord%20K%20Simonyan%20I%20Danihelka%20O%20Vinyals%20A%20Graves%20and%20K%20Kavukcuoglu%20Video%20pixel%20networks%20In%20ICML%202017"
        },
        {
            "id": "16",
            "entry": "[16] M. Karl, M. Soelch, J. Bayer, and P. van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karl%2C%20M.%20Soelch%2C%20M.%20Bayer%2C%20J.%20van%20der%20Smagt%2C%20P.%20Deep%20variational%20bayes%20filters%3A%20Unsupervised%20learning%20of%20state%20space%20models%20from%20raw%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karl%2C%20M.%20Soelch%2C%20M.%20Bayer%2C%20J.%20van%20der%20Smagt%2C%20P.%20Deep%20variational%20bayes%20filters%3A%20Unsupervised%20learning%20of%20state%20space%20models%20from%20raw%20data%202016"
        },
        {
            "id": "17",
            "entry": "[17] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "18",
            "entry": "[18] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert. Activity forecasting. In ECCV, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kitani%2C%20K.M.%20Ziebart%2C%20B.D.%20Bagnell%2C%20J.A.%20Hebert%2C%20M.%20Activity%20forecasting%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kitani%2C%20K.M.%20Ziebart%2C%20B.D.%20Bagnell%2C%20J.A.%20Hebert%2C%20M.%20Activity%20forecasting%202012"
        },
        {
            "id": "19",
            "entry": "[19] A. R. Kosiorek, H. Kim, I. Posner, and Y. W. Teh. Sequential attend, infer, repeat: Generative modelling of moving objects. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kosiorek%2C%20A.R.%20Kim%2C%20H.%20Posner%2C%20I.%20Teh%2C%20Y.W.%20Sequential%20attend%2C%20infer%2C%20repeat%3A%20Generative%20modelling%20of%20moving%20objects%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kosiorek%2C%20A.R.%20Kim%2C%20H.%20Posner%2C%20I.%20Teh%2C%20Y.W.%20Sequential%20attend%2C%20infer%2C%20repeat%3A%20Generative%20modelling%20of%20moving%20objects%202018"
        },
        {
            "id": "20",
            "entry": "[20] T. Lan, T.-C. Chen, and S. Savarese. A hierarchical representation for future action prediction. In ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20T.%20Chen%2C%20T.-C.%20Savarese%2C%20S.%20A%20hierarchical%20representation%20for%20future%20action%20prediction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20T.%20Chen%2C%20T.-C.%20Savarese%2C%20S.%20A%20hierarchical%20representation%20for%20future%20action%20prediction%202014"
        },
        {
            "id": "21",
            "entry": "[21] W. Lotter, G. Kreiman, and D. Cox. Deep predictive coding networks for video prediction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08104"
        },
        {
            "id": "22",
            "entry": "[22] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016"
        },
        {
            "id": "23",
            "entry": "[23] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in atari games. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20J.%20Guo%2C%20X.%20Lee%2C%20H.%20Lewis%2C%20R.L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20J.%20Guo%2C%20X.%20Lee%2C%20H.%20Lewis%2C%20R.L.%20Action-conditional%20video%20prediction%20using%20deep%20networks%20in%20atari%20games%202015"
        },
        {
            "id": "24",
            "entry": "[24] M. Oliu, J. Selva, and S. Escalera. Folded recurrent neural networks for future video prediction. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oliu%2C%20M.%20Selva%2C%20J.%20Escalera%2C%20S.%20Folded%20recurrent%20neural%20networks%20for%20future%20video%20prediction%202018"
        },
        {
            "id": "25",
            "entry": "[25] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal video autoencoder with differentiable memory. arXiv preprint arXiv:1511.06309, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06309"
        },
        {
            "id": "26",
            "entry": "[26] C. Paxton, Y. Barnoy, K. Katyal, R. Arora, and G. D. Hager. Visual robot task planning. arXiv preprint arXiv:1804.00062, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00062"
        },
        {
            "id": "27",
            "entry": "[27] D. J. R. Gao and K. Grauman. Object-centric representation learning from unlabeled videos. In ACCV, November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20D.J.R.%20Grauman%2C%20K.%20Object-centric%20representation%20learning%20from%20unlabeled%20videos%202016-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20D.J.R.%20Grauman%2C%20K.%20Object-centric%20representation%20learning%20from%20unlabeled%20videos%202016-11"
        },
        {
            "id": "28",
            "entry": "[28] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and S. Chopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6604"
        },
        {
            "id": "29",
            "entry": "[29] D. J. Rezende, S. Mohamed, I. Danihelka, K. Gregor, and D. Wierstra. One-shot generalization in deep generative models. arXiv preprint arXiv:1603.05106, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.05106"
        },
        {
            "id": "30",
            "entry": "[30] B. Soran, A. Farhadi, and L. Shapiro. Generating notifications for missing actions: Don\u2019t forget to turn the lights off! In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soran%2C%20B.%20Farhadi%2C%20A.%20Shapiro%2C%20L.%20Generating%20notifications%20for%20missing%20actions%3A%20Don%E2%80%99t%20forget%20to%20turn%20the%20lights%20off%21%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soran%2C%20B.%20Farhadi%2C%20A.%20Shapiro%2C%20L.%20Generating%20notifications%20for%20missing%20actions%3A%20Don%E2%80%99t%20forget%20to%20turn%20the%20lights%20off%21%202015"
        },
        {
            "id": "31",
            "entry": "[31] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsupervised learning of video representations using lstms. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhudinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhudinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015"
        },
        {
            "id": "32",
            "entry": "[32] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20I.%20Vinyals%2C%20O.%20Le%2C%20Q.V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20I.%20Vinyals%2C%20O.%20Le%2C%20Q.V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "33",
            "entry": "[33] S. Tulyakov, A. Fitzgibbon, and S. Nowozin. Hybrid vae: Improving deep generative models using partial observations. arXiv preprint arXiv:1711.11566, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11566"
        },
        {
            "id": "34",
            "entry": "[34] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz. Mocogan: Decomposing motion and content for video generation. arXiv preprint arXiv:1707.04993, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04993"
        },
        {
            "id": "35",
            "entry": "[35] J. Van Amersfoort, A. Kannan, M. Ranzato, A. Szlam, D. Tran, and S. Chintala. Transformation-based models of video sequences. arXiv preprint arXiv:1701.08435, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08435"
        },
        {
            "id": "36",
            "entry": "[36] S. van Steenkiste, M. Chang, K. Greff, and J. Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Steenkiste%2C%20S.%20Chang%2C%20M.%20Greff%2C%20K.%20Schmidhuber%2C%20J.%20Relational%20neural%20expectation%20maximization%3A%20Unsupervised%20discovery%20of%20objects%20and%20their%20interactions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Steenkiste%2C%20S.%20Chang%2C%20M.%20Greff%2C%20K.%20Schmidhuber%2C%20J.%20Relational%20neural%20expectation%20maximization%3A%20Unsupervised%20discovery%20of%20objects%20and%20their%20interactions%202018"
        },
        {
            "id": "37",
            "entry": "[37] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. Decomposing motion and content for natural video sequence prediction. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20R.%20Yang%2C%20J.%20Hong%2C%20S.%20Lin%2C%20X.%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20R.%20Yang%2C%20J.%20Hong%2C%20S.%20Lin%2C%20X.%20Decomposing%20motion%20and%20content%20for%20natural%20video%20sequence%20prediction%202017"
        },
        {
            "id": "38",
            "entry": "[38] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee. Learning to generate long-term future via hierarchical prediction. arXiv preprint arXiv:1704.05831, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05831"
        },
        {
            "id": "39",
            "entry": "[39] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016"
        },
        {
            "id": "40",
            "entry": "[40] C. Vondrick and A. Torralba. Generating the future with adversarial transformers. CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20C.%20Torralba%2C%20A.%20Generating%20the%20future%20with%20adversarial%20transformers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20C.%20Torralba%2C%20A.%20Generating%20the%20future%20with%20adversarial%20transformers%202017"
        },
        {
            "id": "41",
            "entry": "[41] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An uncertain future: Forecasting from static images using variational autoencoders. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20J.%20Doersch%2C%20C.%20Gupta%2C%20A.%20Hebert%2C%20M.%20An%20uncertain%20future%3A%20Forecasting%20from%20static%20images%20using%20variational%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20J.%20Doersch%2C%20C.%20Gupta%2C%20A.%20Hebert%2C%20M.%20An%20uncertain%20future%3A%20Forecasting%20from%20static%20images%20using%20variational%20autoencoders%202016"
        },
        {
            "id": "42",
            "entry": "[42] J. Walker, K. Marino, A. Gupta, and M. Hebert. The pose knows: Video forecasting by generating pose futures. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20J.%20Marino%2C%20K.%20Gupta%2C%20A.%20Hebert%2C%20M.%20The%20pose%20knows%3A%20Video%20forecasting%20by%20generating%20pose%20futures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20J.%20Marino%2C%20K.%20Gupta%2C%20A.%20Hebert%2C%20M.%20The%20pose%20knows%3A%20Video%20forecasting%20by%20generating%20pose%20futures%202017"
        },
        {
            "id": "43",
            "entry": "[43] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xingjian%2C%20S.%20Chen%2C%20Z.%20Wang%2C%20H.%20Yeung%2C%20D.-Y.%20Convolutional%20lstm%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xingjian%2C%20S.%20Chen%2C%20Z.%20Wang%2C%20H.%20Yeung%2C%20D.-Y.%20Convolutional%20lstm%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015"
        },
        {
            "id": "44",
            "entry": "[44] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xue%2C%20T.%20Wu%2C%20J.%20Bouman%2C%20K.%20Freeman%2C%20B.%20Visual%20dynamics%3A%20Probabilistic%20future%20frame%20synthesis%20via%20cross%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xue%2C%20T.%20Wu%2C%20J.%20Bouman%2C%20K.%20Freeman%2C%20B.%20Visual%20dynamics%3A%20Probabilistic%20future%20frame%20synthesis%20via%20cross%20convolutional%20networks%202016"
        },
        {
            "id": "45",
            "entry": "[45] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View synthesis by appearance flow. In ECCV, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20T.%20Tulsiani%2C%20S.%20Sun%2C%20W.%20Malik%2C%20J.%20View%20synthesis%20by%20appearance%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20T.%20Tulsiani%2C%20S.%20Sun%2C%20W.%20Malik%2C%20J.%20View%20synthesis%20by%20appearance%20flow%202016"
        }
    ]
}
