{
    "filename": "8148-assessing-the-scalability-of-biologically-motivated-deep-learning-algorithms-and-architectures.pdf",
    "metadata": {
        "title": "Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures",
        "author": "Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey E. Hinton, Timothy Lillicrap",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8148-assessing-the-scalability-of-biologically-motivated-deep-learning-algorithms-and-architectures.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fullyand locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "multi-layer perceptrons",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptrons"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "The suitability of the backpropagation of error (BP) algorithm [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] for explaining learning in the brain was questioned soon after it was popularized [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "(3) We investigate the role of weight-sharing convolutions, which are key to performance on difficult datasets in artificial neural networks, by testing the effectiveness of locally connected architectures trained with backpropagation of error and variants of feedback alignment and target propagation",
        "Simplified difference target propagation We introduce SDTP as a simple modification to difference target propagation",
        "The only prior study of biologically motivated learning methods applied to this data was carried out by Lee et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]; this investigation was limited to difference target propagation with alternating updates and fully connected architectures",
        "Recent progress in machine learning has prompted a revival of this debate; where other approaches have failed, deep networks trained via backpropagation of error have been key to achieving impressive performance on difficult datasets such as ImageNet",
        "None of the algorithms proposed as approximations of backpropagation of error have been tested on the datasets that were instrumental in convincing the machine learning and neuroscience communities to revisit these questions"
    ],
    "key_statements": [
        "The suitability of the backpropagation of error (BP) algorithm [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] for explaining learning in the brain was questioned soon after it was popularized [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "In this work our contribution is threefold: (1) We examine the learning and performance of biologically-motivated algorithms on MNIST, CIFAR, and ImageNet. (2) We introduce variants of difference target propagation which eliminate significant lingering biologically implausible features from the algorithm",
        "(3) We investigate the role of weight-sharing convolutions, which are key to performance on difficult datasets in artificial neural networks, by testing the effectiveness of locally connected architectures trained with backpropagation of error and variants of feedback alignment and target propagation",
        "Simplified difference target propagation We introduce SDTP as a simple modification to difference target propagation",
        "The only prior study of biologically motivated learning methods applied to this data was carried out by Lee et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]; this investigation was limited to difference target propagation with alternating updates and fully connected architectures",
        "We hypothesize that the significant gap in performance between difference target propagation and the gradient-free SDTP on CIFAR-10 is due to the problems with inverting a low-entropy target in the output layer",
        "From the mid 1990s to 2010, work on applying insights from backpropagation of error to help understand learning in the brain declined precipitously",
        "Recent progress in machine learning has prompted a revival of this debate; where other approaches have failed, deep networks trained via backpropagation of error have been key to achieving impressive performance on difficult datasets such as ImageNet",
        "It is once again natural to wonder whether some approximation of backpropagation of error might underlie learning in the brain [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "None of the algorithms proposed as approximations of backpropagation of error have been tested on the datasets that were instrumental in convincing the machine learning and neuroscience communities to revisit these questions",
        "It is worth noting that our work here ignores one other significant issue with respect to the plausibility of feedback communication: backpropagation of error, feedback alignment, all of the target propagation variants, and most known activation propagation algorithms, still require distinct forward and backward phases"
    ],
    "summary": [
        "The suitability of the backpropagation of error (BP) algorithm [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] for explaining learning in the brain was questioned soon after it was popularized [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>].",
        "(3) We investigate the role of weight-sharing convolutions, which are key to performance on difficult datasets in artificial neural networks, by testing the effectiveness of locally connected architectures trained with BP and variants of FA and TP.",
        "In networks with invertible layers one could generate such targets by first finding a loss-optimal output activation hL and propagating it back using inverse transformations hl = f \u22121.",
        "When there are a small number of 1-hot targets (e.g. 10 classes), learning a good inverse mapping from these vectors back to the hidden activity of the penultimate hidden layer (e.g. 1000 units) might be problematic, since the inverse mapping cannot provide information that is both useful and unique to a particular input sample x.",
        "We extend SDTP by introducing a composite structure for the output layer hL = [o, z], where o is the predicted class distribution on which the loss is computed and z is an auxiliary output vector that is meant to provide additional information about activations of the penultimate layer hL\u22121.",
        "The only prior study of biologically motivated learning methods applied to this data was carried out by Lee et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]; this investigation was limited to DTP with alternating updates and fully connected architectures.",
        "As with MNIST, a BP trained convolutional network with shared weights performed better than its locally-connected variant.",
        "We hypothesize that the significant gap in performance between DTP and the gradient-free SDTP on CIFAR-10 is due to the problems with inverting a low-entropy target in the output layer.",
        "Years of research have led to a better understanding of efficient architectures, weight initialization, and optimizers for convolutional networks trained with backpropagation, and perhaps more effort is required to reach comparable results for biologically motivated algorithms and architectures.",
        "Recent progress in machine learning has prompted a revival of this debate; where other approaches have failed, deep networks trained via BP have been key to achieving impressive performance on difficult datasets such as ImageNet. It is once again natural to wonder whether some approximation of BP might underlie learning in the brain [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "We demonstrated that networks trained with SDTP without any weight sharing perform much worse than DTP, likely because of impoverished output targets.",
        "It is worth noting that our work here ignores one other significant issue with respect to the plausibility of feedback communication: BP, FA, all of the TP variants, and most known activation propagation algorithms, still require distinct forward and backward phases."
    ],
    "headline": "We present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ackley, David H, Hinton, Geoffrey E, and Sejnowski, Terrence J. A learning algorithm for boltzmann machines. Cognitive science, 9(1):147\u2013169, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ackley%2C%20David%20H.%20Hinton%2C%20Geoffrey%20E.%20Sejnowski%2C%20Terrence%20J.%20A%20learning%20algorithm%20for%20boltzmann%20machines%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ackley%2C%20David%20H.%20Hinton%2C%20Geoffrey%20E.%20Sejnowski%2C%20Terrence%20J.%20A%20learning%20algorithm%20for%20boltzmann%20machines%201985"
        },
        {
            "id": "2",
            "entry": "[2] Almeida, Luis B. A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In Artificial neural networks, pp. 102\u2013111. IEEE Press, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Almeida%2C%20Luis%20B.%20A%20learning%20rule%20for%20asynchronous%20perceptrons%20with%20feedback%20in%20a%20combinatorial%20environment%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Almeida%2C%20Luis%20B.%20A%20learning%20rule%20for%20asynchronous%20perceptrons%20with%20feedback%20in%20a%20combinatorial%20environment%201990"
        },
        {
            "id": "3",
            "entry": "[3] Bengio, Yoshua. How auto-encoders could provide credit assignment in deep networks via target propagation. arXiv preprint arXiv:1407.7906, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.7906"
        },
        {
            "id": "4",
            "entry": "[4] Bengio, Yoshua and Fischer, Asja. Early inference in energy-based models approximates back-propagation. arXiv preprint arXiv:1510.02777, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.02777"
        },
        {
            "id": "5",
            "entry": "[5] Bengio, Yoshua, Lee, Dong-Hyun, Bornschein, Jorg, Mesnard, Thomas, and Lin, Zhouhan. Towards biologically plausible deep learning. arXiv preprint arXiv:1502.04156, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.04156"
        },
        {
            "id": "6",
            "entry": "[6] Bengio, Yoshua, Scellier, Benjamin, Bilaniuk, Olexa, Sacramento, Joao, and Senn, Walter. Feedforward initialization for fast inference of deep generative networks is biologically plausible. arXiv preprint arXiv:1606.01651, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01651"
        },
        {
            "id": "7",
            "entry": "[7] Bengio, Yoshua, Mesnard, Thomas, Fischer, Asja, Zhang, Saizheng, and Wu, Yuhuai. Stdpcompatible approximation of backpropagation in an energy-based model. Neural computation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Mesnard%2C%20Thomas%20Fischer%2C%20Asja%20Zhang%2C%20Saizheng%20Stdpcompatible%20approximation%20of%20backpropagation%20in%20an%20energy-based%20model.%20Neural%20computation%202017"
        },
        {
            "id": "8",
            "entry": "[8] Crick, Francis. The recent excitement about neural networks. Nature, 337(6203):129\u2013132, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crick%2C%20Francis%20The%20recent%20excitement%20about%20neural%20networks%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crick%2C%20Francis%20The%20recent%20excitement%20about%20neural%20networks%201989"
        },
        {
            "id": "9",
            "entry": "[9] Dumoulin, Vincent and Visin, Francesco. A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.07285"
        },
        {
            "id": "10",
            "entry": "[10] Glorot, Xavier and Bengio, Yoshua. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "11",
            "entry": "[11] Grossberg, Stephen. Competitive learning: From interactive activation to adaptive resonance. Cognitive science, 11(1):23\u201363, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grossberg%2C%20Stephen%20Competitive%20learning%3A%20From%20interactive%20activation%20to%20adaptive%20resonance%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grossberg%2C%20Stephen%20Competitive%20learning%3A%20From%20interactive%20activation%20to%20adaptive%20resonance%201987"
        },
        {
            "id": "12",
            "entry": "[12] Guerguiev, Jordan, Lillicrap, Timothy P, and Richards, Blake A. Towards deep learning with segregated dendrites. ELife, 6:e22901, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guerguiev%2C%20Jordan%20Lillicrap%2C%20Timothy%20P.%20Richards%2C%20Blake%20A.%20Towards%20deep%20learning%20with%20segregated%20dendrites%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guerguiev%2C%20Jordan%20Lillicrap%2C%20Timothy%20P.%20Richards%2C%20Blake%20A.%20Towards%20deep%20learning%20with%20segregated%20dendrites%202017"
        },
        {
            "id": "13",
            "entry": "[13] Hinton, G.E. How to do backpropagation in a brain. NIPS 2007 Deep Learning Workshop, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20How%20to%20do%20backpropagation%20in%20a%20brain%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20How%20to%20do%20backpropagation%20in%20a%20brain%202007"
        },
        {
            "id": "14",
            "entry": "[14] Hinton, Geoffrey E and McClelland, James L. Learning representations by recirculation. In Neural information processing systems, pp. 358\u2013366. New York: American Institute of Physics, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20McClelland%2C%20James%20L.%20Learning%20representations%20by%20recirculation%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20McClelland%2C%20James%20L.%20Learning%20representations%20by%20recirculation%201988"
        },
        {
            "id": "15",
            "entry": "[15] Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "16",
            "entry": "[16] Kording, Konrad P and Konig, Peter. Supervised and unsupervised learning with two sites of synaptic integration. Journal of computational neuroscience, 11(3):207\u2013215, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kording%2C%20Konrad%20P.%20Konig%2C%20Peter%20Supervised%20and%20unsupervised%20learning%20with%20two%20sites%20of%20synaptic%20integration%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kording%2C%20Konrad%20P.%20Konig%2C%20Peter%20Supervised%20and%20unsupervised%20learning%20with%20two%20sites%20of%20synaptic%20integration%202001"
        },
        {
            "id": "17",
            "entry": "[17] Krizhevsky, Alex. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "18",
            "entry": "[18] Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "19",
            "entry": "[19] LeCun, Yann. Learning process in an asymmetric threshold network. In Disordered systems and biological organization, pp. 233\u2013240.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Learning%20process%20in%20an%20asymmetric%20threshold%20network",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Learning%20process%20in%20an%20asymmetric%20threshold%20network"
        },
        {
            "id": "20",
            "entry": "[20] LeCun, Yann. Modeles connexionnistes de lapprentissage. PhD thesis, PhD thesis, These de Doctorat, Universite Paris 6, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Modeles%20connexionnistes%20de%20lapprentissage%201987"
        },
        {
            "id": "21",
            "entry": "[21] Lee, Dong-Hyun, Zhang, Saizheng, Fischer, Asja, and Bengio, Yoshua. Difference target propagation. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 498\u2013515.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Dong-Hyun%20Zhang%2C%20Saizheng%20Fischer%2C%20Asja%20Bengio%2C%20Yoshua%20Difference%20target%20propagation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Dong-Hyun%20Zhang%2C%20Saizheng%20Fischer%2C%20Asja%20Bengio%2C%20Yoshua%20Difference%20target%20propagation"
        },
        {
            "id": "22",
            "entry": "[22] Lillicrap, Timothy P, Cownden, Daniel, Tweed, Douglas B, and Akerman, Colin J. Random feedback weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.0247"
        },
        {
            "id": "23",
            "entry": "[23] Lillicrap, Timothy P, Cownden, Daniel, Tweed, Douglas B, and Akerman, Colin J. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Cownden%2C%20Daniel%20Tweed%2C%20Douglas%20B.%20Akerman%2C%20Colin%20J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Cownden%2C%20Daniel%20Tweed%2C%20Douglas%20B.%20Akerman%2C%20Colin%20J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016"
        },
        {
            "id": "24",
            "entry": "[24] Movellan, Javier R. Contrastive hebbian learning in the continuous hopfield model. In Connectionist models: Proceedings of the 1990 summer school, pp. 10\u201317, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Movellan%2C%20Javier%20R.%20Contrastive%20hebbian%20learning%20in%20the%20continuous%20hopfield%20model%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Movellan%2C%20Javier%20R.%20Contrastive%20hebbian%20learning%20in%20the%20continuous%20hopfield%20model%201991"
        },
        {
            "id": "25",
            "entry": "[25] N\u00f8kland, Arild. Direct feedback alignment provides learning in deep neural networks. In Advances In Neural Information Processing Systems, pp. 1037\u20131045, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=N%C3%B8kland%2C%20Arild%20Direct%20feedback%20alignment%20provides%20learning%20in%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=N%C3%B8kland%2C%20Arild%20Direct%20feedback%20alignment%20provides%20learning%20in%20deep%20neural%20networks%202016"
        },
        {
            "id": "26",
            "entry": "[26] O\u2019Reilly, Randall C. Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. Neural computation, 8(5):895\u2013938, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Reilly%2C%20Randall%20C.%20Biologically%20plausible%20error-driven%20learning%20using%20local%20activation%20differences%3A%20The%20generalized%20recirculation%20algorithm%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=O%E2%80%99Reilly%2C%20Randall%20C.%20Biologically%20plausible%20error-driven%20learning%20using%20local%20activation%20differences%3A%20The%20generalized%20recirculation%20algorithm%201996"
        },
        {
            "id": "27",
            "entry": "[27] Ororbia, Alexander G and Mali, Ankur. Biologically motivated algorithms for propagating local target representations. arXiv preprint arXiv:1805.11703, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11703"
        },
        {
            "id": "28",
            "entry": "[28] Ororbia, Alexander G, Mali, Ankur, Kifer, Daniel, and Giles, C Lee. Conducting credit assignment by aligning local representations. arXiv preprint arXiv:1803.01834, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01834"
        },
        {
            "id": "29",
            "entry": "[29] Parisien, Christopher, Anderson, Charles H, and Eliasmith, Chris. Solving the problem of negative synaptic weights in cortical models. Neural computation, 20(6):1473\u20131494, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisien%2C%20Christopher%20Anderson%2C%20Charles%20H.%20Eliasmith%2C%20Chris%20Solving%20the%20problem%20of%20negative%20synaptic%20weights%20in%20cortical%20models%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisien%2C%20Christopher%20Anderson%2C%20Charles%20H.%20Eliasmith%2C%20Chris%20Solving%20the%20problem%20of%20negative%20synaptic%20weights%20in%20cortical%20models%202008"
        },
        {
            "id": "30",
            "entry": "[30] Pineda, Fernando J. Generalization of back-propagation to recurrent neural networks. Physical review letters, 59(19):2229, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pineda%2C%20Fernando%20J.%20Generalization%20of%20back-propagation%20to%20recurrent%20neural%20networks%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pineda%2C%20Fernando%20J.%20Generalization%20of%20back-propagation%20to%20recurrent%20neural%20networks%201987"
        },
        {
            "id": "31",
            "entry": "[31] Pineda, Fernando J. Dynamics and architecture for neural computation. Journal of Complexity, 4(3):216\u2013245, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pineda%2C%20Fernando%20J.%20Dynamics%20and%20architecture%20for%20neural%20computation%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pineda%2C%20Fernando%20J.%20Dynamics%20and%20architecture%20for%20neural%20computation%201988"
        },
        {
            "id": "32",
            "entry": "[32] Rumelhart, DE, Hinton, GE, and Williams, RJ. Learning representations by back-propagation errors. Nature, 323:533\u2013536, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagation%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagation%20errors%201986"
        },
        {
            "id": "33",
            "entry": "[33] Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "34",
            "entry": "[34] Sacramento, Joao, Costa, Rui Ponte, Bengio, Yoshua, and Senn, Walter. Dendritic error backpropagation in deep cortical microcircuits. arXiv preprint arXiv:1801.00062, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00062"
        },
        {
            "id": "35",
            "entry": "[35] Samadi, Arash, Lillicrap, Timothy P, and Tweed, Douglas B. Deep learning with dynamic spiking neurons and fixed feedback weights. Neural computation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samadi%2C%20Arash%20Lillicrap%2C%20Timothy%20P.%20Tweed%2C%20Douglas%20B.%20Deep%20learning%20with%20dynamic%20spiking%20neurons%20and%20fixed%20feedback%20weights.%20Neural%20computation%202017"
        },
        {
            "id": "36",
            "entry": "[36] Scellier, Benjamin and Bengio, Yoshua. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in computational neuroscience, 11, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scellier%2C%20Benjamin%20Bengio%2C%20Yoshua%20Equilibrium%20propagation%3A%20Bridging%20the%20gap%20between%20energy-based%20models%20and%20backpropagation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scellier%2C%20Benjamin%20Bengio%2C%20Yoshua%20Equilibrium%20propagation%3A%20Bridging%20the%20gap%20between%20energy-based%20models%20and%20backpropagation%202017"
        },
        {
            "id": "37",
            "entry": "[37] Springenberg, Jost Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller, Martin. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6806"
        },
        {
            "id": "38",
            "entry": "[38] Whittington, James CR and Bogacz, Rafal. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Whittington%2C%20James%20C.R.%20Bogacz%2C%20Rafal%20An%20approximation%20of%20the%20error%20backpropagation%20algorithm%20in%20a%20predictive%20coding%20network%20with%20local%20hebbian%20synaptic%20plasticity.%20Neural%20computation%202017"
        },
        {
            "id": "39",
            "entry": "[39] Xie, Xiaohui and Seung, H Sebastian. Equivalence of backpropagation and contrastive hebbian learning in a layered network. Neural computation, 15(2):441\u2013454, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Xiaohui%20Seung%2C%20H.Sebastian%20Equivalence%20of%20backpropagation%20and%20contrastive%20hebbian%20learning%20in%20a%20layered%20network%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Xiaohui%20Seung%2C%20H.Sebastian%20Equivalence%20of%20backpropagation%20and%20contrastive%20hebbian%20learning%20in%20a%20layered%20network%202003"
        }
    ]
}
