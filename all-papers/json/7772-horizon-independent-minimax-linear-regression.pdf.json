{
    "filename": "7772-horizon-independent-minimax-linear-regression.pdf",
    "metadata": {
        "date": 2018,
        "title": "Horizon-Independent Minimax Linear Regression",
        "author": "Alan Malek Laboratory for Information and Decision Systems",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7772-horizon-independent-minimax-linear-regression.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider online linear regression: at each round, an adversary reveals a covariate vector, the learner predicts a real value, the adversary reveals a label, and the learner suffers the squared prediction error. The aim is to minimize the difference between the cumulative loss and that of the linear predictor that is best in hindsight. Previous work demonstrated that the minimax optimal strategy is easy to compute recursively from the end of the game; this requires the entire sequence of covariate vectors in advance. We show that, once provided with a measure of the scale of the problem, we can invert the recursion and play the minimax strategy without knowing the future covariates. Further, we show that this forward recursion remains optimal even against adaptively chosen labels and covariates, provided that the adversary adheres to a set of constraints that prevent misrepresentation of the scale of the problem. This strategy is horizon-independent in that the regret and minimax strategies depend on the size of the constraint set and not on the time-horizon, and hence it incurs no more regret than the optimal strategy that knows in advance the number of rounds of the game. We also provide an interpretation of the minimax algorithm as a follow-the-regularized-leader strategy with a data-dependent regularizer and obtain an explicit expression for the minimax regret."
    },
    "keywords": [
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "minimax algorithm",
            "url": "https://en.wikipedia.org/wiki/minimax_algorithm"
        },
        {
            "term": "minimax strategy",
            "url": "https://en.wikipedia.org/wiki/minimax_strategy"
        }
    ],
    "highlights": [
        "Linear regression is a fundamental prediction problem in machine learning and statistics",
        "We have shown that for the adversarial covariates protocol with X = ABC(\u03a3, \u03b30), (MMS) is the minimax optimal strategy and receives \u03b30 regret",
        "We have presented the minimax optimal strategy for online linear regression where the covariate and label sequence are chosen adversarially and the measure of game length is a covariance budget instead of the number of rounds",
        "The minimax strategy is efficient and only needs to update two parameters: Pt, a summary of the covariance left in the budget, and st =",
        "One could interpret the results of our paper as finding a more natural way to measure the length of the game that admits a tractable minimax strategy"
    ],
    "key_statements": [
        "Linear regression is a fundamental prediction problem in machine learning and statistics",
        "We have shown that for the adversarial covariates protocol with X = ABC(\u03a3, \u03b30), (MMS) is the minimax optimal strategy and receives \u03b30 regret",
        "We have presented the minimax optimal strategy for online linear regression where the covariate and label sequence are chosen adversarially and the measure of game length is a covariance budget instead of the number of rounds",
        "The minimax strategy is efficient and only needs to update two parameters: Pt, a summary of the covariance left in the budget, and st =",
        "One could interpret the results of our paper as finding a more natural way to measure the length of the game that admits a tractable minimax strategy"
    ],
    "summary": [
        "Linear regression is a fundamental prediction problem in machine learning and statistics.",
        "We show that it is possible to remove the fixed-design and known-game-length requirement if we limit the adversary to play sequences that follow the Adversarial Covariate conditions.",
        "Since xT1 \u2208 A(\u03a3), Lemma 1 implies that the Pt matrices from the forwards and backwards recursions are equivalent, and (MMS) corresponds to the minimax strategy for the fixed-design game with P0\u2020 = \u03a3.",
        "The strategy (MMS) is minimax optimal for any covariate sequence xT1 \u2208 A(\u03a3) if the adversary plays covariates that meet the \u03a3 constraint with equality, which is quite restrictive.",
        "The rest of this section proves the main result of this paper: if the adversary plays in ABC(\u03a3, \u03b30) := A(\u03a3) \u2229 B(\u03a3) \u2229 C(\u03a3, \u03b30), (MMS) is minimax optimal.",
        "We show that the C condition eliminates these troublesome cases and the adversary exhausts the budget; the adversary plays x1T \u2208 A(\u03a3) which implies that that (MMS) is minimax optimal by results of the previous section.",
        "This goal of this section is to show i) that it is possible for the adversary to cause more regret by not using up the covariance budget, i.e. PT\u2020 \u03a0T , and ii) that the C conditions are sufficient to stop this.",
        "The instantaneous value-to-go is W = st Pt \u2212 \u03a0t\u2020 st + \u03b3t, the adversary causes more regret by continuing the game, and the optimal learner strategy is (MMS).",
        "The covariate sequence xT1 \u2208 A(P0\u2020), which confirms that (MMS) using the forward recursion is minimax optimal via Theorem 2.",
        "In reasoning about optimal strategies, Theorem 4 allows us to establish conditions when the learner is playing suboptimally and could be causing more regret.",
        "Theorem 4 applies to a fixed design game that is allowed to stop early, and we wish to reason about the adversarial covariate case.",
        "We will show something stronger: the optimal adversary strategy for the game in Figure 1 plays an x1T sequence in ABC and causes exactly \u03b30 regret against (MMS).",
        "We have shown that for the adversarial covariates protocol with X = ABC(\u03a3, \u03b30), (MMS) is the minimax optimal strategy and receives \u03b30 regret.",
        "We have presented the minimax optimal strategy for online linear regression where the covariate and label sequence are chosen adversarially and the measure of game length is a covariance budget instead of the number of rounds.",
        "What other game protocols can be reparameterized to admit efficient minimax strategies? As a general method, one could start with minimax algorithms for constrained cases search for parameterizations which preserve the optimality"
    ],
    "headline": "Once provided with a measure of the scale of the problem, we can invert the recursion and play the minimax strategy without knowing the future covariates",
    "reference_links": [
        {
            "id": "Azoury_2001_a",
            "entry": "Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Machine Learning, 43(3):211\u2013246, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azoury%2C%20Katy%20S.%20Warmuth%2C%20Manfred%20K.%20Relative%20loss%20bounds%20for%20on-line%20density%20estimation%20with%20the%20exponential%20family%20of%20distributions%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azoury%2C%20Katy%20S.%20Warmuth%2C%20Manfred%20K.%20Relative%20loss%20bounds%20for%20on-line%20density%20estimation%20with%20the%20exponential%20family%20of%20distributions%202001"
        },
        {
            "id": "Bartlett_et+al_2015_a",
            "entry": "Peter L. Bartlett, Wouter M. Koolen, Alan Malek, Manfred K. Warmuth, and Eiji Takimoto. Minimax fixed-design linear regression. In P. Gr\u00fcnwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Annual Conference on Learning Theory (COLT), pages 226\u2013239, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Koolen%2C%20Wouter%20M.%20Malek%2C%20Alan%20Warmuth%2C%20Manfred%20K.%20Minimax%20fixed-design%20linear%20regression%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Koolen%2C%20Wouter%20M.%20Malek%2C%20Alan%20Warmuth%2C%20Manfred%20K.%20Minimax%20fixed-design%20linear%20regression%202015"
        },
        {
            "id": "Cesa-Bianchi_et+al_1996_a",
            "entry": "Nicolo Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. Neural Networks, IEEE Transactions on, 7(3):604\u2013619, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicolo%20Long%2C%20Philip%20M.%20Warmuth%2C%20Manfred%20K.%20Worst-case%20quadratic%20loss%20bounds%20for%20prediction%20using%20linear%20functions%20and%20gradient%20descent%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20Nicolo%20Long%2C%20Philip%20M.%20Warmuth%2C%20Manfred%20K.%20Worst-case%20quadratic%20loss%20bounds%20for%20prediction%20using%20linear%20functions%20and%20gradient%20descent%201996"
        },
        {
            "id": "Foster_1991_a",
            "entry": "Dean P. Foster. Prediction in the worst case. Annals of Statistics, 19(2):1084\u20131090, 1991. David A Harville. Matrix algebra from a statistician\u2019s perspective, volume 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foster%2C%20Dean%20P.%20Prediction%20in%20the%20worst%20case%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foster%2C%20Dean%20P.%20Prediction%20in%20the%20worst%20case%201991"
        },
        {
            "id": "Springer_1997_a",
            "entry": "Springer, 1997. Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1\u201363, 1997. Wouter M. Koolen, Alan Malek, and Peter L. Bartlett. Efficient minimax strategies for square loss games. In Advances in Neural Information Processing Systems, pages 3230\u20133238, 2014. Wouter M. Koolen, Alan Malek, Peter L. Bartlett, and Yasin Abbasi. Minimax time series prediction.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%2C%201997.%20Jyrki%20Kivinen%20Warmuth%2C%20Manfred%20K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springer%2C%201997.%20Jyrki%20Kivinen%20Warmuth%2C%20Manfred%20K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997"
        },
        {
            "id": "Moroshko_2015_a",
            "entry": "In C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2548\u20132556. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5730-minimax-time-series-prediction.pdf. Edward Moroshko and Koby Crammer. Weighted last-step min\u2013max algorithm with improved sub-logarithmic regret. Theoretical Computer Science, 558:107\u2013124, 2014. Eiji Takimoto and Manfred K. Warmuth. The minimax strategy for Gaussian density estimation. In 13th COLT, pages 100\u2013106, 2000. Volodimir G. Vovk. Aggregating strategies. In Proc. Third Workshop on Computational Learning Theory, pages 371\u2013383. Morgan Kaufmann, 1990. Volodya Vovk. Competitive on-line linear regression. Advances in Neural Information Processing Systems, pages 364\u2013370, 1998.",
            "url": "http://papers.nips.cc/paper/5730-minimax-time-series-prediction.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moroshko%20Crammer%2C%20Koby%20Weighted%20last-step%20min%E2%80%93max%20algorithm%20with%20improved%20sub-logarithmic%20regret%202015"
        }
    ]
}
