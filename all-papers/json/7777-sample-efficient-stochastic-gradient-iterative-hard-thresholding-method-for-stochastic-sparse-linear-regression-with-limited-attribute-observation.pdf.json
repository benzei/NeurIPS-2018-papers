{
    "filename": "7777-sample-efficient-stochastic-gradient-iterative-hard-thresholding-method-for-stochastic-sparse-linear-regression-with-limited-attribute-observation.pdf",
    "metadata": {
        "title": "Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method for Stochastic Sparse Linear Regression with Limited Attribute Observation",
        "author": "Tomoya Murata, Taiji Suzuki",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7777-sample-efficient-stochastic-gradient-iterative-hard-thresholding-method-for-stochastic-sparse-linear-regression-with-limited-attribute-observation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We develop new stochastic gradient methods for efficiently solving sparse linear regression in a partial attribute observation setting, where learners are only allowed to observe a fixed number of actively chosen attributes per example at training and prediction times. It is shown that the methods achieve essentially a sample complexity of O(1/\u03b5) to attain an error of \u03b5 under a variant of restricted eigenvalue condition, and the rate has better dependency on the problem dimension than existing methods. Particularly, if the smallest magnitude of the non-zero components of the optimal solution is not too small, the rate of our proposed Hybrid algorithm can be boosted to near the minimax optimal sample complexity of full information algorithms. The core ideas are (i) efficient construction of an unbiased gradient estimator by the iterative usage of the hard thresholding operator for configuring an exploration algorithm; and (ii) an adaptive combination of the exploration and an exploitation algorithms for quickly identifying the support of the optimum and efficiently searching the optimal parameter in its support. Experimental results are presented to validate our theoretical findings and the superiority of our proposed methods."
    },
    "keywords": [
        {
            "term": "hybrid algorithm",
            "url": "https://en.wikipedia.org/wiki/hybrid_algorithm"
        },
        {
            "term": "optimal solution",
            "url": "https://en.wikipedia.org/wiki/optimal_solution"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        }
    ],
    "highlights": [
        "In real-world sequential prediction scenarios, the features of examples are typically highdimensional and construction of the all features for each example may be expensive or impossible",
        "Main contribution In this paper, we focus on stochastic i.i.d. sparse linear regression in the limited attribute observation setting and propose new sample efficient algorithms in this setting",
        "We presented sample efficient algorithms for stochastic sparse linear regression problem with limited attribute observation",
        "We developed Exploration algorithm based on an efficient construction of an unbiased gradient estimator by taking advantage of the iterative usage of hard thresholding in the updates of predictors",
        "We have shown that Exploration and Hybrid achieve a sample complexity of O(1/\u03b5) with much better dependency on the problem dimension than the ones in existing work"
    ],
    "key_statements": [
        "In real-world sequential prediction scenarios, the features of examples are typically highdimensional and construction of the all features for each example may be expensive or impossible",
        "Main contribution In this paper, we focus on stochastic i.i.d. sparse linear regression in the limited attribute observation setting and propose new sample efficient algorithms in this setting",
        "We presented sample efficient algorithms for stochastic sparse linear regression problem with limited attribute observation",
        "We developed Exploration algorithm based on an efficient construction of an unbiased gradient estimator by taking advantage of the iterative usage of hard thresholding in the updates of predictors",
        "We have shown that Exploration and Hybrid achieve a sample complexity of O(1/\u03b5) with much better dependency on the problem dimension than the ones in existing work"
    ],
    "summary": [
        "In real-world sequential prediction scenarios, the features of examples are typically highdimensional and construction of the all features for each example may be expensive or impossible.",
        "In non-agnostic settings, the proposed algorithm achieves a sample complexity of O(1/\u03b5)1, but the rate has bad dependency on the problem dimension.",
        "If the updated solution \u03b8 is sparse, computing \u03b8 x requires only observing the attributes of x which correspond to the support of \u03b8 and there exists no need to estimate Ex\u223cDX [xx ], which has a potentially large variance.",
        "We can show that Algorithm 3 achieves at least the same convergence rate as Exploration, and thanks to the usage of Exploitation, its rate can be much boosted when the smallest magnitude of the non-zero components of the optimal solution is not too small.",
        "Under the settings of Theorem 4.5, the necessary number of observed samples to achieve P (L \u2212 L(\u03b8\u2217) \u2264 \u03b5) \u2265 1 \u2212 \u03b4 for Algorithm 3 is",
        "The methods of [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] and [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] solve the stochastic linear regression with limited attribute observation, but the limited information setting is only assumed at training time and not at prediction time, which is different from ours.",
        "The convergence rate in non-agnostic cases is much worse than the ones of ours in terms of the dependency on the problem dimension d, but the method has high versatility since it has theoretical guarantees in agnostic settings, which have not been focused in our work.",
        "The methods of [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] are based on regularized dual averaging with their exploration-exploitation strategies and achieve a sample complexity of O(1/\u03b52) under linear independence of features or compatibility, which is worse than O(1/\u03b5) of ours.",
        "We compare our proposed Exploration and Hybrid with state-of-the-art Dantzig [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and RDA (Algorithm 12in [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]) in our limited attribute observation setting on a synthetic and real dataset.",
        "We presented sample efficient algorithms for stochastic sparse linear regression problem with limited attribute observation.",
        "We developed Exploration algorithm based on an efficient construction of an unbiased gradient estimator by taking advantage of the iterative usage of hard thresholding in the updates of predictors .",
        "We have shown that Exploration and Hybrid achieve a sample complexity of O(1/\u03b5) with much better dependency on the problem dimension than the ones in existing work.",
        "If the smallest magnitude of the non-zero components of the optimal solution is not too small, the rate of Hybrid can be boosted to near the minimax optimal sample complexity of full information algorithms.",
        "Our methods showed superior convergence behaviors compared to preceding methods on synthetic and real data sets"
    ],
    "headline": "We develop new stochastic gradient methods for efficiently solving sparse linear regression in a partial attribute observation setting, where learners are only allowed to observe a fixed number of actively chosen attributes per example at training and prediction times",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Ben-David and E. Dichterman. Learning with restricted focus of attention. In Proceedings of the sixth annual conference on Computational learning theory, pages 287\u2013296. ACM, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20S.%20Dichterman%2C%20E.%20Learning%20with%20restricted%20focus%20of%20attention%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-David%2C%20S.%20Dichterman%2C%20E.%20Learning%20with%20restricted%20focus%20of%20attention%201993"
        },
        {
            "id": "2",
            "entry": "[2] P. B\u00fchlmann and S. Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science & Business Media, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%BChlmann%2C%20P.%20Geer%2C%20S.Van%20De%20Statistics%20for%20high-dimensional%20data%3A%20methods%2C%20theory%20and%20applications%202011"
        },
        {
            "id": "3",
            "entry": "[3] E. Candes, T. Tao, et al. The dantzig selector: Statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20E.%20Tao%2C%20T.%20The%20dantzig%20selector%3A%20Statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20E.%20Tao%2C%20T.%20The%20dantzig%20selector%3A%20Statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007"
        },
        {
            "id": "4",
            "entry": "[4] N. Cesa-Bianchi, S. Shalev-Shwartz, and O. Shamir. Efficient learning with partially observed attributes. Journal of Machine Learning Research, 12(Oct):2857\u20132878, 2011. This dataset is publicly available on https://archive.ics.uci.edu/ml/datasets/ Relative+location+of+CT+slices+on+axial+axis.",
            "url": "https://archive.ics.uci.edu/ml/datasets/Relative+location+of+CT+slices+on+axial+axis",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20N.%20Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20Efficient%20learning%20with%20partially%20observed%20attributes%202011"
        },
        {
            "id": "5",
            "entry": "[5] J. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10(Dec):2899\u20132934, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Singer%2C%20Y.%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Singer%2C%20Y.%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009"
        },
        {
            "id": "6",
            "entry": "[6] D. Foster, S. Kale, and H. Karloff. Online sparse linear regression. In Conference on Learning Theory, pages 960\u2013970, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foster%2C%20D.%20Kale%2C%20S.%20Karloff%2C%20H.%20Online%20sparse%20linear%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foster%2C%20D.%20Kale%2C%20S.%20Karloff%2C%20H.%20Online%20sparse%20linear%20regression%202016"
        },
        {
            "id": "7",
            "entry": "[7] E. Hazan and T. Koren. Linear regression with limited observation. arXiv preprint arXiv:1206.4678, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.4678"
        },
        {
            "id": "8",
            "entry": "[8] S. Ito, D. Hatano, H. Sumita, A. Yabe, T. Fukunaga, N. Kakimura, and K.-I. Kawarabayashi. Efficient sublinear-regret algorithms for online sparse linear regression with limited observation. In Advances in Neural Information Processing Systems, pages 4102\u20134111, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ito%2C%20S.%20Hatano%2C%20D.%20Sumita%2C%20H.%20Yabe%2C%20A.%20Efficient%20sublinear-regret%20algorithms%20for%20online%20sparse%20linear%20regression%20with%20limited%20observation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ito%2C%20S.%20Hatano%2C%20D.%20Sumita%2C%20H.%20Yabe%2C%20A.%20Efficient%20sublinear-regret%20algorithms%20for%20online%20sparse%20linear%20regression%20with%20limited%20observation%202017"
        },
        {
            "id": "9",
            "entry": "[9] P. Jain, A. Tewari, and P. Kar. On iterative hard thresholding methods for high-dimensional m-estimation. In Advances in Neural Information Processing Systems, pages 685\u2013693, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20P.%20Tewari%2C%20A.%20Kar%2C%20P.%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20P.%20Tewari%2C%20A.%20Kar%2C%20P.%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014"
        },
        {
            "id": "10",
            "entry": "[10] S. Kale, Z. Karnin, T. Liang, and D. P\u00e1l. Adaptive feature selection: Computationally efficient online sparse linear regression under rip. arXiv preprint arXiv:1706.04690, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04690"
        },
        {
            "id": "11",
            "entry": "[11] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1\u201363, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kivinen%2C%20J.%20Warmuth%2C%20M.K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kivinen%2C%20J.%20Warmuth%2C%20M.K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997"
        },
        {
            "id": "12",
            "entry": "[12] N. Nguyen, D. Needell, and T. Woolf. Linear convergence of stochastic iterative greedy algorithms with sparse constraints. IEEE Transactions on Information Theory, 63(11):6869\u2013 6895, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20N.%20Needell%2C%20D.%20Woolf%2C%20T.%20Linear%20convergence%20of%20stochastic%20iterative%20greedy%20algorithms%20with%20sparse%20constraints%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20N.%20Needell%2C%20D.%20Woolf%2C%20T.%20Linear%20convergence%20of%20stochastic%20iterative%20greedy%20algorithms%20with%20sparse%20constraints%202017"
        },
        {
            "id": "13",
            "entry": "[13] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression over q -balls. IEEE transactions on information theory, 57(10):6976\u20136994, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20G.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20Minimax%20rates%20of%20estimation%20for%20high-dimensional%20linear%20regression%20over%20q%20-balls%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20G.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20Minimax%20rates%20of%20estimation%20for%20high-dimensional%20linear%20regression%20over%20q%20-balls%202011"
        },
        {
            "id": "14",
            "entry": "[14] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming, 127(1):3\u201330, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Cotter%2C%20A.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20svm%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Cotter%2C%20A.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20svm%202011"
        },
        {
            "id": "15",
            "entry": "[15] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(Oct):2543\u20132596, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010"
        },
        {
            "id": "16",
            "entry": "[16] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928\u2013936, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
