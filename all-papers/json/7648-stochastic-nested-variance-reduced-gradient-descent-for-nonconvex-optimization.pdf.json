{
    "filename": "7648-stochastic-nested-variance-reduced-gradient-descent-for-nonconvex-optimization.pdf",
    "metadata": {
        "title": "Stochastic Nested Variance Reduced Gradient Descent for Nonconvex Optimization",
        "author": "Dongruo Zhou, Pan Xu, Quanquan Gu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7648-stochastic-nested-variance-reduced-gradient-descent-for-nonconvex-optimization.pdf"
        },
        "abstract": "We study finite-sum nonconvex optimization problems, where the objective function is an average of n nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses K + 1 nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, the proposed algorithm converges to an \u270f-approximate first-order stationary point (i.e., krF (x)k2"
    },
    "keywords": [
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "Gradient Descent",
            "url": "https://en.wikipedia.org/wiki/Gradient_Descent"
        },
        {
            "term": "SVRG",
            "url": "https://en.wikipedia.org/wiki/SVRG"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "reference point",
            "url": "https://en.wikipedia.org/wiki/reference_point"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "To measure the convergence of different first-order algorithms.2 For nonconvex optimization, it is well-known that Gradient Descent (GD) can converge to an \u270f-approximate stationary point with O(n \u00b7 \u270f 2) [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] number of stochastic gradient evaluations",
        "While Stochastic Gradient Descent only needs to calculate a minibatch of stochastic gradients in each iteration, due to the noise brought by stochastic gradients, its gradient complexity has a worse dependency on \u270f",
        "One representative variance reduction method is Stochastic Variance Reduced Gradient, which is based on a semi-stochastic gradient that is defined by two reference points",
        "Lei et al [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] proposed a Stochastically Controlled Stochastic Gradient (SCSG) based on variance reduction, which further reduces the gradient complexity of Stochastic Variance Reduced Gradient to O(n ^ \u270f 2 + \u270f 10/3 ^ (n2/3\u270f 2))",
        "We propose a stochastic nested variance reduction technique for stochastic gradient method, which reduces the dependence of the gradient complexity on n compared with Stochastic Variance Reduced Gradient and Stochastic Gradient",
        "We proposed a stochastic nested variance reduced gradient method for finite-sum nonconvex optimization"
    ],
    "key_statements": [
        "To measure the convergence of different first-order algorithms.2 For nonconvex optimization, it is well-known that Gradient Descent (GD) can converge to an \u270f-approximate stationary point with O(n \u00b7 \u270f 2) [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] number of stochastic gradient evaluations",
        "While Stochastic Gradient Descent only needs to calculate a minibatch of stochastic gradients in each iteration, due to the noise brought by stochastic gradients, its gradient complexity has a worse dependency on \u270f",
        "In order to improve the dependence of the gradient complexity of Stochastic Gradient Descent on n and \u270f for nonconvex optimization, variance reduction technique was firstly proposed in [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] for convex finite-sum optimization",
        "One representative variance reduction method is Stochastic Variance Reduced Gradient, which is based on a semi-stochastic gradient that is defined by two reference points",
        "The analysis for the general nonconvex function F was done by [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], which shows that Stochastic Variance Reduced Gradient can converge to an \u270f-approximate stationary point with O(n2/3 \u00b7 \u270f 2) number of stochastic gradient evaluations",
        "Lei et al [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] proposed a Stochastically Controlled Stochastic Gradient (SCSG) based on variance reduction, which further reduces the gradient complexity of Stochastic Variance Reduced Gradient to O(n ^ \u270f 2 + \u270f 10/3 ^ (n2/3\u270f 2))",
        "We propose a novel algorithm namely Stochastic Nested Variance-Reduced Gradient descent (SNVRG)",
        "We propose a stochastic nested variance reduction technique for stochastic gradient method, which reduces the dependence of the gradient complexity on n compared with Stochastic Variance Reduced Gradient and Stochastic Gradient",
        "Stochastic Nested Variance-Reduced Gradient descent-PL: In addition, when function F in (1.1) is gradient dominated as defined in Definition 2.5 (P-L condition), it has been proved that the global minimum can be found by Stochastic Gradient Descent [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], Stochastic Variance Reduced Gradient [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] and Stochastic Gradient [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] very efficiently",
        "We proposed a stochastic nested variance reduced gradient method for finite-sum nonconvex optimization"
    ],
    "summary": [
        "To measure the convergence of different first-order algorithms.2 For nonconvex optimization, it is well-known that Gradient Descent (GD) can converge to an \u270f-approximate stationary point with O(n \u00b7 \u270f 2) [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] number of stochastic gradient evaluations.",
        "In order to improve the dependence of the gradient complexity of SGD on n and \u270f for nonconvex optimization, variance reduction technique was firstly proposed in [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] for convex finite-sum optimization.",
        "One representative variance reduction method is SVRG, which is based on a semi-stochastic gradient that is defined by two reference points.",
        "The analysis for the general nonconvex function F was done by [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], which shows that SVRG can converge to an \u270f-approximate stationary point with O(n2/3 \u00b7 \u270f 2) number of stochastic gradient evaluations.",
        "We propose a stochastic nested variance reduction technique for stochastic gradient method, which reduces the dependence of the gradient complexity on n compared with SVRG and SCSG.",
        "2) stochastic gradient evaluations, which outperforms all existing first-order algorithms such as GD, SGD, SVRG and SCSG.",
        "We plot the gradient complexities of different algorithms in Figure 1 for nonconvex smooth functions.",
        "We only plot the gradient complexity of SVRG, SCSG and our proposed SNVRG in Figure 1.",
        "SNVRG-PL: In addition, when function F in (1.1) is gradient dominated as defined in Definition 2.5 (P-L condition), it has been proved that the global minimum can be found by SGD [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], SVRG [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] and SCSG [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] very efficiently.",
        "For our algorithm SNVRG, we Figure 2: Illustration of reference need to store K reference gradients, its space complexity points and gradients.",
        "The following theorem shows the gradient complexity for Algorithm 2 to find an \u270f-approximate stationary point with a constant base batch size B.",
        "Implementation Details & Parameter Tuning We did not use the random data augmentation which is set as default by Pytorch, because it will apply random transformation at the beginning of each epoch on the original image dataset, which will ruin the finite-sum structure of the loss function.",
        "It can be seen that with learning rate decay schedule, our algorithm SNVRG outperforms all baseline algorithms, which confirms that the use of nested reference points and gradients can accelerate the nonconvex finite-sum optimization.",
        "We proposed a stochastic nested variance reduced gradient method for finite-sum nonconvex optimization.",
        "There is still an open question: whether Oe(n ^ \u270f"
    ],
    "headline": "We study finite-sum nonconvex optimization problems, where the objective function is an average of n nonconvex functions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Naman Agarwal, Zeyuan Allenzhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima for nonconvex optimization in linear time. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Naman%20Allenzhu%2C%20Zeyuan%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Finding%20approximate%20local%20minima%20for%20nonconvex%20optimization%20in%20linear%20time%202017"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200\u20131205. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arXiv:1708.08694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "4",
            "entry": "[4] Zeyuan Allen-Zhu. Katyusha x: Practical momentum method for stochastic sum-of-nonconvex optimization. arXiv preprint arXiv:1802.03866, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03866"
        },
        {
            "id": "5",
            "entry": "[5] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International Conference on Machine Learning, pages 699\u2013707, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Hazan%2C%20Elad%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "6",
            "entry": "[6] Alberto Bietti and Julien Mairal. Stochastic optimization with variance reduction for infinite datasets with finite sum structure. In Advances in Neural Information Processing Systems, pages 1622\u20131632, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bietti%2C%20Alberto%20Mairal%2C%20Julien%20Stochastic%20optimization%20with%20variance%20reduction%20for%20infinite%20datasets%20with%20finite%20sum%20structure%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bietti%2C%20Alberto%20Mairal%2C%20Julien%20Stochastic%20optimization%20with%20variance%20reduction%20for%20infinite%20datasets%20with%20finite%20sum%20structure%202017"
        },
        {
            "id": "7",
            "entry": "[7] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Yair%20Duchi%2C%20John%20C.%20Hinder%2C%20Oliver%20Sidford%2C%20Aaron%20Accelerated%20methods%20for%20non-convex%20optimization%202016"
        },
        {
            "id": "8",
            "entry": "[8] Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. \u201cconvex until proven guilty\": Dimension-free acceleration of gradient descent on non-convex functions. In International Conference on Machine Learning, pages 654\u2013663, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Yair%20Duchi%2C%20John%20C.%20Hinder%2C%20Oliver%20Sidford%2C%20Aaron%20%E2%80%9Cconvex%20until%20proven%20guilty%22%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmon%2C%20Yair%20Duchi%2C%20John%20C.%20Hinder%2C%20Oliver%20Sidford%2C%20Aaron%20%E2%80%9Cconvex%20until%20proven%20guilty%22%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017"
        },
        {
            "id": "9",
            "entry": "[9] Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points of non-convex, smooth high-dimensional functions. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Yair%20Duchi%2C%20John%20C.%20Hinder%2C%20Oliver%20Sidford%2C%20Aaron%20Lower%20bounds%20for%20finding%20stationary%20points%20of%20non-convex%2C%20smooth%20high-dimensional%20functions%202017"
        },
        {
            "id": "10",
            "entry": "[10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "11",
            "entry": "[11] Aaron Defazio, Justin Domke, et al. Finito: A faster, permutable incremental gradient method for big data problems. In International Conference on Machine Learning, pages 1125\u20131133, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Domke%2C%20Justin%20Finito%3A%20A%20faster%2C%20permutable%20incremental%20gradient%20method%20for%20big%20data%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Domke%2C%20Justin%20Finito%3A%20A%20faster%2C%20permutable%20incremental%20gradient%20method%20for%20big%20data%20problems%202014"
        },
        {
            "id": "12",
            "entry": "[12] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal nonconvex optimization via stochastic path integrated differential estimator. arXiv preprint arXiv:1807.01695, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.01695"
        },
        {
            "id": "13",
            "entry": "[13] Dan Garber and Elad Hazan. Fast and simple pca via convex optimization. arXiv preprint arXiv:1509.05647, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.05647"
        },
        {
            "id": "14",
            "entry": "[14] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469\u20131492, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Optimal%20stochastic%20approximation%20algorithms%20for%20strongly%20convex%20stochastic%20composite%20optimization%20i%3A%20A%20generic%20algorithmic%20framework%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Optimal%20stochastic%20approximation%20algorithms%20for%20strongly%20convex%20stochastic%20composite%20optimization%20i%3A%20A%20generic%20algorithmic%20framework%202012"
        },
        {
            "id": "15",
            "entry": "[15] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1-2):59\u201399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016"
        },
        {
            "id": "16",
            "entry": "[16] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM (JACM), 60(6):45, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hillar%2C%20Christopher%20J.%20Lim%2C%20Lek-Heng%20Most%20tensor%20problems%20are%20np-hard%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hillar%2C%20Christopher%20J.%20Lim%2C%20Lek-Heng%20Most%20tensor%20problems%20are%20np-hard%202013"
        },
        {
            "id": "17",
            "entry": "[17] Chonghai Hu, Weike Pan, and James T Kwok. Accelerated gradient methods for stochastic optimization and online learning. In Advances in Neural Information Processing Systems, pages 781\u2013789, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Chonghai%20Pan%2C%20Weike%20Kwok%2C%20James%20T.%20Accelerated%20gradient%20methods%20for%20stochastic%20optimization%20and%20online%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Chonghai%20Pan%2C%20Weike%20Kwok%2C%20James%20T.%20Accelerated%20gradient%20methods%20for%20stochastic%20optimization%20and%20online%20learning%202009"
        },
        {
            "id": "18",
            "entry": "[18] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "19",
            "entry": "[19] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795\u2013811.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition"
        },
        {
            "id": "20",
            "entry": "[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "21",
            "entry": "[21] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "22",
            "entry": "[22] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1):365\u2013397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Guanghui%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Guanghui%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012"
        },
        {
            "id": "23",
            "entry": "[23] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. Mathematical programming, pages 1\u201349, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Guanghui%20Zhou%2C%20Yi%20An%20optimal%20randomized%20incremental%20gradient%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Guanghui%20Zhou%2C%20Yi%20An%20optimal%20randomized%20incremental%20gradient%20method%202017"
        },
        {
            "id": "24",
            "entry": "[24] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "25",
            "entry": "[25] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017"
        },
        {
            "id": "26",
            "entry": "[26] Huan Li and Zhouchen Lin. Accelerated proximal gradient methods for nonconvex programming. In Advances in neural information processing systems, pages 379\u2013387, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Huan%20Lin%2C%20Zhouchen%20Accelerated%20proximal%20gradient%20methods%20for%20nonconvex%20programming%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Huan%20Lin%2C%20Zhouchen%20Accelerated%20proximal%20gradient%20methods%20for%20nonconvex%20programming%202015"
        },
        {
            "id": "27",
            "entry": "[27] Qunwei Li, Yi Zhou, Yingbin Liang, and Pramod K Varshney. Convergence analysis of proximal gradient with momentum for nonconvex optimization. arXiv preprint arXiv:1705.04925, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.04925"
        },
        {
            "id": "28",
            "entry": "[28] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization. In Advances in Neural Information Processing Systems, pages 3384\u20133392, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20universal%20catalyst%20for%20first-order%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20universal%20catalyst%20for%20first-order%20optimization%202015"
        },
        {
            "id": "29",
            "entry": "[29] Julien Mairal. Incremental majorization-minimization optimization with application to largescale machine learning. SIAM Journal on Optimization, 25(2):829\u2013855, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20Julien%20Incremental%20majorization-minimization%20optimization%20with%20application%20to%20largescale%20machine%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20Julien%20Incremental%20majorization-minimization%20optimization%20with%20application%20to%20largescale%20machine%20learning%202015"
        },
        {
            "id": "30",
            "entry": "[30] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103 (1):127\u2013152, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yu%20Smooth%20minimization%20of%20non-smooth%20functions.%20Mathematical%20programming%2C%20103%202005"
        },
        {
            "id": "31",
            "entry": "[31] Yurii Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20Lectures%20on%20Convex%20Optimization%202014"
        },
        {
            "id": "32",
            "entry": "[32] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20and%20Andrew%20Y%20Ng.%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning"
        },
        {
            "id": "33",
            "entry": "[33] Courtney Paquette, Hongzhou Lin, Dmitriy Drusvyatskiy, Julien Mairal, and Zaid Harchaoui. Catalyst acceleration for gradient-based non-convex optimization. arXiv preprint arXiv:1703.10993, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10993"
        },
        {
            "id": "34",
            "entry": "[34] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964"
        },
        {
            "id": "35",
            "entry": "[35] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 3(4):643\u2013653, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20Teodorovich%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20Teodorovich%20Gradient%20methods%20for%20minimizing%20functionals.%20Zhurnal%20Vychislitel%E2%80%99noi%20Matematiki%20i%201963"
        },
        {
            "id": "36",
            "entry": "[36] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):145\u2013151, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qian%2C%20Ning%20On%20the%20momentum%20term%20in%20gradient%20descent%20learning%20algorithms%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qian%2C%20Ning%20On%20the%20momentum%20term%20in%20gradient%20descent%20learning%20algorithms%201999"
        },
        {
            "id": "37",
            "entry": "[37] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. pages 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "38",
            "entry": "[38] Sashank J Reddi, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alex Smola. Fast incremental method for smooth nonconvex optimization. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pages 1971\u20131977. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Smola%2C%20Alex%20Fast%20incremental%20method%20for%20smooth%20nonconvex%20optimization%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Smola%2C%20Alex%20Fast%20incremental%20method%20for%20smooth%20nonconvex%20optimization%201971"
        },
        {
            "id": "39",
            "entry": "[39] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. In Advances in Neural Information Processing Systems, pages 1145\u20131153, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Smola%2C%20Alexander%20J.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20Poczos%2C%20Barnabas%20Smola%2C%20Alexander%20J.%20Proximal%20stochastic%20methods%20for%20nonsmooth%20nonconvex%20finite-sum%20optimization%202016"
        },
        {
            "id": "40",
            "entry": "[40] Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential convergence _rate for finite training sets. In Advances in Neural Information Processing Systems, pages 2663\u20132671, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20_rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20_rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "41",
            "entry": "[41] Bernhard Sch\u00f6lkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20Bernhard%20Smola%2C%20Alexander%20J.%20Learning%20with%20kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202002"
        },
        {
            "id": "42",
            "entry": "[42] Shai Shalev-Shwartz. Sdca without duality. arXiv preprint arXiv:1502.06177, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.06177"
        },
        {
            "id": "43",
            "entry": "[43] Shai Shalev-Shwartz. Sdca without duality, regularization, and individual convexity. In International Conference on Machine Learning, pages 747\u2013754, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Sdca%20without%20duality%2C%20regularization%2C%20and%20individual%20convexity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Sdca%20without%20duality%2C%20regularization%2C%20and%20individual%20convexity%202016"
        },
        {
            "id": "44",
            "entry": "[44] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "45",
            "entry": "[45] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014"
        }
    ]
}
