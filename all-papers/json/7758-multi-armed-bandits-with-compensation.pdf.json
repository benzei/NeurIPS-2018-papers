{
    "filename": "7758-multi-armed-bandits-with-compensation.pdf",
    "metadata": {
        "title": "Multi-armed Bandits with Compensation",
        "author": "Siwei Wang, Longbo Huang",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7758-multi-armed-bandits-with-compensation.pdf"
        },
        "abstract": "We propose and study the known-compensation multi-arm bandit (KCMAB) problem, where a system controller offers a set of arms to many short-term players for\nT steps. In each step, one short-term player arrives to the system. Upon arrival, the player aims to select an arm with the current best average reward and receives a stochastic reward associated with the arm. In order to incentivize players to explore other arms, the controller provides a proper payment compensation to players. The objective of the controller is to maximize the total reward collected by players while minimizing the compensation. We first provide a compensation lower bound \u0398("
    },
    "keywords": [
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "time slot",
            "url": "https://en.wikipedia.org/wiki/time_slot"
        },
        {
            "term": "bandit problem",
            "url": "https://en.wikipedia.org/wiki/bandit_problem"
        }
    ],
    "highlights": [
        "Multi-arm bandit (MAB) is a game that lasts for an unknown time horizon T [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "The controller aims to maximize the sum of rewards during the game by choosing a proper arm to pull in each time slot, and the decision can depend on all available information, i.e., past chosen arms and feedbacks",
        "We propose three algorithms for solving our problem, which adapt ideas from existing policies for stochastic Multi-arm bandit, i.e., Upper Confidence Bound (UCB) [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], Thompson Sampling (TS) [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and \u03b5-greedy [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "We propose and study the Known-Compensation Multi-Arm Bandit problem where a controller offers compensation to incentivize players for arm exploration",
        "We first provide the analysis of a compensation lower bound achieved by regret-minimizing algorithms",
        "We show that all three algorithms achieve good regret bounds, while keeping order-optimal compensation"
    ],
    "key_statements": [
        "Multi-arm bandit (MAB) is a game that lasts for an unknown time horizon T [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "The controller has N arms to pull, and pulling different arms will result in different feedbacks",
        "In the stochastic Multi-arm bandit model [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], feedbacks from a single arm follow an associated distribution, which is unknown to the player",
        "The controller aims to maximize the sum of rewards during the game by choosing a proper arm to pull in each time slot, and the decision can depend on all available information, i.e., past chosen arms and feedbacks",
        "The common metric for evaluating the performance of a policy is the value of regret, defined as the expected difference between the player\u2019s reward and pulling an arm that generates the largest average reward",
        "We propose three algorithms for solving our problem, which adapt ideas from existing policies for stochastic Multi-arm bandit, i.e., Upper Confidence Bound (UCB) [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], Thompson Sampling (TS) [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and \u03b5-greedy [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "We propose and study the Known-Compensation Multi-arm bandit problem (KCMAB)",
        "In Known-Compensation Multi-Arm Bandit, subject to the algorithm having an o(T \u03b1) regret for any \u03b1 > 0, we provide a \u0398 lower bound for the compensation",
        "We propose algorithms to solve the Known-Compensation Multi-Arm Bandit problem and present their compensation analysis",
        "We provide experimental results to demonstrate the performance of our algorithms",
        "In each time slot t, a short-term player arrives at the system and needs to choose an arm a(t) to pull",
        "Following the Multi-arm bandit tradition, we define the following total regret: Reg(T ) = T max \u03bci \u2212 Rew(T ) = T \u03bc1 \u2212 Rew(T ), i where Rew(T ) denotes the expected total reward that the long-term controller can obtain until time horizon T",
        "The second algorithm we propose is a modified \u03b5-greedy policy, whose details are presented in Algorithm 2",
        "Let ti\u03b5(k) be the time slot that we explore arm i for the k-th time",
        "The third algorithm we propose is a Thompson Sampling (TS) based policy",
        "We propose and study the Known-Compensation Multi-Arm Bandit problem where a controller offers compensation to incentivize players for arm exploration",
        "We first provide the analysis of a compensation lower bound achieved by regret-minimizing algorithms",
        "We show that all three algorithms achieve good regret bounds, while keeping order-optimal compensation"
    ],
    "summary": [
        "Multi-arm bandit (MAB) is a game that lasts for an unknown time horizon T [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>].",
        "[<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] considered the case \u221awhen the rewards are not discounted, and they show an algorithm to achieve regret upper bound of O( T ).",
        "In KCMAB, a long-term controller aims to optimize the accumulated reward but has to offer compensation to a set of short-term players for pulling arms.",
        "In each time slot t, a short-term player arrives at the system and needs to choose an arm a(t) to pull.",
        "Following the MAB tradition, we define the following total regret: Reg(T ) = T max \u03bci \u2212 Rew(T ) = T \u03bc1 \u2212 Rew(T ), i where Rew(T ) denotes the expected total reward that the long-term controller can obtain until time horizon T .",
        "We assume without loss of generality that in the first N time slots of the game, with some constant compensation, the long-term controller can control the short-term players to choose all the arms once.",
        "If the long-term controller wants the short-term player to choose arm i in time slot t, the minimum compensation he needs to pay on pulling arm i is ci(t) = maxj \u03bcj(t) \u2212 \u03bci(t).",
        "For any sub-optimal arm i, with high probability the compensation that the long-term controller pays for it can be upper bounded by: Ni(T )",
        "In the regret analysis of the \u03b5-greedy algorithm, the random exploration ensures that at time slot t, the expectation of explorations on each arm is about N log t.",
        "We provide a compensation upper bound for our modified \u03b5-greedy algorithm.",
        "Proof Sketch: Firstly, our modified \u03b5-greedy algorithm chooses the arm with the largest empirical mean in non-exploration steps.",
        "Due to the complexity of the analysis for the traditional TS algorithm, we propose a modified TS policy and derive its compensation bound.",
        "The modified TS policy is presented in Algorithm 3, and we have the following theorem about its regret and compensation.",
        "We can bound the number of sample steps during which we pull a sub-optimal arm, using existing results in [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], since all sample steps form an approximation of the classic TS algorithm.",
        "The compensation we need to pay on arm 1 is upper bounded by i",
        "We propose and study the KCMAB problem where a controller offers compensation to incentivize players for arm exploration.",
        "We first provide the analysis of a compensation lower bound achieved by regret-minimizing algorithms.",
        "We show that all three algorithms achieve good regret bounds, while keeping order-optimal compensation.",
        "We conduct experiments and the results validate our theoretical findings"
    ],
    "headline": "We propose and study the known-compensation multi-arm bandit  problem, where a system controller offers a set of arms to many short-term players for",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Agrawal, Shipra and Goyal, Navin. Further optimal regret bounds for thompson sampling. In AISTATS, pp. 99\u2013107, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Further%20optimal%20regret%20bounds%20for%20thompson%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Further%20optimal%20regret%20bounds%20for%20thompson%20sampling%202013"
        },
        {
            "id": "2",
            "entry": "[2] Auer, Peter, Cesa-Bianchi, Nicolo, and Fischer, Paul. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235\u2013256, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicolo%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicolo%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "3",
            "entry": "[3] Auer, Peter, Cesa-Bianchi, Nicol\u00f2, Freund, Yoav, and Schapire, Robert E. The non-stochastic multi-armed bandit problem. Siam Journal on Computing, 32(1):48\u201377, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20The%20non-stochastic%20multi-armed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20The%20non-stochastic%20multi-armed%20bandit%20problem%202002"
        },
        {
            "id": "4",
            "entry": "[4] Berry, Donald A and Fristedt, Bert. Bandit problems: sequential allocation of experiments (Monographs on statistics and applied probability). Springer, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berry%2C%20Donald%20A.%20Fristedt%2C%20Bert%20Bandit%20problems%3A%20sequential%20allocation%20of%20experiments%20%28Monographs%20on%20statistics%20and%20applied%20probability%29%201985"
        },
        {
            "id": "5",
            "entry": "[5] Beygelzimer, Alina, Langford, John, Li, Lihong, Reyzin, Lev, and Schapire, Robert. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 19\u201326, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beygelzimer%2C%20Alina%20Langford%2C%20John%20Li%2C%20Lihong%20Reyzin%2C%20Lev%20Contextual%20bandit%20algorithms%20with%20supervised%20learning%20guarantees%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beygelzimer%2C%20Alina%20Langford%2C%20John%20Li%2C%20Lihong%20Reyzin%2C%20Lev%20Contextual%20bandit%20algorithms%20with%20supervised%20learning%20guarantees%202011"
        },
        {
            "id": "6",
            "entry": "[6] Che, Yeon-Koo and H\u00f6rner, Johannes. Recommender systems as mechanisms for social learning. The Quarterly Journal of Economics, 133(2):871\u2013925, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Che%2C%20Yeon-Koo%20H%C3%B6rner%2C%20Johannes%20Recommender%20systems%20as%20mechanisms%20for%20social%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Che%2C%20Yeon-Koo%20H%C3%B6rner%2C%20Johannes%20Recommender%20systems%20as%20mechanisms%20for%20social%20learning%202017"
        },
        {
            "id": "7",
            "entry": "[7] Combes, Richard, Jiang, Chong, and Srikant, Rayadurgam. Bandits with budgets: Regret lower bounds and optimal algorithms. ACM SIGMETRICS Performance Evaluation Review, 43(1): 245\u2013257, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combes%2C%20Richard%20Jiang%2C%20Chong%20Srikant%2C%20Rayadurgam%20Bandits%20with%20budgets%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combes%2C%20Richard%20Jiang%2C%20Chong%20Srikant%2C%20Rayadurgam%20Bandits%20with%20budgets%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202015"
        },
        {
            "id": "8",
            "entry": "[8] Frazier, Peter, Kempe, David, Kleinberg, Jon, and Kleinberg, Robert. Incentivizing exploration. In Fifteenth ACM Conference on Economics and Computation, pp. 5\u201322, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frazier%2C%20Peter%20Kempe%2C%20David%20Kleinberg%2C%20Jon%20Kleinberg%2C%20Robert%20Incentivizing%20exploration%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frazier%2C%20Peter%20Kempe%2C%20David%20Kleinberg%2C%20Jon%20Kleinberg%2C%20Robert%20Incentivizing%20exploration%202014"
        },
        {
            "id": "9",
            "entry": "[9] Gittins, JC. Multi-armed bandit allocation indices. wiley-interscience series in systems and optimization. 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gittins%2C%20J.C.%20Multi-armed%20bandit%20allocation%20indices.%20wiley-interscience%20series%20in%20systems%20and%20optimization%201989"
        },
        {
            "id": "10",
            "entry": "[10] Kalyanakrishnan, Shivaram, Tewari, Ambuj, Auer, Peter, and Stone, Peter. Pac subset selection in stochastic multi-armed bandits. In International Conference on Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalyanakrishnan%2C%20Shivaram%20Tewari%2C%20Ambuj%20Auer%2C%20Peter%20Stone%2C%20Peter%20Pac%20subset%20selection%20in%20stochastic%20multi-armed%20bandits%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalyanakrishnan%2C%20Shivaram%20Tewari%2C%20Ambuj%20Auer%2C%20Peter%20Stone%2C%20Peter%20Pac%20subset%20selection%20in%20stochastic%20multi-armed%20bandits%202012"
        },
        {
            "id": "11",
            "entry": "[11] Kaufmann, Emilie, Korda, Nathaniel, and Munos, R\u00e9mi. Thompson sampling: an asymptotically optimal finite-time analysis. In Proceedings of the 23rd international conference on Algorithmic Learning Theory, pp. 199\u2013213, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaufmann%2C%20Emilie%20Korda%2C%20Nathaniel%20Munos%2C%20R%C3%A9mi%20Thompson%20sampling%3A%20an%20asymptotically%20optimal%20finite-time%20analysis%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaufmann%2C%20Emilie%20Korda%2C%20Nathaniel%20Munos%2C%20R%C3%A9mi%20Thompson%20sampling%3A%20an%20asymptotically%20optimal%20finite-time%20analysis%202012"
        },
        {
            "id": "12",
            "entry": "[12] Lai, Tze Leung and Robbins, Herbert. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4\u201322, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20Tze%20Leung%20Robbins%2C%20Herbert%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20Tze%20Leung%20Robbins%2C%20Herbert%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985"
        },
        {
            "id": "13",
            "entry": "[13] Maillard, Odalric-Ambrym and Munos, R\u00e9mi. Adaptive bandits: Towards the best historydependent strategy. In International Conference on Artificial Intelligence and Statistics, pp. 570\u2013578, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maillard%2C%20Odalric-Ambrym%20Munos%2C%20R%C3%A9mi%20Adaptive%20bandits%3A%20Towards%20the%20best%20historydependent%20strategy%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maillard%2C%20Odalric-Ambrym%20Munos%2C%20R%C3%A9mi%20Adaptive%20bandits%3A%20Towards%20the%20best%20historydependent%20strategy%202011"
        },
        {
            "id": "14",
            "entry": "[14] Mansour, Yishay, Slivkins, Aleksandrs, and Syrgkanis, Vasilis. Bayesian incentive-compatible bandit exploration. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, pp. 565\u2013582. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Slivkins%2C%20Aleksandrs%20Syrgkanis%2C%20Vasilis%20Bayesian%20incentive-compatible%20bandit%20exploration%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Slivkins%2C%20Aleksandrs%20Syrgkanis%2C%20Vasilis%20Bayesian%20incentive-compatible%20bandit%20exploration%202015"
        },
        {
            "id": "15",
            "entry": "[15] Mansour, Yishay, Slivkins, Aleksandrs, Syrgkanis, Vasilis, and Wu, Zhiwei Steven. Bayesian exploration: Incentivizing exploration in bayesian games. arXiv preprint arXiv:1602.07570, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07570"
        },
        {
            "id": "16",
            "entry": "[16] Maron, Oded and Moore, Andrew W. The racing algorithm: Model selection for lazy learners. In Lazy learning, pp. 193\u2013225.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maron%2C%20Oded%20Moore%2C%20Andrew%20W.%20The%20racing%20algorithm%3A%20Model%20selection%20for%20lazy%20learners",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maron%2C%20Oded%20Moore%2C%20Andrew%20W.%20The%20racing%20algorithm%3A%20Model%20selection%20for%20lazy%20learners"
        },
        {
            "id": "17",
            "entry": "[17] Papanastasiou, Yiangos, Bimpikis, Kostas, and Savva, Nicos. Crowdsourcing exploration. Management Science, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papanastasiou%2C%20Yiangos%20Bimpikis%2C%20Kostas%20Savva%2C%20Nicos%20Crowdsourcing%20exploration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papanastasiou%2C%20Yiangos%20Bimpikis%2C%20Kostas%20Savva%2C%20Nicos%20Crowdsourcing%20exploration%202017"
        },
        {
            "id": "18",
            "entry": "[18] Robbins, Herbert. Some aspects of the sequential design of experiments. In Herbert Robbins Selected Papers, pp. 169\u2013177.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20Herbert%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments"
        },
        {
            "id": "19",
            "entry": "[19] Sutton, Richard S and Barto, Andrew G. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "20",
            "entry": "[20] Thompson, William R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "21",
            "entry": "[21] Tran-Thanh, Long, Chapman, Archie C, Rogers, Alex, and Jennings, Nicholas R. Knapsack based optimal policies for budget-limited multi-armed bandits. In AAAI, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran-Thanh%2C%20Long%20Chapman%2C%20Archie%20C.%20Rogers%2C%20Alex%20Jennings%2C%20Nicholas%20R.%20Knapsack%20based%20optimal%20policies%20for%20budget-limited%20multi-armed%20bandits%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran-Thanh%2C%20Long%20Chapman%2C%20Archie%20C.%20Rogers%2C%20Alex%20Jennings%2C%20Nicholas%20R.%20Knapsack%20based%20optimal%20policies%20for%20budget-limited%20multi-armed%20bandits%202012"
        },
        {
            "id": "22",
            "entry": "[22] Watkins, C. J. C. H. Learning form delayed rewards. Ph.d.thesis Kings College University of Cambridge, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watkins%2C%20C.J.C.H.%20Learning%20form%20delayed%20rewards%201989"
        },
        {
            "id": "23",
            "entry": "[23] Xia, Yingce, Li, Haifang, Qin, Tao, Yu, Nenghai, and Liu, Tie-Yan. Thompson sampling for budgeted multi-armed bandits. In IJCAI, pp. 3960\u20133966, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xia%2C%20Yingce%20Li%2C%20Haifang%20Qin%2C%20Tao%20Yu%2C%20Nenghai%20Thompson%20sampling%20for%20budgeted%20multi-armed%20bandits%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xia%2C%20Yingce%20Li%2C%20Haifang%20Qin%2C%20Tao%20Yu%2C%20Nenghai%20Thompson%20sampling%20for%20budgeted%20multi-armed%20bandits%202015"
        }
    ]
}
