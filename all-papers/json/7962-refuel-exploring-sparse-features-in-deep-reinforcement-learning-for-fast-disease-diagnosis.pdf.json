{
    "filename": "7962-refuel-exploring-sparse-features-in-deep-reinforcement-learning-for-fast-disease-diagnosis.pdf",
    "metadata": {
        "title": "REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis",
        "author": "Yu-Shao Peng, Kai-Fu Tang, Hsuan-Tien Lin, Edward Chang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7962-refuel-exploring-sparse-features-in-deep-reinforcement-learning-for-fast-disease-diagnosis.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper proposes REFUEL, a reinforcement learning method with two techniques: reward shaping and feature rebuilding, to improve the performance of online symptom checking for disease diagnosis. Reward shaping can guide the search of policy towards better directions. Feature rebuilding can guide the agent to learn correlations between features. Together, they can find symptom queries that can yield positive responses from a patient with high probability. Experimental results justify that the two techniques in REFUEL allows the symptom checker to identify the disease more rapidly and accurately."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "self diagnosis",
            "url": "https://en.wikipedia.org/wiki/self_diagnosis"
        }
    ],
    "highlights": [
        "One of the rising needs in healthcare is self-diagnosis",
        "This paper proposes the REFUEL1 model, which is a reinforcement learning model with reward shaping and feature rebuilding techniques",
        "To formulate our problem as a decision process, we introduce the Markov decision process (MDP)",
        "We presented REFUEL for disease diagnosis",
        "We design an informative potential function to guide an agent to productively search in the sparse feature space and adopt a reward shaping technique to ensure the policy invariance",
        "The experimental results have confirmed the validity of our proposed method and shown that it is a promising approach to fast and accurate disease diagnosis"
    ],
    "key_statements": [
        "One of the rising needs in healthcare is self-diagnosis",
        "This paper proposes the REFUEL1 model, which is a reinforcement learning model with reward shaping and feature rebuilding techniques",
        "To formulate our problem as a decision process, we introduce the Markov decision process (MDP)",
        "Combining the techniques of reward shaping and enforcing feature rebuilding, we propose the REFUEL algorithm to solve the disease diagnosis problem, as shown in Algorithm 1",
        "We presented REFUEL for disease diagnosis",
        "We design an informative potential function to guide an agent to productively search in the sparse feature space and adopt a reward shaping technique to ensure the policy invariance",
        "The experimental results have confirmed the validity of our proposed method and shown that it is a promising approach to fast and accurate disease diagnosis"
    ],
    "summary": [
        "One of the rising needs in healthcare is self-diagnosis.",
        "This paper proposes the REFUEL1 model, which is a reinforcement learning model with reward shaping and feature rebuilding techniques.",
        "In the following time step t, if M receives an action At \u2264 m (i.e., a feature acquisition action) from the agent and the time step t is less than the maximum length T , the sample model generates the state St+1 and reward Rt+1 by",
        "The na\u00efve reinforcement learning algorithm presented in Section 2 does not explicitly guide the agent to discover positive symptoms quickly, which makes the learning progress slow under the challenging scenario of sparse features.",
        "We propose two novel techniques to guide the reinforcement learning agent to quickly discover key positive symptoms.",
        "Given that certain important negative symptoms are helpful to distinguish diseases, one may think it is counterintuitive to punish an agent with non-positive auxiliary rewards when it queries negative symptoms.",
        "Even though the agent may receive non-positive auxiliary rewards, it can still learn to query those critical negative symptoms.",
        "If the agent queries more key positive symptoms, the model can make a better feature rebuilding at the time step.",
        "Combining the techniques of reward shaping and enforcing feature rebuilding, we propose the REFUEL algorithm to solve the disease diagnosis problem, as shown in Algorithm 1.",
        "If the agent selects an acquisition action (At \u2264 m), the sample model generates the state, which consists of the symptoms from the previous state and one additional symptom acquired by the action, and a reward 0 for t < T, or the terminal state S\u22a5 and a reward \u22121 for t \u2265 T.",
        "The algorithm takes the immediate rewards Rt and the states St stored in the buffer to calculate shaped rewards that encourage the identification of positive symptoms.",
        "The algorithm takes the rebuilt symptoms zt to compute the feature rebuilding loss, and uses the update rule of the new objective function to update model parameters.",
        "We design an informative potential function to guide an agent to productively search in the sparse feature space and adopt a reward shaping technique to ensure the policy invariance.",
        "The two techniques help identify symptom queries that yield positive patient responses at a much higher probability, which in term leads to faster exploration speed and higher diagnosis accuracy.",
        "The experimental results have confirmed the validity of our proposed method and shown that it is a promising approach to fast and accurate disease diagnosis"
    ],
    "headline": "This paper proposes REFUEL, a reinforcement learning method with two techniques: reward shaping and feature rebuilding, to improve the performance of online symptom checking for disease diagnosis",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Grzes and D. Kudenko. Theoretical and empirical analysis of reward shaping in reinforcement learning. In International Conference on Machine Learning and Applications, ICMLA 2009, Miami Beach, Florida, USA, December 13-15, 2009, pages 337\u2013344, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grzes%2C%20M.%20Kudenko%2C%20D.%20Theoretical%20and%20empirical%20analysis%20of%20reward%20shaping%20in%20reinforcement%20learning%202009-12-13",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grzes%2C%20M.%20Kudenko%2C%20D.%20Theoretical%20and%20empirical%20analysis%20of%20reward%20shaping%20in%20reinforcement%20learning%202009-12-13"
        },
        {
            "id": "2",
            "entry": "[2] J. Janisch, T. Pevn\u00fd, and V. Lis\u00fd. Classification with costly features using deep reinforcement learning. CoRR, abs/1711.07364, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07364"
        },
        {
            "id": "3",
            "entry": "[3] H.-C. Kao, K.-F. Tang, and E. Y. Chang. Context-aware symptom checking for disease diagnosis using hierarchical reinforcement learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kao%2C%20H.-C.%20Tang%2C%20K.-F.%20Chang%2C%20E.Y.%20Context-aware%20symptom%20checking%20for%20disease%20diagnosis%20using%20hierarchical%20reinforcement%20learning%202018-02-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kao%2C%20H.-C.%20Tang%2C%20K.-F.%20Chang%2C%20E.Y.%20Context-aware%20symptom%20checking%20for%20disease%20diagnosis%20using%20hierarchical%20reinforcement%20learning%202018-02-02"
        },
        {
            "id": "4",
            "entry": "[4] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "5",
            "entry": "[5] M. J. Kusner, W. Chen, Q. Zhou, Z. E. Xu, K. Q. Weinberger, and Y. Chen. Feature-cost sensitive learning with submodular trees of classifiers. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada., pages 1939\u20131945, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kusner%2C%20M.J.%20Chen%2C%20W.%20Zhou%2C%20Q.%20Xu%2C%20Z.E.%20Feature-cost%20sensitive%20learning%20with%20submodular%20trees%20of%20classifiers%202014-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kusner%2C%20M.J.%20Chen%2C%20W.%20Zhou%2C%20Q.%20Xu%2C%20Z.E.%20Feature-cost%20sensitive%20learning%20with%20submodular%20trees%20of%20classifiers%202014-07"
        },
        {
            "id": "6",
            "entry": "[6] F. Nan and V. Saligrama. Adaptive classification for prediction under a budget. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 4730\u20134740, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nan%2C%20F.%20Saligrama%2C%20V.%20Adaptive%20classification%20for%20prediction%20under%20a%20budget%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nan%2C%20F.%20Saligrama%2C%20V.%20Adaptive%20classification%20for%20prediction%20under%20a%20budget%202017-12"
        },
        {
            "id": "7",
            "entry": "[7] F. Nan, J. Wang, and V. Saligrama. Feature-budgeted random forest. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 1983\u2013 1991, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nan%2C%20F.%20Wang%2C%20J.%20Saligrama%2C%20V.%20Feature-budgeted%20random%20forest%202015-07-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nan%2C%20F.%20Wang%2C%20J.%20Saligrama%2C%20V.%20Feature-budgeted%20random%20forest%202015-07-06"
        },
        {
            "id": "8",
            "entry": "[8] F. Nan, J. Wang, and V. Saligrama. Pruning random forests for prediction on a budget. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2334\u20132342, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nan%2C%20F.%20Wang%2C%20J.%20Saligrama%2C%20V.%20Pruning%20random%20forests%20for%20prediction%20on%20a%20budget%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nan%2C%20F.%20Wang%2C%20J.%20Saligrama%2C%20V.%20Pruning%20random%20forests%20for%20prediction%20on%20a%20budget%202016-12-05"
        },
        {
            "id": "9",
            "entry": "[9] A. Y. Ng, D. Harada, and S. J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the 16th International Conference on Machine Learning ICML 1999, Bled, Slovenia, 27-30 June, 1999, pages 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20A.Y.%20Harada%2C%20D.%20Russell%2C%20S.J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20A.Y.%20Harada%2C%20D.%20Russell%2C%20S.J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999-06"
        },
        {
            "id": "10",
            "entry": "[10] Y.-S. Peng. Exploring sparse features in deep reinforcement learning towards fast disease diagnosis. Master\u2019s thesis, National Taiwan University, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20Y.-S.%20Exploring%20sparse%20features%20in%20deep%20reinforcement%20learning%20towards%20fast%20disease%20diagnosis%202018"
        },
        {
            "id": "11",
            "entry": "[11] J. Randl\u00f8v and P. Alstr\u00f8m. Learning to drive a bicycle using reinforcement learning and shaping. In Proceedings of the 15th International Conference on Machine Learning ICML 1998, Madison, Wisconsin, USA, 24-27 July, 1998, pages 463\u2013471, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Randl%C3%B8v%2C%20J.%20Alstr%C3%B8m%2C%20P.%20Learning%20to%20drive%20a%20bicycle%20using%20reinforcement%20learning%20and%20shaping%201998-07-24",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Randl%C3%B8v%2C%20J.%20Alstr%C3%B8m%2C%20P.%20Learning%20to%20drive%20a%20bicycle%20using%20reinforcement%20learning%20and%20shaping%201998-07-24"
        },
        {
            "id": "12",
            "entry": "[12] H. L. Semigran, J. A. Linder, C. Gidengil, and A. Mehrotra. Evaluation of symptom checkers for self diagnosis and triage: audit study. BMJ, 351, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Semigran%2C%20H.L.%20Linder%2C%20J.A.%20Gidengil%2C%20C.%20Mehrotra%2C%20A.%20Evaluation%20of%20symptom%20checkers%20for%20self%20diagnosis%20and%20triage%3A%20audit%20study%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Semigran%2C%20H.L.%20Linder%2C%20J.A.%20Gidengil%2C%20C.%20Mehrotra%2C%20A.%20Evaluation%20of%20symptom%20checkers%20for%20self%20diagnosis%20and%20triage%3A%20audit%20study%202015"
        },
        {
            "id": "13",
            "entry": "[13] R. Sutton and A. Barto. Reinforcement learning: An introduction. Cambridge Univ Press, second edition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.%20Barto%2C%20A.%20Reinforcement%20learning%3A%20An%20introduction%202018"
        },
        {
            "id": "14",
            "entry": "[14] K.-F. Tang, H.-C. Kao, C.-N. Chou, and E. Y. Chang. Inquire and diagnose: Neural symptom checking ensemble using deep reinforcement learning. In NIPS Workshop on Deep Reinforcement Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20K.-F.%20Kao%2C%20H.-C.%20Chou%2C%20C.-N.%20Chang%2C%20E.Y.%20Inquire%20and%20diagnose%3A%20Neural%20symptom%20checking%20ensemble%20using%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20K.-F.%20Kao%2C%20H.-C.%20Chou%2C%20C.-N.%20Chang%2C%20E.Y.%20Inquire%20and%20diagnose%3A%20Neural%20symptom%20checking%20ensemble%20using%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "15",
            "entry": "[15] H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 2094\u20132100, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Hasselt%2C%20H.%20Guez%2C%20A.%20Silver%2C%20D.%20Deep%20reinforcement%20learning%20with%20double%20q-learning%202016-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Hasselt%2C%20H.%20Guez%2C%20A.%20Silver%2C%20D.%20Deep%20reinforcement%20learning%20with%20double%20q-learning%202016-02"
        },
        {
            "id": "16",
            "entry": "[16] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371\u20133408, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20P.%20Larochelle%2C%20H.%20Lajoie%2C%20I.%20Bengio%2C%20Y.%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20P.%20Larochelle%2C%20H.%20Lajoie%2C%20I.%20Bengio%2C%20Y.%20Stacked%20denoising%20autoencoders%3A%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion%202010"
        },
        {
            "id": "17",
            "entry": "[17] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229\u2013256, May 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992-05"
        },
        {
            "id": "18",
            "entry": "[18] R. J. Williams and J. Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Peng%2C%20J.%20Function%20optimization%20using%20connectionist%20reinforcement%20learning%20algorithms%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20R.J.%20Peng%2C%20J.%20Function%20optimization%20using%20connectionist%20reinforcement%20learning%20algorithms%201991"
        },
        {
            "id": "19",
            "entry": "[19] Z. E. Xu, M. J. Kusner, K. Q. Weinberger, and M. Chen. Cost-sensitive tree of classifiers. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 133\u2013141, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Z.E.%20Kusner%2C%20M.J.%20Weinberger%2C%20K.Q.%20Chen%2C%20M.%20Cost-sensitive%20tree%20of%20classifiers%202013-06-16",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Z.E.%20Kusner%2C%20M.J.%20Weinberger%2C%20K.Q.%20Chen%2C%20M.%20Cost-sensitive%20tree%20of%20classifiers%202013-06-16"
        }
    ]
}
