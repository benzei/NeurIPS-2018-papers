{
    "filename": "7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf",
    "metadata": {
        "title": "How Many Samples are Needed to Estimate a Convolutional Neural Network?",
        "author": "Simon S. Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan R. Salakhutdinov, Aarti Singh",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A widespread folklore for explaining the success of Convolutional Neural Networks (CNNs) is that CNNs use a more compact representation than the Fullyconnected Neural Network (FNN) and thus require fewer training samples to accurately estimate their parameters. We initiate the study of rigorously characterizing the sample complexity of estimating CNNs. We show that for an m-dimensional convolutional filter with linear activation acting on a d-dimensional input, the sample complexity of achieving population prediction error of \u270f is Orpm{\u270f2q 2, whereas the sample-complexity for its FNN counterpart is lower bounded by \u2326pd{\u270f2q samples. Since, in typical settings m ! d, this result demonstrates the advantage of using a CNN. We further consider the sample complexity of estimating a onehidden-layer CNN with linear activation where both the m-dimensional convoluwtioensahlofiwltetrhaatntdhethsearm-pdliemceonmsiopnleaxliotyutipsuOtrwpemightsraqr{e\u270f2unkwnhoewnnt.heForar ttihoisbemtwodeeeln, the stride size and the filter size is a constant. For both models, we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main tools for deriving these results are a localized empirical process analysis and a new lemma characterizing the convolutional structure. We believe that these tools may inspire further developments in understanding CNNs."
    },
    "keywords": [
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Convolutional Neural Networks (CNNs) have achieved remarkable impact in many machine learning applications, including computer vision (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>), natural language processing (<a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\"><a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\">Yu et al, 2018</a></a>) and reinforcement learning (<a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>)",
        "Consider the simplest Convolutional Neural Networks, a single convolutional filter with linear activation followed by average pooling, which represents aEqual contribution. 2We use the standard big-O notation in this paper and use Orpq when we ignore poly-logarithmic factors.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montreal, Canada",
        "In Section 4 we present our main theoretical results for learning a one-hidden-layer Convolutional Neural Networks)",
        "We can show the empirical loss is a good approximation to the population loss and we used this property to derive our upper bound"
    ],
    "key_statements": [
        "Convolutional Neural Networks (CNNs) have achieved remarkable impact in many machine learning applications, including computer vision (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>), natural language processing (<a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\"><a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\">Yu et al, 2018</a></a>) and reinforcement learning (<a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>)",
        "Consider the simplest Convolutional Neural Networks, a single convolutional filter with linear activation followed by average pooling, which represents aEqual contribution. 2We use the standard big-O notation in this paper and use Orpq when we ignore poly-logarithmic factors.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montreal, Canada",
        "In Section 4 we present our main theoretical results for learning a one-hidden-layer Convolutional Neural Networks)",
        "We can show the empirical loss is a good approximation to the population loss and we used this property to derive our upper bound"
    ],
    "summary": [
        "Convolutional Neural Networks (CNNs) have achieved remarkable impact in many machine learning applications, including computer vision (<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>), natural language processing (<a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\"><a class=\"ref-link\" id=\"cYu_et+al_2018_a\" href=\"#rYu_et+al_2018_a\">Yu et al, 2018</a></a>) and reinforcement learning (<a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>).",
        "Consider the simplest CNN, a single convolutional filter with linear activation followed by average pooling, which represents aEqual contribution.",
        "We first consider the problem of learning a convolutional filter with average pooling as in Eq(1)",
        "(b) Prediction function formalized in Eq (3) It consists of a convolutional filter followed by a linear prediction layer.",
        "We are interested in learning a parameter wp n using a training sample tpxi, yiquin\u201c1 of size n so as to minimize the standardized population mean-square b prediction error err\u03bcpwp n, w0; F q \u201c Ex\u201e\u03bc |F px; wp nq F px; w0q|2.",
        "The following theorem upper bounds the expected population mean-square prediction error err\u03bcpwpn, w0; F1q of the least-square estimate wpn in Eq (8).",
        "To overcome the above difficulties, we adopt a localized empirical process approach introduced in to upper bound the expected population mean-square prediction error.",
        "We prove the following information-theoretic lower bound on Eerr\u03bcpwpn, w0q of any estimator wpn calculated on a training sample of size n.",
        "We consider a slightly more complicated convolutional network with two layers: the first layer is a single convolutional filter of size m, applied r times to a d-dimensional input vector with stride s; the second layer is a linear regression prediction layer that produces a single real-valued output.",
        "The following theorem upper bounds the population mean-square prediction error of any global minimizer wp n \u201c pwpn, panq of Eq (12).",
        "We prove the following information-theoretical lower bound on Eerr\u03bcpwp n, w0q of any estimator wp n \u201c pwpn, panq calculated on a training sample of size n.",
        "Theorema4 shows that the error of any estimator wpn computed on a training sample of size n must scale as 2prmq{n, matching the parameter counts of rm for F2.",
        "In Figure 2 and Figure 3, we consider the problem of learning a convolutional filter with average pooling which we analyzed in Section 3.",
        "We compare parameterizing the prediction function as a d-dimensional linear predictor and as a convolutional filter followed by average pooling.",
        "We fix the filter size m \u201c 8 and vary the number of training samples and the stride size.",
        "We believe if there is a better understanding of non-smooth activation which can replace our Lemma 3, we can extend our analysis framework to derive sharp sample complexity bounds for CNN with non-linear activation function."
    ],
    "headline": "We show that for an m-dimensional convolutional filter with linear activation acting on a d-dimensional input, the sample complexity of achieving population prediction error of \u270f is Orpm{\u270f2q 2, whereas the sample-complexity for its Fullyconnected Neural Network counterpart is lower bounded by \u2326pd{\u270f2q samples",
    "reference_links": [
        {
            "id": "Anthony_2009_a",
            "entry": "Anthony, M., & Bartlett, P. L. (2009). Neural network learning: Theoretical foundations. cambridge university press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20M.%20Bartlett%2C%20P.L.%20Neural%20network%20learning%3A%20Theoretical%20foundations%202009"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Arora, S., Ge, R., Neyshabur, B., & Zhang, Y. (2018). Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05296"
        },
        {
            "id": "Bartlett_et+al_2017_a",
            "entry": "Bartlett, P. L., Foster, D. J., & Telgarsky, M. J. (2017a). Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, (pp. 6241\u20136250).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "Bartlett_et+al_2017_b",
            "entry": "Bartlett, P. L., Harvey, N., Liaw, C., & Mehrabian, A. (2017b). Nearly-tight vcdimension and pseudodimension bounds for piecewise linear neural networks. arxiv preprint. arXiv, 1703.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Harvey%2C%20N.%20Liaw%2C%20C.%20Mehrabian%2C%20A.%20Nearly-tight%20vcdimension%20and%20pseudodimension%20bounds%20for%20piecewise%20linear%20neural%20networks.%20arxiv%20p%202017"
        },
        {
            "id": "Bickel_et+al_2009_a",
            "entry": "Bickel, P. J., Ritov, Y., & Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4), 1705\u20131732.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bickel%2C%20P.J.%20Ritov%2C%20Y.%20Tsybakov%2C%20A.B.%20Simultaneous%20analysis%20of%20lasso%20and%20dantzig%20selector%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bickel%2C%20P.J.%20Ritov%2C%20Y.%20Tsybakov%2C%20A.B.%20Simultaneous%20analysis%20of%20lasso%20and%20dantzig%20selector%202009"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Brutzkus, A., & Globerson, A. (2017). Globally optimal gradient descent for a Convnet with Gaussian inputs. arXiv preprint arXiv:1702.07966.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07966"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B., & LeCun, Y. (2015). The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, (pp. 192\u2013204).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Du_2018_a",
            "entry": "Du, S. S., & Lee, J. D. (2018). On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01206"
        },
        {
            "id": "Du_et+al_2017_a",
            "entry": "Du, S. S., Lee, J. D., & Tian, Y. (2017a). When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06129"
        },
        {
            "id": "Du_et+al_2017_b",
            "entry": "Du, S. S., Lee, J. D., Tian, Y., Poczos, B., & Singh, A. (2017b). Gradient descent learns one-hiddenlayer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00779"
        },
        {
            "id": "Dudley_1967_a",
            "entry": "Dudley, R. M. (1967). The sizes of compact subsets of hilbert space and continuity of gaussian processes. Journal of Functional Analysis, 1(3), 290\u2013330.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dudley%2C%20R.M.%20The%20sizes%20of%20compact%20subsets%20of%20hilbert%20space%20and%20continuity%20of%20gaussian%20processes%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dudley%2C%20R.M.%20The%20sizes%20of%20compact%20subsets%20of%20hilbert%20space%20and%20continuity%20of%20gaussian%20processes%201967"
        },
        {
            "id": "Freeman_2016_a",
            "entry": "Freeman, C. D., & Bruna, J. (2016). Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01540"
        },
        {
            "id": "Ge_et+al_2017_a",
            "entry": "Ge, R., Jin, C., & Zheng, Y. (2017a). No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proceedings of the 34th International Conference on Machine Learning, (pp. 1233\u20131242).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Jin%2C%20C.%20Zheng%2C%20Y.%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Jin%2C%20C.%20Zheng%2C%20Y.%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017"
        },
        {
            "id": "Ge_et+al_2017_b",
            "entry": "Ge, R., Lee, J. D., & Ma, T. (2017b). Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "Goel_et+al_2016_a",
            "entry": "Goel, S., Kanade, V., Klivans, A., & Thaler, J. (2016). Reliably learning the ReLU in polynomial time. arXiv preprint arXiv:1611.10258.",
            "arxiv_url": "https://arxiv.org/pdf/1611.10258"
        },
        {
            "id": "Goel_2017_a",
            "entry": "Goel, S., & Klivans, A. (2017a). Eigenvalue decay implies polynomial-time learnability for neural networks. arXiv preprint arXiv:1708.03708.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03708"
        },
        {
            "id": "Goel_2017_b",
            "entry": "Goel, S., & Klivans, A. (2017b). Learning depth-three neural networks in polynomial time. arXiv preprint arXiv:1709.06010.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06010"
        },
        {
            "id": "Goel_et+al_2018_a",
            "entry": "Goel, S., Klivans, A., & Meka, R. (2018). Learning one convolutional layer with overlapping patches. arXiv preprint arXiv:1802.02547.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02547"
        },
        {
            "id": "Graham_1980_a",
            "entry": "Graham, R., & Sloane, N. (1980). Lower bounds for constant weight codes. IEEE Transactions on Information Theory, 26(1), 37\u201343.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graham%2C%20R.%20Sloane%2C%20N.%20Lower%20bounds%20for%20constant%20weight%20codes%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graham%2C%20R.%20Sloane%2C%20N.%20Lower%20bounds%20for%20constant%20weight%20codes%201980"
        },
        {
            "id": "Haeffele_2015_a",
            "entry": "Haeffele, B. D., & Vidal, R. (2015). Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540.",
            "arxiv_url": "https://arxiv.org/pdf/1506.07540"
        },
        {
            "id": "Hardt_2016_a",
            "entry": "Hardt, M., & Ma, T. (2016). Identity matters in deep learning. arXiv preprint arXiv:1611.04231.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04231"
        },
        {
            "id": "Hoeffding_1963_a",
            "entry": "Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301), 13\u201330.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoeffding%2C%20W.%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoeffding%2C%20W.%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963"
        },
        {
            "id": "Huang_2015_a",
            "entry": "Huang, F., & Anandkumar, A. (2015). Convolutional dictionary learning through tensor factorization. In Feature Extraction: Modern Questions and Challenges, (pp. 116\u2013129).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20F.%20Anandkumar%2C%20A.%20Convolutional%20dictionary%20learning%20through%20tensor%20factorization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20F.%20Anandkumar%2C%20A.%20Convolutional%20dictionary%20learning%20through%20tensor%20factorization%202015"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kawaguchi, K. (2016). Deep learning without poor local minima. In Advances in Neural Information Processing Systems, (pp. 586\u2013594).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20K.%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Konstantinos_et+al_2017_a",
            "entry": "Konstantinos, P., Davies, M., & Vandergheynst, P. (2017). Pac-bayesian margin bounds for convolutional neural networks-technical report. arXiv preprint arXiv:1801.00171.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00171"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, (pp. 1097\u20131105).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lecun_1995_a",
            "entry": "LeCun, Y., Bengio, Y., et al. (1995). Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10), 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Convolutional%20networks%20for%20images%2C%20speech%2C%20and%20time%20series%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Convolutional%20networks%20for%20images%2C%20speech%2C%20and%20time%20series%201995"
        },
        {
            "id": "Li_2017_a",
            "entry": "Li, Y., & Yuan, Y. (2017). Convergence analysis of two-layer neural networks with ReLU activation. arXiv preprint arXiv:1705.09886.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09886"
        },
        {
            "id": "Neyshabur_et+al_2017_a",
            "entry": "Neyshabur, B., Bhojanapalli, S., McAllester, D., & Srebro, N. (2017). A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09564"
        },
        {
            "id": "Nguyen_2017_a",
            "entry": "Nguyen, Q., & Hein, M. (2017a). The loss surface and expressivity of deep convolutional neural networks. arXiv preprint arXiv:1710.10928.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10928"
        },
        {
            "id": "Nguyen_2017_b",
            "entry": "Nguyen, Q., & Hein, M. (2017b). The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08045"
        },
        {
            "id": "Qu_et+al_2017_a",
            "entry": "Qu, Q., Zhang, Y., Eldar, Y. C., & Wright, J. (2017). Convolutional phase retrieval via gradient descent. arXiv preprint arXiv:1712.00716.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00716"
        },
        {
            "id": "Safran_2016_a",
            "entry": "Safran, I., & Shamir, O. (2016). On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, (pp. 774\u2013782).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20I.%20Shamir%2C%20O.%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20I.%20Shamir%2C%20O.%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016"
        },
        {
            "id": "Safran_2017_a",
            "entry": "Safran, I., & Shamir, O. (2017). Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08968"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search. Nature, 529(7587), 484\u2013489.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Singh_et+al_2018_a",
            "entry": "Singh, S., Poczos, B., & Ma, J. (2018). Minimax reconstruction risk of convolutional sparse dictionary learning. In International Conference on Artificial Intelligence and Statistics, (pp. 1327\u2013 1336).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20S.%20Poczos%2C%20B.%20Ma%2C%20J.%20Minimax%20reconstruction%20risk%20of%20convolutional%20sparse%20dictionary%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20S.%20Poczos%2C%20B.%20Ma%2C%20J.%20Minimax%20reconstruction%20risk%20of%20convolutional%20sparse%20dictionary%20learning%202018"
        },
        {
            "id": "Song_et+al_2017_a",
            "entry": "Song, L., Vempala, S., Wilmes, J., & Xie, B. (2017). On the complexity of learning neural networks. In Advances in Neural Information Processing Systems, (pp. 5520\u20135528).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20L.%20Vempala%2C%20S.%20Wilmes%2C%20J.%20Xie%2C%20B.%20On%20the%20complexity%20of%20learning%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20L.%20Vempala%2C%20S.%20Wilmes%2C%20J.%20Xie%2C%20B.%20On%20the%20complexity%20of%20learning%20neural%20networks%202017"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Tian, Y. (2017). An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00560"
        },
        {
            "id": "Tsybakov_2009_a",
            "entry": "Tsybakov, A. B. (2009). Introduction to nonparametric estimation. Springer Series in Statistics. Springer, New York.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20A.B.%20Introduction%20to%20nonparametric%20estimation.%20Springer%20Series%20in%20Statistics%202009"
        },
        {
            "id": "Van_2000_a",
            "entry": "van de Geer, S. A. (2000). Empirical Processes in M-estimation, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20de%20Geer%2C%20S.A.%20Empirical%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20de%20Geer%2C%20S.A.%20Empirical%202000"
        },
        {
            "id": "6",
            "entry": "6. Cambridge university press. Van der Vaart, A. W. (1998). Asymptotic statistics, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cambridge%20university%20press.%20Van%20der%20Vaart%2C%20A.W.%20Asymptotic%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cambridge%20university%20press.%20Van%20der%20Vaart%2C%20A.W.%20Asymptotic%201998"
        },
        {
            "id": "3",
            "entry": "3. Cambridge university press. Vershynin, R. (2012). How close is the sample covariance matrix to the actual covariance matrix?",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cambridge%20university%20press.%20Vershynin%2C%20R.%20How%20close%20is%20the%20sample%20covariance%20matrix%20to%20the%20actual%20covariance%20matrix%3F%202012"
        },
        {
            "id": "Of_0000_a",
            "entry": "Journal of Theoretical Probability, 25(3), 655\u2013686.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Journal%20of%20Theoretical%20Probability%20253%20655686",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Journal%20of%20Theoretical%20Probability%20253%20655686"
        },
        {
            "id": "Wang_2016_a",
            "entry": "Wang, Y., & Singh, A. (2016). Noise-adaptive margin-based active learning and lower bounds under tsybakov noise condition. In AAAI. Wasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Singh%2C%20A.%20Noise-adaptive%20margin-based%20active%20learning%20and%20lower%20bounds%20under%20tsybakov%20noise%20condition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Singh%2C%20A.%20Noise-adaptive%20margin-based%20active%20learning%20and%20lower%20bounds%20under%20tsybakov%20noise%20condition%202016"
        },
        {
            "id": "Yu_et+al_2018_a",
            "entry": "Yu, A. W., Dohan, D., Luong, M.-T., Zhao, R., Chen, K., Norouzi, M., & Le, Q. V. (2018). Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09541"
        },
        {
            "id": "Yu_1997_a",
            "entry": "Yu, B. (1997). Assouad, fano, and le cam. In Festschrift for Lucien Le Cam, (pp. 423\u2013435). Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20B.%20Assouad%2C%20fano%2C%20and%20le%20cam.%20In%20Festschrift%20for%20Lucien%20Le%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20B.%20Assouad%2C%20fano%2C%20and%20le%20cam.%20In%20Festschrift%20for%20Lucien%20Le%201997"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Zhang, Y., Lau, Y., Kuo, H.-w., Cheung, S., Pasupathy, A., & Wright, J. (2017). On the global geometry of sphere-constrained sparse blind deconvolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (pp. 4894\u20134902).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Lau%2C%20Y.%20Kuo%2C%20H.-w%20Cheung%2C%20S.%20On%20the%20global%20geometry%20of%20sphere-constrained%20sparse%20blind%20deconvolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Lau%2C%20Y.%20Kuo%2C%20H.-w%20Cheung%2C%20S.%20On%20the%20global%20geometry%20of%20sphere-constrained%20sparse%20blind%20deconvolution%202017"
        },
        {
            "id": "Zhang_et+al_2015_a",
            "entry": "Zhang, Y., Lee, J. D., Wainwright, M. J., & Jordan, M. I. (2015). Learning halfspaces and neural networks with random initialization. arXiv preprint arXiv:1511.07948.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07948"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Zhong, K., Song, Z., & Dhillon, I. S. (2017a). Learning non-overlapping convolutional neural networks with multiple kernels. arXiv preprint arXiv:1711.03440.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03440"
        },
        {
            "id": "Zhong_et+al_2017_b",
            "entry": "Zhong, K., Song, Z., Jain, P., Bartlett, P. L., & Dhillon, I. S. (2017b). Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        },
        {
            "id": "Zhou_2017_a",
            "entry": "Zhou, P., & Feng, J. (2017). The landscape of deep learning algorithms. arXiv preprint arXiv:1705.07038.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07038"
        }
    ]
}
