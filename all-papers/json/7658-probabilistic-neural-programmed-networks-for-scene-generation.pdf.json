{
    "filename": "7658-probabilistic-neural-programmed-networks-for-scene-generation.pdf",
    "metadata": {
        "title": "Probabilistic Neural Programmed Networks for Scene Generation",
        "author": "Zhiwei Deng, Jiacheng Chen, YIFANG FU, Greg Mori",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7658-probabilistic-neural-programmed-networks-for-scene-generation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper we address the text to scene image generation problem. Generative models that capture the variability in complicated scenes containing rich semantics is a grand goal of image generation. Complicated scene images contain varied visual elements, compositional visual concepts, and complicated relations between objects. Generative models, as an analysis-by-synthesis process, should encompass the following three core components: 1) the generation process that composes the scene; 2) what are the primitive visual elements and how are they composed; 3) the rendering of abstract concepts into their pixel-level realizations. We propose PNPNet, a variational auto-encoder framework that addresses these three challenges: it flexibly composes images with a dynamic network structure, learns a set of distribution transformers that can compose distributions based on semantics, and decodes samples from these distributions into realistic images."
    },
    "keywords": [
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "high level",
            "url": "https://en.wikipedia.org/wiki/high_level"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "auto encoder",
            "url": "https://en.wikipedia.org/wiki/auto_encoder"
        },
        {
            "term": "Product of Experts",
            "url": "https://en.wikipedia.org/wiki/Product_of_Experts"
        },
        {
            "term": "mean average precision",
            "url": "https://en.wikipedia.org/wiki/mean_average_precision"
        },
        {
            "term": "Evidence Lower Bound",
            "url": "https://en.wikipedia.org/wiki/Evidence_Lower_Bound"
        },
        {
            "term": "abstract concept",
            "url": "https://en.wikipedia.org/wiki/abstract_concept"
        }
    ],
    "highlights": [
        "We advocate for the use of a modular, compositional prior that permits learning disentangled representations that flexibly scale to variable numbers of, and relationships between, concepts in a scene",
        "Accompanying generative models to decode these rich representations into their varied realizations as data instances, along with learning of the latent representation directly from data represent a coveted guerdon of AI research",
        "Impressive strides in generative models for images have been made via a number of advances in continuous stochastic latent variable models under the variational auto-encoder (VAE) formalism [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We demonstrate that we can learn compositional semantic priors that can capture the variability in complex scenes",
        "Given the primitive concept latent distributions derived from the concept mapping operator, we have the basic elements for composing objects and more complex scenes",
        "We proposed a novel programmatic approach to constructing priors for generative modeling of complex scenes"
    ],
    "key_statements": [
        "We advocate for the use of a modular, compositional prior that permits learning disentangled representations that flexibly scale to variable numbers of, and relationships between, concepts in a scene",
        "Accompanying generative models to decode these rich representations into their varied realizations as data instances, along with learning of the latent representation directly from data represent a coveted guerdon of AI research",
        "Impressive strides in generative models for images have been made via a number of advances in continuous stochastic latent variable models under the variational auto-encoder (VAE) formalism [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We demonstrate that we can learn compositional semantic priors that can capture the variability in complex scenes",
        "Given the primitive concept latent distributions derived from the concept mapping operator, we have the basic elements for composing objects and more complex scenes",
        "In the rest of this section, we describe a set of aggregation mapping functions which operate on distributions",
        "We propose to use the mean average precision of a pre-trained detector to measure whether the conditional generated image samples possess desired semantic information",
        "We proposed a novel programmatic approach to constructing priors for generative modeling of complex scenes"
    ],
    "summary": [
        "We advocate for the use of a modular, compositional prior that permits learning disentangled representations that flexibly scale to variable numbers of, and relationships between, concepts in a scene.",
        "These models outperform competing approaches on Color-MNIST and CLEVR-G image generation tasks, while yielding semantically-meaningful latent representations.",
        "This paper proposes a generative modeling approach by linking language elements to both semantic latent priors and functional symbolic networks.",
        ", \u00b7), which take either semantic concepts or a series of distributions as input, and generate distributions over the latent space capturing their combined meaning; 2) a probabilistic modeling framework which performs inference and learning using this latent space.",
        "We first define a concept mapping operator \u03c4concept(w) which takes the concept word w, and generates distributions in latent space which can describe the properties of the concept.",
        "Given the primitive concept latent distributions derived from the concept mapping operator, we have the basic elements for composing objects and more complex scenes.",
        "We first sample a tuple l = (x, y) from p(l|zs), indicating the offsets between two children\u2019s bounding boxes in latent space, where zs \u223c p and p is generated by our concept mapping operator \u03c4concept taking the relation word of layout node.",
        "A model should be able to gradually compose from primitive latent distributions to a full latent distribution p(z|y) describing a complex scene, where it samples a latent",
        "Note that to better train the Transform and Layout operators, we use ground truth bounding boxes as auxiliary loss for learning size and offset samplers.",
        "Through defining a small set of operators/modules, the prior distribution p(z|y) conditioned on the semantic concepts can acquire much more flexibility.",
        "We propose to use the mean average precision of a pre-trained detector to measure whether the conditional generated image samples possess desired semantic information.",
        "In all the following experiment tables, we report accuracy of detectors on ground truth data by standard mAP to verify that the detectors are highly accurate, and our proposed detector score on generated images by models.",
        "The product of experts combines all concepts and generates a text based distribution without considering scene structures.",
        "We demonstrated that these priors can be used to model the variability and compositional aspects of complex images consisting of multiple entities with different properties, outperforming related methods that do not model scene structure."
    ],
    "headline": "In this paper we address the text to scene image generation problem",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "2",
            "entry": "[2] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and Aaron Courville. Pixelvae: A latent variable model for natural images. ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Kumar%2C%20Kundan%20Ahmed%2C%20Faruk%20Taiga%2C%20Adrien%20Ali%20Pixelvae%3A%20A%20latent%20variable%20model%20for%20natural%20images%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Kumar%2C%20Kundan%20Ahmed%2C%20Faruk%20Taiga%2C%20Adrien%20Ali%20Pixelvae%3A%20A%20latent%20variable%20model%20for%20natural%20images%202017"
        },
        {
            "id": "3",
            "entry": "[3] Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02731"
        },
        {
            "id": "4",
            "entry": "[4] Scott Reed and Nando de Freitas. Neural programmer-interpreters. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%20Reed%20and%20Nando%20de%20Freitas%20Neural%20programmerinterpreters%20In%20ICLR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scott%20Reed%20and%20Nando%20de%20Freitas%20Neural%20programmerinterpreters%20In%20ICLR%202016"
        },
        {
            "id": "5",
            "entry": "[5] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering with neural module networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Deep%20compositional%20question%20answering%20with%20neural%20module%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Deep%20compositional%20question%20answering%20with%20neural%20module%20networks%202016"
        },
        {
            "id": "6",
            "entry": "[6] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, and Oriol Vinyals. Synthesizing programs for images using reinforced adversarial learning. CoRR, abs/1804.01118, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01118"
        },
        {
            "id": "7",
            "entry": "[7] Tuan Anh Le, Atilim Gunes Baydin, and Frank D. Wood. Inference compilation and universal probabilistic programming. CoRR, abs/1610.09900, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09900"
        },
        {
            "id": "8",
            "entry": "[8] Jiajun Wu, Joshua B. Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiajun%20Wu%20Joshua%20B%20Tenenbaum%20and%20Pushmeet%20Kohli%20Neural%20scene%20derendering%20In%20CVPR%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiajun%20Wu%20Joshua%20B%20Tenenbaum%20and%20Pushmeet%20Kohli%20Neural%20scene%20derendering%20In%20CVPR%202017"
        },
        {
            "id": "9",
            "entry": "[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "10",
            "entry": "[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20Efros%2C%20Alexei%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20Efros%2C%20Alexei%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "11",
            "entry": "[11] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Park%2C%20Taesung%20Isola%2C%20Phillip%20Efros%2C%20Alexei%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        },
        {
            "id": "12",
            "entry": "[12] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert. The pose knows: Video forecasting by generating pose futures. In International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20Jacob%20Marino%2C%20Kenneth%20Gupta%2C%20Abhinav%20Hebert%2C%20Martial%20The%20pose%20knows%3A%20Video%20forecasting%20by%20generating%20pose%20futures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20Jacob%20Marino%2C%20Kenneth%20Gupta%2C%20Abhinav%20Hebert%2C%20Martial%20The%20pose%20knows%3A%20Video%20forecasting%20by%20generating%20pose%20futures%202017"
        },
        {
            "id": "13",
            "entry": "[13] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate long-term future via hierarchical prediction. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Zou%2C%20Yuliang%20Sohn%2C%20Sungryull%20Learning%20to%20generate%20long-term%20future%20via%20hierarchical%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20Ruben%20Yang%2C%20Jimei%20Zou%2C%20Yuliang%20Sohn%2C%20Sungryull%20Learning%20to%20generate%20long-term%20future%20via%20hierarchical%20prediction%202017"
        },
        {
            "id": "14",
            "entry": "[14] Shizhan Zhu, Sanja Fidler, Raquel Urtasun, Dahua Lin, and Chen Change Loy. Be your own prada: Fashion synthesis with structural coherence. CoRR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Shizhan%20Fidler%2C%20Sanja%20Urtasun%2C%20Raquel%20Lin%2C%20Dahua%20Be%20your%20own%20prada%3A%20Fashion%20synthesis%20with%20structural%20coherence%202017"
        },
        {
            "id": "15",
            "entry": "[15] Levent Karacan, Zeynep Akata, Aykut Erdem, and Erkut Erdem. Learning to generate images of outdoor scenes from attributes and semantic layouts. CoRR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karacan%2C%20Levent%20Akata%2C%20Zeynep%20Erdem%2C%20Aykut%20Erdem%2C%20Erkut%20Learning%20to%20generate%20images%20of%20outdoor%20scenes%20from%20attributes%20and%20semantic%20layouts%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karacan%2C%20Levent%20Akata%2C%20Zeynep%20Erdem%2C%20Aykut%20Erdem%2C%20Erkut%20Learning%20to%20generate%20images%20of%20outdoor%20scenes%20from%20attributes%20and%20semantic%20layouts%202016"
        },
        {
            "id": "16",
            "entry": "[16] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549\u20133557, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20Besse%2C%20Frederic%20Rezende%2C%20Danilo%20Jimenez%20Danihelka%2C%20Ivo%20Towards%20conceptual%20compression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20Karol%20Besse%2C%20Frederic%20Rezende%2C%20Danilo%20Jimenez%20Danihelka%2C%20Ivo%20Towards%20conceptual%20compression%202016"
        },
        {
            "id": "17",
            "entry": "[17] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "18",
            "entry": "[18] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, pages 4790\u20134798, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20Aaron%20Kalchbrenner%2C%20Nal%20Espeholt%2C%20Lasse%20Vinyals%2C%20Oriol%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016"
        },
        {
            "id": "19",
            "entry": "[19] Matthew D Hoffman. Learning deep latent gaussian models with markov chain monte carlo. In International Conference on Machine Learning, pages 1510\u20131519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Learning%20deep%20latent%20gaussian%20models%20with%20markov%20chain%20monte%20carlo%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Learning%20deep%20latent%20gaussian%20models%20with%20markov%20chain%20monte%20carlo%202017"
        },
        {
            "id": "20",
            "entry": "[20] Jiajun Wu, Erika Lu, Pushmeet Kohli, William T Freeman, and Joshua B Tenenbaum. Learning to see physics via visual de-animation. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiajun%20Lu%2C%20Erika%20Kohli%2C%20Pushmeet%20Freeman%2C%20William%20T.%20Learning%20to%20see%20physics%20via%20visual%20de-animation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiajun%20Lu%2C%20Erika%20Kohli%2C%20Pushmeet%20Freeman%2C%20William%20T.%20Learning%20to%20see%20physics%20via%20visual%20de-animation%202017"
        },
        {
            "id": "21",
            "entry": "[21] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Hoffman%2C%20Judy%20Inferring%20and%20executing%20programs%20for%20visual%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Hariharan%2C%20Bharath%20van%20der%20Maaten%2C%20Laurens%20Hoffman%2C%20Judy%20Inferring%20and%20executing%20programs%20for%20visual%20reasoning%202017"
        },
        {
            "id": "22",
            "entry": "[22] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Gupta%2C%20Agrim%20Fei-Fei%2C%20Li%20Image%20generation%20from%20scene%20graphs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Gupta%2C%20Agrim%20Fei-Fei%2C%20Li%20Image%20generation%20from%20scene%20graphs%202018"
        },
        {
            "id": "23",
            "entry": "[23] Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In arXiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20Emilio%20rahman%20Mohamed%2C%20Abdel%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20Emilio%20rahman%20Mohamed%2C%20Abdel%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202016"
        },
        {
            "id": "24",
            "entry": "[24] Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David M Blei, and Michael I Jordan. Matching words and pictures. Journal of machine learning research, 3(Feb):1107\u20131135, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barnard%2C%20Kobus%20Duygulu%2C%20Pinar%20Forsyth%2C%20David%20de%20Freitas%2C%20Nando%20Matching%20words%20and%20pictures%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barnard%2C%20Kobus%20Duygulu%2C%20Pinar%20Forsyth%2C%20David%20de%20Freitas%2C%20Nando%20Matching%20words%20and%20pictures%202003"
        },
        {
            "id": "25",
            "entry": "[25] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Justin%20Karpathy%2C%20Andrej%20Fei-Fei%2C%20Li%20Densecap%3A%20Fully%20convolutional%20localization%20networks%20for%20dense%20captioning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Justin%20Karpathy%2C%20Andrej%20Fei-Fei%2C%20Li%20Densecap%3A%20Fully%20convolutional%20localization%20networks%20for%20dense%20captioning%202016"
        },
        {
            "id": "26",
            "entry": "[26] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2625\u20132634, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20Anne%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeffrey%20Hendricks%2C%20Lisa%20Anne%20Guadarrama%2C%20Sergio%20Rohrbach%2C%20Marcus%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015"
        },
        {
            "id": "27",
            "entry": "[27] Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep fragment embeddings for bidirectional imagesentence mapping. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Joulin%2C%20Armand%20Fei-Fei%2C%20Li%20Deep%20fragment%20embeddings%20for%20bidirectional%20imagesentence%20mapping%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Joulin%2C%20Armand%20Fei-Fei%2C%20Li%20Deep%20fragment%20embeddings%20for%20bidirectional%20imagesentence%20mapping%202014"
        },
        {
            "id": "28",
            "entry": "[28] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121\u20132129, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013"
        },
        {
            "id": "29",
            "entry": "[29] Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In Advances in neural information processing systems, pages 2539\u20132547, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Tejas%20D.%20Whitney%2C%20William%20F.%20Kohli%2C%20Pushmeet%20Tenenbaum%2C%20Josh%20Deep%20convolutional%20inverse%20graphics%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Tejas%20D.%20Whitney%2C%20William%20F.%20Kohli%2C%20Pushmeet%20Tenenbaum%2C%20Josh%20Deep%20convolutional%20inverse%20graphics%20network%202015"
        },
        {
            "id": "30",
            "entry": "[30] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "31",
            "entry": "[31] Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In Advances in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20Scott%20E.%20Zhang%2C%20Yi%20Zhang%2C%20Yuting%20Lee%2C%20Honglak%20Deep%20visual%20analogy-making%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20Scott%20E.%20Zhang%2C%20Yi%20Zhang%2C%20Yuting%20Lee%2C%20Honglak%20Deep%20visual%20analogy-making%202015"
        },
        {
            "id": "32",
            "entry": "[32] Z. Deng, R. Navarathna, P. Carr, S. Mandt, Y. Yue, I. Matthews, and G. Mori. Factorized variational autoencoders for modeling audience reactions to movies. In Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Z.%20Navarathna%2C%20R.%20Carr%2C%20P.%20Mandt%2C%20S.%20Factorized%20variational%20autoencoders%20for%20modeling%20audience%20reactions%20to%20movies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Z.%20Navarathna%2C%20R.%20Carr%2C%20P.%20Mandt%2C%20S.%20Factorized%20variational%20autoencoders%20for%20modeling%20audience%20reactions%20to%20movies%202017"
        },
        {
            "id": "33",
            "entry": "[33] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "34",
            "entry": "[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "35",
            "entry": "[35] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.05396"
        },
        {
            "id": "36",
            "entry": "[36] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "37",
            "entry": "[37] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.05517"
        },
        {
            "id": "38",
            "entry": "[38] Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of visually grounded imagination. arXiv preprint arXiv:1705.10762, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1705.10762"
        }
    ]
}
