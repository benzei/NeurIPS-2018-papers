{
    "filename": "7429-bilinear-attention-networks.pdf",
    "metadata": {
        "title": "Bilinear Attention Networks",
        "author": "Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets."
    },
    "keywords": [
        {
            "term": "computational cost",
            "url": "https://en.wikipedia.org/wiki/computational_cost"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        }
    ],
    "highlights": [
        "Machine learning for computer vision and natural language processing accelerates the advancement of artificial intelligence",
        "We propose a variant of multimodal residual networks (MRN) to efficiently utilize the multiple bilinear attention maps generated by our model",
        "We define the bilinear attention networks as a function of two multi-channel inputs parameterized by a bilinear attention map as follows: f = bilinear attention networks(X, Y; A)",
        "Inspired by multimodal residual networks (MRN) from Kim et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], we propose a variant of multimodal residual networks to integrate the joint representations from the multiple bilinear attention maps",
        "bilinear attention networks considers every pair of multimodal input channels, the computational cost remains in the same magnitude, since bilinear attention networks consists of matrix chain multiplication for efficient computation",
        "We believe our bilinear attention networks gives a new opportunity to learn the richer joint representation for multimodal multi-channel inputs, which appear in many real-world problems"
    ],
    "key_statements": [
        "Machine learning for computer vision and natural language processing accelerates the advancement of artificial intelligence",
        "Since vision and natural language are the major modalities of human interaction, understanding and reasoning of vision and natural language information become a key challenge",
        "We propose bilinear attention networks (BAN) to use a bilinear attention distribution, on top of low-rank bilinear pooling [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "Notice that the bilinear attention networks exploits bilinear interactions between two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels",
        "We propose the bilinear attention networks (BAN) to learn and use bilinear attention distributions, on top of the low-rank bilinear pooling technique",
        "We propose a variant of multimodal residual networks (MRN) to efficiently utilize the multiple bilinear attention maps generated by our model",
        "To reduce both input channel simultaneously, we introduce bilinear attention map A 2 R\u21e2\u21e5 as follows: fk0 = (XT U0)Tk A(YT V0)k",
        "We define the bilinear attention networks as a function of two multi-channel inputs parameterized by a bilinear attention map as follows: f = bilinear attention networks(X, Y; A)",
        "Inspired by multimodal residual networks (MRN) from Kim et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], we propose a variant of multimodal residual networks to integrate the joint representations from the multiple bilinear attention maps",
        "We evaluate on the Visual Question Answering 2.0 dataset [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], which is improved from the previous version to emphasize visual understanding by reducing the answer bias in the dataset",
        "We argue that this is a novel observation where the residual learning [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] is used for stacked attention networks",
        "Our bilinear attention map to predict the boxes for the phrase entities in a sentence achieves new state-of-the-art with 69.69% for Recall@1",
        "bilinear attention networks considers every pair of multimodal input channels, the computational cost remains in the same magnitude, since bilinear attention networks consists of matrix chain multiplication for efficient computation",
        "We believe our bilinear attention networks gives a new opportunity to learn the richer joint representation for multimodal multi-channel inputs, which appear in many real-world problems"
    ],
    "summary": [
        "Machine learning for computer vision and natural language processing accelerates the advancement of artificial intelligence.",
        "We propose the bilinear attention networks (BAN) to learn and use bilinear attention distributions, on top of the low-rank bilinear pooling technique.",
        "We propose a variant of multimodal residual networks (MRN) to efficiently utilize the multiple bilinear attention maps generated by our model.",
        "We evaluate the visual grounding of bilinear attention map on Flickr30k Entities [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] outperforming previous methods, along with 25.37% improvement of inference speed taking advantage of the processing of multi-channel inputs.",
        "Two single channel inputs x and ycan be used to get the joint representation using the other low-rank bilinear pooling for a classifier.",
        "To reduce both input channel simultaneously, we introduce bilinear attention map A 2 R\u21e2\u21e5 as follows: fk0 = (XT U0)Tk A(YT V0)k",
        "For each pair of channels, the 1-rank bilinear representation of two feature vectors is modeled in XTi (U0kVk0T )Yj of Equation 6.",
        "We define the bilinear attention networks as a function of two multi-channel inputs parameterized by a bilinear attention map as follows: f = BAN(X, Y; A).",
        "Inspired by multimodal residual networks (MRN) from Kim et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], we propose a variant of MRN to integrate the joint representations from the multiple bilinear attention maps.",
        "This improvement pushes the model to have the more effective joint representation of question and image, which fits the motivation of our bilinear attention approach.",
        "For the evaluation of visual grounding by the bilinear attention maps, we use Flickr30k Entities [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] consisting of 31,783 images [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>] and 244,035 annotations that multiple entities in a sentence for an image are mapped to the boxes on the image to indicate the correspondences between them.",
        "BAN significantly outperforms this baseline and successfully utilize up to eight bilinear attention maps to improve its performance taking advantage of residual learning of attention.",
        "BAN-Glove-Counter uses both the previous 600-dimensional word embeddings and counting module [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], which exploits spatial information of detected object boxes from the feature extractor [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>].",
        "Our bilinear attention map to predict the boxes for the phrase entities in a sentence achieves new state-of-the-art with 69.69% for Recall@1.",
        "BAN gracefully extends unitary attention networks exploiting bilinear attention maps, where the joint representations of multimodal multi-channel inputs are extracted using low-rank bilinear pooling.",
        "The proposed residual learning of attention efficiently uses up to eight bilinear attention maps, keeping the size of intermediate features constant.",
        "We believe our BAN gives a new opportunity to learn the richer joint representation for multimodal multi-channel inputs, which appear in many real-world problems"
    ],
    "headline": "We propose bilinear attention networks  that find bilinear attention distributions to utilize given vision-language information seamlessly",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Vqa: Visual question answering. International Journal of Computer Vision, 123(1):4\u201331, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Antol%2C%20Stanislaw%20Margaret%20Mitchell%2C%20C.Lawrence%20Zitnick%20Vqa%3A%20Visual%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Antol%2C%20Stanislaw%20Margaret%20Mitchell%2C%20C.Lawrence%20Zitnick%20Vqa%3A%20Visual%20question%20answering%202017"
        },
        {
            "id": "2",
            "entry": "[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. arXiv preprint arXiv:1707.07998, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07998"
        },
        {
            "id": "3",
            "entry": "[3] Pablo Arbel\u00e1ez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial grouping. In IEEE conference on computer vision and pattern recognition, pages 328\u2013335, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arbel%C3%A1ez%2C%20Pablo%20Pont-Tuset%2C%20Jordi%20Barron%2C%20Jonathan%20T.%20Marques%2C%20Ferran%20Multiscale%20combinatorial%20grouping%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arbel%C3%A1ez%2C%20Pablo%20Pont-Tuset%2C%20Jordi%20Barron%2C%20Jonathan%20T.%20Marques%2C%20Ferran%20Multiscale%20combinatorial%20grouping%202014"
        },
        {
            "id": "4",
            "entry": "[4] Hedi Ben-younes, R\u00e9mi Cadene, Matthieu Cord, and Nicolas Thome. MUTAN: Multimodal Tucker Fusion for Visual Question Answering. In IEEE International Conference on Computer Vision, pages 2612\u20132620, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-younes%2C%20Hedi%20Cadene%2C%20R%C3%A9mi%20Cord%2C%20Matthieu%20Thome%2C%20Nicolas%20MUTAN%3A%20Multimodal%20Tucker%20Fusion%20for%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-younes%2C%20Hedi%20Cadene%2C%20R%C3%A9mi%20Cord%2C%20Matthieu%20Thome%2C%20Nicolas%20MUTAN%3A%20Multimodal%20Tucker%20Fusion%20for%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "5",
            "entry": "[5] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation. In 2014 Conference on Empirical Methods in Natural Language Processing, pages 1724\u20131734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20Merri%C3%ABnboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20Phrase%20Representations%20using%20RNN%20EncoderDecoder%20for%20Statistical%20Machine%20Translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20Merri%C3%ABnboer%2C%20Bart%20Van%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20Phrase%20Representations%20using%20RNN%20EncoderDecoder%20for%20Statistical%20Machine%20Translation%202014"
        },
        {
            "id": "6",
            "entry": "[6] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding. arXiv preprint arXiv:1606.01847, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01847"
        },
        {
            "id": "7",
            "entry": "[7] Ross Girshick. Fast r-cnn. In IEEE International Conference on Computer Vision, pages 1440\u20131448, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%20Girshick%20Fast%20rcnn%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20pages%2014401448%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%20Girshick%20Fast%20rcnn%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20pages%2014401448%202015"
        },
        {
            "id": "8",
            "entry": "[8] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Yash%20Khot%2C%20Tejas%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Making%20the%20V%20in%20VQA%20Matter%3A%20Elevating%20the%20Role%20of%20Image%20Understanding%20in%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Yash%20Khot%2C%20Tejas%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Making%20the%20V%20in%20VQA%20Matter%3A%20Elevating%20the%20Role%20of%20Image%20Understanding%20in%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "9",
            "entry": "[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202016"
        },
        {
            "id": "10",
            "entry": "[10] Ryota Hinami and Shin\u2019ichi Satoh. Query-Adaptive R-CNN for Open-Vocabulary Object Detection and Retrieval. arXiv preprint arXiv:1711.09509, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09509"
        },
        {
            "id": "11",
            "entry": "[11] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. Natural language object retrieval. In IEEE Computer Vision and Pattern Recognition, pages 4555\u20134564, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ronghang%20Hu%20Huazhe%20Xu%20Marcus%20Rohrbach%20Jiashi%20Feng%20Kate%20Saenko%20and%20Trevor%20Darrell%20Natural%20language%20object%20retrieval%20In%20IEEE%20Computer%20Vision%20and%20Pattern%20Recognition%20pages%2045554564%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ronghang%20Hu%20Huazhe%20Xu%20Marcus%20Rohrbach%20Jiashi%20Feng%20Kate%20Saenko%20and%20Trevor%20Darrell%20Natural%20language%20object%20retrieval%20In%20IEEE%20Computer%20Vision%20and%20Pattern%20Recognition%20pages%2045554564%202016"
        },
        {
            "id": "12",
            "entry": "[12] Ilija Ilievski and Jiashi Feng. A Simple Loss Function for Improving the Convergence and Accuracy of Visual Question Answering Models. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ilievski%2C%20Ilija%20Feng%2C%20Jiashi%20A%20Simple%20Loss%20Function%20for%20Improving%20the%20Convergence%20and%20Accuracy%20of%20Visual%20Question%20Answering%20Models%202017"
        },
        {
            "id": "13",
            "entry": "[13] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial Transformer Networks. In Advances in Neural Information Processing Systems 28, pages 2008\u20132016, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Kavukcuoglu%2C%20Koray%20Spatial%20Transformer%20Networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Kavukcuoglu%2C%20Koray%20Spatial%20Transformer%20Networks%202008"
        },
        {
            "id": "14",
            "entry": "[14] Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Multimodal Residual Learning for Visual QA. In Advances in Neural Information Processing Systems 29, pages 361\u2013369, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Jin-Hwa%20Lee%2C%20Sang-Woo%20Kwak%2C%20Donghyun%20Heo%2C%20Min-Oh%20Multimodal%20Residual%20Learning%20for%20Visual%20QA%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Jin-Hwa%20Lee%2C%20Sang-Woo%20Kwak%2C%20Donghyun%20Heo%2C%20Min-Oh%20Multimodal%20Residual%20Learning%20for%20Visual%20QA%202016"
        },
        {
            "id": "15",
            "entry": "[15] Jin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard Product for Low-rank Bilinear Pooling. In The 5th International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Jin-Hwa%20On%2C%20Kyoung%20Woon%20Lim%2C%20Woosang%20Kim%2C%20Jeonghee%20Hadamard%20Product%20for%20Low-rank%20Bilinear%20Pooling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Jin-Hwa%20On%2C%20Kyoung%20Woon%20Lim%2C%20Woosang%20Kim%2C%20Jeonghee%20Hadamard%20Product%20for%20Low-rank%20Bilinear%20Pooling%202017"
        },
        {
            "id": "16",
            "entry": "[16] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015"
        },
        {
            "id": "17",
            "entry": "[17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07332"
        },
        {
            "id": "18",
            "entry": "[18] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical Question-Image CoAttention for Visual Question Answering. arXiv preprint arXiv:1606.00061, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00061"
        },
        {
            "id": "19",
            "entry": "[19] Vinod Nair and Geoffrey E Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. 27th International Conference on Machine Learning, pages 807\u2013814, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20Linear%20Units%20Improve%20Restricted%20Boltzmann%20Machines%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Vinod%20Hinton%2C%20Geoffrey%20E.%20Rectified%20Linear%20Units%20Improve%20Restricted%20Boltzmann%20Machines%202010"
        },
        {
            "id": "20",
            "entry": "[20] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual Attention Networks for Multimodal Reasoning and Matching. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nam%2C%20Hyeonseob%20Ha%2C%20Jung-Woo%20Kim%2C%20Jeonghee%20Dual%20Attention%20Networks%20for%20Multimodal%20Reasoning%20and%20Matching%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nam%2C%20Hyeonseob%20Ha%2C%20Jung-Woo%20Kim%2C%20Jeonghee%20Dual%20Attention%20Networks%20for%20Multimodal%20Reasoning%20and%20Matching%202016"
        },
        {
            "id": "21",
            "entry": "[21] Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20GloVe%3A%20Global%20Vectors%20for%20Word%20Representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20D.%20GloVe%3A%20Global%20Vectors%20for%20Word%20Representation%202014"
        },
        {
            "id": "22",
            "entry": "[22] Hamed Pirsiavash, Deva Ramanan, and Charless C. Fowlkes. Bilinear classifiers for visual recognition. In Advances in Neural Information Processing Systems 22, pages 1482\u20131490, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pirsiavash%2C%20Hamed%20Ramanan%2C%20Deva%20Fowlkes%2C%20Charless%20C.%20Bilinear%20classifiers%20for%20visual%20recognition%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pirsiavash%2C%20Hamed%20Ramanan%2C%20Deva%20Fowlkes%2C%20Charless%20C.%20Bilinear%20classifiers%20for%20visual%20recognition%202009"
        },
        {
            "id": "23",
            "entry": "[23] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. International Journal of Computer Vision, 123:74\u201393, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Plummer%2C%20Bryan%20A.%20Wang%2C%20Liwei%20Cervantes%2C%20Chris%20M.%20Caicedo%2C%20Juan%20C.%20Flickr30k%20Entities%3A%20Collecting%20Region-to-Phrase%20Correspondences%20for%20Richer%20Image-to-Sentence%20Models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Plummer%2C%20Bryan%20A.%20Wang%2C%20Liwei%20Cervantes%2C%20Chris%20M.%20Caicedo%2C%20Juan%20C.%20Flickr30k%20Entities%3A%20Collecting%20Region-to-Phrase%20Correspondences%20for%20Richer%20Image-to-Sentence%20Models%202017"
        },
        {
            "id": "24",
            "entry": "[24] Joseph Redmon and Ali Farhadi. YOLO9000: Better, Faster, Stronger. In IEEE Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%20Redmon%20and%20Ali%20Farhadi%20YOLO9000%20Better%20Faster%20Stronger%20In%20IEEE%20Computer%20Vision%20and%20Pattern%20Recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joseph%20Redmon%20and%20Ali%20Farhadi%20YOLO9000%20Better%20Faster%20Stronger%20In%20IEEE%20Computer%20Vision%20and%20Pattern%20Recognition%202017"
        },
        {
            "id": "25",
            "entry": "[25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20Real-Time%20Object%20Detection%20with%20Region%20Proposal%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20Real-Time%20Object%20Detection%20with%20Region%20Proposal%20Networks%202017"
        },
        {
            "id": "26",
            "entry": "[26] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases in images by reconstruction. In European Conference on Computer Vision, pages 817\u2013834, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rohrbach%2C%20Anna%20Rohrbach%2C%20Marcus%20Hu%2C%20Ronghang%20Darrell%2C%20Trevor%20Grounding%20of%20textual%20phrases%20in%20images%20by%20reconstruction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rohrbach%2C%20Anna%20Rohrbach%2C%20Marcus%20Hu%2C%20Ronghang%20Darrell%2C%20Trevor%20Grounding%20of%20textual%20phrases%20in%20images%20by%20reconstruction%202016"
        },
        {
            "id": "27",
            "entry": "[27] Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. arXiv preprint arXiv:1602.07868, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07868"
        },
        {
            "id": "28",
            "entry": "[28] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout : A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20and%20Ruslan%20Salakhutdinov.%20Dropout%20%3A%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from%20Overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20and%20Ruslan%20Salakhutdinov.%20Dropout%20%3A%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from%20Overfitting%202014"
        },
        {
            "id": "29",
            "entry": "[29] Damien Teney, Peter Anderson, Xiaodong He, and Anton van den Hengel. Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge. arXiv preprint arXiv:1708.02711, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02711"
        },
        {
            "id": "30",
            "entry": "[30] Alexander Trott, Caiming Xiong, and Richard Socher. Interpretable Counting for Visual Question Answering. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trott%2C%20Alexander%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Interpretable%20Counting%20for%20Visual%20Question%20Answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trott%2C%20Alexander%20Xiong%2C%20Caiming%20Socher%2C%20Richard%20Interpretable%20Counting%20for%20Visual%20Question%20Answering%202018"
        },
        {
            "id": "31",
            "entry": "[31] Andreas Veit, Michael J Wilber, and Serge Belongie. Residual Networks are Exponential Ensembles of Relatively Shallow Networks. In Advances in Neural Information Processing Systems 29, pages 550\u2013558, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veit%2C%20Andreas%20Wilber%2C%20Michael%20J.%20Belongie%2C%20Serge%20Residual%20Networks%20are%20Exponential%20Ensembles%20of%20Relatively%20Shallow%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veit%2C%20Andreas%20Wilber%2C%20Michael%20J.%20Belongie%2C%20Serge%20Residual%20Networks%20are%20Exponential%20Ensembles%20of%20Relatively%20Shallow%20Networks%202016"
        },
        {
            "id": "32",
            "entry": "[32] Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning Deep Structure-Preserving ImageText Embeddings. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5005\u20135013, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Liwei%20Li%2C%20Yin%20Lazebnik%2C%20Svetlana%20Learning%20Deep%20Structure-Preserving%20ImageText%20Embeddings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Liwei%20Li%2C%20Yin%20Lazebnik%2C%20Svetlana%20Learning%20Deep%20Structure-Preserving%20ImageText%20Embeddings%202016"
        },
        {
            "id": "33",
            "entry": "[33] Mingzhe Wang, Mahmoud Azab, Noriyuki Kojima, Rada Mihalcea, and Jia Deng. Structured Matching for Phrase Localization. In European Conference on Computer Vision, volume 9908, pages 696\u2013711, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Mingzhe%20Azab%2C%20Mahmoud%20Kojima%2C%20Noriyuki%20Mihalcea%2C%20Rada%20Structured%20Matching%20for%20Phrase%20Localization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Mingzhe%20Azab%2C%20Mahmoud%20Kojima%2C%20Noriyuki%20Mihalcea%2C%20Rada%20Structured%20Matching%20for%20Phrase%20Localization%202016"
        },
        {
            "id": "34",
            "entry": "[34] Peng Wang, Qi Wu, Chunhua Shen, and Anton van den Hengel. The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions. In Computer Vision and Pattern Recognition (CVPR), pages 1173\u20131182, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Peng%20Wu%2C%20Qi%20Shen%2C%20Chunhua%20van%20den%20Hengel%2C%20Anton%20The%20VQA-Machine%3A%20Learning%20How%20to%20Use%20Existing%20Vision%20Algorithms%20to%20Answer%20New%20Questions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Peng%20Wu%2C%20Qi%20Shen%2C%20Chunhua%20van%20den%20Hengel%2C%20Anton%20The%20VQA-Machine%3A%20Learning%20How%20to%20Use%20Existing%20Vision%20Algorithms%20to%20Answer%20New%20Questions%202017"
        },
        {
            "id": "35",
            "entry": "[35] Lior Wolf, Hueihan Jhuang, and Tamir Hazan. Modeling appearances with low-rank SVM. IEEE Conference on Computer Vision and Pattern Recognition, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolf%2C%20Lior%20Jhuang%2C%20Hueihan%20Hazan%2C%20Tamir%20Modeling%20appearances%20with%20low-rank%20SVM%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolf%2C%20Lior%20Jhuang%2C%20Hueihan%20Hazan%2C%20Tamir%20Modeling%20appearances%20with%20low-rank%20SVM%202007"
        },
        {
            "id": "36",
            "entry": "[36] Huijuan Xu and Kate Saenko. Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering. In European Conference on Computer Vision, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huijuan%20Ask%2C%20Kate%20Saenko%20Attend%20and%20Answer%3A%20Exploring%20Question-Guided%20Spatial%20Attention%20for%20Visual%20Question%20Answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huijuan%20Ask%2C%20Kate%20Saenko%20Attend%20and%20Answer%3A%20Exploring%20Question-Guided%20Spatial%20Attention%20for%20Visual%20Question%20Answering%202016"
        },
        {
            "id": "37",
            "entry": "[37] Raymond A Yeh, Jinjun Xiong, Wen-Mei W Hwu, Minh N Do, and Alexander G Schwing. Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts. In Advances in Neural Information Processing Systems 30, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20Raymond%20A.%20Xiong%2C%20Jinjun%20Hwu%2C%20Wen-Mei%20W.%20Do%2C%20Minh%20N.%20Interpretable%20and%20Globally%20Optimal%20Prediction%20for%20Textual%20Grounding%20using%20Image%20Concepts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yeh%2C%20Raymond%20A.%20Xiong%2C%20Jinjun%20Hwu%2C%20Wen-Mei%20W.%20Do%2C%20Minh%20N.%20Interpretable%20and%20Globally%20Optimal%20Prediction%20for%20Textual%20Grounding%20using%20Image%20Concepts%202017"
        },
        {
            "id": "38",
            "entry": "[38] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Young%2C%20Peter%20Lai%2C%20Alice%20Hodosh%2C%20Micah%20Hockenmaier%2C%20Julia%20From%20image%20descriptions%20to%20visual%20denotations%3A%20New%20similarity%20metrics%20for%20semantic%20inference%20over%20event%20descriptions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Young%2C%20Peter%20Lai%2C%20Alice%20Hodosh%2C%20Micah%20Hockenmaier%2C%20Julia%20From%20image%20descriptions%20to%20visual%20denotations%3A%20New%20similarity%20metrics%20for%20semantic%20inference%20over%20event%20descriptions%202014"
        },
        {
            "id": "39",
            "entry": "[39] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. Beyond Bilinear: Generalized Multi-modal Factorized High-order Pooling for Visual Question Answering. IEEE Transactions on Neural Networks and Learning Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Zhou%20Yu%2C%20Jun%20Xiang%2C%20Chenchao%20Fan%2C%20Jianping%20Beyond%20Bilinear%3A%20Generalized%20Multi-modal%20Factorized%20High-order%20Pooling%20for%20Visual%20Question%20Answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Zhou%20Yu%2C%20Jun%20Xiang%2C%20Chenchao%20Fan%2C%20Jianping%20Beyond%20Bilinear%3A%20Generalized%20Multi-modal%20Factorized%20High-order%20Pooling%20for%20Visual%20Question%20Answering%202018"
        },
        {
            "id": "40",
            "entry": "[40] Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-Down Neural Attention by Excitation Backprop. In European Conference on Computer Vision, volume 9908, pages 543\u2013559, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Jianming%20Lin%2C%20Zhe%20Brandt%2C%20Jonathan%20Shen%2C%20Xiaohui%20Top-Down%20Neural%20Attention%20by%20Excitation%20Backprop%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Jianming%20Lin%2C%20Zhe%20Brandt%2C%20Jonathan%20Shen%2C%20Xiaohui%20Top-Down%20Neural%20Attention%20by%20Excitation%20Backprop%202016"
        },
        {
            "id": "41",
            "entry": "[41] Yan Zhang, Jonathon Hare, and Adam Pr\u00fcgel-Bennett. Learning to Count Objects in Natural Images for Visual Question Answering. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yan%20Hare%2C%20Jonathon%20Pr%C3%BCgel-Bennett%2C%20Adam%20Learning%20to%20Count%20Objects%20in%20Natural%20Images%20for%20Visual%20Question%20Answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yan%20Hare%2C%20Jonathon%20Pr%C3%BCgel-Bennett%2C%20Adam%20Learning%20to%20Count%20Objects%20in%20Natural%20Images%20for%20Visual%20Question%20Answering%202018"
        },
        {
            "id": "42",
            "entry": "[42] C Lawrence Zitnick and Piotr Doll\u00e1r. Edge boxes: Locating object proposals from edges. In European Conference on Computer Vision, pages 391\u2013405, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zitnick%2C%20C.Lawrence%20Doll%C3%A1r%2C%20Piotr%20Edge%20boxes%3A%20Locating%20object%20proposals%20from%20edges%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zitnick%2C%20C.Lawrence%20Doll%C3%A1r%2C%20Piotr%20Edge%20boxes%3A%20Locating%20object%20proposals%20from%20edges%202014"
        }
    ]
}
