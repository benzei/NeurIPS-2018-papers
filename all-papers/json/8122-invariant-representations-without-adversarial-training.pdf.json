{
    "filename": "8122-invariant-representations-without-adversarial-training.pdf",
    "metadata": {
        "title": "Invariant Representations without Adversarial Training",
        "author": "Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, Greg Ver Steeg",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8122-invariant-representations-without-adversarial-training.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations."
    },
    "keywords": [
        {
            "term": "fair representation",
            "url": "https://en.wikipedia.org/wiki/fair_representation"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        }
    ],
    "highlights": [
        "The removal of unwanted information is a surprisingly common task",
        "Encodings satisfying this condition are invariant under changes in c, called \u201cinvariant representations\u201d",
        "We compare against the Variational Fair Autoencoder (VFAE) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and the adversarial method proposed in Xie et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "It introduces an extra layer of complexity (a second optimization problem) into our system. In this particular case of invariant representation, our results lead us to believe that adversarial training is unnecessary. This does not mean that adversarial training for invariant representations is strictly worse in practice",
        "We have derived a variational upper bound for the mutual information between latent representations and covariate factors",
        "Provided a dataset with labeled covariates, we can train both supervised and unsupervised learning methods that are invariant to these factors without the use of adversarial training"
    ],
    "key_statements": [
        "The removal of unwanted information is a surprisingly common task",
        "Encodings satisfying this condition are invariant under changes in c, called \u201cinvariant representations\u201d",
        "We compare against the Variational Fair Autoencoder (VFAE) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and the adversarial method proposed in Xie et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "It introduces an extra layer of complexity (a second optimization problem) into our system. In this particular case of invariant representation, our results lead us to believe that adversarial training is unnecessary. This does not mean that adversarial training for invariant representations is strictly worse in practice",
        "We have derived a variational upper bound for the mutual information between latent representations and covariate factors",
        "Provided a dataset with labeled covariates, we can train both supervised and unsupervised learning methods that are invariant to these factors without the use of adversarial training"
    ],
    "summary": [
        "The removal of unwanted information is a surprisingly common task.",
        "In practice these constraints are often relaxed to other measures; in recent works an adversary\u2019s ability to predict z from c has been used as such a proxy [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], transplanting adversarial losses from generative literature to encoder/decoder settings.",
        "An empirical solution was proposed by Lample et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], who removed specific visual features from a latent representation through adversarial training.",
        "Both methods generate representations that make it difficult for an adversary to recover the protected class but are still useful for a classification task.",
        "Consider a general task that includes an encoding of observed data x into latent variables z through the conditional likelihood q(z|x).",
        "Putting this together with the penalty term Eq 7, we have the following variational bound on the combined objective: E(x,c)[log P (x|c)] \u2212 \u03bbI(z, c) \u2265 E(x,c)[ \u2212KL[ q(z|x) p(z)] \u2212 \u03bbKL[ q(z|x) q(z) ] + (1 + \u03bb)Ez\u223cq[log p(x|z, c)] ].",
        "For each fair classification dataset/task we evaluated both prediction accuracy and adversarial error in predicting c from the latent code.",
        "We compare against the Variational Fair Autoencoder (VFAE) [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and the adversarial method proposed in Xie et al [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "All encoders and decoders are single layer, as specified by Louizos et al [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and for both datasets we use 64 hidden units in our method as in Xie et al, while for VFAE we use their described architecture.",
        "These discriminators are trained independently from the encoder/decoder/within-method adversaries.",
        "As show analytically in Section 2.1.2, in the optimal case adversarial training can perform as well as our derived method; it is intuitively simple and allows for more nuanced tuning.",
        "Bad approximations may provide bad gradient information to the system, leading to poor performance of the encoder against a post-hoc adversary.",
        "Our experimental results do not match those reported in Xie et al While in general their method has comparable performance for predictive accuracy, we do not find that their adversarial error is low; instead, we find that the encoder/adversary pair becomes stuck in local minima.",
        "We have derived a variational upper bound for the mutual information between latent representations and covariate factors.",
        "Provided a dataset with labeled covariates, we can train both supervised and unsupervised learning methods that are invariant to these factors without the use of adversarial training.",
        "Information-theoretic optimization approach avoids the pitfalls inherent in adversarial learning for invariant representation and produces results that match or exceed capabilities of these state-of-the-art methods."
    ],
    "headline": "We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Achille and S. Soatto. On the emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:1706.01350, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01350"
        },
        {
            "id": "2",
            "entry": "[2] A. Achille and S. Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achille%2C%20A.%20Soatto%2C%20S.%20Information%20dropout%3A%20Learning%20optimal%20representations%20through%20noisy%20computation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achille%2C%20A.%20Soatto%2C%20S.%20Information%20dropout%3A%20Learning%20optimal%20representations%20through%20noisy%20computation%202018"
        },
        {
            "id": "3",
            "entry": "[3] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00410"
        },
        {
            "id": "4",
            "entry": "[4] T. Cohen and M. Welling. Group equivariant convolutional networks. In International Conference on Machine Learning, pages 2990\u20132999, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20T.%20Welling%2C%20M.%20Group%20equivariant%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20T.%20Welling%2C%20M.%20Group%20equivariant%20convolutional%20networks%202016"
        },
        {
            "id": "5",
            "entry": "[5] R. A. Feis et al. Ica-based artifact removal diminishes scan site differences in multi-center resting-state fMRI. Frontiers in Neuroscience, 9:395, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feis%2C%20R.A.%20Ica-based%20artifact%20removal%20diminishes%20scan%20site%20differences%20in%20multi-center%20resting-state%20fMRI%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feis%2C%20R.A.%20Ica-based%20artifact%20removal%20diminishes%20scan%20site%20differences%20in%20multi-center%20resting-state%20fMRI%202015"
        },
        {
            "id": "6",
            "entry": "[6] J.-P. Fortin et al. Harmonization of multi-site diffusion tensor imaging data. Neuroimage, 161:149\u2013170, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fortin%2C%20J.-P.%20Harmonization%20of%20multi-site%20diffusion%20tensor%20imaging%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fortin%2C%20J.-P.%20Harmonization%20of%20multi-site%20diffusion%20tensor%20imaging%20data%202017"
        },
        {
            "id": "7",
            "entry": "[7] R. P. Freckleton. On the misuse of residuals in ecology: regression of residuals vs. multiple regression. Journal of Animal Ecology, 71(3):542\u2013545, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freckleton%2C%20R.P.%20On%20the%20misuse%20of%20residuals%20in%20ecology%3A%20regression%20of%20residuals%20vs.%20multiple%20regression%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freckleton%2C%20R.P.%20On%20the%20misuse%20of%20residuals%20in%20ecology%3A%20regression%20of%20residuals%20vs.%20multiple%20regression%202002"
        },
        {
            "id": "8",
            "entry": "[8] W. T. Freeman, E. H. Adelson, et al. The design and use of steerable filters. IEEE Transactions on Pattern analysis and machine intelligence, 13(9):891\u2013906, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freeman%2C%20W.T.%20Adelson%2C%20E.H.%20The%20design%20and%20use%20of%20steerable%20filters%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freeman%2C%20W.T.%20Adelson%2C%20E.H.%20The%20design%20and%20use%20of%20steerable%20filters%201991"
        },
        {
            "id": "9",
            "entry": "[9] R. J. Gallagher, K. Reing, D. Kale, and G. V. Steeg. Anchored correlation explanation: Topic modeling with minimal domain knowledge. arXiv preprint arXiv:1611.10277, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.10277"
        },
        {
            "id": "10",
            "entry": "[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "11",
            "entry": "[11] H. Greenspan, S. Belongie, R. Goodman, P. Perona, S. Rakshit, and C. H. Anderson. Overcomplete steerable pyramid filters and rotation invariance. 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greenspan%2C%20H.%20Belongie%2C%20S.%20Goodman%2C%20R.%20Perona%2C%20P.%20Overcomplete%20steerable%20pyramid%20filters%20and%20rotation%20invariance%201994"
        },
        {
            "id": "12",
            "entry": "[12] F. Kamiran and T. Calders. Classifying without discriminating. In 2nd International Conference on Computer, Control and Communication, pages 1\u20136. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20F.%20Calders%2C%20T.%20Classifying%20without%20discriminating%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20F.%20Calders%2C%20T.%20Classifying%20without%20discriminating%202009"
        },
        {
            "id": "13",
            "entry": "[13] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "14",
            "entry": "[14] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer, et al. Fader networks: Manipulating images by sliding attributes. In Advances in Neural Information Processing Systems, pages 5969\u20135978, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20G.%20Zeghidour%2C%20N.%20Usunier%2C%20N.%20Bordes%2C%20A.%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lample%2C%20G.%20Zeghidour%2C%20N.%20Usunier%2C%20N.%20Bordes%2C%20A.%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%202017"
        },
        {
            "id": "15",
            "entry": "[15] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair autoencoder. arXiv preprint arXiv:1511.00830, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.00830"
        },
        {
            "id": "17",
            "entry": "[17] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=v.%20d.%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "18",
            "entry": "[18] S. W. Raudenbush. Random effects models. The handbook of research synthesis, 421, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raudenbush%2C%20S.W.%20Random%20effects%20models.%20The%20handbook%20of%20research%20synthesis%201994"
        },
        {
            "id": "19",
            "entry": "[19] S. Soatto and A. Chiuso. Visual representations: Defining properties and deep approximations. arXiv preprint arXiv:1411.7676, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.7676"
        },
        {
            "id": "20",
            "entry": "[20] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20N.%20Pereira%2C%20F.C.%20Bialek%2C%20W.%20The%20information%20bottleneck%20method%202000"
        },
        {
            "id": "21",
            "entry": "[21] Q. Xie, Z. Dai, Y. Du, E. Hovy, and G. Neubig. Controllable invariance through adversarial feature learning. In Advances in Neural Information Processing Systems, pages 585\u2013596, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Q.%20Dai%2C%20Z.%20Du%2C%20Y.%20Hovy%2C%20E.%20Controllable%20invariance%20through%20adversarial%20feature%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Q.%20Dai%2C%20Z.%20Du%2C%20Y.%20Hovy%2C%20E.%20Controllable%20invariance%20through%20adversarial%20feature%20learning%202017"
        },
        {
            "id": "22",
            "entry": "[22] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325\u2013333, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zemel%2C%20R.%20Wu%2C%20Y.%20Swersky%2C%20K.%20Pitassi%2C%20T.%20Dwork.%20Learning%20fair%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20R.%20Wu%2C%20Y.%20Swersky%2C%20K.%20Pitassi%2C%20T.%20Dwork.%20Learning%20fair%20representations%202013"
        }
    ]
}
