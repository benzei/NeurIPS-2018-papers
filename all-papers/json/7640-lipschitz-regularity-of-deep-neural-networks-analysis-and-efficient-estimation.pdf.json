{
    "filename": "7640-lipschitz-regularity-of-deep-neural-networks-analysis-and-efficient-estimation.pdf",
    "metadata": {
        "title": "Lipschitz regularity of deep neural networks: analysis and efficient estimation",
        "author": "Aladin Virmaux, Kevin Scaman",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7640-lipschitz-regularity-of-deep-neural-networks-analysis-and-efficient-estimation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-of-art methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations."
    },
    "keywords": [
        {
            "term": "upper bound",
            "url": "https://en.wikipedia.org/wiki/upper_bound"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "np hard",
            "url": "https://en.wikipedia.org/wiki/np_hard"
        },
        {
            "term": "automatic differentiation",
            "url": "https://en.wikipedia.org/wiki/automatic_differentiation"
        },
        {
            "term": "lipschitz constant",
            "url": "https://en.wikipedia.org/wiki/lipschitz_constant"
        },
        {
            "term": "differentiable function",
            "url": "https://en.wikipedia.org/wiki/differentiable_function"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "differential operator",
            "url": "https://en.wikipedia.org/wiki/differential_operator"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "We denote as x, y and x 2 the scalar product and L2-norm of the Hilbert space Rn, x \u00b7 y the coordinate-wise product of x and y, and f \u25e6 g the composition between the functions f : Rk \u2192 Rm and g : Rn \u2192 Rk",
        "In Section 6, we provide a theoretical analysis of AutoLip in the case of sequential neural networks, and show that the upper bound may lose a multiplicative factor per activation layer, which may significantly downgrade the estimation quality of AutoLip and lead to a very large and unrealistic upper bound",
        "We studied the Lispchitz regularity of neural networks",
        "We provided a generic upper bound called AutoLip for the Lipschitz constant of any automatically differentiable function",
        "We proposed a refinement of the previous method for MLPs called SeqLip and showed how this algorithm can improve on AutoLip theoretically and in applications, sometimes improving up to a factor of 8 the AutoLip upper bound",
        "While the AutoLip and SeqLip upper bounds remain extremely large for neural networks of the computer vision literature (e.g. AlexNet, see Section 7), it is yet an open question to know if these values are close to the true Lipschitz constant or substantially overestimating it"
    ],
    "key_statements": [
        "We denote as x, y and x 2 the scalar product and L2-norm of the Hilbert space Rn, x \u00b7 y the coordinate-wise product of x and y, and f \u25e6 g the composition between the functions f : Rk \u2192 Rm and g : Rn \u2192 Rk",
        "In Section 6, we provide a theoretical analysis of AutoLip in the case of sequential neural networks, and show that the upper bound may lose a multiplicative factor per activation layer, which may significantly downgrade the estimation quality of AutoLip and lead to a very large and unrealistic upper bound",
        "We studied the Lispchitz regularity of neural networks",
        "We provided a generic upper bound called AutoLip for the Lipschitz constant of any automatically differentiable function",
        "We proposed a refinement of the previous method for MLPs called SeqLip and showed how this algorithm can improve on AutoLip theoretically and in applications, sometimes improving up to a factor of 8 the AutoLip upper bound",
        "While the AutoLip and SeqLip upper bounds remain extremely large for neural networks of the computer vision literature (e.g. AlexNet, see Section 7), it is yet an open question to know if these values are close to the true Lipschitz constant or substantially overestimating it"
    ],
    "summary": [
        "We denote as x, y and x 2 the scalar product and L2-norm of the Hilbert space Rn, x \u00b7 y the coordinate-wise product of x and y, and f \u25e6 g the composition between the functions f : Rk \u2192 Rm and g : Rn \u2192 Rk.",
        "To the best of our knowledge, the first upper-bound on the Lipschitz constant of a neural network was described in [9, Section 4.3], as the product of the spectral norms of linear layers (a special case of our generic algorithm, see Proposition 1).",
        "In Section 4, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function.",
        "We show how the Lipschitz constant of most neural network layers may be computed efficiently using automatic differentiation algorithms [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and libraries such as PyTorch [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "The Lipschitz constant of an affine function f : x \u2192 M x + b where M \u2208 Rm\u00d7n and b \u2208 Rm is the largest singular value of its associated matrix M , which may be computed efficiently, up to a given precision, using the power method [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "We improve on the AutoLip upper bound by a more refined analysis of deep learning architectures in the case of MLPs. More specifically, the Lipschitz constant of MLPs have an explicit formula using Theorem 1 and the chain rule: L = sup MK diag)Mk\u22121...M2 diag(g1(\u03b81))M1 2, (6)",
        "Differential operators of activation layers, being diagonal matrices, can only have a limited effect on input vectors, and in practice, first singular vectors will tend to misalign, leading to a drop in the Lipschitz constant of the MLP.",
        "When the ratios between the second and first singular values are sufficiently small, each activation layer decreases the Lipschitz constant by a factor 1/\u03c0 and",
        "As for the experiment on a CNN, we use the 200 highest singular values of each linear layer for Greedy SeqLip. We obtain 5.45 \u00d7 106 as an upper bound approximation, which remains large despite its 6 fold improvement over AutoLip. Note that we do not get the same results as [9, Section 4.3] as we did not use the same weights.",
        "We provided a generic upper bound called AutoLip for the Lipschitz constant of any automatically differentiable function.",
        "While the AutoLip and SeqLip upper bounds remain extremely large for neural networks of the computer vision literature (e.g. AlexNet, see Section 7), it is yet an open question to know if these values are close to the true Lipschitz constant or substantially overestimating it."
    ],
    "headline": "We investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "2",
            "entry": "[2] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Vanhoucke%2C%20Vincent%20Ioffe%2C%20Sergey%20Shlens%2C%20Jon%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "3",
            "entry": "[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "4",
            "entry": "[4] G. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261\u20132269, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20v.%20d.%20Maaten%2C%20L.%20Weinberger%2C%20K.Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20v.%20d.%20Maaten%2C%20L.%20Weinberger%2C%20K.Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In International Conference on Machine Learning, pages 1764\u20131772, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Jaitly%2C%20Navdeep%20Towards%20end-to-end%20speech%20recognition%20with%20recurrent%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Jaitly%2C%20Navdeep%20Towards%20end-to-end%20speech%20recognition%20with%20recurrent%20neural%20networks%202014"
        },
        {
            "id": "6",
            "entry": "[6] A\u00e4ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In SSW, page 125. ISCA, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%2C%20A%C3%A4ron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%2C%20A%C3%A4ron%20Dieleman%2C%20Sander%20Zen%2C%20Heiga%20Simonyan%2C%20Karen%20Wavenet%3A%20A%20generative%20model%20for%20raw%20audio%202016"
        },
        {
            "id": "7",
            "entry": "[7] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746\u2013751, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Yih%2C%20Wen-tau%20Zweig%2C%20Geoffrey%20Linguistic%20regularities%20in%20continuous%20space%20word%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Yih%2C%20Wen-tau%20Zweig%2C%20Geoffrey%20Linguistic%20regularities%20in%20continuous%20space%20word%20representations%202013"
        },
        {
            "id": "8",
            "entry": "[8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0142ukasz Kaiser, and Illia Polosukhin. Attention is All You Need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaswani%2C%20Ashish%20Shazeer%2C%20Noam%20Parmar%2C%20Niki%20Uszkoreit%2C%20Jakob%20Attention%20is%20All%20You%20Need%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaswani%2C%20Ashish%20Shazeer%2C%20Noam%20Parmar%2C%20Niki%20Uszkoreit%2C%20Jakob%20Attention%20is%20All%20You%20Need%202017"
        },
        {
            "id": "9",
            "entry": "[9] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "10",
            "entry": "[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "11",
            "entry": "[11] Mart\u00edn Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, ICML, pages 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mart%C3%ADn%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mart%C3%ADn%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "12",
            "entry": "[12] C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C%C3%A9dric%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338%202008"
        },
        {
            "id": "13",
            "entry": "[13] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "14",
            "entry": "[14] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20Wasserstein%20GANs%202017"
        },
        {
            "id": "15",
            "entry": "[15] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Evaluating%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20Tsui-Wei%20Zhang%2C%20Huan%20Chen%2C%20Pin-Yu%20Yi%2C%20Jinfeng%20Evaluating%202018"
        },
        {
            "id": "16",
            "entry": "[16] Ulrike von Luxburg and Olivier Bousquet. Distance\u2013based classification with lipschitz functions. J. Mach. Learn. Res., 5:669\u2013695, December 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=von%20Luxburg%2C%20Ulrike%20Bousquet%2C%20Olivier%20Distance%E2%80%93based%20classification%20with%20lipschitz%20functions%202004-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=von%20Luxburg%2C%20Ulrike%20Bousquet%2C%20Olivier%20Distance%E2%80%93based%20classification%20with%20lipschitz%20functions%202004-12"
        },
        {
            "id": "17",
            "entry": "[17] Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017-12"
        },
        {
            "id": "18",
            "entry": "[18] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5949\u20135958, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017-12"
        },
        {
            "id": "19",
            "entry": "[19] R. Balan, M. K. Singh, and D. Zou. Lipschitz properties for deep convolutional networks. to appear in Contemporary Mathematics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balan%2C%20R.%20Singh%2C%20M.K.%20Zou%2C%20D.%20Lipschitz%20properties%20for%20deep%20convolutional%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balan%2C%20R.%20Singh%2C%20M.K.%20Zou%2C%20D.%20Lipschitz%20properties%20for%20deep%20convolutional%20networks%202018"
        },
        {
            "id": "20",
            "entry": "[20] Louis B. Rall. Automatic Differentiation: Techniques and Applications, volume 120 of Lecture Notes in Computer Science. Springer, Berlin, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rall%2C%20Louis%20B.%20Automatic%20Differentiation%3A%20Techniques%20and%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rall%2C%20Louis%20B.%20Automatic%20Differentiation%3A%20Techniques%20and%201981"
        },
        {
            "id": "21",
            "entry": "[21] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20PyTorch%202017"
        },
        {
            "id": "22",
            "entry": "[22] Herbert Federer. Geometric measure theory. Classics in Mathematics. Springer-Verlag Berlin Heidelberg, 1969.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Federer%2C%20Herbert%20Geometric%20measure%20theory.%20Classics%20in%20Mathematics%201969"
        },
        {
            "id": "23",
            "entry": "[23] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015. Software available from tensorflow.org.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mart%C3%ADn%20Abadi%20Ashish%20Agarwal%20Paul%20Barham%20Eugene%20Brevdo%20Zhifeng%20Chen%20Craig%20Citro%20Greg%20S%20Corrado%20Andy%20Davis%20Jeffrey%20Dean%20Matthieu%20Devin%20Sanjay%20Ghemawat%20Ian%20Goodfellow%20Andrew%20Harp%20Geoffrey%20Irving%20Michael%20Isard%20Yangqing%20Jia%20Rafal%20Jozefowicz%20Lukasz%20Kaiser%20Manjunath%20Kudlur%20Josh%20Levenberg%20Dandelion%20Man%C3%A9%20Rajat%20Monga%20Sherry%20Moore%20Derek%20Murray%20Chris%20Olah%20Mike%20Schuster%20Jonathon%20Shlens%20Benoit%20Steiner%20Ilya%20Sutskever%20Kunal%20Talwar%20Paul%20Tucker%20Vincent%20Vanhoucke%20Vijay%20Vasudevan%20Fernanda%20Vi%C3%A9gas%20Oriol%20Vinyals%20Pete%20Warden%20Martin%20Wattenberg%20Martin%20Wicke%20Yuan%20Yu%20and%20Xiaoqiang%20Zheng%20TensorFlow%20LargeScale%20Machine%20Learning%20on%20Heterogeneous%20Systems%202015%20Software%20available%20from%20tensorfloworg",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mart%C3%ADn%20Abadi%20Ashish%20Agarwal%20Paul%20Barham%20Eugene%20Brevdo%20Zhifeng%20Chen%20Craig%20Citro%20Greg%20S%20Corrado%20Andy%20Davis%20Jeffrey%20Dean%20Matthieu%20Devin%20Sanjay%20Ghemawat%20Ian%20Goodfellow%20Andrew%20Harp%20Geoffrey%20Irving%20Michael%20Isard%20Yangqing%20Jia%20Rafal%20Jozefowicz%20Lukasz%20Kaiser%20Manjunath%20Kudlur%20Josh%20Levenberg%20Dandelion%20Man%C3%A9%20Rajat%20Monga%20Sherry%20Moore%20Derek%20Murray%20Chris%20Olah%20Mike%20Schuster%20Jonathon%20Shlens%20Benoit%20Steiner%20Ilya%20Sutskever%20Kunal%20Talwar%20Paul%20Tucker%20Vincent%20Vanhoucke%20Vijay%20Vasudevan%20Fernanda%20Vi%C3%A9gas%20Oriol%20Vinyals%20Pete%20Warden%20Martin%20Wattenberg%20Martin%20Wicke%20Yuan%20Yu%20and%20Xiaoqiang%20Zheng%20TensorFlow%20LargeScale%20Machine%20Learning%20on%20Heterogeneous%20Systems%202015%20Software%20available%20from%20tensorfloworg"
        },
        {
            "id": "24",
            "entry": "[24] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic differentiation, volume 105.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20Walther%2C%20Andrea%20Evaluating%20derivatives%3A%20principles%20and%20techniques%20of%20algorithmic%20differentiation"
        },
        {
            "id": "25",
            "entry": "[25] Seppo Linnainmaa. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master\u2019s Thesis (in Finnish), Univ. Helsinki, pages 6\u20137, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linnainmaa%2C%20Seppo%20The%20representation%20of%20the%20cumulative%20rounding%20error%20of%20an%20algorithm%20as%20a%20Taylor%20expansion%20of%20the%20local%20rounding%20errors%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linnainmaa%2C%20Seppo%20The%20representation%20of%20the%20cumulative%20rounding%20error%20of%20an%20algorithm%20as%20a%20Taylor%20expansion%20of%20the%20local%20rounding%20errors%201970"
        },
        {
            "id": "26",
            "entry": "[26] RV Mises and Hilda Pollaczek-Geiringer. Praktische verfahren der gleichungsaufl\u00f6sung. ZAMMJournal of Applied Mathematics and Mechanics/Zeitschrift f\u00fcr Angewandte Mathematik und Mechanik, 9(1):58\u201377, 1929.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mises%2C%20R.V.%20Pollaczek-Geiringer%2C%20Hilda%20Praktische%20verfahren%20der%20gleichungsaufl%C3%B6sung.%20ZAMMJournal%20of%201929",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mises%2C%20R.V.%20Pollaczek-Geiringer%2C%20Hilda%20Praktische%20verfahren%20der%20gleichungsaufl%C3%B6sung.%20ZAMMJournal%20of%201929"
        },
        {
            "id": "27",
            "entry": "[27] Jan R. Magnus. On Differentiating Eigenvalues and Eigenvectors. Econometric Theory, 1(2):pp. 179\u2013191, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magnus%2C%20Jan%20R.%20On%20Differentiating%20Eigenvalues%20and%20Eigenvectors%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Magnus%2C%20Jan%20R.%20On%20Differentiating%20Eigenvalues%20and%20Eigenvectors%201985"
        },
        {
            "id": "28",
            "entry": "[28] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Deep%20Learning%202016"
        },
        {
            "id": "29",
            "entry": "[29] Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/. ",
            "url": "http://yann.lecun.com/exdb/mnist/"
        }
    ]
}
