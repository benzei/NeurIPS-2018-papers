{
    "filename": "7944-deep-homogeneous-mixture-models-representation-separation-and-approximation.pdf",
    "metadata": {
        "title": "Deep Homogeneous Mixture Models: Representation, Separation, and Approximation",
        "author": "Priyank Jaini, Pascal Poupart, Yaoliang Yu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7944-deep-homogeneous-mixture-models-representation-separation-and-approximation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "At their core, many unsupervised learning models provide a compact representation of homogeneous density mixtures, but their similarities and differences are not always clearly understood. In this work, we formally establish the relationships among latent tree graphical models (including special cases such as hidden Markov models and tensorial mixture models), hierarchical tensor formats and sum-product networks. Based on this connection, we then give a unified treatment of exponential separation in exact representation size between deep mixture architectures and shallow ones. In contrast, for approximate representation, we show that the conditional gradient algorithm can approximate any homogeneous mixture within accuracy by combining O(1/ 2) \u201cshallow\u201d architectures, where the hidden constant may decrease (exponentially) with respect to the depth. Our experiments on both synthetic and real datasets confirm the benefits of depth in density estimation."
    },
    "keywords": [
        {
            "term": "Gaussian mixture model",
            "url": "https://en.wikipedia.org/wiki/Gaussian_mixture_model"
        },
        {
            "term": "density estimation",
            "url": "https://en.wikipedia.org/wiki/density_estimation"
        },
        {
            "term": "conditional probability tables",
            "url": "https://en.wikipedia.org/wiki/Conditional_Probability_Table"
        },
        {
            "term": "homogeneous mixture",
            "url": "https://en.wikipedia.org/wiki/homogeneous_mixture"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "hidden markov model",
            "url": "https://en.wikipedia.org/wiki/hidden_markov_model"
        }
    ],
    "highlights": [
        "A widely studied problem in statistics and machine learning [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], is becoming even more relevant nowadays due to the availability of huge amounts of unlabeled data in various applications",
        "We study the problem of density estimation with an explicit representation through finite mixture models (FMMs) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], which have endured thorough scientific scrutiny over decades",
        "Borrowing a classic idea from [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] we show that minimizing the KL divergence using the conditional gradient algorithm can approximate any homogeneous mixture within accuracy by combining O(1/ 2) base Sum-Product Networks, where the hidden constant decreases exponentially wrt the depth of the base Sum-Product Networks",
        "Hierarchical Tensor Format (HTF) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] We showed in (1) that a homogeneous mixture can be identified with a tensor W, whose explicit storage can, be quite challenging since its size is d i=1 ki",
        "We have formally established the relationships among some popular unsupervised learning models, such as latent tree graphical models, hierarchical tensor formats and sum-product networks, based on which we further provided a unified treatment of exponential separation in exact representation size between deep architectures and shallow ones",
        "For approximate representation, the conditional gradient algorithm can approximate any homogeneous mixture within accuracy by combining O(1/ 2) shallow models, where the hidden constant may decrease exponentially wrt the depth"
    ],
    "key_statements": [
        "A widely studied problem in statistics and machine learning [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], is becoming even more relevant nowadays due to the availability of huge amounts of unlabeled data in various applications",
        "We study the problem of density estimation with an explicit representation through finite mixture models (FMMs) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], which have endured thorough scientific scrutiny over decades",
        "Borrowing a classic idea from [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] we show that minimizing the KL divergence using the conditional gradient algorithm can approximate any homogeneous mixture within accuracy by combining O(1/ 2) base Sum-Product Networks, where the hidden constant decreases exponentially wrt the depth of the base Sum-Product Networks",
        "Hierarchical Tensor Format (HTF) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] We showed in (1) that a homogeneous mixture can be identified with a tensor W, whose explicit storage can, be quite challenging since its size is d i=1 ki",
        "We proved that homogeneous mixtures representable by \u201cdeep\u201d architectures of polynomial size cannot be exactly represented by a shallow one with sub-exponential size",
        "We have formally established the relationships among some popular unsupervised learning models, such as latent tree graphical models, hierarchical tensor formats and sum-product networks, based on which we further provided a unified treatment of exponential separation in exact representation size between deep architectures and shallow ones",
        "For approximate representation, the conditional gradient algorithm can approximate any homogeneous mixture within accuracy by combining O(1/ 2) shallow models, where the hidden constant may decrease exponentially wrt the depth"
    ],
    "summary": [
        "A widely studied problem in statistics and machine learning [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], is becoming even more relevant nowadays due to the availability of huge amounts of unlabeled data in various applications.",
        "We first prove that the tensor rank exactly characterizes the minimum size of a shallow SPN that represents a given homogeneous mixture.",
        "{TMM, HMM} \u2286 LTM = dHTF+ \u2286 HTF+ = S3PN \u2286 SPN, in the sense that we can convert in linear time from a lower representation class to an upper one, without any increase in size.",
        "Using the equivalence in Section 3, we know a shallow SPN is equivalent to an LCM (a latent tree model with one hidden node taking as many values as the number of product nodes), or an HTF+ whose DPT has depth 1 (c.f. Figure 1).",
        "If a shallow SPN T, with leaf nodes from G, represents the density mixture W, T has at least rank+(W) many product nodes.",
        "The nonnegative rank characterizes the smallest size of shallow SPNs (LCMs) that represent the density mixture W.",
        "If a shallow SPN T, with leaf nodes from F, represents the density mixture W, either T has at least nnz(W) product nodes or rank+(W) = 1.",
        "[<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] was the first to construct a polynomial that, while representable by a polynomially-sized depth-log d SPN, would require exponentially many product nodes if represented by a shallow SPN.",
        "We proved that homogeneous mixtures representable by \u201cdeep\u201d architectures of polynomial size cannot be exactly represented by a shallow one with sub-exponential size.",
        "-rank+ is precisely the minimum size of a shallow SPN (LCM) that approximates a specified mixture W with accuracy .",
        "Due to space limit Figure 4 only presents results comparing our model with (i) data imputation techniques that complete missing pixels with zeros or NICE [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], a generative model suited for inpainting, and using a ConvNet for prediction, an SPN with structure learned using data as proposed in [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] augmented with a class variable to maximize joint probability, and shallow networks to demonstrate the benefits of depth.",
        "We have formally established the relationships among some popular unsupervised learning models, such as latent tree graphical models, hierarchical tensor formats and sum-product networks, based on which we further provided a unified treatment of exponential separation in exact representation size between deep architectures and shallow ones.",
        "For approximate representation, the conditional gradient algorithm can approximate any homogeneous mixture within accuracy by combining O(1/ 2) shallow models, where the hidden constant may decrease exponentially wrt the depth.",
        "Experiments on both synthetic and real datasets confirmed our theoretical findings"
    ],
    "headline": "We show that the conditional gradient algorithm can approximate any homogeneous mixture within accuracy by combining O \u201cshallow\u201d architectures, where the hidden constant may decrease  with respect to the depth",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Noga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics, Probability and Computing, 18(1-2):3\u201315, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alon%2C%20Noga%20Perturbed%20identity%20matrices%20have%20high%20rank%3A%20Proof%20and%20applications%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alon%2C%20Noga%20Perturbed%20identity%20matrices%20have%20high%20rank%3A%20Proof%20and%20applications%202009"
        },
        {
            "id": "2",
            "entry": "[2] Animashree Anandkumar, Daniel Hsu, Furong Huang, and Sham M. Kakade. Learning mixtures of tree graphical models. In Advances in Neural Information Processing Systems, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Hsu%2C%20Daniel%20Huang%2C%20Furong%20Kakade%2C%20Sham%20M.%20Learning%20mixtures%20of%20tree%20graphical%20models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Hsu%2C%20Daniel%20Huang%2C%20Furong%20Kakade%2C%20Sham%20M.%20Learning%20mixtures%20of%20tree%20graphical%20models%202012"
        },
        {
            "id": "3",
            "entry": "[3] Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554\u20131563, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baum%2C%20Leonard%20E.%20Petrie%2C%20Ted%20Statistical%20inference%20for%20probabilistic%20functions%20of%20finite%20state%20markov%20chains%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baum%2C%20Leonard%20E.%20Petrie%2C%20Ted%20Statistical%20inference%20for%20probabilistic%20functions%20of%20finite%20state%20markov%20chains%201966"
        },
        {
            "id": "4",
            "entry": "[4] Richard Caron and Tim Traynor. The zero set of a polynomial. Technical report, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caron%2C%20Richard%20Traynor%2C%20Tim%20The%20zero%20set%20of%20a%20polynomial%202005"
        },
        {
            "id": "5",
            "entry": "[5] Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar, and Alan S.Willsky. Learning latent tree graphical models. Journal of Machine Learning Research, 12:1771\u20131812, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Myung%20Jin%20Tan%2C%20Vincent%20Y.F.%20Anandkumar%2C%20Animashree%20Willsky%2C%20Alan%20S.%20Learning%20latent%20tree%20graphical%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20Myung%20Jin%20Tan%2C%20Vincent%20Y.F.%20Anandkumar%2C%20Animashree%20Willsky%2C%20Alan%20S.%20Learning%20latent%20tree%20graphical%20models%202011"
        },
        {
            "id": "6",
            "entry": "[6] Nadav Cohen, Or Sharir, Yoav Levine, Ronen Tamari, David Yakira, and Amnon Shashua. Analysis and design of convolutional networks via hierarchical tensor decompositions, 2017. arXiv:1705.02302v4.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02302v4"
        },
        {
            "id": "7",
            "entry": "[7] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In Conference on Learning Theory, pages 698\u2013728, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Nadav%20Sharir%2C%20Or%20Shashua%2C%20Amnon%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Nadav%20Sharir%2C%20Or%20Shashua%2C%20Amnon%20On%20the%20expressive%20power%20of%20deep%20learning%3A%20A%20tensor%20analysis%202016"
        },
        {
            "id": "8",
            "entry": "[8] Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Nadav%20Shashua%2C%20Amnon%20Convolutional%20rectifier%20networks%20as%20generalized%20tensor%20decompositions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Nadav%20Shashua%2C%20Amnon%20Convolutional%20rectifier%20networks%20as%20generalized%20tensor%20decompositions%202016"
        },
        {
            "id": "9",
            "entry": "[9] Adnan Darwiche. A differential approach to inference in bayesian networks. Journal of the ACM (JACM), 50(3):280\u2013305, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Darwiche%2C%20Adnan%20A%20differential%20approach%20to%20inference%20in%20bayesian%20networks%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Darwiche%2C%20Adnan%20A%20differential%20approach%20to%20inference%20in%20bayesian%20networks%202003"
        },
        {
            "id": "10",
            "entry": "[10] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems, pages 666\u2013674, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Delalleau%2C%20Olivier%20Bengio%2C%20Yoshua%20Shallow%20vs.%20deep%20sum-product%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Delalleau%2C%20Olivier%20Bengio%2C%20Yoshua%20Shallow%20vs.%20deep%20sum-product%20networks%202011"
        },
        {
            "id": "11",
            "entry": "[11] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.8516"
        },
        {
            "id": "12",
            "entry": "[12] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3(1-2):95\u2013110, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming%201956"
        },
        {
            "id": "13",
            "entry": "[13] Wolfgang Hackbusch. Tensor Spaces and Numerical Tensor Calculus. Springer, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hackbusch%2C%20Wolfgang%20Tensor%20Spaces%20and%20Numerical%20Tensor%20Calculus%202012"
        },
        {
            "id": "14",
            "entry": "[14] M. Ishteva. Tensors and latent variable models. In The 12th International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), pages 49\u2013\u00e2A S 55, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ishteva%2C%20M.%20Tensors%20and%20latent%20variable%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ishteva%2C%20M.%20Tensors%20and%20latent%20variable%20models%202015"
        },
        {
            "id": "15",
            "entry": "[15] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "16",
            "entry": "[16] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II\u2013104. IEEE, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Huang%2C%20Fu%20Jie%20Bottou%2C%20Leon%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Huang%2C%20Fu%20Jie%20Bottou%2C%20Leon%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004"
        },
        {
            "id": "17",
            "entry": "[17] Jonathan Q Li and Andrew R Barron. Mixture density estimation. In Advances in neural information processing systems, pages 279\u2013285, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Jonathan%20Q.%20Barron%2C%20Andrew%20R.%20Mixture%20density%20estimation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Jonathan%20Q.%20Barron%2C%20Andrew%20R.%20Mixture%20density%20estimation%202000"
        },
        {
            "id": "18",
            "entry": "[18] James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks. arXiv preprint arXiv:1411.7717, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.7717"
        },
        {
            "id": "19",
            "entry": "[19] Geoffrey McLachlan and David Peel. Finite mixture models. John Wiley & Sons, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McLachlan%2C%20Geoffrey%20Peel%2C%20David%20Finite%20mixture%20models%202004"
        },
        {
            "id": "20",
            "entry": "[20] Marina Meila and Michael I. Jordan. Learning with mixtures of trees. Journal of Machine Learning Research, 1:1\u201348, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meila%2C%20Marina%20Jordan%2C%20Michael%20I.%20Learning%20with%20mixtures%20of%20trees%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meila%2C%20Marina%20Jordan%2C%20Michael%20I.%20Learning%20with%20mixtures%20of%20trees%202000"
        },
        {
            "id": "21",
            "entry": "[21] Rapha\u00ebl Mourad, Christine Sinoquet, Nevin L. Zhang, Tengfei Liu, and Philippe Leray. A survey on latent tree models and applications. Journal of Artificial Intelligence Research, 47:157\u2013203, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mourad%2C%20Rapha%C3%ABl%20Sinoquet%2C%20Christine%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20and%20Philippe%20Leray.%20A%20survey%20on%20latent%20tree%20models%20and%20applications%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mourad%2C%20Rapha%C3%ABl%20Sinoquet%2C%20Christine%20Zhang%2C%20Nevin%20L.%20Liu%2C%20Tengfei%20and%20Philippe%20Leray.%20A%20survey%20on%20latent%20tree%20models%20and%20applications%202013"
        },
        {
            "id": "22",
            "entry": "[22] Hien D Nguyen and Geoffrey J McLachlan. On approximations via convolution-defined mixture models. arXiv preprint arXiv:1611.03974, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03974"
        },
        {
            "id": "23",
            "entry": "[23] Robert Peharz, Robert Gens, Franz Pernkopf, and Pedro Domingos. On the latent variable interpretation in sum-product networks. IEEE transactions on pattern analysis and machine intelligence, 39(10):2030\u20132044, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peharz%2C%20Robert%20Gens%2C%20Robert%20Pernkopf%2C%20Franz%20Domingos%2C%20Pedro%20On%20the%20latent%20variable%20interpretation%20in%20sum-product%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peharz%2C%20Robert%20Gens%2C%20Robert%20Pernkopf%2C%20Franz%20Domingos%2C%20Pedro%20On%20the%20latent%20variable%20interpretation%20in%20sum-product%20networks%202017"
        },
        {
            "id": "24",
            "entry": "[24] Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In Uncertainty in Artificial Intelligence. UAI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poon%2C%20Hoifung%20Domingos%2C%20Pedro%20Sum-product%20networks%3A%20A%20new%20deep%20architecture%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poon%2C%20Hoifung%20Domingos%2C%20Pedro%20Sum-product%20networks%3A%20A%20new%20deep%20architecture%202011"
        },
        {
            "id": "25",
            "entry": "[25] Lawrence R Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257\u2013286, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rabiner%2C%20Lawrence%20R.%20A%20tutorial%20on%20hidden%20markov%20models%20and%20selected%20applications%20in%20speech%20recognition%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rabiner%2C%20Lawrence%20R.%20A%20tutorial%20on%20hidden%20markov%20models%20and%20selected%20applications%20in%20speech%20recognition%201989"
        },
        {
            "id": "26",
            "entry": "[26] Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tensorial mixture models, 2018. arXiv:1610.04167v5.",
            "arxiv_url": "https://arxiv.org/pdf/1610.04167v5"
        },
        {
            "id": "27",
            "entry": "[27] Le Song, Haesun Park, Mariya Ishteva, Ankur Parikh, and Eric Xing. Hierarchical tensor decomposition of latent tree graphical models. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Le%20Park%2C%20Haesun%20Ishteva%2C%20Mariya%20Ankur%20Parikh%2C%20and%20Eric%20Xing.%20Hierarchical%20tensor%20decomposition%20of%20latent%20tree%20graphical%20models.%20In%20ICML%202013"
        },
        {
            "id": "28",
            "entry": "[28] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexandre%20B.%20Introduction%20to%20Nonparametric%20Estimation%202009"
        },
        {
            "id": "29",
            "entry": "[29] Han Zhao, Mazen Melibari, and Pascal Poupart. On the relationship between sum-product networks and bayesian networks. In International Conference on Machine Learning, pages 116\u2013124, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Han%20Melibari%2C%20Mazen%20Poupart%2C%20Pascal%20On%20the%20relationship%20between%20sum-product%20networks%20and%20bayesian%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Han%20Melibari%2C%20Mazen%20Poupart%2C%20Pascal%20On%20the%20relationship%20between%20sum-product%20networks%20and%20bayesian%20networks%202015"
        }
    ]
}
