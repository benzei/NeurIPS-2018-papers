{
    "filename": "7407-adaptive-online-learning-in-dynamic-environments.pdf",
    "metadata": {
        "title": "Adaptive Online Learning in Dynamic Environments",
        "author": "Lijun Zhang, Shiyin Lu, Zhi-Hua Zhou",
        "date": 2016,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7407-adaptive-online-learning-in-dynamic-environments.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of\u221acomparators. Existing work have shown that online gradient descent enjoys an O( T (1 + PT )) dynamic regret, where T is the number of iterations and PT is the path-length of the comparator sequence. However, this result is unsatisfactory, as there exists a large gap from the \u03a9( T (1 + PT )) lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an optimal O( T (1 + PT )) dynamic regret. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm. Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from O(log T ) to 1. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators."
    },
    "keywords": [
        {
            "term": "online learning",
            "url": "https://en.wikipedia.org/wiki/online_learning"
        },
        {
            "term": "path length",
            "url": "https://en.wikipedia.org/wiki/path_length"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "adaptive learning",
            "url": "https://en.wikipedia.org/wiki/adaptive_learning"
        },
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        }
    ],
    "highlights": [
        "Online convex optimization (OCO) has become a popular learning framework for modeling various real-world problems, such as online routing, ad selection for search engines and spam filtering [<a class=\"ref-link\" id=\"cHazan_2016_a\" href=\"#rHazan_2016_a\"><a class=\"ref-link\" id=\"cHazan_2016_a\" href=\"#rHazan_2016_a\">Hazan, 2016</a></a>]",
        "We propose a novel online method, namely adaptive learning for dynamic environment (Ader), which attains an O( T (1 + PT )) dynamic regret",
        "We study the general form of dynamic regret, which compares the cumulative loss of the online learner against an arbitrary sequence of comparators",
        "We develop a novel method, named as adaptive learning for dynamic environment (Ader)",
        "Theoretical analysis shows that adaptive learning for dynamic environment achieves an optimal O( T (1 + PT )) dynamic regret",
        "When a sequence of dynamical models is available, we extend adaptive learning for dynamic environment to incorporate this additional information, and obtain an O( T (1 + PT )) dynamic regret"
    ],
    "key_statements": [
        "Online convex optimization (OCO) has become a popular learning framework for modeling various real-world problems, such as online routing, ad selection for search engines and spam filtering [<a class=\"ref-link\" id=\"cHazan_2016_a\" href=\"#rHazan_2016_a\"><a class=\"ref-link\" id=\"cHazan_2016_a\" href=\"#rHazan_2016_a\">Hazan, 2016</a></a>]",
        "The learner suffers an instantaneous loss ft, and the goal is to minimize the cumulative loss over T iterations",
        "Instead of following the definition in (2), most of existing studies on dynamic regret consider a restricted form, in which the sequence of comparators consists of local minimizers of online functions [<a class=\"ref-link\" id=\"cBesbes_et+al_2015_a\" href=\"#rBesbes_et+al_2015_a\">Besbes et al, 2015</a>], i.e., T",
        "We focus on the general dynamic regret in this paper",
        "One result is given by <a class=\"ref-link\" id=\"cZinkevich_2003_a\" href=\"#rZinkevich_2003_a\">Zinkevich [2003</a>], who demonstrates that online gradient descent (OGD) achieves the following dynamic regret bound",
        "We propose a novel online method, namely adaptive learning for dynamic environment (Ader), which attains an O( T (1 + PT )) dynamic regret",
        "While the basic version of adaptive learning for dynamic environment needs to query the gradient O times in each round, we develop an improved version based on surrogate loss and reduce the number of gradient evaluations to 1",
        "\u03a9( T (1 + PT )). We develop a serial of novel methods for minimizing the general dynamic regret, and prove an optimal O( T (1 + PT )) upper bound. Compared to existing work for the restricted dynamic regret in (3), our result is universal in the sense that the regret bound holds for any sequence of comparators. Our result is adaptive because the upper bound depends on the path-length of the comparator sequence, so it automatically becomes small when comparators change slowly.\n2 Related Work",
        "online gradient descent achieves the O(PT\u2217) dynamic regret, when the online functions are strongly convex and smooth [<a class=\"ref-link\" id=\"cMokhtari_et+al_2016_a\" href=\"#rMokhtari_et+al_2016_a\">Mokhtari et al, 2016</a>], or when they are convex and smooth and all the minimizers lie in the interior of X [<a class=\"ref-link\" id=\"cYang_et+al_2016_a\" href=\"#rYang_et+al_2016_a\">Yang et al, 2016</a>]",
        "We first state assumptions about the online problem, provide our motivations, including a lower bound of the general dynamic regret, and present the proposed methods as well as their theoretical guarantees",
        "Similar to previous studies in online learning, we introduce the following common assumptions",
        "When bounding the general dynamic regret, we face the following challenge: On one hand, we want the regret bound to hold for any sequence of comparators, but on the other hand, to get a tighter bound, we need to tune the step size for a specific path-length",
        "adaptive learning for dynamic environment maintains a set of experts, each attaining an optimal dynamic regret for a different path-length, and chooses the best one using an expert-tracking algorithm",
        "We have the following theorem to bound the dynamic regret of the improved adaptive learning for dynamic environment",
        "We study the general form of dynamic regret, which compares the cumulative loss of the online learner against an arbitrary sequence of comparators",
        "We develop a novel method, named as adaptive learning for dynamic environment (Ader)",
        "Theoretical analysis shows that adaptive learning for dynamic environment achieves an optimal O( T (1 + PT )) dynamic regret",
        "When a sequence of dynamical models is available, we extend adaptive learning for dynamic environment to incorporate this additional information, and obtain an O( T (1 + PT )) dynamic regret"
    ],
    "summary": [
        "Online convex optimization (OCO) has become a popular learning framework for modeling various real-world problems, such as online routing, ad selection for search engines and spam filtering [<a class=\"ref-link\" id=\"cHazan_2016_a\" href=\"#rHazan_2016_a\"><a class=\"ref-link\" id=\"cHazan_2016_a\" href=\"#rHazan_2016_a\">Hazan, 2016</a></a>].",
        "Instead of following the definition in (2), most of existing studies on dynamic regret consider a restricted form, in which the sequence of comparators consists of local minimizers of online functions [<a class=\"ref-link\" id=\"cBesbes_et+al_2015_a\" href=\"#rBesbes_et+al_2015_a\">Besbes et al, 2015</a>], i.e., T",
        "One result is given by <a class=\"ref-link\" id=\"cZinkevich_2003_a\" href=\"#rZinkevich_2003_a\"><a class=\"ref-link\" id=\"cZinkevich_2003_a\" href=\"#rZinkevich_2003_a\">Zinkevich [2003</a></a>], who demonstrates that online gradient descent (OGD) achieves the following dynamic regret bound",
        "We develop a serial of novel methods for minimizing the general dynamic regret, and prove an optimal O( T (1 + PT )) upper bound.",
        "\u221a In static setting, online gradient descent (OGD) achieves an O( T ) regret bound for general convex functions.",
        "When the online functions are both smooth and convex, the regret bound could be improved if the cumulative loss of the optimal prediction is small [<a class=\"ref-link\" id=\"cSrebro_et+al_2010_a\" href=\"#rSrebro_et+al_2010_a\">Srebro et al, 2010</a>].",
        "OGD achieves the O(PT\u2217) dynamic regret, when the online functions are strongly convex and smooth [<a class=\"ref-link\" id=\"cMokhtari_et+al_2016_a\" href=\"#rMokhtari_et+al_2016_a\"><a class=\"ref-link\" id=\"cMokhtari_et+al_2016_a\" href=\"#rMokhtari_et+al_2016_a\">Mokhtari et al, 2016</a></a>], or when they are convex and smooth and all the minimizers lie in the interior of X [<a class=\"ref-link\" id=\"cYang_et+al_2016_a\" href=\"#rYang_et+al_2016_a\">Yang et al, 2016</a>].",
        "<a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al [2017</a>] propose online multiple gradient descent, and establish an O) regret bound forstrongly convex and smooth functions.",
        "We first state assumptions about the online problem, provide our motivations, including a lower bound of the general dynamic regret, and present the proposed methods as well as their theoretical guarantees.",
        "According to Theorem 2 of <a class=\"ref-link\" id=\"cZinkevich_2003_a\" href=\"#rZinkevich_2003_a\"><a class=\"ref-link\" id=\"cZinkevich_2003_a\" href=\"#rZinkevich_2003_a\">Zinkevich [2003</a></a>], we have the following dynamic regret bound for online gradient descent (OGD) with a constant step size.",
        "When bounding the general dynamic regret, we face the following challenge: On one hand, we want the regret bound to hold for any sequence of comparators, but on the other hand, to get a tighter bound, we need to tune the step size for a specific path-length.",
        "Ader maintains a set of experts, each attaining an optimal dynamic regret for a different path-length, and chooses the best one using an expert-tracking algorithm.",
        "We have the following theorem to bound the dynamic regret of the improved Ader.",
        "We study the general form of dynamic regret, which compares the cumulative loss of the online learner against an arbitrary sequence of comparators.",
        "We note that in the setting of the restricted dynamic regret, the curvature of functions makes the upper bound tighter [<a class=\"ref-link\" id=\"cMokhtari_et+al_2016_a\" href=\"#rMokhtari_et+al_2016_a\"><a class=\"ref-link\" id=\"cMokhtari_et+al_2016_a\" href=\"#rMokhtari_et+al_2016_a\">Mokhtari et al, 2016</a></a>, <a class=\"ref-link\" id=\"cZhang_et+al_2017_a\" href=\"#rZhang_et+al_2017_a\">Zhang et al, 2017</a>].",
        "When a sequence of dynamical models is available, we extend Ader to incorporate this additional information, and obtain an O( T (1 + PT )) dynamic regret"
    ],
    "headline": "We study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of\u221acomparators",
    "reference_links": [
        {
            "id": "Abernethy_et+al_2008_a",
            "entry": "J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal stragies and minimax lower bounds for online convex games. In Proceedings of the 21st Annual Conference on Learning Theory, pages 415\u2013423, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abernethy%2C%20J.%20Bartlett%2C%20P.L.%20Rakhlin%2C%20A.%20Tewari%2C%20A.%20Optimal%20stragies%20and%20minimax%20lower%20bounds%20for%20online%20convex%20games%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abernethy%2C%20J.%20Bartlett%2C%20P.L.%20Rakhlin%2C%20A.%20Tewari%2C%20A.%20Optimal%20stragies%20and%20minimax%20lower%20bounds%20for%20online%20convex%20games%202008"
        },
        {
            "id": "Besbes_et+al_2015_a",
            "entry": "O. Besbes, Y. Gur, and A. Zeevi. Non-stationary stochastic optimization. Operations Research, 63 (5):1227\u20131244, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Besbes%2C%20O.%20Gur%2C%20Y.%20Zeevi%2C%20A.%20Non-stationary%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Besbes%2C%20O.%20Gur%2C%20Y.%20Zeevi%2C%20A.%20Non-stationary%20stochastic%20optimization%202015"
        },
        {
            "id": "Boyd_2004_a",
            "entry": "S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20Optimization%202004"
        },
        {
            "id": "Cesa-Bianchi_2006_a",
            "entry": "N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20N.%20Prediction%2C%20G.Lugosi%20Learning%2C%20and%20Games%202006"
        },
        {
            "id": "Chiang_et+al_2012_a",
            "entry": "C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In Proceedings of the 25th Annual Conference on Learning Theory, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chiang%2C%20C.-K.%20Yang%2C%20T.%20Lee%2C%20C.-J.%20Mahdavi%2C%20M.%20Online%20optimization%20with%20gradual%20variations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chiang%2C%20C.-K.%20Yang%2C%20T.%20Lee%2C%20C.-J.%20Mahdavi%2C%20M.%20Online%20optimization%20with%20gradual%20variations%202012"
        },
        {
            "id": "Daniely_et+al_2015_a",
            "entry": "A. Daniely, A. Gonen, and S. Shalev-Shwartz. Strongly adaptive online learning. In Proceedings of the 32nd International Conference on Machine Learning, pages 1405\u20131411, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniely%2C%20A.%20Gonen%2C%20A.%20Shalev-Shwartz%2C%20S.%20Strongly%20adaptive%20online%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniely%2C%20A.%20Gonen%2C%20A.%20Shalev-Shwartz%2C%20S.%20Strongly%20adaptive%20online%20learning%202015"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Hall_2013_a",
            "entry": "E. C. Hall and R. M. Willett. Dynamical models and tracking regret in online convex programming. In Proceedings of the 30th International Conference on Machine Learning, pages 579\u2013587, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hall%2C%20E.C.%20Willett%2C%20R.M.%20Dynamical%20models%20and%20tracking%20regret%20in%20online%20convex%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hall%2C%20E.C.%20Willett%2C%20R.M.%20Dynamical%20models%20and%20tracking%20regret%20in%20online%20convex%20programming%202013"
        },
        {
            "id": "Hazan_2016_a",
            "entry": "E. Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2 (3-4):157\u2013325, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Introduction%20to%20online%20convex%20optimization.%20Foundations%20and%20Trends%20in%20Optimization%202016"
        },
        {
            "id": "Hazan_2007_a",
            "entry": "E. Hazan and C. Seshadhri. Adaptive algorithms for online decision problems. Electronic Colloquium on Computational Complexity, 88, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Seshadhri%2C%20C.%20Adaptive%20algorithms%20for%20online%20decision%20problems%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Seshadhri%2C%20C.%20Adaptive%20algorithms%20for%20online%20decision%20problems%202007"
        },
        {
            "id": "Hazan_et+al_2007_b",
            "entry": "E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169\u2013192, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Agarwal%2C%20A.%20Kale%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Agarwal%2C%20A.%20Kale%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007"
        },
        {
            "id": "Herbster_1998_a",
            "entry": "M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32(2):151\u2013178, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Herbster%2C%20M.%20Warmuth%2C%20M.K.%20Tracking%20the%20best%20expert%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Herbster%2C%20M.%20Warmuth%2C%20M.K.%20Tracking%20the%20best%20expert%201998"
        },
        {
            "id": "Jadbabaie_et+al_2015_a",
            "entry": "A. Jadbabaie, A. Rakhlin, S. Shahrampour, and K. Sridharan. Online optimization: Competing with dynamic comparators. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, pages 398\u2013406, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jadbabaie%2C%20A.%20Rakhlin%2C%20A.%20Shahrampour%2C%20S.%20Sridharan%2C%20K.%20Online%20optimization%3A%20Competing%20with%20dynamic%20comparators%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jadbabaie%2C%20A.%20Rakhlin%2C%20A.%20Shahrampour%2C%20S.%20Sridharan%2C%20K.%20Online%20optimization%3A%20Competing%20with%20dynamic%20comparators%202015"
        },
        {
            "id": "Jun_et+al_2017_a",
            "entry": "K.-S. Jun, F. Orabona, S. Wright, and R. Willett. Improved strongly adaptive online learning using coin betting. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pages 943\u2013951, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jun%2C%20K.-S.%20Orabona%2C%20F.%20Wright%2C%20S.%20Willett%2C%20R.%20Improved%20strongly%20adaptive%20online%20learning%20using%20coin%20betting%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jun%2C%20K.-S.%20Orabona%2C%20F.%20Wright%2C%20S.%20Willett%2C%20R.%20Improved%20strongly%20adaptive%20online%20learning%20using%20coin%20betting%202017"
        },
        {
            "id": "Mokhtari_et+al_2016_a",
            "entry": "A. Mokhtari, S. Shahrampour, A. Jadbabaie, and A. Ribeiro. Online optimization in dynamic environments: Improved regret rates for strongly convex problems. In IEEE 55th Conference on Decision and Control, pages 7195\u20137201, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mokhtari%2C%20A.%20Shahrampour%2C%20S.%20Jadbabaie%2C%20A.%20Ribeiro%2C%20A.%20Online%20optimization%20in%20dynamic%20environments%3A%20Improved%20regret%20rates%20for%20strongly%20convex%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mokhtari%2C%20A.%20Shahrampour%2C%20S.%20Jadbabaie%2C%20A.%20Ribeiro%2C%20A.%20Online%20optimization%20in%20dynamic%20environments%3A%20Improved%20regret%20rates%20for%20strongly%20convex%20problems%202016"
        },
        {
            "id": "Rakhlin_2013_a",
            "entry": "A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of the 26th Conference on Learning Theory, pages 993\u20131019, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20A.%20Sridharan%2C%20K.%20Online%20learning%20with%20predictable%20sequences%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakhlin%2C%20A.%20Sridharan%2C%20K.%20Online%20learning%20with%20predictable%20sequences%202013"
        },
        {
            "id": "Shalev-Shwartz_2011_a",
            "entry": "S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107\u2013194, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Online%20learning%20and%20online%20convex%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Online%20learning%20and%20online%20convex%20optimization%202011"
        },
        {
            "id": "Shalev-Shwartz_et+al_2007_a",
            "entry": "S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: primal estimated sub-gradient solver for SVM. In Proceedings of the 24th International Conference on Machine Learning, pages 807\u2013814, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Pegasos%3A%20primal%20estimated%20sub-gradient%20solver%20for%20SVM%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Pegasos%3A%20primal%20estimated%20sub-gradient%20solver%20for%20SVM%202007"
        },
        {
            "id": "Srebro_et+al_2010_a",
            "entry": "N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In Advances in Neural Information Processing Systems 23, pages 2199\u20132207, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srebro%2C%20N.%20Sridharan%2C%20K.%20Tewari%2C%20A.%20Smoothness%2C%20low%20noise%20and%20fast%20rates%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srebro%2C%20N.%20Sridharan%2C%20K.%20Tewari%2C%20A.%20Smoothness%2C%20low%20noise%20and%20fast%20rates%202010"
        },
        {
            "id": "Van_2016_a",
            "entry": "T. van Erven and W. M. Koolen. Metagrad: Multiple learning rates in online learning. In Advances in Neural Information Processing Systems 29, pages 3666\u20133674, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Erven%2C%20T.%20Koolen%2C%20W.M.%20Metagrad%3A%20Multiple%20learning%20rates%20in%20online%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Erven%2C%20T.%20Koolen%2C%20W.M.%20Metagrad%3A%20Multiple%20learning%20rates%20in%20online%20learning%202016"
        },
        {
            "id": "Yang_et+al_2016_a",
            "entry": "T. Yang, L. Zhang, R. Jin, and J. Yi. Tracking slowly moving clairvoyant: Optimal dynamic regret of online learning with true and noisy gradient. In Proceedings of the 33rd International Conference on Machine Learning, pages 449\u2013457, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20T.%20Zhang%2C%20L.%20Jin%2C%20R.%20Yi%2C%20J.%20Tracking%20slowly%20moving%20clairvoyant%3A%20Optimal%20dynamic%20regret%20of%20online%20learning%20with%20true%20and%20noisy%20gradient%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20T.%20Zhang%2C%20L.%20Jin%2C%20R.%20Yi%2C%20J.%20Tracking%20slowly%20moving%20clairvoyant%3A%20Optimal%20dynamic%20regret%20of%20online%20learning%20with%20true%20and%20noisy%20gradient%202016"
        },
        {
            "id": "Zhang_et+al_2013_a",
            "entry": "L. Zhang, J. Yi, R. Jin, M. Lin, and X. He. Online kernel learning with a near optimal sparsity bound. In Proceedings of the 30th International Conference on Machine Learning, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20L.%20Yi%2C%20J.%20Jin%2C%20R.%20Lin%2C%20M.%20Online%20kernel%20learning%20with%20a%20near%20optimal%20sparsity%20bound%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20L.%20Yi%2C%20J.%20Jin%2C%20R.%20Lin%2C%20M.%20Online%20kernel%20learning%20with%20a%20near%20optimal%20sparsity%20bound%202013"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "L. Zhang, T. Yang, J. Yi, R. Jin, and Z.-H. Zhou. Improved dynamic regret for non-degenerate functions. In Advances in Neural Information Processing Systems 30, pages 732\u2013741, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20L.%20Yang%2C%20T.%20Yi%2C%20J.%20Jin%2C%20R.%20Improved%20dynamic%20regret%20for%20non-degenerate%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20L.%20Yang%2C%20T.%20Yi%2C%20J.%20Jin%2C%20R.%20Improved%20dynamic%20regret%20for%20non-degenerate%20functions%202017"
        },
        {
            "id": "Zhang_et+al_0000_a",
            "entry": "L. Zhang, S. Lu, and Z.-H. Zhou. Adaptive online learning in dynamic environments. ArXiv e-prints, arXiv:1810.10815, 2018a.",
            "arxiv_url": "https://arxiv.org/pdf/1810.10815"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "L. Zhang, T. Yang, R. Jin, and Z.-H. Zhou. Dynamic regret of strongly adaptive methods. In Proceedings of the 35th International Conference on Machine Learning, 2018b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20L.%20Yang%2C%20T.%20Jin%2C%20R.%20Zhou%2C%20Z.-H.%20Dynamic%20regret%20of%20strongly%20adaptive%20methods%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20L.%20Yang%2C%20T.%20Jin%2C%20R.%20Zhou%2C%20Z.-H.%20Dynamic%20regret%20of%20strongly%20adaptive%20methods%202018"
        },
        {
            "id": "Zinkevich_2003_a",
            "entry": "M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, pages 928\u2013936, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
