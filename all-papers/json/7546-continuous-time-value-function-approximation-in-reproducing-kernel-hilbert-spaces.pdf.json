{
    "filename": "7546-continuous-time-value-function-approximation-in-reproducing-kernel-hilbert-spaces.pdf",
    "metadata": {
        "title": "Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces",
        "author": "Motoya Ohnishi, Masahiro Yukawa, Mikael Johansson, Masashi Sugiyama",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7546-continuous-time-value-function-approximation-in-reproducing-kernel-hilbert-spaces.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Motivated by the success of reinforcement learning (RL) for discrete-time tasks such as AlphaGo and Atari games, there has been a recent surge of interest in using RL for continuous-time control of physical systems (cf. many challenging tasks in OpenAI Gym and DeepMind Control Suite). Since discretization of time is susceptible to error, it is methodologically more desirable to handle the system dynamics directly in continuous time. However, very few techniques exist for continuous-time RL and they lack flexibility in value function approximation. In this paper, we propose a novel framework for model-based continuous-time value function approximation in reproducing kernel Hilbert spaces. The resulting framework is so flexible that it can accommodate any kind of kernel-based approach, such as Gaussian processes and kernel adaptive filters, and it allows us to handle uncertainties and nonstationarity without prior knowledge about the environment or what basis functions to employ. We demonstrate the validity of the presented framework through experiments."
    },
    "keywords": [
        {
            "term": "stochastic differential equations",
            "url": "https://en.wikipedia.org/wiki/stochastic_differential_equations"
        },
        {
            "term": "linear-quadratic regulator",
            "url": "https://en.wikipedia.org/wiki/linear-quadratic_regulator"
        },
        {
            "term": "quadratic programming",
            "url": "https://en.wikipedia.org/wiki/quadratic_programming"
        },
        {
            "term": "continuous time",
            "url": "https://en.wikipedia.org/wiki/continuous_time"
        },
        {
            "term": "function approximation",
            "url": "https://en.wikipedia.org/wiki/function_approximation"
        },
        {
            "term": "Gaussian processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_processes"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "RKHS",
            "url": "https://en.wikipedia.org/wiki/RKHS"
        },
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "reproducing kernel Hilbert spaces",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_spaces"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "optimal control",
            "url": "https://en.wikipedia.org/wiki/optimal_control"
        }
    ],
    "highlights": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] has been successful in a variety of applications such as AlphaGo and Atari games, particularly for discrete stochastic systems",
        "We present a novel theoretical framework of model-based CT-value function approximation in reproducing kernel Hilbert spaces (RKHSs) [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] for systems described by stochastic differential equations",
        "Under Assumptions 1 and 2, Theorem 1 implies that the value function V can be uniquely determined by the immediate cost function R for a policy if the value function is in an reproducing kernel Hilbert spaces of a particular class",
        "For example, the work in [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which aims at attaining a sparse representation of the unknown function in an online fashion in reproducing kernel Hilbert spaces, was extended to the policy evaluation [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] by addressing the double-sampling problem, our framework does not suffer from the double-sampling problem, and any kernel-based online learning (e.g., [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]) can be straightforwardly applied",
        "Stochastic optimal control such as the work in [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] requires sample trajectories over predefined finite time horizons and the value is computed along the trajectories while the value function is estimated in an reproducing kernel Hilbert spaces even without having to follow the trajectory in our framework.\n6 Applications and practical implementation",
        "We can consider uncertainties in value function approximation by virtue of the reproducing kernel Hilbert spaces-based formulation, which might be used for safety verifications"
    ],
    "key_statements": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] has been successful in a variety of applications such as AlphaGo and Atari games, particularly for discrete stochastic systems",
        "We present a novel theoretical framework of model-based CT-value function approximation in reproducing kernel Hilbert spaces (RKHSs) [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] for systems described by stochastic differential equations",
        "Under Assumptions 1 and 2, Theorem 1 implies that the value function V can be uniquely determined by the immediate cost function R for a policy if the value function is in an reproducing kernel Hilbert spaces of a particular class",
        "For example, the work in [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which aims at attaining a sparse representation of the unknown function in an online fashion in reproducing kernel Hilbert spaces, was extended to the policy evaluation [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] by addressing the double-sampling problem, our framework does not suffer from the double-sampling problem, and any kernel-based online learning (e.g., [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]) can be straightforwardly applied",
        "Stochastic optimal control such as the work in [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] requires sample trajectories over predefined finite time horizons and the value is computed along the trajectories while the value function is estimated in an reproducing kernel Hilbert spaces even without having to follow the trajectory in our framework.\n6 Applications and practical implementation",
        "We presented a novel theoretical framework that renders the",
        "There are several possible directions to explore as future works; First, we can employ the state-of-the-art kernel methods within our theoretical framework or use other variants of Reinforcement learning, such as actor-critic methods, to improve practical performances",
        "We can consider uncertainties in value function approximation by virtue of the reproducing kernel Hilbert spaces-based formulation, which might be used for safety verifications"
    ],
    "summary": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] has been successful in a variety of applications such as AlphaGo and Atari games, particularly for discrete stochastic systems.",
        "3. Estimate the immediate cost function R in the RKHS H by kernel-based supervised",
        "Policy update while restricting certain regions of the state space As mentioned above, one of the advantages of a CT framework is its affinity for control-theoretic analyses such as stability and forward invariance, which are useful for safety-critical applications.",
        "Lipschitz continuous policy satisfying control barrier certificates renders the set C forward invariant [<a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>], i.e., the state",
        "Under Assumptions 1 and 2, Theorem 1 implies that the VF V can be uniquely determined by the immediate cost function R for a policy if the VF is in an RKHS of a particular class.",
        "Any kernel-based supervised learning to estimate the function R in the RKHS H , such as GPs and",
        "Our proposed framework takes advantage of the capability of learning complicated functions and nonparametric flexibility of RKHSs, and reproduces some of the existing model-based DT-VF approximation techniques.",
        "Since the RKHS H for learning is explicitly defined in our framework, any kernel-based method and its",
        "For example, the work in [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which aims at attaining a sparse representation of the unknown function in an online fashion in RKHSs, was extended to the policy evaluation [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] by addressing the double-sampling problem, our framework does not suffer from the double-sampling problem, and any kernel-based online learning (e.g., [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]) can be straightforwardly applied.",
        "Stochastic optimal control such as the work in [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] requires sample trajectories over predefined finite time horizons and the value is computed along the trajectories while the VF is estimated in an RKHS even without having to follow the trajectory in our framework.",
        "Note the immediate cost for the DT cases is given by R x t , u t \u270f t, where t is the time interval.",
        "It is observed that the CT approach is immune to the choice of t while the performance of the DT approach degrades when the time interval becomes small, and that the barrier-certified policy updates work efficiently.",
        "RKHS by conducting kernel-based supervised learning for the immediate cost function in the properly defined RKHS.",
        "There are several possible directions to explore as future works; First, we can employ the state-of-the-art kernel methods within our theoretical framework or use other variants of RL, such as actor-critic methods, to improve practical performances.",
        "We can consider uncertainties in value function approximation by virtue of the RKHS-based formulation, which might be used for safety verifications."
    ],
    "headline": "We propose a novel framework for model-based continuous-time value function approximation in reproducing kernel Hilbert spaces",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agrawal and K. Sreenath. \u201cDiscrete control barrier functions for safety-critical control of discrete systems with application to bipedal robot navigation\u201d. In: Proc. RSS. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20A.%20Sreenath%2C%20K.%20Discrete%20control%20barrier%20functions%20for%20safety-critical%20control%20of%20discrete%20systems%20with%20application%20to%20bipedal%20robot%20navigation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20A.%20Sreenath%2C%20K.%20Discrete%20control%20barrier%20functions%20for%20safety-critical%20control%20of%20discrete%20systems%20with%20application%20to%20bipedal%20robot%20navigation%202017"
        },
        {
            "id": "2",
            "entry": "[2] A. D. Ames et al. \u201cControl barrier function based quadratic programs with application to automotive safety systems\u201d. In: arXiv preprint arXiv:1609.06408 (2016).",
            "arxiv_url": "https://arxiv.org/pdf/1609.06408"
        },
        {
            "id": "3",
            "entry": "[3] N. Aronszajn. \u201cTheory of reproducing kernels\u201d. In: Trans. Amer. Math. Soc. 68.3 (1950), pp. 337\u2013404.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20N.%20%E2%80%9CTheory%20of%20reproducing%20kernels%E2%80%9D.%20In%3A%20Trans%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aronszajn%2C%20N.%20%E2%80%9CTheory%20of%20reproducing%20kernels%E2%80%9D.%20In%3A%20Trans%201950"
        },
        {
            "id": "4",
            "entry": "[4] L. Baird. \u201cReinforcement learning in continuous time: Advantage updating\u201d. In: Proc. IEEE ICNN. Vol. 4. 1994, pp. 2448\u20132453.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baird%2C%20L.%20Reinforcement%20learning%20in%20continuous%20time%3A%20Advantage%20updating%201994"
        },
        {
            "id": "5",
            "entry": "[5] L. Baird. \u201cResidual algorithms: Reinforcement learning with function approximation\u201d. In: Proc. ICML. 1995, pp. 30\u201337.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baird%2C%20L.%20Residual%20algorithms%3A%20Reinforcement%20learning%20with%20function%20approximation%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baird%2C%20L.%20Residual%20algorithms%3A%20Reinforcement%20learning%20with%20function%20approximation%201995"
        },
        {
            "id": "6",
            "entry": "[6] J. Bonet and R. D. Wood. Nonlinear continuum mechanics for finite element analysis. Cambridge University Press, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonet%2C%20J.%20Wood%2C%20R.D.%20Nonlinear%20continuum%20mechanics%20for%20finite%20element%20analysis%201997"
        },
        {
            "id": "7",
            "entry": "[7] G. Brockman et al. \u201cOpenAI Gym\u201d. In: arXiv preprint arXiv:1606.01540 (2016). 12.1 (2000), pp. 219\u2013245.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "9",
            "entry": "[9] Y. Engel, S. Mannor, and R. Meir. \u201cReinforcement learning with Gaussian processes\u201d. In: Proc. ICML. 2005, pp. 201\u2013208.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Y.%20Mannor%2C%20S.%20Meir%2C%20R.%20Reinforcement%20learning%20with%20Gaussian%20processes%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engel%2C%20Y.%20Mannor%2C%20S.%20Meir%2C%20R.%20Reinforcement%20learning%20with%20Gaussian%20processes%202005"
        },
        {
            "id": "10",
            "entry": "[10] W. H. Fleming and H. M. Soner. Controlled Markov processes and viscosity solutions. Vol. 25. Springer Science & Business Media, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fleming%2C%20W.H.%20Soner%2C%20H.M.%20Controlled%20Markov%20processes%20and%20viscosity%20solutions%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fleming%2C%20W.H.%20Soner%2C%20H.M.%20Controlled%20Markov%20processes%20and%20viscosity%20solutions%202006"
        },
        {
            "id": "11",
            "entry": "[11] J. Garc\u0131a and F. Fern\u00e1ndez. \u201cA comprehensive survey on safe reinforcement learning\u201d. In: J. Mach. Learn. Res. 16.1 (2015), pp. 1437\u20131480.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garc%C4%B1a%2C%20J.%20Fern%C3%A1ndez%2C%20F.%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garc%C4%B1a%2C%20J.%20Fern%C3%A1ndez%2C%20F.%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015"
        },
        {
            "id": "12",
            "entry": "[12] P. Glotfelter, J. Cort\u00e9s, and M. Egerstedt. \u201cNonsmooth barrier functions with applications to multi-robot systems\u201d. In: IEEE Control Systems Letters 1.2 (2017), pp. 310\u2013315.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=P.%20Glotfelter%2C%20J.%20Cort%C3%A9s%20Egerstedt%2C%20M.%20Nonsmooth%20barrier%20functions%20with%20applications%20to%20multi-robot%20systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=P.%20Glotfelter%2C%20J.%20Cort%C3%A9s%20Egerstedt%2C%20M.%20Nonsmooth%20barrier%20functions%20with%20applications%20to%20multi-robot%20systems%202017"
        },
        {
            "id": "13",
            "entry": "[13] S. Grunewalder et al. \u201cModelling transition dynamics in MDPs with RKHS embeddings\u201d. In: Proc. ICML. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grunewalder%2C%20S.%20Modelling%20transition%20dynamics%20in%20MDPs%20with%20RKHS%20embeddings%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grunewalder%2C%20S.%20Modelling%20transition%20dynamics%20in%20MDPs%20with%20RKHS%20embeddings%202012"
        },
        {
            "id": "14",
            "entry": "[14] H. K Khalil. \u201cNonlinear systems\u201d. In: Prentice-Hall 3 (1996). Business Media, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khalil%2C%20H.K.%20%E2%80%9CNonlinear%20systems%E2%80%9D.%20In%3A%20Prentice-Hall%203%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khalil%2C%20H.K.%20%E2%80%9CNonlinear%20systems%E2%80%9D.%20In%3A%20Prentice-Hall%203%201996"
        },
        {
            "id": "16",
            "entry": "[16] V. R. Konda, J. N. Tsitsiklis, et al. \u201cConvergence rate of linear two-time-scale stochastic approximation\u201d. In: The Annals of Applied Probability 14.2 (2004), pp. 796\u2013819.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konda%2C%20V.R.%20Tsitsiklis%2C%20J.N.%20Convergence%20rate%20of%20linear%20two-time-scale%20stochastic%20approximation%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konda%2C%20V.R.%20Tsitsiklis%2C%20J.N.%20Convergence%20rate%20of%20linear%20two-time-scale%20stochastic%20approximation%202004"
        },
        {
            "id": "17",
            "entry": "[17] A. Koppel et al. \u201cParsimonious online learning with kernels via sparse projections in function space\u201d. In: Proc. IEEE ICASSP. 2017, pp. 4671\u20134675.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koppel%2C%20A.%20Parsimonious%20online%20learning%20with%20kernels%20via%20sparse%20projections%20in%20function%20space%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koppel%2C%20A.%20Parsimonious%20online%20learning%20with%20kernels%20via%20sparse%20projections%20in%20function%20space%202017"
        },
        {
            "id": "18",
            "entry": "[18] A. Koppel et al. \u201cPolicy evaluation in continuous MDPs with efficient kernelized gradient temporal difference\u201d. In: IEEE Trans. Automatic Control (submitted) (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koppel%2C%20A.%20Policy%20evaluation%20in%20continuous%20MDPs%20with%20efficient%20kernelized%20gradient%20temporal%20difference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koppel%2C%20A.%20Policy%20evaluation%20in%20continuous%20MDPs%20with%20efficient%20kernelized%20gradient%20temporal%20difference%202017"
        },
        {
            "id": "19",
            "entry": "[19] N. V. Krylov. Controlled diffusion processes. Vol. 14. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krylov%2C%20N.V.%20Controlled%20diffusion%20processes.%20Vol.%2014%202008"
        },
        {
            "id": "20",
            "entry": "[20] F. L. Lewis and D. Vrabie. \u201cReinforcement learning and adaptive dynamic programming for feedback control\u201d. In: IEEE Circuits and Systems Magazine 9.3 (2009).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lewis%2C%20F.L.%20Vrabie%2C%20D.%20Reinforcement%20learning%20and%20adaptive%20dynamic%20programming%20for%20feedback%20control%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lewis%2C%20F.L.%20Vrabie%2C%20D.%20Reinforcement%20learning%20and%20adaptive%20dynamic%20programming%20for%20feedback%20control%202009"
        },
        {
            "id": "21",
            "entry": "[21] D. Liberzon. Calculus of variations and optimal control theory: a concise introduction. Princeton University Press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liberzon%2C%20D.%20Calculus%20of%20variations%20and%20optimal%20control%20theory%3A%20a%20concise%20introduction%202011"
        },
        {
            "id": "22",
            "entry": "[22] W. Liu, J. Pr\u00edncipe, and S. Haykin. Kernel adaptive filtering. New Jersey: Wiley, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20W.%20Pr%C3%ADncipe%2C%20J.%20Haykin%2C%20S.%20Kernel%20adaptive%20filtering%202010"
        },
        {
            "id": "23",
            "entry": "[23] H. Q. Minh. \u201cSome properties of Gaussian reproducing kernel Hilbert spaces and their implications for function approximation and learning theory\u201d. In: Constructive Approximation 32.2 (2010), pp. 307\u2013338.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minh%2C%20H.Q.%20Some%20properties%20of%20Gaussian%20reproducing%20kernel%20Hilbert%20spaces%20and%20their%20implications%20for%20function%20approximation%20and%20learning%20theory%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minh%2C%20H.Q.%20Some%20properties%20of%20Gaussian%20reproducing%20kernel%20Hilbert%20spaces%20and%20their%20implications%20for%20function%20approximation%20and%20learning%20theory%202010"
        },
        {
            "id": "24",
            "entry": "[24] B. Morris, M. J. Powell, and A. D. Ames. \u201cSufficient conditions for the Lipschitz continuity of QP-based multi-objective control of humanoid robots\u201d. In: Proc. CDC. 2013, pp. 2920\u20132926.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morris%2C%20B.%20Powell%2C%20M.J.%20Ames%2C%20A.D.%20Sufficient%20conditions%20for%20the%20Lipschitz%20continuity%20of%20QP-based%20multi-objective%20control%20of%20humanoid%20robots%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morris%2C%20B.%20Powell%2C%20M.J.%20Ames%2C%20A.D.%20Sufficient%20conditions%20for%20the%20Lipschitz%20continuity%20of%20QP-based%20multi-objective%20control%20of%20humanoid%20robots%202013"
        },
        {
            "id": "25",
            "entry": "[25] R. Munos and P. Bourgine. \u201cReinforcement learning for continuous stochastic control problems\u201d. In: Proc. NIPS. 1998, pp. 1029\u20131035.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R.%20Bourgine%2C%20P.%20Reinforcement%20learning%20for%20continuous%20stochastic%20control%20problems%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R.%20Bourgine%2C%20P.%20Reinforcement%20learning%20for%20continuous%20stochastic%20control%20problems%201998"
        },
        {
            "id": "26",
            "entry": "[26] Y. Nishiyama et al. \u201cHilbert space embeddings of POMDPs\u201d. In: arXiv preprint arXiv:1210.4887 (2012).",
            "arxiv_url": "https://arxiv.org/pdf/1210.4887"
        },
        {
            "id": "27",
            "entry": "[27] M. Ohnishi et al. \u201cBarrier-certified adaptive reinforcement learning with applications to brushbot navigation\u201d. In: arXiv preprint arXiv:1801.09627 (2018).",
            "arxiv_url": "https://arxiv.org/pdf/1801.09627"
        },
        {
            "id": "28",
            "entry": "[28] B. \u00d8ksendal. Stochastic differential equations. Springer, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C3%98ksendal%2C%20B.%20Stochastic%20differential%20equations%202003"
        },
        {
            "id": "29",
            "entry": "[29] D. Ormoneit and S . Sen. \u201cKernel-based reinforcement learning\u201d. In: Machine Learning 49.2 (2002), pp. 161\u2013178.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ormoneit%2C%20D.%20Sen%2C%20S%20.%20Kernel-based%20reinforcement%20learning%202002"
        },
        {
            "id": "30",
            "entry": "[30] Y. Pan and E. Theodorou. \u201cProbabilistic differential dynamic programming\u201d. In: Proc. NIPS. 2014, pp. 1907\u20131915.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Y.%20E.%20Theodorou.%20%E2%80%9CProbabilistic%20differential%20dynamic%20programming%E2%80%9D%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Y.%20E.%20Theodorou.%20%E2%80%9CProbabilistic%20differential%20dynamic%20programming%E2%80%9D%202014"
        },
        {
            "id": "31",
            "entry": "[31] A. Paszke et al. \u201cAutomatic differentiation in PyTorch\u201d. In: (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20A.%20Automatic%20differentiation%20in%20PyTorch%202017"
        },
        {
            "id": "32",
            "entry": "[32] C. E. Rasmussen and C. K. Williams. Gaussian processes for machine learning. Vol. 1. MIT press Cambridge, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.%20Gaussian%20processes%20for%20machine%20learning%202006"
        },
        {
            "id": "33",
            "entry": "[33] C. Richard, J. Bermudez, and P. Honeine. \u201cOnline prediction of time series data with kernels\u201d. In: IEEE Trans. Signal Process. 57.3 (2009), pp. 1058\u20131067.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richard%2C%20C.%20Bermudez%2C%20J.%20Honeine%2C%20P.%20Online%20prediction%20of%20time%20series%20data%20with%20kernels%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richard%2C%20C.%20Bermudez%2C%20J.%20Honeine%2C%20P.%20Online%20prediction%20of%20time%20series%20data%20with%20kernels%202009"
        },
        {
            "id": "Res_2001_a",
            "entry": "In: J. Mach. Learn. Res. 2 (2001), pp. 67\u201393.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=In%20J%20Mach%20Learn%20Res%202%202001%20pp%206793"
        },
        {
            "id": "36",
            "entry": "[36] W. Sun and J. A. Bagnell. \u201cOnline Bellman residual and temporal difference algorithms with predictive error guarantees\u201d. In: Proc. IJCAI. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20W.%20Bagnell%2C%20J.A.%20Online%20Bellman%20residual%20and%20temporal%20difference%20algorithms%20with%20predictive%20error%20guarantees%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20W.%20Bagnell%2C%20J.A.%20Online%20Bellman%20residual%20and%20temporal%20difference%20algorithms%20with%20predictive%20error%20guarantees%202016"
        },
        {
            "id": "37",
            "entry": "[37] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "38",
            "entry": "[38] Rfo.rSo.fSf-uptotolinc,yHle. aRr.nMingaewi,iathndlinCe.aSrzfeupnecstvio\u00e1nria. p\u201cAprocxoinmveartgioenn\u201dt.OIn(:nA)dtevmanpcoersailn-dNifefeurreanlcIenfaolrgmoraittihomn Processing Systems. 2009, pp. 1609\u20131616.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rfo.rSo.fSf-uptotolinc%2C%20yHle%20aRrnMingaewi%20iathndlinCeaSrzfeupnecstvio%C3%A1nria%20p%E2%80%9CAprocxoinmveartgioenn%E2%80%9Dt.%20OIn%28%3AnA%29dtevmanpcoersailn-dNifefeurreanlcIenfaolrgmoraittihomn%20Processing%20Systems%202009"
        },
        {
            "id": "39",
            "entry": "[39] M. Takizawa and M. Yukawa. \u201cAdaptive nonlinear estimation based on parallel projection along affine subspaces in reproducing kernel Hilbert space\u201d. In: IEEE Trans. Signal Processing 63.16 (2015), pp. 4257\u20134269.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takizawa%2C%20M.%20Yukawa%2C%20M.%20Adaptive%20nonlinear%20estimation%20based%20on%20parallel%20projection%20along%20affine%20subspaces%20in%20reproducing%20kernel%20Hilbert%20space%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takizawa%2C%20M.%20Yukawa%2C%20M.%20Adaptive%20nonlinear%20estimation%20based%20on%20parallel%20projection%20along%20affine%20subspaces%20in%20reproducing%20kernel%20Hilbert%20space%202015"
        },
        {
            "id": "40",
            "entry": "[40] Y. Tassa et al. \u201cDeepMind Control Suite\u201d. In: arXiv preprint arXiv:1801.00690 (2018).",
            "arxiv_url": "https://arxiv.org/pdf/1801.00690"
        },
        {
            "id": "41",
            "entry": "[41] G. Taylor and R. Parr. \u201cKernelized value function approximation for reinforcement learning\u201d. In: Proc. ICML. 2009, pp. 1017\u20131024.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20G.%20Parr%2C%20R.%20Kernelized%20value%20function%20approximation%20for%20reinforcement%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20G.%20Parr%2C%20R.%20Kernelized%20value%20function%20approximation%20for%20reinforcement%20learning%202009"
        },
        {
            "id": "42",
            "entry": "[42] E. Theodorou, J. Buchli, and S. Schaal. \u201cReinforcement learning of motor skills in high dimensions: A path integral approach\u201d. In: Proc. IEEE ICRA. 2010, pp. 2397\u20132403.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theodorou%2C%20E.%20Buchli%2C%20J.%20Schaal%2C%20S.%20Reinforcement%20learning%20of%20motor%20skills%20in%20high%20dimensions%3A%20A%20path%20integral%20approach%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theodorou%2C%20E.%20Buchli%2C%20J.%20Schaal%2C%20S.%20Reinforcement%20learning%20of%20motor%20skills%20in%20high%20dimensions%3A%20A%20path%20integral%20approach%202010"
        },
        {
            "id": "43",
            "entry": "[43] E. Theodorou, Y. Tassa, and E. Todorov. \u201cStochastic differential dynamic programming\u201d. In: Proc. IEEE ACC. 2010, pp. 1125\u20131132.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theodorou%2C%20E.%20Tassa%2C%20Y.%20Todorov%2C%20E.%20Stochastic%20differential%20dynamic%20programming%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theodorou%2C%20E.%20Tassa%2C%20Y.%20Todorov%2C%20E.%20Stochastic%20differential%20dynamic%20programming%202010"
        },
        {
            "id": "44",
            "entry": "[44] J. N. Tsitsiklis and B. Van R. \u201cAnalysis of temporal-diffference learning with function approximation\u201d. In: Proc. NIPS. 1997, pp. 1075\u20131081.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20J.N.%20R%2C%20B.Van%20Analysis%20of%20temporal-diffference%20learning%20with%20function%20approximation%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20J.N.%20R%2C%20B.Van%20Analysis%20of%20temporal-diffference%20learning%20with%20function%20approximation%201997"
        },
        {
            "id": "45",
            "entry": "[45] K. G. Vamvoudakis and F. L. Lewis. \u201cOnline actor-critic algorithm to solve the continuous-time infinite horizon optimal control problem\u201d. In: Automatica 46.5 (2010), pp. 878\u2013888.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vamvoudakis%2C%20K.G.%20Lewis%2C%20F.L.%20Online%20actor-critic%20algorithm%20to%20solve%20the%20continuous-time%20infinite%20horizon%20optimal%20control%20problem%202010"
        },
        {
            "id": "46",
            "entry": "[46] L. Wang, A. D. Ames, and M. Egerstedt. \u201cSafety barrier certificates for collisions-free multirobot systems\u201d. In: IEEE Trans. Robotics (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20L.%20Ames%2C%20A.D.%20Egerstedt%2C%20M.%20Safety%20barrier%20certificates%20for%20collisions-free%20multirobot%20systems%202017"
        },
        {
            "id": "47",
            "entry": "[47] L. Wang, E. A. Theodorou, and M. Egerstedt. \u201cSafe learning of quadrotor dynamics using barrier certificates\u201d. In: arXiv preprint arXiv:1710.05472 (2017).",
            "arxiv_url": "https://arxiv.org/pdf/1710.05472"
        },
        {
            "id": "48",
            "entry": "[48] P. Wieland and F. Allg\u00f6wer. \u201cConstructive safety using control barrier functions\u201d. In: Proc. IFAC 40.12 (2007), pp. 462\u2013467.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wieland%2C%20P.%20Allg%C3%B6wer%2C%20F.%20Constructive%20safety%20using%20control%20barrier%20functions%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wieland%2C%20P.%20Allg%C3%B6wer%2C%20F.%20Constructive%20safety%20using%20control%20barrier%20functions%202007"
        },
        {
            "id": "49",
            "entry": "[49] X. Xu, D. Hu, and X. Lu. \u201cKernel-based least squares policy iteration for reinforcement learning\u201d. In: IEEE Trans. Neural Networks 18.4 (2007), pp. 973\u2013992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20X.%20Hu%2C%20D.%20Lu%2C%20X.%20Kernel-based%20least%20squares%20policy%20iteration%20for%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20X.%20Hu%2C%20D.%20Lu%2C%20X.%20Kernel-based%20least%20squares%20policy%20iteration%20for%20reinforcement%20learning%202007"
        },
        {
            "id": "50",
            "entry": "[50] X. Xu et al. \u201cRobustness of control barrier functions for safety critical control\u201d. In: Proc. IFAC 48.27 (2015), pp. 54\u201361.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20X.%20Robustness%20of%20control%20barrier%20functions%20for%20safety%20critical%20control%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20X.%20Robustness%20of%20control%20barrier%20functions%20for%20safety%20critical%20control%202015"
        },
        {
            "id": "51",
            "entry": "[51] M. Yukawa. \u201cMultikernel Adaptive Filtering\u201d. In: IEEE Trans. Signal Processing 60.9 (2012), pp. 4672\u20134682.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yukawa%2C%20M.%20Multikernel%20Adaptive%20Filtering%202012"
        },
        {
            "id": "52",
            "entry": "[52] DX. Zhou. \u201cDerivative reproducing properties for kernel methods in learning theory\u201d. In: Journal of Computational and Applied Mathematics 220.1-2 (2008), pp. 456\u2013463.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20D.X.%20Derivative%20reproducing%20properties%20for%20kernel%20methods%20in%20learning%20theory%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20D.X.%20Derivative%20reproducing%20properties%20for%20kernel%20methods%20in%20learning%20theory%202008"
        },
        {
            "id": "53",
            "entry": "[53] K. Zhou, J. C. Doyle, K. Glover, et al. Robust and optimal control. Vol. 40. Prentice Hall, 1996. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20K.%20Doyle%2C%20J.C.%20Glover%2C%20K.%20Robust%20and%20optimal%20control%201996"
        }
    ]
}
