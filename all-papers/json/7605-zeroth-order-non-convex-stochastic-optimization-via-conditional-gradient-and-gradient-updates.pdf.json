{
    "filename": "7605-zeroth-order-non-convex-stochastic-optimization-via-conditional-gradient-and-gradient-updates.pdf",
    "metadata": {
        "title": "Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates",
        "author": "Krishnakumar Balasubramanian, Saeed Ghadimi",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7605-zeroth-order-non-convex-stochastic-optimization-via-conditional-gradient-and-gradient-updates.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we propose and analyze zeroth-order stochastic approximation algorithms for nonconvex and convex optimization. Specifically, we propose generalizations of the conditional gradient algorithm achieving rates similar to the standard stochastic gradient algorithm using only zeroth-order information. Furthermore, under a structural sparsity assumption, we first illustrate an implicit regularization phenomenon where the standard stochastic gradient algorithm with zeroth-order information adapts to the sparsity of the problem at hand by just varying the stepsize. Next, we propose a truncated stochastic gradient algorithm with zeroth-order information, whose rate depends only poly-logarithmically on the dimensionality."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Gradient Descent",
            "url": "https://en.wikipedia.org/wiki/Gradient_Descent"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "We study zeroth-order stochastic conditional gradient (ZSCG) algorithms in the low-dimensional setting for solving constrained stochastic optimization problems",
        "We propose and analyze algorithms for solving the following stochastic optimization problem",
        "Several methods are available for solving such stochastic optimization problems under access to different oracle information, for example, function queries, gradient queries, and higher-order oracles",
        "We assume that we only have access to noisy evaluation of f through a stochastic zeroth-order oracle described in detail in Assumption 1",
        "When the performance is measured by function values, we show that a simple thresholded zeroth-order Gradient Descent algorithm achieves a poly-logarithmic dependence on dimensionality",
        "Our contributions: To summarize the above discussion, in this paper we make the following contributions to the literature on zeroth-order stochastic optimization: (i) We first analyze a classical version of Conditional Gradient algorithm in the nonconvex setting, under access to zeroth-order information and provide results on the convergence rates in the low-dimensional setting; We propose and analyze a modified Conditional Gradient algorithm in the convex setting with zeroth-order information and show that it attains improved rates in the low-dimensional setting; we consider a zeroth-order stochastic gradient algorithm in the high-dimensional nonconvex setting and illustrate an implicit regularization phenomenon"
    ],
    "key_statements": [
        "We study zeroth-order stochastic conditional gradient (ZSCG) algorithms in the low-dimensional setting for solving constrained stochastic optimization problems",
        "We propose and analyze algorithms for solving the following stochastic optimization problem",
        "Several methods are available for solving such stochastic optimization problems under access to different oracle information, for example, function queries, gradient queries, and higher-order oracles",
        "We assume that we only have access to noisy evaluation of f through a stochastic zeroth-order oracle described in detail in Assumption 1",
        "We propose and analyze in Section 2 a classical version of Conditional Gradient algorithm with zeroth-order information and present convergence results",
        "When the performance is measured by the size of the gradient, we show in Section 3 that zeroth-order Gradient Descent algorithm, has poly-logarithmic dependence on the dimensionality thereby demonstrating an implicit regularization phenomenon in this setting",
        "When the performance is measured by function values, we show that a simple thresholded zeroth-order Gradient Descent algorithm achieves a poly-logarithmic dependence on dimensionality",
        "Our contributions: To summarize the above discussion, in this paper we make the following contributions to the literature on zeroth-order stochastic optimization: (i) We first analyze a classical version of Conditional Gradient algorithm in the nonconvex setting, under access to zeroth-order information and provide results on the convergence rates in the low-dimensional setting; We propose and analyze a modified Conditional Gradient algorithm in the convex setting with zeroth-order information and show that it attains improved rates in the low-dimensional setting; we consider a zeroth-order stochastic gradient algorithm in the high-dimensional nonconvex setting and illustrate an implicit regularization phenomenon"
    ],
    "summary": [
        "We study zeroth-order stochastic conditional gradient (ZSCG) algorithms in the low-dimensional setting for solving constrained stochastic optimization problems.",
        "Note that this algorithm differs from the classical CG method in estimating the gradient using zeroth-order information and in outputting a random solution from the generated trajectory.",
        ", N gk total number of calls to the zeroth-order stochastic oracle and linear subproblems required to be solved to find an \u270f-stationary point of problem (1.1) are, respectively, bounded by",
        "The total number of calls to the zeroth-order stochastic oracle and linear subproblems required to be solved to find and \u270f-optimal solution of problem (1.1) are, respectively, bounded by",
        "We study unconstrained variant of problem 1.1 i.e, X = Rd, under certain sparsity assumptions on the objective function f to facilitate zeroth-order optimization in high-dimensions.",
        "[<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] considered the convex case and proposed algorithms for high-dimensional zeroth-order stochastic optimization.",
        "We present zeroth-order stochastic gradient methods for solving problem (1.1) when f is nonconvex and convex, in Subsections 3.1 and 3.2 respectively.",
        "We consider the zeroth-order stochastic gradient method presented in [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] and provide a refined convergence analysis for it under the sparsity assumption 1, when f is nonconvex.",
        "Remark 3 Note that the above theorem establishes rate of convergence of Algorithm 4 which only poly-logarithmically depends on the problem dimension d, by just selecting the step-size appropriately, under additional assumption that the gradient is sparse.",
        "This significantly improves the linear dimensionality dependence of the rate of convergence of this algorithm as presented in [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] for general nonconvex smooth problems.",
        "Algorithm 5 Truncated Zeroth-Order Stochastic Gradient Method Given a positive integer s, replace updating step of Algorithm 4 with xk = Ps) , (3.4)",
        "This demonstrates an implicit regularization phenomenon exhibited by the zeroth-order stochastic gradient method in the high-dimensional setting when the performance is measured by the size of the gradient in the dual norm.",
        "Remark 5 While for convex case, similar to the nonconvex case, the complexity of Algorithm 5 depends poly-logarithmically on d, it only linearly depends on the choice of s, facilitating zeroth-order stochastic optimization in high-dimensions under sparsity assumptions.",
        "The performance of conditional gradient algorithm in the high-dimensional constrained optimization setting is not well-explored; the interaction between the geometry of the constraint set, sparsity structure and zeroth-order information is extremely interesting to explore."
    ],
    "headline": "We propose and analyze zeroth-order stochastic approximation algorithms for nonconvex and convex optimization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S\u00e9bastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4):231\u2013357, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015"
        },
        {
            "id": "2",
            "entry": "[2] S\u00e9bastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "3",
            "entry": "[3] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 15\u201326. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Pin-Yu%20Zhang%2C%20Huan%20Sharma%2C%20Yash%20Yi%2C%20Jinfeng%20Zoo%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%202017"
        },
        {
            "id": "4",
            "entry": "[4] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller. Structured evolution with compact architectures for scalable policy optimization. In Proceedings of the 35th International Conference on Machine Learning. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanski%2C%20Krzysztof%20Rowland%2C%20Mark%20Sindhwani%2C%20Vikas%20Turner%2C%20Richard%20Structured%20evolution%20with%20compact%20architectures%20for%20scalable%20policy%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanski%2C%20Krzysztof%20Rowland%2C%20Mark%20Sindhwani%2C%20Vikas%20Turner%2C%20Richard%20Structured%20evolution%20with%20compact%20architectures%20for%20scalable%20policy%20optimization%202018"
        },
        {
            "id": "5",
            "entry": "[5] Andrew Conn, Katya Scheinberg, and Luis Vicente. Introduction to derivative-free optimization, volume 8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conn%2C%20Andrew%20Scheinberg%2C%20Katya%20Vicente%2C%20Luis%20Introduction%20to%20derivative-free",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conn%2C%20Andrew%20Scheinberg%2C%20Katya%20Vicente%2C%20Luis%20Introduction%20to%20derivative-free"
        },
        {
            "id": "6",
            "entry": "[6] V. Demyanov and A. Rubinov. Approximate methods in optimization problems. American Elsevier Publishing Co, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Demyanov%2C%20V.%20Rubinov%2C%20A.%20Approximate%20methods%20in%20optimization%20problems%201970"
        },
        {
            "id": "7",
            "entry": "[7] John Duchi, Michael Jordan, Martin Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788\u20132806, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Jordan%2C%20Michael%20Wainwright%2C%20Martin%20Wibisono%2C%20Andre%20Optimal%20rates%20for%20zero-order%20convex%20optimization%3A%20The%20power%20of%20two%20function%20evaluations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Jordan%2C%20Michael%20Wainwright%2C%20Martin%20Wibisono%2C%20Andre%20Optimal%20rates%20for%20zero-order%20convex%20optimization%3A%20The%20power%20of%20two%20function%20evaluations%202015"
        },
        {
            "id": "8",
            "entry": "[8] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3:95\u2013110, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming%201956"
        },
        {
            "id": "9",
            "entry": "[9] S. Ghadimi and G. Lan. Stochastic firstand zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "10",
            "entry": "[10] Saeed Ghadimi. Conditional gradient type methods for composite nonlinear and stochastic optimization. Mathematical Programming, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Conditional%20gradient%20type%20methods%20for%20composite%20nonlinear%20and%20stochastic%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Conditional%20gradient%20type%20methods%20for%20composite%20nonlinear%20and%20stochastic%20optimization%202018"
        },
        {
            "id": "11",
            "entry": "[11] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Deep%20learning%2C%20volume%201%202016"
        },
        {
            "id": "12",
            "entry": "[12] Elad Hazan and Satyen Kale. Projection-free online learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1843\u20131850. Omnipress, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Projection-free%20online%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Kale%2C%20Satyen%20Projection-free%20online%20learning%202012"
        },
        {
            "id": "13",
            "entry": "[13] Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization. In International Conference on Machine Learning, pages 1263\u20131271, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Luo%2C%20Haipeng%20Variance-reduced%20and%20projection-free%20stochastic%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Luo%2C%20Haipeng%20Variance-reduced%20and%20projection-free%20stochastic%20optimization%202016"
        },
        {
            "id": "14",
            "entry": "[14] Donald Hearn. The gap function of a convex program. Operations Research Letters, 2, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hearn%2C%20Donald%20The%20gap%20function%20of%20a%20convex%20program%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hearn%2C%20Donald%20The%20gap%20function%20of%20a%20convex%20program%201982"
        },
        {
            "id": "15",
            "entry": "[15] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1), pages 427\u2013435, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaggi%2C%20Martin%20Revisiting%20frank-wolfe%3A%20Projection-free%20sparse%20convex%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaggi%2C%20Martin%20Revisiting%20frank-wolfe%3A%20Projection-free%20sparse%20convex%20optimization%202013"
        },
        {
            "id": "16",
            "entry": "[16] Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. Foundations and Trends R in Machine Learning, 10(3-4):142\u2013336, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Prateek%20Kar%2C%20Purushottam%20Non-convex%20optimization%20for%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Prateek%20Kar%2C%20Purushottam%20Non-convex%20optimization%20for%20machine%20learning%202017"
        },
        {
            "id": "17",
            "entry": "[17] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-dimensional m-estimation. In Advances in Neural Information Processing Systems, pages 685\u2013693, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Prateek%20Tewari%2C%20Ambuj%20Kar%2C%20Purushottam%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Prateek%20Tewari%2C%20Ambuj%20Kar%2C%20Purushottam%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014"
        },
        {
            "id": "18",
            "entry": "[18] Kevin Jamieson, Robert Nowak, and Ben Recht. Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems, pages 2672\u20132680, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jamieson%2C%20Kevin%20Nowak%2C%20Robert%20Recht%2C%20Ben%20Query%20complexity%20of%20derivative-free%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jamieson%2C%20Kevin%20Nowak%2C%20Robert%20Recht%2C%20Ben%20Query%20complexity%20of%20derivative-free%20optimization%202012"
        },
        {
            "id": "19",
            "entry": "[19] Guanghui Lan and Yi Zhou. Conditional gradient sliding for convex optimization. SIAM Journal on Optimization, 26(2):1379\u20131409, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Guanghui%20Zhou%2C%20Yi%20Conditional%20gradient%20sliding%20for%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Guanghui%20Zhou%2C%20Yi%20Conditional%20gradient%20sliding%20for%20convex%20optimization%202016"
        },
        {
            "id": "20",
            "entry": "[20] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive approach to reinforcement learning. In Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mania%2C%20Horia%20Guy%2C%20Aurelia%20Recht%2C%20Benjamin%20Simple%20random%20search%20provides%20a%20competitive%20approach%20to%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mania%2C%20Horia%20Guy%2C%20Aurelia%20Recht%2C%20Benjamin%20Simple%20random%20search%20provides%20a%20competitive%20approach%20to%20reinforcement%20learning%202018"
        },
        {
            "id": "21",
            "entry": "[21] Jonas Mockus. Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mockus%2C%20Jonas%20Bayesian%20approach%20to%20global%20optimization%3A%20theory%20and%20applications%2C%20volume%2037%202012"
        },
        {
            "id": "22",
            "entry": "[22] Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Conditional gradient method for stochastic submodular maximization: Closing the gap. In International Conference on Artificial Intelligence and Statistics, pages 1886\u20131895, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mokhtari%2C%20Aryan%20Hassani%2C%20Hamed%20Karbasi%2C%20Amin%20Conditional%20gradient%20method%20for%20stochastic%20submodular%20maximization%3A%20Closing%20the%20gap%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mokhtari%2C%20Aryan%20Hassani%2C%20Hamed%20Karbasi%2C%20Amin%20Conditional%20gradient%20method%20for%20stochastic%20submodular%20maximization%3A%20Closing%20the%20gap%202018"
        },
        {
            "id": "23",
            "entry": "[23] Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods: From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09554"
        },
        {
            "id": "24",
            "entry": "[24] A. S. Nemirovski and D. Yudin. Problem complexity and method efficiency in optimization. Wiley-Interscience Series in Discrete Mathematics. John Wiley, XV, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.S.%20Yudin%2C%20D.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization.%20Wiley-Interscience%20Series%20in%20Discrete%20Mathematics%201983"
        },
        {
            "id": "25",
            "entry": "[25] Y. E. Nesterov. Introductory Lectures on Convex Optimization: a basic course. Kluwer Academic Publishers, Massachusetts, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.E.%20Introductory%20Lectures%20on%20Convex%20Optimization%3A%20a%20basic%20course%202004"
        },
        {
            "id": "26",
            "entry": "[26] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "27",
            "entry": "[27] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527\u2013566, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Spokoiny%2C%20Vladimir%20Random%20gradient-free%20minimization%20of%20convex%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Spokoiny%2C%20Vladimir%20Random%20gradient-free%20minimization%20of%20convex%20functions%202017"
        },
        {
            "id": "28",
            "entry": "[28] Sashank Reddi, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander Smola. Stochastic Frank-Wolfe Methods for Nonconvex Optimization. 2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1244\u20131251, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sashank%20Reddi%20Suvrit%20Sra%20Barnab%C3%A1s%20P%C3%B3czos%20and%20Alexander%20Smola%20Stochastic%20FrankWolfe%20Methods%20for%20Nonconvex%20Optimization%202016%2054th%20Annual%20Allerton%20Conference%20on%20Communication%20Control%20and%20Computing%20Allerton%20pages%2012441251%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sashank%20Reddi%20Suvrit%20Sra%20Barnab%C3%A1s%20P%C3%B3czos%20and%20Alexander%20Smola%20Stochastic%20FrankWolfe%20Methods%20for%20Nonconvex%20Optimization%202016%2054th%20Annual%20Allerton%20Conference%20on%20Communication%20Control%20and%20Computing%20Allerton%20pages%2012441251%202016"
        },
        {
            "id": "29",
            "entry": "[29] Reuven Rubinstein and Dirk Kroese. Simulation and the Monte Carlo method, volume 10. John Wiley & Sons, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20Reuven%20Kroese%2C%20Dirk%20Simulation%20and%20the%20Monte%20Carlo%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20Reuven%20Kroese%2C%20Dirk%20Simulation%20and%20the%20Monte%20Carlo%202016"
        },
        {
            "id": "30",
            "entry": "[30] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "31",
            "entry": "[31] Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In Conference on Learning Theory, pages 3\u201324, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20On%20the%20complexity%20of%20bandit%20and%20derivative-free%20stochastic%20convex%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20On%20the%20complexity%20of%20bandit%20and%20derivative-free%20stochastic%20convex%20optimization%202013"
        },
        {
            "id": "32",
            "entry": "[32] Jasper Snoek, Hugo Larochelle, and Ryan Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "33",
            "entry": "[33] James Spall. Introduction to stochastic search and optimization: estimation, simulation, and control, volume 65. John Wiley & Sons, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spall%2C%20James%20Introduction%20to%20stochastic%20search%20and%20optimization%3A%20estimation%2C%20simulation%2C%20and%20control%2C%20volume%2065%202005"
        },
        {
            "id": "34",
            "entry": "[34] Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order optimization in high dimensions. Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yining%20Du%2C%20Simon%20Balakrishnan%2C%20Sivaraman%20Singh%2C%20Aarti%20Stochastic%20zeroth-order%20optimization%20in%20high%20dimensions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yining%20Du%2C%20Simon%20Balakrishnan%2C%20Sivaraman%20Singh%2C%20Aarti%20Stochastic%20zeroth-order%20optimization%20in%20high%20dimensions%202018"
        }
    ]
}
