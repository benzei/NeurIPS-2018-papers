{
    "filename": "7975-multi-agent-generative-adversarial-imitation-learning.pdf",
    "metadata": {
        "title": "Multi-Agent Generative Adversarial Imitation Learning",
        "author": "Jiaming Song, Hongyu Ren, Dorsa Sadigh, Stefano Ermon",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7975-multi-agent-generative-adversarial-imitation-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multiagent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents."
    },
    "keywords": [
        {
            "term": "temporal difference",
            "url": "https://en.wikipedia.org/wiki/temporal_difference"
        },
        {
            "term": "markov decision process",
            "url": "https://en.wikipedia.org/wiki/markov_decision_process"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "actor critic",
            "url": "https://en.wikipedia.org/wiki/actor_critic"
        },
        {
            "term": "Markov decision processes",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_processes"
        },
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "highlights": [
        "Reinforcement learning (RL) methods are becoming increasingly successful at optimizing reward signals in complex, high dimensional environments [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We propose a new framework for multi-agent imitation learning \u2013 provided with demonstrations of a set of experts interacting with each other in the same environment, we aim to learn multiple parametrized policies that imitate the behavior of each expert respectively",
        "We show that for a specific choice of \u03bb we can recover the difference of the sum of expected rewards between two policies, a performance gap similar to the one used in single agent Inverse Reinforcement Learning in Equation (4)",
        "The above theorem suggests that \u03c8-regularized multi-agent inverse reinforcement learning is seeking, for each agent i, a policy whose occupancy measure is close to one where we replace policy \u03c0i with expert \u03c0Ei , as measured by the convex function \u03c8i",
        "Inverse Reinforcement Learning (IRL) assumes the expert policy optimizes over some unknown reward, recovers the reward, and learns the policy through reinforcement learning (RL)",
        "In non-cooperative settings, [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] consider the case of Inverse Reinforcement Learning for two-player zero-sum games and cast the Inverse Reinforcement Learning problem as Bayesian inference, while [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>] assume agents are non-cooperative but the reward function is a linear combination of pre-specified features"
    ],
    "key_statements": [
        "Reinforcement learning (RL) methods are becoming increasingly successful at optimizing reward signals in complex, high dimensional environments [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We propose a new framework for multi-agent imitation learning \u2013 provided with demonstrations of a set of experts interacting with each other in the same environment, we aim to learn multiple parametrized policies that imitate the behavior of each expert respectively",
        "We show that for a specific choice of \u03bb we can recover the difference of the sum of expected rewards between two policies, a performance gap similar to the one used in single agent Inverse Reinforcement Learning in Equation (4)",
        "The above theorem suggests that \u03c8-regularized multi-agent inverse reinforcement learning is seeking, for each agent i, a policy whose occupancy measure is close to one where we replace policy \u03c0i with expert \u03c0Ei , as measured by the convex function \u03c8i",
        "We select \u03c8i to be our reward function regularizer in Proposition 2; this corresponds to the two-player game introduced in Generative Adversarial Imitation Learning (GAIL, [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>])",
        "Inverse Reinforcement Learning (IRL) assumes the expert policy optimizes over some unknown reward, recovers the reward, and learns the policy through reinforcement learning (RL)",
        "In non-cooperative settings, [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] consider the case of Inverse Reinforcement Learning for two-player zero-sum games and cast the Inverse Reinforcement Learning problem as Bayesian inference, while [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>] assume agents are non-cooperative but the reward function is a linear combination of pre-specified features"
    ],
    "summary": [
        "Reinforcement learning (RL) methods are becoming increasingly successful at optimizing reward signals in complex, high dimensional environments [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "Let us first consider imitation in Markov decision processes and the framework of single-agent Maximum Entropy IRL [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] where the goal is to recover a reward function r that rationalizes the expert behavior \u03c0E: IRL = arg max E\u03c0E [r(s, a)] \u2212",
        "[<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] interprets the imitation learning problem as matching two occupancy measures, i.e., the distribution over states and actions encountered when navigating the environment with a policy.",
        "Extending imitation learning to multi-agent settings is difficult because there are multiple rewards and the notion of optimality is complicated by the need to consider an equilibrium solution [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "We use MARL(r) to denote the set of policies that form a Nash equilibrium under r and have the maximum \u03b3-discounted causal entropy: MARL(r) = arg min fr(\u03c0, v) \u2212 H(\u03c0)",
        "The Nash equilibrium constraints imply that any agent i cannot improve \u03c0i via 1-step temporal difference learning; if the condition for Equation (3) is not satisfied for some vi, qi, and (s, ai), this would suggest that we can update the policy for agent i and its value function.",
        "We show that for a specific choice of \u03bb we can recover the difference of the sum of expected rewards between two policies, a performance gap similar to the one used in single agent IRL in Equation (4).",
        "The above theorem suggests that \u03c8-regularized multi-agent inverse reinforcement learning is seeking, for each agent i, a policy whose occupancy measure is close to one where we replace policy \u03c0i with expert \u03c0Ei , as measured by the convex function \u03c8i .",
        "Arg min \u03c0 i=1 \u03c8i has a unique solution, which is always true in the single agent case due to convexity of the space of the optimal policies.",
        "We select \u03c8i to be our reward function regularizer in Proposition 2; this corresponds to the two-player game introduced in Generative Adversarial Imitation Learning (GAIL, [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]).",
        "We use the expert policies to simulate trajectories D, and do imitation learning on D as demonstrations, where we assume the underlying rewards are unknown.",
        "In non-cooperative settings, [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] consider the case of IRL for two-player zero-sum games and cast the IRL problem as Bayesian inference, while [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>] assume agents are non-cooperative but the reward function is a linear combination of pre-specified features."
    ],
    "headline": "We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu, \u201cImpala: Scalable distributed deep-rl with importance weighted actor-learner architectures,\u201d arXiv preprint arXiv:1802.01561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "2",
            "entry": "[2] D. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan, \u201cInverse reward design,\u201d in Advances in Neural Information Processing Systems, pp. 6768\u20136777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadfield-Menell%2C%20D.%20Milli%2C%20S.%20Abbeel%2C%20P.%20Russell%2C%20S.J.%20Inverse%20reward%20design%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadfield-Menell%2C%20D.%20Milli%2C%20S.%20Abbeel%2C%20P.%20Russell%2C%20S.J.%20Inverse%20reward%20design%2C%202017"
        },
        {
            "id": "3",
            "entry": "[3] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man\u00e9, \u201cConcrete problems in ai safety,\u201d arXiv preprint arXiv:1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "4",
            "entry": "[4] D. Amodei and J. Clark, \u201cFaulty reward functions in the wild,\u201d 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amodei%2C%20D.%20Clark%2C%20J.%20Faulty%20reward%20functions%20in%20the%20wild%2C%202016"
        },
        {
            "id": "5",
            "entry": "[5] P. Peng, Q. Yuan, Y. Wen, Y. Yang, Z. Tang, H. Long, and J. Wang, \u201cMultiagent bidirectionallycoordinated nets for learning to play starcraft combat games,\u201d arXiv preprint arXiv:1703.10069, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10069"
        },
        {
            "id": "6",
            "entry": "[6] L. Matignon, L. Jeanpierre, A.-I. Mouaddib, et al., \u201cCoordinated multi-robot exploration under communication constraints using decentralized markov decision processes.,\u201d in AAAI, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matignon%2C%20L.%20Jeanpierre%2C%20L.%20Mouaddib%2C%20A.-I.%20%E2%80%9CCoordinated%20multi-robot%20exploration%20under%20communication%20constraints%20using%20decentralized%20markov%20decision%20processes.%2C%E2%80%9D%20in%20AAAI%202012"
        },
        {
            "id": "7",
            "entry": "[7] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel, \u201cMulti-agent reinforcement learning in sequential social dilemmas,\u201d in Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pp. 464\u2013473, International Foundation for Autonomous Agents and Multiagent Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leibo%2C%20J.Z.%20Zambaldi%2C%20V.%20Lanctot%2C%20M.%20Marecki%2C%20J.%20Multi-agent%20reinforcement%20learning%20in%20sequential%20social%20dilemmas%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leibo%2C%20J.Z.%20Zambaldi%2C%20V.%20Lanctot%2C%20M.%20Marecki%2C%20J.%20Multi-agent%20reinforcement%20learning%20in%20sequential%20social%20dilemmas%2C%202017"
        },
        {
            "id": "8",
            "entry": "[8] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, \u201cMaximum entropy inverse reinforcement learning.,\u201d in AAAI, vol. 8, pp. 1433\u20131438, Chicago, IL, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20%E2%80%9CMaximum%20entropy%20inverse%20reinforcement%20learning.%2C%E2%80%9D%20in%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20%E2%80%9CMaximum%20entropy%20inverse%20reinforcement%20learning.%2C%E2%80%9D%20in%202008"
        },
        {
            "id": "9",
            "entry": "[9] P. Englert and M. Toussaint, \u201cInverse kkt\u2013learning cost functions of manipulation tasks from demonstrations,\u201d in Proceedings of the International Symposium of Robotics Research, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Englert%2C%20P.%20Toussaint%2C%20M.%20Inverse%20kkt%E2%80%93learning%20cost%20functions%20of%20manipulation%20tasks%20from%20demonstrations%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Englert%2C%20P.%20Toussaint%2C%20M.%20Inverse%20kkt%E2%80%93learning%20cost%20functions%20of%20manipulation%20tasks%20from%20demonstrations%2C%202015"
        },
        {
            "id": "10",
            "entry": "[10] C. Finn, S. Levine, and P. Abbeel, \u201cGuided cost learning: Deep inverse optimal control via policy optimization,\u201d in International Conference on Machine Learning, pp. 49\u201358, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%2C%202016"
        },
        {
            "id": "11",
            "entry": "[11] B. Stadie, P. Abbeel, and I. Sutskever, \u201cThird person imitation learning,\u201d in ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stadie%2C%20B.%20Abbeel%2C%20P.%20Sutskever%2C%20I.%20%E2%80%9CThird%20person%20imitation%20learning%2C%E2%80%9D%20in%20ICLR%202017"
        },
        {
            "id": "12",
            "entry": "[12] A. Y. Ng, S. J. Russell, et al., \u201cAlgorithms for inverse reinforcement learning.,\u201d in Icml, pp. 663\u2013 670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20A.Y.%20Russell%2C%20S.J.%20%E2%80%9CAlgorithms%20for%20inverse%20reinforcement%20learning.%2C%E2%80%9D%20in%20Icml%202000"
        },
        {
            "id": "13",
            "entry": "[13] P. Abbeel and A. Y. Ng, \u201cApprenticeship learning via inverse reinforcement learning,\u201d in Proceedings of the twenty-first international conference on Machine learning, p. 1, ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%2C%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%2C%202004"
        },
        {
            "id": "14",
            "entry": "[14] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, \u201cMulti-agent actor-critic for mixed cooperative-competitive environments,\u201d arXiv preprint arXiv:1706.02275, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02275"
        },
        {
            "id": "15",
            "entry": "[15] J. Hu, M. P. Wellman, et al., \u201cMultiagent reinforcement learning: theoretical framework and an algorithm.,\u201d in ICML, vol. 98, pp. 242\u2013250, Citeseer, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20J.%20Wellman%2C%20M.P.%20Multiagent%20reinforcement%20learning%3A%20theoretical%20framework%20and%20an%20algorithm.%2C%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20J.%20Wellman%2C%20M.P.%20Multiagent%20reinforcement%20learning%3A%20theoretical%20framework%20and%20an%20algorithm.%2C%201998"
        },
        {
            "id": "16",
            "entry": "[16] J. Ho and S. Ermon, \u201cGenerative adversarial imitation learning,\u201d in Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%2C%202016"
        },
        {
            "id": "17",
            "entry": "[17] J. Foerster, Y. Assael, N. de Freitas, and S. Whiteson, \u201cLearning to communicate with deep multi-agent reinforcement learning,\u201d in Advances in Neural Information Processing Systems, pp. 2137\u20132145, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20J.%20Assael%2C%20Y.%20de%20Freitas%2C%20N.%20Whiteson%2C%20S.%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20J.%20Assael%2C%20Y.%20de%20Freitas%2C%20N.%20Whiteson%2C%20S.%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%2C%202016"
        },
        {
            "id": "18",
            "entry": "[18] M. L. Littman, \u201cMarkov games as a framework for multi-agent reinforcement learning,\u201d in Proceedings of the eleventh international conference on machine learning, vol. 157, pp. 157\u2013 163, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20M.L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%2C%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20M.L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%2C%201994"
        },
        {
            "id": "19",
            "entry": "[19] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction"
        },
        {
            "id": "1",
            "entry": "1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MIT%20press%20Cambridge%201998"
        },
        {
            "id": "20",
            "entry": "[20] M. Bloem and N. Bambos, \u201cInfinite time horizon maximum causal entropy inverse reinforcement learning,\u201d in Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on, pp. 4911\u2013 4916, IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bloem%2C%20M.%20Bambos%2C%20N.%20Infinite%20time%20horizon%20maximum%20causal%20entropy%20inverse%20reinforcement%20learning%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bloem%2C%20M.%20Bambos%2C%20N.%20Infinite%20time%20horizon%20maximum%20causal%20entropy%20inverse%20reinforcement%20learning%2C%202014"
        },
        {
            "id": "21",
            "entry": "[21] J. Filar and K. Vrieze, Competitive Markov decision processes. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Filar%2C%20J.%20Vrieze%2C%20K.%20Competitive%20Markov%20decision%20processes%202012"
        },
        {
            "id": "22",
            "entry": "[22] H. Prasad and S. Bhatnagar, \u201cA study of gradient descent schemes for general-sum stochastic games,\u201d arXiv preprint arXiv:1507.00093, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00093"
        },
        {
            "id": "23",
            "entry": "[23] S. Ross, G. J. Gordon, and D. Bagnell, \u201cA reduction of imitation learning and structured prediction to no-regret online learning.,\u201d in AISTATS, p. 6, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20S.%20Gordon%2C%20G.J.%20Bagnell%2C%20D.%20%E2%80%9CA%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning.%2C%E2%80%9D%20in%20AISTATS%202011"
        },
        {
            "id": "24",
            "entry": "[24] D. Hadfield-Menell, S. J. Russell, P. Abbeel, and A. Dragan, \u201cCooperative inverse reinforcement learning,\u201d in Advances in neural information processing systems, pp. 3909\u20133917, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadfield-Menell%2C%20D.%20Russell%2C%20S.J.%20Abbeel%2C%20P.%20Dragan%2C%20A.%20Cooperative%20inverse%20reinforcement%20learning%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadfield-Menell%2C%20D.%20Russell%2C%20S.J.%20Abbeel%2C%20P.%20Dragan%2C%20A.%20Cooperative%20inverse%20reinforcement%20learning%2C%202016"
        },
        {
            "id": "25",
            "entry": "[25] Y. Wu, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba, \u201cScalable trust-region method for deep reinforcement learning using kronecker-factored approximation,\u201d in Advances in neural information processing systems, pp. 5285\u20135294, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Mansimov%2C%20E.%20Grosse%2C%20R.B.%20Liao%2C%20S.%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Y.%20Mansimov%2C%20E.%20Grosse%2C%20R.B.%20Liao%2C%20S.%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%2C%202017"
        },
        {
            "id": "26",
            "entry": "[26] Y. Song, J. Song, and S. Ermon, \u201cAccelerating natural gradient with higher-order invariance,\u201d in International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Y.%20Song%2C%20J.%20Ermon%2C%20S.%20Accelerating%20natural%20gradient%20with%20higher-order%20invariance%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Y.%20Song%2C%20J.%20Ermon%2C%20S.%20Accelerating%20natural%20gradient%20with%20higher-order%20invariance%2C%202018"
        },
        {
            "id": "27",
            "entry": "[27] Y. Song, R. Shu, N. Kushman, and S. Ermon, \u201cConstructing unrestricted adversarial examples with generative models,\u201d arXiv preprint arXiv:1805.07894, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07894"
        },
        {
            "id": "28",
            "entry": "[28] S.-I. Amari, \u201cNatural gradient works efficiently in learning,\u201d Neural computation, vol. 10, no. 2, pp. 251\u2013276, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%2C%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%2C%201998"
        },
        {
            "id": "29",
            "entry": "[29] S. M. Kakade, \u201cA natural policy gradient,\u201d in Advances in neural information processing systems, pp. 1531\u20131538, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20S.M.%20%E2%80%9CA%20natural%20policy%20gradient%2C%E2%80%9D%20in%20Advances%20in%20neural%20information%20processing%20systems%202002"
        },
        {
            "id": "30",
            "entry": "[30] E. Jang, S. Gu, and B. Poole, \u201cCategorical reparameterization with gumbel-softmax,\u201d arXiv preprint arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "31",
            "entry": "[31] C. J. Maddison, A. Mnih, and Y. W. Teh, \u201cThe concrete distribution: A continuous relaxation of discrete random variables,\u201d arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "32",
            "entry": "[32] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "33",
            "entry": "[33] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu, \u201cOpenai baselines.\u201d https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "34",
            "entry": "[34] Y. Li, J. Song, and S. Ermon, \u201cInfogail: Interpretable imitation learning from visual demonstrations,\u201d arXiv preprint arXiv:1703.08840, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.08840"
        },
        {
            "id": "35",
            "entry": "[35] J. K. Gupta and M. Egorov, \u201cMulti-agent deep reinforcement learning environment.\u201d https://github.com/sisl/madrl, 2017.",
            "url": "https://github.com/sisl/madrl"
        },
        {
            "id": "36",
            "entry": "[36] J. A. Bagnell, \u201cAn invitation to imitation,\u201d tech. rep., CARNEGIE-MELLON UNIV PITTSBURGH PA ROBOTICS INST, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bagnell%2C%20J.A.%20An%20invitation%20to%20imitation%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bagnell%2C%20J.A.%20An%20invitation%20to%20imitation%2C%202015"
        },
        {
            "id": "37",
            "entry": "[37] D. A. Pomerleau, \u201cEfficient training of artificial neural networks for autonomous navigation,\u201d Neural Computation, vol. 3, no. 1, pp. 88\u201397, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pomerleau%2C%20D.A.%20Efficient%20training%20of%20artificial%20neural%20networks%20for%20autonomous%20navigation%2C%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pomerleau%2C%20D.A.%20Efficient%20training%20of%20artificial%20neural%20networks%20for%20autonomous%20navigation%2C%201991"
        },
        {
            "id": "38",
            "entry": "[38] S. Ross and D. Bagnell, \u201cEfficient reductions for imitation learning.,\u201d in AISTATS, pp. 3\u20135, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20S.%20Bagnell%2C%20D.%20%E2%80%9CEfficient%20reductions%20for%20imitation%20learning.%2C%E2%80%9D%20in%20AISTATS%202010"
        },
        {
            "id": "39",
            "entry": "[39] S. Barrett, A. Rosenfeld, S. Kraus, and P. Stone, \u201cMaking friends on the fly: Cooperating with new teammates,\u201d Artificial Intelligence, vol. 242, pp. 132\u2013171, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barrett%2C%20S.%20Rosenfeld%2C%20A.%20Kraus%2C%20S.%20Stone%2C%20P.%20Making%20friends%20on%20the%20fly%3A%20Cooperating%20with%20new%20teammates%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barrett%2C%20S.%20Rosenfeld%2C%20A.%20Kraus%2C%20S.%20Stone%2C%20P.%20Making%20friends%20on%20the%20fly%3A%20Cooperating%20with%20new%20teammates%2C%202017"
        },
        {
            "id": "40",
            "entry": "[40] H. M. Le, Y. Yue, and P. Carr, \u201cCoordinated multi-agent imitation learning,\u201d arXiv preprint arXiv:1703.03121, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03121"
        },
        {
            "id": "41",
            "entry": "[41] A. \u0160o\u0161ic, W. R. KhudaBukhsh, A. M. Zoubir, and H. Koeppl, \u201cInverse reinforcement learning in swarm systems,\u201d stat, vol. 1050, p. 17, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C5%A0o%C5%A1ic%2C%20A.%20KhudaBukhsh%2C%20W.R.%20Zoubir%2C%20A.M.%20Koeppl%2C%20H.%20Inverse%20reinforcement%20learning%20in%20swarm%20systems%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=%C5%A0o%C5%A1ic%2C%20A.%20KhudaBukhsh%2C%20W.R.%20Zoubir%2C%20A.M.%20Koeppl%2C%20H.%20Inverse%20reinforcement%20learning%20in%20swarm%20systems%2C%202016"
        },
        {
            "id": "42",
            "entry": "[42] K. Bogert and P. Doshi, \u201cMulti-robot inverse reinforcement learning under occlusion with interactions,\u201d in Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pp. 173\u2013180, International Foundation for Autonomous Agents and Multiagent Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%20Bogert%20and%20P%20Doshi%20Multirobot%20inverse%20reinforcement%20learning%20under%20occlusion%20with%20interactions%20in%20Proceedings%20of%20the%202014%20international%20conference%20on%20Autonomous%20agents%20and%20multiagent%20systems%20pp%20173180%20International%20Foundation%20for%20Autonomous%20Agents%20and%20Multiagent%20Systems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%20Bogert%20and%20P%20Doshi%20Multirobot%20inverse%20reinforcement%20learning%20under%20occlusion%20with%20interactions%20in%20Proceedings%20of%20the%202014%20international%20conference%20on%20Autonomous%20agents%20and%20multiagent%20systems%20pp%20173180%20International%20Foundation%20for%20Autonomous%20Agents%20and%20Multiagent%20Systems%202014"
        },
        {
            "id": "43",
            "entry": "[43] X. Lin, P. A. Beling, and R. Cogill, \u201cMulti-agent inverse reinforcement learning for zero-sum games,\u201d arXiv preprint arXiv:1403.6508, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1403.6508"
        },
        {
            "id": "44",
            "entry": "[44] T. S. Reddy, V. Gopikrishna, G. Zaruba, and M. Huber, \u201cInverse reinforcement learning for decentralized non-cooperative multiagent systems,\u201d in Systems, Man, and Cybernetics (SMC), 2012 IEEE International Conference on, pp. 1930\u20131935, IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddy%2C%20T.S.%20Gopikrishna%2C%20V.%20Zaruba%2C%20G.%20Huber%2C%20M.%20%E2%80%9CInverse%20reinforcement%20learning%20for%20decentralized%20non-cooperative%20multiagent%20systems%2C%E2%80%9D%20in%20Systems%2C%20Man%2C%20and%20Cybernetics%202012"
        },
        {
            "id": "45",
            "entry": "[45] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d in Advances in neural information processing systems, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014"
        },
        {
            "id": "46",
            "entry": "[46] J. Martens and R. Grosse, \u201cOptimizing neural networks with kronecker-factored approximate curvature,\u201d in International Conference on Machine Learning, pp. 2408\u20132417, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20neural%20networks%20with%20kronecker-factored%20approximate%20curvature%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20neural%20networks%20with%20kronecker-factored%20approximate%20curvature%2C%202015"
        },
        {
            "id": "47",
            "entry": "[47] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, \u201cHigh-dimensional continuous control using generalized advantage estimation,\u201d arXiv preprint arXiv:1506.02438, 2015. ",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        }
    ]
}
