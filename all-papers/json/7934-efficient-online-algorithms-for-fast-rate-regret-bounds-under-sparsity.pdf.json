{
    "filename": "7934-efficient-online-algorithms-for-fast-rate-regret-bounds-under-sparsity.pdf",
    "metadata": {
        "title": "Efficient online algorithms for fast-rate regret bounds under sparsity",
        "author": "Pierre Gaillard, Olivier Wintenberger",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7934-efficient-online-algorithms-for-fast-rate-regret-bounds-under-sparsity.pdf"
        },
        "abstract": "We consider the problem of online convex optimization in two different settings: arbitrary and i.i.d. sequence of convex loss functions. In both settings, we provide efficient algorithms whose cumulative excess risks are controlled with fast-rate sparse bounds. First, the excess risks bounds depend on the sparsity of the objective rather than on the dimension of the parameters space. Second, their rates are faster than the slow-rate 1/pT under additional convexity assumptions on the loss functions. In the adversarial setting, we develop an algorithm BOA+ whose cumulative excess risks is controlled by several bounds with different trade-offs between sparsity and rate for strongly convex loss functions. In the i.i.d. setting under the \u0141ojasiewicz\u2019s assumption, we establish new risk bounds that are sparse with a rate adaptive to the convexity of the risk (ranging from a rate 1/pT for general convex risk to 1/T for strongly convex risk). These results generalize previous works on sparse online learning under weak assumptions on the risk."
    },
    "keywords": [
        {
            "term": "online algorithm",
            "url": "https://en.wikipedia.org/wiki/online_algorithm"
        },
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        },
        {
            "term": "excess risk",
            "url": "https://en.wikipedia.org/wiki/excess_risk"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "We consider the following setting of online convex optimization where a sequence of random convex loss functions (`t : Rd ! R)t>1 is sequentially observed",
        "We show in Theorem 2.1 that BOA [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and Squint [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] achieve a fast rate quantile bound with high probability: i.e. E\u21e1[RT (\u2713k)] 6 O (K(\u21e1, \u21e1b0)/T )1/(2 )",
        "In Theorem 3.4 we show that our new efficient procedure SABOA achieves a fast rate upper-bound on the average excess risk of order O((k\u2713\u21e4k0 ln(d)/T )1/(2 )) when the optimal parameters have1-norm bounded by 1 < 1",
        "For prediction with K > 1 expert advice, [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] showed that a fast rate O/T can be obtained by the BOA algorithm under the LIST condition (i.e., Lipschitz and strongly convex loss functions)",
        "In Section 3.2 we introduce a pseudo-metric in order to bound the regret of grids consisting of the 2d corners and some arbitrary fixed points",
        "We say that a regret bound is accelerable if it provides a fast rate except a term depending on the distance with the grid) that decreases with T"
    ],
    "key_statements": [
        "We consider the following setting of online convex optimization where a sequence of random convex loss functions (`t : Rd ! R)t>1 is sequentially observed",
        "We show in Theorem 2.1 that BOA [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and Squint [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] achieve a fast rate quantile bound with high probability: i.e. E\u21e1[RT (\u2713k)] 6 O (K(\u21e1, \u21e1b0)/T )1/(2 )",
        "In Theorem 3.4 we show that our new efficient procedure SABOA achieves a fast rate upper-bound on the average excess risk of order O((k\u2713\u21e4k0 ln(d)/T )1/(2 )) when the optimal parameters have1-norm bounded by 1 < 1",
        "For prediction with K > 1 expert advice, [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] showed that a fast rate O/T can be obtained by the BOA algorithm under the LIST condition (i.e., Lipschitz and strongly convex loss functions)",
        "In Section 3.2 we introduce a pseudo-metric in order to bound the regret of grids consisting of the 2d corners and some arbitrary fixed points",
        "We say that a regret bound is accelerable if it provides a fast rate except a term depending on the distance with the grid) that decreases with T",
        "We provide an algorithm with fast-rate sparsity risk-bound on B1 by regularly restarting Algorithm 1 with an updated discretization grid \u21e50 approaching the set of minimizers \u21e5\u21e4 := arg min\u27132B1 E[`t(\u2713)]",
        "Conclusion In this paper, we show that BOA is an optimal online algorithm for aggregating experts under very weak conditions on the loss"
    ],
    "summary": [
        "We consider the following setting of online convex optimization where a sequence of random convex loss functions (`t : Rd ! R)t>1 is sequentially observed.",
        "We consider online averaging algorithms on adaptive finite discretization grids that achieve sparse oracle bounds on \u21e5 = B1 = {\u2713 2 Rd : k\u2713k1 6 1}.",
        "Fast rate sparse regret bounds involving k\u2713k0 were, up to our knowledge, only obtained through non-efficient procedures.",
        "Third contribution: sparse regret bound under \u0141ojasiewicz assumption (\u21e5 = B1, i.i.d. data) In Section 3.4 we turn to a stochastic setting where the loss functions1, .",
        "They provide a sparse regret bound of order O(k\u2713\u21e4k02 ln d/T ) where \u2713\u21e4 is the optimum in B1 when the loss functions are strongly convex.",
        "In Theorem 3.4 we show that our new efficient procedure SABOA achieves a fast rate upper-bound on the average excess risk of order O((k\u2713\u21e4k0 ln(d)/T )1/(2 )) when the optimal parameters have1-norm bounded by 1 < 1.",
        "-the first high-probability quantile bound achieving a fast rate in Theorem 2.1; - an accelerable bound on RT (\u2713) that is small whenever \u2713 is close to a prior grid \u21e50 (Thm. 3.2); - two efficient algorithms with sparse regret bounds in the adversarial setting with strongly convex loss functions (BOA+, Thm. 3.3) and in the i.i.d. setting (SABOA, Thm. 3.4).",
        "For prediction with K > 1 expert advice, [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] showed that a fast rate O/T can be obtained by the BOA algorithm under the LIST condition (i.e., Lipschitz and strongly convex loss functions).",
        "We say that a regret bound is accelerable if it provides a fast rate except a term depending on the distance with the grid) that decreases with T .",
        "For obtaining regret bounds on BOA+, the restricted condition (4) with = 1 should hold at any time t 1, which is unlikely in the regression setting.",
        "3.4 Fast-rate sparse excess risk bound in the i.i.d. setting",
        "We assume the loss functionst to be i.i.d. We provide an algorithm with fast-rate sparsity risk-bound on B1 by regularly restarting Algorithm 1 with an updated discretization grid \u21e50 approaching the set of minimizers \u21e5\u21e4 := arg min\u27132B1 E[`t(\u2713)].",
        "The \u0141ojasiewicz assumption In order to obtain sparse oracle inequalities we work under \u0141ojasiewicz\u2019s Assumption (A3) which is a relaxed version of strong convexity of the risk.",
        "Assumption (A3) is more restrictive because it is design dependent in the regression setting; The constant \u03bc corresponds to the smallest non-zero eigenvalue of the covariance matrix while \u21b5 = 1/G2 for the square loss functions.",
        "Our main condition (A3) is weaker and more realistic than the usual ones when seeking for sequential sparse rate bounds for any t 1"
    ],
    "headline": "We provide efficient algorithms whose cumulative excess risks are controlled with fast-rate sparse bounds",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agarwal, S. Negahban, and M. J. Wainwright. Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions. In Advances in Neural Information Processing Systems 25, pages 1538\u20131546. Curran Associates, Inc., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Negahban%2C%20S.%20Wainwright%2C%20M.J.%20Stochastic%20optimization%20and%20sparse%20statistical%20recovery%3A%20Optimal%20algorithms%20for%20high%20dimensions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Negahban%2C%20S.%20Wainwright%2C%20M.J.%20Stochastic%20optimization%20and%20sparse%20statistical%20recovery%3A%20Optimal%20algorithms%20for%20high%20dimensions%202012"
        },
        {
            "id": "2",
            "entry": "[2] J.-Y. Audibert. Progressive mixture rules are deviation suboptimal. In Advances in Neural Information Processing Systems, pages 41\u201348, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.-Y.%20Progressive%20mixture%20rules%20are%20deviation%20suboptimal%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.-Y.%20Progressive%20mixture%20rules%20are%20deviation%20suboptimal%202008"
        },
        {
            "id": "3",
            "entry": "[3] F. Bunea, A. Tsybakov, and M. Wegkamp. Sparsity oracle inequalities for the lasso. Electronic Journal of Statistics, 1:169\u2013194, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bunea%2C%20F.%20Tsybakov%2C%20A.%20Wegkamp%2C%20M.%20Sparsity%20oracle%20inequalities%20for%20the%20lasso%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bunea%2C%20F.%20Tsybakov%2C%20A.%20Wegkamp%2C%20M.%20Sparsity%20oracle%20inequalities%20for%20the%20lasso%202007"
        },
        {
            "id": "4",
            "entry": "[4] O. Catoni. Universal aggregation rules with exact bias bounds. preprint, 510, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Catoni%2C%20O.%20Universal%20aggregation%20rules%20with%20exact%20bias%20bounds%201999"
        },
        {
            "id": "5",
            "entry": "[5] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20N.%20Lugosi%2C%20G.%20Prediction%2C%20learning%2C%20and%20games%202006"
        },
        {
            "id": "6",
            "entry": "[6] N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66(2-3):321\u2013352, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20N.%20Mansour%2C%20Y.%20Stoltz%2C%20G.%20Improved%20second-order%20bounds%20for%20prediction%20with%20expert%20advice%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20N.%20Mansour%2C%20Y.%20Stoltz%2C%20G.%20Improved%20second-order%20bounds%20for%20prediction%20with%20expert%20advice%202007"
        },
        {
            "id": "7",
            "entry": "[7] J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In COLT, pages 14\u201326, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.C.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Tewari%2C%20A.%20Composite%20objective%20mirror%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.C.%20Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Tewari%2C%20A.%20Composite%20objective%20mirror%20descent%202010"
        },
        {
            "id": "8",
            "entry": "[8] D. J. Foster, S. Kale, and H. Karloff. Online sparse linear regression. In Conference on Learning Theory, pages 960\u2013970, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foster%2C%20D.J.%20Kale%2C%20S.%20Karloff%2C%20H.%20Online%20sparse%20linear%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foster%2C%20D.J.%20Kale%2C%20S.%20Karloff%2C%20H.%20Online%20sparse%20linear%20regression%202016"
        },
        {
            "id": "9",
            "entry": "[9] P. Gaillard and O. Wintenberger. Sparse Accelerated Exponential Weights. In 20th International Conference on Artificial Intelligence and Statistics (AISTATS), Apr. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gaillard%2C%20P.%20Wintenberger%2C%20O.%20Sparse%20Accelerated%20Exponential%20Weights%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gaillard%2C%20P.%20Wintenberger%2C%20O.%20Sparse%20Accelerated%20Exponential%20Weights%202017-04"
        },
        {
            "id": "10",
            "entry": "[10] S. Gerchinovitz. Prediction of individual sequences and prediction in the statistical framework: some links around sparse regression and aggregation techniques. PhD thesis, Universit\u00e9 Paris-Sud 11, Orsay, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerchinovitz%2C%20S.%20Prediction%20of%20individual%20sequences%20and%20prediction%20in%20the%20statistical%20framework%3A%20some%20links%20around%20sparse%20regression%20and%20aggregation%20techniques%202011"
        },
        {
            "id": "11",
            "entry": "[11] S. Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression. The Journal of Machine Learning Research, 14(1):729\u2013769, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerchinovitz%2C%20S.%20Sparsity%20regret%20bounds%20for%20individual%20sequences%20in%20online%20linear%20regression%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gerchinovitz%2C%20S.%20Sparsity%20regret%20bounds%20for%20individual%20sequences%20in%20online%20linear%20regression%202013"
        },
        {
            "id": "12",
            "entry": "[12] C. Giraud. Introduction to high-dimensional statistics. Chapman and Hall/CRC, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giraud%2C%20C.%20Introduction%20to%20high-dimensional%20statistics%202014"
        },
        {
            "id": "13",
            "entry": "[13] E. Hazan. Introduction to online convex optimization. Foundations and Trends R in Optimization, 2(3-4):157\u2013325, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Introduction%20to%20online%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Introduction%20to%20online%20convex%20optimization%202016"
        },
        {
            "id": "14",
            "entry": "[14] S. Kale, Z. Karnin, T. Liang, and D. P\u00e1l. Adaptive feature selection: Computationally efficient online sparse linear regression under rip. arXiv preprint arXiv:1706.04690, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04690"
        },
        {
            "id": "15",
            "entry": "[15] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1\u201363, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kivinen%2C%20J.%20Warmuth%2C%20M.K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kivinen%2C%20J.%20Warmuth%2C%20M.K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997"
        },
        {
            "id": "16",
            "entry": "[16] W. M. Koolen and T. Van Erven. Second-order quantile methods for experts and combinatorial games. In COLT, volume 40, pages 1155\u20131175, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koolen%2C%20W.M.%20Erven%2C%20T.Van%20Second-order%20quantile%20methods%20for%20experts%20and%20combinatorial%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koolen%2C%20W.M.%20Erven%2C%20T.Van%20Second-order%20quantile%20methods%20for%20experts%20and%20combinatorial%20games%202015"
        },
        {
            "id": "17",
            "entry": "[17] W. M. Koolen, P. Gr\u00fcnwald, and T. van Erven. Combining adversarial guarantees and stochastic fast rates in online learning. In Advances in Neural Information Processing Systems, pages 4457\u20134465, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koolen%2C%20W.M.%20Gr%C3%BCnwald%2C%20P.%20van%20Erven%2C%20T.%20Combining%20adversarial%20guarantees%20and%20stochastic%20fast%20rates%20in%20online%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koolen%2C%20W.M.%20Gr%C3%BCnwald%2C%20P.%20van%20Erven%2C%20T.%20Combining%20adversarial%20guarantees%20and%20stochastic%20fast%20rates%20in%20online%20learning%202016"
        },
        {
            "id": "18",
            "entry": "[18] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. Journal of Machine Learning Research, 10(Mar):777\u2013801, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20Li%2C%20L.%20Zhang%2C%20T.%20Sparse%20online%20learning%20via%20truncated%20gradient%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20J.%20Li%2C%20L.%20Zhang%2C%20T.%20Sparse%20online%20learning%20via%20truncated%20gradient%202009"
        },
        {
            "id": "19",
            "entry": "[19] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and computation, 108(2):212\u2013261, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littlestone%2C%20N.%20Warmuth%2C%20M.K.%20The%20weighted%20majority%20algorithm%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littlestone%2C%20N.%20Warmuth%2C%20M.K.%20The%20weighted%20majority%20algorithm%201994"
        },
        {
            "id": "20",
            "entry": "[20] N. A. Mehta. Fast rates with high probability in exp-concave statistical learning. In Artificial Intelligence and Statistics, pages 1085\u20131093, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mehta%2C%20N.A.%20Fast%20rates%20with%20high%20probability%20in%20exp-concave%20statistical%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mehta%2C%20N.A.%20Fast%20rates%20with%20high%20probability%20in%20exp-concave%20statistical%20learning%202017"
        },
        {
            "id": "21",
            "entry": "[21] P. Rigollet and A. Tsybakov. Exponential screening and optimal rates of sparse estimation. The Annals of Statistics, pages 731\u2013771, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20P.%20Tsybakov%2C%20A.%20Exponential%20screening%20and%20optimal%20rates%20of%20sparse%20estimation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rigollet%2C%20P.%20Tsybakov%2C%20A.%20Exponential%20screening%20and%20optimal%20rates%20of%20sparse%20estimation%202011"
        },
        {
            "id": "22",
            "entry": "[22] V. Roulet and A. d\u2019Aspremont. Sharpness, restart and acceleration. In Advances in Neural Information Processing Systems, pages 1119\u20131129, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=V%20Roulet%20and%20A%20dAspremont%20Sharpness%20restart%20and%20acceleration%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2011191129%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=V%20Roulet%20and%20A%20dAspremont%20Sharpness%20restart%20and%20acceleration%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2011191129%202017"
        },
        {
            "id": "23",
            "entry": "[23] J. Steinhardt, S. Wager, and P. Liang. The statistics of streaming sparse regression. arXiv preprint arXiv:1412.4182, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.4182"
        },
        {
            "id": "24",
            "entry": "[24] I. Steinwart and A. Christmann. Estimating conditional quantiles with the help of the pinball loss. Bernoulli, 17(1):211\u2013225, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20I.%20Christmann%2C%20A.%20Estimating%20conditional%20quantiles%20with%20the%20help%20of%20the%20pinball%20loss%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20I.%20Christmann%2C%20A.%20Estimating%20conditional%20quantiles%20with%20the%20help%20of%20the%20pinball%20loss%202011"
        },
        {
            "id": "25",
            "entry": "[25] T. Van Erven, P. D. Gr\u00fcnwald, N. A. Mehta, M. D. Reid, and R. C. Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16:1793\u20131861, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erven%2C%20T.Van%20Gr%C3%BCnwald%2C%20P.D.%20Mehta%2C%20N.A.%20Reid%2C%20M.D.%20Fast%20rates%20in%20statistical%20and%20online%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erven%2C%20T.Van%20Gr%C3%BCnwald%2C%20P.D.%20Mehta%2C%20N.A.%20Reid%2C%20M.D.%20Fast%20rates%20in%20statistical%20and%20online%20learning%202015"
        },
        {
            "id": "26",
            "entry": "[26] V. G. Vovk. Aggregating strategies. Proc. of Computational Learning Theory, 1990, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vovk%2C%20V.G.%20Aggregating%20strategies%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vovk%2C%20V.G.%20Aggregating%20strategies%201990"
        },
        {
            "id": "27",
            "entry": "[27] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using1-constrained quadratic programming (lasso). IEEE transactions on information theory, 55(5): 2183\u20132202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using1-constrained%20quadratic%20programming%20%28lasso%29%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20M.J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using1-constrained%20quadratic%20programming%20%28lasso%29%202009"
        },
        {
            "id": "28",
            "entry": "[28] O. Wintenberger. Optimal learning with bernstein online aggregation. Machine Learning, 106 (1):119\u2013141, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wintenberger%2C%20O.%20Optimal%20learning%20with%20bernstein%20online%20aggregation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wintenberger%2C%20O.%20Optimal%20learning%20with%20bernstein%20online%20aggregation%202017"
        },
        {
            "id": "29",
            "entry": "[29] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(Oct):2543\u20132596, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010"
        },
        {
            "id": "30",
            "entry": "[30] Y. Yang. Combining forecasting procedures: some theoretical results. Econometric Theory, 20 (01):176\u2013222, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Y.%20Combining%20forecasting%20procedures%3A%20some%20theoretical%20results.%20Econometric%20Theory%202004"
        },
        {
            "id": "31",
            "entry": "[31] Y. Zhang, M. J. Wainwright, and M. I. Jordan. Lower bounds on the performance of polynomialtime algorithms for sparse linear regression. In Conference on Learning Theory, pages 921\u2013948, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Lower%20bounds%20on%20the%20performance%20of%20polynomialtime%20algorithms%20for%20sparse%20linear%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Lower%20bounds%20on%20the%20performance%20of%20polynomialtime%20algorithms%20for%20sparse%20linear%20regression%202014"
        },
        {
            "id": "32",
            "entry": "[32] S. \u0141ojasiewicz. Une propri\u00e9t\u00e9 topologique des sous-ensembles analytiques r\u00e9els. Les \u00e9quations aux d\u00e9riv\u00e9es partielles, pages 87\u201389, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C5%81ojasiewicz%2C%20S.%20Une%20propri%C3%A9t%C3%A9%20topologique%20des%20sous-ensembles%20analytiques%20r%C3%A9els.%20Les%20%C3%A9quations%20aux%20d%C3%A9riv%C3%A9es%20partielles%201963"
        },
        {
            "id": "33",
            "entry": "[33] S. \u0141ojasiewicz. Sur la g\u00e9om\u00e9trie semi-et sous-analytique. Annales de l\u2019institut Fourier, 43(5): 1575\u20131595, 1993. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C5%81ojasiewicz%2C%20S.%20Sur%20la%20g%C3%A9om%C3%A9trie%20semi-et%20sous-analytique.%20Annales%20de%20l%E2%80%99institut%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=%C5%81ojasiewicz%2C%20S.%20Sur%20la%20g%C3%A9om%C3%A9trie%20semi-et%20sous-analytique.%20Annales%20de%20l%E2%80%99institut%201993"
        }
    ]
}
