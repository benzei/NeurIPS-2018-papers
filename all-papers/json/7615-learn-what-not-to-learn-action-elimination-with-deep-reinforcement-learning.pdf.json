{
    "filename": "7615-learn-what-not-to-learn-action-elimination-with-deep-reinforcement-learning.pdf",
    "metadata": {
        "title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
        "author": "Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, Shie Mannor",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7615-learn-what-not-to-learn-action-elimination-with-deep-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions."
    },
    "keywords": [
        {
            "term": "discrete action",
            "url": "https://en.wikipedia.org/wiki/discrete_action"
        },
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        },
        {
            "term": "Convolutional Neural Network",
            "url": "https://en.wikipedia.org/wiki/Convolutional_Neural_Network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "Generalized Linear Models",
            "url": "https://en.wikipedia.org/wiki/Generalized_Linear_Models"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        }
    ],
    "highlights": [
        "Learning control policies for sequential decision-making tasks where both the state space and the action space are vast is critical when applying Reinforcement Learning (RL) to real-world problems",
        "We propose a new approach for dealing with large action spaces that is based on action elimination; that is, restricting the available actions in each state to a subset of the most likely ones (Figure 1(b))",
        "We introduce a novel Deep RL approach with two Deep Neural Network, a DQN and an Action Elimination Network (AEN), both designed using a Convolutional Neural Network (CNN) that is suited to NLP tasks (<a class=\"ref-link\" id=\"cKim_2014_a\" href=\"#rKim_2014_a\">Kim, 2014</a>)",
        "Our simulations show that action elimination dramatically improves the results in large action spaces",
        "We proposed the Action-Elimination Deep Q-Network, a Deep RL approach for eliminating actions while performing Q-learning, for solving MDPs with large state and action spaces",
        "Another direction is to generate elimination signals in real-world domains. This can be done by designing a rule-based system for actions that should be eliminated, and training an Action Elimination Network to generalize these rules for states that were not included in these rules"
    ],
    "key_statements": [
        "Learning control policies for sequential decision-making tasks where both the state space and the action space are vast is critical when applying Reinforcement Learning (RL) to real-world problems",
        "We propose a new approach for dealing with large action spaces that is based on action elimination; that is, restricting the available actions in each state to a subset of the most likely ones (Figure 1(b))",
        "We introduce a novel Deep RL approach with two Deep Neural Network, a DQN and an Action Elimination Network (AEN), both designed using a Convolutional Neural Network (CNN) that is suited to NLP tasks (<a class=\"ref-link\" id=\"cKim_2014_a\" href=\"#rKim_2014_a\">Kim, 2014</a>)",
        "We show that under the framework of linear contextual bandits (<a class=\"ref-link\" id=\"cChu_et+al_2011_a\" href=\"#rChu_et+al_2011_a\">Chu et al, 2011</a>), probability concentration results (Abbasi-Yadkori, Pal, and Szepesvari, 2011) can be adapted to guarantee that action elimination is correct in high probability",
        "Function Approximation: It is well known that errors in the Q-function estimates may cause the learning algorithm to converge to a suboptimal policy, a phenomenon that becomes more noticeable in environments with large action spaces (<a class=\"ref-link\" id=\"cThrun_1993_a\" href=\"#rThrun_1993_a\">Thrun and Schwartz, 1993</a>)",
        "The contextual bandit learns a mapping from states) to the elimination signal e(s, a) that estimates which actions should be eliminated",
        "We focus on linear contextual bandits as they enjoy easier implementation and tighter confidence intervals in comparison to Generalized Linear Models",
        "We show how the Q-learning and contextual bandit algorithms can learn simultaneously, resulting in the convergence of both algorithms, i.e., finding an optimal policy and a minimal valid action space",
        "If the elimination is done based on the concentration bounds of the linear contextual bandits, we can ensure that Action Elimination Q-learning converges, as can be seen in Proposition 1 (See Appendix A for a full proof)",
        "If the action does not fit the state category, the elimination signal equals 1, and if the action and state belong to the same category, it equals 0",
        "Our simulations show that action elimination dramatically improves the results in large action spaces",
        "For A2, with T=100, (Figure 3b) the Action-Elimination Deep Q-Network agent learns considerably faster, implying that action elimination is more robust to hyperparameters settings when there are many actions",
        "We show the learning curves for both Action-Elimination Deep Q-Network and DQN agents",
        "We proposed the Action-Elimination Deep Q-Network, a Deep RL approach for eliminating actions while performing Q-learning, for solving MDPs with large state and action spaces",
        "We provided theoretical guarantees on the convergence of our approach using linear contextual bandits",
        "We aim to investigate other mechanisms for action elimination, e.g., eliminating actions that result from low Q-values (Even-Dar, Mannor, and Mansour, 2003)",
        "Another direction is to generate elimination signals in real-world domains. This can be done by designing a rule-based system for actions that should be eliminated, and training an Action Elimination Network to generalize these rules for states that were not included in these rules"
    ],
    "summary": [
        "Learning control policies for sequential decision-making tasks where both the state space and the action space are vast is critical when applying Reinforcement Learning (RL) to real-world problems.",
        "Using the last layer activations of the AEN, we design a linear contextual bandit model that eliminates irrelevant actions with high probability, balancing exploration/exploitation, and allowing the DQN to explore and learn Q-values only for valid actions.",
        "Action elimination allows the agent to overcome some of the main difficulties in large action spaces, namely: Function Approximation and Sample Complexity.",
        "Function Approximation: It is well known that errors in the Q-function estimates may cause the learning algorithm to converge to a suboptimal policy, a phenomenon that becomes more noticeable in environments with large action spaces (<a class=\"ref-link\" id=\"cThrun_1993_a\" href=\"#rThrun_1993_a\">Thrun and Schwartz, 1993</a>).",
        "The contextual bandit learns a mapping from states) to the elimination signal e(s, a) that estimates which actions should be eliminated.",
        "We show how the Q-learning and contextual bandit algorithms can learn simultaneously, resulting in the convergence of both algorithms, i.e., finding an optimal policy and a minimal valid action space.",
        "We allow the base Q-learning algorithm to be any algorithm that converges to Q\u2217 with probability 1 after observing each state-action infinitely often.",
        "If the elimination is done based on the concentration bounds of the linear contextual bandits, we can ensure that Action Elimination Q-learning converges, as can be seen in Proposition 1 (See Appendix A for a full proof).",
        "With a probability of at least 1 \u2212 \u03b4, action elimination Q-learning converges to the optimal Q-function for any valid state-action pairs.",
        "The contextual linear bandit model (E\u2212, V ) is used to eliminate actions via the ACT() and Targets() functions.",
        "Zork presents multiple challenges to the player, like building plans to achieve long-term goals; dealing with random events like troll attacks; remembering implicit clues as well as learning the interactions between objects in the game and specific actions.",
        "For A2, with T=100, (Figure 3b) the AE-DQN agent learns considerably faster, implying that action elimination is more robust to hyperparameters settings when there are many actions.",
        "We proposed the AE-DQN, a DRL approach for eliminating actions while performing Q-learning, for solving MDPs with large state and action spaces.",
        "We tested our approach on the text-based game Zork, showing that by eliminating actions the size of the action space is reduced, exploration is more effective, and learning is improved.",
        "This can be done by designing a rule-based system for actions that should be eliminated, and training an AEN to generalize these rules for states that were not included in these rules.",
        "Elimination signals may be provided implicitly, e.g., by human demonstrations of actions that should not be taken"
    ],
    "headline": "We propose the Action-Elimination Deep Q-Network  architecture that combines a Deep Reinforcement Learning algorithm with an Action Elimination Network  that eliminates sub-optimal actions",
    "reference_links": [
        {
            "id": "Abbasi-Yadkori_et+al_2011_a",
            "entry": "Abbasi-Yadkori, Y.; Pal, D.; and Szepesvari, C. 2011. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, 2312\u20132320.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Y.%20Pal%2C%20D.%20Szepesvari%2C%20C.%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Y.%20Pal%2C%20D.%20Szepesvari%2C%20C.%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011"
        },
        {
            "id": "Azizzadenesheli_et+al_2018_a",
            "entry": "Azizzadenesheli, K.; Brunskill, E.; and Anandkumar, A. 2018. Efficient exploration through bayesian deep q-networks. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azizzadenesheli%2C%20K.%20Brunskill%2C%20E.%20Anandkumar%2C%20A.%20Efficient%20exploration%20through%20bayesian%20deep%20q-networks.%20arXiv%202018"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M. 2013. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research 47:253\u2013279.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "Bertsekas_1995_a",
            "entry": "Bertsekas, D. P., and Tsitsiklis, J. N. 1995. Neuro-dynamic programming: an overview. In Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, 560\u2013564. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Tsitsiklis%2C%20J.N.%20Neuro-dynamic%20programming%3A%20an%20overview%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20D.P.%20Tsitsiklis%2C%20J.N.%20Neuro-dynamic%20programming%3A%20an%20overview%201995"
        },
        {
            "id": "Budzianowski_et+al_2017_a",
            "entry": "Budzianowski, P.; Ultes, S.; Su, P.-H.; Mrksic, N.; Wen, T.-H.; Casanueva, I.; Rojas-Barahona, L.; and Gasic, M. 2017. Sub-domain modelling for dialogue management with hierarchical reinforcement learning. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Budzianowski%2C%20P.%20Ultes%2C%20S.%20Su%2C%20P.-H.%20Mrksic%2C%20N.%20Sub-domain%20modelling%20for%20dialogue%20management%20with%20hierarchical%20reinforcement%20learning.%20arXiv%20p%202017"
        },
        {
            "id": "Chu_et+al_2011_a",
            "entry": "Chu, W.; Li, L.; Reyzin, L.; and Schapire, R. 2011. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chu%2C%20W.%20Li%2C%20L.%20Reyzin%2C%20L.%20Schapire%2C%20R.%20Contextual%20bandits%20with%20linear%20payoff%20functions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chu%2C%20W.%20Li%2C%20L.%20Reyzin%2C%20L.%20Schapire%2C%20R.%20Contextual%20bandits%20with%20linear%20payoff%20functions%202011"
        },
        {
            "id": "C_et+al_2018_a",
            "entry": "C\u00f4t\u00e9, M.-A.; K\u00e1d\u00e1r, \u00c1.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.; et al. 2018. Textworld: A learning environment for text-based games. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%C3%B4t%C3%A9%2C%20M.-A.%20K%C3%A1d%C3%A1r%2C%20%C3%81.%20Yuan%2C%20X.%20Kybartas%2C%20B.%20Textworld%3A%20A%20learning%20environment%20for%20text-based%20games.%20arXiv%202018"
        },
        {
            "id": "Dalal_et+al_2016_a",
            "entry": "Dalal, G.; Gilboa, E.; and Mannor, S. 2016. Hierarchical decision making in electricity grid management. In International Conference on Machine Learning, 2197\u20132206.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalal%2C%20G.%20Gilboa%2C%20E.%20Mannor%2C%20S.%20Hierarchical%20decision%20making%20in%20electricity%20grid%20management%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalal%2C%20G.%20Gilboa%2C%20E.%20Mannor%2C%20S.%20Hierarchical%20decision%20making%20in%20electricity%20grid%20management%202016"
        },
        {
            "id": "Dhingra_et+al_2016_a",
            "entry": "Dhingra, B.; Li, L.; Li, X.; Gao, J.; Chen, Y.-N.; Ahmed, F.; and Deng, L. 2016. End-to-end reinforcement learning of dialogue agents for information access. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhingra%2C%20B.%20Li%2C%20L.%20Li%2C%20X.%20Gao%2C%20J.%20End-to-end%20reinforcement%20learning%20of%20dialogue%20agents%20for%20information%20access%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhingra%2C%20B.%20Li%2C%20L.%20Li%2C%20X.%20Gao%2C%20J.%20End-to-end%20reinforcement%20learning%20of%20dialogue%20agents%20for%20information%20access%202016"
        },
        {
            "id": "Dulac-Arnold_et+al_2012_a",
            "entry": "Dulac-Arnold, G.; Denoyer, L.; Preux, P.; and Gallinari, P. 2012. Fast reinforcement learning with large action sets using error-correcting output codes for mdp factorization. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 180\u2013194. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dulac-Arnold%2C%20G.%20Denoyer%2C%20L.%20Preux%2C%20P.%20Gallinari%2C%20P.%20Fast%20reinforcement%20learning%20with%20large%20action%20sets%20using%20error-correcting%20output%20codes%20for%20mdp%20factorization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dulac-Arnold%2C%20G.%20Denoyer%2C%20L.%20Preux%2C%20P.%20Gallinari%2C%20P.%20Fast%20reinforcement%20learning%20with%20large%20action%20sets%20using%20error-correcting%20output%20codes%20for%20mdp%20factorization%202012"
        },
        {
            "id": "Dulac-Arnold_et+al_2015_a",
            "entry": "Dulac-Arnold, G.; Evans, R.; van Hasselt, H.; Sunehag, P.; Lillicrap, T.; Hunt, J.; Mann, T.; Weber, T.; Degris, T.; and Coppin, B. 2015. Deep reinforcement learning in large discrete action spaces. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dulac-Arnold%2C%20G.%20Evans%2C%20R.%20van%20Hasselt%2C%20H.%20Sunehag%2C%20P.%20Deep%20reinforcement%20learning%20in%20large%20discrete%20action%20spaces.%20arXiv%202015"
        },
        {
            "id": "Even-Dar_et+al_2003_a",
            "entry": "Even-Dar, E.; Mannor, S.; and Mansour, Y. 2003. Action elimination and stopping conditions for reinforcement learning. In Proceedings of the 20th International Conference on Machine Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20E.%20Mannor%2C%20S.%20Mansour%2C%20Y.%20Action%20elimination%20and%20stopping%20conditions%20for%20reinforcement%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20E.%20Mannor%2C%20S.%20Mansour%2C%20Y.%20Action%20elimination%20and%20stopping%20conditions%20for%20reinforcement%20learning%202003"
        },
        {
            "id": "Fulda_et+al_2017_a",
            "entry": "Fulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What can you do with a rock? affordance extraction via word embeddings. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fulda%2C%20N.%20Ricks%2C%20D.%20Murdoch%2C%20B.%20Wingate%2C%20D.%20What%20can%20you%20do%20with%20a%20rock%3F%20affordance%20extraction%20via%20word%20embeddings.%20arXiv%202017"
        },
        {
            "id": "Glavic_et+al_2017_a",
            "entry": "Glavic, M.; Fonteneau, R.; and Ernst, D. 2017. Reinforcement learning for electric power system decision and control: Past considerations and perspectives. IFAC-PapersOnLine 6918\u20136927.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glavic%2C%20M.%20Fonteneau%2C%20R.%20Ernst%2C%20D.%20Reinforcement%20learning%20for%20electric%20power%20system%20decision%20and%20control%3A%20Past%20considerations%20and%20perspectives.%20IFAC-PapersOnLine%206918%E2%80%936927%202017"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Ostendorf, M. 2015. Deep reinforcement learning with an unbounded action space. CoRR abs/1511.04636.",
            "arxiv_url": "https://arxiv.org/pdf/1511.04636"
        },
        {
            "id": "Hester_et+al_2018_a",
            "entry": "Hester, T.; Vecerik, M.; Pietquin, O.; Lanctot, M.; Schaul, T.; Piot, B.; Sendonaris, A.; Dulac-Arnold, G.; Osband, I.; Agapiou, J.; et al. 2018. Learning from demonstrations for real world reinforcement learning. AAAI.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hester%2C%20T.%20Vecerik%2C%20M.%20Pietquin%2C%20O.%20Lanctot%2C%20M.%20Learning%20from%20demonstrations%20for%20real%20world%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hester%2C%20T.%20Vecerik%2C%20M.%20Pietquin%2C%20O.%20Lanctot%2C%20M.%20Learning%20from%20demonstrations%20for%20real%20world%20reinforcement%20learning%202018"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural computation 9(8):1735\u2013 1780.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Jaderberg_et+al_2016_a",
            "entry": "Jaderberg, M.; Mnih, V.; Czarnecki, W. M.; Schaul, T.; Leibo, J. Z.; Silver, D.; and Kavukcuoglu, K. 2016. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05397"
        },
        {
            "id": "Kakade_2003_a",
            "entry": "Kakade, S. M., et al. 2003. On the sample complexity of reinforcement learning. Ph.D. Dissertation, University of London, England.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20S.M.%20On%20the%20sample%20complexity%20of%20reinforcement%20learning%202003"
        },
        {
            "id": "Kim_2014_a",
            "entry": "Kim, Y. 2014. Convolutional neural networks for sentence classification. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.%20Convolutional%20neural%20networks%20for%20sentence%20classification.%20arXiv%20p%202014"
        },
        {
            "id": "Kostka_et+al_2017_a",
            "entry": "Kostka, B.; Kwiecieli, J.; Kowalski, J.; and Rychlikowski, P. 2017. Text-based adventures of the golovin ai agent. In Computational Intelligence and Games (CIG), 2017 IEEE Conference on. IEEE.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kostka%2C%20B.%20Kwiecieli%2C%20J.%20Kowalski%2C%20J.%20Rychlikowski%2C%20P.%20Text-based%20adventures%20of%20the%20golovin%20ai%20agent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kostka%2C%20B.%20Kwiecieli%2C%20J.%20Kowalski%2C%20J.%20Rychlikowski%2C%20P.%20Text-based%20adventures%20of%20the%20golovin%20ai%20agent%202017"
        },
        {
            "id": "Lagoudakis_2003_a",
            "entry": "Lagoudakis, M. G., and Parr, R. 2003. Reinforcement learning as classification: Leveraging modern classifiers. In Proceedings of the 20th International Conference on Machine Learning (ICML-03).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lagoudakis%2C%20M.G.%20Parr%2C%20R.%20Reinforcement%20learning%20as%20classification%3A%20Leveraging%20modern%20classifiers%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lagoudakis%2C%20M.G.%20Parr%2C%20R.%20Reinforcement%20learning%20as%20classification%3A%20Leveraging%20modern%20classifiers%202003"
        },
        {
            "id": "Lattimore_2012_a",
            "entry": "Lattimore, T., and Hutter, M. 2012. Pac bounds for discounted mdps. In International Conference on Algorithmic Learning Theory.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lattimore%2C%20T.%20Hutter%2C%20M.%20Pac%20bounds%20for%20discounted%20mdps%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lattimore%2C%20T.%20Hutter%2C%20M.%20Pac%20bounds%20for%20discounted%20mdps%202012"
        },
        {
            "id": "Levine_et+al_2017_a",
            "entry": "Levine, N.; Zahavy, T.; Mankowitz, D. J.; Tamar, A.; and Mannor, S. 2017. Shallow updates for deep reinforcement learning. In Advances in Neural Information Processing Systems, 3138\u20133148.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20N.%20Zahavy%2C%20T.%20Mankowitz%2C%20D.J.%20Tamar%2C%20A.%20Shallow%20updates%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20N.%20Zahavy%2C%20T.%20Mankowitz%2C%20D.J.%20Tamar%2C%20A.%20Shallow%20updates%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Li, J.; Monroe, W.; Ritter, A.; Galley, M.; Gao, J.; and Jurafsky, D. 2016. Deep reinforcement learning for dialogue generation. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Monroe%2C%20W.%20Ritter%2C%20A.%20Galley%2C%20M.%20Deep%20reinforcement%20learning%20for%20dialogue%20generation.%20arXiv%202016"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Li, X.; Chen, Y.-N.; Li, L.; and Gao, J. 2017. End-to-end task-completion neural dialogue systems. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20X.%20Chen%2C%20Y.-N.%20Li%2C%20L.%20Gao%2C%20J.%20End-to-end%20task-completion%20neural%20dialogue%20systems.%20arXiv%20p%202017"
        },
        {
            "id": "Lin_1992_a",
            "entry": "Lin, L.-J. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning 8(3-4).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20L.-J.%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20L.-J.%20Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992"
        },
        {
            "id": "Lipton_et+al_2016_a",
            "entry": "Lipton, Z. C.; Gao, J.; Li, L.; Chen, J.; and Deng, L. 2016a. Combating reinforcement learning\u2019s sisyphean curse with intrinsic fear. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lipton%2C%20Z.C.%20Gao%2C%20J.%20Li%2C%20L.%20Chen%2C%20J.%20Combating%20reinforcement%20learning%E2%80%99s%20sisyphean%20curse%20with%20intrinsic%20fear.%20arXiv%202016"
        },
        {
            "id": "Lipton_et+al_2016_b",
            "entry": "Lipton, Z. C.; Gao, J.; Li, L.; Li, X.; Ahmed, F.; and Deng, L. 2016b. Efficient exploration for dialogue policy learning with bbq networks and replay buffer spiking. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lipton%2C%20Z.C.%20Gao%2C%20J.%20Li%2C%20L.%20Li%2C%20X.%20Efficient%20exploration%20for%20dialogue%20policy%20learning%20with%20bbq%20networks%20and%20replay%20buffer%20spiking.%20arXiv%202016"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Liu, B.; Tur, G.; Hakkani-Tur, D.; Shah, P.; and Heck, L. 2017. End-to-end optimization of task-oriented dialogue model with deep reinforcement learning. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20B.%20Tur%2C%20G.%20Hakkani-Tur%2C%20D.%20Shah%2C%20P.%20End-to-end%20optimization%20of%20task-oriented%20dialogue%20model%20with%20deep%20reinforcement%20learning.%20arXiv%20p%202017"
        },
        {
            "id": "Machado_et+al_2017_a",
            "entry": "Machado, M. C.; Bellemare, M. G.; Talvitie, E.; Veness, J.; Hausknecht, M.; and Bowling, M. 2017. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Machado%2C%20M.C.%20Bellemare%2C%20M.G.%20Talvitie%2C%20E.%20Veness%2C%20J.%20Revisiting%20the%20arcade%20learning%20environment%3A%20Evaluation%20protocols%20and%20open%20problems%20for%20general%20agents.%20arXiv%202017"
        },
        {
            "id": "Mannion_et+al_2016_a",
            "entry": "Mannion, P.; Duggan, J.; and Howley, E. 2016. An experimental review of reinforcement learning algorithms for adaptive traffic signal control. In Autonomic Road Transport Support Systems. Springer. 47\u201366.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mannion%2C%20P.%20Duggan%2C%20J.%20Howley%2C%20E.%20An%20experimental%20review%20of%20reinforcement%20learning%20algorithms%20for%20adaptive%20traffic%20signal%20control.%20In%20Autonomic%20Road%20Transport%20Support%20Systems%202016"
        },
        {
            "id": "Mikolov_et+al_2013_a",
            "entry": "Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Sutskever%2C%20I.%20Chen%2C%20K.%20Corrado%2C%20G.S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality.%20In%20Advances%20in%20neural%20information%20processing%20systems%202013"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. Nature 518(7540):529\u2013533.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Narasimhan_et+al_2015_a",
            "entry": "Narasimhan, K.; Kulkarni, T. D.; and Barzilay, R. 2015. Language understanding for text-based games using deep reinforcement learning. CoRR abs/1506.08941.",
            "arxiv_url": "https://arxiv.org/pdf/1506.08941"
        },
        {
            "id": "Pazis_2011_a",
            "entry": "Pazis, J., and Parr, R. 2011. Generalized value functions for large action sets. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1185\u20131192.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pazis%2C%20J.%20Parr%2C%20R.%20Generalized%20value%20functions%20for%20large%20action%20sets%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pazis%2C%20J.%20Parr%2C%20R.%20Generalized%20value%20functions%20for%20large%20action%20sets%202011"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Peng, B.; Li, X.; Li, L.; Gao, J.; Celikyilmaz, A.; Lee, S.; and Wong, K.-F. 2017. Composite task-completion dialogue system via hierarchical deep reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20B.%20Li%2C%20X.%20Li%2C%20L.%20Gao%2C%20J.%20Composite%20task-completion%20dialogue%20system%20via%20hierarchical%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20B.%20Li%2C%20X.%20Li%2C%20L.%20Gao%2C%20J.%20Composite%20task-completion%20dialogue%20system%20via%20hierarchical%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Riquelme_et+al_2018_a",
            "entry": "Riquelme, C.; Tucker, G.; and Snoek, J. 2018. Deep bayesian bandits showdown. International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Riquelme%2C%20C.%20Tucker%2C%20G.%20Snoek%2C%20J.%20Deep%20bayesian%20bandits%20showdown.%20International%20Conference%20on%20Learning%20Representations%202018"
        },
        {
            "id": "Serban_et+al_2017_a",
            "entry": "Serban, I. V.; Sankar, C.; Germain, M.; Zhang, S.; Lin, Z.; Subramanian, S.; Kim, T.; Pieper, M.; Chandar, S.; Ke, N. R.; et al. 2017. A deep reinforcement learning chatbot. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Serban%2C%20I.V.%20Sankar%2C%20C.%20Germain%2C%20M.%20Zhang%2C%20S.%20A%20deep%20reinforcement%20learning%20chatbot.%20arXiv%20p%202017"
        },
        {
            "id": "Su_et+al_2016_a",
            "entry": "Su, P.-H.; Gasic, M.; Mrksic, N.; Rojas-Barahona, L.; Ultes, S.; Vandyke, D.; Wen, T.-H.; and Young, S. 2016. Continuously learning neural dialogue management. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20P.-H.%20Gasic%2C%20M.%20Mrksic%2C%20N.%20Rojas-Barahona%2C%20L.%20Continuously%20learning%20neural%20dialogue%20management.%20arXiv%20p%202016"
        },
        {
            "id": "Sutton_1998_a",
            "entry": "Sutton, R. S., and Barto, A. G. 1998. Reinforcement learning: An introduction. MIT press Cambridge.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "Tesauro_1995_a",
            "entry": "Tesauro, G. 1995. Temporal difference learning and TD-Gammon. Communications of the ACM",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tesauro%2C%20G.%20Temporal%20difference%20learning%20and%20TD-Gammon%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tesauro%2C%20G.%20Temporal%20difference%20learning%20and%20TD-Gammon%201995"
        },
        {
            "id": "Thrun_1993_a",
            "entry": "Thrun, S., and Schwartz, A. 1993. Issues in using function approximation for reinforcement learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20S.%20Schwartz%2C%20A.%20Issues%20in%20using%20function%20approximation%20for%20reinforcement%20learning%201993"
        },
        {
            "id": "Connectionist_1993_a",
            "entry": "In Proceedings of the 1993 Connectionist Models. Van der Pol, E., and Oliehoek, F. A. 2016. Coordinated deep reinforcement learners for traffic light control. In In proceedings of NIPS. Van Hasselt, H., and Wiering, M. A. 2009. Using continuous action spaces to solve discrete problems.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Connectionist%20Models.%20Van%20der%20Pol%2C%20E.%20Oliehoek%2C%20F.A.%20Coordinated%20deep%20reinforcement%20learners%20for%20traffic%20light%20control%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Connectionist%20Models.%20Van%20der%20Pol%2C%20E.%20Oliehoek%2C%20F.A.%20Coordinated%20deep%20reinforcement%20learners%20for%20traffic%20light%20control%201993"
        },
        {
            "id": "On_2009_a",
            "entry": "In Neural Networks, 2009. IJCNN 2009. International Joint Conference on, 1149\u20131156. IEEE. Watkins, C. J., and Dayan, P. 1992. Q-learning. Machine learning 8(3-4):279\u2013292.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=on%2C%201149%E2%80%931156.%20IEEE.%20Watkins%2C%20C.%20J.%20Dayan%2C%20P%20Q-learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=on%2C%201149%E2%80%931156.%20IEEE.%20Watkins%2C%20C.%20J.%20Dayan%2C%20P%20Q-learning%202009"
        },
        {
            "id": "Wen_et+al_2015_a",
            "entry": "Wen, Z.; O\u2019Neill, D.; and Maei, H. 2015. Optimal demand response using device-based reinforcement learning. IEEE Transactions on Smart Grid 2312\u20132324.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Z.%20O%E2%80%99Neill%2C%20D.%20Maei%2C%20H.%20Optimal%20demand%20response%20using%20device-based%20reinforcement%20learning.%20IEEE%20Transactions%20on%20Smart%20Grid%202312%E2%80%932324%202015"
        },
        {
            "id": "Wu_et+al_2016_a",
            "entry": "Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Schuster%2C%20M.%20Chen%2C%20Z.%20Le%2C%20Q.V.%20Google%E2%80%99s%20neural%20machine%20translation%20system%3A%20Bridging%20the%20gap%20between%20human%20and%20machine%20translation.%20arXiv%20p%202016"
        },
        {
            "id": "Yuan_et+al_2018_a",
            "entry": "Yuan, X.; C\u00f4t\u00e9, M.-A.; Sordoni, A.; Laroche, R.; Combes, R. T. d.; Hausknecht, M.; and Trischler, A. 2018. Counting to explore and generalize in text-based games. arXiv preprint arXiv:1806.11525.",
            "arxiv_url": "https://arxiv.org/pdf/1806.11525"
        },
        {
            "id": "Zahavy_et+al_2016_a",
            "entry": "Zahavy, T.; Ben-Zrihem, N.; and Mannor, S. 2016. Graying the black box: Understanding dqns. In International Conference on Machine Learning, 1899\u20131908.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zahavy%2C%20T.%20Ben-Zrihem%2C%20N.%20Mannor%2C%20S.%20Graying%20the%20black%20box%3A%20Understanding%20dqns%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zahavy%2C%20T.%20Ben-Zrihem%2C%20N.%20Mannor%2C%20S.%20Graying%20the%20black%20box%3A%20Understanding%20dqns%202016"
        },
        {
            "id": "Zahavy_et+al_2018_a",
            "entry": "Zahavy, T.; Magnani, A.; Krishnan, A.; and Mannor, S. 2018. Is a picture worth a thousand words? a deep multi-modal fusion architecture for product classification in e-commerce. The Thirtieth Conference on Innovative Applications of Artificial Intelligence (IAAI). Zelinka, M. 2018. Using reinforcement learning to learn how to play text-based games. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zahavy%2C%20T.%20Magnani%2C%20A.%20Krishnan%2C%20A.%20Mannor%2C%20S.%20Is%20a%20picture%20worth%20a%20thousand%20words%3F%20a%20deep%20multi-modal%20fusion%20architecture%20for%20product%20classification%20in%20e-commerce%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zahavy%2C%20T.%20Magnani%2C%20A.%20Krishnan%2C%20A.%20Mannor%2C%20S.%20Is%20a%20picture%20worth%20a%20thousand%20words%3F%20a%20deep%20multi-modal%20fusion%20architecture%20for%20product%20classification%20in%20e-commerce%202018"
        },
        {
            "id": "Zhao_2016_a",
            "entry": "Zhao, T., and Eskenazi, M. 2016. Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning. arXiv preprint.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20T.%20Eskenazi%2C%20M.%20Towards%20end-to-end%20learning%20for%20dialog%20state%20tracking%20and%20management%20using%20deep%20reinforcement%20learning.%20arXiv%20p%202016"
        },
        {
            "id": "Zrihem_et+al_2016_a",
            "entry": "Zrihem, N. B.; Zahavy, T.; and Mannor, S. 2016. Visualizing dynamics: from t-sne to semi-mdps. arXiv preprint arXiv:1606.07112.",
            "arxiv_url": "https://arxiv.org/pdf/1606.07112"
        }
    ]
}
