{
    "filename": "8059-learning-to-play-with-intrinsically-motivated-self-aware-agents.pdf",
    "metadata": {
        "title": "Learning to Play With Intrinsically-Motivated, Self-Aware Agents",
        "author": "Nick Haber, Damian Mrowca, Stephanie Wang, Li F. Fei-Fei, Daniel L. Yamins",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8059-learning-to-play-with-intrinsically-motivated-self-aware-agents.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a \u201cworld-model\u201d network that learns to predict the dynamic consequences of the agent\u2019s actions. Simultaneously, we train a separate explicit \u201cself-model\u201d that allows the agent to track the error map of its worldmodel. It then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in realistic physical environments."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "active learning",
            "url": "https://en.wikipedia.org/wiki/active_learning"
        },
        {
            "term": "HARD",
            "url": "https://en.wikipedia.org/wiki/Hard"
        },
        {
            "term": "world model",
            "url": "https://en.wikipedia.org/wiki/world_model"
        },
        {
            "term": "OBJECT",
            "url": "https://en.wikipedia.org/wiki/Object"
        },
        {
            "term": "intrinsic motivation",
            "url": "https://en.wikipedia.org/wiki/intrinsic_motivation"
        }
    ],
    "highlights": [
        "Autonomous artificial agents must be able to discover useful behaviors in complex environments without having humans present to constantly pre-specify tasks and rewards",
        "We have constructed a simple self-supervised mechanism that spontaneously generates a spectrum of emergent naturalistic behaviors via an active learning process, experiencing \u201cdevelopmental",
        "Models are trained with one object per room unless stated",
        "The agent first learns the dynamics of its own motion, gets \u201cbored\u201d, shifts its attention to locating, moving toward, and interacting with single objects (\u21e4 in Fig. 4 and Fig. 6)",
        "Once these are better understood ( in Fig. 4 and Fig. 6), the agent transitions to gathering multiple objects and learning from their interactions (\u21e5 in Fig. 6)",
        "Our ablation studies show that without this active learning policy, world-model accuracy remains poor and visual encodings transfer much less well. These results constitute a proof-of-concept that both complex behaviors and useful visual features can arise from simple intrinsic motivations in a three-dimensional physical environment with realistically large and continuous state and action spaces"
    ],
    "key_statements": [
        "Autonomous artificial agents must be able to discover useful behaviors in complex environments without having humans present to constantly pre-specify tasks and rewards",
        "We have constructed a simple self-supervised mechanism that spontaneously generates a spectrum of emergent naturalistic behaviors via an active learning process, experiencing \u201cdevelopmental",
        "Models are trained with one object per room unless stated",
        "The agent first learns the dynamics of its own motion, gets \u201cbored\u201d, shifts its attention to locating, moving toward, and interacting with single objects (\u21e4 in Fig. 4 and Fig. 6)",
        "Once these are better understood ( in Fig. 4 and Fig. 6), the agent transitions to gathering multiple objects and learning from their interactions (\u21e5 in Fig. 6)",
        "Our ablation studies show that without this active learning policy, world-model accuracy remains poor and visual encodings transfer much less well. These results constitute a proof-of-concept that both complex behaviors and useful visual features can arise from simple intrinsic motivations in a three-dimensional physical environment with realistically large and continuous state and action spaces"
    ],
    "summary": [
        "Autonomous artificial agents must be able to discover useful behaviors in complex environments without having humans present to constantly pre-specify tasks and rewards.",
        "Our work differs in using a physically realistic three-dimensional environment and shows how intrinsic motivation can lead to substantially more sophisticated agent-object behavior generation.",
        "The world-model seeks to solve one or more dynamics prediction problems based on inputs from the environment.",
        "The self-model seeks to estimate the world-model\u2019s losses for several timesteps into the future, as a function both of visual input and of potential agent actions.",
        "If the agent explicitly predicts its own world-model loss L!t incurred at future timesteps as a function of visual input and current action, a simple antagonistic policy could seek to maximize L!t over some number of future timesteps.",
        "We evaluate improvements in dynamic task prediction in the agents\u2019 world-models, on challenging held-out validation data constructed to test learning about both egomotion dynamics and object physical interactions.",
        "In addition to sharp stage-like transitions in world-model loss and self-model evaluations, to quantify these behaviors we measure play state frequency and the average distance between the agent and objects.",
        "All agents start with behaviors indistinguishable from the random policy, choosing largely self-motion actions and rarely interacting with objects.",
        "The loss temporarily decreases as the agent learns to predict its ego-motion and rises when its attention shifts towards objects, which it interacts with.",
        "We measure the inverse dynamics prediction performance on two held-out validation subsets of data generated from the uncontrolled background distribution of events: (i) an easy dataset consisting solely of ego-motion with no play states, and a hard dataset heavily enriched for play states, each with 4800 examples.",
        "Self-model driven agents substantially outperform alternatives on all three transfer tasks, As shown in Table 1, the SP (T = 5) agents outperform baselines on inverse dynamics and object presence metrics, while ID-SP outperforms LF-SP on localization and recognition.",
        "This increasingly challenging self-generated curriculum leads to performance gains in the agent\u2019s world-model and improved transfer to other useful visual tasks on which the system never received any explicit training signal.",
        "These results constitute a proof-of-concept that both complex behaviors and useful visual features can arise from simple intrinsic motivations in a three-dimensional physical environment with realistically large and continuous state and action spaces.",
        "It will likely be necessary to improve both the formulation of the world-model dynamics prediction tasks our agent solves as well as the antagonistic action policies of the agent\u2019s self-model."
    ],
    "headline": "Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a \u201cworld-model\u201d network that learns to predict the dynamic consequences of the agent\u2019s actions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J. Achiam and S. Sastry. Surprise-based intrinsic motivation for deep reinforcement learning. CoRR, abs/1703.01732, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01732"
        },
        {
            "id": "2",
            "entry": "[2] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20P.%20Nair%2C%20A.V.%20Abbeel%2C%20P.%20Malik%2C%20J.%20Learning%20to%20poke%20by%20poking%3A%20Experiential%20learning%20of%20intuitive%20physics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20P.%20Nair%2C%20A.V.%20Abbeel%2C%20P.%20Malik%2C%20J.%20Learning%20to%20poke%20by%20poking%3A%20Experiential%20learning%20of%20intuitive%20physics%202016"
        },
        {
            "id": "3",
            "entry": "[3] A. Baranes and P.-Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49\u201373, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baranes%2C%20A.%20Oudeyer%2C%20P.-Y.%20Active%20learning%20of%20inverse%20models%20with%20intrinsically%20motivated%20goal%20exploration%20in%20robots%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baranes%2C%20A.%20Oudeyer%2C%20P.-Y.%20Active%20learning%20of%20inverse%20models%20with%20intrinsically%20motivated%20goal%20exploration%20in%20robots%202013"
        },
        {
            "id": "4",
            "entry": "[4] K. Begus, T. Gliga, and V. Southgate. Infants learn what they want to learn: Responding to infant pointing leads to superior learning. PLOS ONE, 9(10):1\u20134, 10 2014. doi: 10.1371/journal. pone.0108817.",
            "crossref": "https://dx.doi.org/10.1371/journal.pone.0108817",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1371/journal.pone.0108817"
        },
        {
            "id": "5",
            "entry": "[5] A. Boeing and T. Br\u00e4unl. Evaluation of real-time physics simulation systems. In Proceedings of the 5th international conference on Computer graphics and interactive techniques in Australia and Southeast Asia, pages 281\u2013288. ACM, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boeing%2C%20A.%20Br%C3%A4unl%2C%20T.%20Evaluation%20of%20real-time%20physics%20simulation%20systems%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boeing%2C%20A.%20Br%C3%A4unl%2C%20T.%20Evaluation%20of%20real-time%20physics%20simulation%20systems%202007"
        },
        {
            "id": "6",
            "entry": "[6] N. Chentanez, A. G. Barto, and S. P. Singh. Intrinsically motivated reinforcement learning. In NIPS, pages 1281\u20131288, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chentanez%2C%20N.%20Barto%2C%20A.G.%20Singh%2C%20S.P.%20Intrinsically%20motivated%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chentanez%2C%20N.%20Barto%2C%20A.G.%20Singh%2C%20S.P.%20Intrinsically%20motivated%20reinforcement%20learning%202005"
        },
        {
            "id": "7",
            "entry": "[7] F. Ebert, C. Finn, A. X. Lee, and S. Levine. Self-supervised visual planning with temporal skip connections. In CoRL, volume 78, pages 344\u2013356. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ebert%2C%20F.%20Finn%2C%20C.%20Lee%2C%20A.X.%20Levine%2C%20S.%20Self-supervised%20visual%20planning%20with%20temporal%20skip%20connections%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ebert%2C%20F.%20Finn%2C%20C.%20Lee%2C%20A.X.%20Levine%2C%20S.%20Self-supervised%20visual%20planning%20with%20temporal%20skip%20connections%202017"
        },
        {
            "id": "8",
            "entry": "[8] E. Elhamifar, G. Sapiro, A. Y. Yang, and S. S. Sastry. A convex optimization framework for active learning. In ICCV, pages 209\u2013216. IEEE Computer Society, 2013. ISBN 978-1-47992839-2.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elhamifar%2C%20E.%20Sapiro%2C%20G.%20Yang%2C%20A.Y.%20Sastry%2C%20S.S.%20A%20convex%20optimization%20framework%20for%20active%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elhamifar%2C%20E.%20Sapiro%2C%20G.%20Yang%2C%20A.Y.%20Sastry%2C%20S.S.%20A%20convex%20optimization%20framework%20for%20active%20learning%202013"
        },
        {
            "id": "9",
            "entry": "[9] R. L. Fantz. Visual experience in infants: Decreased attention to familiar patterns relative to novel ones. Science, 146:668\u2013670, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fantz%2C%20R.L.%20Visual%20experience%20in%20infants%3A%20Decreased%20attention%20to%20familiar%20patterns%20relative%20to%20novel%20ones%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fantz%2C%20R.L.%20Visual%20experience%20in%20infants%3A%20Decreased%20attention%20to%20familiar%20patterns%20relative%20to%20novel%20ones%201964"
        },
        {
            "id": "10",
            "entry": "[10] C. Finn and S. Levine. Deep visual foresight for planning robot motion. In ICRA, pages 2786\u20132793. IEEE, 2017. ISBN 978-1-5090-4633-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Levine%2C%20S.%20Deep%20visual%20foresight%20for%20planning%20robot%20motion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Levine%2C%20S.%20Deep%20visual%20foresight%20for%20planning%20robot%20motion%202017"
        },
        {
            "id": "11",
            "entry": "[11] M. Frank, J. Leitner, M. F. Stollenga, A. F\u00f6rster, and J. Schmidhuber. Curiosity driven reinforcement learning for motion planning on humanoids. Front. Neurorobot., 2014, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frank%2C%20M.%20Leitner%2C%20J.%20Stollenga%2C%20M.F.%20F%C3%B6rster%2C%20A.%20Curiosity%20driven%20reinforcement%20learning%20for%20motion%20planning%20on%20humanoids%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frank%2C%20M.%20Leitner%2C%20J.%20Stollenga%2C%20M.F.%20F%C3%B6rster%2C%20A.%20Curiosity%20driven%20reinforcement%20learning%20for%20motion%20planning%20on%20humanoids%202014"
        },
        {
            "id": "12",
            "entry": "[12] R. Gilad-Bachrach, A. Navot, and N. Tishby. Query by committee made real. In NIPS, pages 443\u2013450, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilad-Bachrach%2C%20R.%20Navot%2C%20A.%20Tishby%2C%20N.%20Query%20by%20committee%20made%20real%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilad-Bachrach%2C%20R.%20Navot%2C%20A.%20Tishby%2C%20N.%20Query%20by%20committee%20made%20real%202005"
        },
        {
            "id": "13",
            "entry": "[13] A. Gopnik, A. Meltzoff, and P. Kuhl. The Scientist In The Crib: Minds, Brains, And How Children Learn. HarperCollins, 2009. ISBN 9780061846915.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopnik%2C%20A.%20Meltzoff%2C%20A.%20Kuhl%2C%20P.%20The%20Scientist%20In%20The%20Crib%3A%20Minds%2C%20Brains%2C%20And%20How%20Children%20Learn%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopnik%2C%20A.%20Meltzoff%2C%20A.%20Kuhl%2C%20P.%20The%20Scientist%20In%20The%20Crib%3A%20Minds%2C%20Brains%2C%20And%20How%20Children%20Learn%202009"
        },
        {
            "id": "14",
            "entry": "[14] J. Gottlieb, P.-Y. Oudeyer, M. Lopes, and A. Baranes. Information-seeking, curiosity, and attention: computational and neural mechanisms. Trends in cognitive sciences, 17(11):585\u2013593, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gottlieb%2C%20J.%20Oudeyer%2C%20P.-Y.%20Lopes%2C%20M.%20Baranes%2C%20A.%20Information-seeking%2C%20curiosity%2C%20and%20attention%3A%20computational%20and%20neural%20mechanisms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gottlieb%2C%20J.%20Oudeyer%2C%20P.-Y.%20Lopes%2C%20M.%20Baranes%2C%20A.%20Information-seeking%2C%20curiosity%2C%20and%20attention%3A%20computational%20and%20neural%20mechanisms%202013"
        },
        {
            "id": "15",
            "entry": "[15] L. Goupil, M. Romand-Monnier, and S. Kouider. Infants ask for help when they know they don\u2019t know. Proceedings of the National Academy of Sciences, 113(13):3492\u20133496, 2016. doi: 10.1073/pnas.1515129113.",
            "crossref": "https://dx.doi.org/10.1073/pnas.1515129113",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1073/pnas.1515129113"
        },
        {
            "id": "16",
            "entry": "[16] D. Held, X. Geng, C. Florensa, and P. Abbeel. Automatic goal generation for reinforcement learning agents. CoRR, abs/1705.06366, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06366"
        },
        {
            "id": "17",
            "entry": "[17] J. Ho and S. Ermon. Generative adversarial imitation learning. In NIPS, pages 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20J.%20Ermon%2C%20S.%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "18",
            "entry": "[18] S. Hong, D. Yeo, S. Kwak, H. Lee, and B. Han. Weakly supervised semantic segmentation using web-crawled videos. In CVPR, pages 2224\u20132232. IEEE Computer Society, 2017. ISBN 978-1-5386-0457-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20S.%20Yeo%2C%20D.%20Kwak%2C%20S.%20Lee%2C%20H.%20Weakly%20supervised%20semantic%20segmentation%20using%20web-crawled%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20S.%20Yeo%2C%20D.%20Kwak%2C%20S.%20Lee%2C%20H.%20Weakly%20supervised%20semantic%20segmentation%20using%20web-crawled%20videos%202017"
        },
        {
            "id": "19",
            "entry": "[19] R. Houthooft, X. Chen, X. Chen, Y. Duan, J. Schulman, F. D. Turck, and P. Abbeel. Vime: Variational information maximizing exploration. In NIPS, pages 1109\u20131117, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Houthooft%2C%20R.%20Chen%2C%20X.%20Chen%2C%20X.%20Duan%2C%20Y.%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Houthooft%2C%20R.%20Chen%2C%20X.%20Chen%2C%20X.%20Duan%2C%20Y.%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016"
        },
        {
            "id": "20",
            "entry": "[20] K. B. Hurley and L. M. Oakes. Experience and distribution of attention: Pet exposure and infants\u2019 scanning of animal images. J Cogn Dev, 16(1):11\u201330, Jan 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hurley%2C%20K.B.%20Oakes%2C%20L.M.%20Experience%20and%20distribution%20of%20attention%3A%20Pet%20exposure%20and%20infants%E2%80%99%20scanning%20of%20animal%20images%202015-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hurley%2C%20K.B.%20Oakes%2C%20L.M.%20Experience%20and%20distribution%20of%20attention%3A%20Pet%20exposure%20and%20infants%E2%80%99%20scanning%20of%20animal%20images%202015-01"
        },
        {
            "id": "21",
            "entry": "[21] K. B. Hurley, K. A. Kovack-Lesh, and L. M. Oakes. The influence of pets on infants\u2019 processing of cat and dog images. Infant Behav Dev, 33(4):619\u2013628, Dec 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hurley%2C%20K.B.%20Kovack-Lesh%2C%20K.A.%20Oakes%2C%20L.M.%20The%20influence%20of%20pets%20on%20infants%E2%80%99%20processing%20of%20cat%20and%20dog%20images%202010-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hurley%2C%20K.B.%20Kovack-Lesh%2C%20K.A.%20Oakes%2C%20L.M.%20The%20influence%20of%20pets%20on%20infants%E2%80%99%20processing%20of%20cat%20and%20dog%20images%202010-12"
        },
        {
            "id": "22",
            "entry": "[22] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. CoRR, abs/1611.05397, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05397"
        },
        {
            "id": "23",
            "entry": "[23] N. Kalchbrenner, A. van den Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. In ICML, volume 70 of JMLR Workshop and Conference Proceedings, pages 1771\u20131779. JMLR.org, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalchbrenner%2C%20N.%20van%20den%20Oord%2C%20A.%20Simonyan%2C%20K.%20Danihelka%2C%20I.%20Video%20pixel%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalchbrenner%2C%20N.%20van%20den%20Oord%2C%20A.%20Simonyan%2C%20K.%20Danihelka%2C%20I.%20Video%20pixel%20networks%202017"
        },
        {
            "id": "24",
            "entry": "[24] C. Kidd, S. T. Piantadosi, and R. N. Aslin. The goldilocks effect: Human infants allocate attention to visual sequences that are neither too simple nor too complex. PLOS ONE, 7(5):1\u20138, 05 2012. doi: 10.1371/journal.pone.0036399.",
            "crossref": "https://dx.doi.org/10.1371/journal.pone.0036399",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1371/journal.pone.0036399"
        },
        {
            "id": "25",
            "entry": "[25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "26",
            "entry": "[26] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In NIPS, pages 3675\u20133683, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20T.D.%20Narasimhan%2C%20K.%20Saeedi%2C%20A.%20Tenenbaum%2C%20J.%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20T.D.%20Narasimhan%2C%20K.%20Saeedi%2C%20A.%20Tenenbaum%2C%20J.%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "27",
            "entry": "[27] L.-J. Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Pittsburgh, PA, USA, 1992. UMI Order No. GAX93-22750.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20L.-J.%20Reinforcement%20Learning%20for%20Robots%20Using%20Neural%20Networks%201992"
        },
        {
            "id": "28",
            "entry": "[28] M. C. Machado, M. G. Bellemare, and M. Bowling. A laplacian framework for option discovery in reinforcement learning. arXiv preprint arXiv:1703.00956, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00956"
        },
        {
            "id": "29",
            "entry": "[29] C. Mitash, K. E. Bekris, and A. Boularias. A self-supervised learning system for object detection using physics simulation and multi-view pose estimation. In IROS, pages 545\u2013551. IEEE, 2017. ISBN 978-1-5386-2682-5.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitash%2C%20C.%20Bekris%2C%20K.E.%20Boularias%2C%20A.%20A%20self-supervised%20learning%20system%20for%20object%20detection%20using%20physics%20simulation%20and%20multi-view%20pose%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mitash%2C%20C.%20Bekris%2C%20K.E.%20Boularias%2C%20A.%20A%20self-supervised%20learning%20system%20for%20object%20detection%20using%20physics%20simulation%20and%20multi-view%20pose%20estimation%202017"
        },
        {
            "id": "30",
            "entry": "[30] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, ICML\u201916, pages 1928\u20131937. JMLR.org, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "31",
            "entry": "[31] D. Mrowca, C. Zhuang, E. Wang, N. Haber, L. Fei-Fei, J. B. Tenenbaum, and D. L. Yamins. Flexible neural representation for physics prediction. arXiv preprint arXiv:1806.08047, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.08047"
        },
        {
            "id": "32",
            "entry": "[32] M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV (6), volume 9910 of Lecture Notes in Computer Science, pages 69\u201384. Springer, 2016. ISBN 978-3-319-46465-7.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noroozi%2C%20M.%20Favaro%2C%20P.%20Unsupervised%20learning%20of%20visual%20representations%20by%20solving%20jigsaw%20puzzles%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noroozi%2C%20M.%20Favaro%2C%20P.%20Unsupervised%20learning%20of%20visual%20representations%20by%20solving%20jigsaw%20puzzles%202016"
        },
        {
            "id": "33",
            "entry": "[33] P.-Y. Oudeyer and L. B. Smith. How evolution may work through curiosity-driven developmental process. Topics in Cognitive Science, 8(2):492\u2013502, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oudeyer%2C%20P.-Y.%20Smith%2C%20L.B.%20How%20evolution%20may%20work%20through%20curiosity-driven%20developmental%20process%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oudeyer%2C%20P.-Y.%20Smith%2C%20L.B.%20How%20evolution%20may%20work%20through%20curiosity-driven%20developmental%20process%202016"
        },
        {
            "id": "34",
            "entry": "[34] P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265\u2013286, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oudeyer%2C%20P.-Y.%20Kaplan%2C%20F.%20Hafner%2C%20V.V.%20Intrinsic%20motivation%20systems%20for%20autonomous%20mental%20development%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oudeyer%2C%20P.-Y.%20Kaplan%2C%20F.%20Hafner%2C%20V.V.%20Intrinsic%20motivation%20systems%20for%20autonomous%20mental%20development%202007"
        },
        {
            "id": "35",
            "entry": "[35] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by selfsupervised prediction. In ICML, volume 70 of JMLR Workshop and Conference Proceedings, pages 2778\u20132787. JMLR.org, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction%202017"
        },
        {
            "id": "36",
            "entry": "[36] I. Popov, N. Heess, T. Lillicrap, R. Hafner, G. Barth-Maron, M. Vecerik, T. Lampe, Y. Tassa, T. Erez, and M. Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. arXiv preprint arXiv:1704.03073, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03073"
        },
        {
            "id": "37",
            "entry": "[37] R. Saxe and N. Kanwisher. People thinking about thinking people: the role of the temporoparietal junction in \u201ctheory of mind\u201d. Neuroimage, 19(4):1835\u20131842, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20R.%20Kanwisher%2C%20N.%20People%20thinking%20about%20thinking%20people%3A%20the%20role%20of%20the%20temporoparietal%20junction%20in%20%E2%80%9Ctheory%20of%20mind%E2%80%9D%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20R.%20Kanwisher%2C%20N.%20People%20thinking%20about%20thinking%20people%3A%20the%20role%20of%20the%20temporoparietal%20junction%20in%20%E2%80%9Ctheory%20of%20mind%E2%80%9D%202003"
        },
        {
            "id": "38",
            "entry": "[38] J. Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE Trans. Autonomous Mental Development, 2(3):230\u2013247, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Trans%20Autonomous%20Mental%20Development%2023230247%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Trans%20Autonomous%20Mental%20Development%2023230247%202010"
        },
        {
            "id": "39",
            "entry": "[39] O. Sener and S. Savarese. A geometric approach to active learning for convolutional neural networks. CoRR, abs/1708.00489, 2017. Morgan & Claypool Publishers, 2011.",
            "arxiv_url": "https://arxiv.org/pdf/1708.00489"
        },
        {
            "id": "41",
            "entry": "[41] S. P. Singh, R. L. Lewis, A. G. Barto, and J. Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Trans. Autonomous Mental Development, 2(2):70\u201382, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20S.P.%20Lewis%2C%20R.L.%20Barto%2C%20A.G.%20Sorg%2C%20J.%20Intrinsically%20motivated%20reinforcement%20learning%3A%20An%20evolutionary%20perspective%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20S.P.%20Lewis%2C%20R.L.%20Barto%2C%20A.G.%20Sorg%2C%20J.%20Intrinsically%20motivated%20reinforcement%20learning%3A%20An%20evolutionary%20perspective%202010"
        },
        {
            "id": "42",
            "entry": "[42] E. Sokolov. Perception and the conditioned reflex. Pergamon Press, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sokolov%2C%20E.%20Perception%20and%20the%20conditioned%20reflex%201963"
        },
        {
            "id": "43",
            "entry": "[43] N. K. Spyros Gidaris, Praveer Singh. Unsupervised representation learning by predicting image rotations. ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidaris%2C%20N.K.Spyros%20Singh%2C%20Praveer%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidaris%2C%20N.K.Spyros%20Singh%2C%20Praveer%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018"
        },
        {
            "id": "44",
            "entry": "[44] K. E. Twomey and G. Westermann. Curiosity-based learning in infants: a neurocomputational approach. Dev Sci, Oct 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Twomey%2C%20K.E.%20Westermann%2C%20G.%20Curiosity-based%20learning%20in%20infants%3A%20a%20neurocomputational%20approach%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Twomey%2C%20K.E.%20Westermann%2C%20G.%20Curiosity-based%20learning%20in%20infants%3A%20a%20neurocomputational%20approach%202017-10"
        },
        {
            "id": "45",
            "entry": "[45] K. Wang, D. Zhang, Y. Li, R. Zhang, and L. Lin. Cost-effective active learning for deep image classification. IEEE Trans. Circuits Syst. Video Techn., 27(12):2591\u20132600, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20K.%20Zhang%2C%20D.%20Li%2C%20Y.%20Zhang%2C%20R.%20Cost-effective%20active%20learning%20for%20deep%20image%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20K.%20Zhang%2C%20D.%20Li%2C%20Y.%20Zhang%2C%20R.%20Cost-effective%20active%20learning%20for%20deep%20image%20classification%202017"
        },
        {
            "id": "46",
            "entry": "[46] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In B. Leibe, J. Matas, N. Sebe, and M. Welling, editors, ECCV (3), volume 9907 of Lecture Notes in Computer Science, pages 649\u2013666. Springer, 2016. ISBN 978-3-319-46486-2.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Colorful%20image%20colorization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Colorful%20image%20colorization%202016"
        }
    ]
}
