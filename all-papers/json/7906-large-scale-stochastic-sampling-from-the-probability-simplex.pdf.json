{
    "filename": "7906-large-scale-stochastic-sampling-from-the-probability-simplex.pdf",
    "metadata": {
        "title": "Large-Scale Stochastic Sampling from the Probability Simplex",
        "author": "Jack Baker, Paul Fearnhead, Emily Fox, Christopher Nemeth",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7906-large-scale-stochastic-sampling-from-the-probability-simplex.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference. These methods are based on sampling a discrete-time approximation to a continuous time process, such as the Langevin diffusion. When applied to distributions defined on a constrained space the timediscretization error can dominate when we are near the boundary of the space. We demonstrate that because of this, current SGMCMC methods for the simplex struggle with sparse simplex spaces; when many of the components are close to zero. Unfortunately, many popular large-scale Bayesian models, such as network or topic models, require inference on sparse simplex spaces. To avoid the biases caused by this discretization error, we propose the stochastic Cox-Ingersoll-Ross process (SCIR), which removes all discretization error and we prove that samples from the SCIR process are asymptotically unbiased. We discuss how this idea can be extended to target other constrained spaces. Use of the SCIR process within a SGMCMC algorithm is shown to give substantially better performance for a topic model and a Dirichlet process mixture model than existing SGMCMC approaches."
    },
    "keywords": [
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "dirichlet process",
            "url": "https://en.wikipedia.org/wiki/dirichlet_process"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "markov chain monte carlo",
            "url": "https://en.wikipedia.org/wiki/markov_chain_monte_carlo"
        },
        {
            "term": "topic model",
            "url": "https://en.wikipedia.org/wiki/topic_model"
        },
        {
            "term": "discretization error",
            "url": "https://en.wikipedia.org/wiki/discretization_error"
        },
        {
            "term": "continuous process",
            "url": "https://en.wikipedia.org/wiki/continuous_process"
        },
        {
            "term": "latent Dirichlet allocation",
            "url": "https://en.wikipedia.org/wiki/latent_Dirichlet_allocation"
        },
        {
            "term": "stochastic gradient Langevin dynamics",
            "url": "https://en.wikipedia.org/wiki/Stochastic_Gradient_Langevin_Dynamics"
        },
        {
            "term": "moment generating function",
            "url": "https://en.wikipedia.org/wiki/moment_generating_function"
        }
    ],
    "highlights": [
        "Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference (Welling and Teh, 2011; <a class=\"ref-link\" id=\"cChen_et+al_2014_a\" href=\"#rChen_et+al_2014_a\"><a class=\"ref-link\" id=\"cChen_et+al_2014_a\" href=\"#rChen_et+al_2014_a\">Chen et al, 2014</a></a>; <a class=\"ref-link\" id=\"cDing_et+al_2014_a\" href=\"#rDing_et+al_2014_a\"><a class=\"ref-link\" id=\"cDing_et+al_2014_a\" href=\"#rDing_et+al_2014_a\">Ding et al, 2014</a></a>; <a class=\"ref-link\" id=\"cMa_et+al_2015_a\" href=\"#rMa_et+al_2015_a\"><a class=\"ref-link\" id=\"cMa_et+al_2015_a\" href=\"#rMa_et+al_2015_a\">Ma et al, 2015</a></a>)",
        "The foundation of Stochastic gradient MCMC methods are a class of continuous processes that explore a target distribution\u2014e.g., the posterior\u2014using gradient information",
        "For multimodal mixture models such as this, Stochastic gradient MCMC methods are known to get stuck in local modes (<a class=\"ref-link\" id=\"cBaker_et+al_2017_a\" href=\"#rBaker_et+al_2017_a\">Baker et al, 2017</a>), so we use a fixed stepsize for both stochastic gradient Riemannian Langevin dynamics and Stochastic Cox-Ingersoll-Ross",
        "We presented an Stochastic gradient MCMC method, the Stochastic Cox-Ingersoll-Ross algorithm, for simplex spaces",
        "We show that the method has no discretization error and is asymptotically unbiased",
        "Our experiments demonstrate that these properties give the sampler improved performance over other Stochastic gradient MCMC methods for sampling from sparse simplex spaces"
    ],
    "key_statements": [
        "Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference (Welling and Teh, 2011; <a class=\"ref-link\" id=\"cChen_et+al_2014_a\" href=\"#rChen_et+al_2014_a\"><a class=\"ref-link\" id=\"cChen_et+al_2014_a\" href=\"#rChen_et+al_2014_a\">Chen et al, 2014</a></a>; <a class=\"ref-link\" id=\"cDing_et+al_2014_a\" href=\"#rDing_et+al_2014_a\"><a class=\"ref-link\" id=\"cDing_et+al_2014_a\" href=\"#rDing_et+al_2014_a\">Ding et al, 2014</a></a>; <a class=\"ref-link\" id=\"cMa_et+al_2015_a\" href=\"#rMa_et+al_2015_a\"><a class=\"ref-link\" id=\"cMa_et+al_2015_a\" href=\"#rMa_et+al_2015_a\">Ma et al, 2015</a></a>)",
        "The foundation of Stochastic gradient MCMC methods are a class of continuous processes that explore a target distribution\u2014e.g., the posterior\u2014using gradient information",
        "The result of these two sources of error is that Stochastic gradient MCMC targets an approximate posterior (Welling and Teh, 2011; <a class=\"ref-link\" id=\"cTeh_et+al_2016_a\" href=\"#rTeh_et+al_2016_a\">Teh et al, 2016</a>; <a class=\"ref-link\" id=\"cVollmer_et+al_2016_a\" href=\"#rVollmer_et+al_2016_a\">Vollmer et al, 2016</a>)",
        "In order to demonstrate the problems with using stochastic gradient Riemannian Langevin dynamics in this case, we provide a similar experiment to <a class=\"ref-link\" id=\"cPatterson_2013_a\" href=\"#rPatterson_2013_a\">Patterson and Teh (2013</a>)",
        "The standard CIR process has more parameters, but we found changing these made no difference to the properties of our proposed scalable sampler, so we omit them",
        "The result means that similar to stochastic gradient Langevin dynamics, we can replace the CIR parameter a with an unbiased estimate acreated from a minibatch of data",
        "latent Dirichlet allocation is a good example for this method because \u03c6k is likely to be very sparse, there are many words which will not be associated with a given topic at all",
        "We apply Stochastic Cox-Ingersoll-Ross and stochastic gradient Riemannian Langevin dynamics to latent Dirichlet allocation on a dataset of scraped Wikipedia documents, by adapting the code released by <a class=\"ref-link\" id=\"cPatterson_2013_a\" href=\"#rPatterson_2013_a\">Patterson and Teh (2013</a>)",
        "We apply Stochastic Cox-Ingersoll-Ross to sample from a Bayesian nonparametric mixture model of categorical data, proposed by <a class=\"ref-link\" id=\"cDunson_2009_a\" href=\"#rDunson_2009_a\">Dunson and Xing (2009</a>)",
        "We develop a truncation free, scalable sampler based on Stochastic gradient MCMC for Dirichlet processes (DP, see <a class=\"ref-link\" id=\"cFerguson_1973_a\" href=\"#rFerguson_1973_a\">Ferguson, 1973</a>)",
        "For multimodal mixture models such as this, Stochastic gradient MCMC methods are known to get stuck in local modes (<a class=\"ref-link\" id=\"cBaker_et+al_2017_a\" href=\"#rBaker_et+al_2017_a\">Baker et al, 2017</a>), so we use a fixed stepsize for both stochastic gradient Riemannian Langevin dynamics and Stochastic Cox-Ingersoll-Ross",
        "The comparison shows, while Stochastic Cox-Ingersoll-Ross outperforms stochastic gradient Riemannian Langevin dynamics, the scalable stochastic gradient approximation itself does not perform well in this case compared to the exact Gibbs sampler",
        "We presented an Stochastic gradient MCMC method, the Stochastic Cox-Ingersoll-Ross algorithm, for simplex spaces",
        "We show that the method has no discretization error and is asymptotically unbiased",
        "Our experiments demonstrate that these properties give the sampler improved performance over other Stochastic gradient MCMC methods for sampling from sparse simplex spaces"
    ],
    "summary": [
        "Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference (Welling and Teh, 2011; <a class=\"ref-link\" id=\"cChen_et+al_2014_a\" href=\"#rChen_et+al_2014_a\"><a class=\"ref-link\" id=\"cChen_et+al_2014_a\" href=\"#rChen_et+al_2014_a\">Chen et al, 2014</a></a>; <a class=\"ref-link\" id=\"cDing_et+al_2014_a\" href=\"#rDing_et+al_2014_a\"><a class=\"ref-link\" id=\"cDing_et+al_2014_a\" href=\"#rDing_et+al_2014_a\">Ding et al, 2014</a></a>; <a class=\"ref-link\" id=\"cMa_et+al_2015_a\" href=\"#rMa_et+al_2015_a\"><a class=\"ref-link\" id=\"cMa_et+al_2015_a\" href=\"#rMa_et+al_2015_a\">Ma et al, 2015</a></a>).",
        "<a class=\"ref-link\" id=\"cPatterson_2013_a\" href=\"#rPatterson_2013_a\">Patterson and Teh (2013</a>) develop an improved SGMCMC method for sampling from the probability simplex: stochastic gradient Riemannian Langevin dynamics (SGRLD).",
        "Theorem 3.1, allows us to show that applying the transformation g(\u00b7) to the approximate SCIR process, leads to a discretization free SGLD algorithm for a generalized gamma distribution.",
        "Applying g\u22121(\u00b7) to the approximate target of this SGLD algorithm leads to the desired Gamma(a, 1) distribution.",
        "The result means that similar to SGLD, we can replace the CIR parameter a with an unbiased estimate acreated from a minibatch of data.",
        "We run 1000 iterations of optimally tuned SGRLD and SCIR algorithms and compare to an exact sampler.",
        "The SCIR process is achieving much better results than SGRLD, and converging towards the exact sampler at larger minibatch sizes.",
        "The SCIR algorithm is not limited to z being categorical, and it can be used to sample from most constructions that use Dirichlet distributions, provided the z are not integrated out.",
        "The method can be used to sample from constrained spaces on (0, \u221e) that are gamma distributed by just sampling from the SCIR process itself.",
        "We compare this to an exact CIR process with stationary distribution Gamma(a, 1), defined by the transition equation in (3.2).",
        "We empirically compare SCIR to SGRLD on two challenging models: latent Dirichlet allocation (LDA) and a Bayesian nonparametric mixture.",
        "We apply SCIR to sample from a Bayesian nonparametric mixture model of categorical data, proposed by <a class=\"ref-link\" id=\"cDunson_2009_a\" href=\"#rDunson_2009_a\">Dunson and Xing (2009</a>).",
        "For multimodal mixture models such as this, SGMCMC methods are known to get stuck in local modes (<a class=\"ref-link\" id=\"cBaker_et+al_2017_a\" href=\"#rBaker_et+al_2017_a\">Baker et al, 2017</a>), so we use a fixed stepsize for both SGRLD and SCIR.",
        "This is probably due to the asymptotic bias of Proposition 2.1, since this would lead to an inferred model that has a higher \u03b1 parameter than is set, meaning more clusters would be proposed than are needed.",
        "In the Supplementary Material we provide plots comparing the stochastic gradient methods to the exact, but non-scalable Gibbs slice sampler (<a class=\"ref-link\" id=\"cWalker_2007_a\" href=\"#rWalker_2007_a\">Walker, 2007</a>; <a class=\"ref-link\" id=\"cPapaspiliopoulos_2008_a\" href=\"#rPapaspiliopoulos_2008_a\">Papaspiliopoulos, 2008</a>; <a class=\"ref-link\" id=\"cKalli_et+al_2011_a\" href=\"#rKalli_et+al_2011_a\">Kalli et al, 2011</a>).",
        "The comparison shows, while SCIR outperforms SGRLD, the scalable stochastic gradient approximation itself does not perform well in this case compared to the exact Gibbs sampler.",
        "Our experiments demonstrate that these properties give the sampler improved performance over other SGMCMC methods for sampling from sparse simplex spaces."
    ],
    "headline": "We demonstrate that because of this, current Stochastic gradient MCMC methods for the simplex struggle with sparse simplex spaces; when many of the components are close to zero",
    "reference_links": [
        {
            "id": "Baker_et+al_2017_a",
            "entry": "Baker, J., Fearnhead, P., Fox, E. B., and Nemeth, C. (2017). Control variates for stochastic gradient MCMC. Available from https://arxiv.org/abs/1706.05439.",
            "url": "https://arxiv.org/abs/1706.05439",
            "arxiv_url": "https://arxiv.org/pdf/1706.05439"
        },
        {
            "id": "Blackwell_1973_a",
            "entry": "Blackwell, D. and MacQueen, J. B. (1973). Ferguson distributions via Polya urn schemes. The Annals of Statistics, 1(2):353\u2013355.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blackwell%2C%20D.%20MacQueen%2C%20J.B.%20Ferguson%20distributions%20via%20Polya%20urn%20schemes%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blackwell%2C%20D.%20MacQueen%2C%20J.B.%20Ferguson%20distributions%20via%20Polya%20urn%20schemes%201973"
        },
        {
            "id": "Blei_et+al_2003_a",
            "entry": "Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993\u20131022.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20D.M.%20Ng%2C%20A.Y.%20Jordan%2C%20M.I.%20Latent%20Dirichlet%20allocation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20D.M.%20Ng%2C%20A.Y.%20Jordan%2C%20M.I.%20Latent%20Dirichlet%20allocation%202003"
        },
        {
            "id": "Breese_et+al_1998_a",
            "entry": "Breese, J. S., Heckerman, D., and Kadie, C. (1998). Empirical analysis of predictive algorithms for collaborative filtering. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pages 43\u201352.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breese%2C%20J.S.%20Heckerman%2C%20D.%20Kadie%2C%20C.%20Empirical%20analysis%20of%20predictive%20algorithms%20for%20collaborative%20filtering%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breese%2C%20J.S.%20Heckerman%2C%20D.%20Kadie%2C%20C.%20Empirical%20analysis%20of%20predictive%20algorithms%20for%20collaborative%20filtering%201998"
        },
        {
            "id": "Chatterji_et+al_2018_a",
            "entry": "Chatterji, N. S., Flammarion, N., Ma, Y.-A., Bartlett, P. L., and Jordan, M. I. (2018). On the theory of variance reduction for stochastic gradient Monte Carlo. Available at https://arxiv.org/abs/1802.05431v1.",
            "url": "https://arxiv.org/abs/1802.05431v1",
            "arxiv_url": "https://arxiv.org/pdf/1802.05431v1"
        },
        {
            "id": "Chen_et+al_2014_a",
            "entry": "Chen, T., Fox, E., and Guestrin, C. (2014). Stochastic gradient Hamiltonian Monte Carlo. In Proceedings of the 31st International Conference on Machine Learning, pages 1683\u20131691. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20T.%20Fox%2C%20E.%20Guestrin%2C%20C.%20Stochastic%20gradient%20Hamiltonian%20Monte%20Carlo%202014"
        },
        {
            "id": "Cox_et+al_1985_a",
            "entry": "Cox, J. C., Ingersoll, J. E., and Ross, S. A. (1985). A theory of the term structure of interest rates. Econometrica, 53(2):385\u2013407.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cox%2C%20J.C.%20Ingersoll%2C%20J.E.%20Ross%2C%20S.A.%20A%20theory%20of%20the%20term%20structure%20of%20interest%20rates%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cox%2C%20J.C.%20Ingersoll%2C%20J.E.%20Ross%2C%20S.A.%20A%20theory%20of%20the%20term%20structure%20of%20interest%20rates%201985"
        },
        {
            "id": "Ding_et+al_2014_a",
            "entry": "Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., and Neven, H. (2014). Bayesian sampling using stochastic gradient thermostats. In Advances in Neural Information Processing Systems 27, pages 3203\u20133211.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20N.%20Fang%2C%20Y.%20Babbush%2C%20R.%20Chen%2C%20C.%20Bayesian%20sampling%20using%20stochastic%20gradient%20thermostats%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20N.%20Fang%2C%20Y.%20Babbush%2C%20R.%20Chen%2C%20C.%20Bayesian%20sampling%20using%20stochastic%20gradient%20thermostats%202014"
        },
        {
            "id": "Dubey_et+al_2016_a",
            "entry": "Dubey, K. A., Reddi, S. J., Williamson, S. A., Poczos, B., Smola, A. J., and Xing, E. P. (2016). Variance reduction in stochastic gradient Langevin dynamics. In Advances in Neural Information Processing Systems 29, pages 1154\u20131162.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dubey%2C%20K.A.%20Reddi%2C%20S.J.%20Williamson%2C%20S.A.%20Poczos%2C%20B.%20Variance%20reduction%20in%20stochastic%20gradient%20Langevin%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dubey%2C%20K.A.%20Reddi%2C%20S.J.%20Williamson%2C%20S.A.%20Poczos%2C%20B.%20Variance%20reduction%20in%20stochastic%20gradient%20Langevin%20dynamics%202016"
        },
        {
            "id": "Dunson_2009_a",
            "entry": "Dunson, D. B. and Xing, C. (2009). Nonparametric Bayes modeling of multivariate categorical data. Journal of the American Statistical Association, 104(487):1042\u20131051.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dunson%2C%20D.B.%20Xing%2C%20C.%20Nonparametric%20Bayes%20modeling%20of%20multivariate%20categorical%20data%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dunson%2C%20D.B.%20Xing%2C%20C.%20Nonparametric%20Bayes%20modeling%20of%20multivariate%20categorical%20data%202009"
        },
        {
            "id": "Escobar_1995_a",
            "entry": "Escobar, M. D. and West, M. (1995). Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90(430):577\u2013588.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Escobar%2C%20M.D.%20West%2C%20M.%20Bayesian%20density%20estimation%20and%20inference%20using%20mixtures%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Escobar%2C%20M.D.%20West%2C%20M.%20Bayesian%20density%20estimation%20and%20inference%20using%20mixtures%201995"
        },
        {
            "id": "Ferguson_1973_a",
            "entry": "Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209\u2013230.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ferguson%2C%20T.S.%20A%20Bayesian%20analysis%20of%20some%20nonparametric%20problems%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ferguson%2C%20T.S.%20A%20Bayesian%20analysis%20of%20some%20nonparametric%20problems%201973"
        },
        {
            "id": "Girolami_2011_a",
            "entry": "Girolami, M. and Calderhead, B. (2011). Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123\u2013214.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girolami%2C%20M.%20Calderhead%2C%20B.%20Riemann%20manifold%20Langevin%20and%20Hamiltonian%20Monte%20Carlo%20methods%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girolami%2C%20M.%20Calderhead%2C%20B.%20Riemann%20manifold%20Langevin%20and%20Hamiltonian%20Monte%20Carlo%20methods%202011"
        },
        {
            "id": "Griffiths_2004_a",
            "entry": "Griffiths, T. L. and Steyvers, M. (2004). Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101:5228\u20135235.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffiths%2C%20T.L.%20Steyvers%2C%20M.%20Finding%20scientific%20topics%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffiths%2C%20T.L.%20Steyvers%2C%20M.%20Finding%20scientific%20topics%202004"
        },
        {
            "id": "Kalli_et+al_2011_a",
            "entry": "Kalli, M., Griffin, J. E., and Walker, S. G. (2011). Slice sampling mixture models. Statistics and Computing, 21(1):93\u2013105.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalli%2C%20M.%20Griffin%2C%20J.E.%20Walker%2C%20S.G.%20Slice%20sampling%20mixture%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalli%2C%20M.%20Griffin%2C%20J.E.%20Walker%2C%20S.G.%20Slice%20sampling%20mixture%20models%202011"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Li, W., Ahn, S., and Welling, M. (2016). Scalable MCMC for mixed membership stochastic blockmodels. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 723\u2013731.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20W.%20Ahn%2C%20S.%20Welling%2C%20M.%20Scalable%20MCMC%20for%20mixed%20membership%20stochastic%20blockmodels%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20W.%20Ahn%2C%20S.%20Welling%2C%20M.%20Scalable%20MCMC%20for%20mixed%20membership%20stochastic%20blockmodels%202016"
        },
        {
            "id": "Liverani_et+al_2015_a",
            "entry": "Liverani, S., Hastie, D., Azizi, L., Papathomas, M., and Richardson, S. (2015). PReMiuM: An R package for profile regression mixture models using Dirichlet processes. Journal of Statistical Software, 64(7):1\u201330.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liverani%2C%20S.%20Hastie%2C%20D.%20Azizi%2C%20L.%20Papathomas%2C%20M.%20PReMiuM%3A%20An%20R%20package%20for%20profile%20regression%20mixture%20models%20using%20Dirichlet%20processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liverani%2C%20S.%20Hastie%2C%20D.%20Azizi%2C%20L.%20Papathomas%2C%20M.%20PReMiuM%3A%20An%20R%20package%20for%20profile%20regression%20mixture%20models%20using%20Dirichlet%20processes%202015"
        },
        {
            "id": "Ma_et+al_2015_a",
            "entry": "Ma, Y.-A., Chen, T., and Fox, E. (2015). A complete recipe for stochastic gradient MCMC. In Advances in Neural Information Processing Systems, pages 2917\u20132925.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Y.-A.%20Chen%2C%20T.%20Fox%2C%20E.%20A%20complete%20recipe%20for%20stochastic%20gradient%20MCMC%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Y.-A.%20Chen%2C%20T.%20Fox%2C%20E.%20A%20complete%20recipe%20for%20stochastic%20gradient%20MCMC%202015"
        },
        {
            "id": "Nagapetyan_et+al_2017_a",
            "entry": "Nagapetyan, T., Duncan, A., Hasenclever, L., Vollmer, S. J., Szpruch, L., and Zygalakis, K. (2017). The true cost of stochastic gradient Langevin dynamics. Available at https://arxiv.org/abs/1706.02692.",
            "url": "https://arxiv.org/abs/1706.02692",
            "arxiv_url": "https://arxiv.org/pdf/1706.02692"
        },
        {
            "id": "Papaspiliopoulos_2008_a",
            "entry": "Papaspiliopoulos, O. (2008). A note on posterior sampling from Dirichlet mixture models. Technical Report. Available at http://wrap.warwick.ac.uk/35493/1/WRAP_papaspliiopoulos_08-20wv2.pdf.",
            "url": "http://wrap.warwick.ac.uk/35493/1/WRAP_papaspliiopoulos_08-20wv2.pdf"
        },
        {
            "id": "Papaspiliopoulos_2008_b",
            "entry": "Papaspiliopoulos, O. and Roberts, G. O. (2008). Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models. Biometrika, 95(1):169\u2013186.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papaspiliopoulos%2C%20O.%20Roberts%2C%20G.O.%20Retrospective%20Markov%20chain%20Monte%20Carlo%20methods%20for%20Dirichlet%20process%20hierarchical%20models%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papaspiliopoulos%2C%20O.%20Roberts%2C%20G.O.%20Retrospective%20Markov%20chain%20Monte%20Carlo%20methods%20for%20Dirichlet%20process%20hierarchical%20models%202008"
        },
        {
            "id": "Patterson_2013_a",
            "entry": "Patterson, S. and Teh, Y. W. (2013). Stochastic gradient Riemannian Langevin dynamics on the probability simplex. In Advances in Neural Information Processing Systems 26, pages 3102\u20133110.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patterson%2C%20S.%20Teh%2C%20Y.W.%20Stochastic%20gradient%20Riemannian%20Langevin%20dynamics%20on%20the%20probability%20simplex%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patterson%2C%20S.%20Teh%2C%20Y.W.%20Stochastic%20gradient%20Riemannian%20Langevin%20dynamics%20on%20the%20probability%20simplex%202013"
        },
        {
            "id": "Rosenblatt_1952_a",
            "entry": "Rosenblatt, M. (1952). Remarks on a multivariate transformation. The Annals of Mathematical Statistics, 23(3):470\u2013472.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenblatt%2C%20M.%20Remarks%20on%20a%20multivariate%20transformation%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosenblatt%2C%20M.%20Remarks%20on%20a%20multivariate%20transformation%201952"
        },
        {
            "id": "Sato_2014_a",
            "entry": "Sato, I. and Nakagawa, H. (2014). Approximation analysis of stochastic gradient Langevin dynamics by using Fokker-Planck equation and Ito process. In Proceedings of the 31st International Conference on Machine Learning, pages 982\u2013990. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sato%2C%20I.%20Nakagawa%2C%20H.%20Approximation%20analysis%20of%20stochastic%20gradient%20Langevin%20dynamics%20by%20using%20Fokker-Planck%20equation%20and%20Ito%20process%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sato%2C%20I.%20Nakagawa%2C%20H.%20Approximation%20analysis%20of%20stochastic%20gradient%20Langevin%20dynamics%20by%20using%20Fokker-Planck%20equation%20and%20Ito%20process%202014"
        },
        {
            "id": "Sethuraman_1994_a",
            "entry": "Sethuraman, J. (1994). A constructive definition of Dirichlet priors. Statistica Sinica, 4(2):639\u2013650.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sethuraman%2C%20J.%20A%20constructive%20definition%20of%20Dirichlet%20priors%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sethuraman%2C%20J.%20A%20constructive%20definition%20of%20Dirichlet%20priors%201994"
        },
        {
            "id": "Teh_et+al_2016_a",
            "entry": "Teh, Y. W., Thi\u00e9ry, A. H., and Vollmer, S. J. (2016). Consistency and fluctuations for stochastic gradient Langevin dynamics. Journal of Machine Learning Research, 17(7):1\u201333.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Y.W.%20Thi%C3%A9ry%2C%20A.H.%20Vollmer%2C%20S.J.%20Consistency%20and%20fluctuations%20for%20stochastic%20gradient%20Langevin%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Y.W.%20Thi%C3%A9ry%2C%20A.H.%20Vollmer%2C%20S.J.%20Consistency%20and%20fluctuations%20for%20stochastic%20gradient%20Langevin%20dynamics%202016"
        },
        {
            "id": "Vollmer_et+al_2016_a",
            "entry": "Vollmer, S. J., Zygalakis, K. C., and Teh, Y. W. (2016). Exploration of the (non-)asymptotic bias and variance of stochastic gradient Langevin dynamics. Journal of Machine Learning Research, 17(159):1\u201348.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vollmer%2C%20S.J.%20Zygalakis%2C%20K.C.%20Teh%2C%20Y.W.%20Exploration%20of%20the%20%28non-%29asymptotic%20bias%20and%20variance%20of%20stochastic%20gradient%20Langevin%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vollmer%2C%20S.J.%20Zygalakis%2C%20K.C.%20Teh%2C%20Y.W.%20Exploration%20of%20the%20%28non-%29asymptotic%20bias%20and%20variance%20of%20stochastic%20gradient%20Langevin%20dynamics%202016"
        },
        {
            "id": "Walker_2007_a",
            "entry": "Walker, S. G. (2007). Sampling the Dirichlet mixture model with slices. Communications in Statistics, 36(1):45\u201354.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20S.G.%20Sampling%20the%20Dirichlet%20mixture%20model%20with%20slices%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20S.G.%20Sampling%20the%20Dirichlet%20mixture%20model%20with%20slices%202007"
        },
        {
            "id": "Wallach_et+al_2009_a",
            "entry": "Wallach, H. M., Murray, I., Salakhutdinov, R., and Mimno, D. (2009). Evaluation methods for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1105\u20131112. PMLR. Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, pages 681\u2013688. PMLR. Zygalakis, K. C. (2011). On the existence and the applications of modified equations for stochastic differential equations. SIAM Journal on Scientific Computing, 33(1):102\u2013130.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wallach%2C%20H.M.%20Murray%2C%20I.%20Salakhutdinov%2C%20R.%20Mimno%2C%20D.%20Evaluation%20methods%20for%20topic%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wallach%2C%20H.M.%20Murray%2C%20I.%20Salakhutdinov%2C%20R.%20Mimno%2C%20D.%20Evaluation%20methods%20for%20topic%20models%202009"
        }
    ]
}
