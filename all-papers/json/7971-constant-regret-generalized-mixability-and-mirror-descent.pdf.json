{
    "filename": "7971-constant-regret-generalized-mixability-and-mirror-descent.pdf",
    "metadata": {
        "title": "Constant Regret, Generalized Mixability, and Mirror Descent",
        "author": "Zakaria Mhammedi, Robert C. Williamson",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7971-constant-regret-generalized-mixability-and-mirror-descent.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and for the right choice of loss function and \u201cmixing\u201d algorithm, it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example, a constant regret can be achieved for mixable losses using the aggregating algorithm. The Generalized Aggregating Algorithm (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the aggregating algorithm when using the Shannon entropy S. For a given entropy \u03a6, losses for which a constant regret is possible using the GAA are called \u03a6-mixable. Which losses are \u03a6-mixable was previously left as an open question. We fully characterize \u03a6-mixability and answer other open questions posed by [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]. We show that the Shannon entropy S is fundamental in nature when it comes to mixability; any \u03a6-mixable loss is necessarily S-mixable, and the lowest worst-case regret of the GAA is achieved using the Shannon entropy. Finally, by leveraging the connection between the mirror descent algorithm and the update step of the GAA, we suggest a new adaptive generalized aggregating algorithm and analyze its performance in terms of the regret bound."
    },
    "keywords": [
        {
            "term": "open question",
            "url": "https://en.wikipedia.org/wiki/open_question"
        },
        {
            "term": "shannon entropy",
            "url": "https://en.wikipedia.org/wiki/shannon_entropy"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "Two fundamental problems in learning are how to aggregate information and under what circumstances can one learn fast",
        "We extend these to allow for the use of losses which can take infinite values, and we show in this case that under the \u03a6-mixability condition a constant regret is achievable using the Generalized Aggregating Algorithm",
        "We introduce our new algorithm \u2014 the Aggregating Algorithm \u2014 and analyze its performance",
        "Contrary to what was conjectured previously [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], the generalized mixability condition does not induce a correspondence between losses and entropies; for a given loss , there is no particular entropy \u03a6 \u2014 specific to the choice of \u2014 which minimizes the regret of the Generalized Aggregating Algorithm",
        "Theorem 18 is consistent with Vovk\u2019s result [10, \u00a75] which essentially states that the regret bound RS = \u03b7\u22121 log k is in general tight for \u03b7-mixable losses.\n4 Adaptive Generalized Aggregating Algorithm",
        "We derived a characterization of \u03a6-mixability, which enables a better understanding of when a constant regret is achievable in the game of prediction with expert advice"
    ],
    "key_statements": [
        "Two fundamental problems in learning are how to aggregate information and under what circumstances can one learn fast",
        "We extend these to allow for the use of losses which can take infinite values, and we show in this case that under the \u03a6-mixability condition a constant regret is achievable using the Generalized Aggregating Algorithm",
        "We introduce our new algorithm \u2014 the Aggregating Algorithm \u2014 and analyze its performance",
        "Contrary to what was conjectured previously [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], the generalized mixability condition does not induce a correspondence between losses and entropies; for a given loss , there is no particular entropy \u03a6 \u2014 specific to the choice of \u2014 which minimizes the regret of the Generalized Aggregating Algorithm",
        "Theorem 18 is consistent with Vovk\u2019s result [10, \u00a75] which essentially states that the regret bound RS = \u03b7\u22121 log k is in general tight for \u03b7-mixable losses.\n4 Adaptive Generalized Aggregating Algorithm",
        "We derived a characterization of \u03a6-mixability, which enables a better understanding of when a constant regret is achievable in the game of prediction with expert advice",
        "A loss function defined on a set A and taking values in [0, +\u221e]n The finite part of the superprediction set of a loss The support loss of a loss The Bayes risk corresponding to a loss The composition of the Bayes risk with an affine function; L := L \u25e6 n) The Shannon Entropy) The mixability constant of ; essentially the largest \u03b7 s.t. is \u03b7-mixable"
    ],
    "summary": [
        "Two fundamental problems in learning are how to aggregate information and under what circumstances can one learn fast.",
        "We extend these to allow for the use of losses which can take infinite values, and we show in this case that under the \u03a6-mixability condition a constant regret is achievable using the GAA.",
        "The theorem extends this result by showing that the mixability constant \u03b7 of any loss is lower bounded by \u03b7 in (5), as long as satisfies Assumption 1 and its Bayes risk is twice differentiable.",
        "It was shown that under some regularity conditions on \u03a6, the GAA achieves a constant regret in the Gn(A, k) game for any finite, (\u03b7, \u03a6)-mixable loss.",
        "It should be noted that since the Bayes risk of a loss must be differentiable for it to be \u03a6-mixable for some entropy \u03a6, Theorem 5 says that we can essentially work with a proper support loss of .",
        "Contrary to what was conjectured previously [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], the generalized mixability condition does not induce a correspondence between losses and entropies; for a given loss , there is no particular entropy \u03a6 \u2014 specific to the choice of \u2014 which minimizes the regret of the GAA.",
        "Theorem 18 is consistent with Vovk\u2019s result [10, \u00a75] which essentially states that the regret bound RS = \u03b7\u22121 log k is in general tight for \u03b7-mixable losses.",
        "Given a (\u03b7, \u03a6)-mixable loss , we verify that Algorithm 3 is well defined; for simplicity, assume that dom = A and L is twice differentiable on ]0, +\u221e[n.",
        "Input :\u03b81 = 0 \u2208 Rk; A \u2206-differentiable entropy \u03a6 : Rk \u2192 R \u222a {+\u221e}; \u03b7 > 0; A (\u03b7, \u03a6)-mixable loss : A \u2192 [0, +\u221e[n; A substitution function S ; A protocol of choosing \u03b2t at round t.",
        "We derived a characterization of \u03a6-mixability, which enables a better understanding of when a constant regret is achievable in the game of prediction with expert advice.",
        "Vovk [10, \u00a75] essentially showed that given an \u03b7-mixable loss there is no algorithm that can achieve a lower regret bound than \u03b7\u22121 log k on all sequences of outcomes.",
        "A loss function defined on a set A and taking values in [0, +\u221e]n The finite part of the superprediction set of a loss The support loss of a loss The Bayes risk corresponding to a loss The composition of the Bayes risk with an affine function; L := L \u25e6 n) The Shannon Entropy) The mixability constant of ; essentially the largest \u03b7 s.t. is \u03b7-mixable.",
        "A substitution function of a loss The regret achieved by the GAA using entropy \u03a6 and Algorithm 2)"
    ],
    "headline": "We show that the Shannon entropy S is fundamental in nature when it comes to mixability; any \u03a6-mixable loss is necessarily S-mixable, and the lowest worst-case regret of the Generalized Aggregating Algorithm is achieved using the Shannon entropy",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alexey Chernov, Yuri Kalnishkan, Fedor Zhdanov, and Vladimir Vovk. Supermartingales in prediction with expert advice. Theoretical Computer Science, 411(29-30):2647\u20132669, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chernov%2C%20Alexey%20Kalnishkan%2C%20Yuri%20Zhdanov%2C%20Fedor%20Vovk%2C%20Vladimir%20Supermartingales%20in%20prediction%20with%20expert%20advice%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chernov%2C%20Alexey%20Kalnishkan%2C%20Yuri%20Zhdanov%2C%20Fedor%20Vovk%2C%20Vladimir%20Supermartingales%20in%20prediction%20with%20expert%20advice%202010"
        },
        {
            "id": "2",
            "entry": "[2] J-B. Hiriart-Urruty and C. Lemar\u00e9chal. Fundamentals of Convex Analysis. Springer, New York, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hiriart-Urruty%2C%20J.-B.%20Lemar%C3%A9chal%2C%20C.%20Fundamentals%20of%20Convex%20Analysis%202001"
        },
        {
            "id": "3",
            "entry": "[3] Yuri Kalnishkan, Volodya Vovk, and Michael V. Vyugin. Loss functions, complexities, and the Legendre transformation. Theoretical Computer Science, 313(2):195\u2013207, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalnishkan%2C%20Yuri%20Vovk%2C%20Volodya%20Vyugin%2C%20Michael%20V.%20Loss%20functions%2C%20complexities%2C%20and%20the%20Legendre%20transformation%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalnishkan%2C%20Yuri%20Vovk%2C%20Volodya%20Vyugin%2C%20Michael%20V.%20Loss%20functions%2C%20complexities%2C%20and%20the%20Legendre%20transformation%202004"
        },
        {
            "id": "4",
            "entry": "[4] Parameswaran Kamalaruban, Robert Williamson, and Xinhua Zhang. Exp-concavity of proper composite losses. In Conference on Learning Theory, pages 1035\u20131065, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamalaruban%2C%20Parameswaran%20Williamson%2C%20Robert%20Zhang%2C%20Xinhua%20Exp-concavity%20of%20proper%20composite%20losses%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamalaruban%2C%20Parameswaran%20Williamson%2C%20Robert%20Zhang%2C%20Xinhua%20Exp-concavity%20of%20proper%20composite%20losses%202015"
        },
        {
            "id": "5",
            "entry": "[5] Francesco Orabona, Koby Crammer, and Nicol\u00f2 Cesa-Bianchi. A generalized online mirror descent with applications to classification and regression. Machine Learning, 99(3):411\u2013435, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orabona%2C%20Francesco%20Crammer%2C%20Koby%20Cesa-Bianchi%2C%20Nicol%C3%B2%20A%20generalized%20online%20mirror%20descent%20with%20applications%20to%20classification%20and%20regression%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orabona%2C%20Francesco%20Crammer%2C%20Koby%20Cesa-Bianchi%2C%20Nicol%C3%B2%20A%20generalized%20online%20mirror%20descent%20with%20applications%20to%20classification%20and%20regression%202015"
        },
        {
            "id": "6",
            "entry": "[6] Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson, and Nishant Mehta. Generalized mixability via entropic duality. In Conference on Learning Theory, pages 1501\u20131522, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reid%2C%20Mark%20D.%20Frongillo%2C%20Rafael%20M.%20Williamson%2C%20Robert%20C.%20Mehta%2C%20Nishant%20Generalized%20mixability%20via%20entropic%20duality%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reid%2C%20Mark%20D.%20Frongillo%2C%20Rafael%20M.%20Williamson%2C%20Robert%20C.%20Mehta%2C%20Nishant%20Generalized%20mixability%20via%20entropic%20duality%202015"
        },
        {
            "id": "7",
            "entry": "[7] R. Tyrrell Rockafellar. Convex analysis. Princeton University Press, Princeton, NJ, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.Tyrrell%20Convex%20analysis%201997"
        },
        {
            "id": "8",
            "entry": "[8] Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In International Conference on Machine Learning, pages 1593\u20131601, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Adaptivity%20and%20optimism%3A%20An%20improved%20exponentiated%20gradient%20algorithm%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinhardt%2C%20Jacob%20Liang%2C%20Percy%20Adaptivity%20and%20optimism%3A%20An%20improved%20exponentiated%20gradient%20algorithm%202014"
        },
        {
            "id": "9",
            "entry": "[9] Tim van Erven, Mark D. Reid, and Robert C. Williamson. Mixability is Bayes risk curvature relative to log loss. Journal of Machine Learning Research, 13:1639\u20131663, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Erven%2C%20Tim%20Reid%2C%20Mark%20D.%20Williamson%2C%20Robert%20C.%20Mixability%20is%20Bayes%20risk%20curvature%20relative%20to%20log%20loss%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Erven%2C%20Tim%20Reid%2C%20Mark%20D.%20Williamson%2C%20Robert%20C.%20Mixability%20is%20Bayes%20risk%20curvature%20relative%20to%20log%20loss%202012"
        },
        {
            "id": "10",
            "entry": "[10] Vladimir Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences, 56(2):153\u2013173, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vovk%2C%20Vladimir%20A%20game%20of%20prediction%20with%20expert%20advice%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vovk%2C%20Vladimir%20A%20game%20of%20prediction%20with%20expert%20advice%201998"
        },
        {
            "id": "11",
            "entry": "[11] Vladimir Vovk and Fedor Zhdanov. Prediction with expert advice for the Brier game. Journal of Machine Learning Research, 10(Nov):2445\u20132471, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vovk%2C%20Vladimir%20Zhdanov%2C%20Fedor%20Prediction%20with%20expert%20advice%20for%20the%20Brier%20game%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vovk%2C%20Vladimir%20Zhdanov%2C%20Fedor%20Prediction%20with%20expert%20advice%20for%20the%20Brier%20game%202009"
        },
        {
            "id": "12",
            "entry": "[12] Volodya Vovk. Competitive on-line statistics. International Statistical Review, 69(2):213\u2013248, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vovk%2C%20Volodya%20Competitive%20on-line%20statistics%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vovk%2C%20Volodya%20Competitive%20on-line%20statistics%202001"
        },
        {
            "id": "13",
            "entry": "[13] Robert C. Williamson. The geometry of losses. In Conference on Learning Theory, pages 1078\u20131108, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williamson%2C%20Robert%20C.%20The%20geometry%20of%20losses%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williamson%2C%20Robert%20C.%20The%20geometry%20of%20losses%202014"
        },
        {
            "id": "14",
            "entry": "[14] Robert C. Williamson, Elodie Vernet, and Mark D. Reid. Composite multiclass losses. Journal of Machine Learning Research, 17(223):1\u201352, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williamson%2C%20Robert%20C.%20Vernet%2C%20Elodie%20Reid%2C%20Mark%20D.%20Composite%20multiclass%20losses%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williamson%2C%20Robert%20C.%20Vernet%2C%20Elodie%20Reid%2C%20Mark%20D.%20Composite%20multiclass%20losses%202016"
        }
    ]
}
