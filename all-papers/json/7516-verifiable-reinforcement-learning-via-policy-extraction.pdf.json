{
    "filename": "7516-verifiable-reinforcement-learning-via-policy-extraction.pdf",
    "metadata": {
        "title": "Verifiable Reinforcement Learning via Policy Extraction",
        "author": "Osbert Bastani, Yewen Pu, Armando Solar-Lezama",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7516-verifiable-reinforcement-learning-via-policy-extraction.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy."
    },
    "keywords": [
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "VIPER",
            "url": "https://en.wikipedia.org/wiki/VIPER"
        }
    ],
    "highlights": [
        "Deep reinforcement learning has proven to be a promising approach for automatically learning policies for control problems [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "We show how existing verification techniques can be adapted to efficiently verify desirable properties of extracted decision tree policies: (i) we learn a decision tree policy that plays Atari Pong [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] and verify its robustness [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], we learn a decision tree policy to play a toy game based on Pong, and prove that it never loses,3 and we learn a decision tree policy for cart-pole [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], and compute its region of stability around the goal state",
        "We propose a novel imitation learning algorithm called VIPER, which is based on DAGGER but leverages a Q-function for the oracle",
        "Another important direction is exploring whether we can automatically repair errors discovered in a decision tree policy",
        "Our decision tree policies may be useful for improving the efficiency of safe reinforcement learning algorithms that rely on verification"
    ],
    "key_statements": [
        "Deep reinforcement learning has proven to be a promising approach for automatically learning policies for control problems [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "We show how existing verification techniques can be adapted to efficiently verify desirable properties of extracted decision tree policies: (i) we learn a decision tree policy that plays Atari Pong [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] and verify its robustness [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], we learn a decision tree policy to play a toy game based on Pong, and prove that it never loses,3 and we learn a decision tree policy for cart-pole [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], and compute its region of stability around the goal state",
        "We propose a novel imitation learning algorithm called VIPER, which is based on DAGGER but leverages a Q-function for the oracle",
        "We show that VIPER learns relatively small decision trees (< 1000 nodes) that play perfectly on Atari Pong, a toy game based on Pong, and cart-pole",
        "We describe how to verify correctness, stability, and robustness of decision tree policies, and show that verification is orders of magnitude more scalable than approaches compatible with deep neural network policies",
        "There has been work training decision tree policies for reinforcement learning [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], but we find that their approach does not even scale to cart-pole",
        "We use policy gradients to train a deep neural network policy to play toy pong, which achieves a perfect reward of 250, which is the maximum number of time steps",
        "Achieving a perfect reward only requires that the pole does not fall below a given height, not stability; neither the extracted decision tree policy nor the original neural network policy are stable",
        "Another important direction is exploring whether we can automatically repair errors discovered in a decision tree policy",
        "Our decision tree policies may be useful for improving the efficiency of safe reinforcement learning algorithms that rely on verification"
    ],
    "summary": [
        "Deep reinforcement learning has proven to be a promising approach for automatically learning policies for control problems [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "Our goal is to learn policies for which desirable properties such as safety, stability, and robustness can be efficiently verified.",
        "We show that VIPER learns relatively small decision trees (< 1000 nodes) that play perfectly on Atari Pong, a toy game based on Pong, and cart-pole.",
        "We describe how to verify correctness, stability, and robustness of decision tree policies, and show that verification is orders of magnitude more scalable than approaches compatible with DNN policies.",
        "There has been work training decision tree policies for reinforcement learning [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], but we find that their approach does not even scale to cart-pole.",
        "Because the loss function for decision trees is not convex, there do not exist online learning algorithms with the theoretical guarantees required by DAGGER.",
        "Let \u21e1 be a decision tree whose leaf nodes are associated with linear functions of the state s.",
        "9 VIPER extracts a decision tree policy \u21e1 with 769 nodes that achieves perfect reward 21.0.",
        "We use policy gradients to train a DNN policy to play toy pong, which achieves a perfect reward of 250, which is the maximum number of time steps.",
        "We use VIPER as before to extract a decision tree policy.",
        "We tried to verify stability of the cart-pole controller, trained as before except without moving the cart to the right; as before, the decision tree achieves a perfect reward of 200.0.",
        "Achieving a perfect reward only requires that the pole does not fall below a given height, not stability; neither the extracted decision tree policy nor the original neural network policy are stable.",
        "We use continuous actions A = [ amax, amax], so we extract a (3 node) decision tree policy \u21e1 with linear regressors at the leaves; \u21e1 achieves a reward of 200.0.",
        "On the cart-pole benchmark, we compare VIPER to fitted Q iteration [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], which is an actor-critic algorithm that uses a decision tree policy that is retrained on",
        "We have proposed an approach to learning decision tree policies that can be verified efficiently.",
        "We used a number of approximations to verify correctness for the cart-pole controller; it may be possible to avoid these approximations, e.g., by finding an invariant set, and by using upper and lower piecewise linear bounds on transition function.",
        "Our decision tree policies may be useful for improving the efficiency of safe reinforcement learning algorithms that rely on verification."
    ],
    "headline": "We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies , yet can be efficiently verified using existing techniques ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "2",
            "entry": "[2] Anayo K Akametalu, Jaime F Fisac, Jeremy H Gillula, Shahab Kaynama, Melanie N Zeilinger, and Claire J Tomlin. Reachability-based safe learning with gaussian processes. In CDC, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akametalu%2C%20Anayo%20K.%20Fisac%2C%20Jaime%20F.%20Gillula%2C%20Jeremy%20H.%20Kaynama%2C%20Shahab%20Reachability-based%20safe%20learning%20with%20gaussian%20processes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akametalu%2C%20Anayo%20K.%20Fisac%2C%20Jaime%20F.%20Gillula%2C%20Jeremy%20H.%20Kaynama%2C%20Shahab%20Reachability-based%20safe%20learning%20with%20gaussian%20processes%202014"
        },
        {
            "id": "3",
            "entry": "[3] Anil Aswani, Humberto Gonzalez, S Shankar Sastry, and Claire Tomlin. Provably safe and robust learning-based model predictive control. Automatica, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aswani%2C%20Anil%20Humberto%20Gonzalez%2C%20S.Shankar%20Sastry%20Tomlin%2C%20Claire%20Provably%20safe%20and%20robust%20learning-based%20model%20predictive%20control%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aswani%2C%20Anil%20Humberto%20Gonzalez%2C%20S.Shankar%20Sastry%20Tomlin%2C%20Claire%20Provably%20safe%20and%20robust%20learning-based%20model%20predictive%20control%202013"
        },
        {
            "id": "4",
            "entry": "[4] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20Jimmy%20Caruana%2C%20Rich%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014"
        },
        {
            "id": "5",
            "entry": "[5] A. Barto, R. Sutton, and C. Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barto%2C%20A.%20Sutton%2C%20R.%20Anderson%2C%20C.%20Neuronlike%20adaptive%20elements%20that%20can%20solve%20difficult%20learning%20control%20problems.%20IEEE%20transactions%20on%20systems%2C%20man%2C%20and%20cybernetics%201983"
        },
        {
            "id": "6",
            "entry": "[6] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and Antonio Criminisi. Measuring neural net robustness with constraints. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bastani%2C%20Osbert%20Ioannou%2C%20Yani%20Lampropoulos%2C%20Leonidas%20Vytiniotis%2C%20Dimitrios%20Measuring%20neural%20net%20robustness%20with%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bastani%2C%20Osbert%20Ioannou%2C%20Yani%20Lampropoulos%2C%20Leonidas%20Vytiniotis%2C%20Dimitrios%20Measuring%20neural%20net%20robustness%20with%20constraints%202016"
        },
        {
            "id": "7",
            "entry": "[7] Osbert Bastani, Carolyn Kim, and Hamsa Bastani. Interpretability via model extraction. In FAT/ML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bastani%2C%20Osbert%20Kim%2C%20Carolyn%20Bastani%2C%20Hamsa%20Interpretability%20via%20model%20extraction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bastani%2C%20Osbert%20Kim%2C%20Carolyn%20Bastani%2C%20Hamsa%20Interpretability%20via%20model%20extraction%202017"
        },
        {
            "id": "8",
            "entry": "[8] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berkenkamp%2C%20Felix%20Turchetta%2C%20Matteo%20Schoellig%2C%20Angela%20Krause%2C%20Andreas%20Safe%20model-based%20reinforcement%20learning%20with%20stability%20guarantees%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berkenkamp%2C%20Felix%20Turchetta%2C%20Matteo%20Schoellig%2C%20Angela%20Krause%2C%20Andreas%20Safe%20model-based%20reinforcement%20learning%20with%20stability%20guarantees%202017"
        },
        {
            "id": "9",
            "entry": "[9] Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. Classification and Regression Trees. Wadsworth, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Friedman%2C%20Jerome%20Olshen%2C%20Richard%20Stone%2C%20Charles%20Classification%20and%20Regression%20Trees%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Friedman%2C%20Jerome%20Olshen%2C%20Richard%20Stone%2C%20Charles%20Classification%20and%20Regression%20Trees%201984"
        },
        {
            "id": "10",
            "entry": "[10] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bucilua%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bucilua%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006"
        },
        {
            "id": "11",
            "entry": "[11] Steve Collins, Andy Ruina, Russ Tedrake, and Martijn Wisse. Efficient bipedal robots based on passivedynamic walkers. Science, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collins%2C%20Steve%20Ruina%2C%20Andy%20Tedrake%2C%20Russ%20Wisse%2C%20Martijn%20Efficient%20bipedal%20robots%20based%20on%20passivedynamic%20walkers%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collins%2C%20Steve%20Ruina%2C%20Andy%20Tedrake%2C%20Russ%20Wisse%2C%20Martijn%20Efficient%20bipedal%20robots%20based%20on%20passivedynamic%20walkers%202005"
        },
        {
            "id": "12",
            "entry": "[12] Leonardo De Moura and Nikolaj Bj\u00f8rner. Z3: An efficient smt solver. In TACAS, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moura%2C%20Leonardo%20De%20Bj%C3%B8rner%2C%20Nikolaj%20Z3%3A%20An%20efficient%20smt%20solver%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moura%2C%20Leonardo%20De%20Bj%C3%B8rner%2C%20Nikolaj%20Z3%3A%20An%20efficient%20smt%20solver%202008"
        },
        {
            "id": "13",
            "entry": "[13] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. JMLR, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-based%20batch%20mode%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ernst%2C%20Damien%20Geurts%2C%20Pierre%20Wehenkel%2C%20Louis%20Tree-based%20batch%20mode%20reinforcement%20learning%202005"
        },
        {
            "id": "14",
            "entry": "[14] Javier Garc\u0131a and Fernando Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. JMLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garc%C4%B1a%2C%20Javier%20Fern%C3%A1ndez%2C%20Fernando%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garc%C4%B1a%2C%20Javier%20Fern%C3%A1ndez%2C%20Fernando%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015"
        },
        {
            "id": "15",
            "entry": "[15] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai 2: Safety and robustness certification of neural networks with abstract interpretation. In IEEE Security & Privacy, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehr%2C%20Timon%20Mirman%2C%20Matthew%20Drachsler-Cohen%2C%20Dana%20Tsankov%2C%20Petar%20Ai%202%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehr%2C%20Timon%20Mirman%2C%20Matthew%20Drachsler-Cohen%2C%20Dana%20Tsankov%2C%20Petar%20Ai%202%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%202018"
        },
        {
            "id": "16",
            "entry": "[16] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "17",
            "entry": "[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning Workshop, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeff%20Distilling%20the%20knowledge%20in%20a%20neural%20network%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeff%20Distilling%20the%20knowledge%20in%20a%20neural%20network%202014"
        },
        {
            "id": "18",
            "entry": "[18] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In CAV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Xiaowei%20Kwiatkowska%2C%20Marta%20Wang%2C%20Sen%20Wu%2C%20Min%20Safety%20verification%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Xiaowei%20Kwiatkowska%2C%20Marta%20Wang%2C%20Sen%20Wu%2C%20Min%20Safety%20verification%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "19",
            "entry": "[19] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In CAV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katz%2C%20Guy%20Barrett%2C%20Clark%20Dill%2C%20David%20L.%20Julian%2C%20Kyle%20Reluplex%3A%20An%20efficient%20smt%20solver%20for%20verifying%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katz%2C%20Guy%20Barrett%2C%20Clark%20Dill%2C%20David%20L.%20Julian%2C%20Kyle%20Reluplex%3A%20An%20efficient%20smt%20solver%20for%20verifying%20deep%20neural%20networks%202017"
        },
        {
            "id": "20",
            "entry": "[20] Scott Kuindersma, Robin Deits, Maurice Fallon, Andr\u00e9s Valenzuela, Hongkai Dai, Frank Permenter, Twan Koolen, Pat Marion, and Russ Tedrake. Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot. Autonomous Robots, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuindersma%2C%20Scott%20Deits%2C%20Robin%20Fallon%2C%20Maurice%20Valenzuela%2C%20Andr%C3%A9s%20Optimization-based%20locomotion%20planning%2C%20estimation%2C%20and%20control%20design%20for%20the%20atlas%20humanoid%20robot%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuindersma%2C%20Scott%20Deits%2C%20Robin%20Fallon%2C%20Maurice%20Valenzuela%2C%20Andr%C3%A9s%20Optimization-based%20locomotion%20planning%2C%20estimation%2C%20and%20control%20design%20for%20the%20atlas%20humanoid%20robot%202016"
        },
        {
            "id": "21",
            "entry": "[21] Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pages 1\u20139, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Guided%20policy%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Koltun%2C%20Vladlen%20Guided%20policy%20search%202013"
        },
        {
            "id": "22",
            "entry": "[22] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "23",
            "entry": "[23] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In ICML, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moldovan%2C%20Teodor%20Mihai%20Abbeel%2C%20Pieter%20Safe%20exploration%20in%20markov%20decision%20processes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moldovan%2C%20Teodor%20Mihai%20Abbeel%2C%20Pieter%20Safe%20exploration%20in%20markov%20decision%20processes%202012"
        },
        {
            "id": "24",
            "entry": "[24] Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization. PhD thesis, California Institute of Technology, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pablo%20A%20Parrilo.%20Structured%20semidefinite%20programs%20and%20semialgebraic%20geometry%20methods%20in%20robustness%20and%20optimization%202000"
        },
        {
            "id": "25",
            "entry": "[25] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20S.%20Gordon%2C%20G.%20Bagnell%2C%20D.%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20S.%20Gordon%2C%20G.%20Bagnell%2C%20D.%20A%20reduction%20of%20imitation%20learning%20and%20structured%20prediction%20to%20no-regret%20online%20learning%202011"
        },
        {
            "id": "26",
            "entry": "[26] Dorsa Sadigh, S Shankar Sastry, Sanjit A Seshia, and Anca Dragan. Information gathering actions over human internal state. In IROS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dorsa%20Sadigh%2C%20S.Shankar%20Sastry%20Seshia%2C%20Sanjit%20A.%20Dragan%2C%20Anca%20Information%20gathering%20actions%20over%20human%20internal%20state%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dorsa%20Sadigh%2C%20S.Shankar%20Sastry%20Seshia%2C%20Sanjit%20A.%20Dragan%2C%20Anca%20Information%20gathering%20actions%20over%20human%20internal%20state%202016"
        },
        {
            "id": "27",
            "entry": "[27] Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaal%2C%20Stefan%20Is%20imitation%20learning%20the%20route%20to%20humanoid%20robots%3F%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schaal%2C%20Stefan%20Is%20imitation%20learning%20the%20route%20to%20humanoid%20robots%3F%201999"
        },
        {
            "id": "28",
            "entry": "[28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "29",
            "entry": "[29] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "30",
            "entry": "[30] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "31",
            "entry": "[31] Russ Tedrake. Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tedrake%2C%20Russ%20Underactuated%20Robotics%3A%20Algorithms%20for%20Walking%2C%20Running%2C%20Swimming%2C%20Flying%2C%20and%20Manipulation%202018"
        },
        {
            "id": "32",
            "entry": "[32] Russ Tedrake, Ian R Manchester, Mark Tobenkin, and John W Roberts. Lqr-trees: Feedback motion planning via sums-of-squares verification. IJRR, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tedrake%2C%20Russ%20Manchester%2C%20Ian%20R.%20Tobenkin%2C%20Mark%20Roberts%2C%20John%20W.%20Lqr-trees%3A%20Feedback%20motion%20planning%20via%20sums-of-squares%20verification%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tedrake%2C%20Russ%20Manchester%2C%20Ian%20R.%20Tobenkin%2C%20Mark%20Roberts%2C%20John%20W.%20Lqr-trees%3A%20Feedback%20motion%20planning%20via%20sums-of-squares%20verification%202010"
        },
        {
            "id": "33",
            "entry": "[33] Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in finite markov decision processes with gaussian processes. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Turchetta%2C%20Matteo%20Berkenkamp%2C%20Felix%20Krause%2C%20Andreas%20Safe%20exploration%20in%20finite%20markov%20decision%20processes%20with%20gaussian%20processes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Turchetta%2C%20Matteo%20Berkenkamp%2C%20Felix%20Krause%2C%20Andreas%20Safe%20exploration%20in%20finite%20markov%20decision%20processes%20with%20gaussian%20processes%202016"
        },
        {
            "id": "34",
            "entry": "[34] Gilles Vandewiele, Olivier Janssens, Femke Ongenae, Filip De Turck, and Sofie Van Hoecke. Genesim: genetic extraction of a single, interpretable model. In NIPS Workshop on Interpretable Machine Learning in Complex Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vandewiele%2C%20Gilles%20Janssens%2C%20Olivier%20Ongenae%2C%20Femke%20Turck%2C%20Filip%20De%20Genesim%3A%20genetic%20extraction%20of%20a%20single%2C%20interpretable%20model%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vandewiele%2C%20Gilles%20Janssens%2C%20Olivier%20Ongenae%2C%20Femke%20Turck%2C%20Filip%20De%20Genesim%3A%20genetic%20extraction%20of%20a%20single%2C%20interpretable%20model%202016"
        },
        {
            "id": "35",
            "entry": "[35] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. arXiv preprint arXiv:1804.02477, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02477"
        },
        {
            "id": "36",
            "entry": "[36] Yifan Wu, Roshan Shariff, Tor Lattimore, and Csaba Szepesv\u00e1ri. Conservative bandits. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yifan%20Shariff%2C%20Roshan%20Lattimore%2C%20Tor%20Szepesv%C3%A1ri%2C%20Csaba%20Conservative%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yifan%20Shariff%2C%20Roshan%20Lattimore%2C%20Tor%20Szepesv%C3%A1ri%2C%20Csaba%20Conservative%20bandits%202016"
        },
        {
            "id": "37",
            "entry": "[37] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        }
    ]
}
