{
    "filename": "7324-deep-defense-training-dnns-with-improved-adversarial-robustness.pdf",
    "metadata": {
        "title": "Deep Defense: Training DNNs with Improved Adversarial Robustness",
        "author": "Ziang Yan1* Yiwen Guo2,1* Changshui Zhang1 1Institute for Artificial Intelligence, Tsinghua University (THUAI),",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7324-deep-defense-training-dnns-with-improved-adversarial-robustness.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named \u201cdeep defense\u201d. Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at https://github.com/ZiangYan/deepdefense.pytorch."
    },
    "keywords": [
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "real image",
            "url": "https://en.wikipedia.org/wiki/real_image"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        },
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "deep defense",
            "url": "https://en.wikipedia.org/wiki/deep_defense"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Deep neural networks (DNNs) have advanced the state-of-the-art of many challenging computer vision tasks, they are vulnerable to adversarial examples [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>].<br/><br/>A general way of synthesizing the adversarial examples is to apply worst-case perturbations to real images [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Extensive experiments on MNIST, CIFAR-10 and ImageNet show that our method significantly improves the robustness of different deep neural networks under advanced adversarial attacks, in the no accuracy degradation is observed",
        "Our main results are summarized in Table 1, where the fourth column demonstrates the inference accuracy of different models on benign test images, the fifth column compares the robustness of different models to DeepFool adversarial examples, and the subsequent columns compare the robustness to",
        "We investigate the vulnerability of deep neural networks to adversarial examples and propose a novel method to address it, by incorporating an adversarial perturbation-based regularization into the classification objective",
        "Extensive experiments on MNIST, CIFAR-10 and ImageNet have shown the effectiveness of our method"
    ],
    "key_statements": [
        "Deep neural networks (DNNs) have advanced the state-of-the-art of many challenging computer vision tasks, they are vulnerable to adversarial examples [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>].<br/><br/>A general way of synthesizing the adversarial examples is to apply worst-case perturbations to real images [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Extensive experiments on MNIST, CIFAR-10 and ImageNet show that our method significantly improves the robustness of different deep neural networks under advanced adversarial attacks, in the no accuracy degradation is observed",
        "Our main results are summarized in Table 1, where the fourth column demonstrates the inference accuracy of different models on benign test images, the fifth column compares the robustness of different models to DeepFool adversarial examples, and the subsequent columns compare the robustness to",
        "We investigate the vulnerability of deep neural networks to adversarial examples and propose a novel method to address it, by incorporating an adversarial perturbation-based regularization into the classification objective",
        "Extensive experiments on MNIST, CIFAR-10 and ImageNet have shown the effectiveness of our method"
    ],
    "summary": [
        "Deep neural networks (DNNs) have advanced the state-of-the-art of many challenging computer vision tasks, they are vulnerable to adversarial examples [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>].<br/><br/>A general way of synthesizing the adversarial examples is to apply worst-case perturbations to real images [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "We introduce \u201cdeep defense\u201d, an adversarial regularization method to train DNNs with improved robustness.",
        "Extensive experiments on MNIST, CIFAR-10 and ImageNet show that our method significantly improves the robustness of different DNNs under advanced adversarial attacks, in the no accuracy degradation is observed.",
        "As Kurakin et al [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] have reported, it may result in a decreased benign-set accuracy on large-scale datasets like ImageNet. An alternative way of defending such attacks is to train a detector, to detect and reject adversarial examples.",
        "Many methods regularize the learning objective of DNNs approximately, which may lead to a degraded prediction accuracy on the benign test sets or unsatisfactory robustness to advanced adversarial examples.",
        "Problem (3) integrates an adversarial perturbation-based regularization into the classification objective, which should endow parameterized models with the ability of learning from adversarial attacks and resisting them.",
        "We test Goodfellow et al.\u2019s adversarial training objective [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] and compare it with our method intensively, considering there exists trade-offs between accuracies on benign and adversarial examples.",
        "We conduct the famous FGS and DeepFool as representatives of l\u221e and l2 attacks and compare the robustness of obtained models using different defense methods.",
        "Regularized models and denote it as ref , test prediction accuracies of those models produced by adversarial and Parseval training at this level of perturbation.",
        "We use identical fine-tuning policies and hyper-parameters for different defense methods We cut the learning rate by 2\u00d7 after four epochs of training because it can be beneficial for convergence.",
        "We see Deep Defense consistently and significantly outperforms competitive methods in the sense of both robustness and accuracy, even though our implementation of Adv. Train I achieves slightly better results than those reported in [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "We testify the effectiveness of our method on several different benchmark networks on the CIFAR-10 and ImageNet datasets.",
        "When compared with the reference models, our regularized models achieve higher test-set accuracies on benign examples and gain absolute error decreases of 26.15% and 16.44% under the FGS attack.",
        "After only 10 epochs of fine-tuning for AlexNet and 1 epoch for ResNet, we achieve roughly 1.5\u00d7 improved robustness to the DeepFool attack on both architectures, along with a slightly increased benign-set accuracy, highlighting the effectiveness of our method.",
        "We investigate the vulnerability of DNNs to adversarial examples and propose a novel method to address it, by incorporating an adversarial perturbation-based regularization into the classification objective.",
        "Future works shall include explorations on resisting black-box attacks and attacks in the physical world"
    ],
    "headline": "We propose a training recipe named \u201cdeep defense\u201d",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alemi%2C%20Alexander%20A.%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Murphy%2C%20Kevin%20Deep%20variational%20information%20bottleneck%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alemi%2C%20Alexander%20A.%20Fischer%2C%20Ian%20Dillon%2C%20Joshua%20V.%20Murphy%2C%20Kevin%20Deep%20variational%20information%20bottleneck%202017"
        },
        {
            "id": "2",
            "entry": "[2] Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buckman%2C%20Jacob%20Roy%2C%20Aurko%20Raffel%2C%20Colin%20Goodfellow%2C%20Ian%20Thermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buckman%2C%20Jacob%20Roy%2C%20Aurko%20Raffel%2C%20Colin%20Goodfellow%2C%20Ian%20Thermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%202018"
        },
        {
            "id": "3",
            "entry": "[3] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In ACM Workshop on Artificial Intelligence and Security, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017"
        },
        {
            "id": "4",
            "entry": "[4] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (SP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017"
        },
        {
            "id": "6",
            "entry": "[6] Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhillon%2C%20Guneet%20S.%20Azizzadenesheli%2C%20Kamyar%20Lipton%2C%20Zachary%20C.%20Bernstein%2C%20Jeremy%20and%20Anima%20Anandkumar.%20Stochastic%20activation%20pruning%20for%20robust%20adversarial%20defense%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhillon%2C%20Guneet%20S.%20Azizzadenesheli%2C%20Kamyar%20Lipton%2C%20Zachary%20C.%20Bernstein%2C%20Jeremy%20and%20Anima%20Anandkumar.%20Stochastic%20activation%20pruning%20for%20robust%20adversarial%20defense%202018"
        },
        {
            "id": "7",
            "entry": "[7] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016"
        },
        {
            "id": "8",
            "entry": "[8] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "9",
            "entry": "[9] Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. In ICLR Workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shixiang%20Rigazio%2C%20Luca%20Towards%20deep%20neural%20network%20architectures%20robust%20to%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shixiang%20Rigazio%2C%20Luca%20Towards%20deep%20neural%20network%20architectures%20robust%20to%20adversarial%20examples%202015"
        },
        {
            "id": "10",
            "entry": "[10] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%202016"
        },
        {
            "id": "11",
            "entry": "[11] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "12",
            "entry": "[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "13",
            "entry": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "14",
            "entry": "[14] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "15",
            "entry": "[15] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1207.0580"
        },
        {
            "id": "16",
            "entry": "[16] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "17",
            "entry": "[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "18",
            "entry": "[18] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20Bengio%2C%20Samy%20Adversarial%20machine%20learning%20at%20scale%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20Bengio%2C%20Samy%20Adversarial%20machine%20learning%20at%20scale%202017"
        },
        {
            "id": "19",
            "entry": "[19] Yann LeCun, Patrick Haffner, L\u00e9on Bottou, and Yoshua Bengio. Object recognition with gradient-based learning. Shape, contour and grouping in computer vision, pages 823\u2013823, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Haffner%2C%20Patrick%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Object%20recognition%20with%20gradient-based%20learning.%20Shape%2C%20contour%20and%20grouping%20in%20computer%20vision%201999"
        },
        {
            "id": "20",
            "entry": "[20] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Min%20Lin%20Qiang%20Chen%20and%20Shuicheng%20Yan%20Network%20in%20network%20In%20ICLR%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Min%20Lin%20Qiang%20Chen%20and%20Shuicheng%20Yan%20Network%20in%20network%20In%20ICLR%202014"
        },
        {
            "id": "21",
            "entry": "[21] Jiajun Lu, Theerasit Issaranon, and David Forsyth. Safetynet: Detecting and rejecting adversarial examples robustly. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Jiajun%20Issaranon%2C%20Theerasit%20Forsyth%2C%20David%20Safetynet%3A%20Detecting%20and%20rejecting%20adversarial%20examples%20robustly%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Jiajun%20Issaranon%2C%20Theerasit%20Forsyth%2C%20David%20Safetynet%3A%20Detecting%20and%20rejecting%20adversarial%20examples%20robustly%202017"
        },
        {
            "id": "22",
            "entry": "[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "23",
            "entry": "[23] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzen%2C%20Jan%20Hendrik%20Genewein%2C%20Tim%20Fischer%2C%20Volker%20Bischoff%2C%20Bastian%20On%20detecting%20adversarial%20perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzen%2C%20Jan%20Hendrik%20Genewein%2C%20Tim%20Fischer%2C%20Volker%20Bischoff%2C%20Bastian%20On%20detecting%20adversarial%20perturbations%202017"
        },
        {
            "id": "24",
            "entry": "[24] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03976"
        },
        {
            "id": "25",
            "entry": "[25] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Universal%20adversarial%20perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Fawzi%2C%20Omar%20Frossard%2C%20Pascal%20Universal%20adversarial%20perturbations%202017"
        },
        {
            "id": "26",
            "entry": "[26] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple and accurate method to fool deep neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "27",
            "entry": "[27] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noh%2C%20Hyeonwoo%20Hong%2C%20Seunghoon%20Han%2C%20Bohyung%20Learning%20deconvolution%20network%20for%20semantic%20segmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noh%2C%20Hyeonwoo%20Hong%2C%20Seunghoon%20Han%2C%20Bohyung%20Learning%20deconvolution%20network%20for%20semantic%20segmentation%202015"
        },
        {
            "id": "28",
            "entry": "[28] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Asia Conference on Computer and Communications Security, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "29",
            "entry": "[29] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of security and privacy in machine learning. In IEEE European Symposium on Security and Privacy, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nicolas%20Papernot%20Patrick%20McDaniel%20Arunesh%20Sinha%20and%20Michael%20Wellman%20Towards%20the%20science%20of%20security%20and%20privacy%20in%20machine%20learning%20In%20IEEE%20European%20Symposium%20on%20Security%20and%20Privacy%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nicolas%20Papernot%20Patrick%20McDaniel%20Arunesh%20Sinha%20and%20Michael%20Wellman%20Towards%20the%20science%20of%20security%20and%20privacy%20in%20machine%20learning%20In%20IEEE%20European%20Symposium%20on%20Security%20and%20Privacy%202018"
        },
        {
            "id": "30",
            "entry": "[30] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy (SP), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "31",
            "entry": "[31] Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Andrew%20Slavin%20Ross%20and%20Finale%20Doshi-Velez.%20Improving%20the%20adversarial%20robustness%20and%20interpretability%20of%20deep%20neural%20networks%20by%20regularizing%20their%20input%20gradients%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Andrew%20Slavin%20Ross%20and%20Finale%20Doshi-Velez.%20Improving%20the%20adversarial%20robustness%20and%20interpretability%20of%20deep%20neural%20networks%20by%20regularizing%20their%20input%20gradients%202018"
        },
        {
            "id": "32",
            "entry": "[32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "33",
            "entry": "[33] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep neural networks. IEEE Transactions on Signal Processing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sokolic%2C%20Jure%20Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Rodrigues%2C%20Miguel%20R.D.%20Robust%20large%20margin%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sokolic%2C%20Jure%20Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Rodrigues%2C%20Miguel%20R.D.%20Robust%20large%20margin%20deep%20neural%20networks%202017"
        },
        {
            "id": "34",
            "entry": "[34] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "35",
            "entry": "[35] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Ren%2C%20Zhou%20Mitigating%20adversarial%20effects%20through%20randomization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Ren%2C%20Zhou%20Mitigating%20adversarial%20effects%20through%20randomization%202018"
        },
        {
            "id": "36",
            "entry": "[36] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Zhou%2C%20Yuyin%20Adversarial%20examples%20for%20semantic%20segmentation%20and%20object%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Cihang%20Wang%2C%20Jianyu%20Zhang%2C%20Zhishuai%20Zhou%2C%20Yuyin%20Adversarial%20examples%20for%20semantic%20segmentation%20and%20object%20detection%202017"
        },
        {
            "id": "37",
            "entry": "[37] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391\u2013423, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huan%20Mannor%2C%20Shie%20Robustness%20and%20generalization%202012"
        },
        {
            "id": "38",
            "entry": "[38] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeiler%2C%20Matthew%20D.%20Fergus%2C%20Rob%20Visualizing%20and%20understanding%20convolutional%20networks%202014"
        }
    ]
}
