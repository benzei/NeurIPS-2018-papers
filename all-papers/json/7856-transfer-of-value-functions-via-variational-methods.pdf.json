{
    "filename": "7856-transfer-of-value-functions-via-variational-methods.pdf",
    "metadata": {
        "date": 2018,
        "title": "Transfer of Value Functions via Variational Methods",
        "author": "Andrea Tirinzoni\u2217 Politecnico di Milano andrea.tirinzoni@polimi.it",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7856-transfer-of-value-functions-via-variational-methods.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider the problem of transferring value functions in reinforcement learning. We propose an approach that uses the given source tasks to learn a prior distribution over optimal value functions and provide an efficient variational approximation of the corresponding posterior in a new target task. We show our approach to be general, in the sense that it can be combined with complex parametric function approximators and distribution models, while providing two practical algorithms based on Gaussians and Gaussian mixtures. We theoretically analyze them by deriving a finite-sample analysis and provide a comprehensive empirical evaluation in four different domains."
    },
    "keywords": [
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "Gaussian processes",
            "url": "https://en.wikipedia.org/wiki/Gaussian_processes"
        },
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "learning process",
            "url": "https://en.wikipedia.org/wiki/learning_process"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "highlights": [
        "Recent advancements have allowed reinforcement learning (RL) [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] to achieve impressive results in a wide variety of complex tasks, ranging from Atari [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] through the game of Go [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] to the control of sophisticated robotics systems [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "One of the most promising solutions to alleviate this problem is transfer learning, which focuses on reusing past knowledge available to the agent in order to reduce the sample-complexity for learning new tasks",
        "This reuse of knowledge constitutes a significant advantage over plain reinforcement learning, where the agent learns each new task from scratch independently of any previous learning experience",
        "We presented a variational method for transferring value functions in reinforcement learning",
        "We showed our approach to be general, in the sense that it can be combined with several distributions and function approximators, while providing two practical algorithms based on Gaussians and mixtures of Gaussians, respectively",
        "We evaluated the proposed algorithms in different domains, showing that both achieve excellent performance in simple tasks, while only the mixture version is able to handle complex environments"
    ],
    "key_statements": [
        "Recent advancements have allowed reinforcement learning (RL) [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] to achieve impressive results in a wide variety of complex tasks, ranging from Atari [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] through the game of Go [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] to the control of sophisticated robotics systems [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "The main limitation is that these reinforcement learning algorithms still require an enormous amount of experience samples before successfully learning such complicated tasks",
        "One of the most promising solutions to alleviate this problem is transfer learning, which focuses on reusing past knowledge available to the agent in order to reduce the sample-complexity for learning new tasks",
        "In the typical settings of transfer in reinforcement learning [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>], the agent is assumed to have already solved a set of source tasks generated from some unknown distribution",
        "Given a target task, the agent can rely on knowledge from the source tasks to speed up the learning process",
        "This reuse of knowledge constitutes a significant advantage over plain reinforcement learning, where the agent learns each new task from scratch independently of any previous learning experience",
        "Several algorithms have been proposed in the literature to transfer different elements involved in the learning process: experience samples [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>], policies/options [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], rewards [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], features [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], parameters [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "[<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] considers the problem of jointly learning all given tasks, while we focus on transferring information from a set of source tasks to the target task",
        "Both approaches result in a model-based algorithm that quickly adapts to new tasks by estimating their hidden parameters, while we propose a model-free method which does not require such assumptions",
        "In order to provide a better understanding of the differences between our settings and the ones typically considered in fast-adaptation algorithms, we show a comparison to the recently proposed metalearner MAML [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "We presented a variational method for transferring value functions in reinforcement learning",
        "We showed our approach to be general, in the sense that it can be combined with several distributions and function approximators, while providing two practical algorithms based on Gaussians and mixtures of Gaussians, respectively",
        "We evaluated the proposed algorithms in different domains, showing that both achieve excellent performance in simple tasks, while only the mixture version is able to handle complex environments"
    ],
    "summary": [
        "Recent advancements have allowed reinforcement learning (RL) [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] to achieve impressive results in a wide variety of complex tasks, ranging from Atari [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] through the game of Go [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] to the control of sophisticated robotics systems [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "We use this distribution as a prior for learning the target task and we propose a variational approximation of the corresponding posterior that is computationally efficient.",
        "Notice the generality of the proposed approach: as far as the objective L is differentiable in the variational parameters \u03be, and its gradients can be efficiently computed, any approximator for the Q-function and any prior/posterior distributions can be adopted.",
        "Our algorithm is likely to start from Q-functions that are very close to an optimum and aims only to adapt the weights in some direction of lower error so as to quickly converge to the solution of the target task.",
        "We assume only a finite dataset is available and provide a finite-sample analysis bounding the expected Bellman error under the variational distribution minimizing the objective (2) for any fixed target task M\u03c4 .",
        "We both assume the tasks to share similarities in their value functions, [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] consider only linear approximators and adopt a hierarchical Bayesian model of the corresponding weights\u2019 distribution, which is assumed Gaussian.",
        "[<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] propose a Dirichlet process model for the case where weights cluster into different classes, which relates to our mixture formulation and proves the importance of capturing more complicated task distributions.",
        "Both approaches result in a model-based algorithm that quickly adapts to new tasks by estimating their hidden parameters, while we propose a model-free method which does not require such assumptions.",
        "Our goal is to speed-up the learning process of a new target task from D by transferring only these data, without requiring additional source tasks or experience samples from them.",
        "We generate a set of 50 source tasks for the three-room environment of Figure 1 by sampling both door locations uniformly in the allowed space, and solve all of them by directly minimizing the TD error as presented in Section 3.4.",
        "We generate 20 source tasks by uniformly sampling their physical parameters and solve them by directly minimizing the TD error as in the previous experiment.",
        "We showed our approach to be general, in the sense that it can be combined with several distributions and function approximators, while providing two practical algorithms based on Gaussians and mixtures of Gaussians, respectively.",
        "Our variational approach could be extended to model a distribution over optimal policies instead of value functions, which might allow better transferred behavior.",
        "We evaluated the proposed algorithms in different domains, showing that both achieve excellent performance in simple tasks, while only the mixture version is able to handle complex environments"
    ],
    "headline": "We propose an approach that uses the given source tasks to learn a prior distribution over optimal value functions and provide an efficient variational approximation of the corresponding posterior in a new target task",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory. In Proceedings of the 35th International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amit%2C%20Ron%20Meir%2C%20Ron%20Meta-learning%20by%20adjusting%20priors%20based%20on%20extended%20PAC-Bayes%20theory%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amit%2C%20Ron%20Meir%2C%20Ron%20Meta-learning%20by%20adjusting%20priors%20based%20on%20extended%20PAC-Bayes%20theory%202018"
        },
        {
            "id": "2",
            "entry": "[2] Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning. In International Conference on Machine Learning, pages 243\u2013252, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asadi%2C%20Kavosh%20Littman%2C%20Michael%20L.%20An%20alternative%20softmax%20operator%20for%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asadi%2C%20Kavosh%20Littman%2C%20Michael%20L.%20An%20alternative%20softmax%20operator%20for%20reinforcement%20learning%202017"
        },
        {
            "id": "3",
            "entry": "[3] Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration through bayesian deep q-networks. arXiv preprint arXiv:1802.04412, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04412"
        },
        {
            "id": "4",
            "entry": "[4] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995, pages 30\u201337.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baird%2C%20Leemon%20Residual%20algorithms%3A%20Reinforcement%20learning%20with%20function%20approximation%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baird%2C%20Leemon%20Residual%20algorithms%3A%20Reinforcement%20learning%20with%20function%20approximation%201995"
        },
        {
            "id": "5",
            "entry": "[5] Andr\u00e9 Barreto, Will Dabney, R\u00e9mi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pages 4055\u20134065, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barreto%2C%20Andr%C3%A9%20Dabney%2C%20Will%20Munos%2C%20R%C3%A9mi%20Hunt%2C%20Jonathan%20J.%20Successor%20features%20for%20transfer%20in%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barreto%2C%20Andr%C3%A9%20Dabney%2C%20Will%20Munos%2C%20R%C3%A9mi%20Hunt%2C%20Jonathan%20J.%20Successor%20features%20for%20transfer%20in%20reinforcement%20learning%202017"
        },
        {
            "id": "6",
            "entry": "[6] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "7",
            "entry": "[7] S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "8",
            "entry": "[8] Olivier Catoni. Pac-bayesian supervised classification: the thermodynamics of statistical learning. arXiv preprint arXiv:0712.0248, 2007.",
            "arxiv_url": "https://arxiv.org/pdf/0712.0248"
        },
        {
            "id": "9",
            "entry": "[9] Finale Doshi-Velez and George Konidaris. Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations. In IJCAI: proceedings of the conference, volume 2016, page 1432, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Finale%20Doshi-Velez%20and%20George%20Konidaris.%20Hidden%20parameter%20markov%20decision%20processes%3A%20A%20semiparametric%20regression%20approach%20for%20discovering%20latent%20task%20parametrizations",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Finale%20Doshi-Velez%20and%20George%20Konidaris.%20Hidden%20parameter%20markov%20decision%20processes%3A%20A%20semiparametric%20regression%20approach%20for%20discovering%20latent%20task%20parametrizations"
        },
        {
            "id": "10",
            "entry": "[10] Fernando Fern\u00e1ndez and Manuela Veloso. Probabilistic policy reuse in a reinforcement learning agent. In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 720\u2013727. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fern%C3%A1ndez%2C%20Fernando%20Veloso%2C%20Manuela%20Probabilistic%20policy%20reuse%20in%20a%20reinforcement%20learning%20agent%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fern%C3%A1ndez%2C%20Fernando%20Veloso%2C%20Manuela%20Probabilistic%20policy%20reuse%20in%20a%20reinforcement%20learning%20agent%202006"
        },
        {
            "id": "11",
            "entry": "[11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "12",
            "entry": "[12] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.08930"
        },
        {
            "id": "13",
            "entry": "[13] John R Hershey and Peder A Olsen. Approximating the kullback leibler divergence between gaussian mixture models. In Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hershey%2C%20John%20R.%20Olsen%2C%20Peder%20A.%20Approximating%20the%20kullback%20leibler%20divergence%20between%20gaussian%20mixture%20models%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hershey%2C%20John%20R.%20Olsen%2C%20Peder%20A.%20Approximating%20the%20kullback%20leibler%20divergence%20between%20gaussian%20mixture%20models%202007"
        },
        {
            "id": "14",
            "entry": "[14] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "15",
            "entry": "[15] Taylor W Killian, Samuel Daulton, George Konidaris, and Finale Doshi-Velez. Robust and efficient transfer learning with hidden parameter markov decision processes. In Advances in Neural Information Processing Systems, pages 6250\u20136261, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Killian%2C%20Taylor%20W.%20Daulton%2C%20Samuel%20Konidaris%2C%20George%20Doshi-Velez%2C%20Finale%20Robust%20and%20efficient%20transfer%20learning%20with%20hidden%20parameter%20markov%20decision%20processes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Killian%2C%20Taylor%20W.%20Daulton%2C%20Samuel%20Konidaris%2C%20George%20Doshi-Velez%2C%20Finale%20Robust%20and%20efficient%20transfer%20learning%20with%20hidden%20parameter%20markov%20decision%20processes%202017"
        },
        {
            "id": "16",
            "entry": "[16] Jens Kober and Jan R Peters. Policy search for motor primitives in robotics. In Advances in neural information processing systems, pages 849\u2013856, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kober%2C%20Jens%20Peters%2C%20Jan%20R.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kober%2C%20Jens%20Peters%2C%20Jan%20R.%20Policy%20search%20for%20motor%20primitives%20in%20robotics%202009"
        },
        {
            "id": "17",
            "entry": "[17] George Konidaris and Andrew Barto. Autonomous shaping: Knowledge transfer in reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pages 489\u2013496. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20George%20Barto%2C%20Andrew%20Autonomous%20shaping%3A%20Knowledge%20transfer%20in%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20George%20Barto%2C%20Andrew%20Autonomous%20shaping%3A%20Knowledge%20transfer%20in%20reinforcement%20learning%202006"
        },
        {
            "id": "18",
            "entry": "[18] George Konidaris and Andrew G Barto. Building portable options: Skill transfer in reinforcement learning. 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20George%20Barto%2C%20Andrew%20G.%20Building%20portable%20options%3A%20Skill%20transfer%20in%20reinforcement%20learning%202007"
        },
        {
            "id": "19",
            "entry": "[19] Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement Learning. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaric%2C%20Alessandro%20Transfer%20in%20reinforcement%20learning%3A%20a%20framework%20and%20a%20survey.%20In%20Reinforcement%20Learning%202012"
        },
        {
            "id": "20",
            "entry": "[20] Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In ICML-27th International Conference on Machine Learning, pages 599\u2013606. Omnipress, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaric%2C%20Alessandro%20Ghavamzadeh%2C%20Mohammad%20Bayesian%20multi-task%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazaric%2C%20Alessandro%20Ghavamzadeh%2C%20Mohammad%20Bayesian%20multi-task%20reinforcement%20learning%202010"
        },
        {
            "id": "21",
            "entry": "[21] Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch reinforcement learning. In Proceedings of the 25th international conference on Machine learning, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaric%2C%20Alessandro%20Restelli%2C%20Marcello%20Bonarini%2C%20Andrea%20Transfer%20of%20samples%20in%20batch%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazaric%2C%20Alessandro%20Restelli%2C%20Marcello%20Bonarini%2C%20Andrea%20Transfer%20of%20samples%20in%20batch%20reinforcement%20learning%202008"
        },
        {
            "id": "22",
            "entry": "[22] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "23",
            "entry": "[23] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "24",
            "entry": "[24] Odalric-Ambrym Maillard, R\u00e9mi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh. Finitesample analysis of bellman residual minimization. In Proceedings of 2nd Asian Conference on Machine Learning, pages 299\u2013314, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maillard%2C%20Odalric-Ambrym%20Munos%2C%20R%C3%A9mi%20Lazaric%2C%20Alessandro%20Ghavamzadeh%2C%20Mohammad%20Finitesample%20analysis%20of%20bellman%20residual%20minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maillard%2C%20Odalric-Ambrym%20Munos%2C%20R%C3%A9mi%20Lazaric%2C%20Alessandro%20Ghavamzadeh%2C%20Mohammad%20Finitesample%20analysis%20of%20bellman%20residual%20minimization%202010"
        },
        {
            "id": "25",
            "entry": "[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "26",
            "entry": "[26] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. arXiv preprint arXiv:1402.0635, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0635"
        },
        {
            "id": "27",
            "entry": "[27] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY, USA, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20Martin%20L.%20Markov%20Decision%20Processes%3A%20Discrete%20Stochastic%20Dynamic%20Programming%201994"
        },
        {
            "id": "28",
            "entry": "[28] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "29",
            "entry": "[29] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05952"
        },
        {
            "id": "30",
            "entry": "[30] David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley & Sons, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20David%20W.%20Multivariate%20density%20estimation%3A%20theory%2C%20practice%2C%20and%20visualization%202015"
        },
        {
            "id": "31",
            "entry": "[31] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "32",
            "entry": "[32] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "33",
            "entry": "[33] Matthew E Taylor, Nicholas K Jong, and Peter Stone. Transferring instances for model-based reinforcement learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 488\u2013505.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Matthew%20E.%20Jong%2C%20Nicholas%20K.%20Stone%2C%20Peter%20Transferring%20instances%20for%20model-based%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Matthew%20E.%20Jong%2C%20Nicholas%20K.%20Stone%2C%20Peter%20Transferring%20instances%20for%20model-based%20reinforcement%20learning"
        },
        {
            "id": "34",
            "entry": "[34] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633\u20131685, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Matthew%20E.%20Stone%2C%20Peter%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Matthew%20E.%20Stone%2C%20Peter%20Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20A%20survey%202009"
        },
        {
            "id": "35",
            "entry": "[35] Andrea Tirinzoni, Andrea Sessa, Matteo Pirotta, and Marcello Restelli. Importance weighted transfer of samples in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4936\u20134945. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tirinzoni%2C%20Andrea%20Sessa%2C%20Andrea%20Pirotta%2C%20Matteo%20Restelli%2C%20Marcello%20Importance%20weighted%20transfer%20of%20samples%20in%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tirinzoni%2C%20Andrea%20Sessa%2C%20Andrea%20Pirotta%2C%20Matteo%20Restelli%2C%20Marcello%20Importance%20weighted%20transfer%20of%20samples%20in%20reinforcement%20learning%202018"
        },
        {
            "id": "36",
            "entry": "[36] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hasselt%2C%20Hado%20Van%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20q-learning%202016"
        },
        {
            "id": "37",
            "entry": "[37] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine learning, pages 1015\u20131022. ACM, 2007. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Aaron%20Fern%2C%20Alan%20Ray%2C%20Soumya%20Tadepalli%2C%20Prasad%20Multi-task%20reinforcement%20learning%3A%20a%20hierarchical%20bayesian%20approach%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Aaron%20Fern%2C%20Alan%20Ray%2C%20Soumya%20Tadepalli%2C%20Prasad%20Multi-task%20reinforcement%20learning%3A%20a%20hierarchical%20bayesian%20approach%202007"
        }
    ]
}
