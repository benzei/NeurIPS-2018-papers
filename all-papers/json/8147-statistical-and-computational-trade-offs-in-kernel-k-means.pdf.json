{
    "filename": "8147-statistical-and-computational-trade-offs-in-kernel-k-means.pdf",
    "metadata": {
        "title": "Statistical and Computational Trade-Offs in Kernel K-Means",
        "author": "Daniele Calandriello, Lorenzo Rosasco",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8147-statistical-and-computational-trade-offs-in-kernel-k-means.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We investigate the efficiency of k-means in terms of both statistical and computational requirements. More precisely, we study a Nystr\u00f6m approach to kernel k-means. We analyze the statistical properties of the proposed method and show that it achieves the same accuracy of exact kernel k-means with only a fract\u221aion of computations. Indeed, we prove under basic assumptions that sampling n Nystr\u00f6m landmarks allows to greatly reduce computational costs without incurring in any loss of accuracy. To the best of our knowledge this is the first result of this kind for unsupervised learning."
    },
    "keywords": [
        {
            "term": "kernel method",
            "url": "https://en.wikipedia.org/wiki/kernel_method"
        },
        {
            "term": "space x",
            "url": "https://en.wikipedia.org/wiki/space_x"
        },
        {
            "term": "empirical risk minimizer",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimizer"
        },
        {
            "term": "hilbert space",
            "url": "https://en.wikipedia.org/wiki/hilbert_space"
        }
    ],
    "highlights": [
        "We replace the true feature map \u03c6(\u00b7) and points \u03c6i = \u03c6 \u2208 H with a finite-dimensional approximation \u03c6i = \u03c6 \u2208 Rm. Nystr\u00f6m kernel k-means Given a dataset D, we denote with I = {\u03c6j}m j=1 a dictionary of m points \u03c6j from D, and with \u03a6m : Rm \u2192 H the map with these points as columns",
        "Exploiting the error bound for \u03b3-preserving dictionaries we are ready for the main result of this paper: showing that we can improve the computational aspect of kernel k-means using Nystr\u00f6m embedding, while maintaining optimal generalization guarantees",
        "None of these optimization based methods focus on the underlying excess risk problem, and their analysis cannot be integrated in existing results, as the approximate minimum found has no clear interpretation as a statistical empirical risk minimizer",
        "We provided guarantees for Cn,m, that this the empirical risk minimizer in Hm"
    ],
    "key_statements": [
        "We replace the true feature map \u03c6(\u00b7) and points \u03c6i = \u03c6 \u2208 H with a finite-dimensional approximation \u03c6i = \u03c6 \u2208 Rm. Nystr\u00f6m kernel k-means Given a dataset D, we denote with I = {\u03c6j}m j=1 a dictionary of m points \u03c6j from D, and with \u03a6m : Rm \u2192 H the map with these points as columns",
        "Exploiting the error bound for \u03b3-preserving dictionaries we are ready for the main result of this paper: showing that we can improve the computational aspect of kernel k-means using Nystr\u00f6m embedding, while maintaining optimal generalization guarantees",
        "None of these optimization based methods focus on the underlying excess risk problem, and their analysis cannot be integrated in existing results, as the approximate minimum found has no clear interpretation as a statistical empirical risk minimizer",
        "We provided guarantees for Cn,m, that this the empirical risk minimizer in Hm"
    ],
    "summary": [
        "We replace the true feature map \u03c6(\u00b7) and points \u03c6i = \u03c6 \u2208 H with a finite-dimensional approximation \u03c6i = \u03c6 \u2208 Rm. Nystr\u00f6m kernel k-means Given a dataset D, we denote with I = {\u03c6j}m j=1 a dictionary of m points \u03c6j from D, and with \u03a6m : Rm \u2192 H the map with these points as columns.",
        "Using uniform sampling we can reduce the size of the search space Hm by a 1/\u03b3 factor in exchange for a \u03b3 additive error, resulting in a computation/approximation trade-off that is linear in \u03b3.",
        "Exploiting the error bound for \u03b3-preserving dictionaries we are ready for the main result of this paper: showing that we can improve the computational aspect of kernel k-means using Nystr\u00f6m embedding, while maintaining optimal generalization guarantees.",
        "In the full generality of our setting this is practically optimal, since computing a n-preserving dictionary is in general as hard as matrix multiplication [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], which requires \u03a9(n2) time.",
        "None of these optimization based methods focus on the underlying excess risk problem, and their analysis cannot be integrated in existing results, as the approximate minimum found has no clear interpretation as a statistical ERM.",
        "No existing embedding based methods can guarantee at the same time optimal excess risk rates and a reduction in the size of Hm, and a reduction in computational cost.",
        "In the 1/ n risk rate setting Gaussian projections would require \u03bd = 1/ n resulting in m \u2265 n log(n) random features, which would not bring any improvement over the exact embedding ki.",
        "Under the hood, traditional embedding methods such as those based on JL lemma, usually provide only bounds of the form \u03a0n \u2212 \u03a0m \u03b3\u03a0n, and an error \u03a0\u22a5m\u03c6i 2 \u2264 \u03b3 \u03c6i 2.",
        "As we already remarked for [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], to achieve the 1/ n excess risk rate using a multiplicative error bound we would require an unreasonably small \u03bd, resulting in a large m that brings no computational improvement over the exact solution.",
        "Note that computing RLSs exactly requires constructing Kn and O(n2) time and space, but in recent years a number of fast approximate RLSs sampling methods [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] have emerged that can construct \u03b3-preserving dictionaries of size O) in just O2) time."
    ],
    "headline": "We investigate the efficiency of k-means in terms of both statistical and computational requirements",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Nir Ailon, Ragesh Jaiswal, and Claire Monteleoni. Streaming k-means approximation. In Advances in neural information processing systems, pages 10\u201318, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ailon%2C%20Nir%20Jaiswal%2C%20Ragesh%20Monteleoni%2C%20Claire%20Streaming%20k-means%20approximation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ailon%2C%20Nir%20Jaiswal%2C%20Ragesh%20Monteleoni%2C%20Claire%20Streaming%20k-means%20approximation%202009"
        },
        {
            "id": "2",
            "entry": "[2] M. A. Aizerman, E. A. Braverman, and L. Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. In Automation and Remote Control,, number 25 in Automation and Remote Control\u201e pages 821\u2013837, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aizerman%2C%20M.A.%20Braverman%2C%20E.A.%20Rozonoer%2C%20L.%20Theoretical%20foundations%20of%20the%20potential%20function%20method%20in%20pattern%20recognition%20learning%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aizerman%2C%20M.A.%20Braverman%2C%20E.A.%20Rozonoer%2C%20L.%20Theoretical%20foundations%20of%20the%20potential%20function%20method%20in%20pattern%20recognition%20learning%201964"
        },
        {
            "id": "3",
            "entry": "[3] Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel methods with statistical guarantees. In Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alaoui%2C%20Ahmed%20El%20Mahoney%2C%20Michael%20W.%20Fast%20randomized%20kernel%20methods%20with%20statistical%20guarantees%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alaoui%2C%20Ahmed%20El%20Mahoney%2C%20Michael%20W.%20Fast%20randomized%20kernel%20methods%20with%20statistical%20guarantees%202015"
        },
        {
            "id": "4",
            "entry": "[4] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of euclidean sum-of-squares clustering. Machine learning, 75(2):245\u2013248, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aloise%2C%20Daniel%20Deshpande%2C%20Amit%20Hansen%2C%20Pierre%20Popat%2C%20Preyas%20Np-hardness%20of%20euclidean%20sum-of-squares%20clustering%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aloise%2C%20Daniel%20Deshpande%2C%20Amit%20Hansen%2C%20Pierre%20Popat%2C%20Preyas%20Np-hardness%20of%20euclidean%20sum-of-squares%20clustering%202009"
        },
        {
            "id": "5",
            "entry": "[5] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arthur%2C%20David%20Vassilvitskii%2C%20Sergei%20k-means%2B%2B%3A%20The%20advantages%20of%20careful%20seeding%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arthur%2C%20David%20Vassilvitskii%2C%20Sergei%20k-means%2B%2B%3A%20The%20advantages%20of%20careful%20seeding%202007"
        },
        {
            "id": "6",
            "entry": "[6] Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. Random Fourier features for kernel ridge regression: Approximation bounds and statistical guarantees. In Proceedings of the 34th International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avron%2C%20Haim%20Kapralov%2C%20Michael%20Musco%2C%20Cameron%20Musco%2C%20Christopher%20Random%20Fourier%20features%20for%20kernel%20ridge%20regression%3A%20Approximation%20bounds%20and%20statistical%20guarantees%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avron%2C%20Haim%20Kapralov%2C%20Michael%20Musco%2C%20Cameron%20Musco%2C%20Christopher%20Random%20Fourier%20features%20for%20kernel%20ridge%20regression%3A%20Approximation%20bounds%20and%20statistical%20guarantees%202017"
        },
        {
            "id": "7",
            "entry": "[7] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on Learning Theory, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Sharp%20analysis%20of%20low-rank%20kernel%20matrix%20approximations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Sharp%20analysis%20of%20low-rank%20kernel%20matrix%20approximations%202013"
        },
        {
            "id": "8",
            "entry": "[8] Francis R Bach and Michael I Jordan. Predictive low-rank decomposition for kernel methods. In Proceedings of the 22nd international conference on Machine learning, pages 33\u201340. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20R.%20Jordan%2C%20Michael%20I.%20Predictive%20low-rank%20decomposition%20for%20kernel%20methods%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20R.%20Jordan%2C%20Michael%20I.%20Predictive%20low-rank%20decomposition%20for%20kernel%20methods%202005"
        },
        {
            "id": "9",
            "entry": "[9] Arturs Backurs, Piotr Indyk, and Ludwig Schmidt. On the fine-grained complexity of empirical risk minimization: Kernel methods and neural networks. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Backurs%2C%20Arturs%20Indyk%2C%20Piotr%20Schmidt%2C%20Ludwig%20On%20the%20fine-grained%20complexity%20of%20empirical%20risk%20minimization%3A%20Kernel%20methods%20and%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Backurs%2C%20Arturs%20Indyk%2C%20Piotr%20Schmidt%2C%20Ludwig%20On%20the%20fine-grained%20complexity%20of%20empirical%20risk%20minimization%3A%20Kernel%20methods%20and%20neural%20networks%202017"
        },
        {
            "id": "10",
            "entry": "[10] G\u00e9rard Biau, Luc Devroye, and G\u00e1bor Lugosi. On the performance of clustering in hilbert spaces. IEEE Transactions on Information Theory, 54(2):781\u2013790, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%C3%A9rard%20Biau%2C%20Luc%20Devroye%20Lugosi%2C%20G%C3%A1bor%20On%20the%20performance%20of%20clustering%20in%20hilbert%20spaces%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%C3%A9rard%20Biau%2C%20Luc%20Devroye%20Lugosi%2C%20G%C3%A1bor%20On%20the%20performance%20of%20clustering%20in%20hilbert%20spaces%202008"
        },
        {
            "id": "11",
            "entry": "[11] Daniele Calandriello. Efficient Sequential Learning in Structured and Constrained Environments. PhD thesis, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calandriello%2C%20Daniele%20Efficient%20Sequential%20Learning%20in%20Structured%20and%20Constrained%20Environments%202017"
        },
        {
            "id": "12",
            "entry": "[12] Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Second-order kernel online convex optimization with adaptive sketching. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calandriello%2C%20Daniele%20Lazaric%2C%20Alessandro%20Valko%2C%20Michal%20Second-order%20kernel%20online%20convex%20optimization%20with%20adaptive%20sketching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calandriello%2C%20Daniele%20Lazaric%2C%20Alessandro%20Valko%2C%20Michal%20Second-order%20kernel%20online%20convex%20optimization%20with%20adaptive%20sketching%202017"
        },
        {
            "id": "13",
            "entry": "[13] Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Distributed sequential sampling for kernel matrix approximation. In AISTATS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calandriello%2C%20Daniele%20Lazaric%2C%20Alessandro%20Valko%2C%20Michal%20Distributed%20sequential%20sampling%20for%20kernel%20matrix%20approximation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calandriello%2C%20Daniele%20Lazaric%2C%20Alessandro%20Valko%2C%20Michal%20Distributed%20sequential%20sampling%20for%20kernel%20matrix%20approximation%202017"
        },
        {
            "id": "14",
            "entry": "[14] Guillermo Canas, Tomaso Poggio, and Lorenzo Rosasco. Learning manifolds with k-means and k-flats. In Advances in Neural Information Processing Systems, pages 2465\u20132473, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Canas%2C%20Guillermo%20Poggio%2C%20Tomaso%20Rosasco%2C%20Lorenzo%20Learning%20manifolds%20with%20k-means%20and%20k-flats%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Canas%2C%20Guillermo%20Poggio%2C%20Tomaso%20Rosasco%2C%20Lorenzo%20Learning%20manifolds%20with%20k-means%20and%20k-flats%202012"
        },
        {
            "id": "15",
            "entry": "[15] Radha Chitta, Rong Jin, Timothy C. Havens, and Anil K. Jain. Approximate kernel k-means: Solution to large scale kernel clustering. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 895\u2013903. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chitta%2C%20Radha%20Jin%2C%20Rong%20Havens%2C%20Timothy%20C.%20Jain%2C%20Anil%20K.%20Approximate%20kernel%20k-means%3A%20Solution%20to%20large%20scale%20kernel%20clustering%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chitta%2C%20Radha%20Jin%2C%20Rong%20Havens%2C%20Timothy%20C.%20Jain%2C%20Anil%20K.%20Approximate%20kernel%20k-means%3A%20Solution%20to%20large%20scale%20kernel%20clustering%202011"
        },
        {
            "id": "16",
            "entry": "[16] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. A unified view of kernel k-means, spectral clustering and graph cuts. Technical Report No. UTCS TR-04-25, University of Texas at Austin, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhillon%2C%20Inderjit%20S.%20Guan%2C%20Yuqiang%20Kulis%2C%20Brian%20A%20unified%20view%20of%20kernel%20k-means%2C%20spectral%20clustering%20and%20graph%20cuts%202004"
        },
        {
            "id": "17",
            "entry": "[17] Siegfried Graf and Harald Luschgy. Foundations of Quantization for Probability Distributions. Lecture Notes in Mathematics. Springer-Verlag, Berlin Heidelberg, 2000. ISBN 978-3-54067394-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graf%2C%20Siegfried%20Luschgy%2C%20Harald%20Foundations%20of%20Quantization%20for%20Probability%20Distributions%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graf%2C%20Siegfried%20Luschgy%2C%20Harald%20Foundations%20of%20Quantization%20for%20Probability%20Distributions%202000"
        },
        {
            "id": "18",
            "entry": "[18] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary Mathematics, 26, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984"
        },
        {
            "id": "19",
            "entry": "[19] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "20",
            "entry": "[20] Cl\u00e9ment Levrard et al. Nonasymptotic bounds for vector quantization in hilbert spaces. The Annals of Statistics, 43(2):592\u2013619, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levrard%2C%20Cl%C3%A9ment%20Nonasymptotic%20bounds%20for%20vector%20quantization%20in%20hilbert%20spaces%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levrard%2C%20Cl%C3%A9ment%20Nonasymptotic%20bounds%20for%20vector%20quantization%20in%20hilbert%20spaces%202015"
        },
        {
            "id": "21",
            "entry": "[21] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28 (2):129\u2013137, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lloyd%2C%20Stuart%20Least%20squares%20quantization%20in%20pcm.%20IEEE%20transactions%20on%20information%20theory%201982"
        },
        {
            "id": "22",
            "entry": "[22] Ga\u00eblle Loosli, St\u00e9phane Canu, and L\u00e9on Bottou. Training invariant support vector machines using selective sampling. In Large Scale Kernel Machines, pages 301\u2013320. MIT Press, Cambridge, MA., 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ga%C3%ABlle%20Loosli%2C%20St%C3%A9phane%20Canu%20Bottou%2C%20L%C3%A9on%20Training%20invariant%20support%20vector%20machines%20using%20selective%20sampling%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ga%C3%ABlle%20Loosli%2C%20St%C3%A9phane%20Canu%20Bottou%2C%20L%C3%A9on%20Training%20invariant%20support%20vector%20machines%20using%20selective%20sampling%202007"
        },
        {
            "id": "23",
            "entry": "[23] Andreas Maurer and Massimiliano Pontil. $ K $-dimensional coding schemes in Hilbert spaces. IEEE Transactions on Information Theory, 56(11):5839\u20135846, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20Andreas%20K%2C%20Massimiliano%20Pontil.%20%24%20%24-dimensional%20coding%20schemes%20in%20Hilbert%20spaces%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20K%2C%20Massimiliano%20Pontil.%20%24%20%24-dimensional%20coding%20schemes%20in%20Hilbert%20spaces%202010"
        },
        {
            "id": "24",
            "entry": "[24] Cameron Musco and Christopher Musco. Recursive Sampling for the Nystr\u00f6m Method. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Recursive%20Sampling%20for%20the%20Nystr%C3%B6m%20Method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Recursive%20Sampling%20for%20the%20Nystr%C3%B6m%20Method%202017"
        },
        {
            "id": "25",
            "entry": "[25] Cameron Musco and David Woodruff. Is input sparsity time possible for kernel low-rank approximation? In Advances in Neural Information Processing Systems 30. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Musco%2C%20Cameron%20Woodruff%2C%20David%20Is%20input%20sparsity%20time%20possible%20for%20kernel%20low-rank%20approximation%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Musco%2C%20Cameron%20Woodruff%2C%20David%20Is%20input%20sparsity%20time%20possible%20for%20kernel%20low-rank%20approximation%3F%202017"
        },
        {
            "id": "26",
            "entry": "[26] Dino Oglic and Thomas G\u00e4rtner. Nystr\u00f6m method with kernel k-means++ samples as landmarks. Journal of Machine Learning Research, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oglic%2C%20Dino%20G%C3%A4rtner%2C%20Thomas%20Nystr%C3%B6m%20method%20with%20kernel%20k-means%2B%2B%20samples%20as%20landmarks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oglic%2C%20Dino%20G%C3%A4rtner%2C%20Thomas%20Nystr%C3%B6m%20method%20with%20kernel%20k-means%2B%2B%20samples%20as%20landmarks%202017"
        },
        {
            "id": "27",
            "entry": "[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011"
        },
        {
            "id": "28",
            "entry": "[28] Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. In Neural Information Processing Systems, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Ben%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Ben%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "29",
            "entry": "[29] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pages 3218\u20133228, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "30",
            "entry": "[30] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystr\u00f6m computational regularization. In Advances in Neural Information Processing Systems, pages 1657\u20131665, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015"
        },
        {
            "id": "31",
            "entry": "[31] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholkopf%2C%20Bernhard%20Smola%2C%20Alexander%20J.%20Learning%20with%20kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202001"
        },
        {
            "id": "32",
            "entry": "[32] Bernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299\u20131319, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20Bernhard%20Smola%2C%20Alexander%20M%C3%BCller%2C%20Klaus-Robert%20Nonlinear%20component%20analysis%20as%20a%20kernel%20eigenvalue%20problem%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%B6lkopf%2C%20Bernhard%20Smola%2C%20Alexander%20M%C3%BCller%2C%20Klaus-Robert%20Nonlinear%20component%20analysis%20as%20a%20kernel%20eigenvalue%20problem%201998"
        },
        {
            "id": "33",
            "entry": "[33] Joel A Tropp, Alp Yurtsever, Madeleine Udell, and Volkan Cevher. Fixed-rank approximation of a positive-semidefinite matrix from streaming data. In Advances in Neural Information Processing Systems, pages 1225\u20131234, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20Yurtsever%2C%20Alp%20Udell%2C%20Madeleine%20Cevher%2C%20Volkan%20Fixed-rank%20approximation%20of%20a%20positive-semidefinite%20matrix%20from%20streaming%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20Yurtsever%2C%20Alp%20Udell%2C%20Madeleine%20Cevher%2C%20Volkan%20Fixed-rank%20approximation%20of%20a%20positive-semidefinite%20matrix%20from%20streaming%20data%202017"
        },
        {
            "id": "34",
            "entry": "[34] Shusen Wang, Alex Gittens, and Michael W Mahoney. Scalable kernel k-means clustering with nystrom approximation: Relative-error bounds. Journal of Machine Learning Research, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shusen%20Gittens%2C%20Alex%20Mahoney%2C%20Michael%20W.%20Scalable%20kernel%20k-means%20clustering%20with%20nystrom%20approximation%3A%20Relative-error%20bounds%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shusen%20Gittens%2C%20Alex%20Mahoney%2C%20Michael%20W.%20Scalable%20kernel%20k-means%20clustering%20with%20nystrom%20approximation%3A%20Relative-error%20bounds%202018"
        },
        {
            "id": "35",
            "entry": "[35] Christopher Williams and Matthias Seeger. Using the Nystrom method to speed up kernel machines. In Neural Information Processing Systems, 2001. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20Seeger%2C%20Matthias%20Using%20the%20Nystrom%20method%20to%20speed%20up%20kernel%20machines%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Christopher%20Seeger%2C%20Matthias%20Using%20the%20Nystrom%20method%20to%20speed%20up%20kernel%20machines%202001"
        }
    ]
}
