{
    "filename": "8232-pca-of-high-dimensional-random-walks-with-comparison-to-neural-network-training.pdf",
    "metadata": {
        "date": 2018,
        "title": "PCA of high dimensional random walks with comparison to neural network training",
        "author": "Joseph M. Antognini\u2217 Whisper AI",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8232-pca-of-high-dimensional-random-walks-with-comparison-to-neural-network-training.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "One technique to visualize the training of neural networks is to perform PCA on the parameters over the course of training and to project to the subspace spanned by the first few PCA components. In this paper we compare this technique to the PCA of a high dimensional random walk. We compute the eigenvalues and eigenvectors of the covariance of the trajectory and prove that in the long trajectory and high dimensional limit most of the variance is in the first few PCA components, and that the projection of the trajectory onto any subspace spanned by PCA components is a Lissajous curve. We generalize these results to a random walk with momentum and to an Ornstein-Uhlenbeck processes (i.e., a random walk in a quadratic potential) and show that in high dimensions the walk is not mean reverting, but will instead be trapped at a fixed distance from the minimum. We finally analyze PCA projected training trajectories for: a linear model trained on CIFAR-10; a fully connected model trained on MNIST; and ResNet-50-v2 trained on Imagenet. In all cases, both the distribution of PCA eigenvalues and the projected trajectories resemble those of a random walk with drift."
    },
    "keywords": [
        {
            "term": "principal component analysis",
            "url": "https://en.wikipedia.org/wiki/principal_component_analysis"
        },
        {
            "term": "lissajous curve",
            "url": "https://en.wikipedia.org/wiki/lissajous_curve"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "random walk",
            "url": "https://en.wikipedia.org/wiki/random_walk"
        },
        {
            "term": "quadratic potential",
            "url": "https://en.wikipedia.org/wiki/quadratic_potential"
        },
        {
            "term": "image recognition",
            "url": "https://en.wikipedia.org/wiki/image_recognition"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        }
    ],
    "highlights": [
        "If we consider the limit of a large number of steps, S asymptotically approaches a circulant matrix S that is equal to S in every entry except the top right, where there appears a \u22121 instead of a 0.3",
        "We show that principal component analysis projections of random walks in flat space qualitatively have many of the same properties as projections of neural networks training trajectories",
        "We extend their contribution by proving that the trajectories of high dimensional random walk in principal component analysis subspaces are Lissajous curves and generalizing to random walks with momentum and Ornstein-Uhlenbeck processes",
        "In Fig. 1 we plot the trajectories of a high dimensional random walk projected to various principal component analysis components and compare to the corresponding Lissajous curves",
        "First phase the process will beha\u221ave from the origin will increase proportionally to n as a and random walk in flat space and the distance the variance of the kth principal component analysis component will be proportional to k\u22122",
        "We have argued that the principal component analysis projected trajectory of a random walk in a general quadratic potential will be dominated by the dimensions with the smallest curvatures where they will appear similar to a random walk in flat space"
    ],
    "key_statements": [
        "If we consider the limit of a large number of steps, S asymptotically approaches a circulant matrix S that is equal to S in every entry except the top right, where there appears a \u22121 instead of a 0.3",
        "We show that principal component analysis projections of random walks in flat space qualitatively have many of the same properties as projections of neural networks training trajectories",
        "We extend their contribution by proving that the trajectories of high dimensional random walk in principal component analysis subspaces are Lissajous curves and generalizing to random walks with momentum and Ornstein-Uhlenbeck processes",
        "In Fig. 1 we plot the trajectories of a high dimensional random walk projected to various principal component analysis components and compare to the corresponding Lissajous curves",
        "First phase the process will beha\u221ave from the origin will increase proportionally to n as a and random walk in flat space and the distance the variance of the kth principal component analysis component will be proportional to k\u22122",
        "We show in Fig. 3 the distribution of the principal component analysis variances at the beginning, middle, and end of training for both models and compare to the distribution of variances from an infinite dimensional random walk",
        "The trajectories appear almost identical to those of random walks shown in Fig. 1, with the exception that there is more variance along the first principal component analysis component than in the random walk case, particularly at the start and end points",
        "We have argued that the principal component analysis projected trajectory of a random walk in a general quadratic potential will be dominated by the dimensions with the smallest curvatures where they will appear similar to a random walk in flat space"
    ],
    "summary": [
        "If we consider the limit of a large number of steps, S asymptotically approaches a circulant matrix S that is equal to S in every entry except the top right, where there appears a \u22121 instead of a 0.3",
        "In Fig. 1 we plot the trajectories of a high dimensional random walk projected to various PCA components and compare to the corresponding Lissajous curves.",
        "We include the PCA projections and eigenvalue distributions of random walks using non-isotropic multivariate Gaussian distributions in Figs.",
        "In the limit of n \u2192 \u221e, the distribution of eigenvalues is identical to that of a random walk in flat space, for finite n, it has the effect of shifting the distribution towards the lower PCA components.",
        "First phase the process will beha\u221ave from the origin will increase proportionally to n as a and random walk in flat space and the distance the variance of the kth PCA component will be proportional to k\u22122.",
        "To get a sense for the effect of these differences we compare the distribution of the variances in the PCA components between two models and a random walk.",
        "We show in Fig. 3 the distribution of the PCA variances at the beginning, middle, and end of training for both models and compare to the distribution of variances from an infinite dimensional random walk.",
        "The trajectories appear almost identical to those of random walks shown in Fig. 1, with the exception that there is more variance along the first PCA component than in the random walk case, particularly at the start and end points.",
        "We note that the PCA projected trajectories of the linear model and ResNet-50-v2 over the entire course of training qualitatively resemble those of a high dimensional random walk with exponentially decaying step sizes.",
        "We compare in Fig. 16 of the supplementary material the PCA projected trajectories of the linear model trained on synthetic data to the decayed random walk.",
        "We have derived the distribution of the variances of the PCA components of a random walk both with and without momentum in the limit of infinite dimensions, and proved that the PCA projections of the trajectory are Lissajous curves.",
        "We have argued that the PCA projected trajectory of a random walk in a general quadratic potential will be dominated by the dimensions with the smallest curvatures where they will appear similar to a random walk in flat space.",
        "We find that the PCA projections of the training trajectory of a layer in ResNet-50-v2 qualitatively resemble those of a high dimensional random walk despite the many differences between the optimization of a large NN and a high dimensional random walk"
    ],
    "headline": "We show that principal component analysis projections of random walks in flat space qualitatively have many of the same properties as projections of neural networks training trajectories",
    "reference_links": [
        {
            "id": "Ahn_et+al_2012_a",
            "entry": "Ahn, S., Korattikara, A., and Welling, M. Bayesian posterior sampling via stochastic gradient fisher scoring. In International Conference on Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ahn%2C%20S.%20Korattikara%2C%20A.%20Welling%2C%20M.%20Bayesian%20posterior%20sampling%20via%20stochastic%20gradient%20fisher%20scoring%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ahn%2C%20S.%20Korattikara%2C%20A.%20Welling%2C%20M.%20Bayesian%20posterior%20sampling%20via%20stochastic%20gradient%20fisher%20scoring%202012"
        },
        {
            "id": "Baity-Jesi_et+al_2018_a",
            "entry": "Baity-Jesi, M., Sagun, L., Geiger, M., Spigler, S., Arous, G. B., Cammarota, C., LeCun, Y., Wyart, M., and Biroli, G. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.06969"
        },
        {
            "id": "Boettcher_et+al_2003_a",
            "entry": "B\u00f6ttcher, A., Embree, M., and Sokolov, V. The spectra of large toeplitz band matrices with a randomly perturbed entry. Mathematics of computation, 72(243):1329\u20131348, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%B6ttcher%2C%20A.%20Embree%2C%20M.%20Sokolov%2C%20V.%20The%20spectra%20of%20large%20toeplitz%20band%20matrices%20with%20a%20randomly%20perturbed%20entry%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=B%C3%B6ttcher%2C%20A.%20Embree%2C%20M.%20Sokolov%2C%20V.%20The%20spectra%20of%20large%20toeplitz%20band%20matrices%20with%20a%20randomly%20perturbed%20entry%202003"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B., and LeCun, Y. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20A.%20Henaff%2C%20M.%20Mathieu%2C%20M.%20Arous%2C%20G.B.%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Dauphin_et+al_2014_a",
            "entry": "Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933\u20132941, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "Dinh_et+al_2017_a",
            "entry": "Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04933"
        },
        {
            "id": "Goodfellow_et+al_2015_a",
            "entry": "Goodfellow, I. J., Vinyals, O., and Saxe, A. M. Qualitatively characterizing neural network optimization problems. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Vinyals%2C%20O.%20Saxe%2C%20A.M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Vinyals%2C%20O.%20Saxe%2C%20A.M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015"
        },
        {
            "id": "Gray_2006_a",
            "entry": "Gray, R. M. et al. Toeplitz and circulant matrices: A review. Foundations and Trends R in Communications and Information Theory, 2(3):155\u2013239, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gray%2C%20R.M.%20Toeplitz%20and%20circulant%20matrices%3A%20A%20review.%20Foundations%20and%20Trends%20R%20in%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gray%2C%20R.M.%20Toeplitz%20and%20circulant%20matrices%3A%20A%20review.%20Foundations%20and%20Trends%20R%20in%202006"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Hochreiter, S. and Schmidhuber, J. Flat minima. Neural Computation, 9(1):1\u201342, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Flat%20minima%201997"
        },
        {
            "id": "Jozefowicz_et+al_2016_a",
            "entry": "Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02410"
        },
        {
            "id": "Keskar_et+al_2017_a",
            "entry": "Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20N.S.%20Mudigere%2C%20D.%20Nocedal%2C%20J.%20Smelyanskiy%2C%20M.%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20N.S.%20Mudigere%2C%20D.%20Nocedal%2C%20J.%20Smelyanskiy%2C%20M.%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017"
        },
        {
            "id": "Li_et+al_2018_a",
            "entry": "Li, H., Xu, Z., Taylor, G., and Goldstein, T. Visualizing the loss landscape of neural nets. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20H.%20Xu%2C%20Z.%20Taylor%2C%20G.%20Goldstein%2C%20T.%20Visualizing%20the%20loss%20landscape%20of%20neural%20nets%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20H.%20Xu%2C%20Z.%20Taylor%2C%20G.%20Goldstein%2C%20T.%20Visualizing%20the%20loss%20landscape%20of%20neural%20nets%202018"
        },
        {
            "id": "Lipton_2016_a",
            "entry": "Lipton, Z. C. Stuck in a what? adventures in weight space. arXiv preprint arXiv:1602.07320, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07320"
        },
        {
            "id": "Lorch_2016_a",
            "entry": "Lorch, E. Visualizing deep network training trajectories with pca. In The 33rd International Conference on Machine Learning JMLR volume, volume 48, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lorch%2C%20E.%20Visualizing%20deep%20network%20training%20trajectories%20with%20pca%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lorch%2C%20E.%20Visualizing%20deep%20network%20training%20trajectories%20with%20pca%202016"
        },
        {
            "id": "Mandt_et+al_2016_a",
            "entry": "Mandt, S., Hoffman, M., and Blei, D. A variational analysis of stochastic gradient algorithms. In International Conference on Machine Learning, pp. 354\u2013363, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandt%2C%20S.%20Hoffman%2C%20M.%20Blei%2C%20D.%20A%20variational%20analysis%20of%20stochastic%20gradient%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mandt%2C%20S.%20Hoffman%2C%20M.%20Blei%2C%20D.%20A%20variational%20analysis%20of%20stochastic%20gradient%20algorithms%202016"
        },
        {
            "id": "Moore_et+al_2018_a",
            "entry": "Moore, J., Ahmed, H., and Antia, R. High dimensional random walks can appear low dimensional: Application to influenza h3n2 evolution. Journal of theoretical biology, 447:56\u201364, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moore%2C%20J.%20Ahmed%2C%20H.%20Antia%2C%20R.%20High%20dimensional%20random%20walks%20can%20appear%20low%20dimensional%3A%20Application%20to%20influenza%20h3n2%20evolution%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moore%2C%20J.%20Ahmed%2C%20H.%20Antia%2C%20R.%20High%20dimensional%20random%20walks%20can%20appear%20low%20dimensional%3A%20Application%20to%20influenza%20h3n2%20evolution%202018"
        },
        {
            "id": "Novak_et+al_2018_a",
            "entry": "Novak, R., Bahri, Y., Abolafia, D. A., Pennington, J., and Sohl-Dickstein, J. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Novak%2C%20R.%20Bahri%2C%20Y.%20Abolafia%2C%20D.A.%20Pennington%2C%20J.%20Sensitivity%20and%20generalization%20in%20neural%20networks%3A%20an%20empirical%20study%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Novak%2C%20R.%20Bahri%2C%20Y.%20Abolafia%2C%20D.A.%20Pennington%2C%20J.%20Sensitivity%20and%20generalization%20in%20neural%20networks%3A%20an%20empirical%20study%202018"
        },
        {
            "id": "Rump_2006_a",
            "entry": "Rump, S. M. Eigenvalues, pseudospectrum and structured perturbations. Linear algebra and its applications, 413(2-3):567\u2013593, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rump%2C%20S.M.%20Eigenvalues%2C%20pseudospectrum%20and%20structured%20perturbations%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rump%2C%20S.M.%20Eigenvalues%2C%20pseudospectrum%20and%20structured%20perturbations%202006"
        },
        {
            "id": "Smith_2018_a",
            "entry": "Smith, S. L. and Le, Q. V. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20S.L.%20Le%2C%20Q.V.%20A%20bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20S.L.%20Le%2C%20Q.V.%20A%20bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "Uhlenbeck_1930_a",
            "entry": "Uhlenbeck, G. E. and Ornstein, L. S. On the theory of the brownian motion. Physical review, 36(5): 823, 1930.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uhlenbeck%2C%20G.E.%20Ornstein%2C%20L.S.%20On%20the%20theory%20of%20the%20brownian%20motion%201930",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uhlenbeck%2C%20G.E.%20Ornstein%2C%20L.S.%20On%20the%20theory%20of%20the%20brownian%20motion%201930"
        },
        {
            "id": "Zhu_2017_a",
            "entry": "Zhu, Z. and Wakin, M. B. On the asymptotic equivalence of circulant and toeplitz matrices. IEEE Trans. Information Theory, 63(5):2975\u20132992, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Z.%20Wakin%2C%20M.B.%20On%20the%20asymptotic%20equivalence%20of%20circulant%20and%20toeplitz%20matrices%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Z.%20Wakin%2C%20M.B.%20On%20the%20asymptotic%20equivalence%20of%20circulant%20and%20toeplitz%20matrices%202017"
        }
    ]
}
