{
    "filename": "7442-video-prediction-via-selective-sampling.pdf",
    "metadata": {
        "title": "Video Prediction via Selective Sampling",
        "author": "Jingwei Xu, Bingbing Ni, Xiaokang Yang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7442-video-prediction-via-selective-sampling.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Most adversarial learning based video prediction methods suffer from image blur, since the commonly used adversarial and regression loss pair work rather in a competitive way than collaboration, yielding compromised blur effect. In the meantime, as often relying on a single-pass architecture, the predictor is inadequate to explicitly capture the forthcoming uncertainty. Our work involves two key insights: (1) Video prediction can be approached as a stochastic process: we sample a collection of proposals conforming to possible frame distribution at following time stamp, and one can select the final prediction from it. (2) De-coupling combined loss functions into dedicatedly designed sub-networks encourages them to work in a collaborative way. Combining above two insights we propose a two-stage framework called VPSS (Video Prediction via Selective Sampling). Specifically a Sampling module produces a collection of high quality proposals, facilitated by a multiple choice adversarial learning scheme, yielding diverse frame proposal set. Subsequently a Selection module selects high possibility candidates from proposals and combines them to produce final prediction. Extensive experiments on diverse challenging datasets demonstrate the effectiveness of proposed video prediction approach, i.e., yielding more diverse proposals and accurate prediction results."
    },
    "keywords": [
        {
            "term": "adversarial learning",
            "url": "https://en.wikipedia.org/wiki/adversarial_learning"
        },
        {
            "term": "PSNR",
            "url": "https://en.wikipedia.org/wiki/PSNR"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "Video prediction has been receiving increasing research attention in computer vision [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], which has great potentials in applications such as future decision, robot manipulation and autonomous driving [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "We start to think: (1) To address the blur issue, is it possible to sample a collection of high quality proposals conforming to possible frame distribution at following time stamp, and select the final prediction from it? (2) To encourage collaboration between loss functions, is it possible to design dedicated sub-networks for adversarial and regression loss respectively?\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "Inspired by previous work [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], we show that video prediction can be approached as a stochastic process: we gather a random collection of high quality proposals in one shot, with a multiple choice adversarial learning scheme that encourages diversity within the collection",
        "In this paper we propose a two-stage framework, called VPSS to study video prediction task from a novel view",
        "We propose a selection module for final prediction",
        "Extensive experiments on diverse challenging datasets demonstrate the effectiveness of the proposed video prediction framework"
    ],
    "key_statements": [
        "Video prediction has been receiving increasing research attention in computer vision [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], which has great potentials in applications such as future decision, robot manipulation and autonomous driving [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "We start to think: (1) To address the blur issue, is it possible to sample a collection of high quality proposals conforming to possible frame distribution at following time stamp, and select the final prediction from it? (2) To encourage collaboration between loss functions, is it possible to design dedicated sub-networks for adversarial and regression loss respectively?\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "We propose a two-stage framework called VPSS which utilizes different modules to handle both losses respectively, to encourage collaboration between them, instead of competition",
        "Inspired by previous work [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], we show that video prediction can be approached as a stochastic process: we gather a random collection of high quality proposals in one shot, with a multiple choice adversarial learning scheme that encourages diversity within the collection",
        "Inspired from Guzman-Rivera et al [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], we develop a multiple choice adversarial learning scheme as follows, LKGen =",
        "One can consider that the kernel generator \u03c6KGen is boosted to capture motion information based on previous inputs and infer time-stamp motion direction without constrain of regression loss",
        "We evaluate our framework (VPSS) on three diverse datasets: MovingMnist [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], RobotPush [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and Human3.6M [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which represent challenges in different aspects",
        "Our framework first samples high quality proposals combines them into final prediction, which effectively tackles above two problems",
        "As shown in Figure 4.4, compared to CDNA [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and SV2P [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] our model keeps relative higher scores throughout the prediction procedure, which mainly benefits from high quality proposals during the sampling stage",
        "We observe that results produced by stochastic prediction (SVG [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]) and adversarial learning (Ours and MCNet [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]) related methods seem more realistic to human, which demonstrates",
        "As shown in Table 4, we evaluate the choice of N and K in term of PSNR and Inception Score [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] on Human3.6M Datasets [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "In this paper we propose a two-stage framework, called VPSS to study video prediction task from a novel view",
        "We propose a selection module for final prediction",
        "Extensive experiments on diverse challenging datasets demonstrate the effectiveness of the proposed video prediction framework"
    ],
    "summary": [
        "Video prediction has been receiving increasing research attention in computer vision [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], which has great potentials in applications such as future decision, robot manipulation and autonomous driving [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "As shown in Figure 1, the Sampling module produces multiple high quality video frame proposals, by making use of a multiple choice adversarial learning scheme, yielding diverse video prediction set.",
        "Inspired by previous work [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], we show that video prediction can be approached as a stochastic process: we gather a random collection of high quality proposals in one shot, with a multiple choice adversarial learning scheme that encourages diversity within the collection.",
        "One can consider that the kernel generator \u03c6KGen is boosted to capture motion information based on previous inputs and infer time-stamp motion direction without constrain of regression loss.",
        "Sub-networks are designed dedicated for different objectives, e.g., the sampler is required to produce high quality proposals without requirement of motion accuracy, while the selector should be able to fully capture the motion information of previous inputs and select out proposals with high motion accuracy.",
        "To quantitatively evaluate our proposed framework, we compute the PSNR and SSIM value for DFN [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], SVG [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] and our model on MovingMnist Datasets [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>].",
        "Our framework first samples high quality proposals combines them into final prediction, which effectively tackles above two problems.",
        "It clearly proves that the two-stage framework with dedicated designed sub-networks unifies both adversarial and regression loss functions into prediction system successfully.",
        "As shown in Figure 4.4, compared to CDNA [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and SV2P [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] our model keeps relative higher scores throughout the prediction procedure, which mainly benefits from high quality proposals during the sampling stage.",
        "While for SVG [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] and SV2P [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], image quality is much better, but compared to ground truth, the prediction accuracy is not so satisfying.",
        "The proposed two-stage framework achieves both high image quality and precise motion prediction.",
        "We observe that results produced by stochastic prediction (SVG [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]) and adversarial learning (Ours and MCNet [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]) related methods seem more realistic to human, which demonstrates",
        "As shown in the second row of Figure 5 (B), the operation of sampler is sampling N examples based on previous inputs, and motion direction of all proposals is roughly towards the ground truth.",
        "Our model still performs better than these baselines in both terms of motion accuracy and image quality when prediction length is extended to 30 (Long-term prediction, trained for 10 steps).",
        "Extensive experiments on diverse challenging datasets demonstrate the effectiveness of the proposed video prediction framework."
    ],
    "headline": "Combining above two insights we propose a two-stage framework called VPSS ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, P. Barham, J. Chen, and Z. Chen. Tensorflow: A system for large-scale machine learning. CoRR, abs/1605.08695, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08695"
        },
        {
            "id": "2",
            "entry": "[2] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. CoRR, abs/1701.04862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "3",
            "entry": "[3] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine. Stochastic variational video prediction. CoRR, abs/1710.11252, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11252"
        },
        {
            "id": "4",
            "entry": "[4] D. Berthelot, T. Schumm, and L. Metz. BEGAN: boundary equilibrium generative adversarial networks. CoRR, abs/1703.10717, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10717"
        },
        {
            "id": "5",
            "entry": "[5] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 1520\u20131529, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Q.%20Koltun%2C%20V.%20Photographic%20image%20synthesis%20with%20cascaded%20refinement%20networks%202017-10-22",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Q.%20Koltun%2C%20V.%20Photographic%20image%20synthesis%20with%20cascaded%20refinement%20networks%202017-10-22"
        },
        {
            "id": "6",
            "entry": "[6] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016-12-05"
        },
        {
            "id": "7",
            "entry": "[7] E. Denton and R. Fergus. Stochastic video generation with a learned prior. CoRR, abs/1802.07687, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07687"
        },
        {
            "id": "8",
            "entry": "[8] E. L. Denton and V. Birodkar. Unsupervised learning of disentangled representations from video. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 4417\u20134426, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denton%2C%20E.L.%20Birodkar%2C%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denton%2C%20E.L.%20Birodkar%2C%20V.%20Unsupervised%20learning%20of%20disentangled%20representations%20from%20video%202017-12"
        },
        {
            "id": "9",
            "entry": "[9] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. C. Courville. Adversarially learned inference. CoRR, abs/1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "10",
            "entry": "[10] C. Finn, I. J. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through video prediction. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 64\u201372, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Goodfellow%2C%20I.J.%20Levine%2C%20S.%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Goodfellow%2C%20I.J.%20Levine%2C%20S.%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016-12-05"
        },
        {
            "id": "11",
            "entry": "[11] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2414\u20132423, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016-06-27",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%202016-06-27"
        },
        {
            "id": "12",
            "entry": "[12] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011, pages 315\u2013323, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011-04-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bordes%2C%20A.%20Bengio%2C%20Y.%20Deep%20sparse%20rectifier%20neural%20networks%202011-04-11"
        },
        {
            "id": "13",
            "entry": "[13] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014-12-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014-12-08"
        },
        {
            "id": "14",
            "entry": "[14] A. Guzm\u00e1n-Rivera, D. Batra, and P. Kohli. Multiple choice learning: Learning to produce multiple structured outputs. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 1808\u20131816, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guzm%C3%A1n-Rivera%2C%20A.%20Batra%2C%20D.%20Kohli%2C%20P.%20Multiple%20choice%20learning%3A%20Learning%20to%20produce%20multiple%20structured%20outputs%202012-12-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guzm%C3%A1n-Rivera%2C%20A.%20Batra%2C%20D.%20Kohli%2C%20P.%20Multiple%20choice%20learning%3A%20Learning%20to%20produce%20multiple%20structured%20outputs%202012-12-03"
        },
        {
            "id": "15",
            "entry": "[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "16",
            "entry": "[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "17",
            "entry": "[17] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell., 36(7):1325\u2013 1339, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ionescu%2C%20C.%20Papava%2C%20D.%20Olaru%2C%20V.%20Sminchisescu%2C%20C.%20Human3.%206m%3A%20Large%20scale%20datasets%20and%20predictive%20methods%20for%203d%20human%20sensing%20in%20natural%20environments",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ionescu%2C%20C.%20Papava%2C%20D.%20Olaru%2C%20V.%20Sminchisescu%2C%20C.%20Human3.%206m%3A%20Large%20scale%20datasets%20and%20predictive%20methods%20for%203d%20human%20sensing%20in%20natural%20environments"
        },
        {
            "id": "18",
            "entry": "[18] P. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5967\u20135976, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017-07-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017-07-21"
        },
        {
            "id": "19",
            "entry": "[19] X. Jia, B. D. Brabandere, T. Tuytelaars, and L. V. Gool. Dynamic filter networks. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 667\u2013675, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20X.%20Brabandere%2C%20B.D.%20Tuytelaars%2C%20T.%20Gool%2C%20L.V.%20Dynamic%20filter%20networks%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20X.%20Brabandere%2C%20B.D.%20Tuytelaars%2C%20T.%20Gool%2C%20L.V.%20Dynamic%20filter%20networks%202016-12-05"
        },
        {
            "id": "20",
            "entry": "[20] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen, Z. Jie, J. Feng, and S. Yan. Predicting scene parsing and motion dynamics in the future. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6918\u20136927, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20X.%20Xiao%2C%20H.%20Shen%2C%20X.%20Yang%2C%20J.%20Predicting%20scene%20parsing%20and%20motion%20dynamics%20in%20the%20future%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20X.%20Xiao%2C%20H.%20Shen%2C%20X.%20Yang%2C%20J.%20Predicting%20scene%20parsing%20and%20motion%20dynamics%20in%20the%20future%202017-12"
        },
        {
            "id": "21",
            "entry": "[21] N. Kalchbrenner, A. van den Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1771\u20131779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalchbrenner%2C%20N.%20van%20den%20Oord%2C%20A.%20Simonyan%2C%20K.%20Danihelka%2C%20I.%20Video%20pixel%20networks%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalchbrenner%2C%20N.%20van%20den%20Oord%2C%20A.%20Simonyan%2C%20K.%20Danihelka%2C%20I.%20Video%20pixel%20networks%202017-08"
        },
        {
            "id": "22",
            "entry": "[22] F. Khan, X. J. Zhu, and B. Mutlu. How do humans teach: On curriculum learning and teaching dimension. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain., pages 1449\u20131457, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khan%2C%20F.%20Zhu%2C%20X.J.%20Mutlu%2C%20B.%20How%20do%20humans%20teach%3A%20On%20curriculum%20learning%20and%20teaching%20dimension%202011-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khan%2C%20F.%20Zhu%2C%20X.J.%20Mutlu%2C%20B.%20How%20do%20humans%20teach%3A%20On%20curriculum%20learning%20and%20teaching%20dimension%202011-12"
        },
        {
            "id": "23",
            "entry": "[23] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR,2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization"
        },
        {
            "id": "24",
            "entry": "[24] A. B. L. Larsen, S. K. S\u00f8nderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1558\u20131566, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20B.%20L.%20Larsen%2C%20S.%20K.%20S%C3%B8nderby%2C%20H.%20Larochelle%20Winther%2C%20O.%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016-06-19",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20B.%20L.%20Larsen%2C%20S.%20K.%20S%C3%B8nderby%2C%20H.%20Larochelle%20Winther%2C%20O.%20Autoencoding%20beyond%20pixels%20using%20a%20learned%20similarity%20metric%202016-06-19"
        },
        {
            "id": "25",
            "entry": "[25] A. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and S. Levine. Stochastic adversarial video prediction. CoRR, abs/1804.01523, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.01523"
        },
        {
            "id": "26",
            "entry": "[26] C. Li, H. Liu, C. Chen, Y. Pu, L. Chen, R. Henao, and L. Carin. ALICE: towards understanding adversarial learning for joint distribution matching. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5501\u20135509, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Liu%2C%20H.%20Chen%2C%20C.%20Pu%2C%20Y.%20ALICE%3A%20towards%20understanding%20adversarial%20learning%20for%20joint%20distribution%20matching%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Liu%2C%20H.%20Chen%2C%20C.%20Pu%2C%20Y.%20ALICE%3A%20towards%20understanding%20adversarial%20learning%20for%20joint%20distribution%20matching%202017-12"
        },
        {
            "id": "27",
            "entry": "[27] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, page 3, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20A.L.%20Hannun%2C%20A.Y.%20Ng%2C%20A.Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20A.L.%20Hannun%2C%20A.Y.%20Ng%2C%20A.Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013"
        },
        {
            "id": "28",
            "entry": "[28] S. Marsland. Machine Learning - An Algorithmic Perspective. Chapman and Hall / CRC machine learning and pattern recognition series. CRC Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marsland%2C%20S.%20Machine%20Learning%20-%20An%20Algorithmic%20Perspective.%20Chapman%20and%20Hall%20/%20CRC%20machine%20learning%20and%20pattern%20recognition%20series%202009"
        },
        {
            "id": "29",
            "entry": "[29] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mathieu%2C%20M.%20Couprie%2C%20C.%20LeCun%2C%20Y.%20Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%202016"
        },
        {
            "id": "30",
            "entry": "[30] A. Nguyen, J. Clune, Y. Bengio, A. Dosovitskiy, and J. Yosinski. Plug & play generative networks: Conditional iterative generation of images in latent space. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 3510\u20133520, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20A.%20Clune%2C%20J.%20Bengio%2C%20Y.%20Dosovitskiy%2C%20A.%20Plug%20%26%20play%20generative%20networks%3A%20Conditional%20iterative%20generation%20of%20images%20in%20latent%20space%202017-07-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20A.%20Clune%2C%20J.%20Bengio%2C%20Y.%20Dosovitskiy%2C%20A.%20Plug%20%26%20play%20generative%20networks%3A%20Conditional%20iterative%20generation%20of%20images%20in%20latent%20space%202017-07-21"
        },
        {
            "id": "31",
            "entry": "[31] S. E. Reed, Y. Zhang, Y. Zhang, and H. Lee. Deep visual analogy-making. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1252\u20131260, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.E.%20Zhang%2C%20Y.%20Zhang%2C%20Y.%20Lee%2C%20H.%20Deep%20visual%20analogy-making%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.E.%20Zhang%2C%20Y.%20Zhang%2C%20Y.%20Lee%2C%20H.%20Deep%20visual%20analogy-making%202015-12-07"
        },
        {
            "id": "32",
            "entry": "[32] X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong, and W. Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 802\u2013810, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20X.%20Chen%2C%20Z.%20Wang%2C%20H.%20Yeung%2C%20D.%20Convolutional%20LSTM%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20X.%20Chen%2C%20Z.%20Wang%2C%20H.%20Yeung%2C%20D.%20Convolutional%20LSTM%20network%3A%20A%20machine%20learning%20approach%20for%20precipitation%20nowcasting%202015-12-07"
        },
        {
            "id": "33",
            "entry": "[33] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using lstms. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 843\u2013852, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhutdinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015-07-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhutdinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015-07-06"
        },
        {
            "id": "34",
            "entry": "[34] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. Decomposing motion and content for natural video sequence prediction. CoRR, abs/1706.08033, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08033"
        },
        {
            "id": "35",
            "entry": "[35] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee. Learning to generate long-term future via hierarchical prediction. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 3560\u20133569, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villegas%2C%20R.%20Yang%2C%20J.%20Zou%2C%20Y.%20Sohn%2C%20S.%20Learning%20to%20generate%20long-term%20future%20via%20hierarchical%20prediction%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Villegas%2C%20R.%20Yang%2C%20J.%20Zou%2C%20Y.%20Sohn%2C%20S.%20Learning%20to%20generate%20long-term%20future%20via%20hierarchical%20prediction%202017-08"
        },
        {
            "id": "36",
            "entry": "[36] J. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 2242\u20132251, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017-10"
        }
    ]
}
