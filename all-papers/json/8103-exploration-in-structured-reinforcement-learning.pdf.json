{
    "filename": "8103-exploration-in-structured-reinforcement-learning.pdf",
    "metadata": {
        "title": "Exploration in Structured Reinforcement Learning",
        "date": 2012,
        "author": "Jungseul Ok KTH, EECS Stockholm, Sweden ockjs@illinois.edu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8103-exploration-in-structured-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal (state, action) pairs. For any arbitrary structure, we derive problem-specific regret lower bounds satisfied by any learning algorithm. These lower bounds are made explicit for unstructured MDPs and for those whose transition probabilities and average reward functions are Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs, the bounds are shown not to scale with the sizes S and A of the state and action spaces, i.e., they are smaller than c log T where T is the time horizon and the constant c only depends on the Lipschitz structure, the span of the bias function, and the minimal action sub-optimality gap. This contrasts with unstructured MDPs where the regret lower bound typically scales as SA log T . We devise DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds. We further simplify the algorithm for Lipschitz MDPs, and show that the simplified version is still able to efficiently exploit the structure."
    },
    "keywords": [
        {
            "term": "time horizon",
            "url": "https://en.wikipedia.org/wiki/time_horizon"
        },
        {
            "term": "system dynamic",
            "url": "https://en.wikipedia.org/wiki/system_dynamic"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "transition probability",
            "url": "https://en.wikipedia.org/wiki/transition_probability"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "highlights": [
        "Real-world Reinforcement Learning (RL) problems often concern dynamical systems with large state and action spaces, which make the design of efficient algorithms extremely challenging",
        "In \u221aonline RL problems with undiscounted reward, regret lower bounds typically scale as SA log T or SAT 1, where S, A, and T denote the sizes of the state and action spaces and the time horizon, respectively",
        "With large state and action spaces, it is essential to identify and exploit any possible structure existing in the system dynamics and reward function so as to minimize exploration phases and in turn reduce regret to reasonable values",
        "We explore structured RL problems with finite state and action spaces",
        "The results derived in this paper are valid under any structure, but we give a particular attention to the cases of (i) Unstructured MDPs: \u03c6 \u2208 \u03a6 if for all (x, a), p\u03c6(\u00b7 | x, a) \u2208 P(S) and q\u03c6(\u00b7 | x, a) \u2208 P([0, 1])3; Lipschitz MDPs: \u03c6 \u2208 \u03a6 if p\u03c6(\u00b7|x, a) and r\u03c6(x, a) are Lipschitz-continuous w.r.t. x and a in some metric space"
    ],
    "key_statements": [
        "Real-world Reinforcement Learning (RL) problems often concern dynamical systems with large state and action spaces, which make the design of efficient algorithms extremely challenging",
        "In \u221aonline RL problems with undiscounted reward, regret lower bounds typically scale as SA log T or SAT 1, where S, A, and T denote the sizes of the state and action spaces and the time horizon, respectively",
        "With large state and action spaces, it is essential to identify and exploit any possible structure existing in the system dynamics and reward function so as to minimize exploration phases and in turn reduce regret to reasonable values",
        "We explore structured RL problems with finite state and action spaces",
        "The results derived in this paper are valid under any structure, but we give a particular attention to the cases of (i) Unstructured MDPs: \u03c6 \u2208 \u03a6 if for all (x, a), p\u03c6(\u00b7 | x, a) \u2208 P(S) and q\u03c6(\u00b7 | x, a) \u2208 P([0, 1])3; Lipschitz MDPs: \u03c6 \u2208 \u03a6 if p\u03c6(\u00b7|x, a) and r\u03c6(x, a) are Lipschitz-continuous w.r.t. x and a in some metric space"
    ],
    "summary": [
        "Real-world Reinforcement Learning (RL) problems often concern dynamical systems with large state and action spaces, which make the design of efficient algorithms extremely challenging.",
        "In \u221aonline RL problems with undiscounted reward, regret lower bounds typically scale as SA log T or SAT 1, where S, A, and T denote the sizes of the state and action spaces and the time horizon, respectively.",
        "We provide examples of Lipschitz MDPs, for which the regret under DEL does not seem to scale with S and A, and significantly outperforms algorithms that do not exploit the structure.",
        "Algorithms devised to exploit some known structure are most often applicable to RL problems with continuous state or action spaces.",
        "Assume that the system starts in state x and evolves according to the initially unknown MDP \u03c6 \u2208 \u03a6 for given structure \u03a6, the objective is to devise a policy \u03c0 \u2208 \u03a0 maximizing VT\u03c0(x) or equivalently, minimizing the regret RT\u03c0 (x) up to step T defined as the difference between the cumulative reward of an optimal policy and that obtained under \u03c0: RT\u03c0 (x) := VT\u2217(x) \u2212 VT\u03c0(x)",
        "This can be interpreted as the long-term regret obtained by initially selecting action a in state x rather than following an optimal policy.",
        "The minimal number of times any sub-optimal action a in state x has to be explored scales as \u03b7\u2217(x, a) log T where \u03b7\u2217(x, a) solves the optimization problem",
        "The regret lower bound stated in Theorem 1 extends the problem-specific regret lower bound derived in [<a class=\"ref-link\" id=\"cBurnetas_1997_a\" href=\"#rBurnetas_1997_a\">Burnetas and Katehakis, 1997</a>] for unstructured ergodic MDPs with known reward function.",
        "To obtain the lower bound in unstructured MDPs, we may just consider confusing MDPs \u03c8 which make an initially sub-optimal action a in state x optimal by locally changing the kernels and rewards of \u03c6 at (x, a) only.",
        "It states that exploiting the Lipschitz structure optimally, one may achieve a regret at most scaling as",
        "As shown leveraging the simplified structure in Flip(\u03c6), we may devise a simple algorithm achieving these improvements, i.e., having a regret scaling at most as Klip(\u03c6) log T .",
        "C : S A is a non-empty subset of A), let \u03c6(C) denote the restricted MDP where the set of actions available at state x is C(x).",
        "DEL design aims at exploring sub-optimal actions no more than what the regret lower bound prescribes."
    ],
    "headline": "We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal  pairs",
    "reference_links": [
        {
            "id": "Agrawal_2017_a",
            "entry": "Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret bounds. In Advances in Neural Information Processing Systems 31, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017"
        },
        {
            "id": "Auer_2007_a",
            "entry": "Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement learning. In Advances in Neural Information Processing Systems 19, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Ortner%2C%20Ronald%20Logarithmic%20online%20regret%20bounds%20for%20undiscounted%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Ortner%2C%20Ronald%20Logarithmic%20online%20regret%20bounds%20for%20undiscounted%20reinforcement%20learning%202007"
        },
        {
            "id": "Auer_et+al_2009_a",
            "entry": "Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In Advances in Neural Information Processing Systems 22, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202009"
        },
        {
            "id": "Bartlett_2009_a",
            "entry": "Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Tewari%2C%20Ambuj%20REGAL%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20MDPs%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Tewari%2C%20Ambuj%20REGAL%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20MDPs%202009"
        },
        {
            "id": "Burnetas_1997_a",
            "entry": "Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision processes. Mathematics of Operations Research, 22(1):222\u2013255, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burnetas%2C%20Apostolos%20N.%20Katehakis%2C%20Michael%20N.%20Optimal%20adaptive%20policies%20for%20Markov%20decision%20processes%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burnetas%2C%20Apostolos%20N.%20Katehakis%2C%20Michael%20N.%20Optimal%20adaptive%20policies%20for%20Markov%20decision%20processes%201997"
        },
        {
            "id": "Combes_2014_a",
            "entry": "Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In Proceedings of the 31st International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Unimodal%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Unimodal%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014"
        },
        {
            "id": "Combes_et+al_2017_a",
            "entry": "Richard Combes, Stefan Magureanu, and Alexandre Proutiere. Minimal exploration in structured stochastic bandits. In Advances in Neural Information Processing Systems 30, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combes%2C%20Richard%20Magureanu%2C%20Stefan%20Proutiere%2C%20Alexandre%20Minimal%20exploration%20in%20structured%20stochastic%20bandits%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combes%2C%20Richard%20Magureanu%2C%20Stefan%20Proutiere%2C%20Alexandre%20Minimal%20exploration%20in%20structured%20stochastic%20bandits%202017"
        },
        {
            "id": "Sarah_2010_a",
            "entry": "Sarah Filippi, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. Optimism in reinforcement learning and Kullback-Leibler divergence. In 48th Annual Allerton Conference on Communication, Control, and Computing, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sarah%20Filippi%2C%20Olivier%20Capp%C3%A9%20Garivier%2C%20Aur%C3%A9lien%20Optimism%20in%20reinforcement%20learning%20and%20Kullback-Leibler%20divergence%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sarah%20Filippi%2C%20Olivier%20Capp%C3%A9%20Garivier%2C%20Aur%C3%A9lien%20Optimism%20in%20reinforcement%20learning%20and%20Kullback-Leibler%20divergence%202010"
        },
        {
            "id": "Garivier_et+al_2018_a",
            "entry": "Aur\u00e9lien Garivier, Pierre M\u00e9nard, and Gilles Stoltz. Explore first, exploit next: The true shape of regret in bandit problems. Mathematics of Operations Research, Jun. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garivier%2C%20Aur%C3%A9lien%20M%C3%A9nard%2C%20Pierre%20Stoltz%2C%20Gilles%20Explore%20first%2C%20exploit%20next%3A%20The%20true%20shape%20of%20regret%20in%20bandit%20problems%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garivier%2C%20Aur%C3%A9lien%20M%C3%A9nard%2C%20Pierre%20Stoltz%2C%20Gilles%20Explore%20first%2C%20exploit%20next%3A%20The%20true%20shape%20of%20regret%20in%20bandit%20problems%202018-06"
        },
        {
            "id": "Graves_1997_a",
            "entry": "Todd L. Graves and Tze Leung Lai. Asymptotically efficient adaptive choice of control laws in controlled Markov chains. SIAM J. Control and Optimization, 35(3):715\u2013743, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Todd%20L.%20Lai%2C%20Tze%20Leung%20Asymptotically%20efficient%20adaptive%20choice%20of%20control%20laws%20in%20controlled%20Markov%20chains%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Todd%20L.%20Lai%2C%20Tze%20Leung%20Asymptotically%20efficient%20adaptive%20choice%20of%20control%20laws%20in%20controlled%20Markov%20chains%201997"
        },
        {
            "id": "Emilie_2016_a",
            "entry": "Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best-arm identification in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):1\u201342, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Emilie%20Kaufmann%2C%20Olivier%20Capp%C3%A9%20Garivier%2C%20Aur%C3%A9lien%20On%20the%20complexity%20of%20best-arm%20identification%20in%20multi-armed%20bandit%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Emilie%20Kaufmann%2C%20Olivier%20Capp%C3%A9%20Garivier%2C%20Aur%C3%A9lien%20On%20the%20complexity%20of%20best-arm%20identification%20in%20multi-armed%20bandit%20models%202016"
        },
        {
            "id": "Lakshmanan_et+al_2015_a",
            "entry": "Kailasam Lakshmanan, Ronald Ortner, and Daniil Ryabko. Improved regret bounds for undiscounted continuous reinforcement learning. In 32nd International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshmanan%2C%20Kailasam%20Ortner%2C%20Ronald%20Ryabko%2C%20Daniil%20Improved%20regret%20bounds%20for%20undiscounted%20continuous%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshmanan%2C%20Kailasam%20Ortner%2C%20Ronald%20Ryabko%2C%20Daniil%20Improved%20regret%20bounds%20for%20undiscounted%20continuous%20reinforcement%20learning%202015"
        },
        {
            "id": "Magureanu_et+al_2014_a",
            "entry": "Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bounds and optimal algorithms. In Conference on Learning Theory, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magureanu%2C%20Stefan%20Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Lipschitz%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Magureanu%2C%20Stefan%20Combes%2C%20Richard%20Proutiere%2C%20Alexandre%20Lipschitz%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Ortner_2012_a",
            "entry": "Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement learning. In Advances in Neural Information Processing Systems 25, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ortner%2C%20Ronald%20Ryabko%2C%20Daniil%20Online%20regret%20bounds%20for%20undiscounted%20continuous%20reinforcement%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ortner%2C%20Ronald%20Ryabko%2C%20Daniil%20Online%20regret%20bounds%20for%20undiscounted%20continuous%20reinforcement%20learning%202012"
        },
        {
            "id": "Osband_2014_a",
            "entry": "Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder dimension. In Advances in Neural Information Processing Systems 27, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Model-based%20reinforcement%20learning%20and%20the%20Eluder%20dimension%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Model-based%20reinforcement%20learning%20and%20the%20Eluder%20dimension%202014"
        },
        {
            "id": "Osband_2016_a",
            "entry": "Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv preprint arXiv:1608.02732, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.02732"
        },
        {
            "id": "Tewari_2008_a",
            "entry": "Ambuj Tewari and Peter L. Bartlett. Optimistic linear programming gives logarithmic regret for irreducible MDPs. In Advances in Neural Information Processing Systems 20, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tewari%2C%20Ambuj%20Bartlett%2C%20Peter%20L.%20Optimistic%20linear%20programming%20gives%20logarithmic%20regret%20for%20irreducible%20MDPs%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tewari%2C%20Ambuj%20Bartlett%2C%20Peter%20L.%20Optimistic%20linear%20programming%20gives%20logarithmic%20regret%20for%20irreducible%20MDPs%202008"
        }
    ]
}
