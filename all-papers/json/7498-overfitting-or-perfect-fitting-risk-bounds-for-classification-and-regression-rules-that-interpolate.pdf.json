{
    "filename": "7498-overfitting-or-perfect-fitting-risk-bounds-for-classification-and-regression-rules-that-interpolate.pdf",
    "metadata": {
        "title": "Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate",
        "author": "Mikhail Belkin, Daniel J. Hsu, Partha Mitra",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7498-overfitting-or-perfect-fitting-risk-bounds-for-classification-and-regression-rules-that-interpolate.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for \u201coverfitted\u201d / interpolated classifiers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is consistently robust even when the data contain large amounts of label noise. Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including geometric simplicial interpolation algorithm and singularly weighted k-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in classification and regression problems. Moreover, the nearest neighbor schemes exhibit optimal rates under some standard statistical assumptions. Finally, this paper suggests a way to explain the phenomenon of adversarial examples, which are seemingly ubiquitous in modern machine learning, and also discusses some connections to kernel machines and random forests in the interpolated regime."
    },
    "keywords": [
        {
            "term": "kernel machine",
            "url": "https://en.wikipedia.org/wiki/kernel_machine"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "random forest",
            "url": "https://en.wikipedia.org/wiki/random_forest"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "The central problem of supervised inference is to predict labels of unseen data points from a set of labeled training data",
        "We identify the challenge of providing a rigorous understanding of generalization in machine learning models that interpolate training data",
        "Our results suggest an explanation for the phenomenon of adversarial examples [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>], which are seemingly ubiquitous in modern machine learning",
        "In Section 5, we argue that interpolation inevitably results in adversarial examples in the presence of any amount of label noise",
        "The difference can be striking in high dimensions: 1/d for nearest neighbor versus 1/2d for simplicial interpolation in d-dimensional version of Figure 1. This provides an intuition why, in contrast to the nearest neighbor rule, simplicial interpolation can yield to classifiers that are nearly optimal in high dimensions",
        "We considered two types of algorithms, one based on simplicial interpolation and another based on interpolation by weighted nearest neighbor schemes"
    ],
    "key_statements": [
        "The central problem of supervised inference is to predict labels of unseen data points from a set of labeled training data",
        "We identify the challenge of providing a rigorous understanding of generalization in machine learning models that interpolate training data",
        "Our results suggest an explanation for the phenomenon of adversarial examples [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>], which are seemingly ubiquitous in modern machine learning",
        "In Section 5, we argue that interpolation inevitably results in adversarial examples in the presence of any amount of label noise",
        "The difference can be striking in high dimensions: 1/d for nearest neighbor versus 1/2d for simplicial interpolation in d-dimensional version of Figure 1. This provides an intuition why, in contrast to the nearest neighbor rule, simplicial interpolation can yield to classifiers that are nearly optimal in high dimensions",
        "Any interpolating inferential procedure must have abundant adversarial examples in the presence of any amount of label noise",
        "An even more striking result is that for the Hilbert scheme of Devroye et al, the regression estimator almost surely does not converge at any fixed point, even for the simple case of a constant function corrupted by label noise [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "We considered two types of algorithms, one based on simplicial interpolation and another based on interpolation by weighted nearest neighbor schemes",
        "To provide some evidence for this line of thought, we show that in one dimension simplicial interpolation is a special case of interpolating kernel machine"
    ],
    "summary": [
        "The central problem of supervised inference is to predict labels of unseen data points from a set of labeled training data.",
        "The analyses of the nearest neighbor rule and Hilbert kernel regression estimate are not based on bounding generalization gap, the difference between the true risk and the empirical risk.",
        "The resulting function is somewhat less natural than that obtained by simplicial interpolation, but like the Hilbert kernel regression estimate, the prediction rule is statistically consistent in any dimension.",
        "Our analysis provides the first known non-asymptotic rates of convergence to the Bayes risk for an interpolated predictor, as well as tighter bounds under margin conditions for classification.",
        "We analyze two interpolating schemes, one based on triangulating and constructing the simplicial interpolant for the data, and another, based on weighted nearest neighbors with singular weight function.",
        "This provides an intuition why, in contrast to the nearest neighbor rule, simplicial interpolation can yield to classifiers that are nearly optimal in high dimensions.",
        "We assume that \u03bc is the uniform distribution on a full-dimensional compact and convex subset of Rd. In general, each Yi may deviate from its conditional mean \u2318(Xi) by a non-negligible amount, and any function that interpolates the training data is \u201cfitting noise\u201d.",
        "The use of singular kernels in the context of interpolation was originally proposed by Shepard [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>]; they do not appear to be commonly used in machine learning and statistics, perhaps due to a view that interpolating schemes are unlikely to generalize or even be consistent; the non-adaptive Hilbert kernel regression estimate [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] is the only exception we know of.",
        "In consistent on nearly consistent schemes, like those considered in this paper, while the predictor agrees with the Bayes classifier on the bulk of the probability distribution, every \u201cincorrectly labeled\u201d training example has a small \u201cbasin of attraction\u201d with every point in the basin misclassified by the predictor.",
        "Assuming non-zero label noise, the union of these adversarial basins asymptotically is a dense subset of the support for the underlying probability measure and there are misclassified examples in every open set.",
        "We conjecture that it may be a general property or perhaps a weakness of interpolating methods, as some non-interpolating local classification rules can be robust against certain forms of adversarial examples [<a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>].",
        "An even more striking result is that for the Hilbert scheme of Devroye et al, the regression estimator almost surely does not converge at any fixed point, even for the simple case of a constant function corrupted by label noise [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]."
    ],
    "headline": "This paper suggests a way to explain the phenomenon of adversarial examples, which are seemingly ubiquitous in modern machine learning, and discusses some connections to kernel machines and random forests in the interpolated regime",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Fernando Affentranger and John A Wieacker. On the convex hull of uniform random points in a simpled-polytope. Discrete & Computational Geometry, 6(3):291\u2013305, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Fernando%20Affentranger%20and%20John%20A%20Wieacker.%20On%20the%20convex%20hull%20of%20uniform%20random%20points%20in%20a%20simpled-polytope%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Fernando%20Affentranger%20and%20John%20A%20Wieacker.%20On%20the%20convex%20hull%20of%20uniform%20random%20points%20in%20a%20simpled-polytope%201991"
        },
        {
            "id": "2",
            "entry": "[2] Nina Amenta, Dominique Attali, and Olivier Devillers. Complexity of delaunay triangulation for points on lower-dimensional polyhedra. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201907, pages 1106\u20131113, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amenta%2C%20Nina%20Attali%2C%20Dominique%20Devillers%2C%20Olivier%20Complexity%20of%20delaunay%20triangulation%20for%20points%20on%20lower-dimensional%20polyhedra%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amenta%2C%20Nina%20Attali%2C%20Dominique%20Devillers%2C%20Olivier%20Complexity%20of%20delaunay%20triangulation%20for%20points%20on%20lower-dimensional%20polyhedra%202007"
        },
        {
            "id": "3",
            "entry": "[3] Martin Anthony and Peter L Bartlett. Function learning from interpolation. In Computational Learning Theory: Second European Conference, EUROCOLT 95, Barcelona Spain, March 1995, Proceedings, pages 211\u2013221, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Function%20learning%20from%20interpolation%201995-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Function%20learning%20from%20interpolation%201995-03"
        },
        {
            "id": "4",
            "entry": "[4] Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20Network%20Learning%3A%20Theoretical%20Foundations%201999"
        },
        {
            "id": "5",
            "entry": "[5] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The Annals of statistics, 35(2):608\u2013633, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20Jean-Yves%20Tsybakov%2C%20Alexandre%20B.%20Fast%20learning%20rates%20for%20plug-in%20classifiers%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20Jean-Yves%20Tsybakov%2C%20Alexandre%20B.%20Fast%20learning%20rates%20for%20plug-in%20classifiers%202007"
        },
        {
            "id": "6",
            "entry": "[6] Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20Foster%2C%20Dylan%20J.%20Telgarsky%2C%20Matus%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "7",
            "entry": "[7] Peter L Bartlett, Philip M Long, and Robert C Williamson. Fat-shattering and the learnability of real-valued functions. Journal of Computer and System Sciences, 52(3):434\u2013452, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Long%2C%20Philip%20M.%20Williamson%2C%20Robert%20C.%20Fat-shattering%20and%20the%20learnability%20of%20real-valued%20functions%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Long%2C%20Philip%20M.%20Williamson%2C%20Robert%20C.%20Fat-shattering%20and%20the%20learnability%20of%20real-valued%20functions%201996"
        },
        {
            "id": "8",
            "entry": "[8] Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1046\u20131059. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bassily%2C%20Raef%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20Steinke%2C%20Thomas%20Algorithmic%20stability%20for%20adaptive%20data%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bassily%2C%20Raef%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20Steinke%2C%20Thomas%20Algorithmic%20stability%20for%20adaptive%20data%20analysis%202016"
        },
        {
            "id": "9",
            "entry": "[9] Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On regularization algorithms in learning theory. Journal of complexity, 23(1):52\u201372, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauer%2C%20Frank%20Pereverzev%2C%20Sergei%20Rosasco%2C%20Lorenzo%20On%20regularization%20algorithms%20in%20learning%20theory%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bauer%2C%20Frank%20Pereverzev%2C%20Sergei%20Rosasco%2C%20Lorenzo%20On%20regularization%20algorithms%20in%20learning%20theory%202007"
        },
        {
            "id": "10",
            "entry": "[10] Mikhail Belkin, Irina Matveeva, and Partha Niyogi. Regularization and semi-supervised learning on large graphs. In International Conference on Computational Learning Theory, pages 624\u2013638.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20Mikhail%20Matveeva%2C%20Irina%20Niyogi%2C%20Partha%20Regularization%20and%20semi-supervised%20learning%20on%20large%20graphs",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20Mikhail%20Matveeva%2C%20Irina%20Niyogi%2C%20Partha%20Regularization%20and%20semi-supervised%20learning%20on%20large%20graphs"
        },
        {
            "id": "11",
            "entry": "[11] Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate. arXiv preprint arXiv:1806.05161, 2018. URL https://arxiv.org/abs/1806.05161.",
            "url": "https://arxiv.org/abs/1806.05161",
            "arxiv_url": "https://arxiv.org/pdf/1806.05161"
        },
        {
            "id": "12",
            "entry": "[12] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In Proceedings of the 35th International Conference on Machine Learning, pages 541\u2013549, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20Mikhail%20Ma%2C%20Siyuan%20Mandal%2C%20Soumik%20To%20understand%20deep%20learning%20we%20need%20to%20understand%20kernel%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20Mikhail%20Ma%2C%20Siyuan%20Mandal%2C%20Soumik%20To%20understand%20deep%20learning%20we%20need%20to%20understand%20kernel%20learning%202018"
        },
        {
            "id": "13",
            "entry": "[13] Mikhail Belkin, Alexander Rakhlin, and Alexandre B. Tsybakov. Does data interpolation contradict statistical optimality? arXiv preprint arXiv:1806.09471, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09471"
        },
        {
            "id": "14",
            "entry": "[14] Olivier Bousquet and Andr\u00e9 Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2: 499\u2013526, March 2002. ISSN 1532-4435.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20Olivier%20Stability%2C%20Andr%C3%A9%20Elisseeff%20generalization.%20J.%20Mach.%20Learn%20ISSN%201532-4435%202002-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20Olivier%20Stability%2C%20Andr%C3%A9%20Elisseeff%20generalization.%20J.%20Mach.%20Learn%20ISSN%201532-4435%202002-03"
        },
        {
            "id": "15",
            "entry": "[15] Leo Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Random%20forests%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Random%20forests%202001"
        },
        {
            "id": "16",
            "entry": "[16] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007"
        },
        {
            "id": "17",
            "entry": "[17] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification. In Advances in Neural Information Processing Systems, pages 3437\u20133445, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20Kamalika%20Dasgupta%2C%20Sanjoy%20Rates%20of%20convergence%20for%20nearest%20neighbor%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20Kamalika%20Dasgupta%2C%20Sanjoy%20Rates%20of%20convergence%20for%20nearest%20neighbor%20classification%202014"
        },
        {
            "id": "18",
            "entry": "[18] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information theory, 13(1):21\u201327, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20Thomas%20Hart%2C%20Peter%20Nearest%20neighbor%20pattern%20classification%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cover%2C%20Thomas%20Hart%2C%20Peter%20Nearest%20neighbor%20pattern%20classification%201967"
        },
        {
            "id": "19",
            "entry": "[19] Adele Cutler and Guohua Zhao. Pert-perfect random tree ensembles. Computing Science and Statistics, 33:490\u2013497, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cutler%2C%20Adele%20Zhao%2C%20Guohua%20Pert-perfect%20random%20tree%20ensembles%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cutler%2C%20Adele%20Zhao%2C%20Guohua%20Pert-perfect%20random%20tree%20ensembles%202001"
        },
        {
            "id": "20",
            "entry": "[20] Scott Davies. Multidimensional triangulation and interpolation for reinforcement learning. In Advances in Neural Information Processing Systems, pages 1005\u20131011, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davies%2C%20Scott%20Multidimensional%20triangulation%20and%20interpolation%20for%20reinforcement%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davies%2C%20Scott%20Multidimensional%20triangulation%20and%20interpolation%20for%20reinforcement%20learning%201997"
        },
        {
            "id": "21",
            "entry": "[21] Luc Devroye, Laszlo Gy\u00f6rfi, and Adam Krzyzak. The hilbert kernel regression estimate. Journal of Multivariate Analysis, 65(2):209\u2013227, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devroye%2C%20Luc%20Gy%C3%B6rfi%2C%20Laszlo%20Krzyzak%2C%20Adam%20The%20hilbert%20kernel%20regression%20estimate%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devroye%2C%20Luc%20Gy%C3%B6rfi%2C%20Laszlo%20Krzyzak%2C%20Adam%20The%20hilbert%20kernel%20regression%20estimate%201998"
        },
        {
            "id": "22",
            "entry": "[22] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems, pages 1632\u20131640, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20Alhussein%20Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Frossard%2C%20Pascal%20Robustness%20of%20classifiers%3A%20from%20adversarial%20to%20random%20noise%202016"
        },
        {
            "id": "23",
            "entry": "[23] Komei Fukuda. Polyhedral computation FAQ. Technical report, Swiss Federal Institute of Technology, Lausanne and Zurich, Switzerland, 2004. URL https://www.cs.mcgill.ca/~fukuda/download/paper/polyfaq.pdf.",
            "url": "https://www.cs.mcgill.ca/~fukuda/download/paper/polyfaq.pdf"
        },
        {
            "id": "24",
            "entry": "[24] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Thirty-First Annual Conference on Learning Theory, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20Noah%20Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018"
        },
        {
            "id": "25",
            "entry": "[25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Deep%20Learning%202016"
        },
        {
            "id": "26",
            "entry": "[26] L\u00e1szl\u00f3 Gy\u00f6rfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of Nonparametric Regression. Springer series in statistics. Springer, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gy%C3%B6rfi%2C%20L%C3%A1szl%C3%B3%20Kohler%2C%20Michael%20Krzyzak%2C%20Adam%20Walk%2C%20Harro%20A%20Distribution-Free%20Theory%20of%20Nonparametric%20Regression.%20Springer%20series%20in%20statistics%202002"
        },
        {
            "id": "27",
            "entry": "[27] John H Halton. Simplicial multivariable linear interpolation. Technical Report TR91-002, University of North Carolina at Chapel Hill, Department of Computer Science, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Halton%2C%20John%20H.%20Simplicial%20multivariable%20linear%20interpolation%201991"
        },
        {
            "id": "28",
            "entry": "[28] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, pages 1\u201350, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20Vladimir%20Panchenko%2C%20Dmitry%20Empirical%20margin%20distributions%20and%20bounding%20the%20generalization%20error%20of%20combined%20classifiers%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koltchinskii%2C%20Vladimir%20Panchenko%2C%20Dmitry%20Empirical%20margin%20distributions%20and%20bounding%20the%20generalization%20error%20of%20combined%20classifiers%202002"
        },
        {
            "id": "29",
            "entry": "[29] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01530"
        },
        {
            "id": "30",
            "entry": "[30] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20Aleksander%20Makelov%2C%20Aleksandar%20Schmidt%2C%20Ludwig%20Tsipras%2C%20Dimitris%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%202018"
        },
        {
            "id": "31",
            "entry": "[31] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27(6):1808\u20131829, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mammen%2C%20Enno%20Tsybakov%2C%20Alexandre%20B.%20Smooth%20discrimination%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mammen%2C%20Enno%20Tsybakov%2C%20Alexandre%20B.%20Smooth%20discrimination%20analysis%201999"
        },
        {
            "id": "32",
            "entry": "[32] Pascal Massart and \u00c9lodie N\u00e9d\u00e9lec. Risk bounds for statistical learning. The Annals of Statistics, 34(5):2326\u20132366, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Massart%2C%20Pascal%20N%C3%A9d%C3%A9lec%2C%20%C3%89lodie%20Risk%20bounds%20for%20statistical%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Massart%2C%20Pascal%20N%C3%A9d%C3%A9lec%2C%20%C3%89lodie%20Risk%20bounds%20for%20statistical%20learning%202006"
        },
        {
            "id": "33",
            "entry": "[33] Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1): 141\u2013142, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Elizbar%20A%20Nadaraya.%20On%20estimating%20regression.%20Theory%20of%20Probability%20%26%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Elizbar%20A%20Nadaraya.%20On%20estimating%20regression.%20Theory%20of%20Probability%20%26%201964"
        },
        {
            "id": "34",
            "entry": "[34] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20Srebro%2C%20Nathan%20A%20PAC-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018"
        },
        {
            "id": "35",
            "entry": "[35] Ruslan Salakhutdinov. Deep learning tutorial at the Simons Institute, Berkeley, 2017. URL https://simons.berkeley.edu/talks/ruslan-salakhutdinov-01-26-2017-1.",
            "url": "https://simons.berkeley.edu/talks/ruslan-salakhutdinov-01-26-2017-1"
        },
        {
            "id": "36",
            "entry": "[36] Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. MIT press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schapire%2C%20Robert%20E.%20Freund%2C%20Yoav%20Boosting%3A%20Foundations%20and%20algorithms%202012"
        },
        {
            "id": "37",
            "entry": "[37] Robert E Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: a new explanation for the effectiveness of voting methods. Ann. Statist., 26(5), 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schapire%2C%20Robert%20E.%20Freund%2C%20Yoav%20Bartlett%2C%20Peter%20Lee%2C%20Wee%20Sun%20Boosting%20the%20margin%3A%20a%20new%20explanation%20for%20the%20effectiveness%20of%20voting%20methods%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schapire%2C%20Robert%20E.%20Freund%2C%20Yoav%20Bartlett%2C%20Peter%20Lee%2C%20Wee%20Sun%20Boosting%20the%20margin%3A%20a%20new%20explanation%20for%20the%20effectiveness%20of%20voting%20methods%201998"
        },
        {
            "id": "38",
            "entry": "[38] Rolf Schneider. Discrete aspects of stochastic geometry. Handbook of discrete and computational geometry, page 255, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schneider%2C%20Rolf%20Discrete%20aspects%20of%20stochastic%20geometry.%20Handbook%20of%20discrete%20and%20computational%20geometry%202004"
        },
        {
            "id": "39",
            "entry": "[39] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scholkopf%2C%20Bernhard%20Smola%2C%20Alexander%20J.%20Learning%20with%20kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202001"
        },
        {
            "id": "40",
            "entry": "[40] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Ben-David%2C%20Shai%20Understanding%20machine%20learning%3A%20From%20theory%20to%20algorithms%202014"
        },
        {
            "id": "41",
            "entry": "[41] Donald Shepard. A two-dimensional interpolation function for irregularly-spaced data. In Proceedings of the 1968 23rd ACM national conference, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shepard%2C%20Donald%20A%20two-dimensional%20interpolation%20function%20for%20irregularly-spaced%20data%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shepard%2C%20Donald%20A%20two-dimensional%20interpolation%20function%20for%20irregularly-spaced%20data%201968"
        },
        {
            "id": "42",
            "entry": "[42] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20vector%20machines%202008"
        },
        {
            "id": "43",
            "entry": "[43] Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. One pixel attack for fooling deep neural networks. arXiv preprint arXiv:1710.08864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.08864"
        },
        {
            "id": "44",
            "entry": "[44] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "45",
            "entry": "[45] Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135\u2013166, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexander%20B.%20Optimal%20aggregation%20of%20classifiers%20in%20statistical%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsybakov%2C%20Alexander%20B.%20Optimal%20aggregation%20of%20classifiers%20in%20statistical%20learning%202004"
        },
        {
            "id": "46",
            "entry": "[46] Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics. Springer, New York, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexandre%20B.%20Introduction%20to%20nonparametric%20estimation.%20Springer%20Series%20in%20Statistics%202009"
        },
        {
            "id": "47",
            "entry": "[47] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, pages 5133\u20135142, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yizhen%20Jha%2C%20Somesh%20Chaudhuri%2C%20Kamalika%20Analyzing%20the%20robustness%20of%20nearest%20neighbors%20to%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yizhen%20Jha%2C%20Somesh%20Chaudhuri%2C%20Kamalika%20Analyzing%20the%20robustness%20of%20nearest%20neighbors%20to%20adversarial%20examples%202018"
        },
        {
            "id": "48",
            "entry": "[48] Larry Wasserman. All of statistics. Springer, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wasserman%2C%20Larry%20All%20of%20statistics%202004"
        },
        {
            "id": "49",
            "entry": "[49] Larry Wasserman. All of nonparametric statistics. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wasserman%2C%20Larry%20All%20of%20nonparametric%20statistics%202006"
        },
        {
            "id": "50",
            "entry": "[50] Geoffrey S Watson. Smooth regression analysis. Sankhya: The Indian Journal of Statistics, Series A, pages 359\u2013372, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watson%2C%20Geoffrey%20S.%20Smooth%20regression%20analysis.%20Sankhya%3A%20The%20Indian",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watson%2C%20Geoffrey%20S.%20Smooth%20regression%20analysis.%20Sankhya%3A%20The%20Indian"
        },
        {
            "id": "51",
            "entry": "[51] Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of adaboost and random forests as interpolating classifiers. Journal of Machine Learning Research, 18(48):1\u201333, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wyner%2C%20Abraham%20J.%20Olson%2C%20Matthew%20Bleich%2C%20Justin%20Mease%2C%20David%20Explaining%20the%20success%20of%20adaboost%20and%20random%20forests%20as%20interpolating%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wyner%2C%20Abraham%20J.%20Olson%2C%20Matthew%20Bleich%2C%20Justin%20Mease%2C%20David%20Explaining%20the%20success%20of%20adaboost%20and%20random%20forests%20as%20interpolating%20classifiers%202017"
        },
        {
            "id": "52",
            "entry": "[52] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007"
        },
        {
            "id": "53",
            "entry": "[53] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "54",
            "entry": "[54] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912\u2013919, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003"
        }
    ]
}
