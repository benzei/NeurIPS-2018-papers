{
    "filename": "7860-learning-towards-minimum-hyperspherical-energy.pdf",
    "metadata": {
        "title": "Learning towards Minimum Hyperspherical Energy",
        "author": "Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, Le Song",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7860-learning-towards-minimum-hyperspherical-energy.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics \u2013 Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization."
    },
    "keywords": [
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "face recognition",
            "url": "https://en.wikipedia.org/wiki/face_recognition"
        },
        {
            "term": "network architecture",
            "url": "https://en.wikipedia.org/wiki/network_architecture"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "The recent success of deep neural networks has led to its wide applications in a variety of tasks",
        "Having redundant and highly correlated neurons) caused by over-parametrization presents an issue [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], which motivated a series of influential works in network compression [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and parameter-efficient network architectures [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>]",
        "To reduce the redundancy of neurons and improve the neural networks, we propose a novel minimum hyperspherical energy (MHE) regularization framework, where the diversity of neurons is promoted by minimizing the hyperspherical energy in each layer",
        "In order to avoid such redundancy, we propose the half-space minimum hyperspherical energy which constructs a group of virtual neurons and minimize the hyperspherical energy of both existing and virtual neurons",
        "We propose angular minimum hyperspherical energy (A-minimum hyperspherical energy) as a simple extension, where the hyperspherical energy is rewritten as: N",
        "Such diversity can be viewed as uniform distribution over a hypersphere"
    ],
    "key_statements": [
        "The recent success of deep neural networks has led to its wide applications in a variety of tasks",
        "Having redundant and highly correlated neurons) caused by over-parametrization presents an issue [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], which motivated a series of influential works in network compression [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and parameter-efficient network architectures [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>]",
        "We identify the intrinsic resemblance between the Thomson problem and our target, in the sense that diversifying neurons can be seen as searching for an optimal configuration of electron locations",
        "To reduce the redundancy of neurons and improve the neural networks, we propose a novel minimum hyperspherical energy (MHE) regularization framework, where the diversity of neurons is promoted by minimizing the hyperspherical energy in each layer",
        "In order to avoid such redundancy, we propose the half-space minimum hyperspherical energy which constructs a group of virtual neurons and minimize the hyperspherical energy of both existing and virtual neurons",
        "We argue that the power of neural representation of each layer can be characterized by the hyperspherical energy of its neurons, and a minimal energy configuration of neurons can induce better generalization",
        "That we have introduced the formulation of minimum hyperspherical energy, we propose minimum hyperspherical energy regularization for neural networks",
        "To make neurons in the hidden layers more discriminative and less redundant, we propose to use minimum hyperspherical energy as a form of regularization",
        "We propose to enhance the inter-class feature separability with minimum hyperspherical energy to learn discriminative and well-separated features",
        "-w^ 1 such redundancy, we propose the half-space minimum hyperspherical energy regularization which constructs some virtual neurons and minimizes the hyperspherical energy of both original and virtual neurons together",
        "We propose angular minimum hyperspherical energy (A-minimum hyperspherical energy) as a simple extension, where the hyperspherical energy is rewritten as: N",
        "We propose the mini-batch version of minimum hyperspherical energy to approximate the minimum hyperspherical energy objective",
        "Our minimum hyperspherical energy regularization, which encourages the diversity of neurons, naturally matches the theoretical intuition in [<a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>], and effectively promotes the generalization of neural networks",
        "We evaluate all different variants of minimum hyperspherical energy on CIFAR-10",
        "The half-space minimum hyperspherical energy has more significant performance gain compared to the other minimum hyperspherical energy variants, and minimum hyperspherical energy with Euclidean and angular distance perform",
        "We evaluate minimum hyperspherical energy variants by 27 separately applying minimum hyperspherical energy to the output layer (\u201cO\u201d), minimum hyperspherical energy to the hidden 26.5 layers (\u201cH\u201d), and the half-space minimum hyperspherical energy to the hidden layers (\u201cH\u201d)",
        "We evaluate minimum hyperspherical energy on large-scale ImageNet-2012 datasets",
        "Because minimum hyperspherical energy aims to maximize the hyperspherical margin between different classifier neurons in the output layer, we can naturally apply minimum hyperspherical energy to class-imbalance learning where the number of training samples in different classes is imbalanced",
        "We have shown that full-space minimum hyperspherical energy for output layers can encourage classifier neurons to distribute more evenly on hypersphere and improve inter-class feature separability",
        "Such diversity can be viewed as uniform distribution over a hypersphere"
    ],
    "summary": [
        "The recent success of deep neural networks has led to its wide applications in a variety of tasks.",
        "To reduce the redundancy of neurons and improve the neural networks, we propose a novel minimum hyperspherical energy (MHE) regularization framework, where the diversity of neurons is promoted by minimizing the hyperspherical energy in each layer.",
        "-w^ 1 such redundancy, we propose the half-space MHE regularization which constructs some virtual neurons and minimizes the hyperspherical energy of both original and virtual neurons together.",
        "Mini-batch approximation iteratively takes a random batch of neurons as input and minimizes their hyperspherical energy as an approximation to the MHE.",
        "Our MHE regularization, which encourages the diversity of neurons, naturally matches the theoretical intuition in [<a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>], and effectively promotes the generalization of neural networks.",
        "All methods use CNN-9 Table 1: Testing error (%) of different MHE on CIFAR-10/100..",
        "Half-space MHE performs consistently better than MHE, showing the necessity of reducing colinearity redundancy among neurons.",
        "We see that half-space MHE can consistently show better generalization than MHE with different network depth.",
        "Adding MHE regularization will not affect the original architecture settings, and it can readily improve the network generalization at a neglectable computational cost.",
        "Because MHE aims to maximize the hyperspherical margin between different classifier neurons in the output layer, we can naturally apply MHE to class-imbalance learning where the number of training samples in different classes is imbalanced.",
        "Even only given a small portion of training data in a few classes, CNN-9 with MHE can achieve very competitive accuracy on the full testing set, showing MHE\u2019s superior generalization power.",
        "We have shown that full-space MHE for output layers can encourage classifier neurons to distribute more evenly on hypersphere and improve inter-class feature separability.",
        "Because face datesets usually have thousands of identities, we will use the data-dependent mini-batch approximation MHE as shown in Eq (8) in the output layer to reduce computational cost.",
        "We borrow some useful ideas and insights from physics and propose a novel regularization method for neural networks, called minimum hyperspherical energy (MHE), to encourage the angular diversity of neuron weights.",
        "MHE has been specifically used to improve network generalization for generic image classification, class-imbalance learning and large-scale face recognition, showing consistent improvements in all tasks.",
        "MHE can significantly improve the image generation quality of GANs. In summary, MHE introduces hyperspherical diversity and casts a novel view on regularizing the neurons.",
        "MHE introduces hyperspherical diversity and casts a novel view on regularizing the neurons"
    ],
    "headline": "In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy  objective as generic regularization for neural networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alireza Aghasi, Nam Nguyen, and Justin Romberg. Net-trim: A layer-wise convex pruning of deep neural networks. In AISTATS, 2017. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aghasi%2C%20Alireza%20Nguyen%2C%20Nam%20Romberg%2C%20Justin%20Net-trim%3A%20A%20layer-wise%20convex%20pruning%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aghasi%2C%20Alireza%20Nguyen%2C%20Nam%20Romberg%2C%20Justin%20Net-trim%3A%20A%20layer-wise%20convex%20pruning%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "2",
            "entry": "[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 20",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "3",
            "entry": "[3] Dmitriy Bilyk and Michael T Lacey. One-bit sensing, discrepancy and stolarsky\u2019s principle. Sbornik: Mathematics, 208(6):744, 2017. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bilyk%2C%20Dmitriy%20Lacey%2C%20Michael%20T.%20One-bit%20sensing%2C%20discrepancy%20and%20stolarsky%E2%80%99s%20principle%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bilyk%2C%20Dmitriy%20Lacey%2C%20Michael%20T.%20One-bit%20sensing%2C%20discrepancy%20and%20stolarsky%E2%80%99s%20principle%202017"
        },
        {
            "id": "4",
            "entry": "[4] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. In ICLR, 2017. 2, 20",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Neural%20photo%20editing%20with%20introspective%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Lim%2C%20Theodore%20Ritchie%2C%20James%20M.%20Weston%2C%20Nick%20Neural%20photo%20editing%20with%20introspective%20adversarial%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep networks by decorrelating representations. In ICLR, 2016. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cogswell%2C%20Michael%20Ahmed%2C%20Faruk%20Girshick%2C%20Ross%20Zitnick%2C%20Larry%20Reducing%20overfitting%20in%20deep%20networks%20by%20decorrelating%20representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cogswell%2C%20Michael%20Ahmed%2C%20Faruk%20Girshick%2C%20Ross%20Zitnick%2C%20Larry%20Reducing%20overfitting%20in%20deep%20networks%20by%20decorrelating%20representations%202016"
        },
        {
            "id": "6",
            "entry": "[6] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. arXiv preprint arXiv:1801.07698, 2018. 9",
            "arxiv_url": "https://arxiv.org/pdf/1801.07698"
        },
        {
            "id": "7",
            "entry": "[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 20",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014%2020",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014%2020"
        },
        {
            "id": "8",
            "entry": "[8] Mario G\u00f6tz and Edward B Saff. Note on d\u2014extremal configurations for the sphere in r d+1. In Recent Progress in Multivariate Approximation, pages 159\u2013162. Springer, 2001. 5, 15",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%C3%B6tz%2C%20Mario%20Saff%2C%20Edward%20B.%20Note%20on%20d%E2%80%94extremal%20configurations%20for%20the%20sphere%20in%20r%20d%2B1%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%C3%B6tz%2C%20Mario%20Saff%2C%20Edward%20B.%20Note%20on%20d%E2%80%94extremal%20configurations%20for%20the%20sphere%20in%20r%20d%2B1%202001"
        },
        {
            "id": "9",
            "entry": "[9] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, 2017. 20",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "10",
            "entry": "[10] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Mao%2C%20Huizi%20Dally%2C%20William%20J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20huffman%20coding%202016"
        },
        {
            "id": "11",
            "entry": "[11] DP Hardin and EB Saff. Minimal riesz energy point configurations for rectifiable d-dimensional manifolds. arXiv preprint math-ph/0311024, 2003. 5, 15",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardin%2C%20D.P.%20Saff%2C%20E.B.%20Minimal%20riesz%20energy%20point%20configurations%20for%20rectifiable%20d-dimensional%20manifolds%202003"
        },
        {
            "id": "12",
            "entry": "[12] DP Hardin and EB Saff. Discretizing manifolds via minimum energy points. Notices of the AMS, 51(10):1186\u20131194, 2004. 5, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardin%2C%20D.P.%20Saff%2C%20E.B.%20Discretizing%20manifolds%20via%20minimum%20energy%20points%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardin%2C%20D.P.%20Saff%2C%20E.B.%20Discretizing%20manifolds%20via%20minimum%20energy%20points%202004"
        },
        {
            "id": "13",
            "entry": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "14",
            "entry": "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 6, 8, 13",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "15",
            "entry": "[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV, 2016. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016"
        },
        {
            "id": "16",
            "entry": "[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 1",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "17",
            "entry": "[17] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017. 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "18",
            "entry": "[18] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical report, Technical Report, 2007. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gary%20B.%20Ramesh%2C%20Manu%20Berg%2C%20Tamara%20Learned-Miller%2C%20Erik%20Labeled%20faces%20in%20the%20wild%3A%20A%20database%20for%20studying%20face%20recognition%20in%20unconstrained%20environments%202007"
        },
        {
            "id": "19",
            "entry": "[19] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016. 1",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "20",
            "entry": "[20] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 2, 6, 20",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "21",
            "entry": "[21] Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. Self-paced learning with diversity. In NIPS, 2014. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Lu%20Meng%2C%20Deyu%20Yu%2C%20Shoou-I.%20Lan%2C%20Zhenzhong%20Self-paced%20learning%20with%20diversity%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Lu%20Meng%2C%20Deyu%20Yu%2C%20Shoou-I.%20Lan%2C%20Zhenzhong%20Self-paced%20learning%20with%20diversity%202014"
        },
        {
            "id": "22",
            "entry": "[22] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface benchmark: 1 million faces for recognition at scale. In CVPR, 2016. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kemelmacher-Shlizerman%2C%20Ira%20Seitz%2C%20Steven%20M.%20Miller%2C%20Daniel%20Brossard%2C%20Evan%20The%20megaface%20benchmark%3A%201%20million%20faces%20for%20recognition%20at%20scale%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kemelmacher-Shlizerman%2C%20Ira%20Seitz%2C%20Steven%20M.%20Miller%2C%20Daniel%20Brossard%2C%20Evan%20The%20megaface%20benchmark%3A%201%20million%20faces%20for%20recognition%20at%20scale%202016"
        },
        {
            "id": "23",
            "entry": "[23] Arno Kuijlaars and E Saff. Asymptotics for minimal discrete energy on the sphere. Transactions of the American Mathematical Society, 350(2):523\u2013538, 1998. 5, 6, 15",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuijlaars%2C%20Arno%20Saff%2C%20E.%20Asymptotics%20for%20minimal%20discrete%20energy%20on%20the%20sphere%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuijlaars%2C%20Arno%20Saff%2C%20E.%20Asymptotics%20for%20minimal%20discrete%20energy%20on%20the%20sphere%201998"
        },
        {
            "id": "24",
            "entry": "[24] Ludmila I Kuncheva and Christopher J Whitaker. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Machine learning, 51(2):181\u2013207, 2003. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuncheva%2C%20Ludmila%20I.%20Whitaker%2C%20Christopher%20J.%20Measures%20of%20diversity%20in%20classifier%20ensembles%20and%20their%20relationship%20with%20the%20ensemble%20accuracy%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuncheva%2C%20Ludmila%20I.%20Whitaker%2C%20Christopher%20J.%20Measures%20of%20diversity%20in%20classifier%20ensembles%20and%20their%20relationship%20with%20the%20ensemble%20accuracy%202003"
        },
        {
            "id": "25",
            "entry": "[25] Naum Samouilovich Landkof. Foundations of modern potential theory, volume 180. Springer, 1972. 5, 15",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Landkof%2C%20Naum%20Samouilovich%20Foundations%20of%20modern%20potential%20theory%2C%20volume%20180%201972"
        },
        {
            "id": "26",
            "entry": "[26] Nan Li, Yang Yu, and Zhi-Hua Zhou. Diversity regularized ensemble pruning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2012. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Nan%20Yu%2C%20Yang%20Zhou%2C%20Zhi-Hua%20Diversity%20regularized%20ensemble%20pruning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Nan%20Yu%2C%20Yang%20Zhou%2C%20Zhi-Hua%20Diversity%20regularized%20ensemble%20pruning%202012"
        },
        {
            "id": "27",
            "entry": "[27] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and Le Song. Decoupled networks. CVPR, 2018. 1, 5, 6, 16",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Liu%2C%20Zhen%20Yu%2C%20Zhiding%20Dai%2C%20Bo%20Decoupled%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Liu%2C%20Zhen%20Yu%2C%20Zhiding%20Dai%2C%20Bo%20Decoupled%20networks%202018"
        },
        {
            "id": "28",
            "entry": "[28] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In CVPR, 2017. 1, 2, 5, 9, 14, 19",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Li%2C%20Ming%20Sphereface%3A%20Deep%20hypersphere%20embedding%20for%20face%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Li%2C%20Ming%20Sphereface%3A%20Deep%20hypersphere%20embedding%20for%20face%20recognition%202017"
        },
        {
            "id": "29",
            "entry": "[29] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In ICML, 2016. 1, 2, 4, 9, 22",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "30",
            "entry": "[30] Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep hyperspherical learning. In NIPS, 2017. 1, 2, 4, 5, 6, 8, 16, 18",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Zhang%2C%20Yan-Ming%20Li%2C%20Xingguo%20Yu%2C%20Zhiding%20Deep%20hyperspherical%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Zhang%2C%20Yan-Ming%20Li%2C%20Xingguo%20Yu%2C%20Zhiding%20Deep%20hyperspherical%20learning%202017"
        },
        {
            "id": "31",
            "entry": "[31] Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking feature discrimination and polymerization for large-scale recognition. arXiv preprint arXiv:1710.00870, 2017. 9",
            "arxiv_url": "https://arxiv.org/pdf/1710.00870"
        },
        {
            "id": "32",
            "entry": "[32] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In ICML, 2009. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sapiro%2C%20Guillermo%20Online%20dictionary%20learning%20for%20sparse%20coding%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sapiro%2C%20Guillermo%20Online%20dictionary%20learning%20for%20sparse%20coding%202009"
        },
        {
            "id": "33",
            "entry": "[33] Dmytro Mishkin and Jiri Matas. All you need is a good init. In ICLR, 2016. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishkin%2C%20Dmytro%20Matas%2C%20Jiri%20All%20you%20need%20is%20a%20good%20init%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishkin%2C%20Dmytro%20Matas%2C%20Jiri%20All%20you%20need%20is%20a%20good%20init%202016"
        },
        {
            "id": "34",
            "entry": "[34] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018. 2, 20",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "35",
            "entry": "[35] Ignacio Ramirez, Pablo Sprechmann, and Guillermo Sapiro. Classification and clustering via dictionary learning with structured incoherence and shared features. In CVPR, 2010. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramirez%2C%20Ignacio%20Sprechmann%2C%20Pablo%20Sapiro%2C%20Guillermo%20Classification%20and%20clustering%20via%20dictionary%20learning%20with%20structured%20incoherence%20and%20shared%20features%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramirez%2C%20Ignacio%20Sprechmann%2C%20Pablo%20Sapiro%2C%20Guillermo%20Classification%20and%20clustering%20via%20dictionary%20learning%20with%20structured%20incoherence%20and%20shared%20features%202010"
        },
        {
            "id": "36",
            "entry": "[36] Pau Rodr\u00edguez, Jordi Gonzalez, Guillem Cucurull, Josep M Gonfaus, and Xavier Roca. Regularizing cnns with locally constrained decorrelations. In ICLR, 2017. 1, 2, 5, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodr%C3%ADguez%2C%20Pau%20Gonzalez%2C%20Jordi%20Cucurull%2C%20Guillem%20Gonfaus%2C%20Josep%20M.%20Regularizing%20cnns%20with%20locally%20constrained%20decorrelations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodr%C3%ADguez%2C%20Pau%20Gonzalez%2C%20Jordi%20Cucurull%2C%20Guillem%20Gonfaus%2C%20Josep%20M.%20Regularizing%20cnns%20with%20locally%20constrained%20decorrelations%202017"
        },
        {
            "id": "37",
            "entry": "[37] Aruni RoyChowdhury, Prakhar Sharma, Erik Learned-Miller, and Aruni Roy. Reducing duplicate filters in deep neural networks. In NIPS workshop on Deep Learning: Bridging Theory and Practice, 2017. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RoyChowdhury%2C%20Aruni%20Sharma%2C%20Prakhar%20Learned-Miller%2C%20Erik%20Roy%2C%20Aruni%20Reducing%20duplicate%20filters%20in%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RoyChowdhury%2C%20Aruni%20Sharma%2C%20Prakhar%20Learned-Miller%2C%20Erik%20Roy%2C%20Aruni%20Reducing%20duplicate%20filters%20in%20deep%20neural%20networks%202017"
        },
        {
            "id": "38",
            "entry": "[38] Edward B Saff and Amo BJ Kuijlaars. Distributing many points on a sphere. The mathematical intelligencer, 19(1):5\u201311, 1997. 5, 6",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saff%2C%20Edward%20B.%20Kuijlaars%2C%20Amo%20B.J.%20Distributing%20many%20points%20on%20a%20sphere%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saff%2C%20Edward%20B.%20Kuijlaars%2C%20Amo%20B.J.%20Distributing%20many%20points%20on%20a%20sphere%201997"
        },
        {
            "id": "39",
            "entry": "[39] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NIPS, 2016. 20",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "40",
            "entry": "[40] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In CVPR, 2015. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schroff%2C%20Florian%20Kalenichenko%2C%20Dmitry%20Philbin%2C%20James%20Facenet%3A%20A%20unified%20embedding%20for%20face%20recognition%20and%20clustering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schroff%2C%20Florian%20Kalenichenko%2C%20Dmitry%20Philbin%2C%20James%20Facenet%3A%20A%20unified%20embedding%20for%20face%20recognition%20and%20clustering%202015"
        },
        {
            "id": "41",
            "entry": "[41] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In ICML, 2016. 1",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shang%2C%20Wenling%20Sohn%2C%20Kihyuk%20Almeida%2C%20Diogo%20Lee%2C%20Honglak%20Understanding%20and%20improving%20convolutional%20neural%20networks%20via%20concatenated%20rectified%20linear%20units%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shang%2C%20Wenling%20Sohn%2C%20Kihyuk%20Almeida%2C%20Diogo%20Lee%2C%20Honglak%20Understanding%20and%20improving%20convolutional%20neural%20networks%20via%20concatenated%20rectified%20linear%20units%202016"
        },
        {
            "id": "42",
            "entry": "[42] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014. 1",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "43",
            "entry": "[43] Steve Smale. Mathematical problems for the next century. The mathematical intelligencer, 20(2):7\u201315, 1998. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smale%2C%20Steve%20Mathematical%20problems%20for%20the%20next%20century%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smale%2C%20Steve%20Mathematical%20problems%20for%20the%20next%20century%201998"
        },
        {
            "id": "44",
            "entry": "[44] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15(1):1929\u20131958, 2014. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "45",
            "entry": "[45] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation from predicting 10,000 classes. In CVPR, 2014. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Yi%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20representation%20from%20predicting%2010%2C000%20classes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Yi%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20representation%20from%20predicting%2010%2C000%20classes%202014"
        },
        {
            "id": "46",
            "entry": "[46] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 1, 8",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "47",
            "entry": "[47] Pieter Merkus Lambertus Tammes. On the origin of number and arrangement of the places of exit on the surface of pollen-grains. Recueil des travaux botaniques n\u00e9erlandais, 27(1):1\u201384, 1930. 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Merkus%20Lambertus%20Tammes.%20On%20the%20origin%20of%20number%20and%20arrangement%20of%20the%20places%20of%20exit%20on%20the%20surface%20of%20pollen-grains.%20Recueil%20des%20travaux%20botaniques%201930",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Merkus%20Lambertus%20Tammes.%20On%20the%20origin%20of%20number%20and%20arrangement%20of%20the%20places%20of%20exit%20on%20the%20surface%20of%20pollen-grains.%20Recueil%20des%20travaux%20botaniques%201930"
        },
        {
            "id": "48",
            "entry": "[48] Joseph John Thomson. Xxiv. on the structure of the atom: an investigation of the stability and periods of oscillation of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory of atomic structure. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 7(39):237\u2013265, 1904. 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomson%2C%20Joseph%20John%20Xxiv.%20on%20the%20structure%20of%20the%20atom%3A%20an%20investigation%20of%20the%20stability%20and%20periods%20of%20oscillation%20of%20a%20number%20of%20corpuscles%20arranged%20at%20equal%20intervals%20around%20the%20circumference%20of%20a%20circle%3B%20with%20application%20of%20the%20results%20to%20the%20theory%20of%20atomic%20structure%201904",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomson%2C%20Joseph%20John%20Xxiv.%20on%20the%20structure%20of%20the%20atom%3A%20an%20investigation%20of%20the%20stability%20and%20periods%20of%20oscillation%20of%20a%20number%20of%20corpuscles%20arranged%20at%20equal%20intervals%20around%20the%20circumference%20of%20a%20circle%3B%20with%20application%20of%20the%20results%20to%20the%20theory%20of%20atomic%20structure%201904"
        },
        {
            "id": "49",
            "entry": "[49] Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Additive margin softmax for face verification. arXiv preprint arXiv:1801.05599, 2018. 9, 19",
            "arxiv_url": "https://arxiv.org/pdf/1801.05599"
        },
        {
            "id": "50",
            "entry": "[50] Feng Wang, Xiang Xiang, Jian Cheng, and Alan L Yuille. Normface: L2 hypersphere embedding for face verification. arXiv preprint arXiv:1704.06369, 2017. 19",
            "arxiv_url": "https://arxiv.org/pdf/1704.06369"
        },
        {
            "id": "51",
            "entry": "[51] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. arXiv preprint arXiv:1801.09414, 2018. 9, 14",
            "arxiv_url": "https://arxiv.org/pdf/1801.09414"
        },
        {
            "id": "52",
            "entry": "[52] David Warde-Farley and Yoshua Bengio. Improving generative adversarial networks with denoising feature matching. In ICLR, 2017. 20",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Warde-Farley%2C%20David%20Bengio%2C%20Yoshua%20Improving%20generative%20adversarial%20networks%20with%20denoising%20feature%20matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Warde-Farley%2C%20David%20Bengio%2C%20Yoshua%20Improving%20generative%20adversarial%20networks%20with%20denoising%20feature%20matching%202017"
        },
        {
            "id": "53",
            "entry": "[53] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In ECCV, 2016. 9",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Yandong%20Zhang%2C%20Kaipeng%20Li%2C%20Zhifeng%20Qiao%2C%20Yu%20A%20discriminative%20feature%20learning%20approach%20for%20deep%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Yandong%20Zhang%2C%20Kaipeng%20Li%2C%20Zhifeng%20Qiao%2C%20Yu%20A%20discriminative%20feature%20learning%20approach%20for%20deep%20face%20recognition%202016"
        },
        {
            "id": "54",
            "entry": "[54] Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv preprint arXiv:1611.03131, 2016. 5, 6",
            "arxiv_url": "https://arxiv.org/pdf/1611.03131"
        },
        {
            "id": "55",
            "entry": "[55] Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. arXiv:1703.01827, 2017. 2",
            "arxiv_url": "https://arxiv.org/pdf/1703.01827"
        },
        {
            "id": "56",
            "entry": "[56] Pengtao Xie, Yuntian Deng, Yi Zhou, Abhimanu Kumar, Yaoliang Yu, James Zou, and Eric P Xing. Learning latent space models with angular constraints. In ICML, 2017. 1, 2, 5",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Pengtao%20Deng%2C%20Yuntian%20Zhou%2C%20Yi%20Kumar%2C%20Abhimanu%20and%20Eric%20P%20Xing.%20Learning%20latent%20space%20models%20with%20angular%20constraints%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Pengtao%20Deng%2C%20Yuntian%20Zhou%2C%20Yi%20Kumar%2C%20Abhimanu%20and%20Eric%20P%20Xing.%20Learning%20latent%20space%20models%20with%20angular%20constraints%202017"
        },
        {
            "id": "57",
            "entry": "[57] Pengtao Xie, Aarti Singh, and Eric P Xing. Uncorrelation and evenness: a new diversity-promoting regularizer. In ICML, 2017. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Pengtao%20Singh%2C%20Aarti%20and%20Eric%20P%20Xing.%20Uncorrelation%20and%20evenness%3A%20a%20new%20diversity-promoting%20regularizer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Pengtao%20Singh%2C%20Aarti%20and%20Eric%20P%20Xing.%20Uncorrelation%20and%20evenness%3A%20a%20new%20diversity-promoting%20regularizer%202017"
        },
        {
            "id": "58",
            "entry": "[58] Pengtao Xie, Wei Wu, Yichen Zhu, and Eric P Xing. Orthogonality-promoting distance metric learning: convex relaxation and theoretical analysis. In ICML, 2018. 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Pengtao%20Wu%2C%20Wei%20Zhu%2C%20Yichen%20and%20Eric%20P%20Xing.%20Orthogonality-promoting%20distance%20metric%20learning%3A%20convex%20relaxation%20and%20theoretical%20analysis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Pengtao%20Wu%2C%20Wei%20Zhu%2C%20Yichen%20and%20Eric%20P%20Xing.%20Orthogonality-promoting%20distance%20metric%20learning%3A%20convex%20relaxation%20and%20theoretical%20analysis%202018"
        },
        {
            "id": "59",
            "entry": "[59] Pengtao Xie, Jun Zhu, and Eric Xing. Diversity-promoting bayesian learning of latent variable models. In ICML, 2016. 1, 2",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Pengtao%20Zhu%2C%20Jun%20and%20Eric%20Xing.%20Diversity-promoting%20bayesian%20learning%20of%20latent%20variable%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Pengtao%20Zhu%2C%20Jun%20and%20Eric%20Xing.%20Diversity-promoting%20bayesian%20learning%20of%20latent%20variable%20models%202016"
        },
        {
            "id": "60",
            "entry": "[60] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv:1411.7923, 2014. 9",
            "arxiv_url": "https://arxiv.org/pdf/1411.7923"
        },
        {
            "id": "61",
            "entry": "[61] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE Signal Processing Letters, 23(10):1499\u20131503, 2016. 14",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Kaipeng%20Zhang%2C%20Zhanpeng%20Li%2C%20Zhifeng%20Qiao%2C%20Yu%20Joint%20face%20detection%20and%20alignment%20using%20multitask%20cascaded%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Kaipeng%20Zhang%2C%20Zhanpeng%20Li%2C%20Zhifeng%20Qiao%2C%20Yu%20Joint%20face%20detection%20and%20alignment%20using%20multitask%20cascaded%20convolutional%20networks%202016"
        },
        {
            "id": "62",
            "entry": "[62] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017. 1 ",
            "arxiv_url": "https://arxiv.org/pdf/1707.01083"
        }
    ]
}
