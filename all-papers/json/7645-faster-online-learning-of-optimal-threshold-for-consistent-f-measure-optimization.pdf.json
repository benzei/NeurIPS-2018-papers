{
    "filename": "7645-faster-online-learning-of-optimal-threshold-for-consistent-f-measure-optimization.pdf",
    "metadata": {
        "title": "Faster Online Learning of Optimal Threshold for Consistent F-measure Optimization",
        "author": "Xiaoxuan Zhang, Mingrui Liu, Xun Zhou, Tianbao Yang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7645-faster-online-learning-of-optimal-threshold-for-consistent-f-measure-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (e.g., classification error rate), F-measure is nondecomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack statistical consistency guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is a novel stochastic algorithm with lo\u221aw memory and computational costs, which can enjoy a convergence rate of O(1/ n) for learning the optimal threshold under a mild condition on the convergence of the posterior probability, where n is the number of processed examples. It is provably faster than its predecessor based on a heuristic for updating the threshold. The experiments verify the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms."
    },
    "keywords": [
        {
            "term": "error rate",
            "url": "https://en.wikipedia.org/wiki/error_rate"
        },
        {
            "term": "computational cost",
            "url": "https://en.wikipedia.org/wiki/computational_cost"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "online learning",
            "url": "https://en.wikipedia.org/wiki/online_learning"
        },
        {
            "term": "posterior probability",
            "url": "https://en.wikipedia.org/wiki/posterior_probability"
        },
        {
            "term": "convex function",
            "url": "https://en.wikipedia.org/wiki/convex_function"
        },
        {
            "term": "Jaccard similarity coefficient",
            "url": "https://en.wikipedia.org/wiki/Jaccard_similarity_coefficient"
        },
        {
            "term": "performance metric",
            "url": "https://en.wikipedia.org/wiki/performance_metric"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "A learning algorithm is to optimize a certain performance metric defined over a set or population of examples",
        "While many studies are devoted to learning with traditional performance metrics, there has been an increasing interest in designing efficient online learning algorithms to maximize Fmeasure for tackling large-scale streaming data or by one pass of large-scale batch data [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We propose a Fast Online F-measure Optimization (FOFO) with a novel component for learning the optimal threshold for a probabilistic binary classifier",
        "We present a quick comparison between this work and related studies in Table 1 from various perspectives, including theoretical convergence results, consistency of F-measure optimization, memory and computational costs, where linear models are assumed for different algorithms in order to compare the memory and computational costs",
        "As long as maxx\u2208X |\u03b7t(x) \u2212 \u03b7(x)| \u2264 O(1/t\u03b1) for some \u03b1 > 0, a convergence result can be established for \u03b8m, which implies the consistency of Fast Online F-measure Optimization for F-measure optimization by the Proposition 13 in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We proposed a fast online F-measure optimization algorithm with low memory and computational costs by learning the optimal threshold for a probabilistic classifier"
    ],
    "key_statements": [
        "A learning algorithm is to optimize a certain performance metric defined over a set or population of examples",
        "While many studies are devoted to learning with traditional performance metrics, there has been an increasing interest in designing efficient online learning algorithms to maximize Fmeasure for tackling large-scale streaming data or by one pass of large-scale batch data [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We propose a Fast Online F-measure Optimization (FOFO) with a novel component for learning the optimal threshold for a probabilistic binary classifier",
        "We present a quick comparison between this work and related studies in Table 1 from various perspectives, including theoretical convergence results, consistency of F-measure optimization, memory and computational costs, where linear models are assumed for different algorithms in order to compare the memory and computational costs",
        "We address these issues by proposing a novel stochastic algorithm that does not require using the strong convexity parameter to set the step size, and can tolerate moderate noise in \u03b7t(x) to enjoy a fast convergence rate of O(1/) for minimizing Q(\u03b8)",
        "As long as maxx\u2208X |\u03b7t(x) \u2212 \u03b7(x)| \u2264 O(1/t\u03b1) for some \u03b1 > 0, a convergence result can be established for \u03b8m, which implies the consistency of Fast Online F-measure Optimization for F-measure optimization by the Proposition 13 in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We will compare with four baselines including online learning with logistic loss using 0.5 for thresholding the posterior probability, STAMP [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], OMCSL [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], and online F-measure optimization [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We proposed a fast online F-measure optimization algorithm with low memory and computational costs by learning the optimal threshold for a probabilistic classifier"
    ],
    "summary": [
        "A learning algorithm is to optimize a certain performance metric defined over a set or population of examples.",
        "The third family of methods is based on a result that the optimal classifier for maximizing the F-measure can be achieved by thresholding the posterior class probability.",
        "We propose a Fast Online F-measure Optimization (FOFO) with a novel component for learning the optimal threshold for a probabilistic binary classifier.",
        "They can only prove asymptotic convergence for learning the optimal threshold even using the true posterior probability at each iteration.",
        "With ca\u221areful design and analysis of the proposed algorithm, we are able to prove a convergence rate of O(1/ n) for learning the optimal threshold under a mild condition for learning the posterior class probability.",
        "It was shown that the optimal binary classifier that maximizes the F-measure at the population level can be achieved by thresholding the true posterior probability \u03b7(x), i.e., \u03b7\u03b8(x) = I[\u03b7(x)\u2265\u03b8] [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "An algorithm is said to be F-measure consistent if the learned classifier f satisfies F\u2217 \u2212 F (f ) \u2212\u2192p 0, as n \u2192 \u221e, where \u2212\u2192p denotes convergence in probability.",
        "We address these issues by proposing a novel stochastic algorithm that does not require using the strong convexity parameter to set the step size, and can tolerate moderate noise in \u03b7t(x) to enjoy a fast convergence rate of O(1/) for minimizing Q(\u03b8).",
        "Even though the strong convexity parameter of L(w) is not exploited, the stochastic approximation algorithm proposed in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] with a constant step size has an O(1/t) convergence rate of an averged solution\u221awt for minimizing L(w) with a large probability (e.g., 0.99).",
        "As long as maxx\u2208X |\u03b7t(x) \u2212 \u03b7(x)| \u2264 O(1/t\u03b1) for some \u03b1 > 0, a convergence result can be established for \u03b8m, which implies the consistency of FOFO for F-measure optimization by the Proposition 13 in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "We will compare with four baselines including online learning with logistic loss using 0.5 for thresholding the posterior probability, STAMP [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], OMCSL [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], and OFO [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "The comparison between FOFO and OFO verifies that the proposed method for learning the optimal threshold converges faster as they share the same component for learning the posterior probability.",
        "We proposed a fast online F-measure optimization algorithm with low memory and computational costs by learning the optimal threshold for a probabilistic classifier.",
        "We p\u221arove that the proposed algorithm for learning of the optimal threshold has a convergence rate O(1/ n), and the proposed algorithm enjoys F-measure consistency at the population level."
    ],
    "headline": "To advance online F-measure optimization, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing a stochastic strongly convex function with unknown strong convexity parameter",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alekh Agarwal, Sahand Negahban, and Martin J. Wainwright. Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions. In Advances in Neural Information Processing Systems 25 (NIPS), pages 1547\u20131555, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Stochastic%20optimization%20and%20sparse%20statistical%20recovery%3A%20Optimal%20algorithms%20for%20high%20dimensions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Negahban%2C%20Sahand%20Wainwright%2C%20Martin%20J.%20Stochastic%20optimization%20and%20sparse%20statistical%20recovery%3A%20Optimal%20algorithms%20for%20high%20dimensions%202012"
        },
        {
            "id": "2",
            "entry": "[2] Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n). In Advances in Neural Information Processing Systems (NIPS), pages 773\u2013781, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Francis%2C%20R.%20Bach%20and%20Eric%20Moulines.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20o%281/n%29%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Francis%2C%20R.%20Bach%20and%20Eric%20Moulines.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20o%281/n%29%202013"
        },
        {
            "id": "3",
            "entry": "[3] R\u00f3bert Busa-Fekete, Bal\u00e1zs Sz\u00f6r\u00e9nyi, Krzysztof Dembczynski, and Eyke H\u00fcllermeier. Online f-measure optimization. In Advances in Neural Information Processing Systems, pages 595\u2013603, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Busa-Fekete%2C%20R%C3%B3bert%20Sz%C3%B6r%C3%A9nyi%2C%20Bal%C3%A1zs%20Dembczynski%2C%20Krzysztof%20H%C3%BCllermeier%2C%20Eyke%20Online%20f-measure%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Busa-Fekete%2C%20R%C3%B3bert%20Sz%C3%B6r%C3%A9nyi%2C%20Bal%C3%A1zs%20Dembczynski%2C%20Krzysztof%20H%C3%BCllermeier%2C%20Eyke%20Online%20f-measure%20optimization%202015"
        },
        {
            "id": "4",
            "entry": "[4] Nicolo Cesa-Bianchi, Philip M Long, and Manfred K Warmuth. Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. IEEE Transactions on Neural Networks, 7(3):604\u2013619, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicolo%20Long%2C%20Philip%20M.%20Warmuth%2C%20Manfred%20K.%20Worst-case%20quadratic%20loss%20bounds%20for%20prediction%20using%20linear%20functions%20and%20gradient%20descent%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20Nicolo%20Long%2C%20Philip%20M.%20Warmuth%2C%20Manfred%20K.%20Worst-case%20quadratic%20loss%20bounds%20for%20prediction%20using%20linear%20functions%20and%20gradient%20descent%201996"
        },
        {
            "id": "5",
            "entry": "[5] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):27, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20Libsvm%3A%20a%20library%20for%20support%20vector%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20Libsvm%3A%20a%20library%20for%20support%20vector%20machines%202011"
        },
        {
            "id": "6",
            "entry": "[6] Peter D. Gr\u00fcnwald. The Minimum Description Length Principle (Adaptive Computation and Machine Learning). The MIT Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%BCnwald%2C%20Peter%20D.%20The%20Minimum%20Description%20Length%20Principle%20%28Adaptive%20Computation%20and%20Machine%20Learning%29%202007"
        },
        {
            "id": "7",
            "entry": "[7] Thorsten Joachims. A support vector method for multivariate performance measures. In Proceedings of the 22Nd International Conference on Machine Learning (ICML), pages 377\u2013 384, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Thorsten%20Joachims.%20A%20support%20vector%20method%20for%20multivariate%20performance%20measures%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Thorsten%20Joachims.%20A%20support%20vector%20method%20for%20multivariate%20performance%20measures%202005"
        },
        {
            "id": "8",
            "entry": "[8] Anatoli Juditsky, Yuri Nesterov, et al. Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stochastic Systems, 4(1):44\u201380, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Juditsky%2C%20Anatoli%20Nesterov%2C%20Yuri%20Deterministic%20and%20stochastic%20primal-dual%20subgradient%20algorithms%20for%20uniformly%20convex%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Juditsky%2C%20Anatoli%20Nesterov%2C%20Yuri%20Deterministic%20and%20stochastic%20primal-dual%20subgradient%20algorithms%20for%20uniformly%20convex%20minimization%202014"
        },
        {
            "id": "9",
            "entry": "[9] Purushottam Kar, Harikrishna Narasimhan, and Prateek Jain. Online and stochastic gradient methods for non-decomposable loss functions. In Advances in Neural Information Processing Systems 27 (NIPS), pages 694\u2013702, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kar%2C%20Purushottam%20Narasimhan%2C%20Harikrishna%20Jain%2C%20Prateek%20Online%20and%20stochastic%20gradient%20methods%20for%20non-decomposable%20loss%20functions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kar%2C%20Purushottam%20Narasimhan%2C%20Harikrishna%20Jain%2C%20Prateek%20Online%20and%20stochastic%20gradient%20methods%20for%20non-decomposable%20loss%20functions%202014"
        },
        {
            "id": "10",
            "entry": "[10] Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1\u201363, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kivinen%2C%20Jyrki%20Warmuth%2C%20Manfred%20K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kivinen%2C%20Jyrki%20Warmuth%2C%20Manfred%20K.%20Exponentiated%20gradient%20versus%20gradient%20descent%20for%20linear%20predictors%201997"
        },
        {
            "id": "11",
            "entry": "[11] Mingrui Liu, Xiaoxuan Zhang, Zaiyi Chen, Xiaoyu Wang, and Tianbao Yang. Fast stochastic AUC maximization with o(1/n)-convergence rate. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 3195\u20133203, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Mingrui%20Zhang%2C%20Xiaoxuan%20Chen%2C%20Zaiyi%20Wang%2C%20Xiaoyu%20Fast%20stochastic%20AUC%20maximization%20with%20o%281/n%29-convergence%20rate%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Mingrui%20Zhang%2C%20Xiaoxuan%20Chen%2C%20Zaiyi%20Wang%2C%20Xiaoyu%20Fast%20stochastic%20AUC%20maximization%20with%20o%281/n%29-convergence%20rate%202018"
        },
        {
            "id": "12",
            "entry": "[12] Mingrui Liu, Xiaoxuan Zhang, Lijun Zhang, Rong Jin, and Tianbao Yang. Fast rates of erm and stochastic approximation: Adaptive to error bound conditions. In Advances in Neural Information Processing Systems (NIPS), pages \u2013, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Mingrui%20Zhang%2C%20Xiaoxuan%20Zhang%2C%20Lijun%20Jin%2C%20Rong%20Fast%20rates%20of%20erm%20and%20stochastic%20approximation%3A%20Adaptive%20to%20error%20bound%20conditions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Mingrui%20Zhang%2C%20Xiaoxuan%20Zhang%2C%20Lijun%20Jin%2C%20Rong%20Fast%20rates%20of%20erm%20and%20stochastic%20approximation%3A%20Adaptive%20to%20error%20bound%20conditions%202018"
        },
        {
            "id": "13",
            "entry": "[13] Maciej A Mazurowski, Piotr A Habas, Jacek M Zurada, Joseph Y Lo, Jay A Baker, and Georgia D Tourassi. Training neural network classifiers for medical decision making: The effects of imbalanced datasets on classification performance. Neural networks, 21(2-3):427\u2013436, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mazurowski%2C%20Maciej%20A.%20Habas%2C%20Piotr%20A.%20Zurada%2C%20Jacek%20M.%20Lo%2C%20Joseph%20Y.%20Training%20neural%20network%20classifiers%20for%20medical%20decision%20making%3A%20The%20effects%20of%20imbalanced%20datasets%20on%20classification%20performance%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mazurowski%2C%20Maciej%20A.%20Habas%2C%20Piotr%20A.%20Zurada%2C%20Jacek%20M.%20Lo%2C%20Joseph%20Y.%20Training%20neural%20network%20classifiers%20for%20medical%20decision%20making%3A%20The%20effects%20of%20imbalanced%20datasets%20on%20classification%20performance%202008"
        },
        {
            "id": "14",
            "entry": "[14] Harikrishna Narasimhan, Purushottam Kar, and Prateek Jain. Optimizing non-decomposable performance measures: A tale of two classes. In International Conference on Machine Learning, pages 199\u2013208, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Narasimhan%2C%20Harikrishna%20Kar%2C%20Purushottam%20Jain%2C%20Prateek%20Optimizing%20non-decomposable%20performance%20measures%3A%20A%20tale%20of%20two%20classes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Narasimhan%2C%20Harikrishna%20Kar%2C%20Purushottam%20Jain%2C%20Prateek%20Optimizing%20non-decomposable%20performance%20measures%3A%20A%20tale%20of%20two%20classes%202015"
        },
        {
            "id": "15",
            "entry": "[15] Nagarajan Natarajan, Oluwasanmi Koyejo, Pradeep Ravikumar, and Inderjit S. Dhillon. Consistent binary classification with generalized performance metrics. In Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20Nagarajan%20Koyejo%2C%20Oluwasanmi%20Ravikumar%2C%20Pradeep%20Dhillon%2C%20Inderjit%20S.%20Consistent%20binary%20classification%20with%20generalized%20performance%20metrics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20Nagarajan%20Koyejo%2C%20Oluwasanmi%20Ravikumar%2C%20Pradeep%20Dhillon%2C%20Inderjit%20S.%20Consistent%20binary%20classification%20with%20generalized%20performance%20metrics%202014"
        },
        {
            "id": "16",
            "entry": "[16] Shameem Puthiya Parambath, Nicolas Usunier, and Yves Grandvalet. Optimizing f-measures by cost-sensitive classification. In Advances in Neural Information Processing Systems 27, pages 2123\u20132131, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parambath%2C%20Shameem%20Puthiya%20Usunier%2C%20Nicolas%20Grandvalet%2C%20Yves%20Optimizing%20f-measures%20by%20cost-sensitive%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parambath%2C%20Shameem%20Puthiya%20Usunier%2C%20Nicolas%20Grandvalet%2C%20Yves%20Optimizing%20f-measures%20by%20cost-sensitive%20classification%202014"
        },
        {
            "id": "17",
            "entry": "[17] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of International Conference on Machine Learning (ICML), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012"
        },
        {
            "id": "18",
            "entry": "[18] Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenblatt%2C%20Frank%20The%20perceptron%3A%20A%20probabilistic%20model%20for%20information%20storage%20and%20organization%20in%20the%20brain%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosenblatt%2C%20Frank%20The%20perceptron%3A%20A%20probabilistic%20model%20for%20information%20storage%20and%20organization%20in%20the%20brain%201958"
        },
        {
            "id": "19",
            "entry": "[19] Clayton Scott. Calibrated asymmetric surrogate losses. Electron. J. Statist., 6:958\u2013992, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20Clayton%20Calibrated%20asymmetric%20surrogate%20losses%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scott%2C%20Clayton%20Calibrated%20asymmetric%20surrogate%20losses%202012"
        },
        {
            "id": "20",
            "entry": "[20] Yuchun Tang, Sven Krasser, Paul Judge, and Yan-Qing Zhang. Fast and effective spam sender detection with granular svm on highly imbalanced mail server behavior data. In Collaborative Computing: Networking, Applications and Worksharing, 2006. CollaborateCom 2006. International Conference on, pages 1\u20136. IEEE, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Yuchun%20Krasser%2C%20Sven%20Judge%2C%20Paul%20Zhang%2C%20Yan-Qing%20Fast%20and%20effective%20spam%20sender%20detection%20with%20granular%20svm%20on%20highly%20imbalanced%20mail%20server%20behavior%20data%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Yuchun%20Krasser%2C%20Sven%20Judge%2C%20Paul%20Zhang%2C%20Yan-Qing%20Fast%20and%20effective%20spam%20sender%20detection%20with%20granular%20svm%20on%20highly%20imbalanced%20mail%20server%20behavior%20data%202006"
        },
        {
            "id": "21",
            "entry": "[21] Tim van Erven, Peter D. Gr\u00fcnwald, Nishant A. Mehta, Mark D. Reid, and Robert C. Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Erven%2C%20Tim%20Gr%C3%BCnwald%2C%20Peter%20D.%20Mehta%2C%20Nishant%20A.%20Reid%2C%20Mark%20D.%20Fast%20rates%20in%20statistical%20and%20online%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Erven%2C%20Tim%20Gr%C3%BCnwald%2C%20Peter%20D.%20Mehta%2C%20Nishant%20A.%20Reid%2C%20Mark%20D.%20Fast%20rates%20in%20statistical%20and%20online%20learning%202015"
        },
        {
            "id": "22",
            "entry": "[22] Yi Xu, Qihang Lin, and Tianbao Yang. Stochastic convex optimization: Faster local growth implies faster global convergence. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 3821\u20133830, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Yi%20Lin%2C%20Qihang%20Yang%2C%20Tianbao%20Stochastic%20convex%20optimization%3A%20Faster%20local%20growth%20implies%20faster%20global%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Yi%20Lin%2C%20Qihang%20Yang%2C%20Tianbao%20Stochastic%20convex%20optimization%3A%20Faster%20local%20growth%20implies%20faster%20global%20convergence%202017"
        },
        {
            "id": "23",
            "entry": "[23] Yan Yan, Tianbao Yang, Yi Yang, and Jianhui Chen. A framework of online learning with imbalanced streaming data. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI), pages 2817\u20132823, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20Yan%20Yang%2C%20Tianbao%20Yang%2C%20Yi%20Chen%2C%20Jianhui%20A%20framework%20of%20online%20learning%20with%20imbalanced%20streaming%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20Yan%20Yang%2C%20Tianbao%20Yang%2C%20Yi%20Chen%2C%20Jianhui%20A%20framework%20of%20online%20learning%20with%20imbalanced%20streaming%20data%202017"
        },
        {
            "id": "24",
            "entry": "[24] Nan Ye, Kian Ming Adam Chai, Wee Sun Lee, and Hai Leong Chieu. Optimizing f-measure: A tale of two approaches. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20Nan%20Chai%2C%20Kian%20Ming%20Adam%20Lee%2C%20Wee%20Sun%20Chieu%2C%20Hai%20Leong%20Optimizing%20f-measure%3A%20A%20tale%20of%20two%20approaches%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20Nan%20Chai%2C%20Kian%20Ming%20Adam%20Lee%2C%20Wee%20Sun%20Chieu%2C%20Hai%20Leong%20Optimizing%20f-measure%3A%20A%20tale%20of%20two%20approaches%202012"
        },
        {
            "id": "25",
            "entry": "[25] Tong Zhang. From -entropy to kl-entropy: Analysis of minimum information complexity density estimation. Ann. Statist., 34(5):2180\u20132210, 10 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20From%20-entropy%20to%20kl-entropy%3A%20Analysis%20of%20minimum%20information%20complexity%20density%20estimation%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20From%20-entropy%20to%20kl-entropy%3A%20Analysis%20of%20minimum%20information%20complexity%20density%20estimation%202006"
        },
        {
            "id": "26",
            "entry": "[26] Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, and Gavin Brown. Beyond fano\u2019s inequality: bounds on the optimal f-score, ber, and cost-sensitive risk and their implications. Journal of Machine Learning Research, 14(Apr):1033\u20131090, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Ming-Jie%20Edakunni%2C%20Narayanan%20Pocock%2C%20Adam%20Brown%2C%20Gavin%20Beyond%20fano%E2%80%99s%20inequality%3A%20bounds%20on%20the%20optimal%20f-score%2C%20ber%2C%20and%20cost-sensitive%20risk%20and%20their%20implications%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Ming-Jie%20Edakunni%2C%20Narayanan%20Pocock%2C%20Adam%20Brown%2C%20Gavin%20Beyond%20fano%E2%80%99s%20inequality%3A%20bounds%20on%20the%20optimal%20f-score%2C%20ber%2C%20and%20cost-sensitive%20risk%20and%20their%20implications%202013"
        },
        {
            "id": "27",
            "entry": "[27] Peilin Zhao and Steven CH Hoi. Cost-sensitive online active learning with application to malicious url detection. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 919\u2013927. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Peilin%20Hoi%2C%20Steven%20C.H.%20Cost-sensitive%20online%20active%20learning%20with%20application%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Peilin%20Hoi%2C%20Steven%20C.H.%20Cost-sensitive%20online%20active%20learning%20with%20application%202013"
        },
        {
            "id": "28",
            "entry": "[28] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the International Conference on Machine Learning (ICML), pages 928\u2013936, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
