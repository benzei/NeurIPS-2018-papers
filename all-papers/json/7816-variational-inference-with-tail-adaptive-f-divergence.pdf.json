{
    "filename": "7816-variational-inference-with-tail-adaptive-f-divergence.pdf",
    "metadata": {
        "title": "Variational Inference with Tail-adaptive f-Divergence",
        "date": 2018,
        "author": "Dilin Wang UT Austin dilin@cs.utexas.edu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7816-variational-inference-with-tail-adaptive-f-divergence.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Variational inference with \u03b1-divergences has been widely used in modern probabilistic machine learning. Compared to Kullback-Leibler (KL) divergence, a major advantage of using \u03b1-divergences (with positive \u03b1 values) is their mass-covering property. However, estimating and optimizing \u03b1-divergences require to use importance sampling, which may have large or infinite variance due to heavy tails of importance weights. In this paper, we propose a new class of tail-adaptive f divergences that adaptively change the convex function f with the tail distribution of the importance weights, in a way that theoretically guarantees finite moments, while simultaneously achieving mass-covering properties. We test our methods on Bayesian neural networks, as well as deep reinforcement learning in which our method is applied to improve a recent soft actor-critic (SAC) algorithm (<a class=\"ref-link\" id=\"cHaarnoja_et+al_2018_a\" href=\"#rHaarnoja_et+al_2018_a\">Haarnoja et al., 2018</a>). Our results show that our approach yields significant advantages compared with existing methods based on classical KL and \u03b1-divergences."
    },
    "keywords": [
        {
            "term": "heavy tail",
            "url": "https://en.wikipedia.org/wiki/heavy_tail"
        },
        {
            "term": "bayesian neural network",
            "url": "https://en.wikipedia.org/wiki/bayesian_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "Markov chain Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"
        },
        {
            "term": "importance sampling",
            "url": "https://en.wikipedia.org/wiki/importance_sampling"
        },
        {
            "term": "new class",
            "url": "https://en.wikipedia.org/wiki/new_class"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Variational inference",
            "url": "https://en.wikipedia.org/wiki/Variational_inference"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Variational inference (VI) (e.g., <a class=\"ref-link\" id=\"cJordan_et+al_1999_a\" href=\"#rJordan_et+al_1999_a\"><a class=\"ref-link\" id=\"cJordan_et+al_1999_a\" href=\"#rJordan_et+al_1999_a\">Jordan et al, 1999</a></a>; Wainwright et al, 2008) has been established as a powerful tool in modern probabilistic machine learning for approximating intractable posterior distributions",
        "Based on new observations on f -divergence and theoretical properties of fat tail distributions, we design a new class of f -divergence which is tail-adaptive in that it uses different f functions according to the tail distribution of the density ratio p(x)/q(x) to simultaneously obtain stable empirical estimation and a strongest possible mass-covering property",
        "We demonstrate an application of our method in reinforcement learning, applying it as an inner loop to improve a recent soft actor-critic(SAC) algorithm (<a class=\"ref-link\" id=\"cHaarnoja_et+al_2018_a\" href=\"#rHaarnoja_et+al_2018_a\">Haarnoja et al, 2018</a>)",
        "We find that our improvement is especially significant on high dimensional and complex environments like Ant and Humanoid",
        "We present a new class of tail-adaptive f -divergence and exploit its application in variational inference and reinforcement learning",
        "Empirical results on Bayesian neural networks and reinforcement learning indicate that our approach outperforms standard \u03b1-divergence, especially for high dimensional multi-modal distribution"
    ],
    "key_statements": [
        "Variational inference (VI) (e.g., <a class=\"ref-link\" id=\"cJordan_et+al_1999_a\" href=\"#rJordan_et+al_1999_a\"><a class=\"ref-link\" id=\"cJordan_et+al_1999_a\" href=\"#rJordan_et+al_1999_a\">Jordan et al, 1999</a></a>; Wainwright et al, 2008) has been established as a powerful tool in modern probabilistic machine learning for approximating intractable posterior distributions",
        "Based on new observations on f -divergence and theoretical properties of fat tail distributions, we design a new class of f -divergence which is tail-adaptive in that it uses different f functions according to the tail distribution of the density ratio p(x)/q(x) to simultaneously obtain stable empirical estimation and a strongest possible mass-covering property",
        "We address the aforementioned problem by designing a generalization of f -divergence in which f adaptively changes with p and q, in a way that always guarantees the existence of the expectation, while simultaneous achieving strong mass-covering equivalent to that of the \u03b1-divergence with \u03b1 = \u03b1\u2217",
        "We evaluate our approach on Bayesian neural network regression tasks",
        "We demonstrate an application of our method in reinforcement learning, applying it as an inner loop to improve a recent soft actor-critic(SAC) algorithm (<a class=\"ref-link\" id=\"cHaarnoja_et+al_2018_a\" href=\"#rHaarnoja_et+al_2018_a\">Haarnoja et al, 2018</a>)",
        "We find that our improvement is especially significant on high dimensional and complex environments like Ant and Humanoid",
        "We present a new class of tail-adaptive f -divergence and exploit its application in variational inference and reinforcement learning",
        "Empirical results on Bayesian neural networks and reinforcement learning indicate that our approach outperforms standard \u03b1-divergence, especially for high dimensional multi-modal distribution"
    ],
    "summary": [
        "Variational inference (VI) (e.g., <a class=\"ref-link\" id=\"cJordan_et+al_1999_a\" href=\"#rJordan_et+al_1999_a\"><a class=\"ref-link\" id=\"cJordan_et+al_1999_a\" href=\"#rJordan_et+al_1999_a\">Jordan et al, 1999</a></a>; Wainwright et al, 2008) has been established as a powerful tool in modern probabilistic machine learning for approximating intractable posterior distributions.",
        "Based on new observations on f -divergence and theoretical properties of fat tail distributions, we design a new class of f -divergence which is tail-adaptive in that it uses different f functions according to the tail distribution of the density ratio p(x)/q(x) to simultaneously obtain stable empirical estimation and a strongest possible mass-covering property.",
        "This allows us to derive a new adaptive f -divergence-based variational inference by combining with stochastic optimization and reparameterization gradient estimates.",
        "The results above show that it is sufficient to find an increasing function \u03c1f , or a non-negative function \u03b3f to obtain adaptive f -divergence with computable gradients.",
        "Because the magnitude of \u2207\u03b8 log q\u03b8(x), \u2207x log(p(x)/q\u03b8(x)) and \u2207\u03b8g\u03b8(\u03be) are relatively small compared with the ratio p(x)/q(x), we can mainly consider designing function \u03c1 such that they yield finite expectation Ex\u223cq[\u03c1(p(x)/q(x))] < \u221e; we should keep the function large, preferably with the same magnitude as t\u03b1\u2217 , to provide a strong mode-covering property.",
        "As we increase the dimension to 10, our approach with tail-adaptive f -divergence achieves the best performance.",
        "To examine the performance of variational approximation more closely, we show in Figure 2 the average mode-shift distance and the MSE of the estimated mean and variance as we gradually increase the non-Gaussianality of p(x) by changing s.",
        "We can see from Table 1 that our approach takes advantage of an adaptive choice of f -divergence and achieves the best performance for both test RMSE and test log-likelihood in most of the cases.",
        "Where \u03c4 is a temperature parameter, and V (s) = \u03c4 log a exp(Q(a, s)/\u03c4 ), serving as a normalization constant here, is a soft-version of value function and is iteratively updated in SAC.",
        "By using our tail-adaptive f -divergence, we can readily apply our Algorithm 1 to update \u03c0 in SAC, allowing us to obtain \u03c0 that counts the multi-modality of Q(a, s) more efficiently.",
        "For non-negative \u03b1 settings, methods with larger \u03b1 give higher average reward than the original KL-based SAC in most of the cases.",
        "We present a new class of tail-adaptive f -divergence and exploit its application in variational inference and reinforcement learning.",
        "Compared to classic \u03b1-divergence, our approach guarantees finite moments of the density ratio and provides more stable importance weights and gradient estimates.",
        "Empirical results on Bayesian neural networks and reinforcement learning indicate that our approach outperforms standard \u03b1-divergence, especially for high dimensional multi-modal distribution."
    ],
    "headline": "We propose a new class of tail-adaptive f divergences that adaptively change the convex function f with the tail distribution of the importance weights, in a way that theoretically guarantees finite moments, while simultaneously achieving mass-covering properties",
    "reference_links": [
        {
            "id": "Blei_et+al_2017_a",
            "entry": "Blei, David M, Kucukelbir, Alp, and McAuliffe, Jon D. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "Burda_et+al_2016_a",
            "entry": "Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan. Importance weighted autoencoders. International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burda%2C%20Yuri%20Grosse%2C%20Roger%20Salakhutdinov%2C%20Ruslan%20Importance%20weighted%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burda%2C%20Yuri%20Grosse%2C%20Roger%20Salakhutdinov%2C%20Ruslan%20Importance%20weighted%20autoencoders%202016"
        },
        {
            "id": "Capp_et+al_2008_a",
            "entry": "Capp\u00e9, Olivier, Douc, Randal, Guillin, Arnaud, Marin, Jean-Michel, and Robert, Christian P. Adaptive importance sampling in general mixture classes. Statistics and Computing, 18(4):447\u2013459, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Capp%C3%A9%2C%20Olivier%20Douc%2C%20Randal%20Guillin%2C%20Arnaud%20Marin%2C%20Jean-Michel%20Adaptive%20importance%20sampling%20in%20general%20mixture%20classes%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Capp%C3%A9%2C%20Olivier%20Douc%2C%20Randal%20Guillin%2C%20Arnaud%20Marin%2C%20Jean-Michel%20Adaptive%20importance%20sampling%20in%20general%20mixture%20classes%202008"
        },
        {
            "id": "Christopher_2016_a",
            "entry": "Christopher, M Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christopher%2C%20M.Bishop%20Pattern%20Recognition%20and%20Machine%20Learning%202016"
        },
        {
            "id": "Cotter_et+al_2015_a",
            "entry": "Cotter, Colin, Cotter, Simon, and Russell, Paul. Parallel adaptive importance sampling. arXiv preprint arXiv:1508.01132, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.01132"
        },
        {
            "id": "Csisz_2004_a",
            "entry": "Csisz\u00e1r, I. and Shields, P.C. Information theory and statistics: A tutorial. Foundations and Trends in Communications and Information Theory, 1(4):417\u2013528, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Csisz%C3%A1r%2C%20I.%20Shields%2C%20P.C.%20Information%20theory%20and%20statistics%3A%20A%20tutorial.%20Foundations%20and%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Csisz%C3%A1r%2C%20I.%20Shields%2C%20P.C.%20Information%20theory%20and%20statistics%3A%20A%20tutorial.%20Foundations%20and%202004"
        },
        {
            "id": "Boer_et+al_2005_a",
            "entry": "De Boer, Pieter-Tjerk, Kroese, Dirk P, Mannor, Shie, and Rubinstein, Reuven Y. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19\u201367, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boer%2C%20De%20Pieter-Tjerk%2C%20Kroese%20P%2C%20Dirk%20Mannor%2C%20Shie%20A%20tutorial%20on%20the%20cross-entropy%20method%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boer%2C%20De%20Pieter-Tjerk%2C%20Kroese%20P%2C%20Dirk%20Mannor%2C%20Shie%20A%20tutorial%20on%20the%20cross-entropy%20method%202005"
        },
        {
            "id": "Dieng_et+al_2017_a",
            "entry": "Dieng, Adji Bousso, Tran, Dustin, Ranganath, Rajesh, Paisley, John, and Blei, David. Variational inference via \u03c7 upper bound minimization. In Advances in Neural Information Processing Systems (NIPS), pp. 2732\u20132741, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieng%2C%20Adji%20Bousso%20Tran%2C%20Dustin%20Ranganath%2C%20Rajesh%20Paisley%2C%20John%20Variational%20inference%20via%20%CF%87%20upper%20bound%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieng%2C%20Adji%20Bousso%20Tran%2C%20Dustin%20Ranganath%2C%20Rajesh%20Paisley%2C%20John%20Variational%20inference%20via%20%CF%87%20upper%20bound%20minimization%202017"
        },
        {
            "id": "Gal_2016_a",
            "entry": "Gal, Yarin and Ghahramani, Zoubin. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning (ICML), pp. 1050\u2013 1059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20Bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "Haarnoja_et+al_2018_a",
            "entry": "Haarnoja, Tuomas, Zhou, Aurick, Abbeel, Pieter, and Levine, Sergey. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20Tuomas%20Zhou%2C%20Aurick%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20Tuomas%20Zhou%2C%20Aurick%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018"
        },
        {
            "id": "Hern_et+al_2016_a",
            "entry": "Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel, Li, Yingzhen, Rowland, Mark, Hern\u00e1ndez-Lobato, Daniel, Bui, Thang, and Turner, Richard Eric. Black-box \u03b1-divergence minimization. International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Li%2C%20Yingzhen%20Rowland%2C%20Mark%20Hern%C3%A1ndez-Lobato%2C%20Daniel%20Black-box%20%CE%B1-divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20Jos%C3%A9%20Miguel%20Li%2C%20Yingzhen%20Rowland%2C%20Mark%20Hern%C3%A1ndez-Lobato%2C%20Daniel%20Black-box%20%CE%B1-divergence%20minimization%202016"
        },
        {
            "id": "Hill_1975_a",
            "entry": "Hill, Bruce M et al. A simple general approach to inference about the tail of a distribution. The annals of statistics, 3(5):1163\u20131174, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hill%2C%20Bruce%20M.%20A%20simple%20general%20approach%20to%20inference%20about%20the%20tail%20of%20a%20distribution%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hill%2C%20Bruce%20M.%20A%20simple%20general%20approach%20to%20inference%20about%20the%20tail%20of%20a%20distribution%201975"
        },
        {
            "id": "Hoffman_et+al_2013_a",
            "entry": "Hoffman, Matthew D, Blei, David M, Wang, Chong, and Paisley, John. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "Jang_et+al_2017_a",
            "entry": "Jang, Eric, Gu, Shixiang, and Poole, Ben. Categorical reparameterization with Gumbel-softmax. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20Gumbel-softmax.%20International%20Conference%20on%20Learning%20Representations%20%28ICLR%29%202017"
        },
        {
            "id": "Jordan_et+al_1999_a",
            "entry": "Jordan, Michael I, Ghahramani, Zoubin, Jaakkola, Tommi S, and Saul, Lawrence K. An introduction to variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20Michael%20I.%20Ghahramani%2C%20Zoubin%20Jaakkola%2C%20Tommi%20S.%20Saul%2C%20Lawrence%20K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201999"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Kingma, Diederik P and Ba, Jimmy. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Kingma, Diederik P and Welling, Max. Auto-encoding variational Bayes. International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Li_2016_a",
            "entry": "Li, Yingzhen and Turner, Richard E. R\u00e9nyi divergence variational inference. In Advances in Neural Information Processing Systems (NIPS), pp. 1073\u20131081, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yingzhen%20Turner%2C%20Richard%20E.%20R%C3%A9nyi%20divergence%20variational%20inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yingzhen%20Turner%2C%20Richard%20E.%20R%C3%A9nyi%20divergence%20variational%20inference%202016"
        },
        {
            "id": "Maddison_et+al_2017_a",
            "entry": "Maddison, Chris J, Mnih, Andriy, and Teh, Yee Whye. The concrete distribution: A continuous relaxation of discrete random variables. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20Teh%20Yee%20Whye.%20The%20concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20Teh%20Yee%20Whye.%20The%20concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017"
        },
        {
            "id": "Minka_2001_a",
            "entry": "Minka, Thomas P. Expectation propagation for approximate Bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence (UAI), pp. 362\u2013369. Morgan Kaufmann Publishers Inc., 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20Thomas%20P.%20Expectation%20propagation%20for%20approximate%20Bayesian%20inference%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minka%2C%20Thomas%20P.%20Expectation%20propagation%20for%20approximate%20Bayesian%20inference%202001"
        },
        {
            "id": "Minka_2005_a",
            "entry": "Minka, Tom et al. Divergence measures and message passing. Technical report, Microsoft Research, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20Tom%20Divergence%20measures%20and%20message%20passing%202005"
        },
        {
            "id": "Opper_2005_a",
            "entry": "Opper, Manfred and Winther, Ole. Expectation consistent approximate inference. Journal of Machine Learning Research, 6(Dec):2177\u20132204, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opper%2C%20Manfred%20Winther%2C%20Ole%20Expectation%20consistent%20approximate%20inference%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Opper%2C%20Manfred%20Winther%2C%20Ole%20Expectation%20consistent%20approximate%20inference%202005"
        },
        {
            "id": "Ranganath_et+al_2014_a",
            "entry": "Ranganath, Rajesh, Gerrish, Sean, and Blei, David. Black box variational inference. In Artificial Intelligence and Statistics, pp. 814\u2013822, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20Rajesh%20Gerrish%2C%20Sean%20Blei%2C%20David%20Black%20box%20variational%20inference%202014"
        },
        {
            "id": "Resnick_2007_a",
            "entry": "Resnick, Sidney I. Heavy-tail phenomena: probabilistic and statistical modeling. Springer Science & Business Media, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Resnick%2C%20Sidney%20I.%20Heavy-tail%20phenomena%3A%20probabilistic%20and%20statistical%20modeling%202007"
        },
        {
            "id": "Ryu_2014_a",
            "entry": "Ryu, Ernest K and Boyd, Stephen P. Adaptive importance sampling via stochastic convex programming. arXiv preprint arXiv:1412.4845, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.4845"
        },
        {
            "id": "Vehtari_et+al_2015_a",
            "entry": "Vehtari, Aki, Gelman, Andrew, and Gabry, Jonah. Pareto smoothed importance sampling. arXiv preprint arXiv:1507.02646, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.02646"
        },
        {
            "id": "Wainwright_2008_a",
            "entry": "Wainwright, Martin J, Jordan, Michael I, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1\u20132):1\u2013305, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20J.%20Jordan%2C%20Michael%20I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008"
        }
    ]
}
