{
    "filename": "7972-a-bayesian-approach-to-generative-adversarial-imitation-learning.pdf",
    "metadata": {
        "title": "A Bayesian Approach to Generative Adversarial Imitation Learning",
        "author": "Wonseok Jeon, Seokin Seo, Kee-Eung Kim",
        "date": 2016,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7972-a-bayesian-approach-to-generative-adversarial-imitation-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generative adversarial training for imitation learning has shown promising results on high-dimensional and continuous control tasks. This paradigm is based on reducing the imitation learning problem to the density matching problem, where the agent iteratively refines the policy to match the empirical state-action visitation frequency of the expert demonstration. Although this approach can robustly learn to imitate even with scarce demonstration, one must still address the inherent challenge that collecting trajectory samples in each iteration is a costly operation. To address this issue, we first propose a Bayesian formulation of generative adversarial imitation learning (GAIL), where the imitation policy and the cost function are represented as stochastic neural networks. Then, we show that we can significantly enhance the sample efficiency of GAIL leveraging the predictive density of the cost, on an extensive set of imitation learning tasks with high-dimensional states and actions."
    },
    "keywords": [
        {
            "term": "radial basis function",
            "url": "https://en.wikipedia.org/wiki/radial_basis_function"
        },
        {
            "term": "trust region",
            "url": "https://en.wikipedia.org/wiki/trust_region"
        },
        {
            "term": "GAIL",
            "url": "https://en.wikipedia.org/wiki/GAIL"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "Inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Inverse_reinforcement_learning"
        },
        {
            "term": "matching problem",
            "url": "https://en.wikipedia.org/wiki/matching_problem"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "bayesian approach",
            "url": "https://en.wikipedia.org/wiki/bayesian_approach"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "cost function",
            "url": "https://en.wikipedia.org/wiki/cost_function"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Imitation learning is the problem where an agent learns to mimic the demonstration provided by the expert, in an environment with unknown cost function",
        "Generative adversarial imitation learning is analyzed in the Bayesian approach, and such approach can lead to highly sampleefficient model-free imitation learning",
        "Our Bayesian approach is related to Bayesian generative adversarial networks [<a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\">Saatci and Wilson, 2017</a>] that considered the posterior distributions of both generator and discriminator parameters in the generative adversarial training",
        "The posterior for the agentexpert discriminator was used for the predictive density of the cost during training, whereas only a point estimate for the policy parameter was used for simplicity",
        "For the theoretical analysis, ours slightly differs from the analysis in Bayesian generative adversarial networks due to the inter-trajectory correlation from Markov decision process formulation in our work. This makes the objective of original Generative adversarial imitation learning regarded as the surrogate objective in our Bayesian approach, whereas the objective of Bayesian generative adversarial networks is exactly reduced into that of original generative adversarial networks for ML point estimation",
        "We think our analysis fills the gap between theory and experiments in Generative adversarial imitation learning since Generative adversarial imitation learning was theoretically analyzed based on the discounted occupancy measure, which can be defined in the infinite-horizon setting, but their experiments were only done on the finite-horizon tasks in MuJoCo simulator"
    ],
    "key_statements": [
        "Imitation learning is the problem where an agent learns to mimic the demonstration provided by the expert, in an environment with unknown cost function",
        "Generative adversarial imitation learning (GAIL) [Ho and Ermon, 2016], which is of our primary interest, is a recent instance of imitation learning algorithms with policy gradients",
        "Based on the promising results from Generative adversarial imitation learning, a number of improvements appeared in the literature [<a class=\"ref-link\" id=\"cWang_et+al_2017_a\" href=\"#rWang_et+al_2017_a\">Wang et al, 2017</a>, <a class=\"ref-link\" id=\"cLi_et+al_2017_a\" href=\"#rLi_et+al_2017_a\">Li et al, 2017</a>]",
        "We show that Generative adversarial imitation learning can be seen as optimizing a surrogate objective in our approach, with iterative updates being maximum-likelihood (ML) point estimations",
        "We evaluated our Bayesian Generative Adversarial Imitation Learning on five continuous control tasks (Hopper-v1, Walker2d-v1, HalfCheetahv1, Ant-v1, Humanoid-v1) from OpenAI Gym, implemented with the MuJoCo physics simulator [<a class=\"ref-link\" id=\"cTodorov_et+al_2012_a\" href=\"#rTodorov_et+al_2012_a\">Todorov et al, 2012</a>]",
        "Generative adversarial imitation learning is analyzed in the Bayesian approach, and such approach can lead to highly sampleefficient model-free imitation learning",
        "Our Bayesian approach is related to Bayesian generative adversarial networks [<a class=\"ref-link\" id=\"cSaatci_2017_a\" href=\"#rSaatci_2017_a\">Saatci and Wilson, 2017</a>] that considered the posterior distributions of both generator and discriminator parameters in the generative adversarial training",
        "The posterior for the agentexpert discriminator was used for the predictive density of the cost during training, whereas only a point estimate for the policy parameter was used for simplicity",
        "We think our algorithm can be extended to the multi-policy imitation learning, and the sample efficiency of our algorithm may be enhanced by utilizing the posterior of the policy parameter as shown in Stein variational policy gradient (SVPG) [<a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>]",
        "For the theoretical analysis, ours slightly differs from the analysis in Bayesian generative adversarial networks due to the inter-trajectory correlation from Markov decision process formulation in our work. This makes the objective of original Generative adversarial imitation learning regarded as the surrogate objective in our Bayesian approach, whereas the objective of Bayesian generative adversarial networks is exactly reduced into that of original generative adversarial networks for ML point estimation",
        "We think our analysis fills the gap between theory and experiments in Generative adversarial imitation learning since Generative adversarial imitation learning was theoretically analyzed based on the discounted occupancy measure, which can be defined in the infinite-horizon setting, but their experiments were only done on the finite-horizon tasks in MuJoCo simulator"
    ],
    "summary": [
        "Imitation learning is the problem where an agent learns to mimic the demonstration provided by the expert, in an environment with unknown cost function.",
        "The policy being learned becomes the generator, and the cost function becomes the discriminator.",
        "We show that GAIL can be seen as optimizing a surrogate objective in our approach, with iterative updates being maximum-likelihood (ML) point estimations.",
        "Under our Bayesian framework, GAIL can be regarded as an algorithm that iteratively uses (7) and (10) for updating \u03b8 and \u03c6 using their point estimates.",
        "Since the form of the objective in (15) is the same as the policy optimization with an immediate cost function log D\u03c6prev (\u00b7, \u00b7), GAIL uses TRPO, a state-of-the-art policy gradient algorithm, for updating \u03b8.",
        "If we consider the maximization of log p(1A|\u03b8, \u03c6prev), which result from defining the imitation optimality event as oA = 1 , it can be shown that the corresponding surrogate objective becomes the policy optimization with an immediate reward function log(1 \u2212 D\u03c6prev (\u00b7, \u00b7)).",
        "From the Bayesian formulation in the previous section, GAIL can be seen as maximizing the expected log-likelihood in a point-wise manner for discriminator updates, and this makes the algorithm quite inefficient compared to using the full predictive distribution.",
        "The first setting is the same as in the authors\u2019 code, where the variance of the Gaussian policy is learnable constant parameter and a single discriminator update is performed in each iteration.",
        "In all settings of our experiments, the maximum number of expert trajectories was chosen as in Ho and Ermon [2016], i.e. 240 for Humanoid and 25 for all other tasks, and 50000 state-action pairs were used for each iteration in the first experiment.",
        "We think this is due to (1) the expressive power by using the policy with state-dependent variance, (2) the stabilization of the algorithm due to the multiple iteration for discriminator training and (3) the efficient use of expert\u2019s trajectories without sub-sampling procedure.",
        "We checked the sample efficiency of our algorithm by reducing the number of state-action pairs used for each training iteration from 50000 to 1000 for Hopper-v1, Walker2d-v1, HalfCheetahv1 and to 5000 for other much high-dimensional tasks.",
        "The posterior for the agentexpert discriminator was used for the predictive density of the cost during training, whereas only a point estimate for the policy parameter was used for simplicity.",
        "We think our algorithm can be extended to the multi-policy imitation learning, and the sample efficiency of our algorithm may be enhanced by utilizing the posterior of the policy parameter as shown in Stein variational policy gradient (SVPG) [<a class=\"ref-link\" id=\"cLiu_et+al_2017_a\" href=\"#rLiu_et+al_2017_a\">Liu et al, 2017</a>]."
    ],
    "headline": "We show that we can significantly enhance the sample efficiency of Generative adversarial imitation learning leveraging the predictive density of the cost, on an extensive set of imitation learning tasks with high-dimensional states and actions",
    "reference_links": [
        {
            "id": "Ho_et+al_2016_a",
            "entry": "Jonathan Ho, Jayesh Gupta, and Stefano Ermon. Model-free imitation learning with policy optimization. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 2760\u20132769, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Gupta%2C%20Jayesh%20Ermon%2C%20Stefano%20Model-free%20imitation%20learning%20with%20policy%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Gupta%2C%20Jayesh%20Ermon%2C%20Stefano%20Model-free%20imitation%20learning%20with%20policy%20optimization%202016"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "Ho_2016_b",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems (NIPS) 29, pages 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS) 27, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Wang_et+al_2017_a",
            "entry": "Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems (NIPS) 30, pages 5326\u20135335, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ziyu%20Merel%2C%20Josh%20S.%20Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Robust%20imitation%20of%20diverse%20behaviors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ziyu%20Merel%2C%20Josh%20S.%20Reed%2C%20Scott%20E.%20de%20Freitas%2C%20Nando%20Robust%20imitation%20of%20diverse%20behaviors%202017"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Yunzhu Li, Jiaming Song, and Stefano Ermon. InfoGAIL: Interpretable imitation learning from visual demonstrations. In Advances in Neural Information Processing Systems (NIPS) 30, pages 3815\u20133825, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yunzhu%20Song%2C%20Jiaming%20Ermon%2C%20Stefano%20InfoGAIL%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yunzhu%20Song%2C%20Jiaming%20Ermon%2C%20Stefano%20InfoGAIL%3A%20Interpretable%20imitation%20learning%20from%20visual%20demonstrations%202017"
        },
        {
            "id": "Kee-Eung_2018_a",
            "entry": "Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Kee-Eung%20Kim%20and%20Hyun%20Soo%20Park.%20Imitation%20learning%20via%20kernel%20mean%20embedding%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Kee-Eung%20Kim%20and%20Hyun%20Soo%20Park.%20Imitation%20learning%20via%20kernel%20mean%20embedding%202018"
        },
        {
            "id": "Li_et+al_2015_a",
            "entry": "Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 1718\u20131727, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Swersky%2C%20Kevin%20Zemel%2C%20Rich%20Generative%20moment%20matching%20networks%202015"
        },
        {
            "id": "Pomerleau_1991_a",
            "entry": "Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1):88\u201397, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pomerleau%2C%20Dean%20A.%20Efficient%20training%20of%20artificial%20neural%20networks%20for%20autonomous%20navigation%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pomerleau%2C%20Dean%20A.%20Efficient%20training%20of%20artificial%20neural%20networks%20for%20autonomous%20navigation%201991"
        },
        {
            "id": "Bagnell_2015_a",
            "entry": "J Andrew Bagnell. An invitation to imitation. Technical report, Carnegie Mellon University, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bagnell%2C%20J.Andrew%20An%20invitation%20to%20imitation%202015"
        },
        {
            "id": "Russell_1998_a",
            "entry": "Stuart Russell. Learning agents for uncertain environments. In Proceedings of the 11st Annual Conference on Computational Learning Theory (COLT), pages 101\u2013103, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russell%2C%20Stuart%20Learning%20agents%20for%20uncertain%20environments%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russell%2C%20Stuart%20Learning%20agents%20for%20uncertain%20environments%201998"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings of the 17th International Conference on Machine Learning (ICML), pages 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pages 1433\u20131438, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        },
        {
            "id": "Ramachandran_2007_a",
            "entry": "Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20th International Joint Conference on Artifical Intelligence (IJCAI), pages 2586\u20132591, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramachandran%2C%20Deepak%20Amir%2C%20Eyal%20Bayesian%20inverse%20reinforcement%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramachandran%2C%20Deepak%20Amir%2C%20Eyal%20Bayesian%20inverse%20reinforcement%20learning%202007"
        },
        {
            "id": "Choi_1989_a",
            "entry": "Jaedeug Choi and Kee-Eung Kim. MAP inference for Bayesian inverse reinforcement learning. In Advances in Neural Information Processing Systems (NIPS) 24, pages 1989\u20131997, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Jaedeug%20Kim%2C%20Kee-Eung%20MAP%20inference%20for%20Bayesian%20inverse%20reinforcement%20learning%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20Jaedeug%20Kim%2C%20Kee-Eung%20MAP%20inference%20for%20Bayesian%20inverse%20reinforcement%20learning%201989"
        },
        {
            "id": "Ratliff_2006_a",
            "entry": "Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In Proceedings of the 23rd International Conference on Machine Learning, pages 729\u2013736, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20A%20Zinkevich.%20Maximum%20margin%20planning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20A%20Zinkevich.%20Maximum%20margin%20planning%202006"
        },
        {
            "id": "Syed_2008_a",
            "entry": "Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 1032\u20131039, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Schapire.%20Apprenticeship%20learning%20using%20linear%20programming%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Schapire.%20Apprenticeship%20learning%20using%20linear%20programming%202008"
        },
        {
            "id": "Neu_2007_a",
            "entry": "Gergely Neu and Csaba Szepesv\u00e1ri. Apprenticeship learning using inverse reinforcement learning and gradient methods. In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI), pages 295\u2013302, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neu%2C%20Gergely%20Szepesv%C3%A1ri%2C%20Csaba%20Apprenticeship%20learning%20using%20inverse%20reinforcement%20learning%20and%20gradient%20methods%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neu%2C%20Gergely%20Szepesv%C3%A1ri%2C%20Csaba%20Apprenticeship%20learning%20using%20inverse%20reinforcement%20learning%20and%20gradient%20methods%202007"
        },
        {
            "id": "Toussaint_2009_a",
            "entry": "Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th International Conference on Machine Learning (ICML), pages 1049\u20131056. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009"
        },
        {
            "id": "Neumann_2011_a",
            "entry": "Gerhard Neumann. Variational inference for policy search in changing situations. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 817\u2013824, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neumann%2C%20Gerhard%20Variational%20inference%20for%20policy%20search%20in%20changing%20situations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neumann%2C%20Gerhard%20Variational%20inference%20for%20policy%20search%20in%20changing%20situations%202011"
        },
        {
            "id": "Abdolmaleki_et+al_2018_a",
            "entry": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimization. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abdolmaleki%2C%20Abbas%20Springenberg%2C%20Jost%20Tobias%20Tassa%2C%20Yuval%20Munos%2C%20Remi%20Maximum%20a%20posteriori%20policy%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abdolmaleki%2C%20Abbas%20Springenberg%2C%20Jost%20Tobias%20Tassa%2C%20Yuval%20Munos%2C%20Remi%20Maximum%20a%20posteriori%20policy%20optimization%202018"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03852"
        },
        {
            "id": "Fu_et+al_2018_a",
            "entry": "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Justin%20Luo%2C%20Katie%20Levine%2C%20Sergey%20Learning%20robust%20rewards%20with%20adversarial%20inverse%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Justin%20Luo%2C%20Katie%20Levine%2C%20Sergey%20Learning%20robust%20rewards%20with%20adversarial%20inverse%20reinforcement%20learning%202018"
        },
        {
            "id": "Liu_2016_a",
            "entry": "Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In Advances in Neural Information Processing Systems (NIPS) 29, pages 2378\u20132386, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20Bayesian%20inference%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Wang%2C%20Dilin%20Stein%20variational%20gradient%20descent%3A%20A%20general%20purpose%20Bayesian%20inference%20algorithm%202016"
        },
        {
            "id": "Liu_et+al_2016_b",
            "entry": "Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-fit tests. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 276\u2013284, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Lee%2C%20Jason%20Jordan%2C%20Michael%20A%20kernelized%20Stein%20discrepancy%20for%20goodness-of-fit%20tests%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Lee%2C%20Jason%20Jordan%2C%20Michael%20A%20kernelized%20Stein%20discrepancy%20for%20goodness-of-fit%20tests%202016"
        },
        {
            "id": "Todorov_et+al_2012_a",
            "entry": "Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In Proceedings of the 25th IEEE/RSH International Conference on Intelligent Robots and Systems (IROS), pages 5026\u20135033, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20MuJoCo%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20MuJoCo%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. OpenAI Baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. TensorFlow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Saatci_2017_a",
            "entry": "Yunus Saatci and Andrew G Wilson. Bayesian GAN. In Advances in Neural Information Processing Systems (NIPS) 30, pages 3622\u20133631, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yunus%20Saatci%20and%20Andrew%20G%20Wilson%20Bayesian%20GAN%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%2030%20pages%2036223631%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yunus%20Saatci%20and%20Andrew%20G%20Wilson%20Bayesian%20GAN%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%2030%20pages%2036223631%202017"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. arXiv preprint arXiv:1704.02399, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02399"
        }
    ]
}
