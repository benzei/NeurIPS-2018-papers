{
    "filename": "8167-towards-understanding-learning-representations-to-what-extent-do-different-neural-networks-learn-the-same-representation.pdf",
    "metadata": {
        "title": "Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation",
        "author": "Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, John Hopcroft",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8167-towards-understanding-learning-representations-to-what-extent-do-different-neural-networks-learn-the-same-representation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "convolutional layer",
            "url": "https://en.wikipedia.org/wiki/convolutional_layer"
        }
    ],
    "highlights": [
        "It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks [<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>, <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>]",
        "We show the unique existence of the maximum match, and we prove the Decomposition Theorem: every match can be decomposed as the union of a set of simple matches, where simple matches are those which cannot be decomposed any more",
        "We develop a theory based on the neuron activation subspace match model to study the similarity between representations learned by two networks with identical architecture but trained from different initializations",
        "We investigate the similarity between representations learned by two networks with identical architecture but trained from different initializations",
        "We develop a rigorous theory and propose efficient algorithms",
        "We apply the algorithms in experiments and find that representations learned by convolutional layers are not as similar as prevalently expected"
    ],
    "key_statements": [
        "It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks [<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>, <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>]",
        "We move a tiny step towards a theory and a systematic approach that characterize the representations learned by deep nets",
        "We consider a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations",
        "We show the unique existence of the maximum match, and we prove the Decomposition Theorem: every match can be decomposed as the union of a set of simple matches, where simple matches are those which cannot be decomposed any more",
        "We investigate how to characterize these simple matches so that we can develop efficient algorithms for finding them",
        "We develop a theory based on the neuron activation subspace match model to study the similarity between representations learned by two networks with identical architecture but trained from different initializations",
        "We propose efficient algorithms for finding the maximum match and the simple matches, which are the central concepts in our theory.\n3",
        "In Section 2 we formally describe the neuron activation subspace match model",
        "Different from the setting of exact match where v-minimum match is unique for a neuron v, there may be multiple v-minimal matches for v in the setting of approximate match, and in this setting simple matches can be characterized by v-minimal matches instead",
        "We introduce maximum matching similarity to measure the overall similarity between sets of neurons",
        "We investigate the similarity between representations learned by two networks with identical architecture but trained from different initializations",
        "We develop a rigorous theory and propose efficient algorithms",
        "We apply the algorithms in experiments and find that representations learned by convolutional layers are not as similar as prevalently expected",
        "If from each initialization one learns a different representation, how can we interpret the network? If, on the other hand, subspace match is not a good metric, what is the right metric for similarity of representations? We believe this is a fundamental problem for deep learning and worth systematic and in depth studying"
    ],
    "summary": [
        "It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks [<a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\"><a class=\"ref-link\" id=\"cKrizhevsky_et+al_2012_a\" href=\"#rKrizhevsky_et+al_2012_a\">Krizhevsky et al, 2012</a></a>, <a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016</a></a>].",
        "1. We develop a theory based on the neuron activation subspace match model to study the similarity between representations learned by two networks with identical architecture but trained from different initializations.",
        "Given a neuron v \u2208 X \u222a Y, we define the v-minimum match to be the exact match (Xv, Yv) in (X , Y) satisfying the following properties: 1.",
        "Theorem 8 implies that the number of simple exact matches is at most linear with respect to the number of neurons given the activation vectors being linearly independent, because the v-minimum match for each neuron v is unique.",
        "Different from the setting of exact match where v-minimum match is unique for a neuron v, there may be multiple v-minimal matches for v in the setting of approximate match, and in this setting simple matches can be characterized by v-minimal matches instead.",
        "Algorithm 2 outputs one v-minimal match for the given neuron v.",
        "Given two sets of neurons X , Y and , algorithm 1 outputs the maximum match X\u2217, Y \u2217.",
        "We randomly sample d from h \u00d7 w \u00d7 d outputs to form an activation vector for several times, and average the maximal matching similarity.",
        "2. layers close to the output sometimes exhibit high similarity, it is a simple consequence of their alignment to the output: First, the output vector of two networks must be well aligned because they both achieve high accuracy.",
        "(See in Supplementary materials that, for a trained and an untrained networks which have very different accuracies and layers close to output do not have much similarity.)",
        "Given and two sets of neurons X and Y, algorithm 3 will output all the simple matches.",
        "Figure 2 shows the distribution of the size of simple matches on layers close to input or output respectively.",
        "Almost no single neuron learn similar representations, even in layers close to input or output.",
        "We investigate the similarity between representations learned by two networks with identical architecture but trained from different initializations.",
        "We apply the algorithms in experiments and find that representations learned by convolutional layers are not as similar as prevalently expected.",
        "This raises important questions: Does our result imply two networks learn completely different representations, or subspace match is not a good metric for measuring the similarity of representations?",
        "If from each initialization one learns a different representation, how can we interpret the network? If, on the other hand, subspace match is not a good metric, what is the right metric for similarity of representations? We believe this is a fundamental problem for deep learning and worth systematic and in depth studying"
    ],
    "headline": "We study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations",
    "reference_links": [
        {
            "id": "Dauphin_et+al_2014_a",
            "entry": "Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pages 2933\u20132941, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "Deng_et+al_2009_a",
            "entry": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Krizhevsky_et+al_0000_a",
            "entry": "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html.",
            "url": "http://www.cs.toronto.edu/~kriz/cifar.html"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do different neural networks learn the same representations? In International Conference on Learning Representation (ICLR \u201916), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yixuan%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Lipson%2C%20Hod%20Convergent%20learning%3A%20Do%20different%20neural%20networks%20learn%20the%20same%20representations%3F%202016"
        },
        {
            "id": "Raghu_et+al_2017_a",
            "entry": "Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, pages 6078\u20136087, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20Maithra%20Gilmer%2C%20Justin%20Yosinski%2C%20Jason%20Sohl-Dickstein%2C%20Jascha%20Svcca%3A%20Singular%20vector%20canonical%20correlation%20analysis%20for%20deep%20learning%20dynamics%20and%20interpretability%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Gilmer%2C%20Justin%20Yosinski%2C%20Jason%20Sohl-Dickstein%2C%20Jascha%20Svcca%3A%20Singular%20vector%20canonical%20correlation%20analysis%20for%20deep%20learning%20dynamics%20and%20interpretability%202017"
        },
        {
            "id": "Simonyan_2014_a",
            "entry": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        }
    ]
}
