{
    "filename": "8213-low-rank-tucker-decomposition-of-large-tensors-using-tensorsketch.pdf",
    "metadata": {
        "title": "Low-Rank Tucker Decomposition of Large Tensors Using TensorSketch",
        "author": "Osman Asif Malik, Stephen Becker",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8213-low-rank-tucker-decomposition-of-large-tensors-using-tensorsketch.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose two randomized algorithms for low-rank Tucker decomposition of tensors. The algorithms, which incorporate sketching, only require a single pass of the input tensor and can handle tensors whose elements are streamed in any order. To the best of our knowledge, ours are the only algorithms which can do this. We test our algorithms on sparse synthetic data and compare them to multiple other methods. We also apply one of our algorithms to a real dense 38 GB tensor representing a video and use the resulting decomposition to correctly classify frames containing disturbances."
    },
    "keywords": [
        {
            "term": "fast Fourier transform",
            "url": "https://en.wikipedia.org/wiki/fast_Fourier_transform"
        },
        {
            "term": "tucker decomposition",
            "url": "https://en.wikipedia.org/wiki/tucker_decomposition"
        },
        {
            "term": "randomized algorithm",
            "url": "https://en.wikipedia.org/wiki/randomized_algorithm"
        },
        {
            "term": "low rank approximation",
            "url": "https://en.wikipedia.org/wiki/low_rank_approximation"
        },
        {
            "term": "TUCKER",
            "url": "https://en.wikipedia.org/wiki/Tucker"
        },
        {
            "term": "kronecker product",
            "url": "https://en.wikipedia.org/wiki/kronecker_product"
        },
        {
            "term": "conjugate gradient",
            "url": "https://en.wikipedia.org/wiki/conjugate_gradient"
        },
        {
            "term": "tensor decomposition",
            "url": "https://en.wikipedia.org/wiki/tensor_decomposition"
        },
        {
            "term": "singular value decomposition",
            "url": "https://en.wikipedia.org/wiki/singular_value_decomposition"
        }
    ],
    "highlights": [
        "Many real datasets have more than two dimensions and are better represented using tensors, or multi-way arrays, rather than matrices",
        "In the same way that methods such as the singular value decomposition (SVD) can help in the analysis of data in matrix form, tensor decompositions are important tools when working with tensor data",
        "A key challenge to incorporating sketching in the Tucker decomposition is that the relevant design matrices are Kronecker products of the factor matrices",
        "Kolda and Sun [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] introduce the Memory Efficient Tucker (MET) decomposition for sparse tensors as a solution to the so called intermediate blow-up problem which occurs when computing the chain of TTM products in higher-order orthogonal iteration",
        "Neither of these methods correspond to Tucker decomposition of a tensor whose elements are streamed",
        "We have proposed two algorithms for low-rank Tucker decomposition which incorporate TENSORSKETCH and can handle streamed data"
    ],
    "key_statements": [
        "Many real datasets have more than two dimensions and are better represented using tensors, or multi-way arrays, rather than matrices",
        "In the same way that methods such as the singular value decomposition (SVD) can help in the analysis of data in matrix form, tensor decompositions are important tools when working with tensor data",
        "We present two algorithms for computing the Tucker decomposition of a tensor which incorporate random sketching",
        "A key challenge to incorporating sketching in the Tucker decomposition is that the relevant design matrices are Kronecker products of the factor matrices",
        "Recent work [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] has led to a new technique called TENSORSKETCH which is ideally suited for sketching Kronecker products",
        "We propose two algorithms for Tucker decomposition which incorporate TENSORSKETCH",
        "The Tucker decomposition problem of decomposing a data tensor Y \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN can be formulated as arg min",
        "Kolda and Sun [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] introduce the Memory Efficient Tucker (MET) decomposition for sparse tensors as a solution to the so called intermediate blow-up problem which occurs when computing the chain of TTM products in higher-order orthogonal iteration",
        "Neither of these methods correspond to Tucker decomposition of a tensor whose elements are streamed",
        "Defining the sketching operators upfront allows us to make the following improvements: (a) Since Y remains unchanged throughout the algorithm, the N + 1 sketches of Y only need to be computed once, which we do upfront in a single pass over the data",
        "The following informal proposition shows that the error for each sketched computation in TUCKERTTMTS is additive rather than multiplicative as for TUCKER-TS",
        "We have proposed two algorithms for low-rank Tucker decomposition which incorporate TENSORSKETCH and can handle streamed data",
        "Experiments corroborate our complexity analysis which shows that the algorithms scale well both with dimension size and density"
    ],
    "summary": [
        "Many real datasets have more than two dimensions and are better represented using tensors, or multi-way arrays, rather than matrices.",
        "We present two algorithms for computing the Tucker decomposition of a tensor which incorporate random sketching.",
        "We propose two algorithms for Tucker decomposition which incorporate TENSORSKETCH.",
        "The Tucker decomposition problem of decomposing a data tensor Y \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN can be formulated as arg min",
        "Kolda and Sun [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] introduce the Memory Efficient Tucker (MET) decomposition for sparse tensors as a solution to the so called intermediate blow-up problem which occurs when computing the chain of TTM products in HOOI.",
        "Neither of these methods correspond to Tucker decomposition of a tensor whose elements are streamed.",
        "We use the formula in (5) when applying one of the TENSORSKETCH operators to a Kronecker product matrix.",
        "MACH requires an algorithm for computing the HOOI decomposition of the sparsified tensor.",
        "For these guarantees to hold, we would need to define a new TENSORSKETCH operator each time a least-squares problem is solved in Algorithm 2.",
        "Defining the sketching operators upfront allows us to make the following improvements: (a) Since Y remains unchanged throughout the algorithm, the N + 1 sketches of Y only need to be computed once, which we do upfront in a single pass over the data.",
        "We can sketch the computation on line 8 in Algorithm 1 using a TENSORSKETCH operator",
        "The initialization of the factor matrices on line 1a and the definition of the sketching operators on line 1b are done in the same way as in Algorithm 2.",
        "The following informal proposition shows that the error for each sketched computation in TUCKERTTMTS is additive rather than multiplicative as for TUCKER-TS.",
        "In Algorithm 3, we compute an estimate of G using the same formula as in line 8, but using the smaller sketch dimension J1 instead.",
        "Defining TENSORSKETCH operators upfront leads to higher accuracy than redefining them before each application.",
        "We apply TUCKER-TTMTS to a real dense tensor representing a grayscale video.",
        "We compute a rank (10, 10, 10) Tucker decomposition of the tensor using TUCKER-TTMTS with the sketch dimension parameter set to K = 100 and a maximum of 30 iterations.",
        "We have proposed two algorithms for low-rank Tucker decomposition which incorporate TENSORSKETCH and can handle streamed data.",
        "Experiments corroborate our complexity analysis which shows that the algorithms scale well both with dimension size and density.",
        "TUCKER-TS, and to a lesser extent TUCKER-TTMTS, scale poorly with target rank, so they are most useful when R I"
    ],
    "headline": "We propose two randomized algorithms for low-rank Tucker decomposition of tensors",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Woody Austin, Grey Ballard, and Tamara G. Kolda. Parallel Tensor Compression for LargeScale Scientific Data. Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016, pages 912\u2013922, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woody%20Austin%20Grey%20Ballard%20and%20Tamara%20G%20Kolda%20Parallel%20Tensor%20Compression%20for%20LargeScale%20Scientific%20Data%20Proceedings%20%202016%20IEEE%2030th%20International%20Parallel%20and%20Distributed%20Processing%20Symposium%20IPDPS%202016%20pages%20912922%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woody%20Austin%20Grey%20Ballard%20and%20Tamara%20G%20Kolda%20Parallel%20Tensor%20Compression%20for%20LargeScale%20Scientific%20Data%20Proceedings%20%202016%20IEEE%2030th%20International%20Parallel%20and%20Distributed%20Processing%20Symposium%20IPDPS%202016%20pages%20912922%202016"
        },
        {
            "id": "2",
            "entry": "[2] Haim Avron, Huy Nguyen, and David Woodruff. Subspace embeddings for the polynomial kernel. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2258\u20132266. Curran Associates, Inc., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avron%2C%20Haim%20Nguyen%2C%20Huy%20Woodruff%2C%20David%20Subspace%20embeddings%20for%20the%20polynomial%20kernel%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avron%2C%20Haim%20Nguyen%2C%20Huy%20Woodruff%2C%20David%20Subspace%20embeddings%20for%20the%20polynomial%20kernel%202014"
        },
        {
            "id": "3",
            "entry": "[3] Brett W. Bader, Tamara G. Kolda, et al. Matlab tensor toolbox version 2.6. Available online, February 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bader%2C%20Brett%20W.%20Kolda%2C%20Tamara%20G.%20Matlab%20tensor%20toolbox%20version%202.6"
        },
        {
            "id": "4",
            "entry": "[4] Muthu Baskaran, Beno\u00eet Meister, Nicolas Vasilache, and Richard Lethin. Efficient and scalable computations with sparse tensors. 2012 IEEE Conference on High Performance Extreme Computing, HPEC 2012, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baskaran%2C%20Muthu%20Meister%2C%20Beno%C3%AEt%20Vasilache%2C%20Nicolas%20Lethin%2C%20Richard%20Efficient%20and%20scalable%20computations%20with%20sparse%20tensors%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baskaran%2C%20Muthu%20Meister%2C%20Beno%C3%AEt%20Vasilache%2C%20Nicolas%20Lethin%2C%20Richard%20Efficient%20and%20scalable%20computations%20with%20sparse%20tensors%202012"
        },
        {
            "id": "5",
            "entry": "[5] C. Battaglino, G. Ballard, and T. Kolda. A practical randomized CP tensor decomposition. SIAM Journal on Matrix Analysis and Applications, 39(2):876\u2013901, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battaglino%2C%20C.%20Ballard%2C%20G.%20Kolda%2C%20T.%20A%20practical%20randomized%20CP%20tensor%20decomposition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battaglino%2C%20C.%20Ballard%2C%20G.%20Kolda%2C%20T.%20A%20practical%20randomized%20CP%20tensor%20decomposition%202018"
        },
        {
            "id": "6",
            "entry": "[6] Cesar F. Caiafa and Andrzej Cichocki. Generalizing the column-row matrix decomposition to multi-way arrays. Linear Algebra and Its Applications, 433(3):557\u2013573, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caiafa%2C%20Cesar%20F.%20Cichocki%2C%20Andrzej%20Generalizing%20the%20column-row%20matrix%20decomposition%20to%20multi-way%20arrays%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caiafa%2C%20Cesar%20F.%20Cichocki%2C%20Andrzej%20Generalizing%20the%20column-row%20matrix%20decomposition%20to%20multi-way%20arrays%202010"
        },
        {
            "id": "7",
            "entry": "[7] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. Theoretical Computer Science, 312(1):3\u201315, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Charikar%2C%20Moses%20Chen%2C%20Kevin%20Farach-Colton%2C%20Martin%20Finding%20frequent%20items%20in%20data%20streams%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Charikar%2C%20Moses%20Chen%2C%20Kevin%20Farach-Colton%2C%20Martin%20Finding%20frequent%20items%20in%20data%20streams%202004"
        },
        {
            "id": "8",
            "entry": "[8] Kenneth L. Clarkson and David P. Woodruff. Low-Rank Approximation and Regression in Input Sparsity Time. Journal of the ACM, 63(6):1\u201345, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20Kenneth%20L.%20Woodruff%2C%20David%20P.%20Low-Rank%20Approximation%20and%20Regression%20in%20Input%20Sparsity%20Time%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20Kenneth%20L.%20Woodruff%2C%20David%20P.%20Low-Rank%20Approximation%20and%20Regression%20in%20Input%20Sparsity%20Time%202017"
        },
        {
            "id": "9",
            "entry": "[9] M. N. da Costa, R. R. Lopes, and J. M. T. Romano. Randomized methods for higher-order subspace separation. In 2016 24th European Signal Processing Conference (EUSIPCO), pages 215\u2013219, Aug 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=da%20Costa%2C%20M.N.%20Lopes%2C%20R.R.%20Romano%2C%20J.M.T.%20Randomized%20methods%20for%20higher-order%20subspace%20separation%202016-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=da%20Costa%2C%20M.N.%20Lopes%2C%20R.R.%20Romano%2C%20J.M.T.%20Randomized%20methods%20for%20higher-order%20subspace%20separation%202016-08"
        },
        {
            "id": "10",
            "entry": "[10] Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for kronecker product regression and p-splines. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1299\u20131308, Playa Blanca, Lanzarote, Canary Islands, 09\u201311 Apr 2018. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diao%2C%20Huaian%20Song%2C%20Zhao%20Sun%2C%20Wen%20Woodruff%2C%20David%20Sketching%20for%20kronecker%20product%20regression%20and%20p-splines%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diao%2C%20Huaian%20Song%2C%20Zhao%20Sun%2C%20Wen%20Woodruff%2C%20David%20Sketching%20for%20kronecker%20product%20regression%20and%20p-splines%202018-04"
        },
        {
            "id": "11",
            "entry": "[11] Petros Drineas and Michael W. Mahoney. A randomized algorithm for a tensor-based generalization of the singular value decomposition. Linear Algebra and Its Applications, 420(2-3):553\u2013 571, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20A%20randomized%20algorithm%20for%20a%20tensor-based%20generalization%20of%20the%20singular%20value%20decomposition%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20A%20randomized%20algorithm%20for%20a%20tensor-based%20generalization%20of%20the%20singular%20value%20decomposition%202007"
        },
        {
            "id": "12",
            "entry": "[12] Hadi Fanaee-T and Jo\u00e3o Gama. Multi-aspect-streaming tensor analysis. Knowledge-Based Systems, 89(C):332\u2013345, November 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fanaee-T%2C%20Hadi%20Gama%2C%20Jo%C3%A3o%20Multi-aspect-streaming%20tensor%20analysis%202015-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fanaee-T%2C%20Hadi%20Gama%2C%20Jo%C3%A3o%20Multi-aspect-streaming%20tensor%20analysis%202015-11"
        },
        {
            "id": "13",
            "entry": "[13] S. Friedland, V. Mehrmann, A. Miedlar, and M. Nkengla. Fast low rank approximations of matrices and tensors. Electronic Journal of Linear Algebra, 22, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedland%2C%20S.%20Mehrmann%2C%20V.%20Miedlar%2C%20A.%20Nkengla%2C%20M.%20Fast%20low%20rank%20approximations%20of%20matrices%20and%20tensors%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedland%2C%20S.%20Mehrmann%2C%20V.%20Miedlar%2C%20A.%20Nkengla%2C%20M.%20Fast%20low%20rank%20approximations%20of%20matrices%20and%20tensors%202011"
        },
        {
            "id": "14",
            "entry": "[14] Ekta Gujral, Ravdeep Pasricha, and Evangelos E. Papalexakis. Sambaten: Sampling-based batch incremental tensor decomposition. In Proceedings of the 2018 SIAM International Conference on Data Mining, pages 387\u2013395.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gujral%2C%20Ekta%20Pasricha%2C%20Ravdeep%20Papalexakis%2C%20Evangelos%20E.%20Sambaten%3A%20Sampling-based%20batch%20incremental%20tensor%20decomposition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gujral%2C%20Ekta%20Pasricha%2C%20Ravdeep%20Papalexakis%2C%20Evangelos%20E.%20Sambaten%3A%20Sampling-based%20batch%20incremental%20tensor%20decomposition%202018"
        },
        {
            "id": "15",
            "entry": "[15] Inah Jeon, Evangelos E. Papalexakis, U Kang, and Christos Faloutsos. Haten2: Billion-scale tensor decompositions. In IEEE International Conference on Data Engineering (ICDE), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jeon%2C%20Inah%20Papalexakis%2C%20Evangelos%20E.%20Kang%2C%20U.%20Faloutsos%2C%20Christos%20Haten2%3A%20Billion-scale%20tensor%20decompositions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jeon%2C%20Inah%20Papalexakis%2C%20Evangelos%20E.%20Kang%2C%20U.%20Faloutsos%2C%20Christos%20Haten2%3A%20Billion-scale%20tensor%20decompositions%202015"
        },
        {
            "id": "16",
            "entry": "[16] O. Kaya and B. U\u00e7ar. High performance parallel algorithms for the tucker decomposition of sparse tensors. In 2016 45th International Conference on Parallel Processing (ICPP), pages 103\u2013112, Aug 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaya%2C%20O.%20U%C3%A7ar%2C%20B.%20High%20performance%20parallel%20algorithms%20for%20the%20tucker%20decomposition%20of%20sparse%20tensors%202016-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaya%2C%20O.%20U%C3%A7ar%2C%20B.%20High%20performance%20parallel%20algorithms%20for%20the%20tucker%20decomposition%20of%20sparse%20tensors%202016-08"
        },
        {
            "id": "17",
            "entry": "[17] Tamara G. Kolda. Multilinear operators for higher-order decompositions. Technical Report SAND2006-2081, Sandia National Laboratories, April 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Multilinear%20operators%20for%20higher-order%20decompositions.%20Technical%20Report%20SAND2006-2081%2C%20Sandia%20National%20Laboratories%202006-04"
        },
        {
            "id": "18",
            "entry": "[18] Tamara G. Kolda and Brett W. Bader. Tensor Decompositions and Applications. SIAM Review, 51(3):455\u2013500, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20Decompositions%20and%20Applications%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20Decompositions%20and%20Applications%202009"
        },
        {
            "id": "19",
            "entry": "[19] Tamara G. Kolda and Jimeng Sun. Scalable tensor decompositions for multi-aspect data mining. Proceedings - IEEE International Conference on Data Mining, ICDM, pages 363\u2013372, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Sun%2C%20Jimeng%20Scalable%20tensor%20decompositions%20for%20multi-aspect%20data%20mining%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20Tamara%20G.%20Sun%2C%20Jimeng%20Scalable%20tensor%20decompositions%20for%20multi-aspect%20data%20mining%202008"
        },
        {
            "id": "20",
            "entry": "[20] Jiajia Li, Casey Battaglino, Ioakeim Perros, Jimeng Sun, and Richard Vuduc. An input-adaptive and in-place approach to dense tensor-times-matrix multiply. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC \u201915, pages 76:1\u201376:12, New York, NY, USA, 2015. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Jiajia%20Battaglino%2C%20Casey%20Perros%2C%20Ioakeim%20Sun%2C%20Jimeng%20An%20input-adaptive%20and%20in-place%20approach%20to%20dense%20tensor-times-matrix%20multiply%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Jiajia%20Battaglino%2C%20Casey%20Perros%2C%20Ioakeim%20Sun%2C%20Jimeng%20An%20input-adaptive%20and%20in-place%20approach%20to%20dense%20tensor-times-matrix%20multiply%202015"
        },
        {
            "id": "21",
            "entry": "[21] Jiajia Li, Yuchen Ma, Chenggang Yan, and Richard Vuduc. Optimizing sparse tensor times matrix on multi-core and many-core architectures. In Proceedings of the Sixth Workshop on Irregular Applications: Architectures and Algorithms, IA3 \u201916, pages 26\u201333, Piscataway, NJ, USA, 2016. IEEE Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Jiajia%20Ma%2C%20Yuchen%20Yan%2C%20Chenggang%20Vuduc%2C%20Richard%20Optimizing%20sparse%20tensor%20times%20matrix%20on%20multi-core%20and%20many-core%20architectures%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Jiajia%20Ma%2C%20Yuchen%20Yan%2C%20Chenggang%20Vuduc%2C%20Richard%20Optimizing%20sparse%20tensor%20times%20matrix%20on%20multi-core%20and%20many-core%20architectures%202016"
        },
        {
            "id": "22",
            "entry": "[22] Bangtian Liu, Chengyao Wen, Anand D. Sarwate, and Maryam Mehri Dehnavi. A Unified Optimization Approach for Sparse Tensor Operations on GPUs. Proceedings - IEEE International Conference on Cluster Computing, ICCC, pages 47\u201357, Sept 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Bangtian%20Wen%2C%20Chengyao%20Sarwate%2C%20Anand%20D.%20Dehnavi%2C%20Maryam%20Mehri%20A%20Unified%20Optimization%20Approach%20for%20Sparse%20Tensor%20Operations%20on%20GPUs%202017-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Bangtian%20Wen%2C%20Chengyao%20Sarwate%2C%20Anand%20D.%20Dehnavi%2C%20Maryam%20Mehri%20A%20Unified%20Optimization%20Approach%20for%20Sparse%20Tensor%20Operations%20on%20GPUs%202017-09"
        },
        {
            "id": "23",
            "entry": "[23] Michael W. Mahoney, Mauro Maggioni, and Petros Drineas. Tensor-CUR decompositions for tensor-based data. SIAM Journal on Matrix Analysis and Applications, 30(3):957\u2013987, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahoney%2C%20Michael%20W.%20Maggioni%2C%20Mauro%20Drineas%2C%20Petros%20Tensor-CUR%20decompositions%20for%20tensor-based%20data%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahoney%2C%20Michael%20W.%20Maggioni%2C%20Mauro%20Drineas%2C%20Petros%20Tensor-CUR%20decompositions%20for%20tensor-based%20data%202008"
        },
        {
            "id": "24",
            "entry": "[24] Jinoh Oh, Kijung Shin, Evangelos E. Papalexakis, Christos Faloutsos, and Hwanjo Yu. S-HOT: Scalable High-Order Tucker Decomposition. Proceedings of the Tenth ACM International Conference on Web Search and Data Mining - WSDM \u201917, pages 761\u2013770, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oh%2C%20Jinoh%20Shin%2C%20Kijung%20Papalexakis%2C%20Evangelos%20E.%20Faloutsos%2C%20Christos%20S-HOT%3A%20Scalable%20High-Order%20Tucker%20Decomposition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oh%2C%20Jinoh%20Shin%2C%20Kijung%20Papalexakis%2C%20Evangelos%20E.%20Faloutsos%2C%20Christos%20S-HOT%3A%20Scalable%20High-Order%20Tucker%20Decomposition%202017"
        },
        {
            "id": "25",
            "entry": "[25] I. V. Oseledets, D. V. Savostianov, and E. E. Tyrtyshnikov. Tucker dimensionality reduction of three-dimensional arrays in linear time. SIAM Journal on Matrix Analysis and Applications, 30(3):939\u2013956, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oseledets%2C%20I.V.%20Savostianov%2C%20D.V.%20Tyrtyshnikov%2C%20E.E.%20Tucker%20dimensionality%20reduction%20of%20three-dimensional%20arrays%20in%20linear%20time%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oseledets%2C%20I.V.%20Savostianov%2C%20D.V.%20Tyrtyshnikov%2C%20E.E.%20Tucker%20dimensionality%20reduction%20of%20three-dimensional%20arrays%20in%20linear%20time%202008"
        },
        {
            "id": "26",
            "entry": "[26] Rasmus Pagh. Compressed matrix multiplication. ACM Transactions on Computation Theory, 5(3):1\u201317, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pagh%2C%20Rasmus%20Compressed%20matrix%20multiplication%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pagh%2C%20Rasmus%20Compressed%20matrix%20multiplication%202013"
        },
        {
            "id": "27",
            "entry": "[27] Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201913, pages 239\u2013247, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pham%2C%20Ninh%20Pagh%2C%20Rasmus%20Fast%20and%20scalable%20polynomial%20kernels%20via%20explicit%20feature%20maps%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pham%2C%20Ninh%20Pagh%2C%20Rasmus%20Fast%20and%20scalable%20polynomial%20kernels%20via%20explicit%20feature%20maps%202013"
        },
        {
            "id": "28",
            "entry": "[28] Yang Shi, U. N. Niranjan, Animashree Anandkumar, and Cris Cecka. Tensor Contractions with Extended BLAS Kernels on CPU and GPU. Proceedings - 23rd IEEE International Conference on High Performance Computing, HiPC 2016, pages 193\u2013202, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%20Shi%2C%20U.N.Niranjan%20Anandkumar%2C%20Animashree%20Cecka%2C%20Cris%20Tensor%20Contractions%20with%20Extended%20BLAS%20Kernels%20on%20CPU%20and%20GPU.%20Proceedings%20-%2023rd%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%20Shi%2C%20U.N.Niranjan%20Anandkumar%2C%20Animashree%20Cecka%2C%20Cris%20Tensor%20Contractions%20with%20Extended%20BLAS%20Kernels%20on%20CPU%20and%20GPU.%20Proceedings%20-%2023rd%202016"
        },
        {
            "id": "29",
            "entry": "[29] Jimeng Sun, Dacheng Tao, Spiros Papadimitriou, Philip S. Yu, and Christos Faloutsos. Incremental tensor analysis: Theory and applications. ACM Transactions on Knowledge Discovery from Data, 2(3):11:1\u201311:37, October 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Jimeng%20Tao%2C%20Dacheng%20Papadimitriou%2C%20Spiros%20Yu%2C%20Philip%20S.%20Incremental%20tensor%20analysis%3A%20Theory%20and%20applications%202008-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Jimeng%20Tao%2C%20Dacheng%20Papadimitriou%2C%20Spiros%20Yu%2C%20Philip%20S.%20Incremental%20tensor%20analysis%3A%20Theory%20and%20applications%202008-10"
        },
        {
            "id": "30",
            "entry": "[30] Charalampos E. Tsourakakis. MACH: fast randomized tensor decompositions. In Proceedings of the SIAM International Conference on Data Mining, SDM 2010, April 29 - May 1, 2010, Columbus, Ohio, USA, pages 689\u2013700, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsourakakis%2C%20Charalampos%20E.%20MACH%3A%20fast%20randomized%20tensor%20decompositions%202010-04-29",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsourakakis%2C%20Charalampos%20E.%20MACH%3A%20fast%20randomized%20tensor%20decompositions%202010-04-29"
        },
        {
            "id": "31",
            "entry": "[31] Yining Wang, Hsiao-Yu Tung, Alexander J Smola, and Anima Anandkumar. Fast and guaranteed tensor decomposition via sketching. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 991\u2013999. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yining%20Tung%2C%20Hsiao-Yu%20Smola%2C%20Alexander%20J.%20Anandkumar%2C%20Anima%20Fast%20and%20guaranteed%20tensor%20decomposition%20via%20sketching%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yining%20Tung%2C%20Hsiao-Yu%20Smola%2C%20Alexander%20J.%20Anandkumar%2C%20Anima%20Fast%20and%20guaranteed%20tensor%20decomposition%20via%20sketching%202015"
        },
        {
            "id": "32",
            "entry": "[32] Guoxu Zhou, Andrzej Cichocki, and Shengli Xie. Decomposition of big tensors with low multilinear rank. CoRR, abs/1412.1885, 2014. ",
            "arxiv_url": "https://arxiv.org/pdf/1412.1885"
        }
    ]
}
