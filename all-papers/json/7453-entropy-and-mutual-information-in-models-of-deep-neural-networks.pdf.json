{
    "filename": "7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.pdf",
    "metadata": {
        "title": "Entropy and mutual information in models of deep neural networks",
        "author": "Marylou Gabri\u00e9, Andre Manoel, Cl\u00e9ment Luneau, jean barbier, Nicolas Macris, Florent Krzakala, Lenka Zdeborov\u00e1",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "random matrix",
            "url": "https://en.wikipedia.org/wiki/random_matrix"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "mixture of Gaussians",
            "url": "https://en.wikipedia.org/wiki/mixture_of_Gaussians"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "statistical physics",
            "url": "https://en.wikipedia.org/wiki/statistical_physics"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Replica formula\u2014 We shall work in the asymptotic high-dimensional statistics regime where all \u03b1 \u2261 n /n0 are of order one while n0 \u2192 \u221e, and make the important assumption that all matrices",
        "We have presented a class of deep learning models together with a tractable method to compute entropy and mutual information between layers",
        "This, we believe, offers a promising framework for further investigations, and to this aim we provide Python packages that facilitate both the computation of mutual informations and the training, for an arbitrary implementation of the model",
        "Allowing for biases by extending the proposed formula would improve the fitting power of the considered neural network models",
        "While our experiments focused on the supervised learning, the replica formula derived for multi-layer models is general and can be applied in unsupervised contexts, for instance in the theory of VAEs"
    ],
    "key_statements": [
        "Replica formula\u2014 We shall work in the asymptotic high-dimensional statistics regime where all \u03b1 \u2261 n /n0 are of order one while n0 \u2192 \u221e, and make the important assumption that all matrices",
        "We have presented a class of deep learning models together with a tractable method to compute entropy and mutual information between layers",
        "This, we believe, offers a promising framework for further investigations, and to this aim we provide Python packages that facilitate both the computation of mutual informations and the training, for an arbitrary implementation of the model",
        "Allowing for biases by extending the proposed formula would improve the fitting power of the considered neural network models",
        "While our experiments focused on the supervised learning, the replica formula derived for multi-layer models is general and can be applied in unsupervised contexts, for instance in the theory of VAEs"
    ],
    "summary": [
        "Replica formula\u2014 We shall work in the asymptotic high-dimensional statistics regime where all \u03b1 \u2261 n /n0 are of order one while n0 \u2192 \u221e, and make the important assumption that all matrices.",
        "The multi-layer model presented above can be leveraged to simulate two prototypical settings of deep supervised learning on synthetic datasets amenable to the replica tractable computation of entropies and mutual informations.",
        "In Section 3, we consider the training of linear networks for which information-theoretic quantities can be computed analytically, and confirm numerically that with USV-layers the replica predicted entropy is correct at all times.",
        "They investigate the training of larger linear networks on i.i.d. normally distributed inputs where entropies at each hidden layer can be computed analytically for an additive Gaussian noise.",
        "In the following paragraph 3.2, we validate the accuracy of the replica formula in a learning experiment with USV-layers \u2014where it is not proven to be exact \u2014 by considering the case of linear networks for which information-theoretic quantities can be otherwise computed in closed-form.",
        "In the first numerical experiment we place ourselves in the setting of Theorem 1: a 2-layer network with i.i.d weight matrices, where the formula of Claim 1 is rigorously exact in the limit of large networks, and we compare the replica results with the non-parametric estimators of [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>] and [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>].",
        "We consider noisy versions of the latent variables where an additive white Gaussian noise of very small variance is added right before the activation function, T1 = f (W1X + 1) and T2 = f (W2f (W1X) + 2) with 1,2 \u223c N (0, \u03c3n2oiseIn), which is done in the remaining experiments to guarantee the mutual informations to remain finite.",
        "The values of mutual informations I(X; T ) are computed by considering noisy versions of the latent variables where an additive white Gaussian noise of very small variance is added right before the activation function, as in the previous experiment.",
        "We use this linear setting to demonstrate (i) that the replica formula remains correct throughout the learning of the USV-layers and that the replica method gets closer and closer to the exact result in the limit of large networks, as theoretically predicted (2).",
        "3.3 Learning experiments with deep non-linear networks\u2014 we apply the replica formula to estimate mutual informations during the training of non-linear networks on correlated input data.",
        "We have presented a class of deep learning models together with a tractable method to compute entropy and mutual information between layers.",
        "The greater perspective remains proving the replica formula in the general case of multi-layer models, and further"
    ],
    "headline": "We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] N. Tishby, F. C. Pereira, and W. Bialek. The Information Bottleneck Method. 37th Annual Allerton Conference on Communication, Control, and Computing, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20N.%20Pereira%2C%20F.C.%20Bialek%2C%20W.%20The%20Information%20Bottleneck%20Method%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20N.%20Pereira%2C%20F.C.%20Bialek%2C%20W.%20The%20Information%20Bottleneck%20Method%201999"
        },
        {
            "id": "2",
            "entry": "[2] N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In IEEE Information Theory Workshop (ITW), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20N.%20Zaslavsky%2C%20N.%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20N.%20Zaslavsky%2C%20N.%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%202015"
        },
        {
            "id": "3",
            "entry": "[3] R. Shwartz-Ziv and N. Tishby. Opening the Black Box of Deep Neural Networks via Information. arXiv:1703.00810, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00810"
        },
        {
            "id": "4",
            "entry": "[4] G. Chechik, A. Globerson, N. Tishby, and Y. Weiss. Information bottleneck for Gaussian variables. Journal of Machine Learning Research, 6(Jan):165\u2013188, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chechik%2C%20G.%20Globerson%2C%20A.%20Tishby%2C%20N.%20Weiss%2C%20Y.%20Information%20bottleneck%20for%20Gaussian%20variables%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chechik%2C%20G.%20Globerson%2C%20A.%20Tishby%2C%20N.%20Weiss%2C%20Y.%20Information%20bottleneck%20for%20Gaussian%20variables%202005"
        },
        {
            "id": "5",
            "entry": "[5] A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. D. Tracey, and D. D. Cox. On the Information Bottleneck Theory of Deep Learning. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20A.M.%20Bansal%2C%20Y.%20Dapello%2C%20J.%20Advani%2C%20M.%20On%20the%20Information%20Bottleneck%20Theory%20of%20Deep%20Learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20A.M.%20Bansal%2C%20Y.%20Dapello%2C%20J.%20Advani%2C%20M.%20On%20the%20Information%20Bottleneck%20Theory%20of%20Deep%20Learning%202018"
        },
        {
            "id": "6",
            "entry": "[6] Y. Kabashima. Inference from correlated patterns: a unified theory for perceptron learning and linear vector channels. Journal of Physics: Conference Series, 95(1):012001, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kabashima%2C%20Y.%20Inference%20from%20correlated%20patterns%3A%20a%20unified%20theory%20for%20perceptron%20learning%20and%20linear%20vector%20channels%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kabashima%2C%20Y.%20Inference%20from%20correlated%20patterns%3A%20a%20unified%20theory%20for%20perceptron%20learning%20and%20linear%20vector%20channels%202008"
        },
        {
            "id": "7",
            "entry": "[7] A. Manoel, F. Krzakala, M. M\u00e9zard, and L. Zdeborov\u00e1. Multi-layer generalized linear estimation. In IEEE International Symposium on Information Theory (ISIT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Manoel%2C%20A.%20Krzakala%2C%20F.%20M%C3%A9zard%2C%20M.%20Zdeborov%C3%A1%2C%20L.%20Multi-layer%20generalized%20linear%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Manoel%2C%20A.%20Krzakala%2C%20F.%20M%C3%A9zard%2C%20M.%20Zdeborov%C3%A1%2C%20L.%20Multi-layer%20generalized%20linear%20estimation%202017"
        },
        {
            "id": "8",
            "entry": "[8] A. K. Fletcher and S. Rangan. Inference in Deep Networks in High Dimensions. arXiv:1706.06549, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06549"
        },
        {
            "id": "9",
            "entry": "[9] G. Reeves. Additivity of Information in Multilayer Networks via Additive Gaussian Noise Transforms. In 55th Annual Allerton Conference on Communication, Control, and Computing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reeves%2C%20G.%20Additivity%20of%20Information%20in%20Multilayer%20Networks%20via%20Additive%20Gaussian%20Noise%20Transforms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reeves%2C%20G.%20Additivity%20of%20Information%20in%20Multilayer%20Networks%20via%20Additive%20Gaussian%20Noise%20Transforms%202017"
        },
        {
            "id": "10",
            "entry": "[10] M. M\u00e9zard, G. Parisi, and M. Virasoro. Spin Glass Theory and Beyond. World Scientific Publishing Company, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A9zard%2C%20M.%20Parisi%2C%20G.%20Virasoro%2C%20M.%20Spin%20Glass%20Theory%20and%20Beyond%201987"
        },
        {
            "id": "11",
            "entry": "[11] M. M\u00e9zard and A. Montanari. Information, Physics, and Computation. Oxford University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A9zard%2C%20M.%20Information%2C%20A.Montanari%20Physics%20and%20Computation%202009"
        },
        {
            "id": "12",
            "entry": "[12] ArXiv version of this work. https://arxiv.org/abs/1805.09785.",
            "url": "https://arxiv.org/abs/1805.09785",
            "arxiv_url": "https://arxiv.org/pdf/1805.09785"
        },
        {
            "id": "13",
            "entry": "[13] dnner: Deep Neural Networks Entropy with Replicas, Python library. https://github.com/sphinxteam/dnner.",
            "url": "https://github.com/sphinxteam/dnner"
        },
        {
            "id": "14",
            "entry": "[14] A. M. Tulino, G. Caire, S. Verd\u00fa, and S. Shamai (Shitz). Support Recovery With Sparsely Sampled Free Random Matrices. IEEE Transactions on Information Theory, 59(7):4243\u20134271, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20M.%20Tulino%2C%20G.%20Caire%2C%20S.%20Verd%C3%BA%20S%20Shamai%20%28Shitz%29.%20Support%20Recovery%20With%20Sparsely%20Sampled%20Free%20Random%20Matrices%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20M.%20Tulino%2C%20G.%20Caire%2C%20S.%20Verd%C3%BA%20S%20Shamai%20%28Shitz%29.%20Support%20Recovery%20With%20Sparsely%20Sampled%20Free%20Random%20Matrices%202013"
        },
        {
            "id": "15",
            "entry": "[15] D. Donoho and A. Montanari. High dimensional robust M-estimation: asymptotic variance via approximate message passing. Probability Theory and Related Fields, 166(3-4):935\u2013969, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.%20Montanari%2C%20A.%20High%20dimensional%20robust%20M-estimation%3A%20asymptotic%20variance%20via%20approximate%20message%20passing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.%20Montanari%2C%20A.%20High%20dimensional%20robust%20M-estimation%3A%20asymptotic%20variance%20via%20approximate%20message%20passing%202016"
        },
        {
            "id": "16",
            "entry": "[16] H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical Review A, 45(8):6056, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seung%2C%20H.S.%20Sompolinsky%2C%20H.%20Tishby%2C%20N.%20Statistical%20mechanics%20of%20learning%20from%20examples%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seung%2C%20H.S.%20Sompolinsky%2C%20H.%20Tishby%2C%20N.%20Statistical%20mechanics%20of%20learning%20from%20examples%201992"
        },
        {
            "id": "17",
            "entry": "[17] A. Engel and C. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20A.%20den%20Broeck%2C%20C.Van%20Statistical%20Mechanics%20of%20Learning%202001"
        },
        {
            "id": "18",
            "entry": "[18] M. Opper and D. Saad. Advanced mean field methods: Theory and practice. MIT press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opper%2C%20M.%20Saad%2C%20D.%20Advanced%20mean%20field%20methods%3A%20Theory%20and%20practice%202001"
        },
        {
            "id": "19",
            "entry": "[19] J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborov\u00e1. Phase Transitions, Optimal Errors and Optimality of Message-Passing in Generalized Linear Models. arXiv:1708.03395, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03395"
        },
        {
            "id": "20",
            "entry": "[20] J. Barbier, N. Macris, A. Maillard, and F. Krzakala. The Mutual Information in Random Linear Estimation Beyond i.i.d. Matrices. In IEEE International Symposium on Information Theory (ISIT), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barbier%2C%20J.%20Macris%2C%20N.%20Maillard%2C%20A.%20Krzakala%2C%20F.%20The%20Mutual%20Information%20in%20Random%20Linear%20Estimation%20Beyond%20i.i.d.%20Matrices%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barbier%2C%20J.%20Macris%2C%20N.%20Maillard%2C%20A.%20Krzakala%2C%20F.%20The%20Mutual%20Information%20in%20Random%20Linear%20Estimation%20Beyond%20i.i.d.%20Matrices%202018"
        },
        {
            "id": "21",
            "entry": "[21] D. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914\u201318919, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20D.%20Maleki%2C%20A.%20Montanari%2C%20A.%20Message-passing%20algorithms%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20D.%20Maleki%2C%20A.%20Montanari%2C%20A.%20Message-passing%20algorithms%20for%20compressed%20sensing%202009"
        },
        {
            "id": "22",
            "entry": "[22] L. Zdeborov\u00e1 and F. Krzakala. Statistical physics of inference: thresholds and algorithms. Advances in Physics, 65(5):453\u2013552, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zdeborov%C3%A1%2C%20L.%20Krzakala%2C%20F.%20Statistical%20physics%20of%20inference%3A%20thresholds%20and%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zdeborov%C3%A1%2C%20L.%20Krzakala%2C%20F.%20Statistical%20physics%20of%20inference%3A%20thresholds%20and%20algorithms%202016"
        },
        {
            "id": "23",
            "entry": "[23] S. Rangan. Generalized approximate message passing for estimation with random linear mixing. In IEEE International Symposium on Information Theory (ISIT), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rangan%2C%20S.%20Generalized%20approximate%20message%20passing%20for%20estimation%20with%20random%20linear%20mixing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rangan%2C%20S.%20Generalized%20approximate%20message%20passing%20for%20estimation%20with%20random%20linear%20mixing%202011"
        },
        {
            "id": "24",
            "entry": "[24] S. Rangan, P. Schniter, and A. K. Fletcher. Vector approximate message passing. In IEEE International Symposium on Information Theory (ISIT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rangan%2C%20S.%20Schniter%2C%20P.%20Fletcher%2C%20A.K.%20Vector%20approximate%20message%20passing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rangan%2C%20S.%20Schniter%2C%20P.%20Fletcher%2C%20A.K.%20Vector%20approximate%20message%20passing%202017"
        },
        {
            "id": "25",
            "entry": "[25] J. Barbier and N. Macris. The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference. arXiv:1705.02780 to appear in Probability Theory and Related Fields, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02780"
        },
        {
            "id": "26",
            "entry": "[26] J. Barbier, N. Macris, and L. Miolane. The Layered Structure of Tensor Estimation and its Mutual Information. In 55th Annual Allerton Conference on Communication, Control, and Computing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barbier%2C%20J.%20Macris%2C%20N.%20Miolane%2C%20L.%20The%20Layered%20Structure%20of%20Tensor%20Estimation%20and%20its%20Mutual%20Information%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barbier%2C%20J.%20Macris%2C%20N.%20Miolane%2C%20L.%20The%20Layered%20Structure%20of%20Tensor%20Estimation%20and%20its%20Mutual%20Information%202017"
        },
        {
            "id": "27",
            "entry": "[27] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas. ACDC: A Structured Efficient Linear Layer. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moczulski%2C%20M.%20Denil%2C%20M.%20Appleyard%2C%20J.%20de%20Freitas%2C%20N.%20ACDC%3A%20A%20Structured%20Efficient%20Linear%20Layer%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moczulski%2C%20M.%20Denil%2C%20M.%20Appleyard%2C%20J.%20de%20Freitas%2C%20N.%20ACDC%3A%20A%20Structured%20Efficient%20Linear%20Layer%202016"
        },
        {
            "id": "28",
            "entry": "[28] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola, L. Song, and Z. Wang. Deep fried convnets. In IEEE International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Moczulski%2C%20M.%20Denil%2C%20M.%20de%20Freitas%2C%20N.%20Deep%20fried%20convnets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Moczulski%2C%20M.%20Denil%2C%20M.%20de%20Freitas%2C%20N.%20Deep%20fried%20convnets%202015"
        },
        {
            "id": "29",
            "entry": "[29] D. J. Amit, H. Gutfreund, and H. Sompolinsky. Storing infinite numbers of patterns in a spin-glass model of neural networks. Physical Review Letters, 55(14):1530, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amit%2C%20D.J.%20Gutfreund%2C%20H.%20Sompolinsky%2C%20H.%20Storing%20infinite%20numbers%20of%20patterns%20in%20a%20spin-glass%20model%20of%20neural%20networks%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amit%2C%20D.J.%20Gutfreund%2C%20H.%20Sompolinsky%2C%20H.%20Storing%20infinite%20numbers%20of%20patterns%20in%20a%20spin-glass%20model%20of%20neural%20networks%201985"
        },
        {
            "id": "30",
            "entry": "[30] E. Gardner and B. Derrida. Three unfinished works on the optimal storage capacity of networks. Journal of Physics A, 22(12):1983, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gardner%2C%20E.%20Derrida%2C%20B.%20Three%20unfinished%20works%20on%20the%20optimal%20storage%20capacity%20of%20networks%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gardner%2C%20E.%20Derrida%2C%20B.%20Three%20unfinished%20works%20on%20the%20optimal%20storage%20capacity%20of%20networks%201983"
        },
        {
            "id": "31",
            "entry": "[31] M. M\u00e9zard. The space of interactions in neural networks: Gardner\u2019s computation with the cavity method. Journal of Physics A, 22(12):2181, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A9zard%2C%20M.%20The%20space%20of%20interactions%20in%20neural%20networks%3A%20Gardner%E2%80%99s%20computation%20with%20the%20cavity%20method%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%A9zard%2C%20M.%20The%20space%20of%20interactions%20in%20neural%20networks%3A%20Gardner%E2%80%99s%20computation%20with%20the%20cavity%20method%201989"
        },
        {
            "id": "32",
            "entry": "[32] C. Louart and R. Couillet. Harnessing neural networks: A random matrix approach. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louart%2C%20C.%20Couillet%2C%20R.%20Harnessing%20neural%20networks%3A%20A%20random%20matrix%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louart%2C%20C.%20Couillet%2C%20R.%20Harnessing%20neural%20networks%3A%20A%20random%20matrix%20approach%202017"
        },
        {
            "id": "33",
            "entry": "[33] J. Pennington and P. Worah. Nonlinear random matrix theory for deep learning. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Worah%2C%20P.%20Nonlinear%20random%20matrix%20theory%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20J.%20Worah%2C%20P.%20Nonlinear%20random%20matrix%20theory%20for%20deep%20learning%202017"
        },
        {
            "id": "34",
            "entry": "[34] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein. On the Expressive Power of Deep Neural Networks. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20M.%20Poole%2C%20B.%20Kleinberg%2C%20J.%20Ganguli%2C%20S.%20On%20the%20Expressive%20Power%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20M.%20Poole%2C%20B.%20Kleinberg%2C%20J.%20Ganguli%2C%20S.%20On%20the%20Expressive%20Power%202017"
        },
        {
            "id": "35",
            "entry": "[35] A. Saxe, J. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20A.%20McClelland%2C%20J.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20A.%20McClelland%2C%20J.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014"
        },
        {
            "id": "36",
            "entry": "[36] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schoenholz%2C%20S.S.%20Gilmer%2C%20J.%20Ganguli%2C%20S.%20Sohl-Dickstein%2C%20J.%20Deep%20information%20propagation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schoenholz%2C%20S.S.%20Gilmer%2C%20J.%20Ganguli%2C%20S.%20Sohl-Dickstein%2C%20J.%20Deep%20information%20propagation%202017"
        },
        {
            "id": "37",
            "entry": "[37] M. Advani and A. Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv:1710.03667, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03667"
        },
        {
            "id": "38",
            "entry": "[38] C. Baldassi, A. Braunstein, N. Brunel, and R. Zecchina. Efficient supervised learning in networks with binary synapses. Proceedings of the National Academy of Sciences, 104:11079\u201311084, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldassi%2C%20C.%20Braunstein%2C%20A.%20Brunel%2C%20N.%20Zecchina%2C%20R.%20Efficient%20supervised%20learning%20in%20networks%20with%20binary%20synapses%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldassi%2C%20C.%20Braunstein%2C%20A.%20Brunel%2C%20N.%20Zecchina%2C%20R.%20Efficient%20supervised%20learning%20in%20networks%20with%20binary%20synapses%202007"
        },
        {
            "id": "39",
            "entry": "[39] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "40",
            "entry": "[40] R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neural networks with random Gaussian weights: a universal classification strategy? IEEE Transactions on Signal Processing, 64(13):3444\u20133457, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giryes%2C%20R.%20Sapiro%2C%20G.%20Bronstein%2C%20A.M.%20Deep%20neural%20networks%20with%20random%20Gaussian%20weights%3A%20a%20universal%20classification%20strategy%3F%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giryes%2C%20R.%20Sapiro%2C%20G.%20Bronstein%2C%20A.M.%20Deep%20neural%20networks%20with%20random%20Gaussian%20weights%3A%20a%20universal%20classification%20strategy%3F%202016"
        },
        {
            "id": "41",
            "entry": "[41] M. Chalk, O. Marre, and G. Tkacik. Relevant sparse codes with variational information bottleneck. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chalk%2C%20M.%20Marre%2C%20O.%20Tkacik%2C%20G.%20Relevant%20sparse%20codes%20with%20variational%20information%20bottleneck%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chalk%2C%20M.%20Marre%2C%20O.%20Tkacik%2C%20G.%20Relevant%20sparse%20codes%20with%20variational%20information%20bottleneck%202016"
        },
        {
            "id": "42",
            "entry": "[42] A. Achille and S. Soatto. Information Dropout: Learning Optimal Representations Through Noisy Computation. IEEE Transactions on Pattern Analysis and Machine Inteligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achille%2C%20A.%20Soatto%2C%20S.%20Information%20Dropout%3A%20Learning%20Optimal%20Representations%20Through%20Noisy%20Computation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achille%2C%20A.%20Soatto%2C%20S.%20Information%20Dropout%3A%20Learning%20Optimal%20Representations%20Through%20Noisy%20Computation%202018"
        },
        {
            "id": "43",
            "entry": "[43] A. Alemi, I. Fischer, J. Dillon, and K. Murphy. Deep variational information bottleneck. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alemi%2C%20A.%20Fischer%2C%20I.%20Dillon%2C%20J.%20Murphy%2C%20K.%20Deep%20variational%20information%20bottleneck%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alemi%2C%20A.%20Fischer%2C%20I.%20Dillon%2C%20J.%20Murphy%2C%20K.%20Deep%20variational%20information%20bottleneck%202017"
        },
        {
            "id": "44",
            "entry": "[44] A. Achille and S. Soatto. Emergence of Invariance and Disentangling in Deep Representations. In ICML 2017 Workshop on Principled Approaches to Deep Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Achille%20and%20S%20Soatto%20Emergence%20of%20Invariance%20and%20Disentangling%20in%20Deep%20Representations%20In%20ICML%202017%20Workshop%20on%20Principled%20Approaches%20to%20Deep%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Achille%20and%20S%20Soatto%20Emergence%20of%20Invariance%20and%20Disentangling%20in%20Deep%20Representations%20In%20ICML%202017%20Workshop%20on%20Principled%20Approaches%20to%20Deep%20Learning%202017"
        },
        {
            "id": "45",
            "entry": "[45] A. Kolchinsky, B. D. Tracey, and D. H. Wolpert. Nonlinear Information Bottleneck. arXiv:1705.02436, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02436"
        },
        {
            "id": "46",
            "entry": "[46] M.I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio, A. Courville, and R.D. Hjelm. MINE: Mutual Information Neural Estimation. In International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MI%20Belghazi%20A%20Baratin%20S%20Rajeswar%20S%20Ozair%20Y%20Bengio%20A%20Courville%20and%20RD%20Hjelm%20MINE%20Mutual%20Information%20Neural%20Estimation%20In%20International%20Conference%20on%20Machine%20Learning%20ICML%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MI%20Belghazi%20A%20Baratin%20S%20Rajeswar%20S%20Ozair%20Y%20Bengio%20A%20Courville%20and%20RD%20Hjelm%20MINE%20Mutual%20Information%20Neural%20Estimation%20In%20International%20Conference%20on%20Machine%20Learning%20ICML%202018"
        },
        {
            "id": "47",
            "entry": "[47] S. Zhao, J. Song, and S. Ermon. InfoVAE: Information Maximizing Variational Autoencoders. arXiv:1706.02262, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02262"
        },
        {
            "id": "48",
            "entry": "[48] A. Kolchinsky and B. D. Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolchinsky%2C%20A.%20Tracey%2C%20B.D.%20Estimating%20mixture%20entropy%20with%20pairwise%20distances%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolchinsky%2C%20A.%20Tracey%2C%20B.D.%20Estimating%20mixture%20entropy%20with%20pairwise%20distances%202017"
        },
        {
            "id": "49",
            "entry": "[49] A. Kraskov, H. St\u00f6gbauer, and P. Grassberger. Estimating mutual information. Physical Review E, 69(6):066138, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kraskov%2C%20A.%20St%C3%B6gbauer%2C%20H.%20Grassberger%2C%20P.%20Estimating%20mutual%20information%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kraskov%2C%20A.%20St%C3%B6gbauer%2C%20H.%20Grassberger%2C%20P.%20Estimating%20mutual%20information%202004"
        },
        {
            "id": "50",
            "entry": "[50] lsd: Learning with Synthetic Data, Python library. https://github.com/marylou-gabrie/learning-synthetic-data. ",
            "url": "https://github.com/marylou-gabrie/learning-synthetic-data"
        }
    ]
}
