{
    "filename": "7642-leveraging-the-exact-likelihood-of-deep-latent-variable-models.pdf",
    "metadata": {
        "title": "Leveraging the Exact Likelihood of Deep Latent Variable Models",
        "author": "Pierre-Alexandre Mattei, Jes Frellsen",
        "date": 1933,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7642-leveraging-the-exact-likelihood-of-deep-latent-variable-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a DLVM. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs."
    },
    "keywords": [
        {
            "term": "principal component analysis",
            "url": "https://en.wikipedia.org/wiki/principal_component_analysis"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "latent variable model",
            "url": "https://en.wikipedia.org/wiki/latent_variable_model"
        }
    ],
    "highlights": [
        "Dimension reduction aims at summarizing multivariate data using a small number of features that constitute a code",
        "Earliest attempts rested on linear projections, leading to Hotelling\u2019s (1933) principal component analysis (PCA) that has been vastly explored and perfected over the last century (<a class=\"ref-link\" id=\"cJolliffe_2016_a\" href=\"#rJolliffe_2016_a\">Jolliffe and Cadima, 2016</a>)",
        "We show that maximum likelihood is ill-posed for continuous deep latent variable models and well-posed for discrete ones",
        "Extremely difficult to compute in practice, the exact likelihood of deep latent variable models offers several important insights on deep generative modelling",
        "An important research direction for future work is the design of principled regularisation schemes for maximum likelihood estimation",
        "Missing data imputation is often used as a performance metric for deep latent variable models (e.g. <a class=\"ref-link\" id=\"cLi_et+al_2016_a\" href=\"#rLi_et+al_2016_a\">Li et al, 2016</a>; <a class=\"ref-link\" id=\"cDu_et+al_2018_a\" href=\"#rDu_et+al_2018_a\">Du et al, 2018</a>)"
    ],
    "key_statements": [
        "Dimension reduction aims at summarizing multivariate data using a small number of features that constitute a code",
        "Earliest attempts rested on linear projections, leading to Hotelling\u2019s (1933) principal component analysis (PCA) that has been vastly explored and perfected over the last century (<a class=\"ref-link\" id=\"cJolliffe_2016_a\" href=\"#rJolliffe_2016_a\">Jolliffe and Cadima, 2016</a>)",
        "The field has been vividly animated by the successes of latent variable models that probabilistically use the low-dimensional features to define powerful generative models",
        "We show that maximum likelihood is ill-posed for continuous deep latent variable models and well-posed for discrete ones",
        "Extremely difficult to compute in practice, the exact likelihood of deep latent variable models offers several important insights on deep generative modelling",
        "An important research direction for future work is the design of principled regularisation schemes for maximum likelihood estimation",
        "Missing data imputation is often used as a performance metric for deep latent variable models (e.g. <a class=\"ref-link\" id=\"cLi_et+al_2016_a\" href=\"#rLi_et+al_2016_a\">Li et al, 2016</a>; <a class=\"ref-link\" id=\"cDu_et+al_2018_a\" href=\"#rDu_et+al_2018_a\">Du et al, 2018</a>)"
    ],
    "summary": [
        "Dimension reduction aims at summarizing multivariate data using a small number of features that constitute a code.",
        "We investigate the properties of maximum likelihood estimation for DLVMs with Gaussian and Bernoulli observation models.",
        "The infinite maxima of the likelihood happen to be very poor generative models, whose density collapse around some of the data points.",
        "If the parametrisation of the decoder is such that the ima\u221age of \u03a3\u03b8 \u03b8, the log-likelihood function is upper bounded by \u2212np log 2\u03c0\u03be.",
        "The log-likelihood function of a deep latent model with a Bernoulli observation model is everywhere negative.",
        "Given any distribution G over the generic parameter space H, we define the nonparametric mixture model as: pG(x) = \u03a6(x|\u03b7)dG(\u03b7).",
        "Under the conditions of boundedness of the likelihood of deep Gaussian models, the bound is finite and attained for a finite mixture model with no more components than data points.",
        "The likelihood of latent variable models approximation gap expresses how far the posterior is from is usually harder than finding lower the variational family, and the amortisation gap appears due bounds (<a class=\"ref-link\" id=\"cGrosse_et+al_2015_a\" href=\"#rGrosse_et+al_2015_a\">Grosse et al, 2015</a>; Dieng to the limited capacity of the encoder (<a class=\"ref-link\" id=\"cCremer_et+al_2018_a\" href=\"#rCremer_et+al_2018_a\">Cremer et al, 2018</a>).",
        "This pseudo-Gibbs approach is routinely used for missing data imputation using DLVMs. <a class=\"ref-link\" id=\"cRezende_et+al_2014_a\" href=\"#rRezende_et+al_2014_a\">Rezende et al (2014</a>, Proposition F.1) proved that, when these two distributions are close in some sense, pseudo-Gibbs sampling generates points that approximatively follow the conditional distribution p\u03b8.",
        "At each step of the chain, rather Algorithm 1 Metropolis-within-Gibbs sampler for missing than generating codes according to data imputation using a trained VAE",
        "It exactly corresponds to the algorithm described by <a class=\"ref-link\" id=\"cGelman_1993_a\" href=\"#rGelman_1993_a\">Gelman (1993</a>, Section 4.4), and is ensured to asymptotically produce samples from the true conditional distribution p\u03b8, even if the variational approximation is imperfect.",
        "To investigate if the unboundedness of the likelihood of a DLVM with a Gaussian observation model has actual concrete consequences for",
        "Using the nonparametric upper bound as an early stopping criterion for the unconstrained ELBO appears to provide a good regularisation scheme\u2014that perform better than the covariance constraints on this data set.",
        "Extremely difficult to compute in practice, the exact likelihood of DLVMs offers several important insights on deep generative modelling.",
        "The important body of work regarding consistency of maximum likelihood estimates for nonparametric mixtures (e.g. <a class=\"ref-link\" id=\"cKiefer_1956_a\" href=\"#rKiefer_1956_a\">Kiefer and Wolfowitz, 1956</a>; van de Geer, 2003; <a class=\"ref-link\" id=\"cChen_2017_a\" href=\"#rChen_2017_a\">Chen, 2017</a>) could be leveraged to study the asymptotics of DLVMs.",
        "The samples generated by Metropolis-within-Gibbs do not depend on the inference network, and explicitly depend on the prior, which allows us to evaluate mainly the generative performance of the models"
    ],
    "headline": "We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation",
    "reference_links": [
        {
            "id": "Arora_et+al_2018_a",
            "entry": "S. Arora, A. Risteski, and Y. Zhang. Do GANs learn the distribution? Some theory and empirics. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20S.%20Risteski%2C%20A.%20Zhang%2C%20Y.%20Do%20GANs%20learn%20the%20distribution%3F%20Some%20theory%20and%20empirics%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20S.%20Risteski%2C%20A.%20Zhang%2C%20Y.%20Do%20GANs%20learn%20the%20distribution%3F%20Some%20theory%20and%20empirics%202018"
        },
        {
            "id": "Bartholomew_et+al_2011_a",
            "entry": "D. J. Bartholomew, M. Knott, and I. Moustaki. Latent variable models and factor analysis: A unified approach, volume 904. John Wiley & Sons, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartholomew%2C%20D.J.%20Knott%2C%20M.%20Moustaki%2C%20I.%20Latent%20variable%20models%20and%20factor%20analysis%3A%20A%20unified%20approach%2C%20volume%20904%202011"
        },
        {
            "id": "Biernacki_2011_a",
            "entry": "C. Biernacki and G. Castellan. A data-driven bound on variances for avoiding degeneracy in univariate Gaussian mixtures. Pub. IRMA Lille, 71, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biernacki%2C%20C.%20Castellan%2C%20G.%20A%20data-driven%20bound%20on%20variances%20for%20avoiding%20degeneracy%20in%20univariate%20Gaussian%20mixtures%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biernacki%2C%20C.%20Castellan%2C%20G.%20A%20data-driven%20bound%20on%20variances%20for%20avoiding%20degeneracy%20in%20univariate%20Gaussian%20mixtures%202011"
        },
        {
            "id": "Blei_et+al_2017_a",
            "entry": "D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20D.M.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20D.M.%20Kucukelbir%2C%20A.%20McAuliffe%2C%20J.D.%20Variational%20inference%3A%20A%20review%20for%20statisticians%202017"
        },
        {
            "id": "Bowman_et+al_2016_a",
            "entry": "S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences from a continuous space. Proceedings of CoNLL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowman%2C%20S.R.%20Vilnis%2C%20L.%20Vinyals%2C%20O.%20Dai%2C%20A.M.%20Generating%20sentences%20from%20a%20continuous%20space%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowman%2C%20S.R.%20Vilnis%2C%20L.%20Vinyals%2C%20O.%20Dai%2C%20A.M.%20Generating%20sentences%20from%20a%20continuous%20space%202016"
        },
        {
            "id": "Burda_et+al_2016_a",
            "entry": "Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burda%2C%20Y.%20Grosse%2C%20R.%20Salakhutdinov%2C%20R.%20Importance%20weighted%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burda%2C%20Y.%20Grosse%2C%20R.%20Salakhutdinov%2C%20R.%20Importance%20weighted%20autoencoders%202016"
        },
        {
            "id": "Chen_2017_a",
            "entry": "J. Chen. Consistency of the MLE under mixture models. Statistical Science, 32(1):47\u201363, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20J.%20Consistency%20of%20the%20MLE%20under%20mixture%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20J.%20Consistency%20of%20the%20MLE%20under%20mixture%20models%202017"
        },
        {
            "id": "Cremer_et+al_2017_a",
            "entry": "C. Cremer, Q. Morris, and D. Duvenaud. Reinterpreting importance-weighted autoencoders. International Conference on Learning Representations (Workshop track), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cremer%2C%20C.%20Morris%2C%20Q.%20Duvenaud%2C%20D.%20Reinterpreting%20importance-weighted%20autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cremer%2C%20C.%20Morris%2C%20Q.%20Duvenaud%2C%20D.%20Reinterpreting%20importance-weighted%20autoencoders%202017"
        },
        {
            "id": "Cremer_et+al_2018_a",
            "entry": "C. Cremer, X. Li, and D. Duvenaud. Inference suboptimality in variational autoencoders. In Proceedings of the 35th International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cremer%2C%20C.%20Li%2C%20X.%20Duvenaud%2C%20D.%20Inference%20suboptimality%20in%20variational%20autoencoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cremer%2C%20C.%20Li%2C%20X.%20Duvenaud%2C%20D.%20Inference%20suboptimality%20in%20variational%20autoencoders%202018"
        },
        {
            "id": "Dempster_et+al_1977_a",
            "entry": "A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology), pages 1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977"
        },
        {
            "id": "Dieng_et+al_2017_a",
            "entry": "A. B. Dieng, D. Tran, R. Ranganath, J. Paisley, and D. Blei. Variational inference via chi upper bound minimization. In Advances in Neural Information Processing Systems, pages 2732\u20132741, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieng%2C%20A.B.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Paisley%2C%20J.%20Variational%20inference%20via%20chi%20upper%20bound%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieng%2C%20A.B.%20Tran%2C%20D.%20Ranganath%2C%20R.%20Paisley%2C%20J.%20Variational%20inference%20via%20chi%20upper%20bound%20minimization%202017"
        },
        {
            "id": "Dinh_et+al_2017_a",
            "entry": "L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real NVP. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20L.%20Sohl-Dickstein%2C%20J.%20Bengio%2C%20S.%20Density%20estimation%20using%20real%20NVP%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20L.%20Sohl-Dickstein%2C%20J.%20Bengio%2C%20S.%20Density%20estimation%20using%20real%20NVP%202017"
        },
        {
            "id": "Du_et+al_2018_a",
            "entry": "C. Du, J. Zhu, and B. Zhang. Learning deep generative models with doubly stochastic gradient MCMC. IEEE Transactions on Neural Networks and Learning Systems, PP(99):1\u201313, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20C.%20Zhu%2C%20J.%20Zhang%2C%20B.%20Learning%20deep%20generative%20models%20with%20doubly%20stochastic%20gradient%20MCMC%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20C.%20Zhu%2C%20J.%20Zhang%2C%20B.%20Learning%20deep%20generative%20models%20with%20doubly%20stochastic%20gradient%20MCMC%202018"
        },
        {
            "id": "Gelman_1993_a",
            "entry": "A. Gelman. Iterative and non-iterative simulation algorithms. Computing science and statistics, pages 433\u2013433, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gelman%2C%20A.%20Iterative%20and%20non-iterative%20simulation%20algorithms%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gelman%2C%20A.%20Iterative%20and%20non-iterative%20simulation%20algorithms%201993"
        },
        {
            "id": "Geman_1984_a",
            "entry": "S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, (6):721\u2013741, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geman%2C%20S.%20Geman%2C%20D.%20Stochastic%20relaxation%2C%20Gibbs%20distributions%2C%20and%20the%20Bayesian%20restoration%20of%20images%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geman%2C%20S.%20Geman%2C%20D.%20Stochastic%20relaxation%2C%20Gibbs%20distributions%2C%20and%20the%20Bayesian%20restoration%20of%20images%201984"
        },
        {
            "id": "G_et+al_2018_a",
            "entry": "R. G\u00f3mez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hern\u00e1ndez-Lobato, B. S\u00e1nchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%C3%B3mez-Bombarelli%2C%20R.%20Wei%2C%20J.N.%20Duvenaud%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Automatic%20chemical%20design%20using%20a%20data-driven%20continuous%20representation%20of%20molecules%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%C3%B3mez-Bombarelli%2C%20R.%20Wei%2C%20J.N.%20Duvenaud%2C%20D.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Automatic%20chemical%20design%20using%20a%20data-driven%20continuous%20representation%20of%20molecules%202018"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20learning%202016"
        },
        {
            "id": "C_2018_a",
            "entry": "C. H. Gr\u00f8nbech, M. F. Vording, P. N. Timshel, C. K. S\u00f8nderby, T. H. Pers, and O. Winther. scVAE: Variational auto-encoders for single-cell gene expression data. bioRxiv, 2018. URL https://www.biorxiv.org/content/early/2018/05/16/318295.",
            "url": "https://www.biorxiv.org/content/early/2018/05/16/318295",
            "oa_query": "https://api.scholarcy.com/oa_version?query=scVAE%3A%20Variational%20auto-encoders%20for%20single-cell%20gene%20expression%20data%202018"
        },
        {
            "id": "Grosse_et+al_2015_a",
            "entry": "R. Grosse, Z. Ghahramani, and R. P. Adams. Sandwiching the marginal likelihood using bidirectional monte carlo. arXiv preprint arXiv:1511.02543, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.02543"
        },
        {
            "id": "Hastings_1970_a",
            "entry": "W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97\u2013109, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastings%2C%20W.K.%20Monte%20Carlo%20sampling%20methods%20using%20Markov%20chains%20and%20their%20applications%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastings%2C%20W.K.%20Monte%20Carlo%20sampling%20methods%20using%20Markov%20chains%20and%20their%20applications%201970"
        },
        {
            "id": "Hathaway_1985_a",
            "entry": "R. J. Hathaway. A constrained formulation of maximum-likelihood estimation for normal mixture distributions. The Annals of Statistics, 13(2):795\u2013800, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hathaway%2C%20R.J.%20A%20constrained%20formulation%20of%20maximum-likelihood%20estimation%20for%20normal%20mixture%20distributions%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hathaway%2C%20R.J.%20A%20constrained%20formulation%20of%20maximum-likelihood%20estimation%20for%20normal%20mixture%20distributions%201985"
        },
        {
            "id": "Heckerman_et+al_2000_a",
            "entry": "D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for inference, collaborative filtering, and data visualization. Journal of Machine Learning Research, 1 (Oct):49\u201375, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heckerman%2C%20D.%20Chickering%2C%20D.M.%20Meek%2C%20C.%20Rounthwaite%2C%20R.%20Dependency%20networks%20for%20inference%2C%20collaborative%20filtering%2C%20and%20data%20visualization%202000-10-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heckerman%2C%20D.%20Chickering%2C%20D.M.%20Meek%2C%20C.%20Rounthwaite%2C%20R.%20Dependency%20networks%20for%20inference%2C%20collaborative%20filtering%2C%20and%20data%20visualization%202000-10-01"
        },
        {
            "id": "Hotelling_1933_a",
            "entry": "H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hotelling%2C%20H.%20Analysis%20of%20a%20complex%20of%20statistical%20variables%20into%20principal%20components%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hotelling%2C%20H.%20Analysis%20of%20a%20complex%20of%20statistical%20variables%20into%20principal%20components%201933"
        },
        {
            "id": "Jolliffe_2016_a",
            "entry": "I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 374(2065), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jolliffe%2C%20I.T.%20Cadima%2C%20J.%20Principal%20component%20analysis%3A%20a%20review%20and%20recent%20developments%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jolliffe%2C%20I.T.%20Cadima%2C%20J.%20Principal%20component%20analysis%3A%20a%20review%20and%20recent%20developments%202016"
        },
        {
            "id": "Kiefer_1956_a",
            "entry": "J. Kiefer and J. Wolfowitz. Consistency of the maximum likelihood estimator in the presence of infinitely many incidental parameters. The Annals of Mathematical Statistics, pages 887\u2013906, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiefer%2C%20J.%20Wolfowitz%2C%20J.%20Consistency%20of%20the%20maximum%20likelihood%20estimator%20in%20the%20presence%20of%20infinitely%20many%20incidental%20parameters%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiefer%2C%20J.%20Wolfowitz%2C%20J.%20Consistency%20of%20the%20maximum%20likelihood%20estimator%20in%20the%20presence%20of%20infinitely%20many%20incidental%20parameters%201956"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In Proceedings of the International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202014"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743\u20134751, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Jozefowicz%2C%20R.%20Chen%2C%20X.%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Jozefowicz%2C%20R.%20Chen%2C%20X.%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016"
        },
        {
            "id": "Kusner_et+al_2017_a",
            "entry": "M. J. Kusner, B. Paige, and J. M. Hern\u00e1ndez-Lobato. Grammar variational autoencoder. In Proceedings of the 34th International Conference on Machine Learning, pages 1945\u20131954, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kusner%2C%20M.J.%20Paige%2C%20B.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Grammar%20variational%20autoencoder%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kusner%2C%20M.J.%20Paige%2C%20B.%20Hern%C3%A1ndez-Lobato%2C%20J.M.%20Grammar%20variational%20autoencoder%202017"
        },
        {
            "id": "Cam_1990_a",
            "entry": "L. Le Cam. Maximum likelihood: an introduction. International Statistical Review/Revue Internationale de Statistique, pages 153\u2013171, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cam%2C%20L.Le%20Maximum%20likelihood%3A%20an%20introduction%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cam%2C%20L.Le%20Maximum%20likelihood%3A%20an%20introduction%201990"
        },
        {
            "id": "Leshno_et+al_1993_a",
            "entry": "M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6(6):861\u2013867, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leshno%2C%20M.%20Lin%2C%20V.Y.%20Pinkus%2C%20A.%20Schocken%2C%20S.%20Multilayer%20feedforward%20networks%20with%20a%20nonpolynomial%20activation%20function%20can%20approximate%20any%20function%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leshno%2C%20M.%20Lin%2C%20V.Y.%20Pinkus%2C%20A.%20Schocken%2C%20S.%20Multilayer%20feedforward%20networks%20with%20a%20nonpolynomial%20activation%20function%20can%20approximate%20any%20function%201993"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "C. Li, J. Zhu, and B. Zhang. Learning to generate with memory. In Proceedings of The 33rd International Conference on Machine Learning, pages 1177\u20131186, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Zhu%2C%20J.%20Zhang%2C%20B.%20Learning%20to%20generate%20with%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Zhu%2C%20J.%20Zhang%2C%20B.%20Learning%20to%20generate%20with%20memory%202016"
        },
        {
            "id": "Lindsay_1983_a",
            "entry": "B. Lindsay. The geometry of mixture likelihoods: a general theory. The Annals of Statistics, 11(1): 86\u201394, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lindsay%2C%20B.%20The%20geometry%20of%20mixture%20likelihoods%3A%20a%20general%20theory%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lindsay%2C%20B.%20The%20geometry%20of%20mixture%20likelihoods%3A%20a%20general%20theory%201983"
        },
        {
            "id": "Lindsay_1995_a",
            "entry": "B. Lindsay. Mixture Models: Theory, Geometry and Applications, volume 5 of Regional Conference Series in Probability and Statistics. Institute of Mathematical Statistics and American Statistical Association, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lindsay%2C%20B.%20Mixture%20Models%3A%20Theory%2C%20Geometry%20and%20Applications%2C%20volume%205%20of%20Regional%20Conference%20Series%20in%20Probability%20and%20Statistics%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lindsay%2C%20B.%20Mixture%20Models%3A%20Theory%2C%20Geometry%20and%20Applications%2C%20volume%205%20of%20Regional%20Conference%20Series%20in%20Probability%20and%20Statistics%201995"
        },
        {
            "id": "Lucas_et+al_2018_a",
            "entry": "T. Lucas, C. Tallec, J. Verbeek, and Y. Ollivier. Mixed batches and symmetric discriminators for GAN training. In International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucas%2C%20T.%20Tallec%2C%20C.%20Verbeek%2C%20J.%20Ollivier%2C%20Y.%20Mixed%20batches%20and%20symmetric%20discriminators%20for%20GAN%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucas%2C%20T.%20Tallec%2C%20C.%20Verbeek%2C%20J.%20Ollivier%2C%20Y.%20Mixed%20batches%20and%20symmetric%20discriminators%20for%20GAN%20training%202018"
        },
        {
            "id": "L_2016_a",
            "entry": "L. Maal\u00f8e, C. K. S\u00f8nderby, S. K. S\u00f8nderby, and O. Winther. Auxiliary deep generative models. In International Conference on Machine Learning, pages 1445\u20131453, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L.%20Maal%C3%B8e%2C%20C.%20K.%20S%C3%B8nderby%2C%20S.%20K.%20S%C3%B8nderby%20Winther%2C%20O.%20Auxiliary%20deep%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L.%20Maal%C3%B8e%2C%20C.%20K.%20S%C3%B8nderby%2C%20S.%20K.%20S%C3%B8nderby%20Winther%2C%20O.%20Auxiliary%20deep%20generative%20models%202016"
        },
        {
            "id": "Metropolis_et+al_1953_a",
            "entry": "N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6):1087\u20131092, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metropolis%2C%20N.%20Rosenbluth%2C%20A.W.%20Rosenbluth%2C%20M.N.%20Teller%2C%20A.H.%20Equation%20of%20state%20calculations%20by%20fast%20computing%20machines%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metropolis%2C%20N.%20Rosenbluth%2C%20A.W.%20Rosenbluth%2C%20M.N.%20Teller%2C%20A.H.%20Equation%20of%20state%20calculations%20by%20fast%20computing%20machines%201953"
        },
        {
            "id": "Pu_et+al_2016_a",
            "entry": "Y. Pu, Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens, and L. Carin. Variational autoencoder for deep learning of images, labels and captions. In Advances in Neural Information Processing Systems, pages 2352\u20132360, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pu%2C%20Y.%20Gan%2C%20Z.%20Henao%2C%20R.%20Yuan%2C%20X.%20Variational%20autoencoder%20for%20deep%20learning%20of%20images%2C%20labels%20and%20captions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pu%2C%20Y.%20Gan%2C%20Z.%20Henao%2C%20R.%20Yuan%2C%20X.%20Variational%20autoencoder%20for%20deep%20learning%20of%20images%2C%20labels%20and%20captions%202016"
        },
        {
            "id": "Ranganath_et+al_2016_a",
            "entry": "R. Ranganath, D. Tran, and D. Blei. Hierarchical variational models. In Proceedings of the 33rd International Conference on Machine Learning, pages 324\u2013333, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20R.%20Tran%2C%20D.%20Blei%2C%20D.%20Hierarchical%20variational%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20R.%20Tran%2C%20D.%20Blei%2C%20D.%20Hierarchical%20variational%20models%202016"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "D. Rezende and S. Mohamed. Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on Machine Learning, pages 1530\u20131538, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "Rezende_2018_a",
            "entry": "D. Rezende and F. Viola. Taming VAEs. arXiv preprint arXiv:1810.00597, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.00597"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pages 1278\u20131286, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Rezende_et+al_2016_a",
            "entry": "D. Rezende, S. M. A. Eslami, S. Mohamed, P. Battaglia, M. Jaderberg, and N. Heess. Unsupervised learning of 3D structure from images. In Advances in Neural Information Processing Systems, pages 4996\u20135004, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.%20Eslami%2C%20S.M.A.%20Mohamed%2C%20S.%20Battaglia%2C%20P.%20Unsupervised%20learning%20of%203D%20structure%20from%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.%20Eslami%2C%20S.M.A.%20Mohamed%2C%20S.%20Battaglia%2C%20P.%20Unsupervised%20learning%20of%203D%20structure%20from%20images%202016"
        },
        {
            "id": "Richardson_2018_a",
            "entry": "E. Richardson and Y. Weiss. On GANs and GMMs. In Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richardson%2C%20E.%20Weiss%2C%20Y.%20On%20GANs%20and%20GMMs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richardson%2C%20E.%20Weiss%2C%20Y.%20On%20GANs%20and%20GMMs%202018"
        },
        {
            "id": "Roeder_et+al_2017_a",
            "entry": "G. Roeder, Y. Wu, and D. Duvenaud. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. In Advances in Neural Information Processing Systems, pages 6928\u20136937, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roeder%2C%20G.%20Wu%2C%20Y.%20Duvenaud%2C%20D.%20Sticking%20the%20landing%3A%20Simple%2C%20lower-variance%20gradient%20estimators%20for%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roeder%2C%20G.%20Wu%2C%20Y.%20Duvenaud%2C%20D.%20Sticking%20the%20landing%3A%20Simple%2C%20lower-variance%20gradient%20estimators%20for%20variational%20inference%202017"
        },
        {
            "id": "Salimans_et+al_2017_a",
            "entry": "T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. Proceedings of the International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Karpathy%2C%20A.%20Chen%2C%20X.%20Kingma%2C%20D.P.%20Pixelcnn%2B%2B%3A%20Improving%20the%20pixelcnn%20with%20discretized%20logistic%20mixture%20likelihood%20and%20other%20modifications%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Karpathy%2C%20A.%20Chen%2C%20X.%20Kingma%2C%20D.P.%20Pixelcnn%2B%2B%3A%20Improving%20the%20pixelcnn%20with%20discretized%20logistic%20mixture%20likelihood%20and%20other%20modifications%202017"
        },
        {
            "id": "Siddharth_et+al_2017_a",
            "entry": "N. Siddharth, B. Paige, J.-W. van de Meent, A. Desmaison, N. Goodman, P. Kohli, F. Wood, and P. Torr. Learning disentangled representations with semi-supervised deep generative models. In Advances in Neural Information Processing Systems, pages 5927\u20135937, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Siddharth%2C%20N.%20Paige%2C%20B.%20van%20de%20Meent%2C%20J.-W.%20Desmaison%2C%20A.%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Siddharth%2C%20N.%20Paige%2C%20B.%20van%20de%20Meent%2C%20J.-W.%20Desmaison%2C%20A.%20Learning%20disentangled%20representations%20with%20semi-supervised%20deep%20generative%20models%202017"
        },
        {
            "id": "C_2016_a",
            "entry": "C. K. S\u00f8nderby, T. Raiko, L. Maal\u00f8e, S. Kaae S\u00f8nderby, and O. Winther. Ladder variational autoencoders. In Advances in Neural Information Processing Systems, pages 3738\u20133746, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ladder%20variational%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ladder%20variational%20autoencoders%202016"
        },
        {
            "id": "Takahashi_et+al_2018_a",
            "entry": "H. Takahashi, T. Iwata, Y. Yamanaka, M. Yamada, and S. Yagi. Student-t variational autoencoder for robust density estimation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, pages 2696\u20132702. International Joint Conferences on Artificial Intelligence Organization, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takahashi%2C%20H.%20Iwata%2C%20T.%20Yamanaka%2C%20Y.%20Yamada%2C%20M.%20Student-t%20variational%20autoencoder%20for%20robust%20density%20estimation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takahashi%2C%20H.%20Iwata%2C%20T.%20Yamanaka%2C%20Y.%20Yamada%2C%20M.%20Student-t%20variational%20autoencoder%20for%20robust%20density%20estimation%202018"
        },
        {
            "id": "Tikhonov_1977_a",
            "entry": "A. N. Tikhonov and V. Y. Arsenin. Solutions of ill-posed problems. New York: Winston, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tikhonov%2C%20A.N.%20Arsenin%2C%20V.Y.%20Solutions%20of%20ill-posed%20problems%201977"
        },
        {
            "id": "Tipping_1999_a",
            "entry": "M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611\u2013622, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tipping%2C%20M.E.%20Bishop%2C%20C.M.%20Probabilistic%20principal%20component%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tipping%2C%20M.E.%20Bishop%2C%20C.M.%20Probabilistic%20principal%20component%20analysis%201999"
        },
        {
            "id": "Tomczak_2018_a",
            "entry": "J. Tomczak and M. Welling. VAE with a VampPrior. In International Conference on Artificial Intelligence and Statistics, pages 1214\u20131223, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomczak%2C%20J.%20Welling%2C%20M.%20VAE%20with%20a%20VampPrior%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomczak%2C%20J.%20Welling%2C%20M.%20VAE%20with%20a%20VampPrior%202018"
        },
        {
            "id": "Van_2003_a",
            "entry": "S. van de Geer. Asymptotic theory for maximum likelihood in nonparametric mixture models. Computational Statistics & Data Analysis, 41(3-4):453\u2013464, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20de%20Geer%2C%20S.%20Asymptotic%20theory%20for%20maximum%20likelihood%20in%20nonparametric%20mixture%20models%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20de%20Geer%2C%20S.%20Asymptotic%20theory%20for%20maximum%20likelihood%20in%20nonparametric%20mixture%20models%202003"
        },
        {
            "id": "Van_1992_a",
            "entry": "A. W. van der Vaart and J. A. Wellner. Existence and consistency of maximum likelihood in upgraded mixture models. Journal of Multivariate Analysis, 43(1):133\u2013146, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Vaart%2C%20A.W.%20Wellner%2C%20J.A.%20Existence%20and%20consistency%20of%20maximum%20likelihood%20in%20upgraded%20mixture%20models%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Vaart%2C%20A.W.%20Wellner%2C%20J.A.%20Existence%20and%20consistency%20of%20maximum%20likelihood%20in%20upgraded%20mixture%20models%201992"
        },
        {
            "id": "Wang_2007_a",
            "entry": "Y. Wang. On fast computation of the non-parametric maximum likelihood estimate of a mixing distribution. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2): 185\u2013198, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20On%20fast%20computation%20of%20the%20non-parametric%20maximum%20likelihood%20estimate%20of%20a%20mixing%20distribution%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20On%20fast%20computation%20of%20the%20non-parametric%20maximum%20likelihood%20estimate%20of%20a%20mixing%20distribution%202007"
        },
        {
            "id": "Zhao_et+al_2017_a",
            "entry": "S. Zhao, J. Song, and S. Ermon. Learning hierarchical features from deep generative models. In Proceedings of the 34th International Conference on Machine Learning, pages 4091\u20134099, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20S.%20Song%2C%20J.%20Ermon%2C%20S.%20Learning%20hierarchical%20features%20from%20deep%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20S.%20Song%2C%20J.%20Ermon%2C%20S.%20Learning%20hierarchical%20features%20from%20deep%20generative%20models%202017"
        }
    ]
}
