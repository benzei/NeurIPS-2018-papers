{
    "filename": "8237-adversarial-risk-and-robustness-general-definitions-and-implications-for-the-uniform-distribution.pdf",
    "metadata": {
        "title": "Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution",
        "author": "Dimitrios Diochnos, Saeed Mahloujifar, Mohammad Mahmoody",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8237-adversarial-risk-and-robustness-general-definitions-and-implications-for-the-uniform-distribution.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study adversarial perturbations when the instances are uniformly distributed over {0, 1}n. We study both \u201cinherent\u201d bounds that apply to any problem and any classifier for such a problem as well as bounds that apply to specific problems and specific hypothesis classes."
    },
    "keywords": [
        {
            "term": "uniform distribution",
            "url": "https://en.wikipedia.org/wiki/uniform_distribution"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        }
    ],
    "highlights": [
        "Modern machine learning tools have pushed to new heights the classification results on traditional datasets that are used as testbeds for various machine learning methods.1",
        "As the current literature contains multiple definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region",
        "We study some classic algorithms for learning monotone conjunctions and compare their adversarial robustness under different definitions by attacking the hypotheses using instances drawn from the uniform distribution",
        "Like all other current provable bounds in the literature for adversarial risk and robustness only apply to specific distributions that do not cover the case of image distributions",
        "In Section 3, we show that previous definitions of robustness that are not based on the error region, lead to bounds that do not equate the bounds provided by the error-region approach",
        "The message is that the adversarial robustness that is based on the definitions of prediction change and corrupted instance is more or less the same, whereas the adversarial robustness based on the error region definition may obtain wildly different values compared to the other two"
    ],
    "key_statements": [
        "Modern machine learning tools have pushed to new heights the classification results on traditional datasets that are used as testbeds for various machine learning methods.1",
        "As the current literature contains multiple definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region",
        "We study some classic algorithms for learning monotone conjunctions and compare their adversarial robustness under different definitions by attacking the hypotheses using instances drawn from the uniform distribution",
        "Using the isoperimetric inequality for the Boolean hypercube, we show that for\u221ainitial error 0.01, there always exists an adversarial perturbation that changes O( n) bits of the instances to increase the risk to 0.5, making classifier\u2019s decisions meaningless",
        "Modern machine learning tools have pushed to new heights the classification results on traditional datasets that are used as testbeds for various machine learning methods.1",
        "We particularly study adversarial risk and robustness for learning problems where the input distribution is Un which is uniform over the hypercube {0, 1}n",
        "Like all other current provable bounds in the literature for adversarial risk and robustness only apply to specific distributions that do not cover the case of image distributions",
        "As pursued in [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], these works motivate a deeper study of such inequalities for real data sets",
        "We ask: how close/far are these definitions in settings where, e.g., the instances are drawn from the uniform distribution? To answer this question, we make a comparative study of adversarial risk and robustness for a particular case of learning monotone conjunctions under the uniform distribution Un",
        "In Section 3, we show that previous definitions of robustness that are not based on the error region, lead to bounds that do not equate the bounds provided by the error-region approach",
        "After establishing further motivation to use the error-region definition as the default definition for studying adversarial examples in general settings, we turn into studying inherent obstacles against robust classification when the instances are drawn from the uniform distribution",
        "The probability mass of the error region between h and c, denoted by \u03bc, under the uniform distribution Un over {0, 1}n is Pr [x \u2208 E (h, c)] = \u03bc = (2w + 2u \u2212 2) \u00b7 2\u2212m\u2212u\u2212w. In this problem setup we are interested in computing the adversarial risk and robustness that attackers can achieve when instances are drawn from the uniform distribution Un over {0, 1}n",
        "This way we are able to examine how some popular algorithms behave under attack, and we explore the extent to which the generated solutions of such algorithms exhibit differences in their robustness on average against various target functions drawn from the class of monotone conjunctions",
        "The message is that the adversarial robustness that is based on the definitions of prediction change and corrupted instance is more or less the same, whereas the adversarial robustness based on the error region definition may obtain wildly different values compared to the other two",
        "The following theorem, gives a general lower bound for the adversarial risk of any classification problem for uniform distribution Un over the hypercube {0, 1}n, depending on the original error"
    ],
    "summary": [
        "Modern machine learning tools have pushed to new heights the classification results on traditional datasets that are used as testbeds for various machine learning methods.1.",
        "We study some classic algorithms for learning monotone conjunctions and compare their adversarial robustness under different definitions by attacking the hypotheses using instances drawn from the uniform distribution.",
        "Using the error-region definition of adversarial perturbations, we study inherent bounds on risk and robustness of any classifier for any classification problem whose instances are uniformly distributed over {0, 1}n.",
        "We particularly study adversarial risk and robustness for learning problems where the input distribution is Un which is uniform over the hypercube {0, 1}n.",
        "By studying adversarial risk and robustness for such a natural distribution, we can immediately obtain results for a broad class of problems.",
        "We make a comparative study of adversarial risk and robustness for a particular case of learning monotone conjunctions under the uniform distribution Un. A monotone conjunction f is a function of the form f =.",
        "After establishing further motivation to use the error-region definition as the default definition for studying adversarial examples in general settings, we turn into studying inherent obstacles against robust classification when the instances are drawn from the uniform distribution.",
        "We compare the risk and robustness under the three definitions of Section 2 through a study of monotone conjunctions under the uniform distribution.",
        "In this problem setup we are interested in computing the adversarial risk and robustness that attackers can achieve when instances are drawn from the uniform distribution Un over {0, 1}n.",
        "Using the Problem Setup 1, in what follows we compute the adversarial robustness that an arbitrary hypothesis has against an arbitrary target using the error region (ER) definition that we advocate in contexts where the perturbed input is supposed to be misclassified and do the same calculations for adversarial risk and robustness that are based on the definitions of prediction change (PC) and corrupted instance (CI).",
        "The message is that the adversarial robustness that is based on the definitions of prediction change and corrupted instance is more or less the same, whereas the adversarial robustness based on the error region definition may obtain wildly different values compared to the other two.",
        "We state our main theorems about error region adversarial risk and robustness of arbitrary learning problems whose instances are distributed uniformly over the n-dimension hypercube {0, 1}n.",
        "The following theorem, gives a general lower bound for the adversarial risk of any classification problem for uniform distribution Un over the hypercube {0, 1}n, depending on the original error."
    ],
    "headline": "We study adversarial perturbations when the instances are uniformly distributed over {0, 1}n",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Idan Attias, Aryeh Kontorovich, and Yishay Mansour. Improved generalization bounds for robust learning. arXiv preprint arXiv:1810.02180, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.02180"
        },
        {
            "id": "2",
            "entry": "[2] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya V. Nori, and Antonio Criminisi. Measuring Neural Net Robustness with Constraints. In NIPS, pages 2613\u20132621, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bastani%2C%20Osbert%20Ioannou%2C%20Yani%20Lampropoulos%2C%20Leonidas%20Vytiniotis%2C%20Dimitrios%20Measuring%20Neural%20Net%20Robustness%20with%20Constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bastani%2C%20Osbert%20Ioannou%2C%20Yani%20Lampropoulos%2C%20Leonidas%20Vytiniotis%2C%20Dimitrios%20Measuring%20Neural%20Net%20Robustness%20with%20Constraints%202016"
        },
        {
            "id": "3",
            "entry": "[3] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi S. Nemirovski. Robust Optimization. Princeton Series in Applied Mathematics. Princeton University Press, October 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aharon%20BenTal%20Laurent%20El%20Ghaoui%20and%20Arkadi%20S%20Nemirovski%20Robust%20Optimization%20Princeton%20Series%20in%20Applied%20Mathematics%20Princeton%20University%20Press%20October%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aharon%20BenTal%20Laurent%20El%20Ghaoui%20and%20Arkadi%20S%20Nemirovski%20Robust%20Optimization%20Princeton%20Series%20in%20Applied%20Mathematics%20Princeton%20University%20Press%20October%202009"
        },
        {
            "id": "4",
            "entry": "[4] Battista Biggio, Giorgio Fumera, and Fabio Roli. Security evaluation of pattern classifiers under attack. IEEE transactions on knowledge and data engineering, 26(4):984\u2013996, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20Battista%20Fumera%2C%20Giorgio%20Roli%2C%20Fabio%20Security%20evaluation%20of%20pattern%20classifiers%20under%20attack%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20Battista%20Fumera%2C%20Giorgio%20Roli%2C%20Fabio%20Security%20evaluation%20of%20pattern%20classifiers%20under%20attack%202014"
        },
        {
            "id": "5",
            "entry": "[5] Avrim Blum, Merrick L. Furst, Jeffrey C. Jackson, Michael J. Kearns, Yishay Mansour, and Steven Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In STOC, pages 253\u2013262, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Furst%2C%20Merrick%20L.%20Jackson%2C%20Jeffrey%20C.%20Kearns%2C%20Michael%20J.%20Weakly%20learning%20DNF%20and%20characterizing%20statistical%20query%20learning%20using%20Fourier%20analysis%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Furst%2C%20Merrick%20L.%20Jackson%2C%20Jeffrey%20C.%20Kearns%2C%20Michael%20J.%20Weakly%20learning%20DNF%20and%20characterizing%20statistical%20query%20learning%20using%20Fourier%20analysis%201994"
        },
        {
            "id": "6",
            "entry": "[6] Jerome S. Bruner, Jacqueline J. Goodnow, and George A. Austin. A study of thinking. John Wiley & Sons, New York, NY, USA, 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruner%2C%20Jerome%20S.%20Goodnow%2C%20Jacqueline%20J.%20Austin%2C%20George%20A.%20A%20study%20of%20thinking%201957"
        },
        {
            "id": "7",
            "entry": "[7] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 3\u201314. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017"
        },
        {
            "id": "8",
            "entry": "[8] Nicholas Carlini and David A. Wagner. Towards Evaluating the Robustness of Neural Networks. In 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 39\u201357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Towards%20Evaluating%20the%20Robustness%20of%20Neural%20Networks%202017-05-22",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Towards%20Evaluating%20the%20Robustness%20of%20Neural%20Networks%202017-05-22"
        },
        {
            "id": "9",
            "entry": "[9] Dimitrios I. Diochnos. On the Evolution of Monotone Conjunctions: Drilling for Best Approximations. In ALT, pages 98\u2013112, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diochnos%2C%20Dimitrios%20I.%20On%20the%20Evolution%20of%20Monotone%20Conjunctions%3A%20Drilling%20for%20Best%20Approximations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diochnos%2C%20Dimitrios%20I.%20On%20the%20Evolution%20of%20Monotone%20Conjunctions%3A%20Drilling%20for%20Best%20Approximations%202016"
        },
        {
            "id": "10",
            "entry": "[10] Dimitrios I. Diochnos and Gy\u00f6rgy Tur\u00e1n. On Evolvability: The Swapping Algorithm, Product Distributions, and Covariance. In SAGA, pages 74\u201388, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diochnos%2C%20Dimitrios%20I.%20Tur%C3%A1n%2C%20Gy%C3%B6rgy%20On%20Evolvability%3A%20The%20Swapping%20Algorithm%2C%20Product%20Distributions%2C%20and%20Covariance%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diochnos%2C%20Dimitrios%20I.%20Tur%C3%A1n%2C%20Gy%C3%B6rgy%20On%20Evolvability%3A%20The%20Swapping%20Algorithm%2C%20Product%20Distributions%2C%20and%20Covariance%202009"
        },
        {
            "id": "11",
            "entry": "[11] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv preprint arXiv:1802.08686, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08686"
        },
        {
            "id": "12",
            "entry": "[12] Uriel Feige, Yishay Mansour, and Robert Schapire. Learning and inference in the presence of corrupted inputs. In Conference on Learning Theory, pages 637\u2013657, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feige%2C%20Uriel%20Mansour%2C%20Yishay%20Schapire%2C%20Robert%20Learning%20and%20inference%20in%20the%20presence%20of%20corrupted%20inputs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feige%2C%20Uriel%20Mansour%2C%20Yishay%20Schapire%2C%20Robert%20Learning%20and%20inference%20in%20the%20presence%20of%20corrupted%20inputs%202015"
        },
        {
            "id": "13",
            "entry": "[13] Uriel Feige, Yishay Mansour, and Robert E Schapire. Robust inference for multiclass classification. In Algorithmic Learning Theory, pages 368\u2013386, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feige%2C%20Uriel%20Mansour%2C%20Yishay%20Schapire%2C%20Robert%20E.%20Robust%20inference%20for%20multiclass%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feige%2C%20Uriel%20Mansour%2C%20Yishay%20Schapire%2C%20Robert%20E.%20Robust%20inference%20for%20multiclass%20classification%202018"
        },
        {
            "id": "14",
            "entry": "[14] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "15",
            "entry": "[15] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20Harnessing%20Adversarial%20Examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20Harnessing%20Adversarial%20Examples%202015"
        },
        {
            "id": "16",
            "entry": "[16] Lawrence H Harper. Optimal numberings and isoperimetric problems on graphs. Journal of Combinatorial Theory, 1(3):385\u2013393, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harper%2C%20Lawrence%20H.%20Optimal%20numberings%20and%20isoperimetric%20problems%20on%20graphs%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harper%2C%20Lawrence%20H.%20Optimal%20numberings%20and%20isoperimetric%20problems%20on%20graphs%201966"
        },
        {
            "id": "17",
            "entry": "[17] Jeffrey C. Jackson and Rocco A. Servedio. On Learning Random DNF Formulas Under the Uniform Distribution. Theory of Computing, 2(8):147\u2013172, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jackson%2C%20Jeffrey%20C.%20Servedio%2C%20Rocco%20A.%20On%20Learning%20Random%20DNF%20Formulas%20Under%20the%20Uniform%20Distribution%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jackson%2C%20Jeffrey%20C.%20Servedio%2C%20Rocco%20A.%20On%20Learning%20Random%20DNF%20Formulas%20Under%20the%20Uniform%20Distribution%202006"
        },
        {
            "id": "18",
            "entry": "[18] Daniel Lowd and Christopher Meek. Adversarial learning. In KDD, pages 641\u2013647, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lowd%2C%20Daniel%20Meek%2C%20Christopher%20Adversarial%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowd%2C%20Daniel%20Meek%2C%20Christopher%20Adversarial%20learning%202005"
        },
        {
            "id": "19",
            "entry": "[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083; to appear in International Conference on Learning Representations (ICLR), 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "20",
            "entry": "[20] Yishay Mansour, Aviad Rubinstein, and Moshe Tennenholtz. Robust probabilistic inference. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms, pages 449\u2013460. Society for Industrial and Applied Mathematics, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Yishay%20Rubinstein%2C%20Aviad%20Tennenholtz%2C%20Moshe%20Robust%20probabilistic%20inference%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Yishay%20Rubinstein%2C%20Aviad%20Tennenholtz%2C%20Moshe%20Robust%20probabilistic%20inference%202015"
        },
        {
            "id": "21",
            "entry": "[21] Thomas M. Mitchell. Machine Learning. McGraw-Hill, Inc., New York, NY, USA, 1 edition, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitchell%2C%20Thomas%20M.%20Machine%20Learning%201997"
        },
        {
            "id": "22",
            "entry": "[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In CVPR, pages 2574\u20132582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%3A%20A%20Simple%20and%20Accurate%20Method%20to%20Fool%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%3A%20A%20Simple%20and%20Accurate%20Method%20to%20Fool%202016"
        },
        {
            "id": "23",
            "entry": "[23] Blaine Nelson, Benjamin I. P. Rubinstein, Ling Huang, Anthony D. Joseph, and J. D. Tygar. Classifier Evasion: Models and Open Problems. In PSDM, pages 92\u201398, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelson%2C%20Blaine%20Rubinstein%2C%20Benjamin%20I.P.%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Classifier%20Evasion%3A%20Models%20and%20Open%20Problems%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelson%2C%20Blaine%20Rubinstein%2C%20Benjamin%20I.P.%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Classifier%20Evasion%3A%20Models%20and%20Open%20Problems%202010"
        },
        {
            "id": "24",
            "entry": "[24] Blaine Nelson, Benjamin IP Rubinstein, Ling Huang, Anthony D Joseph, Steven J Lee, Satish Rao, and JD Tygar. Query strategies for evading convex-inducing classifiers. Journal of Machine Learning Research, 13(May):1293\u20131332, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelson%2C%20Blaine%20Rubinstein%2C%20Benjamin%20I.P.%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Query%20strategies%20for%20evading%20convex-inducing%20classifiers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelson%2C%20Blaine%20Rubinstein%2C%20Benjamin%20I.P.%20Huang%2C%20Ling%20Joseph%2C%20Anthony%20D.%20Query%20strategies%20for%20evading%20convex-inducing%20classifiers%202012"
        },
        {
            "id": "25",
            "entry": "[25] R. G. Nigmatullin. Some metric relations in the unit cube (in russian). Diskretny Analiz 9, Novosibirsk, pages 47\u201358, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nigmatullin%2C%20R.G.%20Some%20metric%20relations%20in%20the%20unit%20cube%20%28in%20russian%29%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nigmatullin%2C%20R.G.%20Some%20metric%20relations%20in%20the%20unit%20cube%20%28in%20russian%29%201967"
        },
        {
            "id": "26",
            "entry": "[26] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks. In IEEE Symposium on Security and Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016, pages 582\u2013597, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20D.%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20Defense%20to%20Adversarial%20Perturbations%20Against%20Deep%20Neural%20Networks%202016-05-22",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20D.%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20Defense%20to%20Adversarial%20Perturbations%20Against%20Deep%20Neural%20Networks%202016-05-22"
        },
        {
            "id": "27",
            "entry": "[27] Yoshifumi Sakai and Akira Maruoka. Learning Monotone Log-Term DNF Formulas under the Uniform Distribution. Theory of Computing Systems, 33(1):17\u201333, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sakai%2C%20Yoshifumi%20Maruoka%2C%20Akira%20Learning%20Monotone%20Log-Term%20DNF%20Formulas%20under%20the%20Uniform%20Distribution%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sakai%2C%20Yoshifumi%20Maruoka%2C%20Akira%20Learning%20Monotone%20Log-Term%20DNF%20Formulas%20under%20the%20Uniform%20Distribution%202000"
        },
        {
            "id": "28",
            "entry": "[28] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11285"
        },
        {
            "id": "29",
            "entry": "[29] Linda Sellie. Exact learning of random DNF over the uniform distribution. In STOC, pages 45\u201354, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sellie%2C%20Linda%20Exact%20learning%20of%20random%20DNF%20over%20the%20uniform%20distribution%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sellie%2C%20Linda%20Exact%20learning%20of%20random%20DNF%20over%20the%20uniform%20distribution%202009"
        },
        {
            "id": "30",
            "entry": "[30] Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, and Pradeep Ravikumar. On Adversarial Risk and Training. arXiv preprint arXiv:1806.02924, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02924"
        },
        {
            "id": "31",
            "entry": "[31] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "32",
            "entry": "[32] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness May Be at Odds with Accuracy. arXiv preprint arXiv:1805.12152, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12152"
        },
        {
            "id": "33",
            "entry": "[33] Leslie G. Valiant. A Theory of the Learnable. Communications of the ACM, 27(11):1134\u20131142, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valiant%2C%20Leslie%20G.%20A%20Theory%20of%20the%20Learnable%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valiant%2C%20Leslie%20G.%20A%20Theory%20of%20the%20Learnable%201984"
        },
        {
            "id": "34",
            "entry": "[34] Leslie G. Valiant. Evolvability. Journal of the ACM, 56(1):3:1\u20133:21, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leslie%20G%20Valiant%20Evolvability%20Journal%20of%20the%20ACM%2056131321%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leslie%20G%20Valiant%20Evolvability%20Journal%20of%20the%20ACM%2056131321%202009"
        },
        {
            "id": "35",
            "entry": "[35] Weilin Xu, David Evans, and Yanjun Qi. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. arXiv preprint arXiv:1704.01155. To appear in Network and Distributed System Security Symposium (NDSS), 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1704.01155"
        }
    ]
}
