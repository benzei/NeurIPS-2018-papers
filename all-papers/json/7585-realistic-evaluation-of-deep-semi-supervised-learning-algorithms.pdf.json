{
    "filename": "7585-realistic-evaluation-of-deep-semi-supervised-learning-algorithms.pdf",
    "metadata": {
        "title": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms",
        "author": "Avital Oliver, Augustus Odena, Colin A. Raffel, Ekin Dogus Cubuk, Ian Goodfellow",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7585-realistic-evaluation-of-deep-semi-supervised-learning-algorithms.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that SSL algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and performance can degrade substantially when the unlabeled dataset contains out-ofdistribution examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.2"
    },
    "keywords": [
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "semi supervised learning",
            "url": "https://en.wikipedia.org/wiki/semi_supervised_learning"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "It has repeatedly been shown that deep neural networks can achieve humanor super-human-level performance on certain supervised learning problems by leveraging large collections of labeled data",
        "supervised learning algorithms generally provide a way of learning about the structure of the data from the unlabeled examples, alleviating the need for labels",
        "These recent successes raise a natural question: Are supervised learning approaches applicable in \u201creal-world\u201d settings? In this paper, we argue that this de facto way of evaluating supervised learning techniques does not address this question in a satisfying way",
        "We focus on the class of methods which solely involve adding an additional loss term to the training of a neural network, and otherwise leave the training and model unchanged from what would be used in the fully-supervised setting",
        "Many supervised learning techniques are tested only in the core settings we have studied so far, namely CIFAR-10 with 4,000 labels and SVHN with 1,000 labels",
        "We showed that the supervised learning techniques we studied all suffered when the unlabeled data came from different classes than the labeled data \u2014 a realistic scenario that to our knowledge is drastically understudied"
    ],
    "key_statements": [
        "It has repeatedly been shown that deep neural networks can achieve humanor super-human-level performance on certain supervised learning problems by leveraging large collections of labeled data",
        "These successes come at a cost: Creating these large datasets typically requires a great deal of human effort, pain and/or risk or financial expense",
        "For many practical problems and applications, we lack the resources to create a sufficiently large labeled dataset, which limits the wide-spread adoption of deep learning techniques",
        "supervised learning algorithms generally provide a way of learning about the structure of the data from the unlabeled examples, alleviating the need for labels",
        "Some recent results [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] have shown that in certain cases, supervised learning approaches the performance of purely supervised learning, even when a substantial portion of the labels in a given dataset has been discarded",
        "These recent successes raise a natural question: Are supervised learning approaches applicable in \u201creal-world\u201d settings? In this paper, we argue that this de facto way of evaluating supervised learning techniques does not address this question in a satisfying way",
        "Performance of supervised learning techniques can degrade drastically when the unlabeled data contains a different distribution of classes than the labeled data",
        "We focus on the class of methods which solely involve adding an additional loss term to the training of a neural network, and otherwise leave the training and model unchanged from what would be used in the fully-supervised setting",
        "We optimized hyperparameters to minimize classification error on the standard validation set from each dataset, as is common practice",
        "We find the gap between the fully-supervised baseline and those obtained with supervised learning is smaller in our study than what is generally reported in the literature",
        "[<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] report a fully-supervised baseline error rate of 34.85% on CIFAR-10 with 4000 labels which is improved to 12.36% using supervised learning; our improvement for the same approach went from 20.26% to 16.37%",
        "Further to the point of item P.2, we studied the technique of transfer learning using a pre-trained classifier, which is frequently used in limited-data settings but often neglected in supervised learning studies",
        "We examine the case where labeled and unlabeled data come from the same underlying distribution, but the unlabeled data contains classes not present in the labeled data",
        "Many supervised learning techniques are tested only in the core settings we have studied so far, namely CIFAR-10 with 4,000 labels and SVHN with 1,000 labels",
        "We ran experiments on both SVHN and CIFAR with different labeled data amounts; the results are shown in fig.\n4",
        "We evaluated the performance of each supervised learning technique on SVHN with 1,000 labels and varying amounts of unlabeled data from SVHN-extra, which resulted in the test errors shown in fig.\n3",
        "If we want to be 95% confident that our estimate of the validation error differs by less than 1% absolute of the true value, we would need nearly 20,000 validation examples. This is a disheartening estimate due to the fact that the difference in test error achieved by different supervised learning algorithms reported in table 1 is often close to or smaller than 1%, but 20,000 is many times more samples than are provided in the training sets",
        "In order to account for this objection, in fig. 6 we show the mean and standard deviation of the difference in validation error between each supervised learning model and \u21e7-model",
        "We showed that the supervised learning techniques we studied all suffered when the unlabeled data came from different classes than the labeled data \u2014 a realistic scenario that to our knowledge is drastically understudied"
    ],
    "summary": [
        "It has repeatedly been shown that deep neural networks can achieve humanor super-human-level performance on certain supervised learning problems by leveraging large collections of labeled data.",
        "Some recent results [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>] have shown that in certain cases, SSL approaches the performance of purely supervised learning, even when a substantial portion of the labels in a given dataset has been discarded.",
        "Performance of SSL techniques can degrade drastically when the unlabeled data contains a different distribution of classes than the labeled data.",
        "Many papers that evaluate SSL methods on SVHN use only 1,000 labels from the training dataset but retain the full validation set.",
        "Because our model architecture and training hyperparameters differ from those used to test SSL methods in the past, our results are not directly comparable to past work and should be considered in isolation.",
        "[<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] report a fully-supervised baseline error rate of 34.85% on CIFAR-10 with 4000 labels which is improved to 12.36% using SSL; our improvement for the same approach went from 20.26% to 16.37%.",
        "Further to the point of item P.2, we studied the technique of transfer learning using a pre-trained classifier, which is frequently used in limited-data settings but often neglected in SSL studies.",
        "We ran experiments on both SVHN and CIFAR with different labeled data amounts; the results are shown in fig.",
        "We evaluated the performance of each SSL technique on SVHN with 1,000 labels and varying amounts of unlabeled data from SVHN-extra, which resulted in the test errors shown in fig.",
        "This is a disheartening estimate due to the fact that the difference in test error achieved by different SSL algorithms reported in table 1 is often close to or smaller than 1%, but 20,000 is many times more samples than are provided in the training sets.",
        "To measure this phenomenon empirically, we took baseline models trained with each SSL approach on SVHN with 1,000 labels and evaluated them on validation sets with varying sizes.",
        "This suggests that SSL methods which rely on heavy hyperparameter tuning on a large validation set may have limited real-world applicability.",
        "We showed that the SSL techniques we studied all suffered when the unlabeled data came from different classes than the labeled data \u2014 a realistic scenario that to our knowledge is drastically understudied.",
        "A SSL method which requires significant tuning on a per-model or per-task basis in order to perform well will not be useable when validation sets are realistically small.",
        "When the labeled dataset is large enough to accurately estimate validation accuracy, which is necessary when doing model selection and tuning hyperparameters.",
        "The goal of SSL should be to significantly outperform the fully-supervised settings"
    ],
    "headline": "We argue that these benchmarks fail to address many issues that supervised learning algorithms would face in real-world applications",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Bachman, Philip, Alsharif, Ouais, and Precup, Doina. Learning with pseudo-ensembles. In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bachman%2C%20Philip%20Alsharif%2C%20Ouais%20Precup%2C%20Doina%20Learning%20with%20pseudo-ensembles%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bachman%2C%20Philip%20Alsharif%2C%20Ouais%20Precup%2C%20Doina%20Learning%20with%20pseudo-ensembles%202014"
        },
        {
            "id": "2",
            "entry": "[2] Belkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Laplacian%20eigenmaps%20and%20spectral%20techniques%20for%20embedding%20and%20clustering%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Laplacian%20eigenmaps%20and%20spectral%20techniques%20for%20embedding%20and%20clustering%202002"
        },
        {
            "id": "3",
            "entry": "[3] Belkin, Mikhail, Niyogi, Partha, and Sindhwani, Vikas. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research, 7(Nov):2399\u20132434, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Sindhwani%2C%20Vikas%20Manifold%20regularization%3A%20A%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20Mikhail%20Niyogi%2C%20Partha%20Sindhwani%2C%20Vikas%20Manifold%20regularization%3A%20A%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006"
        },
        {
            "id": "4",
            "entry": "[4] Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira, Fernando, and Vaughan, Jennifer Wortman. A theory of learning from different domains. Machine learning Journal, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20A%20theory%20of%20learning%20from%20different%20domains%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-David%2C%20Shai%20Blitzer%2C%20John%20Crammer%2C%20Koby%20Kulesza%2C%20Alex%20A%20theory%20of%20learning%20from%20different%20domains%202010"
        },
        {
            "id": "5",
            "entry": "[5] Bengio, Yoshua, Delalleau, Olivier, and Le Roux, Nicolas. Label Propagation and Quadratic Criterion, chapter 11. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Delalleau%2C%20Olivier%20Roux%2C%20Le%20Nicolas%20Label%20Propagation%20and%20Quadratic%20Criterion%2C%20chapter%2011%202006"
        },
        {
            "id": "6",
            "entry": "[6] Chapelle, Olivier, Scholkopf, Bernhard, and Zien, Alexander. Semi-Supervised Learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Scholkopf%2C%20Bernhard%20Zien%2C%20Alexander%20Semi-Supervised%20Learning%202006"
        },
        {
            "id": "7",
            "entry": "[7] Chrabaszcz, Patryk, Loshchilov, Ilya, and Hutter, Frank. A downsampled variant of ImageNet as an alternative to the CIFAR datasets. arXiv preprint arXiv:1707.08819, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08819"
        },
        {
            "id": "8",
            "entry": "[8] Coates, Adam and Ng, Andrew Y. The importance of encoding versus training with sparse coding and vector quantization. In International Conference on Machine Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Ng%2C%20Andrew%20Y.%20The%20importance%20of%20encoding%20versus%20training%20with%20sparse%20coding%20and%20vector%20quantization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Ng%2C%20Andrew%20Y.%20The%20importance%20of%20encoding%20versus%20training%20with%20sparse%20coding%20and%20vector%20quantization%202011"
        },
        {
            "id": "9",
            "entry": "[9] Dai, Andrew M. and Le, Quoc V. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Andrew%20M.%20Le%2C%20Quoc%20V.%20Semi-supervised%20sequence%20learning%202015"
        },
        {
            "id": "10",
            "entry": "[10] Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. ImageNet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "11",
            "entry": "[11] DeVries, Terrance and Taylor, Graham W. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "12",
            "entry": "[12] Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20Jeff%20Jia%2C%20Yangqing%20Vinyals%2C%20Oriol%20Hoffman%2C%20Judy%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014"
        },
        {
            "id": "13",
            "entry": "[13] Fedus, William, Rosca, Mihaela, Lakshminarayanan, Balaji, Dai, Andrew M., Mohamed, Shakir, and Goodfellow, Ian. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fedus%2C%20William%20Rosca%2C%20Mihaela%20Lakshminarayanan%2C%20Balaji%20Dai%2C%20Andrew%20M.%20Many%20paths%20to%20equilibrium%3A%20GANs%20do%20not%20need%20to%20decrease%20a%20divergence%20at%20every%20step%202018"
        },
        {
            "id": "14",
            "entry": "[14] Forster, Dennis, Sheikh, Abdul-Saboor, and L\u00fccke, J\u00f6rg. Neural simpletrons: Learning in the limit of few labels with directed generative networks. Neural computation, (Early Access): 1\u201362.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Forster%2C%20Dennis%20Sheikh%2C%20Abdul-Saboor%20L%C3%BCcke%2C%20J%C3%B6rg%20Neural%20simpletrons%3A%20Learning%20in%20the%20limit%20of%20few%20labels%20with%20directed%20generative%20networks.%20Neural%20computation"
        },
        {
            "id": "15",
            "entry": "[15] Gammerman, Alexander, Vovk, Volodya, and Vapnik, Vladimir. Learning by transduction. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gammerman%2C%20Alexander%20Vovk%2C%20Volodya%20Vapnik%2C%20Vladimir%20Learning%20by%20transduction%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gammerman%2C%20Alexander%20Vovk%2C%20Volodya%20Vapnik%2C%20Vladimir%20Learning%20by%20transduction%201998"
        },
        {
            "id": "16",
            "entry": "[16] Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Germain, Pascal, Larochelle, Hugo, Laviolette, Fran\u00e7ois, Marchand, Mario, and Lempitsky, Victor. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] Gastaldi, Xavier. Shake-shake regularization. Fifth International Conference on Learning Representations (Workshop Track), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gastaldi%2C%20Xavier%20Shake-shake%20regularization.%20Fifth%20International%20Conference%20on%20Learning%20Representations%202017"
        },
        {
            "id": "18",
            "entry": "[18] Golovin, Daniel, Solnik, Benjamin, Moitra, Subhodeep, Kochanski, Greg, Karro, John, and Sculley, D. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golovin%2C%20Daniel%20Solnik%2C%20Benjamin%20Moitra%2C%20Subhodeep%20Kochanski%2C%20Greg%20Google%20vizier%3A%20A%20service%20for%20black-box%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golovin%2C%20Daniel%20Solnik%2C%20Benjamin%20Moitra%2C%20Subhodeep%20Kochanski%2C%20Greg%20Google%20vizier%3A%20A%20service%20for%20black-box%20optimization%202017"
        },
        {
            "id": "19",
            "entry": "[19] Goodfellow, Ian J., Courville, Aaron, and Bengio, Yoshua. Spike-and-slab sparse coding for unsupervised feature discovery. In NIPS Workshop on Challenges in Learning Hierarchical Models, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Spike-and-slab%20sparse%20coding%20for%20unsupervised%20feature%20discovery%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Courville%2C%20Aaron%20Bengio%2C%20Yoshua%20Spike-and-slab%20sparse%20coding%20for%20unsupervised%20feature%20discovery%202011"
        },
        {
            "id": "20",
            "entry": "[20] Goodfellow, Ian J., Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial examples. In Third International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "21",
            "entry": "[21] Grandvalet, Yves and Bengio, Yoshua. Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grandvalet%2C%20Yves%20Bengio%2C%20Yoshua%20Semi-supervised%20learning%20by%20entropy%20minimization%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grandvalet%2C%20Yves%20Bengio%2C%20Yoshua%20Semi-supervised%20learning%20by%20entropy%20minimization%202005"
        },
        {
            "id": "22",
            "entry": "[22] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks%202016"
        },
        {
            "id": "23",
            "entry": "[23] Henderson, Peter, Islam, Riashat, Bachman, Philip, Pineau, Joelle, Precup, Doina, and Meger, David. Deep reinforcement learning that matters. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henderson%2C%20Peter%20Islam%2C%20Riashat%20Bachman%2C%20Philip%20Pineau%2C%20Joelle%20Deep%20reinforcement%20learning%20that%20matters%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henderson%2C%20Peter%20Islam%2C%20Riashat%20Bachman%2C%20Philip%20Pineau%2C%20Joelle%20Deep%20reinforcement%20learning%20that%20matters%202018"
        },
        {
            "id": "24",
            "entry": "[24] Hoeffding, Wassily. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 58(301):13\u201330, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoeffding%2C%20Wassily%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoeffding%2C%20Wassily%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963"
        },
        {
            "id": "25",
            "entry": "[25] Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "26",
            "entry": "[26] Joachims, Thorsten. Transductive inference for text classification using support vector machines. In International Conference on Machine Learning, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joachims%2C%20Thorsten%20Transductive%20inference%20for%20text%20classification%20using%20support%20vector%20machines%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joachims%2C%20Thorsten%20Transductive%20inference%20for%20text%20classification%20using%20support%20vector%20machines%201999"
        },
        {
            "id": "27",
            "entry": "[27] Joachims, Thorsten. Transductive learning via spectral graph partitioning. In International Conference on Machine Learning, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joachims%2C%20Thorsten%20Transductive%20learning%20via%20spectral%20graph%20partitioning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joachims%2C%20Thorsten%20Transductive%20learning%20via%20spectral%20graph%20partitioning%202003"
        },
        {
            "id": "28",
            "entry": "[28] Ke, Rosemary Nan, Goyal, Anirudh, Lamb, Alex, Pineau, Joelle, Bengio, Samy, and Bengio, Yoshua (eds.). Reproducibility in Machine Learning Research, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ke%20Rosemary%20Nan%20Goyal%20Anirudh%20Lamb%20Alex%20Pineau%20Joelle%20Bengio%20Samy%20and%20Bengio%20Yoshua%20eds%20Reproducibility%20in%20Machine%20Learning%20Research%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ke%20Rosemary%20Nan%20Goyal%20Anirudh%20Lamb%20Alex%20Pineau%20Joelle%20Bengio%20Samy%20and%20Bengio%20Yoshua%20eds%20Reproducibility%20in%20Machine%20Learning%20Research%202017"
        },
        {
            "id": "29",
            "entry": "[29] Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. In Second International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "30",
            "entry": "[30] Kingma, Diederik P., Mohamed, Shakir, Rezende, Danilo Jimenez, and Welling, Max. Semisupervised learning with deep generative models. In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semisupervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semisupervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "31",
            "entry": "[31] Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "32",
            "entry": "[32] Laine, Samuli and Aila, Timo. Temporal ensembling for semi-supervised learning. In Fifth International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20Samuli%20Aila%2C%20Timo%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20Samuli%20Aila%2C%20Timo%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "33",
            "entry": "[33] Lasserre, Julia A., Bishop, Christopher M., and Minka, Thomas P. Principled hybrids of generative and discriminative models. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lasserre%2C%20Julia%20A.%20Bishop%2C%20Christopher%20M.%20Minka%2C%20Thomas%20P.%20Principled%20hybrids%20of%20generative%20and%20discriminative%20models%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lasserre%2C%20Julia%20A.%20Bishop%2C%20Christopher%20M.%20Minka%2C%20Thomas%20P.%20Principled%20hybrids%20of%20generative%20and%20discriminative%20models%202006"
        },
        {
            "id": "34",
            "entry": "[34] Lee, Dong-Hyun. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In ICML Workshop on Challenges in Representation Learning, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Dong-Hyun%20Pseudo-label%3A%20The%20simple%20and%20efficient%20semi-supervised%20learning%20method%20for%20deep%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Dong-Hyun%20Pseudo-label%3A%20The%20simple%20and%20efficient%20semi-supervised%20learning%20method%20for%20deep%20neural%20networks%202013"
        },
        {
            "id": "35",
            "entry": "[35] Lucic, Mario, Kurach, Karol, Michalski, Marcin, Gelly, Sylvain, and Bousquet, Olivier. Are GANs created equal? A large-scale study. arXiv preprint arXiv:1711.10337, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10337"
        },
        {
            "id": "36",
            "entry": "[36] Maas, Andrew L., Hannun, Awni Y., and Ng, Andrew Y. Rectifier nonlinearities improve neural network acoustic models. In International Conference on Machine Learning, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Hannun%2C%20Awni%20Y.%20Ng%2C%20Andrew%20Y.%20Rectifier%20nonlinearities%20improve%20neural%20network%20acoustic%20models%202013"
        },
        {
            "id": "37",
            "entry": "[37] McLachlan, Geoffrey J. Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis. Journal of the American Statistical Association, 70(350):365\u2013369, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McLachlan%2C%20Geoffrey%20J.%20Iterative%20reclassification%20procedure%20for%20constructing%20an%20asymptotically%20optimal%20rule%20of%20allocation%20in%20discriminant%20analysis%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McLachlan%2C%20Geoffrey%20J.%20Iterative%20reclassification%20procedure%20for%20constructing%20an%20asymptotically%20optimal%20rule%20of%20allocation%20in%20discriminant%20analysis%201975"
        },
        {
            "id": "38",
            "entry": "[38] Melis, G\u00e1bor, Dyer, Chris, and Blunsom, Phil. On the state of the art of evaluation in neural language models. In Proceedings of the 6th International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20state%20of%20the%20art%20of%20evaluation%20in%20neural%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20state%20of%20the%20art%20of%20evaluation%20in%20neural%20language%20models%202018"
        },
        {
            "id": "39",
            "entry": "[39] Miyato, Takeru, Maeda, Shin-ichi, Koyama, Masanori, and Ishii, Shin. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03976"
        },
        {
            "id": "40",
            "entry": "[40] Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "41",
            "entry": "[41] Odena, Augustus. Semi-supervised learning with generative adversarial networks. arXiv preprint arXiv:1606.01583, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01583"
        },
        {
            "id": "42",
            "entry": "[42] Pereyra, Gabriel, Tucker, George, Chorowski, Jan, Kaiser, \u0141ukasz, and Hinton, Geoffrey. Regularizing neural networks by penalizing confident output distributions. In Fifth International Conference on Learning Representations (Workshop Track), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pereyra%2C%20Gabriel%20Tucker%2C%20George%20Chorowski%2C%20Jan%20Kaiser%2C%20%C5%81ukasz%20Regularizing%20neural%20networks%20by%20penalizing%20confident%20output%20distributions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pereyra%2C%20Gabriel%20Tucker%2C%20George%20Chorowski%2C%20Jan%20Kaiser%2C%20%C5%81ukasz%20Regularizing%20neural%20networks%20by%20penalizing%20confident%20output%20distributions%202017"
        },
        {
            "id": "43",
            "entry": "[43] Pu, Yunchen, Gan, Zhe, Henao, Ricardo, Yuan, Xin, Li, Chunyuan, Stevens, Andrew, and Carin, Lawrence. Variational autoencoder for deep learning of images, labels and captions. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pu%2C%20Yunchen%20Gan%2C%20Zhe%20Henao%2C%20Ricardo%20Yuan%2C%20Xin%20Variational%20autoencoder%20for%20deep%20learning%20of%20images%2C%20labels%20and%20captions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pu%2C%20Yunchen%20Gan%2C%20Zhe%20Henao%2C%20Ricardo%20Yuan%2C%20Xin%20Variational%20autoencoder%20for%20deep%20learning%20of%20images%2C%20labels%20and%20captions%202016"
        },
        {
            "id": "44",
            "entry": "[44] Rosenberg, Chuck, Hebert, Martial, and Schneiderman, Henry. Semi-supervised self-training of object detection models. In Proceedings of the Seventh IEEE Workshops on Application of Computer Vision, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenberg%2C%20Chuck%20Hebert%2C%20Martial%20Schneiderman%2C%20Henry%20Semi-supervised%20self-training%20of%20object%20detection%20models%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosenberg%2C%20Chuck%20Hebert%2C%20Martial%20Schneiderman%2C%20Henry%20Semi-supervised%20self-training%20of%20object%20detection%20models%202005"
        },
        {
            "id": "45",
            "entry": "[45] Sajjadi, Mehdi, Javanmardi, Mehran, and Tasdizen, Tolga. Mutual exclusivity loss for semisupervised deep learning. In IEEE International Conference on Image Processing, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Mutual%20exclusivity%20loss%20for%20semisupervised%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Mutual%20exclusivity%20loss%20for%20semisupervised%20deep%20learning%202016"
        },
        {
            "id": "46",
            "entry": "[46] Sajjadi, Mehdi, Javanmardi, Mehran, and Tasdizen, Tolga. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Regularization%20with%20stochastic%20transformations%20and%20perturbations%20for%20deep%20semi-supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sajjadi%2C%20Mehdi%20Javanmardi%2C%20Mehran%20Tasdizen%2C%20Tolga%20Regularization%20with%20stochastic%20transformations%20and%20perturbations%20for%20deep%20semi-supervised%20learning%202016"
        },
        {
            "id": "47",
            "entry": "[47] Salakhutdinov, Ruslan and Hinton, Geoffrey E. Using deep belief nets to learn covariance kernels for Gaussian processes. In Advances in Neural Information Processing Systems, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20Ruslan%20Hinton%2C%20Geoffrey%20E.%20Using%20deep%20belief%20nets%20to%20learn%20covariance%20kernels%20for%20Gaussian%20processes%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20Ruslan%20Hinton%2C%20Geoffrey%20E.%20Using%20deep%20belief%20nets%20to%20learn%20covariance%20kernels%20for%20Gaussian%20processes%202007"
        },
        {
            "id": "48",
            "entry": "[48] Salimans, Tim, Goodfellow, Ian, Zaremba, Wojciech, Cheung, Vicki, Radford, Alec, and Chen, Xi. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20GANs%202016"
        },
        {
            "id": "49",
            "entry": "[49] Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fergus, Rob. Intriguing properties of neural networks. In Second International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Zaremba%2C%20Wojciech%20Sutskever%2C%20Ilya%20Bruna%2C%20Joan%20Intriguing%20properties%20of%20neural%20networks%202014"
        },
        {
            "id": "50",
            "entry": "[50] Tarvainen, Antti and Valpola, Harri. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tarvainen%2C%20Antti%20Valpola%2C%20Harri%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tarvainen%2C%20Antti%20Valpola%2C%20Harri%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017"
        },
        {
            "id": "51",
            "entry": "[51] Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "52",
            "entry": "[52] Zagoruyko, Sergey and Komodakis, Nikos. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016"
        },
        {
            "id": "53",
            "entry": "[53] Zhu, Xiaojin, Ghahramani, Zoubin, and Lafferty, John D. Semi-supervised learning using gaussian fields and harmonic functions. In International Conference on Machine Learning, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003"
        }
    ]
}
