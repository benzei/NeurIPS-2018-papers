{
    "filename": "7687-optimization-of-smooth-functions-with-noisy-observations-local-minimax-rates.pdf",
    "metadata": {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates",
        "author": "Yining Wang, Sivaraman Balakrishnan, Aarti Singh",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7687-optimization-of-smooth-functions-with-noisy-observations-local-minimax-rates.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider the problem of global optimization of an unknown non-convex smooth function with noisy zeroth-order feedback. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations. We show that for functions with fast growth around their global minima, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries than worst-case global minimax theory predicts. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems. On the other hand, we show that in the worst case no algorithm can converge faster than the minimax rate of estimating an unknown functions in 8-norm. Finally, we show that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate."
    },
    "keywords": [
        {
            "term": "global optimization",
            "url": "https://en.wikipedia.org/wiki/global_optimization"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "smooth function",
            "url": "https://en.wikipedia.org/wiki/smooth_function"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        }
    ],
    "highlights": [
        "We first review standard asymptotic notation that will be used throughout this paper",
        "Optimization appears to be easier than global reconstruction, we show that the n\u03b1{p2\u03b1dq rate is not improvable in the global minimax sense in Eq (3) over Holder classes",
        "If active queries are not available and x1, . . . , xn are i.i.d. uniformly sampled from X , the n\u03b1{p2\u03b1dq global minimax rate applies locally regardless of how large \u03b2 is",
        "We prove local minimax lower bounds that match the upper bounds in Theorem 1 up to logarithmic terms",
        "Our results imply an n\u03b1{p2\u03b1dq minimax lower bound over all \u03b1-Holder smooth functions, showing that without additional assumptions, noisy optimization of smooth functions is as difficult as reconstructing the unknown function in sup-norm",
        "Matching lower and upper bounds on the local minimax convergence rates are established, which are significantly different from classical minimax rates in nonparametric regression problems"
    ],
    "key_statements": [
        "We first review standard asymptotic notation that will be used throughout this paper",
        "Optimization appears to be easier than global reconstruction, we show that the n\u03b1{p2\u03b1dq rate is not improvable in the global minimax sense in Eq (3) over Holder classes",
        "If active queries are not available and x1, . . . , xn are i.i.d. uniformly sampled from X , the n\u03b1{p2\u03b1dq global minimax rate applies locally regardless of how large \u03b2 is",
        "We prove local minimax lower bounds that match the upper bounds in Theorem 1 up to logarithmic terms",
        "Our results imply an n\u03b1{p2\u03b1dq minimax lower bound over all \u03b1-Holder smooth functions, showing that without additional assumptions, noisy optimization of smooth functions is as difficult as reconstructing the unknown function in sup-norm",
        "Matching lower and upper bounds on the local minimax convergence rates are established, which are significantly different from classical minimax rates in nonparametric regression problems"
    ],
    "summary": [
        "We first review standard asymptotic notation that will be used throughout this paper.",
        "We design an iterative algorithm whose optimization error Lpxpn; f q converges at a rate of Rnpf0q depending on the reference function f0.",
        "The minimax convergence rate of Lpxpn; f q is characterized locally by Rnpf0q which depends on the reference function f0.",
        "In contrast to the upper bound formulation, we assume the potential active optimization estimator xpn has perfect knowledge about the reference function f0 P \u0398.",
        "Xpn f P\u03981,}ff0}8\u010f\u03b5npf0q f where C2 \u0105 0 is another positive constant and \u03b5npf0q, Rnpf0q are desired local convergence rates for functions near the reference f0.",
        "The following theorem is our main result that upper bounds the local minimax rate of noisy global optimization with active queries.",
        "If f0 is strongly smooth and convex as in Example 2, Theorem 1 suggests that Rnpf0q \u2014 n1{2, which is significantly better than the n2{p4`dq baseline rate 5 and matches existing works on zeroth-order optimization of convex functions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "An active query algorithm could focus most of its queries onto the small level sets of the underlying function, resulting in more accurate local function reconstructions and faster optimization error rate.",
        "Our proof of Theorem 1 is constructive, by upper bounding the local minimax optimization error of an explicit algorithm.",
        "To facilitate such a strong local minimax lower bounds, the following additional condition is imposed on the reference function f0 of which the data analyst has perfect information.",
        "Such information-theoretical lower bounds on the convergence rates hold even if the data analyst has perfect information of f0, the reference function on which the n\u03b1{p2\u03b1d\u03b1\u03b2q local rate is based.",
        "Our results imply an n\u03b1{p2\u03b1dq minimax lower bound over all \u03b1-Holder smooth functions, showing that without additional assumptions, noisy optimization of smooth functions is as difficult as reconstructing the unknown function in sup-norm.",
        "The following theorem, on the other hand, shows that for passive algorithms (Definition 1) the n\u03b1{p2\u03b1dq optimization rate is not improvable even with additional level set assumptions imposed on f0.",
        "Matching lower and upper bounds on the local minimax convergence rates are established, which are significantly different from classical minimax rates in nonparametric regression problems.",
        "Many interesting future directions exist along this line of research, including exploitation of additive structures in the underlying function f to completely remove curse of dimensionality, functions with spatially heterogeneous smoothness or level set growth behaviors, and to design more computationally efficient algorithms that work well in practice."
    ],
    "headline": "We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the annual Conference on Learning Theory (COLT), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Dekel%2C%20O.%20Xiao%2C%20L.%20Optimal%20algorithms%20for%20online%20convex%20optimization%20with%20multi-point%20bandit%20feedback%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Dekel%2C%20O.%20Xiao%2C%20L.%20Optimal%20algorithms%20for%20online%20convex%20optimization%20with%20multi-point%20bandit%20feedback%202010"
        },
        {
            "id": "2",
            "entry": "[2] A. Agarwal, D. Foster, D. Hsu, S. Kakade, and A. Rakhlin. Stochastic convex optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213\u2013240, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Foster%2C%20D.%20Hsu%2C%20D.%20Kakade%2C%20S.%20Stochastic%20convex%20optimization%20with%20bandit%20feedback%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Foster%2C%20D.%20Hsu%2C%20D.%20Kakade%2C%20S.%20Stochastic%20convex%20optimization%20with%20bandit%20feedback%202013"
        },
        {
            "id": "3",
            "entry": "[3] N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and T. Ma. Finding approximate local minima faster than gradient descent. In Proceedings of the Annual ACM SIGACT Symposium on Theory of Computing (STOC), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017"
        },
        {
            "id": "4",
            "entry": "[4] S. Balakrishnan, S. Narayanan, A. Rinaldo, A. Singh, and L. Wasserman. Cluster trees on manifolds. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balakrishnan%2C%20S.%20Narayanan%2C%20S.%20Rinaldo%2C%20A.%20Singh%2C%20A.%20Cluster%20trees%20on%20manifolds%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balakrishnan%2C%20S.%20Narayanan%2C%20S.%20Rinaldo%2C%20A.%20Singh%2C%20A.%20Cluster%20trees%20on%20manifolds%202013"
        },
        {
            "id": "5",
            "entry": "[5] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. Journal of Computer and System Sciences, 75(1):78\u201389, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.-F.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.-F.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202009"
        },
        {
            "id": "6",
            "entry": "[6] S. Bubeck, R. Eldan, and Y. T. Lee. Kernel-based methods for bandit convex optimization. In Proceedings of the annual ACM SIGACT Symposium on Theory of Computing (STOC), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Eldan%2C%20R.%20Lee%2C%20Y.T.%20Kernel-based%20methods%20for%20bandit%20convex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Eldan%2C%20R.%20Lee%2C%20Y.T.%20Kernel-based%20methods%20for%20bandit%20convex%20optimization%202017"
        },
        {
            "id": "7",
            "entry": "[7] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings of the International conference on Algorithmic learning theory (ALT), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Munos%2C%20R.%20Stoltz%2C%20G.%20Pure%20exploration%20in%20multi-armed%20bandits%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Munos%2C%20R.%20Stoltz%2C%20G.%20Pure%20exploration%20in%20multi-armed%20bandits%20problems%202009"
        },
        {
            "id": "8",
            "entry": "[8] S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvari. X-armed bandits. Journal of Machine Learning Research, 12(May):1655\u20131695, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Munos%2C%20R.%20Stoltz%2C%20G.%20Szepesvari%2C%20C.%20X-armed%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Munos%2C%20R.%20Stoltz%2C%20G.%20Szepesvari%2C%20C.%20X-armed%20bandits%202011"
        },
        {
            "id": "9",
            "entry": "[9] A. D. Bull. Convergence rates of efficient global optimization algorithms. Journal of Machine Learning Research, 12(Oct):2879\u20132904, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bull%2C%20A.D.%20Convergence%20rates%20of%20efficient%20global%20optimization%20algorithms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bull%2C%20A.D.%20Convergence%20rates%20of%20efficient%20global%20optimization%20algorithms%202011"
        },
        {
            "id": "10",
            "entry": "[10] Y. Carmon, O. Hinder, J. C. Duchi, and A. Sidford. \u201cconvex until proven guilty\u201d: Dimension-free acceleration of gradient descent on non-convex functions. arXiv preprint arXiv:1705.02766, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02766"
        },
        {
            "id": "11",
            "entry": "[11] R. M. Castro and R. D. Nowak. Minimax bounds for active learning. IEEE Transactions on Information Theory, 54(5):2339\u20132353, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Castro%2C%20R.M.%20Nowak%2C%20R.D.%20Minimax%20bounds%20for%20active%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Castro%2C%20R.M.%20Nowak%2C%20R.D.%20Minimax%20bounds%20for%20active%20learning%202008"
        },
        {
            "id": "12",
            "entry": "[12] K. Chaudhuri, S. Dasgupta, S. Kpotufe, and U. von Luxburg. Consistent procedures for cluster tree estimation and pruning. IEEE Transactions on Information Theory, 60(12):7900\u20137912, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20K.%20Dasgupta%2C%20S.%20Kpotufe%2C%20S.%20von%20Luxburg%2C%20U.%20Consistent%20procedures%20for%20cluster%20tree%20estimation%20and%20pruning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20K.%20Dasgupta%2C%20S.%20Kpotufe%2C%20S.%20von%20Luxburg%2C%20U.%20Consistent%20procedures%20for%20cluster%20tree%20estimation%20and%20pruning%202014"
        },
        {
            "id": "13",
            "entry": "[13] H. Chen. Lower rate of convergence for locating a maximum of a function. The Annals of Statistics, 16(3):1330\u20131334, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20H.%20Lower%20rate%20of%20convergence%20for%20locating%20a%20maximum%20of%20a%20function%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20H.%20Lower%20rate%20of%20convergence%20for%20locating%20a%20maximum%20of%20a%20function%201988"
        },
        {
            "id": "14",
            "entry": "[14] S. Dasgupta, D. J. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In Proceedings of Advances in neural information processing systems (NIPS), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20S.%20Hsu%2C%20D.J.%20Monteleoni%2C%20C.%20A%20general%20agnostic%20active%20learning%20algorithm%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20S.%20Hsu%2C%20D.J.%20Monteleoni%2C%20C.%20A%20general%20agnostic%20active%20learning%20algorithm%202008"
        },
        {
            "id": "15",
            "entry": "[15] J. Duchi and F. Ruan. Local asymptotics for some stochastic optimization problems: Optimality, constraint identification, and dual averaging. arXiv preprint arXiv:1612.05612, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.05612"
        },
        {
            "id": "16",
            "entry": "[16] J. C. Duchi, J. Lafferty, and Y. Zhu. Local minimax complexity of stochastic convex optimization. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.C.%20Lafferty%2C%20J.%20Zhu%2C%20Y.%20Local%20minimax%20complexity%20of%20stochastic%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.C.%20Lafferty%2C%20J.%20Zhu%2C%20Y.%20Local%20minimax%20complexity%20of%20stochastic%20convex%20optimization%202016"
        },
        {
            "id": "17",
            "entry": "[17] J. Fan and I. Gijbels. Local polynomial modelling and its applications. CRC Press, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20J.%20Gijbels%2C%20I.%20Local%20polynomial%20modelling%20and%20its%20applications%201996"
        },
        {
            "id": "18",
            "entry": "[18] A. D. Flaxman, A. T. Kalai, and H. B. McHanan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA), 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flaxman%2C%20A.D.%20Kalai%2C%20A.T.%20McHanan%2C%20H.B.%20Online%20convex%20optimization%20in%20the%20bandit%20setting%3A%20gradient%20descent%20without%20a%20gradient%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flaxman%2C%20A.D.%20Kalai%2C%20A.T.%20McHanan%2C%20H.B.%20Online%20convex%20optimization%20in%20the%20bandit%20setting%3A%20gradient%20descent%20without%20a%20gradient%202005"
        },
        {
            "id": "19",
            "entry": "[19] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. In Proceedings of the annual Conference on Learning Theory (COLT), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20-%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20-%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "20",
            "entry": "[20] J.-B. Grill, M. Valko, and R. Munos. Black-box optimization of noisy functions with unknown smoothness. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grill%2C%20J.-B.%20Valko%2C%20M.%20Munos%2C%20R.%20Black-box%20optimization%20of%20noisy%20functions%20with%20unknown%20smoothness%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grill%2C%20J.-B.%20Valko%2C%20M.%20Munos%2C%20R.%20Black-box%20optimization%20of%20noisy%20functions%20with%20unknown%20smoothness%202015"
        },
        {
            "id": "21",
            "entry": "[21] S. Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the International Conference on Machine Learning (ICML), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanneke%2C%20S.%20A%20bound%20on%20the%20label%20complexity%20of%20agnostic%20active%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanneke%2C%20S.%20A%20bound%20on%20the%20label%20complexity%20of%20agnostic%20active%20learning%202007"
        },
        {
            "id": "22",
            "entry": "[22] E. Hazan, A. Klivans, and Y. Yuan. Hyperparameter optimization: A spectral approach. arXiv preprint arXiv:1706.00764, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00764"
        },
        {
            "id": "23",
            "entry": "[23] E. Hazan, K. Levy, and S. Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Levy%2C%20K.%20Shalev-Shwartz%2C%20S.%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Levy%2C%20K.%20Shalev-Shwartz%2C%20S.%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015"
        },
        {
            "id": "24",
            "entry": "[24] K. G. Jamieson, R. Nowak, and B. Recht. Query complexity of derivative-free optimization. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jamieson%2C%20K.G.%20Nowak%2C%20R.%20Recht%2C%20B.%20Query%20complexity%20of%20derivative-free%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jamieson%2C%20K.G.%20Nowak%2C%20R.%20Recht%2C%20B.%20Query%20complexity%20of%20derivative-free%20optimization%202012"
        },
        {
            "id": "25",
            "entry": "[25] A. R. Kan and G. T. Timmer. Stochastic global optimization methods part I: Clustering methods. Mathematical Programming, 39(1):27\u201356, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kan%2C%20A.R.%20Timmer%2C%20G.T.%20Stochastic%20global%20optimization%20methods%20part%20I%3A%20Clustering%20methods%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kan%2C%20A.R.%20Timmer%2C%20G.T.%20Stochastic%20global%20optimization%20methods%20part%20I%3A%20Clustering%20methods%201987"
        },
        {
            "id": "26",
            "entry": "[26] A. R. Kan and G. T. Timmer. Stochastic global optimization methods part II: Multi level methods. Mathematical Programming, 39(1):57\u201378, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kan%2C%20A.R.%20Timmer%2C%20G.T.%20Stochastic%20global%20optimization%20methods%20part%20II%3A%20Multi%20level%20methods%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kan%2C%20A.R.%20Timmer%2C%20G.T.%20Stochastic%20global%20optimization%20methods%20part%20II%3A%20Multi%20level%20methods%201987"
        },
        {
            "id": "27",
            "entry": "[27] J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. The Annals of Mathematical Statistics, 23(3):462\u2013466, 1952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiefer%2C%20J.%20Wolfowitz%2C%20J.%20Stochastic%20estimation%20of%20the%20maximum%20of%20a%20regression%20function%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiefer%2C%20J.%20Wolfowitz%2C%20J.%20Stochastic%20estimation%20of%20the%20maximum%20of%20a%20regression%20function%201952"
        },
        {
            "id": "28",
            "entry": "[28] R. D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural Information Processing Systems (NIPS), 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleinberg%2C%20R.D.%20Nearly%20tight%20bounds%20for%20the%20continuum-armed%20bandit%20problem%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleinberg%2C%20R.D.%20Nearly%20tight%20bounds%20for%20the%20continuum-armed%20bandit%20problem%202005"
        },
        {
            "id": "29",
            "entry": "[29] A. P. Korostelev and A. B. Tsybakov. Minimax theory of image reconstruction, volume 82. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Korostelev%2C%20A.P.%20Tsybakov%2C%20A.B.%20Minimax%20theory%20of%20image%20reconstruction%2C%20volume%2082%202012"
        },
        {
            "id": "30",
            "entry": "[30] O. V. Lepski, E. Mammen, and V. G. Spokoiny. Optimal spatial adaptation to inhomogeneous smoothness: an approach based on kernel estimates with variable bandwidth selectors. The Annals of Statistics, 25(3):929\u2013947, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lepski%2C%20O.V.%20Mammen%2C%20E.%20Spokoiny%2C%20V.G.%20Optimal%20spatial%20adaptation%20to%20inhomogeneous%20smoothness%3A%20an%20approach%20based%20on%20kernel%20estimates%20with%20variable%20bandwidth%20selectors%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lepski%2C%20O.V.%20Mammen%2C%20E.%20Spokoiny%2C%20V.G.%20Optimal%20spatial%20adaptation%20to%20inhomogeneous%20smoothness%3A%20an%20approach%20based%20on%20kernel%20estimates%20with%20variable%20bandwidth%20selectors%201997"
        },
        {
            "id": "31",
            "entry": "[31] C. Malherbe, E. Contal, and N. Vayatis. A ranking approach to global optimization. In Proceedings of the International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malherbe%2C%20C.%20Contal%2C%20E.%20Vayatis%2C%20N.%20A%20ranking%20approach%20to%20global%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malherbe%2C%20C.%20Contal%2C%20E.%20Vayatis%2C%20N.%20A%20ranking%20approach%20to%20global%20optimization%202016"
        },
        {
            "id": "32",
            "entry": "[32] C. Malherbe and N. Vayatis. Global optimization of lipschitz functions. In Proceedings of the International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malherbe%2C%20C.%20Vayatis%2C%20N.%20Global%20optimization%20of%20lipschitz%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malherbe%2C%20C.%20Vayatis%2C%20N.%20Global%20optimization%20of%20lipschitz%20functions%202017"
        },
        {
            "id": "33",
            "entry": "[33] S. Minsker. Non-asymptotic bounds for prediction problems and density estimation. PhD thesis, Georgia Institute of Technology, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minsker%2C%20S.%20Non-asymptotic%20bounds%20for%20prediction%20problems%20and%20density%20estimation%202012"
        },
        {
            "id": "34",
            "entry": "[34] S. Minsker. Estimation of extreme values and associated level sets of a regression function via selective sampling. In Proceedings of Conferences on Learning Theory (COLT), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minsker%2C%20S.%20Estimation%20of%20extreme%20values%20and%20associated%20level%20sets%20of%20a%20regression%20function%20via%20selective%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minsker%2C%20S.%20Estimation%20of%20extreme%20values%20and%20associated%20level%20sets%20of%20a%20regression%20function%20via%20selective%20sampling%202013"
        },
        {
            "id": "35",
            "entry": "[35] N. Nakamura, J. Seepaul, J. B. Kadane, and B. Reeja-Jayan. Design for low-temperature microwave-assisted crystallization of ceramic thin films. Applied Stochastic Models in Business and Industry, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nakamura%2C%20N.%20Seepaul%2C%20J.%20Kadane%2C%20J.B.%20Reeja-Jayan%2C%20B.%20Design%20for%20low-temperature%20microwave-assisted%20crystallization%20of%20ceramic%20thin%20films%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nakamura%2C%20N.%20Seepaul%2C%20J.%20Kadane%2C%20J.B.%20Reeja-Jayan%2C%20B.%20Design%20for%20low-temperature%20microwave-assisted%20crystallization%20of%20ceramic%20thin%20films%202017"
        },
        {
            "id": "36",
            "entry": "[36] A. Nemirovski and D. Yudin. Problem complexity and method efficiency in optimization. A Wiley-Interscience Publication, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Yudin%2C%20D.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983"
        },
        {
            "id": "37",
            "entry": "[37] Y. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Polyak%2C%20B.T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Polyak%2C%20B.T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "38",
            "entry": "[38] W. Polonik. Measuring mass concentrations and estimating density contour clusters-an excess mass approach. The Annals of Statistics, 23(3):855\u2013881, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polonik%2C%20W.%20Measuring%20mass%20concentrations%20and%20estimating%20density%20contour%20clusters-an%20excess%20mass%20approach%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polonik%2C%20W.%20Measuring%20mass%20concentrations%20and%20estimating%20density%20contour%20clusters-an%20excess%20mass%20approach%201995"
        },
        {
            "id": "39",
            "entry": "[39] E. Purzen. On estimation of a probability density and mode. The Annals of Mathematical Statistics, 33(3):1065\u20131076, 1962.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Purzen%2C%20E.%20On%20estimation%20of%20a%20probability%20density%20and%20mode%201962",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Purzen%2C%20E.%20On%20estimation%20of%20a%20probability%20density%20and%20mode%201962"
        },
        {
            "id": "40",
            "entry": "[40] C. E. Rasmussen and C. K. Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.%20Gaussian%20processes%20for%20machine%20learning%2C%20volume%201%202006"
        },
        {
            "id": "41",
            "entry": "[41] B. Reeja-Jayan, K. L. Harrison, K. Yang, C.-L. Wang, A. Yilmaz, and A. Manthiram. Microwave-assisted low-temperature growth of thin films in solution. Scientific reports, 2, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reeja-Jayan%2C%20B.%20Harrison%2C%20K.L.%20Yang%2C%20K.%20Wang%2C%20C.-L.%20Microwave-assisted%20low-temperature%20growth%20of%20thin%20films%20in%20solution%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reeja-Jayan%2C%20B.%20Harrison%2C%20K.L.%20Yang%2C%20K.%20Wang%2C%20C.-L.%20Microwave-assisted%20low-temperature%20growth%20of%20thin%20films%20in%20solution%202012"
        },
        {
            "id": "42",
            "entry": "[42] P. Rigollet and R. Vert. Optimal rates for plug-in estimators of density level sets. Bernoulli, 15(4):1154\u20131178, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20P.%20Vert%2C%20R.%20Optimal%20rates%20for%20plug-in%20estimators%20of%20density%20level%20sets%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rigollet%2C%20P.%20Vert%2C%20R.%20Optimal%20rates%20for%20plug-in%20estimators%20of%20density%20level%20sets%202009"
        },
        {
            "id": "43",
            "entry": "[43] A. Risteski and Y. Li. Algorithms and matching lower bounds for approximately-convex optimization. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Risteski%2C%20A.%20Li%2C%20Y.%20Algorithms%20and%20matching%20lower%20bounds%20for%20approximately-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Risteski%2C%20A.%20Li%2C%20Y.%20Algorithms%20and%20matching%20lower%20bounds%20for%20approximately-convex%20optimization%202016"
        },
        {
            "id": "44",
            "entry": "[44] J. Scarlett, I. Bogunovic, and V. Cevher. Lower bounds on regret for noisy gaussian process bandit optimization. In Proceedings of the annual Conference on Learning Theory (COLT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarlett%2C%20J.%20Bogunovic%2C%20I.%20Cevher%2C%20V.%20Lower%20bounds%20on%20regret%20for%20noisy%20gaussian%20process%20bandit%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarlett%2C%20J.%20Bogunovic%2C%20I.%20Cevher%2C%20V.%20Lower%20bounds%20on%20regret%20for%20noisy%20gaussian%20process%20bandit%20optimization%202017"
        },
        {
            "id": "45",
            "entry": "[45] A. Singh, C. Scott, and R. Nowak. Adaptive hausdorff estimation of density level sets. The Annals of Statistics, 37(5B):2760\u20132782, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20A.%20Scott%2C%20C.%20Nowak%2C%20R.%20Adaptive%20hausdorff%20estimation%20of%20density%20level%20sets%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20A.%20Scott%2C%20C.%20Nowak%2C%20R.%20Adaptive%20hausdorff%20estimation%20of%20density%20level%20sets%202009"
        },
        {
            "id": "46",
            "entry": "[46] A. B. Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics. Springer, New York, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20A.B.%20Introduction%20to%20nonparametric%20estimation.%20Springer%20Series%20in%20Statistics%202009"
        },
        {
            "id": "47",
            "entry": "[47] A. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20W%20Van%20der%20Vaart%20Asymptotic%20statistics%20volume%203%20Cambridge%20university%20press%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20W%20Van%20der%20Vaart%20Asymptotic%20statistics%20volume%203%20Cambridge%20university%20press%201998"
        },
        {
            "id": "48",
            "entry": "[48] Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient langevin dynamics. In Proceedings of the annual Conference on Learning Theory (COLT), 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Liang%2C%20P.%20Charikar%2C%20M.%20A%20hitting%20time%20analysis%20of%20stochastic%20gradient%20langevin%20dynamics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Liang%2C%20P.%20Charikar%2C%20M.%20A%20hitting%20time%20analysis%20of%20stochastic%20gradient%20langevin%20dynamics%202017"
        }
    ]
}
