{
    "filename": "7825-masking-a-new-perspective-of-noisy-supervision.pdf",
    "metadata": {
        "title": "Masking: A New Perspective of Noisy Supervision",
        "author": "Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, Masashi Sugiyama",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7825-masking-a-new-perspective-of-noisy-supervision.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "It is important to learn various types of classifiers given training data with noisy labels. Noisy labels, in the most popular noise model hitherto, are corrupted from ground-truth labels by an unknown noise transition matrix. Thus, by estimating this matrix, classifiers can escape from overfitting those noisy labels. However, such estimation is practically difficult, due to either the indirect nature of two-step approaches, or not big enough data to afford end-to-end approaches. In this paper, we propose a human-assisted approach called \u201cMasking\u201d that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. To this end, we derive a structure-aware probabilistic model incorporating a structure prior, and solve the challenges from structure extraction and structure alignment. Thanks to Masking, we only estimate unmasked noise transition probabilities and the burden of estimation is tremendously reduced. We conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise structures as well as the industrial-level Clothing1M with agnostic noise structure, and the results show that Masking can improve the robustness of classifiers significantly."
    },
    "keywords": [
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "medical image",
            "url": "https://en.wikipedia.org/wiki/medical_image"
        },
        {
            "term": "social network",
            "url": "https://en.wikipedia.org/wiki/social_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "It is always challenging to learn from noisy labels [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], since these labels are systematically corrupted",
        "Noisy labels inevitably degenerate the accuracy of classifiers",
        "This negative effect becomes more prominent for deep learning, since these complex models can fully memorize noisy labels, which correspondingly degenerates their generalization [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>]",
        "To address the structure extraction challenge, we propose a tempered sigmoid function to simulate the human cognition on the structure of the noise transition matrix",
        "To address the structure alignment challenge, we propose a variant of Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] to avoid the difficulty of specifying the explicit distributions",
        "When the noise structure is wrongly set at the initial stage, how does our model correct the initial structure by learning from the finite dataset?"
    ],
    "key_statements": [
        "It is always challenging to learn from noisy labels [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], since these labels are systematically corrupted",
        "Noisy labels inevitably degenerate the accuracy of classifiers",
        "This negative effect becomes more prominent for deep learning, since these complex models can fully memorize noisy labels, which correspondingly degenerates their generalization [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>]",
        "To address the structure extraction challenge, we propose a tempered sigmoid function to simulate the human cognition on the structure of the noise transition matrix",
        "To address the structure alignment challenge, we propose a variant of Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] to avoid the difficulty of specifying the explicit distributions",
        "When we already know the noise structure from human cognition, we only focus on estimating the noise transition probability",
        "The noise transition matrix s \u223c P (s) and its structure so \u223c P, where P (s) is an implicit distribution modeled by neural networks without the exact form, P",
        "The reconstructor is for the first term in Eq (1), facilitating the classifier prediction P (y|x) and the noise transition matrix s to yield noisy labels y",
        "When the noise structure is wrongly set at the initial stage, how does our model correct the initial structure by learning from the finite dataset?"
    ],
    "summary": [
        "It is always challenging to learn from noisy labels [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], since these labels are systematically corrupted.",
        "Masking conveys human cognition of invalid class transitions, and speculates the structure of the noise transition matrix.",
        "To address the structure extraction challenge, we propose a tempered sigmoid function to simulate the human cognition on the structure of the noise transition matrix.",
        "We discuss where noisy labels normally come from, and why we can speculate the structure of the noise transition matrix.",
        "S <latexit sha1_base64=\"N1eK0lTaQQAFA6yHzcECkl4oWJk=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit>",
        "The noise transition matrix (y \u2192 y) modeled by a nonlinear softmax layer connects the classifier (x \u2192 y) with noisy labels yfor the end-to-end training.",
        "Due to the regularization effect in the optimization [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], the noise transition matrix learned by previous models [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] will satisfy our expectation regarding the structure.",
        "Compared to benchmark models in Figure 2(a), we model the noise transition matrix with a random variable s, and we instill the structure information by controlling the prior of its corresponding structure variable, termed as so.",
        "The noise transition matrix s \u223c P (s) and its structure so \u223c P, where P (s) is an implicit distribution modeled by neural networks without the exact form, P",
        "MASKING model benefits from the human guidance in the procedure of learning with noisy supervisions, which avoids the unexpected local minima with incorrect structures in previous works.",
        "We explore a principled solution by simulating human cognition on the structure of the noise transition matrix.",
        "The reconstructor is for the first term in Eq (1), facilitating the classifier prediction P (y|x) and the noise transition matrix s to yield noisy labels y.",
        "For CIFAR-10, we randomly flip the labels of the training set according to the first two types of noise structure, which has been illustrated in Figure 1 with predefined transition probabilities.",
        "For CIFAR-100, we implement the similar procedure to generate the noisy data, but follow the last type of noise structure in Figure 1(c).",
        "With the guidance of the prior structure, MASKING infers the noise transition matrix better than two baselines.",
        "This approach conveys human cognition of invalid class transitions, and speculates the structure of the noise transition matrix.",
        "Table 2: The estimation of the noise transition matrix by F-correction (1st row), S-adaptation (2nd row) and MASKING (3rd row), and the truth (4th row) in the case of three types of noise transition structure: column-diagonal (1st column), tri-diagonal (2nd column), block-diagonal (3rd column).",
        "When the noise structure is wrongly set at the initial stage, how does our model correct the initial structure by learning from the finite dataset?"
    ],
    "headline": "We propose a human-assisted approach called \u201cMasking\u201d that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Y. A\u00eft-Sahalia, J. Fan, and D. Xiu. High-frequency covariance estimates with noisy and asynchronous financial data. Journal of the American Statistical Association, 105(492):1504\u20131517, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%C3%AFt-Sahalia%2C%20Y.%20Fan%2C%20J.%20Xiu%2C%20D.%20High-frequency%20covariance%20estimates%20with%20noisy%20and%20asynchronous%20financial%20data%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%C3%AFt-Sahalia%2C%20Y.%20Fan%2C%20J.%20Xiu%2C%20D.%20High-frequency%20covariance%20estimates%20with%20noisy%20and%20asynchronous%20financial%20data%202010"
        },
        {
            "id": "2",
            "entry": "[2] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2(4):343\u2013370, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Angluin%2C%20D.%20Laird%2C%20P.%20Learning%20from%20noisy%20examples%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Angluin%2C%20D.%20Laird%2C%20P.%20Learning%20from%20noisy%20examples%201988"
        },
        {
            "id": "3",
            "entry": "[3] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. Kanwal, T. Maharaj, A. Fischer, A. Courville, and Y. Bengio. A closer look at memorization in deep networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arpit%2C%20D.%20Jastrzebski%2C%20S.%20Ballas%2C%20N.%20Krueger%2C%20D.%20A%20closer%20look%20at%20memorization%20in%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arpit%2C%20D.%20Jastrzebski%2C%20S.%20Ballas%2C%20N.%20Krueger%2C%20D.%20A%20closer%20look%20at%20memorization%20in%20deep%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] S. Azadi, J. Feng, S. Jegelka, and T. Darrell. Auxiliary image regularization for deep cnns with noisy labels. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azadi%2C%20S.%20Feng%2C%20J.%20Jegelka%2C%20S.%20Darrell%2C%20T.%20Auxiliary%20image%20regularization%20for%20deep%20cnns%20with%20noisy%20labels%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azadi%2C%20S.%20Feng%2C%20J.%20Jegelka%2C%20S.%20Darrell%2C%20T.%20Auxiliary%20image%20regularization%20for%20deep%20cnns%20with%20noisy%20labels%202016"
        },
        {
            "id": "5",
            "entry": "[5] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7(Nov):2399\u20132434, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20M.%20Niyogi%2C%20P.%20Sindhwani%2C%20V.%20Manifold%20regularization%3A%20A%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20M.%20Niyogi%2C%20P.%20Sindhwani%2C%20V.%20Manifold%20regularization%3A%20A%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006"
        },
        {
            "id": "6",
            "entry": "[6] Y. Cha and J. Cho. Social-network analysis using topic models. In SIGIR, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cha%2C%20Y.%20Cho%2C%20J.%20Social-network%20analysis%20using%20topic%20models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cha%2C%20Y.%20Cho%2C%20J.%20Social-network%20analysis%20using%20topic%20models%202012"
        },
        {
            "id": "7",
            "entry": "[7] J. Deng, J. Krause, and L. Fei-Fei. Fine-grained crowdsourcing for fine-grained recognition. In CVPR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Krause%2C%20J.%20Fei-Fei%2C%20L.%20Fine-grained%20crowdsourcing%20for%20fine-grained%20recognition%202013"
        },
        {
            "id": "8",
            "entry": "[8] Y. Dgani, H. Greenspan, and J. Goldberger. Training a neural network based on unreliable human annotation of medical images. In ISBI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dgani%2C%20Y.%20Greenspan%2C%20H.%20Goldberger%2C%20J.%20Training%20a%20neural%20network%20based%20on%20unreliable%20human%20annotation%20of%20medical%20images%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dgani%2C%20Y.%20Greenspan%2C%20H.%20Goldberger%2C%20J.%20Training%20a%20neural%20network%20based%20on%20unreliable%20human%20annotation%20of%20medical%20images%202018"
        },
        {
            "id": "9",
            "entry": "[9] P. Domingos and M. Pazzani. On the optimality of the simple bayesian classifier under zero-one loss. Machine Learning, 29(2-3):103\u2013130, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Domingos%2C%20P.%20Pazzani%2C%20M.%20On%20the%20optimality%20of%20the%20simple%20bayesian%20classifier%20under%20zero-one%20loss%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Domingos%2C%20P.%20Pazzani%2C%20M.%20On%20the%20optimality%20of%20the%20simple%20bayesian%20classifier%20under%20zero-one%20loss%201997"
        },
        {
            "id": "10",
            "entry": "[10] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberger%2C%20J.%20Ben-Reuven%2C%20E.%20Training%20deep%20neural-networks%20using%20a%20noise%20adaptation%20layer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldberger%2C%20J.%20Ben-Reuven%2C%20E.%20Training%20deep%20neural-networks%20using%20a%20noise%20adaptation%20layer%202017"
        },
        {
            "id": "11",
            "entry": "[11] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "12",
            "entry": "[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "13",
            "entry": "[13] N. Goodman. Sense and certainty. The Philosophical Review, pages 160\u2013167, 1952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodman%2C%20N.%20Sense%20and%20certainty%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodman%2C%20N.%20Sense%20and%20certainty%201952"
        },
        {
            "id": "14",
            "entry": "[14] P. Grigolini, G. Aquino, M. Bologna, M. Lukovic, and B. West. A theory of 1/f noise in human cognition. Physica A: Statistical Mechanics and its Applications, 388(19):4192\u20134204, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grigolini%2C%20P.%20Aquino%2C%20G.%20Bologna%2C%20M.%20Lukovic%2C%20M.%20A%20theory%20of%201/f%20noise%20in%20human%20cognition%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grigolini%2C%20P.%20Aquino%2C%20G.%20Bologna%2C%20M.%20Lukovic%2C%20M.%20A%20theory%20of%201/f%20noise%20in%20human%20cognition%202009"
        },
        {
            "id": "15",
            "entry": "[15] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of wasserstein gans. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20I.%20Ahmed%2C%20F.%20Arjovsky%2C%20M.%20Dumoulin%2C%20V.%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "16",
            "entry": "[16] B. Han, I. Tsang, and L. Chen. On the convergence of a family of robust losses for stochastic gradient descent. In ECML-PKDD, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20B.%20Tsang%2C%20I.%20Chen%2C%20L.%20On%20the%20convergence%20of%20a%20family%20of%20robust%20losses%20for%20stochastic%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20B.%20Tsang%2C%20I.%20Chen%2C%20L.%20On%20the%20convergence%20of%20a%20family%20of%20robust%20losses%20for%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "17",
            "entry": "[17] D. Hendrycks, M. Mazeika, D. Wilson, and K. Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hendrycks%2C%20D.%20Mazeika%2C%20M.%20Wilson%2C%20D.%20Gimpel%2C%20K.%20Using%20trusted%20data%20to%20train%20deep%20networks%20on%20labels%20corrupted%20by%20severe%20noise%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hendrycks%2C%20D.%20Mazeika%2C%20M.%20Wilson%2C%20D.%20Gimpel%2C%20K.%20Using%20trusted%20data%20to%20train%20deep%20networks%20on%20labels%20corrupted%20by%20severe%20noise%202018"
        },
        {
            "id": "18",
            "entry": "[18] J. Huang, A. Gretton, K. Borgwardt, B. Sch\u00f6lkopf, and A. Smola. Correcting sample selection bias by unlabeled data. In NIPS, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20J.%20Gretton%2C%20A.%20Borgwardt%2C%20K.%20Sch%C3%B6lkopf%2C%20B.%20Correcting%20sample%20selection%20bias%20by%20unlabeled%20data%202007"
        },
        {
            "id": "19",
            "entry": "[19] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20L.%20Zhou%2C%20Z.%20Leung%2C%20T.%20Li%2C%20L.%20Mentornet%3A%20Learning%20data-driven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20L.%20Zhou%2C%20Z.%20Leung%2C%20T.%20Li%2C%20L.%20Mentornet%3A%20Learning%20data-driven%20curriculum%20for%20very%20deep%20neural%20networks%20on%20corrupted%20labels%202018"
        },
        {
            "id": "20",
            "entry": "[20] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "21",
            "entry": "[21] W. Li, L. Wang, W. Li, E. Agustsson, and L. Van Gool. Webvision database: Visual learning and understanding from web data. arXiv:1708.02862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02862"
        },
        {
            "id": "22",
            "entry": "[22] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, and J. Li. Learning from noisy labels with distillation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Yang%2C%20J.%20Song%2C%20Y.%20Cao%2C%20L.%20Learning%20from%20noisy%20labels%20with%20distillation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Yang%2C%20J.%20Song%2C%20Y.%20Cao%2C%20L.%20Learning%20from%20noisy%20labels%20with%20distillation%202017"
        },
        {
            "id": "23",
            "entry": "[23] T. Liu and D. Tao. Classification with noisy labels by importance reweighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(3):447\u2013461, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20T.%20Tao%2C%20D.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20T.%20Tao%2C%20D.%20Classification%20with%20noisy%20labels%20by%20importance%20reweighting%202016"
        },
        {
            "id": "24",
            "entry": "[24] W. Liu, Y. Jiang, J. Luo, and S. Chang. Noise resistant graph ranking for improved web image search. In CVPR, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20W.%20Jiang%2C%20Y.%20Luo%2C%20J.%20Chang%2C%20S.%20Noise%20resistant%20graph%20ranking%20for%20improved%20web%20image%20search%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20W.%20Jiang%2C%20Y.%20Luo%2C%20J.%20Chang%2C%20S.%20Noise%20resistant%20graph%20ranking%20for%20improved%20web%20image%20search%202011"
        },
        {
            "id": "25",
            "entry": "[25] X. Ma, Y. Wang, M. Houle, S. Zhou, S. Erfani, S. Xia, S. Wijewickrema, and J. Bailey. Dimensionalitydriven learning with noisy labels. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20X.%20Wang%2C%20Y.%20Houle%2C%20M.%20Zhou%2C%20S.%20Dimensionalitydriven%20learning%20with%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20X.%20Wang%2C%20Y.%20Houle%2C%20M.%20Zhou%2C%20S.%20Dimensionalitydriven%20learning%20with%20noisy%20labels%202018"
        },
        {
            "id": "26",
            "entry": "[26] E. Malach and S. Shalev-Shwartz. Decoupling\" when to update\" from\" how to update\". In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Decoupling%22%20when%20to%20update%22%20from%22%20how%20to%20update%22%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malach%2C%20E.%20Shalev-Shwartz%2C%20S.%20Decoupling%22%20when%20to%20update%22%20from%22%20how%20to%20update%22%202017"
        },
        {
            "id": "27",
            "entry": "[27] H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In NIPS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Masnadi-Shirazi%2C%20H.%20Vasconcelos%2C%20N.%20On%20the%20design%20of%20loss%20functions%20for%20classification%3A%20theory%2C%20robustness%20to%20outliers%2C%20and%20savageboost%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Masnadi-Shirazi%2C%20H.%20Vasconcelos%2C%20N.%20On%20the%20design%20of%20loss%20functions%20for%20classification%3A%20theory%2C%20robustness%20to%20outliers%2C%20and%20savageboost%202009"
        },
        {
            "id": "28",
            "entry": "[28] A. Menon, B. Van Rooyen, C. Ong, and B. Williamson. Learning from corrupted binary labels via class-probability estimation. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20A.%20Rooyen%2C%20B.Van%20Ong%2C%20C.%20Williamson%2C%20B.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20A.%20Rooyen%2C%20B.Van%20Ong%2C%20C.%20Williamson%2C%20B.%20Learning%20from%20corrupted%20binary%20labels%20via%20class-probability%20estimation%202015"
        },
        {
            "id": "29",
            "entry": "[29] R. Michalski, J. Carbonell, and T. Mitchell. Machine Learning: An Artificial Intelligence Approach. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Michalski%2C%20R.%20Carbonell%2C%20J.%20Mitchell%2C%20T.%20Machine%20Learning%3A%20An%20Artificial%20Intelligence%20Approach%202013"
        },
        {
            "id": "30",
            "entry": "[30] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Ishii%2C%20S.%20Virtual%20adversarial%20training%3A%20A%20regularization%20method%20for%20supervised%20and%20semi-supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Maeda%2C%20S.%20Koyama%2C%20M.%20Ishii%2C%20S.%20Virtual%20adversarial%20training%3A%20A%20regularization%20method%20for%20supervised%20and%20semi-supervised%20learning%202016"
        },
        {
            "id": "31",
            "entry": "[31] N. Natarajan, I. Dhillon, P. Ravikumar, and A. Tewari. Learning with noisy labels. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20N.%20Dhillon%2C%20I.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20N.%20Dhillon%2C%20I.%20Ravikumar%2C%20P.%20Tewari%2C%20A.%20Learning%20with%20noisy%20labels%202013"
        },
        {
            "id": "32",
            "entry": "[32] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patrini%2C%20G.%20Rozza%2C%20A.%20Menon%2C%20A.%20Nock%2C%20R.%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20A%20loss%20correction%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patrini%2C%20G.%20Rozza%2C%20A.%20Menon%2C%20A.%20Nock%2C%20R.%20Making%20deep%20neural%20networks%20robust%20to%20label%20noise%3A%20A%20loss%20correction%20approach%202017"
        },
        {
            "id": "33",
            "entry": "[33] V. Raykar, S. Yu, L. Zhao, G. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. Journal of Machine Learning Research, 11(Apr):1297\u20131322, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raykar%2C%20V.%20Yu%2C%20S.%20Zhao%2C%20L.%20Valadez%2C%20G.%20Learning%20from%20crowds%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raykar%2C%20V.%20Yu%2C%20S.%20Zhao%2C%20L.%20Valadez%2C%20G.%20Learning%20from%20crowds%202010"
        },
        {
            "id": "34",
            "entry": "[34] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Lee%2C%20H.%20Anguelov%2C%20D.%20Szegedy%2C%20C.%20Training%20deep%20neural%20networks%20on%20noisy%20labels%20with%20bootstrapping%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Lee%2C%20H.%20Anguelov%2C%20D.%20Szegedy%2C%20C.%20Training%20deep%20neural%20networks%20on%20noisy%20labels%20with%20bootstrapping%202015"
        },
        {
            "id": "35",
            "entry": "[35] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20M.%20Zeng%2C%20W.%20Yang%2C%20B.%20Urtasun%2C%20R.%20Learning%20to%20reweight%20examples%20for%20robust%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20M.%20Zeng%2C%20W.%20Yang%2C%20B.%20Urtasun%2C%20R.%20Learning%20to%20reweight%20examples%20for%20robust%20deep%20learning%202018"
        },
        {
            "id": "36",
            "entry": "[36] V. Rockova and K. McAlinn. Dynamic variable selection with Spike-and-Slab process priors. arXiv:1708.00085, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.00085"
        },
        {
            "id": "37",
            "entry": "[37] F. Rodrigues and F. Pereira. Deep learning from crowds. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodrigues%2C%20F.%20Pereira%2C%20F.%20Deep%20learning%20from%20crowds%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodrigues%2C%20F.%20Pereira%2C%20F.%20Deep%20learning%20from%20crowds%202018"
        },
        {
            "id": "38",
            "entry": "[38] T. Sanderson and C. Scott. Class proportion estimation with application to multiclass anomaly rejection. In AISTATS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sanderson%2C%20T.%20Scott%2C%20C.%20Class%20proportion%20estimation%20with%20application%20to%20multiclass%20anomaly%20rejection%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sanderson%2C%20T.%20Scott%2C%20C.%20Class%20proportion%20estimation%20with%20application%20to%20multiclass%20anomaly%20rejection%202014"
        },
        {
            "id": "39",
            "entry": "[39] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels. In ICLR workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20S.%20Bruna%2C%20J.%20Paluri%2C%20M.%20Bourdev%2C%20L.%20Training%20convolutional%20networks%20with%20noisy%20labels%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20S.%20Bruna%2C%20J.%20Paluri%2C%20M.%20Bourdev%2C%20L.%20Training%20convolutional%20networks%20with%20noisy%20labels%202015"
        },
        {
            "id": "40",
            "entry": "[40] D. Tanaka, D. Ikami, T. Yamasaki, and K. Aizawa. Joint optimization framework for learning with noisy labels. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tanaka%2C%20D.%20Ikami%2C%20D.%20Yamasaki%2C%20T.%20Aizawa%2C%20K.%20Joint%20optimization%20framework%20for%20learning%20with%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tanaka%2C%20D.%20Ikami%2C%20D.%20Yamasaki%2C%20T.%20Aizawa%2C%20K.%20Joint%20optimization%20framework%20for%20learning%20with%20noisy%20labels%202018"
        },
        {
            "id": "41",
            "entry": "[41] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tarvainen%2C%20A.%20Valpola%2C%20H.%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tarvainen%2C%20A.%20Valpola%2C%20H.%20Mean%20teachers%20are%20better%20role%20models%3A%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results%202017"
        },
        {
            "id": "42",
            "entry": "[42] D. Tran, R. Ranganath, and D. Blei. Hierarchical implicit models and likelihood-free variational inference. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20D.%20Ranganath%2C%20R.%20Blei%2C%20D.%20Hierarchical%20implicit%20models%20and%20likelihood-free%20variational%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20D.%20Ranganath%2C%20R.%20Blei%2C%20D.%20Hierarchical%20implicit%20models%20and%20likelihood-free%20variational%20inference%202017"
        },
        {
            "id": "43",
            "entry": "[43] A. Veit, N. Alldrin, G. Chechik, I. Krasin, A. Gupta, and S. Belongie. Learning from noisy large-scale datasets with minimal supervision. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veit%2C%20A.%20Alldrin%2C%20N.%20Chechik%2C%20G.%20Krasin%2C%20I.%20Learning%20from%20noisy%20large-scale%20datasets%20with%20minimal%20supervision%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veit%2C%20A.%20Alldrin%2C%20N.%20Chechik%2C%20G.%20Krasin%2C%20I.%20Learning%20from%20noisy%20large-scale%20datasets%20with%20minimal%20supervision%202017"
        },
        {
            "id": "44",
            "entry": "[44] Y. Wang, W. Liu, X. Ma, J. Bailey, H. Zha, L. Song, and S. Xia. Iterative learning with open-set noisy labels. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Liu%2C%20W.%20Ma%2C%20X.%20Bailey%2C%20J.%20Iterative%20learning%20with%20open-set%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Liu%2C%20W.%20Ma%2C%20X.%20Bailey%2C%20J.%20Iterative%20learning%20with%20open-set%20noisy%20labels%202018"
        },
        {
            "id": "45",
            "entry": "[45] P. Welinder, S. Branson, P. Perona, and S. Belongie. The multidimensional wisdom of crowds. In NIPS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welinder%2C%20P.%20Branson%2C%20S.%20Perona%2C%20P.%20Belongie%2C%20S.%20The%20multidimensional%20wisdom%20of%20crowds%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welinder%2C%20P.%20Branson%2C%20S.%20Perona%2C%20P.%20Belongie%2C%20S.%20The%20multidimensional%20wisdom%20of%20crowds%202010"
        },
        {
            "id": "46",
            "entry": "[46] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20T.%20Xia%2C%20T.%20Yang%2C%20Y.%20Huang%2C%20C.%20Learning%20from%20massive%20noisy%20labeled%20data%20for%20image%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20T.%20Xia%2C%20T.%20Yang%2C%20Y.%20Huang%2C%20C.%20Learning%20from%20massive%20noisy%20labeled%20data%20for%20image%20classification%202015"
        },
        {
            "id": "47",
            "entry": "[47] Y. Yan, R. Rosales, G. Fung, R. Subramanian, and J. Dy. Learning from multiple annotators with varying expertise. Machine Learning, 95(3):291\u2013327, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20Y.%20Rosales%2C%20R.%20Fung%2C%20G.%20Subramanian%2C%20R.%20Learning%20from%20multiple%20annotators%20with%20varying%20expertise%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20Y.%20Rosales%2C%20R.%20Fung%2C%20G.%20Subramanian%2C%20R.%20Learning%20from%20multiple%20annotators%20with%20varying%20expertise%202014"
        },
        {
            "id": "48",
            "entry": "[48] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "49",
            "entry": "[49] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NIPS, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Z.%20Sabuncu%2C%20M.%20Generalized%20cross%20entropy%20loss%20for%20training%20deep%20neural%20networks%20with%20noisy%20labels%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Z.%20Sabuncu%2C%20M.%20Generalized%20cross%20entropy%20loss%20for%20training%20deep%20neural%20networks%20with%20noisy%20labels%202018"
        }
    ]
}
