{
    "filename": "8024-spectral-signatures-in-backdoor-attacks.pdf",
    "metadata": {
        "title": "Spectral Signatures in Backdoor Attacks",
        "author": "Brandon Tran, Jerry Li, Aleksander Madry",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8024-spectral-signatures-in-backdoor-attacks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A recent line of work has uncovered a new form of data poisoning: so-called backdoor attacks. These attacks are particularly dangerous because they do not affect a network\u2019s behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by a perturbation planted by an adversary. In this paper, we identify a new property of all known backdoor attacks, which we call spectral signatures. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards designing ML systems secure against such backdoor attacks."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "spectral signature",
            "url": "https://en.wikipedia.org/wiki/spectral_signature"
        },
        {
            "term": "robust statistic",
            "url": "https://en.wikipedia.org/wiki/robust_statistic"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Deep learning has achieved widespread success in a variety of settings, such as computer vision [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], speech recognition [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], and text analysis [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "Perhaps the first setting developed for building secure deep learning models was adversarial examples [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]",
        "Test examples are perturbed by seemingly imperceptible amounts in order to change their classification under a neural network classifier",
        "We demonstrate the existence of spectral signatures for backdoor attacks on image classification tasks and show that they can be used to effectively clean the corrupted training set",
        "We present the notion of spectral signatures and demonstrate how they can be used to detect backdoor poisoning attacks",
        "This paper demonstrates that machinery from robust statistics and classical machine learning can be very useful"
    ],
    "key_statements": [
        "Deep learning has achieved widespread success in a variety of settings, such as computer vision [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], speech recognition [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], and text analysis [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "Perhaps the first setting developed for building secure deep learning models was adversarial examples [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]",
        "Test examples are perturbed by seemingly imperceptible amounts in order to change their classification under a neural network classifier",
        "We demonstrate a new property of backdoor attacks",
        "We show that these attacks tend to leave behind a detectable trace in the spectrum of the covariance of a feature representation learned by the neural network",
        "We are able to use spectral signatures to reliably remove many\u2014often all\u2014of the corrupted training examples, reducing the misclassification rate on backdoored test points to within 1% of the rate achieved by a standard network trained on a clean training set",
        "We demonstrate the existence of spectral signatures for backdoor attacks on image classification tasks and show that they can be used to effectively clean the corrupted training set",
        "While these techniques are more or less implicit in the robust statistics literature, we include them here to provide some intuition as to why spectral techniques should detect deviations in the mean caused by a small sub-population of poisoned inputs",
        "We study backdoor poisoning attacks on the CIFAR10 [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] dataset, using a standard ResNet [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] model with 3 groups of residual layers with filter sizes [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c64\" href=\"#r64\">64</a>] and 5 residual units per layer",
        "We present the notion of spectral signatures and demonstrate how they can be used to detect backdoor poisoning attacks",
        "We demonstrate that the learned representation is necesary; naively utilizing robust statistics tools at the data level does not provide a means with which to remove backdoored examples",
        "This paper demonstrates that machinery from robust statistics and classical machine learning can be very useful"
    ],
    "summary": [
        "Deep learning has achieved widespread success in a variety of settings, such as computer vision [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], speech recognition [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], and text analysis [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "We are able to use spectral signatures to reliably remove many\u2014often all\u2014of the corrupted training examples, reducing the misclassification rate on backdoored test points to within 1% of the rate achieved by a standard network trained on a clean training set.",
        "By running these robust statistics tools on the learned representation, one can detect and remove backdoored inputs.",
        "We demonstrate the existence of spectral signatures for backdoor attacks on image classification tasks and show that they can be used to effectively clean the corrupted training set.",
        "The intuition here is that if the set of inputs with a given label consists of both clean examples as well as corrupted examples from a different label set, the backdoor from the latter set will provide a strong signal in this representation for classification.",
        "We give more rigorous intuition as to why learned representations on the corrupted data may cause the attack to have a detectable spectral signature.",
        "1: Input: Training set Dtrain, randomly initialized neural network model L providing a feature representation R, and upper bound on number of poisoned training set examples \u03b5.",
        "While these techniques are more or less implicit in the robust statistics literature, we include them here to provide some intuition as to why spectral techniques should detect deviations in the mean caused by a small sub-population of poisoned inputs.",
        "This lemma states that if the mean of the true distribution of inputs of a certain class differs enough from the mean of the backdoored images, these two classes can be reliably distinguished via spectral methods.",
        "We record the accuracy on the natural evaluation set as well as the poisoned evaluation set (1000 images of the attack label with a backdoor).",
        "We present the notion of spectral signatures and demonstrate how they can be used to detect backdoor poisoning attacks.",
        "We apply tools from robust statistics to the representations in order to detect and remove the poisoned data.",
        "We provide statistics showing that at the learned representation level, the poisoned inputs shift the distribution enough to be detected with SVD methods.",
        "We demonstrate that the learned representation is necesary; naively utilizing robust statistics tools at the data level does not provide a means with which to remove backdoored examples."
    ],
    "headline": "We identify a new property of all known backdoor attacks, which we call spectral signatures",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. arXiv preprint arXiv:1802.04633, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04633"
        },
        {
            "id": "2",
            "entry": "[2] S. Balakrishnan, S. S. Du, J. Li, and A. Singh. Computationally efficient robust sparse estimation in high dimensions. In Conference on Learning Theory, pages 169\u2013212, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balakrishnan%2C%20S.%20Du%2C%20S.S.%20Li%2C%20J.%20Singh%2C%20A.%20Computationally%20efficient%20robust%20sparse%20estimation%20in%20high%20dimensions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balakrishnan%2C%20S.%20Du%2C%20S.S.%20Li%2C%20J.%20Singh%2C%20A.%20Computationally%20efficient%20robust%20sparse%20estimation%20in%20high%20dimensions%202017"
        },
        {
            "id": "3",
            "entry": "[3] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In ICML, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20B.%20Nelson%2C%20B.%20Laskov%2C%20P.%20Poisoning%20attacks%20against%20support%20vector%20machines%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20B.%20Nelson%2C%20B.%20Laskov%2C%20P.%20Poisoning%20attacks%20against%20support%20vector%20machines%202012"
        },
        {
            "id": "4",
            "entry": "[4] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In USENIX Security), pages 513\u2013530, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Mishra%2C%20P.%20Vaidya%2C%20T.%20Zhang%2C%20Y.%20Hidden%20voice%20commands%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Mishra%2C%20P.%20Vaidya%2C%20T.%20Zhang%2C%20Y.%20Hidden%20voice%20commands%202016"
        },
        {
            "id": "5",
            "entry": "[5] M. Charikar, J. Steinhardt, and G. Valiant. Learning from untrusted data. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 47\u201360. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Charikar%2C%20M.%20Steinhardt%2C%20J.%20Valiant%2C%20G.%20Learning%20from%20untrusted%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Charikar%2C%20M.%20Steinhardt%2C%20J.%20Valiant%2C%20G.%20Learning%20from%20untrusted%20data%202017"
        },
        {
            "id": "6",
            "entry": "[6] X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05526"
        },
        {
            "id": "7",
            "entry": "[7] R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collobert%2C%20R.%20Weston%2C%20J.%20A%20unified%20architecture%20for%20natural%20language%20processing%3A%20Deep%20neural%20networks%20with%20multitask%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collobert%2C%20R.%20Weston%2C%20J.%20A%20unified%20architecture%20for%20natural%20language%20processing%3A%20Deep%20neural%20networks%20with%20multitask%20learning%202008"
        },
        {
            "id": "8",
            "entry": "[8] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 655\u2013664. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Robust%20estimators%20in%20high%20dimensions%20without%20the%20computational%20intractability%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Robust%20estimators%20in%20high%20dimensions%20without%20the%20computational%20intractability%202016"
        },
        {
            "id": "9",
            "entry": "[9] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Being robust (in high dimensions) can be practical. In International Conference on Machine Learning, pages 999\u20131008, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Being%20robust%20%28in%20high%20dimensions%29%20can%20be%20practical%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Being%20robust%20%28in%20high%20dimensions%29%20can%20be%20practical%202017"
        },
        {
            "id": "10",
            "entry": "[10] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A robust metaalgorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02815"
        },
        {
            "id": "11",
            "entry": "[11] e. a. Donahue, Jeff. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=e.%20a.%20Donahue%20Jeff%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=e.%20a.%20Donahue%20Jeff%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition%202014"
        },
        {
            "id": "12",
            "entry": "[12] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song. Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08945"
        },
        {
            "id": "13",
            "entry": "[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%202014"
        },
        {
            "id": "14",
            "entry": "[14] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages 6645\u20136649. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20A.%20Mohamed%2C%20A.-r%20Hinton%2C%20G.%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20A.%20Mohamed%2C%20A.-r%20Hinton%2C%20G.%20Speech%20recognition%20with%20deep%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "15",
            "entry": "[15] T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.06733"
        },
        {
            "id": "16",
            "entry": "[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "17",
            "entry": "[17] A. Klivans, P. K. Kothari, and R. Meka. Efficient algorithms for outlier-robust regression. arXiv preprint arXiv:1803.03241, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03241"
        },
        {
            "id": "18",
            "entry": "[18] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017"
        },
        {
            "id": "19",
            "entry": "[19] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "20",
            "entry": "[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "21",
            "entry": "[21] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "22",
            "entry": "[22] K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 665\u2013674. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20K.A.%20Rao%2C%20A.B.%20Vempala%2C%20S.%20Agnostic%20estimation%20of%20mean%20and%20covariance%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20K.A.%20Rao%2C%20A.B.%20Vempala%2C%20S.%20Agnostic%20estimation%20of%20mean%20and%20covariance%202016"
        },
        {
            "id": "23",
            "entry": "[23] K. Liu, B. Dolan-Gavitt, and S. Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. arXiv preprint arXiv:1805.12185, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12185"
        },
        {
            "id": "24",
            "entry": "[24] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "25",
            "entry": "[25] S. Mei and X. Zhu. The security of latent dirichlet allocation. In Artificial Intelligence and Statistics, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20S.%20Zhu%2C%20X.%20The%20security%20of%20latent%20dirichlet%20allocation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20S.%20Zhu%2C%20X.%20The%20security%20of%20latent%20dirichlet%20allocation%202015"
        },
        {
            "id": "26",
            "entry": "[26] N. Papernot, N. Carlini, I. Goodfellow, R. Feinman, F. Faghri, A. Matyasko, K. Hambardzumyan, Y.-L. Juang, A. Kurakin, R. Sheatsley, et al. cleverhans v2. 0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.00768"
        },
        {
            "id": "27",
            "entry": "[27] A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06485"
        },
        {
            "id": "28",
            "entry": "[28] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. arXiv preprint arXiv:1804.00792, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.00792"
        },
        {
            "id": "29",
            "entry": "[29] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20M.%20Bhagavatula%2C%20S.%20Bauer%2C%20L.%20Reiter%2C%20M.K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20M.%20Bhagavatula%2C%20S.%20Bauer%2C%20L.%20Reiter%2C%20M.K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "30",
            "entry": "[30] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "31",
            "entry": "[31] J. Steinhardt, P. W. W. Koh, and P. S. Liang. Certified defenses for data poisoning attacks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinhardt%2C%20J.%20Koh%2C%20P.W.W.%20Liang%2C%20P.S.%20Certified%20defenses%20for%20data%20poisoning%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinhardt%2C%20J.%20Koh%2C%20P.W.W.%20Liang%2C%20P.S.%20Certified%20defenses%20for%20data%20poisoning%20attacks%202017"
        },
        {
            "id": "32",
            "entry": "[32] F. Tram\u00e8r, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07204"
        },
        {
            "id": "33",
            "entry": "[33] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. 2018. Available at https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf.",
            "url": "https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf"
        },
        {
            "id": "34",
            "entry": "[34] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. Support vector machines under adversarial label contamination. Neurocomputing, 160:53\u201362, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20H.%20Biggio%2C%20B.%20Nelson%2C%20B.%20Xiao%2C%20H.%20Support%20vector%20machines%20under%20adversarial%20label%20contamination%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20H.%20Biggio%2C%20B.%20Nelson%2C%20B.%20Xiao%2C%20H.%20Support%20vector%20machines%20under%20adversarial%20label%20contamination%202015"
        }
    ]
}
