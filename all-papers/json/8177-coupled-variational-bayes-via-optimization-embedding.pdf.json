{
    "filename": "8177-coupled-variational-bayes-via-optimization-embedding.pdf",
    "metadata": {
        "date": 2018,
        "title": "Coupled Variational Bayes via Optimization Embedding",
        "author": "\u2217Bo Dai1,2, \u2217Hanjun Dai1, Niao He3, Weiyang Liu1, Zhen Liu1, Jianshu Chen4, Lin Xiao5, Le Song1,6",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8177-coupled-variational-bayes-via-optimization-embedding.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Variational inference plays a vital role in learning graphical models, especially on large-scale datasets. Much of its success depends on a proper choice of auxiliary distribution class for posterior approximation. However, how to pursue an auxiliary distribution class that achieves both good approximation ability and computation efficiency remains a core challenge. In this paper, we proposed coupled variational Bayes which exploits the primal-dual view of the ELBO with the variational distribution class generated by an optimization procedure, which is termed optimization embedding. This flexible function class couples the variational distribution with the original parameters in the graphical models, allowing end-to-end learning of the graphical models by back-propagation through the variational distribution. Theoretically, we establish an interesting connection to gradient flow and demonstrate the extreme flexibility of this implicit distribution family in the limit sense. Empirically, we demonstrate the effectiveness of the proposed method on multiple graphical models with either continuous or discrete latent variables comparing to state-of-the-art methods."
    },
    "keywords": [
        {
            "term": "gradient flow",
            "url": "https://en.wikipedia.org/wiki/gradient_flow"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "variational bayes",
            "url": "https://en.wikipedia.org/wiki/variational_bayes"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "marginal likelihood",
            "url": "https://en.wikipedia.org/wiki/marginal_likelihood"
        }
    ],
    "highlights": [
        "The proposed approach hinges upon two key components: i), the primaldual view of the evidence lower bound; and ii), the optimization embedding technique for generating variational distributions",
        "We present our coupled variational Bayes in Section 3, which leverages the optimization embedding in the primal-dual view of evidence lower bound to couple the variational distribution with original graphical models",
        "Connections to Langevin dynamics and Stein variational gradient descent As we show in Theorem 3, the optimization embedding could be viewed as a discretization of a nonlinear Fokker-Plank equation, which can be interpreted as a gradient flow of KL-divergence on 2-Wasserstein metric with a special \u03bd (x, z)",
        "While variational auto-encoder generates a mixture of 4 Gaussians that is consistent with the parametrization assumption, the proposed Coupled Variational Bayes divides the latent space with a complex distribution",
        "We propose the coupled variational Bayes, which is designed based on the primal-dual view of evidence lower bound and the optimization embedding technique",
        "Numerical experiments demonstrates the superiority of Coupled Variational Bayes in approximate ability, computational efficiency, and sample complexity"
    ],
    "key_statements": [
        "The proposed approach hinges upon two key components: i), the primaldual view of the evidence lower bound; and ii), the optimization embedding technique for generating variational distributions",
        "This paper provides a method towards such a solution, called coupled variational Bayes (CVB)",
        "We present our coupled variational Bayes in Section 3, which leverages the optimization embedding in the primal-dual view of evidence lower bound to couple the variational distribution with original graphical models",
        "Connections to Langevin dynamics and Stein variational gradient descent As we show in Theorem 3, the optimization embedding could be viewed as a discretization of a nonlinear Fokker-Plank equation, which can be interpreted as a gradient flow of KL-divergence on 2-Wasserstein metric with a special \u03bd (x, z)",
        "While variational auto-encoder generates a mixture of 4 Gaussians that is consistent with the parametrization assumption, the proposed Coupled Variational Bayes divides the latent space with a complex distribution",
        "CelebA We use Coupled Variational Bayes to train a generative model with deep deconvolution network on CelebAdataset for a 64-dimension latent space with N (0, 1) prior [<a class=\"ref-link\" id=\"cMescheder_et+al_2017_a\" href=\"#rMescheder_et+al_2017_a\">Mescheder et al, 2017</a>]. we use convolutional neural network architecture similar to DCGAN",
        "We propose the coupled variational Bayes, which is designed based on the primal-dual view of evidence lower bound and the optimization embedding technique",
        "Numerical experiments demonstrates the superiority of Coupled Variational Bayes in approximate ability, computational efficiency, and sample complexity"
    ],
    "summary": [
        "The proposed approach hinges upon two key components: i), the primaldual view of the ELBO; and ii), the optimization embedding technique for generating variational distributions.",
        "Several carefully designed simple parametric forms of T have been proposed to compromise the invertible requirement and tractability of Jacobian [<a class=\"ref-link\" id=\"cRezende_2015_a\" href=\"#rRezende_2015_a\">Rezende and Mohamed, 2015</a>, <a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a></a>, <a class=\"ref-link\" id=\"cTomczak_2016_a\" href=\"#rTomczak_2016_a\">Tomczak and Welling, 2016</a>, <a class=\"ref-link\" id=\"cDinh_et+al_2016_a\" href=\"#rDinh_et+al_2016_a\">Dinh et al, 2016</a>], at the expense of the flexibility of the corresponding variational distribution families.",
        "The optimization embedding couples the implicit variational distribution with the original graphical models, making the training more efficient.",
        "We emphasize that optimization embedding is a general technique for representing the variational distributions and can be accompanied with the original ELBO, which is provided in Appendix B.",
        "To reduce the effect from neural network parametrization, we update the parameters in \u03bd within the optimization embedding simultaneously, implicitly pushing zT to follow the gradient flow.",
        "Connections to Langevin dynamics and Stein variational gradient descent As we show in Theorem 3, the optimization embedding could be viewed as a discretization of a nonlinear Fokker-Plank equation, which can be interpreted as a gradient flow of KL-divergence on 2-Wasserstein metric with a special \u03bd (x, z).",
        "The most important difference is that CVB couples the adversarial component with original models through optimization embedding, which is flexible enough to approximate the true posterior and promote the learning sample efficiency.",
        "While VAE generates a mixture of 4 Gaussians that is consistent with the parametrization assumption, the proposed CVB divides the latent space with a complex distribution.",
        "To verify the sample efficiency of CVB, we compare the performance of CVB on static binarize MNIST dataset to the current state-of-the-art algorithms, including VAE with inverse autoregressive flow (VAE+IAF) [<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\"><a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a></a>], adversarial variational Bayes (AVB) [<a class=\"ref-link\" id=\"cMescheder_et+al_2017_a\" href=\"#rMescheder_et+al_2017_a\"><a class=\"ref-link\" id=\"cMescheder_et+al_2017_a\" href=\"#rMescheder_et+al_2017_a\">Mescheder et al, 2017</a></a>], and the vanilla VAE with Gaussian assumption for the posterior distribution (VAE) [<a class=\"ref-link\" id=\"cKingma_2013_a\" href=\"#rKingma_2013_a\">Kingma and Welling, 2013</a>].",
        "CelebA We use CVB to train a generative model with deep deconvolution network on CelebAdataset for a 64-dimension latent space with N (0, 1) prior [<a class=\"ref-link\" id=\"cMescheder_et+al_2017_a\" href=\"#rMescheder_et+al_2017_a\"><a class=\"ref-link\" id=\"cMescheder_et+al_2017_a\" href=\"#rMescheder_et+al_2017_a\">Mescheder et al, 2017</a></a>].",
        "We propose the coupled variational Bayes, which is designed based on the primal-dual view of ELBO and the optimization embedding technique.",
        "The optimization embedding technique, automatically generates a nonparametric variational distribution and couples it with the original parameters in generative models, which plays a key role in reducing the sample complexity.",
        "Numerical experiments demonstrates the superiority of CVB in approximate ability, computational efficiency, and sample complexity"
    ],
    "headline": "We proposed coupled variational Bayes which exploits the primal-dual view of the evidence lower bound with the variational distribution class generated by an optimization procedure, which is termed optimization embedding",
    "reference_links": [
        {
            "id": "Beck_2003_a",
            "entry": "Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20Mirror%20descent%20and%20nonlinear%20projected%20subgradient%20methods%20for%20convex%20optimization%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20Mirror%20descent%20and%20nonlinear%20projected%20subgradient%20methods%20for%20convex%20optimization%202003"
        },
        {
            "id": "Belanger_et+al_2017_a",
            "entry": "David Belanger, Bishan Yang, and Andrew McCallum. End-to-end learning for structured prediction energy networks. arXiv preprint arXiv:1703.05667, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.05667"
        },
        {
            "id": "Bertsekas_1999_a",
            "entry": "D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, second edition, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Nonlinear%20Programming.%20Athena%20Scientific%201999"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao, Xinying Song, and Li Deng. End-to-end learning of lda by mirror-descent back propagation over a deep architecture. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1765\u20131773. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jianshu%20He%2C%20Ji%20Shen%2C%20Yelong%20Xiao%2C%20Lin%20End-to-end%20learning%20of%20lda%20by%20mirror-descent%20back%20propagation%20over%20a%20deep%20architecture%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Jianshu%20He%2C%20Ji%20Shen%2C%20Yelong%20Xiao%2C%20Lin%20End-to-end%20learning%20of%20lda%20by%20mirror-descent%20back%20propagation%20over%20a%20deep%20architecture%202015"
        },
        {
            "id": "Chien_2018_a",
            "entry": "Jen-Tzung Chien and Chao-Hsi Lee. Deep unfolding for topic models. IEEE transactions on pattern analysis and machine intelligence, 40(2):318\u2013331, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chien%2C%20Jen-Tzung%20Lee%2C%20Chao-Hsi%20Deep%20unfolding%20for%20topic%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chien%2C%20Jen-Tzung%20Lee%2C%20Chao-Hsi%20Deep%20unfolding%20for%20topic%20models%202018"
        },
        {
            "id": "Dai_et+al_0000_a",
            "entry": "Bo Dai, Niao He, Hanjun Dai, and Le Song. Provable bayesian inference via particle mirror descent. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 985\u2013994, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Bo%20He%2C%20Niao%20Dai%2C%20Hanjun%20Song%2C%20Le%20Provable%20bayesian%20inference%20via%20particle%20mirror%20descent",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Bo%20He%2C%20Niao%20Dai%2C%20Hanjun%20Song%2C%20Le%20Provable%20bayesian%20inference%20via%20particle%20mirror%20descent"
        },
        {
            "id": "Dai_et+al_0000_b",
            "entry": "Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions via dual embeddings. CoRR, abs/1607.04579, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04579"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08803"
        },
        {
            "id": "Domke_2012_a",
            "entry": "Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318\u2013326, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Domke%2C%20Justin%20Generic%20methods%20for%20optimization-based%20modeling%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Domke%2C%20Justin%20Generic%20methods%20for%20optimization-based%20modeling%202012"
        },
        {
            "id": "Doucet_et+al_2001_a",
            "entry": "A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte Carlo Methods in Practice. SpringerVerlag, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doucet%2C%20A.%20de%20Freitas%2C%20N.%20Gordon%2C%20N.%20Sequential%20Monte%20Carlo%20Methods%20in%20Practice%202001"
        },
        {
            "id": "Gershman_et+al_2012_a",
            "entry": "Samuel Gershman, Matt Hoffman, and David M. Blei. Nonparametric variational inference. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 663\u2013670, New York, NY, USA, 2012. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gershman%2C%20Samuel%20Hoffman%2C%20Matt%20Blei%2C%20David%20M.%20Nonparametric%20variational%20inference%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gershman%2C%20Samuel%20Hoffman%2C%20Matt%20Blei%2C%20David%20M.%20Nonparametric%20variational%20inference%202012"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Klaus_2017_a",
            "entry": "Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. Neural expectation maximization. In Advances in Neural Information Processing Systems, pages 6694\u20136704, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klaus%20Greff%20Sjoerd%20van%20Steenkiste%20and%20J%C3%BCrgen%20Schmidhuber%20Neural%20expectation%20maximization%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2066946704%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klaus%20Greff%20Sjoerd%20van%20Steenkiste%20and%20J%C3%BCrgen%20Schmidhuber%20Neural%20expectation%20maximization%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2066946704%202017"
        },
        {
            "id": "Hershey_et+al_2014_a",
            "entry": "John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.2574"
        },
        {
            "id": "Hoffman_et+al_2013_a",
            "entry": "Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14:1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20variational%20inference%202013"
        },
        {
            "id": "Jaakkola_1999_a",
            "entry": "Tommi S. Jaakkola and Michael I. Jordon. Learning in graphical models. chapter Improving the Mean Field Approximation via the Use of Mixture Distributions, pages 163\u2013173. MIT Press, Cambridge, MA, USA, 1999. ISBN 0-262-60032-3.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaakkola%2C%20Tommi%20S.%20Jordon%2C%20Michael%20I.%20Learning%20in%20graphical%20models.%20chapter%20Improving%20the%20Mean%20Field%20Approximation%20via%20the%20Use%20of%20Mixture%20Distributions%201999"
        },
        {
            "id": "Jang_et+al_2016_a",
            "entry": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01144"
        },
        {
            "id": "Jordan_et+al_1998_a",
            "entry": "M. I. Jordan, Z. Gharamani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models, pages 105\u2013162. Kluwer Academic, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.I.%20Gharamani%2C%20Z.%20Jaakkola%2C%20T.S.%20Saul%2C%20L.K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20M.I.%20Gharamani%2C%20Z.%20Jaakkola%2C%20T.S.%20Saul%2C%20L.K.%20An%20introduction%20to%20variational%20methods%20for%20graphical%20models%201998"
        },
        {
            "id": "Kim_et+al_2018_a",
            "entry": "Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized variational autoencoders. arXiv preprint arXiv:1802.02550, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02550"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743\u20134751, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016"
        },
        {
            "id": "Liu_2017_a",
            "entry": "Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in neural information processing systems, pages 3118\u20133126, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Stein%20variational%20gradient%20descent%20as%20gradient%20flow%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Stein%20variational%20gradient%20descent%20as%20gradient%20flow%202017"
        },
        {
            "id": "Maddison_2016_a",
            "entry": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "Marino_et+al_2018_a",
            "entry": "Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. In International Conference on Machine Learning, pages 3400\u20133409, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marino%2C%20Joseph%20Yue%2C%20Yisong%20Mandt%2C%20Stephan%20Iterative%20amortized%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marino%2C%20Joseph%20Yue%2C%20Yisong%20Mandt%2C%20Stephan%20Iterative%20amortized%20inference%202018"
        },
        {
            "id": "Mescheder_et+al_2017_a",
            "entry": "Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04722"
        },
        {
            "id": "Minka_2001_a",
            "entry": "T. Minka. Expectation Propagation for approximative Bayesian inference. PhD thesis, MIT Media Labs, Cambridge, USA, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20T.%20Expectation%20Propagation%20for%20approximative%20Bayesian%20inference%202001"
        },
        {
            "id": "Mnih_2014_a",
            "entry": "Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0030"
        },
        {
            "id": "Neal_1993_a",
            "entry": "Radford M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical report, Dept. of Computer Science, University of Toronto, 1993. CRG-TR-93-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Probabilistic%20inference%20using%20Markov%20chain%20Monte%20Carlo%20methods%201993"
        },
        {
            "id": "Neal_2011_a",
            "entry": "Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2(11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Mcmc%20using%20hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20Radford%20M.%20Mcmc%20using%20hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%202011"
        },
        {
            "id": "Nemirovski_et+al_2009_a",
            "entry": "A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. on Optimization, 19(4):1574\u20131609, January 2009. ISSN 1052-6234.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20A.%20Juditsky%2C%20A.%20Lan%2C%20G.%20Shapiro%2C%20A.%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009-01"
        },
        {
            "id": "Otto_2001_a",
            "entry": "Felix Otto. The geometry of dissipative evolution equations: the porous medium equation. 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Otto%2C%20Felix%20The%20geometry%20of%20dissipative%20evolution%20equations%3A%20the%20porous%20medium%20equation%202001"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.05770"
        },
        {
            "id": "Rockafellar_1998_a",
            "entry": "R. T. Rockafellar and R. J-B. Wets. Variational Analysis. Springer Verlag, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20R.T.%20Wets%2C%20R.J.-B.%20Variational%20Analysis%201998"
        },
        {
            "id": "Salimans_et+al_2015_a",
            "entry": "Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pages 1218\u20131226, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20Welling%2C%20Max%20Markov%20chain%20monte%20carlo%20and%20variational%20inference%3A%20Bridging%20the%20gap%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20Welling%2C%20Max%20Markov%20chain%20monte%20carlo%20and%20variational%20inference%3A%20Bridging%20the%20gap%202015"
        },
        {
            "id": "Shapiro_2014_a",
            "entry": "Alexander Shapiro, Darinka Dentcheva, et al. Lectures on stochastic programming: modeling and theory, volume 16. SIAM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapiro%2C%20Alexander%20Dentcheva%2C%20Darinka%20Lectures%20on%20stochastic%20programming%3A%20modeling%20and%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shapiro%2C%20Alexander%20Dentcheva%2C%20Darinka%20Lectures%20on%20stochastic%20programming%3A%20modeling%20and%202014"
        },
        {
            "id": "Stoyanov_et+al_2011_a",
            "entry": "Veselin Stoyanov, Alexander Ropson, and Jason Eisner. Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 725\u2013733, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stoyanov%2C%20Veselin%20Ropson%2C%20Alexander%20Eisner%2C%20Jason%20Empirical%20risk%20minimization%20of%20graphical%20model%20parameters%20given%20approximate%20inference%2C%20decoding%2C%20and%20model%20structure%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stoyanov%2C%20Veselin%20Ropson%2C%20Alexander%20Eisner%2C%20Jason%20Empirical%20risk%20minimization%20of%20graphical%20model%20parameters%20given%20approximate%20inference%2C%20decoding%2C%20and%20model%20structure%202011"
        },
        {
            "id": "Sugiyama_et+al_2012_a",
            "entry": "Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine learning. Cambridge University Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20Masashi%20Suzuki%2C%20Taiji%20Kanamori%2C%20Takafumi%20Density%20ratio%20estimation%20in%20machine%20learning%202012"
        },
        {
            "id": "Titsias_2014_a",
            "entry": "Michalis Titsias and Miguel L\u00e1zaro-gredilla. Doubly stochastic variational bayes for non-conjugate inference. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1971\u20131979. JMLR Workshop and Conference Proceedings, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20Michalis%20L%C3%A1zaro-gredilla%2C%20Miguel%20Doubly%20stochastic%20variational%20bayes%20for%20non-conjugate%20inference%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20Michalis%20L%C3%A1zaro-gredilla%2C%20Miguel%20Doubly%20stochastic%20variational%20bayes%20for%20non-conjugate%20inference%202014"
        },
        {
            "id": "Tomczak_2016_a",
            "entry": "Jakub M Tomczak and Max Welling. Improving variational auto-encoders using householder flow. arXiv preprint arXiv:1611.09630, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.09630"
        },
        {
            "id": "Tran_et+al_2015_a",
            "entry": "Dustin Tran, Rajesh Ranganath, and David M Blei. The variational gaussian process. arXiv preprint arXiv:1511.06499, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06499"
        },
        {
            "id": "Turner_2011_a",
            "entry": "Richard E Turner and Maneesh Sahani. Two problems with variational expectation maximisation for time-series models. Bayesian Time series models, 1(3.1):3\u20131, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20and%20Maneesh%20Sahani.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20and%20Maneesh%20Sahani.%20Two%20problems%20with%20variational%20expectation%20maximisation%20for%20time-series%20models%202011"
        },
        {
            "id": "Wainwright_2003_a",
            "entry": "M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical Report 649, UC Berkeley, Department of Statistics, September 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202003-09"
        },
        {
            "id": "Zellner_1988_a",
            "entry": "Arnold Zellner. Optimal Information Processing and Bayes\u2019s Theorem. The American Statistician, 42(4), November 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zellner%2C%20Arnold%20Optimal%20Information%20Processing%20and%20Bayes%E2%80%99s%20Theorem%201988-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zellner%2C%20Arnold%20Optimal%20Information%20Processing%20and%20Bayes%E2%80%99s%20Theorem%201988-11"
        }
    ]
}
