{
    "filename": "7992-communication-compression-for-decentralized-training.pdf",
    "metadata": {
        "date": 2018,
        "title": "Communication Compression for Decentralized Training",
        "author": "Hanlin Tang1, Shaoduo Gan2, Ce Zhang2, Tong Zhang3, and Ji Liu3,1",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7992-communication-compression-for-decentralized-training.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Optimizing distributed learning systems is an art of balancing between computation and communication. There have been two lines of research that try to deal with slower networks: communication compression for low bandwidth networks, and decentralization for high latency networks. In this paper, We explore a natural question: can the combination of both techniques lead to a system that is robust to both bandwidth and latency?"
    },
    "keywords": [
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "When training machine learning models in a distributed fashion, the underlying constraints of how workers communication have a significant impact on the training algorithm",
        "We propose two decentralized parallel stochastic gradient descent algorithms (D-PSGD): extrapolation compression decentralized parallel stochastic gradient descent (ECD-PSGD) and difference compre\u221assion decentralized parallel stochastic gradient descent (DCD-PSGD)",
        "We show that when the underlying network has both high latency and low bandwidth, both algorithms outperform state-of-the-arts significantly",
        "We studied the problem of combining two tricks of training distributed stochastic gradient descent under imperfect network conditions: quantization and decentralization",
        "We developed two novel algorithms or quantized, decentralized training, analyze the theoretical property of both algorithms, and empirically study their performance in a various settings of network conditions",
        "We found that when the underlying communication networks has both high latency and low bandwidth, quantized, decentralized algorithm outperforms other strategies significantly"
    ],
    "key_statements": [
        "When training machine learning models in a distributed fashion, the underlying constraints of how workers communication have a significant impact on the training algorithm",
        "We propose two decentralized parallel stochastic gradient descent algorithms (D-PSGD): extrapolation compression decentralized parallel stochastic gradient descent (ECD-PSGD) and difference compre\u221assion decentralized parallel stochastic gradient descent (DCD-PSGD)",
        "We show that when the underlying network has both high latency and low bandwidth, both algorithms outperform state-of-the-arts significantly",
        "The full precision is 32 bits, and the compressed precision is 8 bits. In this paper, we omit the comparison with quantized centralized training because the difference between Decentralized 8bits and Centralized 8bits would be similar to the original decentralized training paper Lian et al [2017a] \u2013 when the network latency is high, decentralized algorithm outperforms centralized algorithm in terms of the time for each epoch",
        "We studied the problem of combining two tricks of training distributed stochastic gradient descent under imperfect network conditions: quantization and decentralization",
        "We developed two novel algorithms or quantized, decentralized training, analyze the theoretical property of both algorithms, and empirically study their performance in a various settings of network conditions",
        "We found that when the underlying communication networks has both high latency and low bandwidth, quantized, decentralized algorithm outperforms other strategies significantly"
    ],
    "summary": [
        "When training machine learning models in a distributed fashion, the underlying constraints of how workers communication have a significant impact on the training algorithm.",
        "We propose two decentralized parallel stochastic gradient descent algorithms (D-PSGD): extrapolation compression D-PSGD (ECD-PSGD) and difference compre\u221assion D-PSGD (DCD-PSGD).",
        "(C-PSGD), the central node performs parameter updates and leaf nodes compute stochastic gradients based on local information in parallel.",
        "The decentralized algorithm only assume a connect computational network, without using the central node to collect information from all nodes.",
        "Unlike the traditional parallel stochastic gradient descent (C-PSGD), which requires a central node to compute the 103 average value of all leaf nodes, the decentralized parallel stochastic",
        "Training Loss gradient descent (D-PSGD) algorithm does not need such a central node.",
        "3. Each node updates its local model xt(i) \u2190 x \u2212 \u03b3t\u2207Fi x; \u03bet(i) using stochastic gradient, where \u03b3t is the learning rate.",
        "We introduce two quantized decentralized algorithms that compress information exchanged between nodes.",
        "To reduce the communication cost, a straightforward idea is to compress the information exchanged within the decentralized network just like centralized algorithms sending compressed stochastic gradient [<a class=\"ref-link\" id=\"cAlistarh_et+al_2017_a\" href=\"#rAlistarh_et+al_2017_a\">Alistarh et al, 2017</a>].",
        "Before propose our solutions to this issue, let us first make some common optimization assumptions for analyzing decentralized stochastic algorithms [Lian et al, 2017b].",
        "1 \u2212 3D1L2\u03b32 > 0, under the Assumption 1, , we have the following convergence rate for Algorithm 1: T",
        "Under Assumptions 1 and 2, choosing \u03b3t in Algorithm 2 to be constant \u03b3 satisfying 1 \u2212 6C1L2\u03b32 > 0, we have the following convergence rate for Algorithm 2",
        "We run experiments under diverse network conditions and show that, decentralized algorithms with low precision can speed up training without hurting convergence.",
        "In this paper, we omit the comparison with quantized centralized training because the difference between Decentralized 8bits and Centralized 8bits would be similar to the original decentralized training paper Lian et al [2017a] \u2013 when the network latency is high, decentralized algorithm outperforms centralized algorithm in terms of the time for each epoch.",
        "We can see that with our algorithms, decentralization and compression would not hurt the convergence rate.",
        "We studied the problem of combining two tricks of training distributed stochastic gradient descent under imperfect network conditions: quantization and decentralization.",
        "We developed two novel algorithms or quantized, decentralized training, analyze the theoretical property of both algorithms, and empirically study their performance in a various settings of network conditions.",
        "We found that when the underlying communication networks has both high latency and low bandwidth, quantized, decentralized algorithm outperforms other strategies significantly."
    ],
    "headline": "We explore a natural question: can the combination of both techniques lead to a system that is robust to both bandwidth and latency?",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Agarwal_2011_a",
            "entry": "A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pages 873\u2013881, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Duchi%2C%20J.C.%20Distributed%20delayed%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Duchi%2C%20J.C.%20Distributed%20delayed%20stochastic%20optimization%202011"
        },
        {
            "id": "Alistarh_et+al_2017_a",
            "entry": "D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alistarh%2C%20D.%20Grubic%2C%20D.%20Li%2C%20J.%20Tomioka%2C%20R.%20QSGD%3A%20Communication-Efficient%20SGD%20via%20Gradient%20Quantization%20and%20Encoding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alistarh%2C%20D.%20Grubic%2C%20D.%20Li%2C%20J.%20Tomioka%2C%20R.%20QSGD%3A%20Communication-Efficient%20SGD%20via%20Gradient%20Quantization%20and%20Encoding%202017"
        },
        {
            "id": "Alistarh_et+al_2017_b",
            "entry": "D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1707\u20131718, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alistarh%2C%20D.%20Grubic%2C%20D.%20Li%2C%20J.%20Tomioka%2C%20R.%20Vojnovic.%20Qsgd%3A%20Communication-efficient%20sgd%20via%20gradient%20quantization%20and%20encoding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alistarh%2C%20D.%20Grubic%2C%20D.%20Li%2C%20J.%20Tomioka%2C%20R.%20Vojnovic.%20Qsgd%3A%20Communication-efficient%20sgd%20via%20gradient%20quantization%20and%20encoding%202017"
        },
        {
            "id": "Bottou_2010_a",
            "entry": "L. Bottou. Large-scale machine learning with stochastic gradient descent. Proc. of the International Conference on Computational Statistics (COMPSTAT), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent%202010"
        },
        {
            "id": "Boyd_et+al_2006_a",
            "entry": "S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Randomized gossip algorithms. IEEE/ACM Trans. Netw., 14(SI):2508\u20132530, June 2006. ISSN 1063-6692. doi: 10.1109/TIT.2006.874516.",
            "crossref": "https://dx.doi.org/10.1109/TIT.2006.874516",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/TIT.2006.874516"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.01274"
        },
        {
            "id": "Colin_et+al_2016_a",
            "entry": "I. Colin, A. Bellet, J. Salmon, and S. Cl\u00e9men\u00e7on. Gossip dual averaging for decentralized optimization of pairwise functions. In International Conference on Machine Learning, pages 1388\u20131396, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Colin%2C%20I.%20Bellet%2C%20A.%20Salmon%2C%20J.%20Cl%C3%A9men%C3%A7on%2C%20S.%20Gossip%20dual%20averaging%20for%20decentralized%20optimization%20of%20pairwise%20functions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Colin%2C%20I.%20Bellet%2C%20A.%20Salmon%2C%20J.%20Cl%C3%A9men%C3%A7on%2C%20S.%20Gossip%20dual%20averaging%20for%20decentralized%20optimization%20of%20pairwise%20functions%202016"
        },
        {
            "id": "Sa_et+al_2017_a",
            "entry": "C. De Sa, M. Feldman, C. R\u00e9, and K. Olukotun. Understanding and optimizing asynchronous lowprecision stochastic gradient descent. In Proceedings of the 44th Annual International Symposium on Computer Architecture, pages 561\u2013574. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20C.De%20Feldman%2C%20M.%20R%C3%A9%2C%20C.%20Olukotun%2C%20K.%20Understanding%20and%20optimizing%20asynchronous%20lowprecision%20stochastic%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20C.De%20Feldman%2C%20M.%20R%C3%A9%2C%20C.%20Olukotun%2C%20K.%20Understanding%20and%20optimizing%20asynchronous%20lowprecision%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "Dekel_et+al_2012_a",
            "entry": "O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13(Jan):165\u2013202, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dekel%2C%20O.%20Gilad-Bachrach%2C%20R.%20Shamir%2C%20O.%20Xiao%2C%20L.%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dekel%2C%20O.%20Gilad-Bachrach%2C%20R.%20Shamir%2C%20O.%20Xiao%2C%20L.%20Optimal%20distributed%20online%20prediction%20using%20mini-batches%202012"
        },
        {
            "id": "Dobbe_et+al_2017_a",
            "entry": "R. Dobbe, D. Fridovich-Keil, and C. Tomlin. Fully decentralized policies for multi-agent systems: An information theoretic approach. In Advances in Neural Information Processing Systems, pages 2945\u20132954, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dobbe%2C%20R.%20Fridovich-Keil%2C%20D.%20Tomlin%2C%20C.%20Fully%20decentralized%20policies%20for%20multi-agent%20systems%3A%20An%20information%20theoretic%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dobbe%2C%20R.%20Fridovich-Keil%2C%20D.%20Tomlin%2C%20C.%20Fully%20decentralized%20policies%20for%20multi-agent%20systems%3A%20An%20information%20theoretic%20approach%202017"
        },
        {
            "id": "Ghadimi_2013_a",
            "entry": "S. Ghadimi and G. Lan. Stochastic firstand zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013. doi: 10.1137/120880811.",
            "crossref": "https://dx.doi.org/10.1137/120880811",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/120880811"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Kashyap_et+al_2007_a",
            "entry": "A. Kashyap, T. Basar, and R. Srikant. Quantized consensus. Automatica, 43(7):1192\u20131203, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kashyap%2C%20A.%20Basar%2C%20T.%20Srikant%2C%20R.%20Quantized%20consensus%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kashyap%2C%20A.%20Basar%2C%20T.%20Srikant%2C%20R.%20Quantized%20consensus%202007"
        },
        {
            "id": "Richt_2016_a",
            "entry": "J. Konecnyand P. Richt\u00e1rik. Randomized distributed mean estimation: Accuracy vs communication. arXiv preprint arXiv:1611.07555, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.07555"
        },
        {
            "id": "Lan_et+al_2017_a",
            "entry": "G. Lan, S. Lee, and Y. Zhou. Communication-efficient algorithms for decentralized and stochastic optimization. 01 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20G.%20Lee%2C%20S.%20Zhou%2C%20Y.%20Communication-efficient%20algorithms%20for%20decentralized%20and%20stochastic%20optimization%202017"
        },
        {
            "id": "Lavaei_2012_a",
            "entry": "J. Lavaei and R. M. Murray. Quantized consensus by means of gossip algorithm. IEEE Transactions on Automatic Control, 57(1):19\u201332, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lavaei%2C%20J.%20Murray%2C%20R.M.%20Quantized%20consensus%20by%20means%20of%20gossip%20algorithm%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lavaei%2C%20J.%20Murray%2C%20R.M.%20Quantized%20consensus%20by%20means%20of%20gossip%20algorithm%202012"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Z. Li, W. Shi, and M. Yan. A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates. arXiv preprint arXiv:1704.07807, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.07807"
        },
        {
            "id": "Lian_et+al_2015_a",
            "entry": "X. Lian, Y. Huang, Y. Li, and J. Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737\u20132745, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Huang%2C%20Y.%20Li%2C%20Y.%20Liu%2C%20J.%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20X.%20Huang%2C%20Y.%20Li%2C%20Y.%20Liu%2C%20J.%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015"
        },
        {
            "id": "Lian_et+al_0000_a",
            "entry": "X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. 05 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Zhang%2C%20C.%20Zhang%2C%20H.%20Hsieh%2C%20C.-J.%20Can%20decentralized%20algorithms%20outperform%20centralized%20algorithms%3F%20a%20case%20study%20for%20decentralized%20parallel%20stochastic%20gradient%20descent"
        },
        {
            "id": "Lian_et+al_0000_b",
            "entry": "X. Lian, W. Zhang, C. Zhang, and J. Liu. Asynchronous decentralized parallel stochastic gradient descent. 10 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20X.%20Zhang%2C%20W.%20Zhang%2C%20C.%20Liu%2C%20J.%20Asynchronous%20decentralized%20parallel%20stochastic%20gradient%20descent"
        },
        {
            "id": "Mhamdi_et+al_2017_a",
            "entry": "E. Mhamdi, E. Mahdi, H. Hendrikx, R. Guerraoui, and A. D. O. Maurer. Dynamic safe interruptibility for decentralized multi-agent reinforcement learning. Technical report, EPFL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhamdi%2C%20E.%20Mahdi%2C%20E.%20Hendrikx%2C%20H.%20Guerraoui%2C%20R.%20Dynamic%20safe%20interruptibility%20for%20decentralized%20multi-agent%20reinforcement%20learning%202017"
        },
        {
            "id": "Moulines_2011_a",
            "entry": "E. Moulines and F. R. Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 451\u2013459. Curran Associates, Inc., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moulines%2C%20E.%20Bach%2C%20F.R.%20Non-asymptotic%20analysis%20of%20stochastic%20approximation%20algorithms%20for%20machine%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moulines%2C%20E.%20Bach%2C%20F.R.%20Non-asymptotic%20analysis%20of%20stochastic%20approximation%20algorithms%20for%20machine%20learning%202011"
        },
        {
            "id": "Nedic_2009_a",
            "entry": "A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48\u201361, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20A.%20Ozdaglar%2C%20A.%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20A.%20Ozdaglar%2C%20A.%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009"
        },
        {
            "id": "Nedic_et+al_2009_b",
            "entry": "A. Nedic, A. Olshevsky, A. Ozdaglar, and J. N. Tsitsiklis. On distributed averaging algorithms and quantization effects. IEEE Transactions on Automatic Control, 54(11):2506\u20132517, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20A.%20Olshevsky%2C%20A.%20Ozdaglar%2C%20A.%20Tsitsiklis%2C%20J.N.%20On%20distributed%20averaging%20algorithms%20and%20quantization%20effects%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20A.%20Olshevsky%2C%20A.%20Ozdaglar%2C%20A.%20Tsitsiklis%2C%20J.N.%20On%20distributed%20averaging%20algorithms%20and%20quantization%20effects%202009"
        },
        {
            "id": "Nemirovski_et+al_2009_a",
            "entry": "A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009. doi: 10.1137/ 070704277.",
            "crossref": "https://dx.doi.org/10.1137/070704277",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/070704277"
        },
        {
            "id": "Omidshafiei_et+al_2017_a",
            "entry": "S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-agent rl under partial observability. arXiv preprint arXiv:1703.06182, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06182"
        },
        {
            "id": "Recht_et+al_2011_a",
            "entry": "B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693\u2013701, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20B.%20Re%2C%20C.%20Wright%2C%20S.%20Niu%2C%20F.%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20B.%20Re%2C%20C.%20Wright%2C%20S.%20Niu%2C%20F.%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "Seide_2016_a",
            "entry": "F. Seide and A. Agarwal. Cntk: Microsoft\u2019s open-source deep-learning toolkit. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pages 2135\u20132135, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2945397.",
            "crossref": "https://dx.doi.org/10.1145/2939672.2945397",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2939672.2945397"
        },
        {
            "id": "Shi_et+al_2015_a",
            "entry": "W. Shi, Q. Ling, G. Wu, and W. Yin. A proximal gradient algorithm for decentralized composite optimization. IEEE Transactions on Signal Processing, 63(22):6013\u20136023, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20W.%20Ling%2C%20Q.%20Wu%2C%20G.%20Yin%2C%20W.%20A%20proximal%20gradient%20algorithm%20for%20decentralized%20composite%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20W.%20Ling%2C%20Q.%20Wu%2C%20G.%20Yin%2C%20W.%20A%20proximal%20gradient%20algorithm%20for%20decentralized%20composite%20optimization%202015"
        },
        {
            "id": "Sirb_2016_a",
            "entry": "B. Sirb and X. Ye. Consensus optimization with delayed and stochastic gradients on decentralized networks. In 2016 IEEE International Conference on Big Data (Big Data), pages 76\u201385, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sirb%2C%20B.%20Ye%2C%20X.%20Consensus%20optimization%20with%20delayed%20and%20stochastic%20gradients%20on%20decentralized%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sirb%2C%20B.%20Ye%2C%20X.%20Consensus%20optimization%20with%20delayed%20and%20stochastic%20gradients%20on%20decentralized%20networks%202016"
        },
        {
            "id": "Suresh_et+al_2017_a",
            "entry": "A. T. Suresh, F. X. Yu, S. Kumar, and H. B. McMahan. Distributed mean estimation with limited communication. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3329\u20133337, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/suresh17a.html.",
            "url": "http://proceedings.mlr.press/v70/suresh17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suresh%2C%20A.T.%20Yu%2C%20F.X.%20Kumar%2C%20S.%20McMahan%2C%20H.B.%20Distributed%20mean%20estimation%20with%20limited%20communication%202017-08"
        },
        {
            "id": "Wangni_et+al_2017_a",
            "entry": "J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for communication-efficient distributed optimization. arXiv preprint arXiv:1710.09854, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09854"
        },
        {
            "id": "Yuan_et+al_2016_a",
            "entry": "K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835\u20131854, 2016. doi: 10.1137/130943170.",
            "crossref": "https://dx.doi.org/10.1137/130943170",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/130943170"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang. Zipml: Training linear models with end-to-end low precision, and a little bit of deep learning. In International Conference on Machine Learning, pages 4035\u20134043, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Li%2C%20J.%20Kara%2C%20K.%20Alistarh%2C%20D.%20Zipml%3A%20Training%20linear%20models%20with%20end-to-end%20low%20precision%2C%20and%20a%20little%20bit%20of%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Li%2C%20J.%20Kara%2C%20K.%20Alistarh%2C%20D.%20Zipml%3A%20Training%20linear%20models%20with%20end-to-end%20low%20precision%2C%20and%20a%20little%20bit%20of%20deep%20learning%202017"
        },
        {
            "id": "Zhang_et+al_2017_b",
            "entry": "W. Zhang, P. Zhao, W. Zhu, S. C. Hoi, and T. Zhang. Projection-free distributed online learning in networks. In International Conference on Machine Learning, pages 4054\u20134062, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20W.%20Zhao%2C%20P.%20Zhu%2C%20W.%20Hoi%2C%20S.C.%20Projection-free%20distributed%20online%20learning%20in%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20W.%20Zhao%2C%20P.%20Zhu%2C%20W.%20Hoi%2C%20S.C.%20Projection-free%20distributed%20online%20learning%20in%20networks%202017"
        },
        {
            "id": "Zhao_2016_a",
            "entry": "L. Zhao and W. Song. Decentralized consensus in distributed networks. International Journal of Parallel, Emergent and Distributed Systems, pages 1\u201320, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20L.%20Song%2C%20W.%20Decentralized%20consensus%20in%20distributed%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20L.%20Song%2C%20W.%20Decentralized%20consensus%20in%20distributed%20networks%202016"
        },
        {
            "id": "Zheng_et+al_2016_a",
            "entry": "S. Zheng, Q. Meng, T. Wang, W. Chen, N. Yu, Z.-M. Ma, and T.-Y. Liu. Asynchronous stochastic gradient descent with delay compensation for distributed deep learning. arXiv preprint arXiv:1609.08326, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08326"
        },
        {
            "id": "Zinkevich_et+al_2010_a",
            "entry": "M. Zinkevich, M. Weimer, L. Li, and A. J. Smola. Parallelized stochastic gradient descent. In Advances in neural information processing systems, pages 2595\u20132603, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20M.%20Weimer%2C%20M.%20Li%2C%20L.%20Smola%2C%20A.J.%20Parallelized%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20M.%20Weimer%2C%20M.%20Li%2C%20L.%20Smola%2C%20A.J.%20Parallelized%20stochastic%20gradient%20descent%202010"
        }
    ]
}
