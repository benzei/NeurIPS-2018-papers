{
    "filename": "8066-teaching-inverse-reinforcement-learners-via-features-and-demonstrations.pdf",
    "metadata": {
        "title": "Teaching Inverse Reinforcement Learners via Features and Demonstrations",
        "author": "Luis Haug, Sebastian Tschiatschek, Adish Singla",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8066-teaching-inverse-reinforcement-learners-via-features-and-demonstrations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning near-optimal behaviour from an expert\u2019s demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner\u2019s worldview, and thus ultimately enable her to find a near-optimal policy."
    },
    "keywords": [
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "NIPS",
            "url": "https://en.wikipedia.org/wiki/NIPS"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        }
    ],
    "highlights": [
        "Reinforcement learning has recently led to impressive and widely recognized results in several challenging application domains, including game-play, e.g., of classical games (Go) and Atari games",
        "Examples for algorithms that can be used to that end are those in [<a class=\"ref-link\" id=\"cAbbeel_2004_a\" href=\"#rAbbeel_2004_a\">Abbeel and Ng, 2004</a>] and [<a class=\"ref-link\" id=\"cZiebart_et+al_2008_a\" href=\"#rZiebart_et+al_2008_a\">Ziebart et al, 2008</a>], which use inverse reinforcement learning (IRL) to estimate a reward function for which the demonstrated behaviour is optimal, and derive a policy based on that",
        "We presented an approach to dealing with the problem of worldview mismatch in situations in which a learner attempts to find a policy matching the feature counts of a teacher\u2019s demonstrations",
        "We introduced the teaching risk, a quantity that depends on the worldview of the learner and the true reward function and which (1) measures the degree to which policies which are optimal from the point of view of the learner can be suboptimal from the point of view of the teacher, and (2) is an obstruction for truly optimal policies to look optimal to the learner",
        "We showed that under the condition that the teaching risk is small, a learner matching feature counts using, e.g., standard inverse reinforcement learning-based methods is guaranteed to learn a near-optimal policy from demonstrations of the teacher even under worldview mismatch",
        "We presented our teaching algorithm TRGREEDY, in which the teacher updates the learner\u2019s worldview by teaching her features which are relevant for the true reward function in a way that greedily minimizes the teaching risk, and provides her with demostrations based on which she learns a policy using any suitable algorithm"
    ],
    "key_statements": [
        "Reinforcement learning has recently led to impressive and widely recognized results in several challenging application domains, including game-play, e.g., of classical games (Go) and Atari games",
        "Examples for algorithms that can be used to that end are those in [<a class=\"ref-link\" id=\"cAbbeel_2004_a\" href=\"#rAbbeel_2004_a\">Abbeel and Ng, 2004</a>] and [<a class=\"ref-link\" id=\"cZiebart_et+al_2008_a\" href=\"#rZiebart_et+al_2008_a\">Ziebart et al, 2008</a>], which use inverse reinforcement learning (IRL) to estimate a reward function for which the demonstrated behaviour is optimal, and derive a policy based on that",
        "We presented an approach to dealing with the problem of worldview mismatch in situations in which a learner attempts to find a policy matching the feature counts of a teacher\u2019s demonstrations",
        "We introduced the teaching risk, a quantity that depends on the worldview of the learner and the true reward function and which (1) measures the degree to which policies which are optimal from the point of view of the learner can be suboptimal from the point of view of the teacher, and (2) is an obstruction for truly optimal policies to look optimal to the learner",
        "We showed that under the condition that the teaching risk is small, a learner matching feature counts using, e.g., standard inverse reinforcement learning-based methods is guaranteed to learn a near-optimal policy from demonstrations of the teacher even under worldview mismatch",
        "We presented our teaching algorithm TRGREEDY, in which the teacher updates the learner\u2019s worldview by teaching her features which are relevant for the true reward function in a way that greedily minimizes the teaching risk, and provides her with demostrations based on which she learns a policy using any suitable algorithm"
    ],
    "summary": [
        "Reinforcement learning has recently led to impressive and widely recognized results in several challenging application domains, including game-play, e.g., of classical games (Go) and Atari games.",
        "Note that R(\u03c0\u2217) > R(\u03c00) > R(\u03c00) > R(\u03c0\u2217), and \u03c00 is a better teaching policy than \u03c0\u2217 regarding the performance that a learner matching feature expectations in her worldview achieves in the worst case.",
        "The discussion in the last section shows that, under our assumptions on how L learns, a teaching scheme in which T solely provides demonstrations to L can generally, i.e., without any assumption on the teaching risk, not lead to reasonable guarantees on the learner\u2019s performance with respect to the true reward.",
        "We used a gridworld with N = 20, n = 2; for each value \u2208 [1, 100], we sampled five random worldview matrices AL \u2208 R \u00d7100, and let L train a policy \u03c0L using the projection algorithm in [<a class=\"ref-link\" id=\"cAbbeel_2004_a\" href=\"#rAbbeel_2004_a\">Abbeel and Ng, 2004</a>], with the goal of matching the feature expectations \u03bc corresponding to an optimal policy \u03c0T for a reward vector w\u2217 that was sampled randomly in each round.",
        "The plots show that the bound for this gap provided in Theorem 1 is overly conservative in general, given that L\u2019s performance is often high and has small variance even if the teaching risk is relatively large.",
        "The plots in Figure 5 show, for each of the three algorithms, the relative performance with respect to the true reward function s \u2192 w\u2217, \u03c6(s) that the learner achieved after each round of feature teaching and training a policy \u03c0L, as well as the corresponding teaching risks and runtimes, plotted over the number of features taught.",
        "We introduced the teaching risk, a quantity that depends on the worldview of the learner and the true reward function and which (1) measures the degree to which policies which are optimal from the point of view of the learner can be suboptimal from the point of view of the teacher, and (2) is an obstruction for truly optimal policies to look optimal to the learner.",
        "We showed that under the condition that the teaching risk is small, a learner matching feature counts using, e.g., standard IRL-based methods is guaranteed to learn a near-optimal policy from demonstrations of the teacher even under worldview mismatch.",
        "We presented our teaching algorithm TRGREEDY, in which the teacher updates the learner\u2019s worldview by teaching her features which are relevant for the true reward function in a way that greedily minimizes the teaching risk, and provides her with demostrations based on which she learns a policy using any suitable algorithm.",
        "We found that TRGREEDY performed comparably to a variant which selected features based on greedily maximizing performance, and consistently better than a variant with randomly selected features"
    ],
    "headline": "We study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert",
    "reference_links": [
        {
            "id": "Abbeel_2004_a",
            "entry": "[Abbeel and Ng, 2004] Abbeel, P. and Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In ICML.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20P.%20Ng%2C%20A.Y.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning.%20In%20ICML%202004"
        },
        {
            "id": "Aodha_et+al_2018_a",
            "entry": "[Aodha et al., 2018] Aodha, O. M., Su, S., Chen, Y., Perona, P., and Yue, Y. (2018). Teaching categories to human learners with visual explanations. In CVPR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aodha%2C%20O.M.%20Su%2C%20S.%20Chen%2C%20Y.%20Perona%2C%20P.%20Teaching%20categories%20to%20human%20learners%20with%20visual%20explanations.%20In%20CVPR%202018"
        },
        {
            "id": "Brown_2018_a",
            "entry": "[Brown and Niekum, 2018] Brown, D. S. and Niekum, S. (2018). Machine teaching for inverse reinforcement learning: Algorithms and applications. CoRR, abs/1805.07687.",
            "arxiv_url": "https://arxiv.org/pdf/1805.07687"
        },
        {
            "id": "Cakmak_2012_a",
            "entry": "[Cakmak et al., 2012] Cakmak, M., Lopes, M., et al. (2012). Algorithmic and human teaching of sequential decision tasks. In AAAI.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cakmak%2C%20M.%20Lopes%2C%20M.%20Algorithmic%20and%20human%20teaching%20of%20sequential%20decision%20tasks.%20In%20AAAI%202012"
        },
        {
            "id": "Cakmak_2014_a",
            "entry": "[Cakmak and Thomaz, 2014] Cakmak, M. and Thomaz, A. L. (2014). Eliciting good teaching from humans for machine learners. Artificial Intelligence, 217:198\u2013215.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cakmak%2C%20M.%20Thomaz%2C%20A.L.%20Eliciting%20good%20teaching%20from%20humans%20for%20machine%20learners%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cakmak%2C%20M.%20Thomaz%2C%20A.L.%20Eliciting%20good%20teaching%20from%20humans%20for%20machine%20learners%202014"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "[Chen et al., 2018] Chen, Y., Singla, A., Mac Aodha, O., Perona, P., and Yue, Y. (2018). Understanding the role of adaptivity in machine teaching: The case of version space learners. In NIPS.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Singla%2C%20A.%20Mac%20Aodha%2C%20O.%20Perona%2C%20P.%20Understanding%20the%20role%20of%20adaptivity%20in%20machine%20teaching%3A%20The%20case%20of%20version%20space%20learners.%20In%20NIPS%202018"
        },
        {
            "id": "Haug_et+al_2018_a",
            "entry": "[Haug et al., 2018] Haug, L., Tschiatschek, S., and Singla, A. (2018). Teaching inverse reinforcement learners via features and demonstrations. CoRR, abs/1810.08926.",
            "arxiv_url": "https://arxiv.org/pdf/1810.08926"
        },
        {
            "id": "Hunziker_et+al_2018_a",
            "entry": "[Hunziker et al., 2018] Hunziker, A., Chen, Y., Mac Aodha, O., Gomez-Rodriguez, M., Krause, A., Perona, P., Yue, Y., and Singla, A. (2018). Teaching multiple concepts to a forgetful learner. CoRR, abs/1805.08322.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08322"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "[Liu et al., 2017] Liu, W., Dai, B., Humayun, A., Tay, C., Yu, C., Smith, L. B., Rehg, J. M., and Song, L. (2017). Iterative machine teaching. In ICML, pages 2149\u20132158.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20W.%20Dai%2C%20B.%20Humayun%2C%20A.%20Tay%2C%20C.%20Iterative%20machine%20teaching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20W.%20Dai%2C%20B.%20Humayun%2C%20A.%20Tay%2C%20C.%20Iterative%20machine%20teaching%202017"
        },
        {
            "id": "Mayer_et+al_2017_a",
            "entry": "[Mayer et al., 2017] Mayer, M., Hamza, J., and Kuncak, V. (2017). Proactive synthesis of recursive tree-to-string functions from examples (artifact). In DARTS-Dagstuhl Artifacts Series, volume 3. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mayer%2C%20M.%20Hamza%2C%20J.%20Kuncak%2C%20V.%20Proactive%20synthesis%20of%20recursive%20tree-to-string%20functions%20from%20examples%20%28artifact%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mayer%2C%20M.%20Hamza%2C%20J.%20Kuncak%2C%20V.%20Proactive%20synthesis%20of%20recursive%20tree-to-string%20functions%20from%20examples%20%28artifact%29%202017"
        },
        {
            "id": "Mei_2015_a",
            "entry": "[Mei and Zhu, 2015] Mei, S. and Zhu, X. (2015). Using machine teaching to identify optimal training-set attacks on machine learners. In AAAI, pages 2871\u20132877.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20S.%20Zhu%2C%20X.%20Using%20machine%20teaching%20to%20identify%20optimal%20training-set%20attacks%20on%20machine%20learners%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20S.%20Zhu%2C%20X.%20Using%20machine%20teaching%20to%20identify%20optimal%20training-set%20attacks%20on%20machine%20learners%202015"
        },
        {
            "id": "Patil_2014_a",
            "entry": "[Patil et al., 2014] Patil, K. R., Zhu, X., Kopec, \u0141., and Love, B. C. (2014). Optimal teaching for limited-capacity human learners. In NIPS, pages 2465\u20132473.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Optimal%20teaching%20for%20limited-capacity%20human%20learners%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Optimal%20teaching%20for%20limited-capacity%20human%20learners%202014"
        },
        {
            "id": "Rafferty_et+al_2016_a",
            "entry": "[Rafferty et al., 2016] Rafferty, A. N., Brunskill, E., Griffiths, T. L., and Shafto, P. (2016). Faster teaching via pomdp planning. Cognitive science, 40(6):1290\u20131332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rafferty%2C%20A.N.%20Brunskill%2C%20E.%20Griffiths%2C%20T.L.%20Shafto%2C%20P.%20Faster%20teaching%20via%20pomdp%20planning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rafferty%2C%20A.N.%20Brunskill%2C%20E.%20Griffiths%2C%20T.L.%20Shafto%2C%20P.%20Faster%20teaching%20via%20pomdp%20planning%202016"
        },
        {
            "id": "Sermanet_et+al_2018_a",
            "entry": "[Sermanet et al., 2018] Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S., and Brain, G. (2018). Time-contrastive networks: Self-supervised learning from video. In ICRA, pages 1134\u20131141.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sermanet%2C%20P.%20Lynch%2C%20C.%20Chebotar%2C%20Y.%20Hsu%2C%20J.%20Time-contrastive%20networks%3A%20Self-supervised%20learning%20from%20video%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sermanet%2C%20P.%20Lynch%2C%20C.%20Chebotar%2C%20Y.%20Hsu%2C%20J.%20Time-contrastive%20networks%3A%20Self-supervised%20learning%20from%20video%202018"
        },
        {
            "id": "Singla_et+al_2013_a",
            "entry": "[Singla et al., 2013] Singla, A., Bogunovic, I., Bart\u00f3k, G., Karbasi, A., and Krause, A. (2013). On actively teaching the crowd to classify. In NIPS Workshop on Data Driven Education.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singla%2C%20A.%20Bogunovic%2C%20I.%20Bart%C3%B3k%2C%20G.%20Karbasi%2C%20A.%20On%20actively%20teaching%20the%20crowd%20to%20classify.%20In%20NIPS%20Workshop%20on%20Data%20Driven%20Education%202013"
        },
        {
            "id": "Singla_et+al_2014_a",
            "entry": "[Singla et al., 2014] Singla, A., Bogunovic, I., Bart\u00f3k, G., Karbasi, A., and Krause, A. (2014). Near-optimally teaching the crowd to classify. In ICML, pages 154\u2013162.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singla%2C%20A.%20Bogunovic%2C%20I.%20Bart%C3%B3k%2C%20G.%20Karbasi%2C%20A.%20Near-optimally%20teaching%20the%20crowd%20to%20classify%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singla%2C%20A.%20Bogunovic%2C%20I.%20Bart%C3%B3k%2C%20G.%20Karbasi%2C%20A.%20Near-optimally%20teaching%20the%20crowd%20to%20classify%202014"
        },
        {
            "id": "Stadie_et+al_2017_a",
            "entry": "[Stadie et al., 2017] Stadie, B. C., Abbeel, P., and Sutskever, I. (2017). Third-person imitation learning. CoRR, abs/1703.01703.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01703"
        },
        {
            "id": "Zhu_2015_a",
            "entry": "[Zhu, 2015] Zhu, X. (2015). Machine teaching: An inverse problem to machine learning and an approach toward optimal education. In AAAI, pages 4083\u20134087.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20X.%20Machine%20teaching%3A%20An%20inverse%20problem%20to%20machine%20learning%20and%20an%20approach%20toward%20optimal%20education%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20X.%20Machine%20teaching%3A%20An%20inverse%20problem%20to%20machine%20learning%20and%20an%20approach%20toward%20optimal%20education%202015"
        },
        {
            "id": "Zhu_et+al_2018_a",
            "entry": "[Zhu et al., 2018] Zhu, X., Singla, A., Zilles, S., and Rafferty, A. N. (2018). An overview of machine teaching. CoRR, abs/1801.05927.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05927"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "[Ziebart et al., 2008] Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K. (2008). Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433\u20131438.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        }
    ]
}
