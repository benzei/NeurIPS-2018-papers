{
    "filename": "8266-thermostat-assisted-continuously-tempered-hamiltonian-monte-carlo-for-bayesian-learning.pdf",
    "metadata": {
        "title": "Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning",
        "author": "Rui Luo, Jianhong Wang, Yaodong Yang, Jun WANG, Zhanxing Zhu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8266-thermostat-assisted-continuously-tempered-hamiltonian-monte-carlo-for-bayesian-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we propose a novel sampling method, the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo, for the purpose of multimodal Bayesian learning. It simulates a noisy dynamical system by incorporating both a continuously-varying tempering variable and the Nos\u00e9-Hoover thermostats. A significant benefit is that it is not only able to efficiently generate i.i.d. samples when the underlying posterior distributions are multimodal, but also capable of adaptively neutralising the noise arising from the use of mini-batches. While the properties of the approach have been studied using synthetic datasets, our experiments on three real datasets have also shown its performance gains over several strong baselines for Bayesian learning with various types of neural networks plunged in."
    },
    "keywords": [
        {
            "term": "effective sample size",
            "url": "https://en.wikipedia.org/wiki/effective_sample_size"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "Central Limit Theorem",
            "url": "https://en.wikipedia.org/wiki/Central_Limit_Theorem"
        },
        {
            "term": "hamiltonian monte carlo",
            "url": "https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo"
        },
        {
            "term": "bayesian learning",
            "url": "https://en.wikipedia.org/wiki/bayesian_learning"
        },
        {
            "term": "Markov chain Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"
        },
        {
            "term": "TACT",
            "url": "https://en.wikipedia.org/wiki/Tact"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "Metropolis-adjusted Langevin algorithm",
            "url": "https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "stochastic gradient Langevin dynamics",
            "url": "https://en.wikipedia.org/wiki/Stochastic_Gradient_Langevin_Dynamics"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        }
    ],
    "highlights": [
        "Bayesian learning with Markov chain Monte Carlo (MCMC) methods is appealing for its inborn nature to capture the uncertainty of the learned parameters",
        "We address these issues by proposing a new sampling method named as the thermostatassisted continuously-tempered Hamiltonian Monte Carlo for multimodal posterior sampling on large datasets",
        "We propose a sampling method, called the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo (TACT-HMC), for multimodal posterior sampling in the presence of unknown noise",
        "We proposed a novel sampling method named as the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for multimodal Bayesian learning",
        "It incorporates the continuously-varying tempering variable and the Nos\u00e9-Hoover thermostats in simulating the noisy dynamical system. This method is developed for two substantial demands: first, to efficiently generate i.i.d. samples from complex multimodal posterior distributions; second, to adaptively neutralise the noise arising from the use of mini-batches",
        "The results validate the efficacy of the tempering and the thermostats techniques we adopted, and show great potentials for being applied to Bayesian learning tasks on complex models such as neural networks"
    ],
    "key_statements": [
        "Bayesian learning with Markov chain Monte Carlo (MCMC) methods is appealing for its inborn nature to capture the uncertainty of the learned parameters",
        "When the number of modes is large, the \u201cdistant\u201d modes could beyond the reach of any closest modes; this would lead to the so-called pseudo-convergence [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], where the ergodicity guarantee of Markov chain Monte Carlo methods breaks",
        "Bayesian learning on large datasets is typically conducted in an online setting: at each iteration, only a mini-batch of data are used to update the model [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "We address these issues by proposing a new sampling method named as the thermostatassisted continuously-tempered Hamiltonian Monte Carlo for multimodal posterior sampling on large datasets",
        "We introduce a set of Nos\u00e9-Hoover thermostats [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] in order to deal with the additional noise from mini-batches",
        "We introduce a novel systematic approach of continuous tempering brought from the recent advances in physics [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] and chemistry [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]: the original system for HMC is extended and coupled with additional degrees of freedom, i.e. the tempering variable and its the conjugate momentum",
        "We propose a sampling method, called the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo (TACT-HMC), for multimodal posterior sampling in the presence of unknown noise",
        "(b) I: Sampling trajectory of TACT-HMC, presenting a robust mixing property; II: Cumulative averages of thermostats, indicating fast convergence to the theoretical reference values drawn by red lines; III: Histograms of sampled thermostats, showing a good fit to the theoretical distributions in blue curves; IV: Autocorrelation plot of samples, the decreasing of sample autocorrelation is significantly fast; V: (A snapshot of) the variation on effective system temperature during simulation, with the standard reference of unity temperature marked in red.\n4 Related work",
        "continuously-tempered HMC extends the Hamiltonian system with an extra Gibbs continuous tempering variate that empowers the dynamics to explore and mix between isolated modes in the distributions of interest. Such tempering variable is updated with a gradient-based HMC dynamics, requiring the information from the full batch data is unable to deal with the noise introduced by the mini-batch sampling",
        "TACT-HMC combines continuous tempering as well as the the Nos\u00e9-Hoover thermostats to tackle both the local trapping issue and the noise introduced by mini-batches, it overcomes the limitations of all listed sampling methods",
        "We find that TACT-HMC recovers those multiple modes for both distributions while neutralising the influence of the gradient noise; the samplings from the ablated alternatives are impaired either by the gradient noise or by the energy barriers as discovered in the 1d scenario.\n5.2",
        "When the random permutation is applied to a greater portion of training labels, the landscape of the objective function becomes rougher and the system dynamics gathers more noise as a result; we find that, TACT-HMC still maintains robust performance on the accuracy of classification",
        "We proposed a novel sampling method named as the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for multimodal Bayesian learning",
        "It incorporates the continuously-varying tempering variable and the Nos\u00e9-Hoover thermostats in simulating the noisy dynamical system. This method is developed for two substantial demands: first, to efficiently generate i.i.d. samples from complex multimodal posterior distributions; second, to adaptively neutralise the noise arising from the use of mini-batches",
        "The results validate the efficacy of the tempering and the thermostats techniques we adopted, and show great potentials for being applied to Bayesian learning tasks on complex models such as neural networks"
    ],
    "summary": [
        "Bayesian learning with Markov chain Monte Carlo (MCMC) methods is appealing for its inborn nature to capture the uncertainty of the learned parameters.",
        "We propose a sampling method, called the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo (TACT-HMC), for multimodal posterior sampling in the presence of unknown noise.",
        "We incorporate a set of independent Nos\u00e9-Hoover thermostats [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] \u2013 apparatuses originally devised for temperature stabilisation in molecular dynamics simulations \u2013 to adaptively cancel the effect of noise.",
        "(b) I: Sampling trajectory of TACT-HMC, presenting a robust mixing property; II: Cumulative averages of thermostats, indicating fast convergence to the theoretical reference values drawn by red lines; III: Histograms of sampled thermostats, showing a good fit to the theoretical distributions in blue curves; IV: Autocorrelation plot of samples, the decreasing of sample autocorrelation is significantly fast; V: (A snapshot of) the variation on effective system temperature during simulation, with the standard reference of unity temperature marked in red.",
        "Proposed for temperature stabilisation in molecular dynamics simulations, the Nos\u00e9-Hoover thermostat has been proved, when properly configured, being able to automatically adapt to the unknown noise injected into the Hamiltonian system [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "Such tempering variable is updated with a gradient-based HMC dynamics, requiring the information from the full batch data is unable to deal with the noise introduced by the mini-batch sampling.",
        "TACT-HMC combines continuous tempering as well as the the Nos\u00e9-Hoover thermostats to tackle both the local trapping issue and the noise introduced by mini-batches, it overcomes the limitations of all listed sampling methods.",
        "For TACT-HMC, we augment Nos\u00e9-Hoover thermostats to the weights and biases: for each layer, we define two independent scalar variables for the weight and bias of that layer, respectively.",
        "[\u03b7\u03b8, c\u03b8, \u03b3\u03b8 ] denote the stepsize, the level of injected Gaussian noise and the thermal inertia, all w.r.t. the parameter of interest \u03b8; [\u03b7\u03be, c\u03be, \u03b3\u03be ] represent the corresponding quantities for the tempering variable \u03be; K defines the number of steps in simulating a unit interval.",
        "When the random permutation is applied to a greater portion of training labels, the landscape of the objective function becomes rougher and the system dynamics gathers more noise as a result; we find that, TACT-HMC still maintains robust performance on the accuracy of classification.",
        "We proposed a novel sampling method named as the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for multimodal Bayesian learning.",
        "This method is developed for two substantial demands: first, to efficiently generate i.i.d. samples from complex multimodal posterior distributions; second, to adaptively neutralise the noise arising from the use of mini-batches.",
        "The results validate the efficacy of the tempering and the thermostats techniques we adopted, and show great potentials for being applied to Bayesian learning tasks on complex models such as neural networks"
    ],
    "headline": "We propose a novel sampling method, the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo, for the purpose of multimodal Bayesian learning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of markov chain monte carlo. CRC press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brooks%2C%20Steve%20Gelman%2C%20Andrew%20Jones%2C%20Galin%20Meng%2C%20Xiao-Li%20Handbook%20of%20markov%20chain%20monte%20carlo%202011"
        },
        {
            "id": "2",
            "entry": "[2] Tianqi Chen, Emily B Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In ICML, pages 1683\u20131691, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Fox%2C%20Emily%20B.%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Tianqi%20Fox%2C%20Emily%20B.%20Guestrin%2C%20Carlos%20Stochastic%20gradient%20hamiltonian%20monte%20carlo%202014"
        },
        {
            "id": "3",
            "entry": "[3] Jeffrey Comer, James C Gumbart, J\u00e9r\u00f4me H\u00e9nin, Tony Leli\u00e8vre, Andrew Pohorille, and Christophe Chipot. The adaptive biasing force method: Everything you always wanted to know but were afraid to ask. The Journal of Physical Chemistry B, 119(3):1129\u20131151, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Comer%2C%20Jeffrey%20Gumbart%2C%20James%20C.%20H%C3%A9nin%2C%20J%C3%A9r%C3%B4me%20Leli%C3%A8vre%2C%20Tony%20The%20adaptive%20biasing%20force%20method%3A%20Everything%20you%20always%20wanted%20to%20know%20but%20were%20afraid%20to%20ask%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Comer%2C%20Jeffrey%20Gumbart%2C%20James%20C.%20H%C3%A9nin%2C%20J%C3%A9r%C3%B4me%20Leli%C3%A8vre%2C%20Tony%20The%20adaptive%20biasing%20force%20method%3A%20Everything%20you%20always%20wanted%20to%20know%20but%20were%20afraid%20to%20ask%202014"
        },
        {
            "id": "4",
            "entry": "[4] Eric Darve and Andrew Pohorille. Calculating free energies using average force. The Journal of Chemical Physics, 115(20):9169\u20139183, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Eric%20Darve%20and%20Andrew%20Pohorille.%20Calculating%20free%20energies%20using%20average%20force%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Eric%20Darve%20and%20Andrew%20Pohorille.%20Calculating%20free%20energies%20using%20average%20force%202001"
        },
        {
            "id": "5",
            "entry": "[5] Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D Skeel, and Hartmut Neven. Bayesian sampling using stochastic gradient thermostats. In Advances in neural information processing systems, pages 3203\u20133211, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Nan%20Fang%2C%20Youhan%20Babbush%2C%20Ryan%20Chen%2C%20Changyou%20Bayesian%20sampling%20using%20stochastic%20gradient%20thermostats%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Nan%20Fang%2C%20Youhan%20Babbush%2C%20Ryan%20Chen%2C%20Changyou%20Bayesian%20sampling%20using%20stochastic%20gradient%20thermostats%202014"
        },
        {
            "id": "6",
            "entry": "[6] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216\u2013222, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duane%2C%20Simon%20Kennedy%2C%20Anthony%20D.%20Pendleton%2C%20Brian%20J.%20Roweth%2C%20Duncan%20Hybrid%20monte%20carlo%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duane%2C%20Simon%20Kennedy%2C%20Anthony%20D.%20Pendleton%2C%20Brian%20J.%20Roweth%2C%20Duncan%20Hybrid%20monte%20carlo%201987"
        },
        {
            "id": "7",
            "entry": "[7] Farhan Feroz and MP Hobson. Multimodal nested sampling: an efficient and robust alternative to markov chain monte carlo methods for astronomical data analyses. Monthly Notices of the Royal Astronomical Society, 384(2):449\u2013463, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feroz%2C%20Farhan%20Hobson%2C%20M.P.%20Multimodal%20nested%20sampling%3A%20an%20efficient%20and%20robust%20alternative%20to%20markov%20chain%20monte%20carlo%20methods%20for%20astronomical%20data%20analyses%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feroz%2C%20Farhan%20Hobson%2C%20M.P.%20Multimodal%20nested%20sampling%3A%20an%20efficient%20and%20robust%20alternative%20to%20markov%20chain%20monte%20carlo%20methods%20for%20astronomical%20data%20analyses%202008"
        },
        {
            "id": "8",
            "entry": "[8] Gianpaolo Gobbo and Benedict J Leimkuhler. Extended hamiltonian approach to continuous tempering. Physical Review E, 91(6):061301, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gobbo%2C%20Gianpaolo%20Leimkuhler%2C%20Benedict%20J.%20Extended%20hamiltonian%20approach%20to%20continuous%20tempering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gobbo%2C%20Gianpaolo%20Leimkuhler%2C%20Benedict%20J.%20Extended%20hamiltonian%20approach%20to%20continuous%20tempering%202015"
        },
        {
            "id": "9",
            "entry": "[9] Matthew M. Graham and Amos J. Storkey. Continuously tempered hamiltonian monte carlo. In Gal Elidan, Kristian Kersting, and Alexander T. Ihler, editors, Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017. AUAI Press, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graham%2C%20Matthew%20M.%20Storkey%2C%20Amos%20J.%20Continuously%20tempered%20hamiltonian%20monte%20carlo%202017-08-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graham%2C%20Matthew%20M.%20Storkey%2C%20Amos%20J.%20Continuously%20tempered%20hamiltonian%20monte%20carlo%202017-08-11"
        },
        {
            "id": "10",
            "entry": "[10] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "11",
            "entry": "[11] William G Hoover. Canonical dynamics: equilibrium phase-space distributions. Physical review A, 31(3):1695, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoover%2C%20William%20G.%20Canonical%20dynamics%3A%20equilibrium%20phase-space%20distributions%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoover%2C%20William%20G.%20Canonical%20dynamics%3A%20equilibrium%20phase-space%20distributions%201985"
        },
        {
            "id": "12",
            "entry": "[12] Andrew Jones and Ben Leimkuhler. Adaptive stochastic methods for sampling driven molecular systems. The Journal of chemical physics, 135(8):084125, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jones%2C%20Andrew%20Leimkuhler%2C%20Ben%20Adaptive%20stochastic%20methods%20for%20sampling%20driven%20molecular%20systems%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jones%2C%20Andrew%20Leimkuhler%2C%20Ben%20Adaptive%20stochastic%20methods%20for%20sampling%20driven%20molecular%20systems%202011"
        },
        {
            "id": "13",
            "entry": "[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "14",
            "entry": "[14] Tony Leli\u00e8vre, Mathias Rousset, and Gabriel Stoltz. Long-time convergence of an adaptive biasing force method. Nonlinearity, 21(6):1155, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leli%C3%A8vre%2C%20Tony%20Rousset%2C%20Mathias%20Stoltz%2C%20Gabriel%20Long-time%20convergence%20of%20an%20adaptive%20biasing%20force%20method%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leli%C3%A8vre%2C%20Tony%20Rousset%2C%20Mathias%20Stoltz%2C%20Gabriel%20Long-time%20convergence%20of%20an%20adaptive%20biasing%20force%20method%202008"
        },
        {
            "id": "15",
            "entry": "[15] Nicolas Lenner and Gerald Mathias. Continuous tempering molecular dynamics: A deterministic approach to simulated tempering. Journal of chemical theory and computation, 12(2):486\u2013498, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lenner%2C%20Nicolas%20Mathias%2C%20Gerald%20Continuous%20tempering%20molecular%20dynamics%3A%20A%20deterministic%20approach%20to%20simulated%20tempering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lenner%2C%20Nicolas%20Mathias%2C%20Gerald%20Continuous%20tempering%20molecular%20dynamics%3A%20A%20deterministic%20approach%20to%20simulated%20tempering%202016"
        },
        {
            "id": "16",
            "entry": "[16] Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2:113\u2013162, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Mcmc%20using%20hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%20Monte%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20Radford%20M.%20Mcmc%20using%20hamiltonian%20dynamics.%20Handbook%20of%20Markov%20Chain%20Monte%202011"
        },
        {
            "id": "17",
            "entry": "[17] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(2):372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20o%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20o%281/k2%29%201983"
        },
        {
            "id": "18",
            "entry": "[18] Shuichi Nos\u00e9. A unified formulation of the constant temperature molecular dynamics methods. The Journal of chemical physics, 81(1):511\u2013519, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nos%C3%A9%2C%20Shuichi%20A%20unified%20formulation%20of%20the%20constant%20temperature%20molecular%20dynamics%20methods%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nos%C3%A9%2C%20Shuichi%20A%20unified%20formulation%20of%20the%20constant%20temperature%20molecular%20dynamics%20methods%201984"
        },
        {
            "id": "19",
            "entry": "[19] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20Boris%20T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20Boris%20T.%20Some%20methods%20of%20speeding%20up%20the%20convergence%20of%20iteration%20methods%201964"
        },
        {
            "id": "20",
            "entry": "[20] H. Risken and H. Haken. The Fokker-Planck Equation: Methods of Solution and Applications Second Edition. Springer, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Risken%2C%20H.%20Haken%2C%20H.%20The%20Fokker-Planck%20Equation%3A%20Methods%20of%20Solution%20and%20Applications%20Second%20Edition%201989"
        },
        {
            "id": "21",
            "entry": "[21] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method.%20The%20annals%20of%20mathematical%20statistics%201951"
        },
        {
            "id": "22",
            "entry": "[22] Gareth O Roberts, Richard L Tweedie, et al. Exponential convergence of langevin distributions and their discrete approximations. Bernoulli, 2(4):341\u2013363, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roberts%2C%20Gareth%20O.%20Tweedie%2C%20Richard%20L.%20Exponential%20convergence%20of%20langevin%20distributions%20and%20their%20discrete%20approximations%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roberts%2C%20Gareth%20O.%20Tweedie%2C%20Richard%20L.%20Exponential%20convergence%20of%20langevin%20distributions%20and%20their%20discrete%20approximations%201996"
        },
        {
            "id": "23",
            "entry": "[23] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u20131147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Martens%2C%20James%20Dahl%2C%20George%20Hinton%2C%20Geoffrey%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "24",
            "entry": "[24] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681\u2013688, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20Max%20Teh%2C%20Yee%20W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20Max%20Teh%2C%20Yee%20W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011"
        },
        {
            "id": "25",
            "entry": "[25] Nanyang Ye, Zhanxing Zhu, and Rafal Mantiuk. Langevin dynamics with continuous tempering for training deep neural networks. In Advances in Neural Information Processing Systems, pages 618\u2013626, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20Nanyang%20Zhu%2C%20Zhanxing%20Mantiuk%2C%20Rafal%20Langevin%20dynamics%20with%20continuous%20tempering%20for%20training%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20Nanyang%20Zhu%2C%20Zhanxing%20Mantiuk%2C%20Rafal%20Langevin%20dynamics%20with%20continuous%20tempering%20for%20training%20deep%20neural%20networks%202017"
        }
    ]
}
