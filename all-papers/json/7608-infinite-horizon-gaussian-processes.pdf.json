{
    "filename": "7608-infinite-horizon-gaussian-processes.pdf",
    "metadata": {
        "title": "Infinite-Horizon Gaussian Processes",
        "author": "Arno Solin, James Hensman, Richard E. Turner",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7608-infinite-horizon-gaussian-processes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension m which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension m to O(m2) per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100 Hz."
    },
    "keywords": [
        {
            "term": "gaussian process regression",
            "url": "https://en.wikipedia.org/wiki/gaussian_process_regression"
        },
        {
            "term": "kalman filtering",
            "url": "https://en.wikipedia.org/wiki/kalman_filtering"
        },
        {
            "term": "gaussian processes",
            "url": "https://en.wikipedia.org/wiki/gaussian_processes"
        },
        {
            "term": "expectation propagation",
            "url": "https://en.wikipedia.org/wiki/expectation_propagation"
        }
    ],
    "highlights": [
        "[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] have shown that for one-dimensional GPs with high-order Markovian structure, an optimal representation is rewriting the GP in terms of a state space model and solving inference in linear time by sequential Kalman filtering methods",
        "We show how the Infinite-horizon Gaussian process can be applied in the streaming setting, including efficient estimation of the marginal likelihood and associated gradients, enabling on-line learning of hyper parameters",
        "In Infinite-horizon Gaussian process, instead of using the true predictive covariance for propagation, we use the one obtained from the stationary state of a system with measurement noise fixed to the current measurement noise and regular spacing",
        "We have presented Infinite-Horizon GPs, a novel approximation scheme for state space Gaussian processes, which reduces the time-complexity to O(m2n)",
        "There is a clear intuition to the approximation: As widely known, in GP regression the posterior marginal variance only depends on the distance between observations, and the likelihood variance",
        "If both these are fixed, and t is larger than the largest length-scale in the prior, the posterior marginal variance reaches a stationary state"
    ],
    "key_statements": [
        "[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] have shown that for one-dimensional GPs with high-order Markovian structure, an optimal representation is rewriting the GP in terms of a state space model and solving inference in linear time by sequential Kalman filtering methods",
        "We show how the Infinite-horizon Gaussian process can be applied in the streaming setting, including efficient estimation of the marginal likelihood and associated gradients, enabling on-line learning of hyper parameters",
        "In Infinite-horizon Gaussian process, instead of using the true predictive covariance for propagation, we use the one obtained from the stationary state of a system with measurement noise fixed to the current measurement noise and regular spacing",
        "We have presented Infinite-Horizon GPs, a novel approximation scheme for state space Gaussian processes, which reduces the time-complexity to O(m2n)",
        "There is a clear intuition to the approximation: As widely known, in GP regression the posterior marginal variance only depends on the distance between observations, and the likelihood variance",
        "If both these are fixed, and t is larger than the largest length-scale in the prior, the posterior marginal variance reaches a stationary state",
        "We showed examples of regression, count data, and classification tasks, and showed how Infinite-horizon Gaussian process can be used in interpreting non-stationary data streams both off-line (Sec. 4.3) and on-line (Sec. 4.4)"
    ],
    "summary": [
        "[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] have shown that for one-dimensional GPs with high-order Markovian structure, an optimal representation is rewriting the GP in terms of a state space model and solving inference in linear time by sequential Kalman filtering methods.",
        "Recent work has focused on showing how many widely used covariance function can be either exactly or approximately converted into state space models.",
        "Appendix B shows an example of representing the Mat\u00e9rn (\u03bd = 3/2) covariance function as a state space model.",
        "In steady-state Kalman filtering we assume t \u226b leff , where leff is the longest time scale in the covariance function, and equidistant observations in time (Ai := A and Qi := Q).",
        "Re-deriving the backward pass in Equation (6) gives the time-invariant smoother gain and posterior state covariance",
        "In IHGP, instead of using the true predictive covariance for propagation, we use the one obtained from the stationary state of a system with measurement noise fixed to the current measurement noise and regular spacing.",
        "The Kalman filter iterations can be used in solving approximate posteriors for models with general likelihoods in form of Eq (1) by manipulating the innovation vi and si.",
        "We derive a generalization of the steady-state iteration allowing for time-dependent measurement noise and non-Gaussian likelihoods.",
        "We propose a practical extension of IHGP for online estimation of hyperparameters \u03b8 by leveraging that (i) new batches of data are guaranteed to be available from the stream, IHGP only requires seeing each data point once for evaluating the marginal likelihood and its gradient, data can be non-stationary, requiring the hyperparameters to adapt.",
        "We use a GP prior with a Mat\u00e9rn (\u03bd = 5/2) covariance function that has an exact state space representation and no approximations regarding handling the latent are required.",
        "We use the phone accelerometer x channel as an input and fit a GP to a window of 2 s with Gaussian likelihood and a Mat\u00e9rn (\u03bd = 3/2) prior covariance function.",
        "We have presented Infinite-Horizon GPs, a novel approximation scheme for state space Gaussian processes, which reduces the time-complexity to O(m2n).",
        "If both these are fixed, and t is larger than the largest length-scale in the prior, the posterior marginal variance reaches a stationary state.",
        "The intuition behind IHGP is that for every time instance, we adapt to the current likelihood variance, discard the Markov-trail, and start over by adapting to the current steady-state marginal posterior distribution."
    ],
    "headline": "We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100 Hz",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] B. D. Anderson and J. B. Moore. Optimal Filtering. Prentice-Hall, Englewood Cliffs, NJ, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20B.D.%20Moore%2C%20J.B.%20Optimal%20Filtering%201979"
        },
        {
            "id": "2",
            "entry": "[2] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Cambridge, MA, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Nonlinear%20Programming.%20Athena%20Scientific%201999"
        },
        {
            "id": "3",
            "entry": "[3] T. D. Bui and R. E. Turner. Tree-structured Gaussian process approximations. In Advances in Neural Information Processing Systems (NIPS), pages 2213\u20132221, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bui%2C%20T.D.%20Turner%2C%20R.E.%20Tree-structured%20Gaussian%20process%20approximations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bui%2C%20T.D.%20Turner%2C%20R.E.%20Tree-structured%20Gaussian%20process%20approximations%202014"
        },
        {
            "id": "4",
            "entry": "[4] T. D. Bui, J. Yan, and R. E. Turner. A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation. Journal of Machine Learning Research (JMLR), 18(104):1\u201372, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bui%2C%20T.D.%20Yan%2C%20J.%20Turner%2C%20R.E.%20A%20unifying%20framework%20for%20Gaussian%20process%20pseudo-point%20approximations%20using%20power%20expectation%20propagation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bui%2C%20T.D.%20Yan%2C%20J.%20Turner%2C%20R.E.%20A%20unifying%20framework%20for%20Gaussian%20process%20pseudo-point%20approximations%20using%20power%20expectation%20propagation%202017"
        },
        {
            "id": "5",
            "entry": "[5] L. Csat\u00f3 and M. Opper. Sparse on-line Gaussian processes. Neural Computation, 14(3):641\u2013668, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Csat%C3%B3%2C%20L.%20Opper%2C%20M.%20Sparse%20on-line%20Gaussian%20processes%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Csat%C3%B3%2C%20L.%20Opper%2C%20M.%20Sparse%20on-line%20Gaussian%20processes%202002"
        },
        {
            "id": "6",
            "entry": "[6] D. Duvenaud, J. R. Lloyd, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani. Structure discovery in nonparametric regression through compositional kernel search. In International Conference on Machine Learning (ICML), volume 28 of PMLR, pages 1166\u20131174, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duvenaud%2C%20D.%20Lloyd%2C%20J.R.%20Grosse%2C%20R.%20Tenenbaum%2C%20J.B.%20Structure%20discovery%20in%20nonparametric%20regression%20through%20compositional%20kernel%20search%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duvenaud%2C%20D.%20Lloyd%2C%20J.R.%20Grosse%2C%20R.%20Tenenbaum%2C%20J.B.%20Structure%20discovery%20in%20nonparametric%20regression%20through%20compositional%20kernel%20search%202013"
        },
        {
            "id": "7",
            "entry": "[7] F. Gustafsson. Adaptive Filtering and Change Detection. John Wiley & Sons, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gustafsson%2C%20F.%20Adaptive%20Filtering%20and%20Change%20Detection%202000"
        },
        {
            "id": "8",
            "entry": "[8] J. Hartikainen and S. S\u00e4rkk\u00e4. Kalman filtering and smoothing solutions to temporal Gaussian process regression models. In Proceedings of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP), pages 379\u2013384, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartikainen%2C%20J.%20S%C3%A4rkk%C3%A4%2C%20S.%20Kalman%20filtering%20and%20smoothing%20solutions%20to%20temporal%20Gaussian%20process%20regression%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hartikainen%2C%20J.%20S%C3%A4rkk%C3%A4%2C%20S.%20Kalman%20filtering%20and%20smoothing%20solutions%20to%20temporal%20Gaussian%20process%20regression%20models%202010"
        },
        {
            "id": "9",
            "entry": "[9] G. H\u00e9brail and A. B\u00e9rard. Individual household electric power consumption data set, 2012. URL https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption. Online: UCI Machine Learning Repository.",
            "url": "https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption"
        },
        {
            "id": "10",
            "entry": "[10] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Uncertainty in Artificial Intelligence (UAI), pages 282\u2013290. AUAI Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Fusi%2C%20N.%20Lawrence%2C%20N.D.%20Gaussian%20processes%20for%20big%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Fusi%2C%20N.%20Lawrence%2C%20N.D.%20Gaussian%20processes%20for%20big%20data%202013"
        },
        {
            "id": "11",
            "entry": "[11] J. Hensman, N. Durrande, and A. Solin. Variational Fourier features for Gaussian processes. Journal of Machine Learning Research (JMLR), 18(151):1\u201352, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Durrande%2C%20N.%20Solin%2C%20A.%20Variational%20Fourier%20features%20for%20Gaussian%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Durrande%2C%20N.%20Solin%2C%20A.%20Variational%20Fourier%20features%20for%20Gaussian%20processes%202018"
        },
        {
            "id": "12",
            "entry": "[12] T. Heskes and O. Zoeter. Expectation propagation for approximate inference in dynamic Bayesian networks. In Uncertainty in Artificial Intelligence (UAI), pages 216\u2013223. Morgan Kaufmann Publishers Inc., 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heskes%2C%20T.%20Zoeter%2C%20O.%20Expectation%20propagation%20for%20approximate%20inference%20in%20dynamic%20Bayesian%20networks%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heskes%2C%20T.%20Zoeter%2C%20O.%20Expectation%20propagation%20for%20approximate%20inference%20in%20dynamic%20Bayesian%20networks%202002"
        },
        {
            "id": "13",
            "entry": "[13] R. G. Keys. Cubic convolution interpolation for digital image processing. IEEE Transactions on Acoustics, Speech and Signal Processing, 29(6):1153\u20131160, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keys%2C%20R.G.%20Cubic%20convolution%20interpolation%20for%20digital%20image%20processing%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keys%2C%20R.G.%20Cubic%20convolution%20interpolation%20for%20digital%20image%20processing%201981"
        },
        {
            "id": "14",
            "entry": "[14] K. Krauth, E. V. Bonilla, K. Cutajar, and M. Filippone. AutoGP: Exploring the capabilities and limitations of Gaussian process models. In Uncertainty in Artificial Intelligence (UAI). AUAI Press, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krauth%2C%20K.%20Bonilla%2C%20E.V.%20Cutajar%2C%20K.%20Filippone%2C%20M.%20AutoGP%3A%20Exploring%20the%20capabilities%20and%20limitations%20of%20Gaussian%20process%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krauth%2C%20K.%20Bonilla%2C%20E.V.%20Cutajar%2C%20K.%20Filippone%2C%20M.%20AutoGP%3A%20Exploring%20the%20capabilities%20and%20limitations%20of%20Gaussian%20process%20models%202017"
        },
        {
            "id": "15",
            "entry": "[15] P. Lancaster and L. Rodman. Algebraic Riccati Equations. Clarendon Press, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lancaster%2C%20P.%20Rodman%2C%20L.%20Algebraic%20Riccati%20Equations%201995"
        },
        {
            "id": "16",
            "entry": "[16] A. Laub. A schur method for solving algebraic Riccati equations. IEEE Transactions on Automatic Control, 24(6):913\u2013921, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laub%2C%20A.%20A%20schur%20method%20for%20solving%20algebraic%20Riccati%20equations%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laub%2C%20A.%20A%20schur%20method%20for%20solving%20algebraic%20Riccati%20equations%201979"
        },
        {
            "id": "17",
            "entry": "[17] M. L\u00e1zaro-Gredilla, J. Qui\u00f1onero-Candela, C. E. Rasmussen, and A. R. Figueiras-Vidal. Sparse spectrum Gaussian process regression. Journal of Machine Learning Research (JMLR), 11:1865\u20131881, Jun 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L%C3%A1zaro-Gredilla%2C%20M.%20Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20Figueiras-Vidal%2C%20A.R.%20Sparse%20spectrum%20Gaussian%20process%20regression%202010-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L%C3%A1zaro-Gredilla%2C%20M.%20Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20Figueiras-Vidal%2C%20A.R.%20Sparse%20spectrum%20Gaussian%20process%20regression%202010-06"
        },
        {
            "id": "18",
            "entry": "[18] P. S. Maybeck. Stochastic Models, Estimation and Control, volume 1. Academic Press, New York, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maybeck%2C%20P.S.%20Stochastic%20Models%2C%20Estimation%20and%20Control%2C%20volume%201%201979"
        },
        {
            "id": "19",
            "entry": "[19] T. Minka. Expectation propagation for approximate Bayesian inference. In Uncertainty in Artificial Intelligence (UAI), volume 17, pages 362\u2013369, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20T.%20Expectation%20propagation%20for%20approximate%20Bayesian%20inference%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minka%2C%20T.%20Expectation%20propagation%20for%20approximate%20Bayesian%20inference%202001"
        },
        {
            "id": "20",
            "entry": "[20] J. M\u00f8ller, A. R. Syversveen, and R. P. Waagepetersen. Log Gaussian Cox processes. Scandinavian Journal of Statistics, 25(3):451\u2013482, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%B8ller%2C%20J.%20Syversveen%2C%20A.R.%20Waagepetersen%2C%20R.P.%20Log%20Gaussian%20Cox%20processes%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%B8ller%2C%20J.%20Syversveen%2C%20A.R.%20Waagepetersen%2C%20R.P.%20Log%20Gaussian%20Cox%20processes%201998"
        },
        {
            "id": "21",
            "entry": "[21] H. Nickisch and C. E. Rasmussen. Approximations for binary Gaussian process classification. Journal of Machine Learning Research (JMLR), 9(10):2035\u20132078, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickisch%2C%20H.%20Rasmussen%2C%20C.E.%20Approximations%20for%20binary%20Gaussian%20process%20classification%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickisch%2C%20H.%20Rasmussen%2C%20C.E.%20Approximations%20for%20binary%20Gaussian%20process%20classification%202008"
        },
        {
            "id": "22",
            "entry": "[22] H. Nickisch, A. Solin, and A. Grigorievskiy. State space Gaussian processes with non-Gaussian likelihood. In International Conference on Machine Learning (ICML), volume 80 of PMLR, pages 3789\u20133798, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickisch%2C%20H.%20Solin%2C%20A.%20Grigorievskiy%2C%20A.%20State%20space%20Gaussian%20processes%20with%20non-Gaussian%20likelihood%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickisch%2C%20H.%20Solin%2C%20A.%20Grigorievskiy%2C%20A.%20State%20space%20Gaussian%20processes%20with%20non-Gaussian%20likelihood%202018"
        },
        {
            "id": "23",
            "entry": "[23] J. Qui\u00f1onero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research (JMLR), 6(Dec):1939\u20131959, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qui%C3%B1onero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20unifying%20view%20of%20sparse%20approximate%20Gaussian%20process%20regression%202005"
        },
        {
            "id": "24",
            "entry": "[24] C. E. Rasmussen and H. Nickisch. Gaussian processes for machine learning (GPML) toolbox. Journal of Machine Learning Research (JMLR), 11:3011\u20133015, 2010. Software package: http://www.gaussianprocess.org/gpml/code.",
            "url": "http://www.gaussianprocess.org/gpml/code",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20C.E.%20Nickisch%2C%20H.%20Gaussian%20processes%20for%20machine%20learning%20%28GPML%29%20toolbox%202010"
        },
        {
            "id": "25",
            "entry": "[25] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "26",
            "entry": "[26] S. Reece and S. Roberts. An introduction to Gaussian processes for the Kalman filter expert. In Proceedings of the 13th Conference on Information Fusion (FUSION). IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reece%2C%20S.%20Roberts%2C%20S.%20An%20introduction%20to%20Gaussian%20processes%20for%20the%20Kalman%20filter%20expert%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reece%2C%20S.%20Roberts%2C%20S.%20An%20introduction%20to%20Gaussian%20processes%20for%20the%20Kalman%20filter%20expert%202010"
        },
        {
            "id": "27",
            "entry": "[27] S. S\u00e4rkk\u00e4. Bayesian Filtering and Smoothing. Cambridge University Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%A4rkk%C3%A4%2C%20S.%20Bayesian%20Filtering%20and%20Smoothing%202013"
        },
        {
            "id": "28",
            "entry": "[28] S. S\u00e4rkk\u00e4 and A. Solin. Applied Stochastic Differential Equations. Cambridge University Press, Cambridge, in press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%A4rkk%C3%A4%2C%20S.%20Solin%2C%20A.%20Applied%20Stochastic%20Differential%20Equations"
        },
        {
            "id": "29",
            "entry": "[29] S. S\u00e4rkk\u00e4, A. Solin, and J. Hartikainen. Spatiotemporal learning via infinite-dimensional Bayesian filtering and smoothing. IEEE Signal Processing Magazine, 30(4):51\u201361, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%A4rkk%C3%A4%2C%20S.%20Solin%2C%20A.%20Hartikainen%2C%20J.%20Spatiotemporal%20learning%20via%20infinite-dimensional%20Bayesian%20filtering%20and%20smoothing%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%A4rkk%C3%A4%2C%20S.%20Solin%2C%20A.%20Hartikainen%2C%20J.%20Spatiotemporal%20learning%20via%20infinite-dimensional%20Bayesian%20filtering%20and%20smoothing%202013"
        },
        {
            "id": "30",
            "entry": "[30] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems (NIPS), pages 1257\u20131264. Curran Associates, Inc., 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20processes%20using%20pseudo-inputs%202006"
        },
        {
            "id": "Dissertation_et+al_2016_a",
            "entry": "Doctoral dissertation, Aalto University, Helsinki, Finland, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doctoral%20dissertation%20Aalto%20University%20Helsinki%20Finland%202016"
        },
        {
            "id": "32",
            "entry": "[32] A. Solin and S. S\u00e4rkk\u00e4. Hilbert space methods for reduced-rank Gaussian process regression. arXiv preprint arXiv:1401.5508, 2014. Conference on Artificial Intelligence and Statistics (AISTATS), volume 5 of PMLR, pages 567\u2013574, 2009.",
            "arxiv_url": "https://arxiv.org/pdf/1401.5508"
        },
        {
            "id": "34",
            "entry": "[34] S. T. Tokdar and J. K. Ghosh. Posterior consistency of logistic Gaussian process priors in density estimation. Journal of Statistical Planning and Inference, 137(1):34\u201342, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tokdar%2C%20S.T.%20Ghosh%2C%20J.K.%20Posterior%20consistency%20of%20logistic%20Gaussian%20process%20priors%20in%20density%20estimation%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tokdar%2C%20S.T.%20Ghosh%2C%20J.K.%20Posterior%20consistency%20of%20logistic%20Gaussian%20process%20priors%20in%20density%20estimation%202007"
        },
        {
            "id": "35",
            "entry": "[35] J. Vanhatalo, J. Riihim\u00e4ki, J. Hartikainen, P. Jyl\u00e4nki, V. Tolvanen, and A. Vehtari. GPstuff: Bayesian modeling with Gaussian processes. Journal of Machine Learning Research (JMLR), 14(Apr):1175\u20131179, 2013. Software package: http://research.cs.aalto.fi/pml/software/gpstuff.",
            "url": "http://research.cs.aalto.fi/pml/software/gpstuff",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vanhatalo%2C%20J.%20Riihim%C3%A4ki%2C%20J.%20Hartikainen%2C%20J.%20Jyl%C3%A4nki%2C%20P.%20GPstuff%3A%20Bayesian%20modeling%20with%20Gaussian%20processes%202013"
        },
        {
            "id": "36",
            "entry": "[36] A. Wilson and R. Adams. Gaussian process kernels for pattern discovery and extrapolation. In International Conference on Machine Learning (ICML), volume 28 of PMLR, pages 1067\u20131075, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.%20Adams%2C%20R.%20Gaussian%20process%20kernels%20for%20pattern%20discovery%20and%20extrapolation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.%20Adams%2C%20R.%20Gaussian%20process%20kernels%20for%20pattern%20discovery%20and%20extrapolation%202013"
        },
        {
            "id": "37",
            "entry": "[37] A. G. Wilson and H. Nickisch. Kernel interpolation for scalable structured Gaussian processes (KISS-GP). In International Conference on Machine Learning (ICML), volume 37 of PMLR, pages 1775\u20131784, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20A.G.%20Nickisch%2C%20H.%20Kernel%20interpolation%20for%20scalable%20structured%20Gaussian%20processes%20%28KISS-GP%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20A.G.%20Nickisch%2C%20H.%20Kernel%20interpolation%20for%20scalable%20structured%20Gaussian%20processes%20%28KISS-GP%29%202015"
        }
    ]
}
