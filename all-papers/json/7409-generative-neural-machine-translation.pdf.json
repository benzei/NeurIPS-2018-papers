{
    "filename": "7409-generative-neural-machine-translation.pdf",
    "metadata": {
        "title": "Generative Neural Machine Translation",
        "author": "Harshil Shah, David Barber",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7409-generative-neural-machine-translation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce Generative Neural Machine Translation (GNMT), a latent variable architecture which is designed to model the semantics of the source and target sentences. We modify an encoder-decoder translation model by adding a latent variable as a language agnostic representation which is encouraged to learn the meaning of the sentence. GNMT achieves competitive BLEU scores on pure translation tasks, and is superior when there are missing words in the source sentence. We augment the model to facilitate multilingual translation and semisupervised learning without adding parameters. This framework significantly reduces overfitting when there is limited paired data available, and is effective for translating between pairs of languages not seen during training."
    },
    "keywords": [
        {
            "term": "BLEU",
            "url": "https://en.wikipedia.org/wiki/BLEU"
        },
        {
            "term": "bleu score",
            "url": "https://en.wikipedia.org/wiki/bleu_score"
        },
        {
            "term": "GNMT",
            "url": "https://en.wikipedia.org/wiki/GNMT"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        }
    ],
    "highlights": [
        "Data from multiple modalities can be used to learn representations with more understanding about their environment compared to when only a single modality is available",
        "We introduce Generative Neural Machine Translation (GNMT), whose latent variable is more explicitly designed to learn the sentence\u2019s semantic meaning",
        "Unlike architectures which model the conditional probability of the target sentence given the source sentence, p(y|x), Generative Neural Machine Translation is naturally suited to performing translation when there are missing words in the source sentence, because it can use the latent representation to infer what those missing words may be",
        "The procedure for generating translations using Generative Neural Machine Translation is described in algorithm 1",
        "We have introduced Generative Neural Machine Translation (GNMT), a latent variable architecture which aims to model the semantic meaning of the source and target sentences",
        "We extend Generative Neural Machine Translation to facilitate multilingual translation without adding parameters to the model"
    ],
    "key_statements": [
        "Data from multiple modalities can be used to learn representations with more understanding about their environment compared to when only a single modality is available",
        "We introduce Generative Neural Machine Translation (GNMT), whose latent variable is more explicitly designed to learn the sentence\u2019s semantic meaning",
        "We show that Generative Neural Machine Translation achieves competitive BLEU scores on translation tasks, relies heavily on the latent variable and is particularly effective at translating long sentences",
        "Generative Neural Machine Translation models the joint probability of the target sentence and the source sentence i.e. p(x, y) by using a latent variable z as a language agnostic representation of the sentence",
        "Unlike architectures which model the conditional probability of the target sentence given the source sentence, p(y|x), Generative Neural Machine Translation is naturally suited to performing translation when there are missing words in the source sentence, because it can use the latent representation to infer what those missing words may be",
        "Most closely related to our work is Variational Neural Machine Translation (VNMT) [<a class=\"ref-link\" id=\"cZhang_et+al_2016_a\" href=\"#rZhang_et+al_2016_a\">Zhang et al, 2016</a>], which introduces a latent variable z with the aim of capturing the source sentence\u2019s semantics",
        "The procedure for generating translations using Generative Neural Machine Translation is described in algorithm 1",
        "We have introduced Generative Neural Machine Translation (GNMT), a latent variable architecture which aims to model the semantic meaning of the source and target sentences",
        "We extend Generative Neural Machine Translation to facilitate multilingual translation without adding parameters to the model"
    ],
    "summary": [
        "Data from multiple modalities can be used to learn representations with more understanding about their environment compared to when only a single modality is available.",
        "It uses the latent variable as a language agnostic representation of the sentence, which generates text in both the source and target languages.",
        "When there are missing words in the source sentence, GNMT is able to use its learned representation to infer what those words may be and produce good translations .",
        "GNMT models the joint probability of the target sentence and the source sentence i.e. p(x, y) by using a latent variable z as a language agnostic representation of the sentence.",
        "Unlike architectures which model the conditional probability of the target sentence given the source sentence, p(y|x), GNMT is naturally suited to performing translation when there are missing words in the source sentence, because it can use the latent representation to infer what those missing words may be.",
        "This is done by setting the source and target language variables lx and ly to the same value, in which case the model must attempt to reconstruct the input sentence, rather than translate it.",
        "Most closely related to our work is Variational Neural Machine Translation (VNMT) [<a class=\"ref-link\" id=\"cZhang_et+al_2016_a\" href=\"#rZhang_et+al_2016_a\">Zhang et al, 2016</a>], which introduces a latent variable z with the aim of capturing the source sentence\u2019s semantics.",
        "In work similar to GNMT-MULTI, Johnson et al [2017] perform multilingual translation whilst sharing parameters by prepending, to the source sentence, a string indicating the target language.",
        "GNMT-MULTI achieves much higher BLEU scores with both 40K and 400K paired sentences, due to the parameter sharing between languages.",
        "Even with 4M paired sentences, adding monolingual data is helpful, with GNMT-MULTI-SSL outperforming the other models.",
        "In order to demonstrate that GNMT does learn a useful representation of the sentence\u2019s semantic meaning, we perform missing word translation.",
        "To generate translations using VNMT, we replace the missing words in the source sentence with the unknown word token and conduct the same conditional likelihood maximization described in section 4.3.1.",
        "We have introduced Generative Neural Machine Translation (GNMT), a latent variable architecture which aims to model the semantic meaning of the source and target sentences.",
        "GNMT performs competitively with a comparable conditional model, places higher reliance on the latent variable and achieves higher BLEU scores when translating long sentences.",
        "This parameter sharing reduces the impact of overfitting when the amount of available paired data is limited, and proves to be effective for translating between pairs of languages which were not seen during training."
    ],
    "headline": "We introduce Generative Neural Machine Translation , a latent variable architecture which is designed to model the semantics of the source and target sentences",
    "reference_links": [
        {
            "id": "Bahdanau_et+al_2015_a",
            "entry": "D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate%202015"
        },
        {
            "id": "Bowman_et+al_2016_a",
            "entry": "S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating Sentences from a Continuous Space. In Conference on Computational Natural Language Learning (CoNLL), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowman%2C%20S.R.%20Vilnis%2C%20L.%20Vinyals%2C%20O.%20Dai%2C%20A.M.%20Generating%20Sentences%20from%20a%20Continuous%20Space%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowman%2C%20S.R.%20Vilnis%2C%20L.%20Vinyals%2C%20O.%20Dai%2C%20A.M.%20Generating%20Sentences%20from%20a%20Continuous%20Space%202016"
        },
        {
            "id": "S_2015_a",
            "entry": "S. Dieleman, J. Schl\u00fcter, C. Raffel, E. Olson, S. K. S\u00f8nderby, et al. Lasagne: First release, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Dieleman%20J%20Schl%C3%BCter%20C%20Raffel%20E%20Olson%20S%20K%20S%C3%B8nderby%20et%20al%20Lasagne%20First%20release%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%20Dieleman%20J%20Schl%C3%BCter%20C%20Raffel%20E%20Olson%20S%20K%20S%C3%B8nderby%20et%20al%20Lasagne%20First%20release%202015"
        },
        {
            "id": "Dieng_et+al_2017_a",
            "entry": "A. B. Dieng, C. Wang, J. Gao, and J. Paisley. TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieng%2C%20A.B.%20Wang%2C%20C.%20Gao%2C%20J.%20Paisley%2C%20J.%20TopicRNN%3A%20A%20Recurrent%20Neural%20Network%20with%20Long-Range%20Semantic%20Dependency%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieng%2C%20A.B.%20Wang%2C%20C.%20Gao%2C%20J.%20Paisley%2C%20J.%20TopicRNN%3A%20A%20Recurrent%20Neural%20Network%20with%20Long-Range%20Semantic%20Dependency%202017"
        },
        {
            "id": "Johnson_et+al_2017_a",
            "entry": "M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. Vi\u00e9gas, M. Wattenberg, G. Corrado, M. Hughes, and J. Dean. Google\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. Transactions of the Association for Computational Linguistics, 5:339\u2013351, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20M.%20Schuster%2C%20M.%20Le%2C%20Q.V.%20Krikun%2C%20M.%20Google%E2%80%99s%20Multilingual%20Neural%20Machine%20Translation%20System%3A%20Enabling%20Zero-Shot%20Translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20M.%20Schuster%2C%20M.%20Le%2C%20Q.V.%20Krikun%2C%20M.%20Google%E2%80%99s%20Multilingual%20Neural%20Machine%20Translation%20System%3A%20Enabling%20Zero-Shot%20Translation%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-Encoding%20Variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-Encoding%20Variational%20Bayes%202014"
        },
        {
            "id": "Neal_1998_a",
            "entry": "R. M. Neal and G. E. Hinton. A View of the EM Algorithm that Justifies Incremental, Sparse, and other Variants. In Learning in Graphical Models, pages 355\u2013368. Springer Netherlands, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20R.M.%20Hinton%2C%20G.E.%20A%20View%20of%20the%20EM%20Algorithm%20that%20Justifies%20Incremental%2C%20Sparse%2C%20and%20other%20Variants%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neal%2C%20R.M.%20Hinton%2C%20G.E.%20A%20View%20of%20the%20EM%20Algorithm%20that%20Justifies%20Incremental%2C%20Sparse%2C%20and%20other%20Variants%201998"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In Proceedings of the 31st International Conference on Machine Learning, volume 32, pages 1278\u20131286. PMLR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20Backpropagation%20and%20Approximate%20Inference%20in%20Deep%20Generative%20Models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20Backpropagation%20and%20Approximate%20Inference%20in%20Deep%20Generative%20Models%202014"
        },
        {
            "id": "Sennrich_et+al_2016_a",
            "entry": "R. Sennrich, B. Haddow, and A. Birch. Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 86\u201396. Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sennrich%2C%20R.%20Haddow%2C%20B.%20Birch%2C%20A.%20Improving%20Neural%20Machine%20Translation%20Models%20with%20Monolingual%20Data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sennrich%2C%20R.%20Haddow%2C%20B.%20Birch%2C%20A.%20Improving%20Neural%20Machine%20Translation%20Models%20with%20Monolingual%20Data%202016"
        },
        {
            "id": "Shu_et+al_2017_a",
            "entry": "R. Shu, H. H. Bui, and M. Ghavamzadeh. Bottleneck Conditional Density Estimation. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 3164\u20133172. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shu%2C%20R.%20Bui%2C%20H.H.%20Ghavamzadeh%2C%20M.%20Bottleneck%20Conditional%20Density%20Estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shu%2C%20R.%20Bui%2C%20H.H.%20Ghavamzadeh%2C%20M.%20Bottleneck%20Conditional%20Density%20Estimation%202017"
        },
        {
            "id": "C_2016_a",
            "entry": "C. K. S\u00f8nderby, T. Raiko, L. Maal\u00f8e, S. K. S\u00f8nderby, and O. Winther. Ladder Variational Autoencoders. In Advances in Neural Information Processing Systems 29, pages 3738\u20133746, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ladder%20Variational%20Autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ladder%20Variational%20Autoencoders%202016"
        },
        {
            "id": "Theano_2016_a",
            "entry": "Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint, arXiv:1605.02688, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.02688"
        },
        {
            "id": "Data_2012_a",
            "entry": "J. Tiedemann. Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the Eight International Conference on Language Resources and Evaluation. European Language Resources Association (ELRA), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Data%2C%20J.Tiedemann%20Parallel%20Tools%20and%20Interfaces%20in%20OPUS%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Data%2C%20J.Tiedemann%20Parallel%20Tools%20and%20Interfaces%20in%20OPUS%202012"
        },
        {
            "id": "Tu_et+al_2016_a",
            "entry": "Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li. Modeling Coverage for Neural Machine Translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 76\u201385. Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Z%20Tu%20Z%20Lu%20Y%20Liu%20X%20Liu%20and%20H%20Li%20Modeling%20Coverage%20for%20Neural%20Machine%20Translation%20In%20Proceedings%20of%20the%2054th%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%20pages%207685%20Association%20for%20Computational%20Linguistics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Z%20Tu%20Z%20Lu%20Y%20Liu%20X%20Liu%20and%20H%20Li%20Modeling%20Coverage%20for%20Neural%20Machine%20Translation%20In%20Proceedings%20of%20the%2054th%20Annual%20Meeting%20of%20the%20Association%20for%20Computational%20Linguistics%20pages%207685%20Association%20for%20Computational%20Linguistics%202016"
        },
        {
            "id": "Yang_et+al_2017_a",
            "entry": "Z. Yang, Z. Hu, R. Salakhutdinov, and T. Berg-Kirkpatrick. Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 3881\u20133890. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Hu%2C%20Z.%20Salakhutdinov%2C%20R.%20Berg-Kirkpatrick%2C%20T.%20Improved%20Variational%20Autoencoders%20for%20Text%20Modeling%20using%20Dilated%20Convolutions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Hu%2C%20Z.%20Salakhutdinov%2C%20R.%20Berg-Kirkpatrick%2C%20T.%20Improved%20Variational%20Autoencoders%20for%20Text%20Modeling%20using%20Dilated%20Convolutions%202017"
        },
        {
            "id": "Zhang_et+al_2016_a",
            "entry": "B. Zhang, D. Xiong, J. Su, H. Duan, and M. Zhang. Variational Neural Machine Translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 521\u2013530. Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%20Zhang%20D%20Xiong%20J%20Su%20H%20Duan%20and%20M%20Zhang%20Variational%20Neural%20Machine%20Translation%20In%20Proceedings%20of%20the%202016%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20pages%20521530%20Association%20for%20Computational%20Linguistics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=B%20Zhang%20D%20Xiong%20J%20Su%20H%20Duan%20and%20M%20Zhang%20Variational%20Neural%20Machine%20Translation%20In%20Proceedings%20of%20the%202016%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20pages%20521530%20Association%20for%20Computational%20Linguistics%202016"
        },
        {
            "id": "Zhang_2016_b",
            "entry": "J. Zhang and C. Zong. Exploiting Source-side Monolingual Data in Neural Machine Translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535\u20131545. Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20J.%20Zong%2C%20C.%20Exploiting%20Source-side%20Monolingual%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20J.%20Zong%2C%20C.%20Exploiting%20Source-side%20Monolingual%202016"
        }
    ]
}
