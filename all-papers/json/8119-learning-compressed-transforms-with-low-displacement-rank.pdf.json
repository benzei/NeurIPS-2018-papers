{
    "filename": "8119-learning-compressed-transforms-with-low-displacement-rank.pdf",
    "metadata": {
        "date": 2018,
        "title": "Learning Compressed Transforms with Low Displacement Rank",
        "author": "Anna T. Thomas\u2020\u2217, Albert Gu\u2020\u2217, Tri Dao\u2020, Atri Rudra\u2021, Christopher R\u00e9\u2020 \u2020 Department of Computer Science, Stanford University",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8119-learning-compressed-transforms-with-low-displacement-rank.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a rich class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks even outperform general unstructured layers while using more than 20x fewer parameters."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "vc dimension",
            "url": "https://en.wikipedia.org/wiki/vc_dimension"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "displacement operator",
            "url": "https://en.wikipedia.org/wiki/displacement_operator"
        },
        {
            "term": "Fast Fourier Transforms",
            "url": "https://en.wikipedia.org/wiki/Fast_Fourier_Transforms"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        }
    ],
    "highlights": [
        "Recent years have seen a surge of interest in structured representations for deep learning, motivated by achieving compression and acceleration while maintaining generalization properties",
        "We prove that the class of neural networks constructed from these matrices retains VC dimension almost linear in the number of parameters, which implies that low displacement rank matrices with learned displacement operators are still efficiently recoverable from data",
        "We prove that the VC dimension of multi-layer neural networks with low displacement rank weight matrices, which encompasses a broad class of previously explored approaches including the low-rank and Toeplitz-like classes, is quasi-linear in the number of parameters (Section 4)",
        "We hypothesize that the main contribution to their marked performance difference is the effect of the learned displacement operator modeling latent invariances in the data, and that the improvement in the displacement\n4In addition to the results reported in Table 1, Figure 3 and Table 7 in Appendix E, we found that at rank 16 the low displacement rank-SD class on the CNN architecture achieved test accuracies of 68.48% and 75.45% on CIFAR-10 and NORB respectively",
        "We substantially generalize the class of low displacement rank matrices explored in machine learning by considering classes of low displacement rank matrices with displacement operators that can be learned from data",
        "We show these matrices can improve performance on downstream tasks compared to compression baselines and, on some tasks, even general unstructured weight layers"
    ],
    "key_statements": [
        "Recent years have seen a surge of interest in structured representations for deep learning, motivated by achieving compression and acceleration while maintaining generalization properties",
        "Examples of structures explored for the weight matrices of deep learning pipelines include low-rank matrices [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], low-distortion projections [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>],circulant matrices [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], Toeplitz-like matrices [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>], and constructions derived from Fourier-related transforms [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>]",
        "When compressing weight matrices in fully-connected, convolutional, and recurrent neural networks, we empirically demonstrate improved accuracy over existing approaches",
        "We provide the first analysis of the sample complexity of learning neural networks with low displacement rank weight matrices, which extends to low-rank, Toeplitz-like and other previously explored fixed classes of low displacement rank matrices",
        "We prove that the class of neural networks constructed from these matrices retains VC dimension almost linear in the number of parameters, which implies that low displacement rank matrices with learned displacement operators are still efficiently recoverable from data",
        "We introduce a rich class of low displacement rank matrices where the displacement operators are explicitly learned from data, and provide multiplication algorithms implemented in PyTorch (Section 3).2",
        "We prove that the VC dimension of multi-layer neural networks with low displacement rank weight matrices, which encompasses a broad class of previously explored approaches including the low-rank and Toeplitz-like classes, is quasi-linear in the number of parameters (Section 4)",
        "We empirically demonstrate that our construction improves downstream quality when compressing weight layers in fully-connected, convolutional, and recurrent neural networks compared to previous compression approaches, and on some tasks can even outperform general unstructured layers (Section 5).\n2 Background: displacement rank",
        "At rank one, our classes with learned operators achieve higher accuracy than the fixed operator classes, and on the MNIST-bg-rot, MNIST-noise, and NORB datasets even improve on FC layers of the same dimensions, by 1.73, 13.30, and 2.92 accuracy points respectively on the single hidden layer task, as shown in Table 1",
        "We hypothesize that the main contribution to their marked performance difference is the effect of the learned displacement operator modeling latent invariances in the data, and that the improvement in the displacement\n4In addition to the results reported in Table 1, Figure 3 and Table 7 in Appendix E, we found that at rank 16 the low displacement rank-SD class on the CNN architecture achieved test accuracies of 68.48% and 75.45% on CIFAR-10 and NORB respectively",
        "Reducing sample complexity We investigate whether low displacement rank models with learned displacement operators require fewer samples to achieve the same test error, compared to unstructured weights, in both the single hidden layer and CNN architectures",
        "We substantially generalize the class of low displacement rank matrices explored in machine learning by considering classes of low displacement rank matrices with displacement operators that can be learned from data",
        "We show these matrices can improve performance on downstream tasks compared to compression baselines and, on some tasks, even general unstructured weight layers"
    ],
    "summary": [
        "Recent years have seen a surge of interest in structured representations for deep learning, motivated by achieving compression and acceleration while maintaining generalization properties.",
        "We prove that the class of neural networks constructed from these matrices retains VC dimension almost linear in the number of parameters, which implies that LDR matrices with learned displacement operators are still efficiently recoverable from data.",
        "We introduce a rich class of LDR matrices where the displacement operators are explicitly learned from data, and provide multiplication algorithms implemented in PyTorch (Section 3).2",
        "We prove that the VC dimension of multi-layer neural networks with LDR weight matrices, which encompasses a broad class of previously explored approaches including the low-rank and Toeplitz-like classes, is quasi-linear in the number of parameters (Section 4).",
        "By increasing the rank to just 16, the LDR-SD class meets or exceeds the accuracy of the unstructured FC layer in all datasets we tested on, for both SHL and CNN.4 Appendix F includes more experimental details and protocols.",
        "Our classes with learned operators achieve higher accuracy than the fixed operator classes, and on the MNIST-bg-rot, MNIST-noise, and NORB datasets even improve on FC layers of the same dimensions, by 1.73, 13.30, and 2.92 accuracy points respectively on the SHL task, as shown in Table 1.",
        "Figure 3 shows that at just rank 16, the LDR-SD class meets or exceeds the performance of the FC layer on all four datasets, by 5.87, 15.05, 0.74, and 6.86 accuracy points on MNIST-bg-rot, MNIST-noise, CIFAR-10, and NORB respectively, while still maintaining at least",
        "4In addition to the results reported in Table 1, Figure 3 and Table 7 in Appendix E, we found that at rank 16 the LDR-SD class on the CNN architecture achieved test accuracies of 68.48% and 75.45% on CIFAR-10 and NORB respectively.",
        "Though our class does not outperform the unstructured model, we did find that it achieves a significantly lower perplexity than the fixed Toeplitz-like class, suggesting that learning the displacement operator can help adapt to different domains.",
        "Theorem 2 states that the theoretical sample complexity of neural networks with structured weight matrices scales almost linearly in the total number of parameters, matching the results for networks with fully-connected layers [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>].",
        "Reducing sample complexity We investigate whether LDR models with learned displacement operators require fewer samples to achieve the same test error, compared to unstructured weights, in both the single hidden layer and CNN architectures.",
        "Figure 4(a,b) shows the heat map of the weight matrix W \u2208 R784\u00d7784 for the Toeplitz-like and LDR-SD classes, trained on MNIST-bg-rot with a single hidden layer model.",
        "We hope this work inspires additional ways of using structure to achieve both more compact and higher quality representations, especially for deep learning models which are commonly acknowledged to be overparameterized"
    ],
    "headline": "We introduce a rich class of low displacement rank matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of the IEEE International Conference on Computer Vision, pages 37\u201345. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Pulkit%20Carreira%2C%20Joao%20Malik%2C%20Jitendra%20Learning%20to%20see%20by%20moving%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Pulkit%20Carreira%2C%20Joao%20Malik%2C%20Jitendra%20Learning%20to%20see%20by%20moving%202015"
        },
        {
            "id": "2",
            "entry": "[2] Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio. Unsupervised learning of invariant representations. Theor. Comput. Sci., 633(C): 112\u2013121, June 2016. ISSN 0304-3975. doi: 10.1016/j.tcs.2015.06.048. URL https://doi.org/10.1016/j.tcs.2015.06.048.",
            "crossref": "https://dx.doi.org/10.1016/j.tcs.2015.06.048",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.tcs.2015.06.048"
        },
        {
            "id": "3",
            "entry": "[3] Martin Anthony and Peter L Bartlett. Neural network learning: theoretical foundations. Cambridge University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20L.%20Neural%20network%20learning%3A%20theoretical%20foundations%202009"
        },
        {
            "id": "4",
            "entry": "[4] Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC dimension bounds for piecewise polynomial networks. In Advances in Neural Information Processing Systems, pages 190\u2013196, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Maiorov%2C%20Vitaly%20Meir%2C%20Ron%20Almost%20linear%20VC%20dimension%20bounds%20for%20piecewise%20polynomial%20networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Maiorov%2C%20Vitaly%20Meir%2C%20Ron%20Almost%20linear%20VC%20dimension%20bounds%20for%20piecewise%20polynomial%20networks%201999"
        },
        {
            "id": "5",
            "entry": "[5] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34 (4):18\u201342, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bronstein%2C%20Michael%20M.%20Bruna%2C%20Joan%20LeCun%2C%20Yann%20Szlam%2C%20Arthur%20Geometric%20deep%20learning%3A%20going%20beyond%20Euclidean%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bronstein%2C%20Michael%20M.%20Bruna%2C%20Joan%20LeCun%2C%20Yann%20Szlam%2C%20Arthur%20Geometric%20deep%20learning%3A%20going%20beyond%20Euclidean%20data%202017"
        },
        {
            "id": "6",
            "entry": "[6] Peter B\u00fcrgisser, Michael Clausen, and Mohammad A Shokrollahi. Algebraic complexity theory, volume 315. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%BCrgisser%2C%20Peter%20Clausen%2C%20Michael%20and%20Mohammad%20A%20Shokrollahi.%20Algebraic%20complexity%20theory%2C%20volume%20315%202013"
        },
        {
            "id": "7",
            "entry": "[7] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2285\u20132294, Lille, France, 07\u201309 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html.",
            "url": "http://proceedings.mlr.press/v37/chenc15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Wenlin%20Wilson%2C%20James%20Tyree%2C%20Stephen%20Weinberger%2C%20Kilian%20Compressing%20neural%20networks%20with%20the%20hashing%20trick%202015-07"
        },
        {
            "id": "8",
            "entry": "[8] Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE International Conference on Computer Vision, pages 2857\u20132865, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Yu%20Yu%2C%20Felix%20X.%20Feris%2C%20Rogerio%20S.%20Kumar%2C%20Sanjiv%20and%20Shi-Fu%20Chang.%20An%20exploration%20of%20parameter%20redundancy%20in%20deep%20networks%20with%20circulant%20projections%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Yu%20Yu%2C%20Felix%20X.%20Feris%2C%20Rogerio%20S.%20Kumar%2C%20Sanjiv%20and%20Shi-Fu%20Chang.%20An%20exploration%20of%20parameter%20redundancy%20in%20deep%20networks%20with%20circulant%20projections%202015"
        },
        {
            "id": "9",
            "entry": "[9] T.S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. URL https://books.google.com/books?id= IkCJSQAACAAJ.",
            "url": "https://books.google.com/books?id="
        },
        {
            "id": "10",
            "entry": "[10] Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear time kernel expansions. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2502\u20132510, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/choromanski16.html.",
            "url": "http://proceedings.mlr.press/v48/choromanski16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanski%2C%20Krzysztof%20Sindhwani%2C%20Vikas%20Recycling%20randomness%20with%20structure%20for%20sublinear%20time%20kernel%20expansions%202016-06-20"
        },
        {
            "id": "11",
            "entry": "[11] Dan C Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber. Flexible, high performance convolutional neural networks for image classification. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22, page 1237. Barcelona, Spain, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=high%20performance%20convolutional%20neural%20networks%20for%20image%20classification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=high%20performance%20convolutional%20neural%20networks%20for%20image%20classification%202011"
        },
        {
            "id": "12",
            "entry": "[12] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International Conference on Machine Learning, pages 2990\u20132999, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Taco%20Welling%2C%20Max%20Group%20equivariant%20convolutional%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Taco%20Welling%2C%20Max%20Group%20equivariant%20convolutional%20networks%202016"
        },
        {
            "id": "13",
            "entry": "[13] Taco S. Cohen, Mario Geiger, Jonas K\u00f6hler, and Max Welling. Spherical CNNs. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hkbd5xZRb.",
            "url": "https://openreview.net/forum?id=Hkbd5xZRb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taco%20S.%20Cohen%2C%20Mario%20Geiger%2C%20Jonas%20K%C3%B6hler%20Welling%2C%20Max%20Spherical%20CNNs%202018"
        },
        {
            "id": "14",
            "entry": "[14] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. A twopronged progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1060\u20131079. SIAM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20Christopher%20De%20Gu%2C%20Albert%20Puttagunta%2C%20Rohan%20R%C3%A9%2C%20Christopher%20A%20twopronged%20progress%20in%20structured%20dense%20matrix%20vector%20multiplication%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20Christopher%20De%20Gu%2C%20Albert%20Puttagunta%2C%20Rohan%20R%C3%A9%2C%20Christopher%20A%20twopronged%20progress%20in%20structured%20dense%20matrix%20vector%20multiplication%202018"
        },
        {
            "id": "15",
            "entry": "[15] Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denil%2C%20Misha%20Shakibi%2C%20Babak%20Dinh%2C%20Laurent%20Freitas%2C%20Nando%20De%20Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "16",
            "entry": "[16] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convolutional neural networks. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1889\u20131898, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/dieleman16.html.",
            "url": "http://proceedings.mlr.press/v48/dieleman16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieleman%2C%20Sander%20Fauw%2C%20Jeffrey%20De%20Kavukcuoglu%2C%20Koray%20Exploiting%20cyclic%20symmetry%20in%20convolutional%20neural%20networks%202016-06-20"
        },
        {
            "id": "17",
            "entry": "[17] Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, et al. CirCNN: accelerating and compressing deep neural networks using block-circulant weight matrices. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture, pages 395\u2013408. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Caiwen%20Liao%2C%20Siyu%20Wang%2C%20Yanzhi%20Li%2C%20Zhe%20CirCNN%3A%20accelerating%20and%20compressing%20deep%20neural%20networks%20using%20block-circulant%20weight%20matrices%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Caiwen%20Liao%2C%20Siyu%20Wang%2C%20Yanzhi%20Li%2C%20Zhe%20CirCNN%3A%20accelerating%20and%20compressing%20deep%20neural%20networks%20using%20block-circulant%20weight%20matrices%202017"
        },
        {
            "id": "18",
            "entry": "[18] Sebastian Egner and Markus P\u00fcschel. Automatic generation of fast discrete signal transforms. IEEE Transactions on Signal Processing, 49(9):1992\u20132002, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Egner%2C%20Sebastian%20P%C3%BCschel%2C%20Markus%20Automatic%20generation%20of%20fast%20discrete%20signal%20transforms%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Egner%2C%20Sebastian%20P%C3%BCschel%2C%20Markus%20Automatic%20generation%20of%20fast%20discrete%20signal%20transforms%202001"
        },
        {
            "id": "19",
            "entry": "[19] Sebastian Egner and Markus P\u00fcschel. Symmetry-based matrix factorization. Journal of Symbolic Computation, 37(2):157\u2013186, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Egner%2C%20Sebastian%20P%C3%BCschel%2C%20Markus%20Symmetry-based%20matrix%20factorization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Egner%2C%20Sebastian%20P%C3%BCschel%2C%20Markus%20Symmetry-based%20matrix%20factorization%202004"
        },
        {
            "id": "20",
            "entry": "[20] Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in Neural Information Processing Systems, pages 2537\u20132545, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gens%2C%20Robert%20Domingos%2C%20Pedro%20M.%20Deep%20symmetry%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gens%2C%20Robert%20Domingos%2C%20Pedro%20M.%20Deep%20symmetry%20networks%202014"
        },
        {
            "id": "21",
            "entry": "[21] C. Lee Giles and Tom Maxwell. Learning, invariance, and generalization in high-order neural networks. Appl. Opt., 26(23):4972\u20134978, Dec 1987. doi: 10.1364/AO.26.004972. URL http://ao.osa.org/abstract.cfm?URI=ao-26-23-4972.",
            "crossref": "https://dx.doi.org/10.1364/AO.26.004972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1364/AO.26.004972"
        },
        {
            "id": "22",
            "entry": "[22] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic differentiation. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, second edition, 2008. ISBN 0898716594, 9780898716597.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20Walther%2C%20Andrea%20Evaluating%20derivatives%3A%20principles%20and%20techniques%20of%20algorithmic%20differentiation.%20Society%20for%20Industrial%20and%20Applied%20Mathematics%202008"
        },
        {
            "id": "23",
            "entry": "[23] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pages 1135\u20131143, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%202015"
        },
        {
            "id": "24",
            "entry": "[24] Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 1064\u20131068, Amsterdam, Netherlands, 07\u201310 Jul 2017. PMLR. URL http://proceedings.mlr.press/v65/harvey17a.html.",
            "url": "http://proceedings.mlr.press/v65/harvey17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harvey%2C%20Nick%20Liaw%2C%20Christopher%20Mehrabian%2C%20Abbas%20Nearly-tight%20VC-dimension%20bounds%20for%20piecewise%20linear%20neural%20networks%202017-07"
        },
        {
            "id": "25",
            "entry": "[25] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NIPS Deep Learning Workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeff%20Distilling%20the%20knowledge%20in%20a%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeff%20Distilling%20the%20knowledge%20in%20a%20neural%20network%202015"
        },
        {
            "id": "26",
            "entry": "[26] Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations. Journal of Mathematical Analysis and Applications, 68(2):395\u2013407, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kailath%2C%20Thomas%20Kung%2C%20Sun-Yuan%20Morf%2C%20Martin%20Displacement%20ranks%20of%20matrices%20and%20linear%20equations%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kailath%2C%20Thomas%20Kung%2C%20Sun-Yuan%20Morf%2C%20Martin%20Displacement%20ranks%20of%20matrices%20and%20linear%20equations%201979"
        },
        {
            "id": "27",
            "entry": "[27] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, pages 2752\u20132760, 2018. URL http://proceedings.mlr.press/v80/kondor18a.html.",
            "url": "http://proceedings.mlr.press/v80/kondor18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kondor%2C%20Risi%20Trivedi%2C%20Shubhendu%20On%20the%20generalization%20of%20equivariance%20and%20convolution%20in%20neural%20networks%20to%20the%20action%20of%20compact%20groups%202018-07-10"
        },
        {
            "id": "28",
            "entry": "[28] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master\u2019s Thesis, Department of Computer Science, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "29",
            "entry": "[29] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 473\u2013480, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-793-3. doi: 10.1145/1273496. 1273556. URL http://doi.acm.org/10.1145/1273496.1273556.",
            "crossref": "https://dx.doi.org/10.1145/1273496.1273556",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1273496.1273556"
        },
        {
            "id": "30",
            "entry": "[30] Quoc Le, Tamas Sarlos, and Alexander Smola. Fastfood - computing Hilbert space expansions in loglinear time. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 244\u2013252, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/le13.html.",
            "url": "http://proceedings.mlr.press/v28/le13.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20Sarlos%2C%20Tamas%20Smola%2C%20Alexander%20Fastfood%20-%20computing%20Hilbert%20space%20expansions%20in%20loglinear%20time%202013-06-17"
        },
        {
            "id": "31",
            "entry": "[31] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Proceedings of the IEEE International Conference on Computer Vision, volume 2, pages II\u2013104. IEEE, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Huang%2C%20Fu%20Jie%20Bottou%2C%20Leon%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Huang%2C%20Fu%20Jie%20Bottou%2C%20Leon%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004"
        },
        {
            "id": "32",
            "entry": "[32] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 991\u2013999, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lenc%2C%20Karel%20Vedaldi%2C%20Andrea%20Understanding%20image%20representations%20by%20measuring%20their%20equivariance%20and%20equivalence%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lenc%2C%20Karel%20Vedaldi%2C%20Andrea%20Understanding%20image%20representations%20by%20measuring%20their%20equivariance%20and%20equivalence%202015"
        },
        {
            "id": "33",
            "entry": "[33] Zhiyun Lu, Vikas Sindhwani, and Tara N Sainath. Learning compact recurrent neural networks. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5960\u20135964. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Zhiyun%20Sindhwani%2C%20Vikas%20Sainath%2C%20Tara%20N.%20Learning%20compact%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Zhiyun%20Sindhwani%2C%20Vikas%20Sainath%2C%20Tara%20N.%20Learning%20compact%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "34",
            "entry": "[34] Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 5058\u20135067, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcos%2C%20Diego%20Volpi%2C%20Michele%20Komodakis%2C%20Nikos%20Tuia%2C%20Devis%20Rotation%20equivariant%20vector%20field%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcos%2C%20Diego%20Volpi%2C%20Michele%20Komodakis%2C%20Nikos%20Tuia%2C%20Devis%20Rotation%20equivariant%20vector%20field%20networks%202017"
        },
        {
            "id": "35",
            "entry": "[35] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Byj72udxe.",
            "url": "https://openreview.net/forum?id=Byj72udxe",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merity%2C%20Stephen%20Xiong%2C%20Caiming%20Bradbury%2C%20James%20Socher%2C%20Richard%20Pointer%20sentinel%20mixture%20models%202017"
        },
        {
            "id": "36",
            "entry": "[36] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. ACDC: a structured efficient linear layer. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moczulski%2C%20Marcin%20Denil%2C%20Misha%20Appleyard%2C%20Jeremy%20de%20Freitas%2C%20Nando%20ACDC%3A%20a%20structured%20efficient%20linear%20layer%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moczulski%2C%20Marcin%20Denil%2C%20Misha%20Appleyard%2C%20Jeremy%20de%20Freitas%2C%20Nando%20ACDC%3A%20a%20structured%20efficient%20linear%20layer%202016"
        },
        {
            "id": "37",
            "entry": "[37] Samet Oymak. Learning compact neural networks with regularization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3966\u20133975, Stockholmsm\u00e4ssan, Stockholm, Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/oymak18a.html.",
            "url": "http://proceedings.mlr.press/v80/oymak18a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oymak%2C%20Samet%20Learning%20compact%20neural%20networks%20with%20regularization%202018-07"
        },
        {
            "id": "38",
            "entry": "[38] Dipan K Pal and Marios Savvides. Non-parametric transformation networks. arXiv preprint arXiv:1801.04520, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04520"
        },
        {
            "id": "39",
            "entry": "[39] Victor Y Pan. Structured matrices and polynomials: unified superfast algorithms. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Victor%20Y.%20Structured%20matrices%20and%20polynomials%3A%20unified%20superfast%20algorithms%202012"
        },
        {
            "id": "40",
            "entry": "[40] Victor Y Pan and Xinmao Wang. Inversion of displacement operators. SIAM Journal on Matrix Analysis and Applications, 24(3):660\u2013677, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Victor%20Y.%20Wang%2C%20Xinmao%20Inversion%20of%20displacement%20operators%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Victor%20Y.%20Wang%2C%20Xinmao%20Inversion%20of%20displacement%20operators%202003"
        },
        {
            "id": "41",
            "entry": "[41] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655\u20136659. IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20Tara%20N.%20Kingsbury%2C%20Brian%20Sindhwani%2C%20Vikas%20Arisoy%2C%20Ebru%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20Tara%20N.%20Kingsbury%2C%20Brian%20Sindhwani%2C%20Vikas%20Arisoy%2C%20Ebru%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013"
        },
        {
            "id": "42",
            "entry": "[42] Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2050\u20132057. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20Uwe%20Roth%2C%20Stefan%20Learning%20rotation-aware%20features%3A%20From%20invariant%20priors%20to%20equivariant%20descriptors%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20Uwe%20Roth%2C%20Stefan%20Learning%20rotation-aware%20features%3A%20From%20invariant%20priors%20to%20equivariant%20descriptors%202012"
        },
        {
            "id": "43",
            "entry": "[43] Valeria Simoncini. Computational methods for linear matrix equations. SIAM Review, 58(3): 377\u2013441, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simoncini%2C%20Valeria%20Computational%20methods%20for%20linear%20matrix%20equations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simoncini%2C%20Valeria%20Computational%20methods%20for%20linear%20matrix%20equations%202016"
        },
        {
            "id": "44",
            "entry": "[44] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In Advances in Neural Information Processing Systems, pages 3088\u20133096, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sindhwani%2C%20Vikas%20Sainath%2C%20Tara%20Kumar%2C%20Sanjiv%20Structured%20transforms%20for%20small-footprint%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sindhwani%2C%20Vikas%20Sainath%2C%20Tara%20Kumar%2C%20Sanjiv%20Structured%20transforms%20for%20small-footprint%20deep%20learning%202015"
        },
        {
            "id": "45",
            "entry": "[45] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classifiers. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 1094\u20131103, Fort Lauderdale, FL, USA, 20\u201322 Apr 2017. PMLR. URL http://proceedings.mlr.press/v54/sokolic17a.html.",
            "url": "http://proceedings.mlr.press/v54/sokolic17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sokolic%2C%20Jure%20Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Rodrigues%2C%20Miguel%20Generalization%20error%20of%20invariant%20classifiers%202017-04-20"
        },
        {
            "id": "46",
            "entry": "[46] Vladimir Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20Statistical%20learning%20theory%201998"
        },
        {
            "id": "47",
            "entry": "[47] Hugh E Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the American Mathematical Society, 133(1):167\u2013178, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Warren%2C%20Hugh%20E.%20Lower%20bounds%20for%20approximation%20by%20nonlinear%20manifolds%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Warren%2C%20Hugh%20E.%20Lower%20bounds%20for%20approximation%20by%20nonlinear%20manifolds%201968"
        },
        {
            "id": "48",
            "entry": "[48] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pages 1476\u20131483, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zichao%20Moczulski%2C%20Marcin%20Denil%2C%20Misha%20de%20Freitas%2C%20Nando%20Deep%20fried%20convnets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zichao%20Moczulski%2C%20Marcin%20Denil%2C%20Misha%20de%20Freitas%2C%20Nando%20Deep%20fried%20convnets%202015"
        },
        {
            "id": "49",
            "entry": "[49] Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, and Bo Yuan. Theoretical properties for neural networks with weight matrices of low displacement rank. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 4082\u20134090, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/zhao17b.html. ",
            "url": "http://proceedings.mlr.press/v70/zhao17b.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Liang%20Liao%2C%20Siyu%20Wang%2C%20Yanzhi%20Li%2C%20Zhe%20Theoretical%20properties%20for%20neural%20networks%20with%20weight%20matrices%20of%20low%20displacement%20rank%202017-08"
        }
    ]
}
