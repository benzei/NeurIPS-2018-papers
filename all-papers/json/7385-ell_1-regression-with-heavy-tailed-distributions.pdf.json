{
    "filename": "7385-ell_1-regression-with-heavy-tailed-distributions.pdf",
    "metadata": {
        "title": "$\\ell_1$-regression with Heavy-tailed Distributions",
        "author": "Lijun Zhang, Zhi-Hua Zhou",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7385-ell_1-regression-with-heavy-tailed-distributions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we consider the problem of linear regression with heavy-tailed distributions. Different from previous studies that use the squared loss to measure the performance, we choose the absolute loss, which is capable of estimating the conditional median. To address the challenge that both the input and output could be heavy-tailed, we propose a truncated minimization problem, and demonstrate that it enjoys an O( d/n) excess risk, where d is the dimensionality and n is the number of samples. Compared with traditional work on 1-regression, the main advantage of our result is that we achieve a high-probability risk bound without exponential moment conditions on the input and output. Furthermore, if the input is bounded, we show that the classical empirical risk minimization is competent for 1-regression even when the output is heavy-tailed."
    },
    "keywords": [
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "excess risk",
            "url": "https://en.wikipedia.org/wiki/excess_risk"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "linear function",
            "url": "https://en.wikipedia.org/wiki/linear_function"
        }
    ],
    "highlights": [
        "Linear regression used to be a mainstay of statistics, and remains one of our most important tools for data analysis [<a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\">Hastie et al, 2009</a></a>]",
        "Theoretical analysis shows that our method achieves an O( d/n) excess risk, which holds with high probability",
        "When the input is bounded, we prove that the classical empirical risk minimization achieves an O(D/ n) excess risk for 1-regression, where D is the maximum norm of the input, and does not require any assumption on the output",
        "Theorem 3 implies empirical risk minimization achieves an O(D/ n) excess risk which holds with high probability",
        "We prove that our method enjoys an O( d/n) excess risk, which holds with high probability",
        "Compared with traditional work on 1-regression, the main advantage of our result is that we establish a high-probability bound without exponential moment conditions on the input and output"
    ],
    "key_statements": [
        "Linear regression used to be a mainstay of statistics, and remains one of our most important tools for data analysis [<a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\">Hastie et al, 2009</a></a>]",
        "Theoretical analysis shows that our method achieves an O( d/n) excess risk, which holds with high probability",
        "When the input is bounded, we prove that the classical empirical risk minimization achieves an O(D/ n) excess risk for 1-regression, where D is the maximum norm of the input, and does not require any assumption on the output",
        "Theorem 3 implies empirical risk minimization achieves an O(D/ n) excess risk which holds with high probability",
        "We prove that our method enjoys an O( d/n) excess risk, which holds with high probability",
        "Compared with traditional work on 1-regression, the main advantage of our result is that we establish a high-probability bound without exponential moment conditions on the input and output",
        "We demonstrate that when the input is bounded, the classical empirical risk minimization is sufficient for 1-regression"
    ],
    "summary": [
        "Linear regression used to be a mainstay of statistics, and remains one of our most important tools for data analysis [<a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\"><a class=\"ref-link\" id=\"cHastie_et+al_2009_a\" href=\"#rHastie_et+al_2009_a\">Hastie et al, 2009</a></a>].",
        "For linear regression with squared loss, i.e., 2-regression, <a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\">Audibert and Catoni [2011</a></a></a></a>] develop a truncated min-max estimator, and establish an O(d/n) excess risk that holds with high probability even when the input and output are heavy-tailed.",
        "Inspired by the truncation function of <a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\">Audibert and Catoni [2011</a></a></a></a>], we propose a truncated minimization problem to support heavy-tailed distributions.",
        "This is the first time an O( d/n) excess risk is established for 1-regression under the condition that both the input and output could be heavy-tailed.",
        "[2015] develop a general theorem for heavy-tailed losses, when applied to 1-regression, it requires the input to be bounded.",
        "When the input is bounded, we prove that the classical ERM achieves an O(D/ n) excess risk for 1-regression, where D is the maximum norm of the input, and does not require any assumption on the output.",
        "<a class=\"ref-link\" id=\"cBrownlees_et+al_2015_a\" href=\"#rBrownlees_et+al_2015_a\">Brownlees et al [2015</a>] apply the robust estimator of <a class=\"ref-link\" id=\"cCatoni_2012_a\" href=\"#rCatoni_2012_a\">Catoni [2012</a>] to the general problem of learning with heavy-tailed losses.",
        "When applying their results to 1-regression, the linear function needs to be bounded, which means the input vector must be bounded [<a class=\"ref-link\" id=\"cBrownlees_et+al_2015_a\" href=\"#rBrownlees_et+al_2015_a\"><a class=\"ref-link\" id=\"cBrownlees_et+al_2015_a\" href=\"#rBrownlees_et+al_2015_a\">Brownlees et al, 2015</a></a>, Section 4.1.1].",
        "When applying to 2-regression, the input needs to be bounded and the theoretical guarantee is no longer a high-probability bound [<a class=\"ref-link\" id=\"cBrownlees_et+al_2015_a\" href=\"#rBrownlees_et+al_2015_a\"><a class=\"ref-link\" id=\"cBrownlees_et+al_2015_a\" href=\"#rBrownlees_et+al_2015_a\">Brownlees et al, 2015</a></a>, Section 4.1.2].",
        "For 2-regression with heavy-tailed distributions, a high-probability O(d/n) excess risk is established, under slightly stronger assumptions than those of <a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\">Audibert and Catoni [2011</a></a></a></a>].",
        "We first present our truncated minimization problem, discuss its theoretical guarantee, and study the special setting of bounded inputs.",
        "Inspired by the truncated min-max estimator of <a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\"><a class=\"ref-link\" id=\"cAudibert_2011_a\" href=\"#rAudibert_2011_a\">Audibert and Catoni [2011</a></a></a></a>], we propose the following truncated minimization problem for 1-regression with heavy-tailed distributions: 1n min w\u2208W n\u03b1 \u03c8 \u03b1|yi \u2212 xi w|",
        "Assumption 1 requires the domain W is bounded, the input and output could be unbounded, which allows us to model heavy-tailed distributions.",
        "Assumption 4 The domain W is a subset of Rd and its radius is bounded by B, that is, w \u2264 B, \u2200w \u2208 W \u2286 Rd. Let Br \u2286 Rd be a ball centered at origin with radius r, and N (Br, \u03b5) be its \u03b5-net with minimal cardinality, denoted by N (Br, \u03b5).",
        "We consider 1-regression with heavy-tailed distributions, and propose a truncated minimization problem.",
        "Compared with traditional work on 1-regression, the main advantage of our result is that we establish a high-probability bound without exponential moment conditions on the input and output.",
        "We demonstrate that when the input is bounded, the classical ERM is sufficient for 1-regression"
    ],
    "headline": "To address the challenge that both the input and output could be heavy-tailed, we propose a truncated minimization problem, and demonstrate that it enjoys an O excess risk, where d is the dimensionality and n is the number of samples",
    "reference_links": [
        {
            "id": "Alon_et+al_1999_a",
            "entry": "N. Alon, Y. Matias, and M. Szegedy. The space complexity of approximating the frequency moments. Journal of Computer and System Sciences, 58(1):137\u2013147, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alon%2C%20N.%20Matias%2C%20Y.%20Szegedy%2C%20M.%20The%20space%20complexity%20of%20approximating%20the%20frequency%20moments%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alon%2C%20N.%20Matias%2C%20Y.%20Szegedy%2C%20M.%20The%20space%20complexity%20of%20approximating%20the%20frequency%20moments%201999"
        },
        {
            "id": "Alquier_et+al_2017_a",
            "entry": "P. Alquier, V. Cottet, and G. Lecu\u00e9. Estimation bounds and sharp oracle inequalities of regularized procedures with lipschitz loss functions. ArXiv e-prints, arXiv:1702.01402, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.01402"
        },
        {
            "id": "Audibert_2011_a",
            "entry": "J.-Y. Audibert and O. Catoni. Robust linear least squares regression. The Annals of Statistics, 39(5): 2766\u20132794, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.-Y.%20Catoni%2C%20O.%20Robust%20linear%20least%20squares%20regression%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.-Y.%20Catoni%2C%20O.%20Robust%20linear%20least%20squares%20regression%202011"
        },
        {
            "id": "Birg_1998_a",
            "entry": "L. Birg\u00e9 and P. Massart. Minimum contrast estimators on sieves: exponential bounds and rates of convergence. Bernoulli, 4(3):329\u2013375, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Birg%C3%A9%2C%20L.%20Massart%2C%20P.%20Minimum%20contrast%20estimators%20on%20sieves%3A%20exponential%20bounds%20and%20rates%20of%20convergence%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Birg%C3%A9%2C%20L.%20Massart%2C%20P.%20Minimum%20contrast%20estimators%20on%20sieves%3A%20exponential%20bounds%20and%20rates%20of%20convergence%201998"
        },
        {
            "id": "Brownlees_et+al_2015_a",
            "entry": "C. Brownlees, E. Joly, and G. Lugosi. Empirical risk minimization for heavy-tailed losses. The Annals of Statistics, 43(6):2507\u20132536, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brownlees%2C%20C.%20Joly%2C%20E.%20Lugosi%2C%20G.%20Empirical%20risk%20minimization%20for%20heavy-tailed%20losses%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brownlees%2C%20C.%20Joly%2C%20E.%20Lugosi%2C%20G.%20Empirical%20risk%20minimization%20for%20heavy-tailed%20losses%202015"
        },
        {
            "id": "Bubeck_et+al_2013_a",
            "entry": "S. Bubeck, N. Cesa-Bianchi, and G. Lugosi. Bandits with heavy tail. IEEE Transactions on Information Theory, 59(11):7711\u20137717, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Lugosi%2C%20G.%20Bandits%20with%20heavy%20tail%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Lugosi%2C%20G.%20Bandits%20with%20heavy%20tail%202013"
        },
        {
            "id": "Catoni_2012_a",
            "entry": "O. Catoni. Challenging the empirical mean and empirical variance: A deviation study. Annales de l\u2019Institut Henri Poincar\u00e9, Probabilit\u00e9s et Statistiques, 48(4):1148\u20131185, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Catoni%2C%20O.%20Challenging%20the%20empirical%20mean%20and%20empirical%20variance%3A%20A%20deviation%20study.%20Annales%20de%20l%E2%80%99Institut%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Catoni%2C%20O.%20Challenging%20the%20empirical%20mean%20and%20empirical%20variance%3A%20A%20deviation%20study.%20Annales%20de%20l%E2%80%99Institut%202012"
        },
        {
            "id": "Cucker_2002_a",
            "entry": "F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39(1):1\u201349, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cucker%2C%20F.%20Smale%2C%20S.%20On%20the%20mathematical%20foundations%20of%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cucker%2C%20F.%20Smale%2C%20S.%20On%20the%20mathematical%20foundations%20of%20learning%202002"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "V. C. Dinh, L. S. Ho, B. Nguyen, and D. Nguyen. Fast learning rates with heavy-tailed losses. In Advances in Neural Information Processing Systems 29, pages 505\u2013513, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20V.C.%20Ho%2C%20L.S.%20Nguyen%2C%20B.%20Nguyen%2C%20D.%20Fast%20learning%20rates%20with%20heavy-tailed%20losses%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20V.C.%20Ho%2C%20L.S.%20Nguyen%2C%20B.%20Nguyen%2C%20D.%20Fast%20learning%20rates%20with%20heavy-tailed%20losses%202016"
        },
        {
            "id": "Finkenstaedt_2003_a",
            "entry": "B. Finkenst\u00e4dt and H. Rootz\u00e9n, editors. Extreme Values in Finance, Telecommunications, and the Environment. Chapman & Hall/CRC, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Extreme%20Values%20in%20Finance%2C%20Telecommunications%2C%20and%20the%20Environment%202003"
        },
        {
            "id": "Foss_et+al_2013_a",
            "entry": "S. Foss, D. Korshunov, and S. Zachary. An Introduction to Heavy-Tailed and Subexponential Distributions. Springer, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foss%2C%20S.%20Korshunov%2C%20D.%20Zachary%2C%20S.%20An%20Introduction%20to%20Heavy-Tailed%20and%20Subexponential%20Distributions%202013"
        },
        {
            "id": "Gyoerfi_et+al_2002_a",
            "entry": "L. Gy\u00f6rfi, M. Kohler, A. Krzyzak, and H. Walk. A Distribution-Free Theory of Nonparametric Regression. Springer, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gy%C3%B6rfi%2C%20L.%20Kohler%2C%20M.%20Krzyzak%2C%20A.%20Walk%2C%20H.%20A%20Distribution-Free%20Theory%20of%20Nonparametric%20Regression%202002"
        },
        {
            "id": "Hastie_et+al_2009_a",
            "entry": "T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Series in Statistics. Springer New York, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20T.%20Tibshirani%2C%20R.%20Friedman%2C%20J.%20The%20Elements%20of%20Statistical%20Learning.%20Springer%20Series%20in%20Statistics%202009"
        },
        {
            "id": "Hazan_et+al_2015_a",
            "entry": "E. Hazan, K. Levy, and S. Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems 28, pages 1594\u20131602, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Levy%2C%20K.%20Shalev-Shwartz%2C%20S.%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Levy%2C%20K.%20Shalev-Shwartz%2C%20S.%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015"
        },
        {
            "id": "Hsu_2014_a",
            "entry": "D. Hsu and S. Sabato. Heavy-tailed regression with a generalized median-of-means. In Proceedings of the 31st International Conference on Machine Learning, pages 37\u201345, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20D.%20Sabato%2C%20S.%20Heavy-tailed%20regression%20with%20a%20generalized%20median-of-means%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20D.%20Sabato%2C%20S.%20Heavy-tailed%20regression%20with%20a%20generalized%20median-of-means%202014"
        },
        {
            "id": "Hsu_2016_a",
            "entry": "D. Hsu and S. Sabato. Loss minimization and parameter estimation with heavy tails. Journal of Machine Learning Research, 17(18):1\u201340, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20D.%20Sabato%2C%20S.%20Loss%20minimization%20and%20parameter%20estimation%20with%20heavy%20tails%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20D.%20Sabato%2C%20S.%20Loss%20minimization%20and%20parameter%20estimation%20with%20heavy%20tails%202016"
        },
        {
            "id": "Koltchinskii_2011_a",
            "entry": "V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20V.%20Oracle%20Inequalities%20in%20Empirical%20Risk%20Minimization%20and%20Sparse%20Recovery%20Problems%202011"
        },
        {
            "id": "Lugosi_2009_a",
            "entry": "G. Lugosi. Concentration-of-measure inequalities. Technical report, Department of Economics, Pompeu Fabra University, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lugosi%2C%20G.%20Concentration-of-measure%20inequalities%202009"
        },
        {
            "id": "Lugosi_2016_a",
            "entry": "G. Lugosi and S. Mendelson. Risk minimization by median-of-means tournaments. ArXiv e-prints, arXiv:1608.00757, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.00757"
        },
        {
            "id": "Mendelson_2014_a",
            "entry": "S. Mendelson. Learning without concentration. In Proceedings of the 27th Annual Conference on Learning Theory, pages 25\u201339, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mendelson%2C%20S.%20Learning%20without%20concentration%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mendelson%2C%20S.%20Learning%20without%20concentration%202014"
        },
        {
            "id": "Mendelson_2015_a",
            "entry": "S. Mendelson. Learning without concentration. Journal of the ACM, 62(3):21:1\u201321:25, 2015. S. Minsker. Geometric median and robust estimation in Banach spaces. Bernoulli, 21(4):2308\u20132335, 2015. A. Nemirovski and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. John",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mendelson%2C%20S.%20Learning%20without%20concentration%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mendelson%2C%20S.%20Learning%20without%20concentration%202015"
        },
        {
            "id": "Wiley_1987_a",
            "entry": "Wiley & Sons Ltd, 1983. A. M. L. Peter J. Rousseeuw. Robust Regression and Outlier Detection. John Wiley & Sons Inc, 1987. G. Pisier. The volume of convex bodies and Banach space geometry. Cambridge Tracts in Mathematics",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wiley%20%26%20Sons%20Ltd%2C%201983.%20A.%20M.%20L.%20Peter%20J.%20Rousseeuw%20Robust%20Regression%20and%20Outlier%20Detection%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wiley%20%26%20Sons%20Ltd%2C%201983.%20A.%20M.%20L.%20Peter%20J.%20Rousseeuw%20Robust%20Regression%20and%20Outlier%20Detection%201987"
        },
        {
            "id": "(No._1989_a",
            "entry": "(No. 94). Cambridge University Press, 1989. Y. Plan and R. Vershynin. One-bit compressed sensing by linear programming. Communications on",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20No.%2094%29%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20No.%2094%29%201989"
        },
        {
            "id": "Vapnik_2013_a",
            "entry": "Pure and Applied Mathematics, 66(8):1275\u20131297, 2013. M. Talagrand. The Generic Chaining. Springer, 2005. V. Vapnik. The Nature of Statistical Learning Theory. Springer, second edition, 2000. L. Zhang and Z.-H. Zhou. 1-regression with heavy-tailed distributions. ArXiv e-prints, arXiv:1805.00616, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00616"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "L. Zhang, T. Yang, and R. Jin. Empirical risk minimization for stochastic convex optimization: O(1/n)-and O(1/n2)-type of risk bounds. In Proceedings of the 30th Annual Conference on Learning Theory, pages 1954\u20131979, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20L.%20Yang%2C%20T.%20Jin%2C%20R.%20Empirical%20risk%20minimization%20for%20stochastic%20convex%20optimization%3A%20O%281/n%29-and%20O%281/n2%29-type%20of%20risk%20bounds%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20L.%20Yang%2C%20T.%20Jin%2C%20R.%20Empirical%20risk%20minimization%20for%20stochastic%20convex%20optimization%3A%20O%281/n%29-and%20O%281/n2%29-type%20of%20risk%20bounds%202017"
        }
    ]
}
