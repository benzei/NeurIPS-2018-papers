{
    "filename": "8175-global-non-convex-optimization-with-discretized-diffusions.pdf",
    "metadata": {
        "date": 2014,
        "title": "S\u00e9minaire de Probabilit\u00e9s XLVI",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8175-global-non-convex-optimization-with-discretized-diffusions.pdf",
            "doi": "10.1007/978-3-319-11970-0"
        },
        "journal": "Lecture Notes in Mathematics",
        "abstract": "An Euler discretization of the Langevin diffusion is known to converge to the global minimizers of certain convex and non-convex optimization problems. We show that this property holds for any suitably smooth diffusion and that different diffusions are suitable for optimizing different classes of convex and non-convex functions. This allows us to design diffusions suitable for globally optimizing convex and non-convex functions not covered by the existing Langevin theory. Our non-asymptotic analysis delivers computable optimization and integration error bounds based on easily accessed properties of the objective and chosen diffusion. Central to our approach are new explicit Stein factor bounds on the solutions of Poisson equations. We complement these results with improved optimization guarantees for targets other than the standard Gibbs measure."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "stochastic differential equation",
            "url": "https://en.wikipedia.org/wiki/stochastic_differential_equation"
        },
        {
            "term": "poisson equation",
            "url": "https://en.wikipedia.org/wiki/poisson_equation"
        },
        {
            "term": "gibbs measure",
            "url": "https://en.wikipedia.org/wiki/gibbs_measure"
        },
        {
            "term": "euler discretization",
            "url": "https://en.wikipedia.org/wiki/euler_discretization"
        }
    ],
    "highlights": [
        "Consider the unconstrained and possibly non-convex optimization problem minimize f (x). x2Rd<br/><br/>Recent studies have shown that the Langevin algorithm \u2013 in which an appropriately scaled isotropic Gaussian vector is added to a gradient descent update \u2013 globally optimizes f whenever the objective is dissipative, xi \u21b5kxk22 for \u21b5 > 0) with a Lipschitz gradient [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "Recent studies have shown that the Langevin algorithm \u2013 in which an appropriately scaled isotropic Gaussian vector is added to a gradient descent update \u2013 globally optimizes f whenever the objective is dissipative, xi \u21b5kxk22 for \u21b5 > 0) with a Lipschitz gradient [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "The intuition behind the success of the Langevin algorithm is that the stochastic optimization method approximately tracks the continuous-time Langevin diffusion which admits the Gibbs measure \u2013 a distribution defined by p (x) / exp( f (x)) \u2013 as its invariant distribution",
        "We demonstrate that improved optimization guarantees can be obtained by targeting distributions other than the standard Gibbs measure. We show that different diffusions are appropriate for different objectives f and detail concrete examples of global non-convex optimization enabled by our framework but not covered by the existing Langevin theory",
        "We established non-asymptotic bounds on global optimization error and integration error with convergence governed by Stein factors obtained from the solution of the Poisson equation",
        "We demonstrated that targeting distributions other than the Gibbs measure can give rise to improved optimization guarantees"
    ],
    "key_statements": [
        "Consider the unconstrained and possibly non-convex optimization problem minimize f (x). x2Rd<br/><br/>Recent studies have shown that the Langevin algorithm \u2013 in which an appropriately scaled isotropic Gaussian vector is added to a gradient descent update \u2013 globally optimizes f whenever the objective is dissipative, xi \u21b5kxk22 for \u21b5 > 0) with a Lipschitz gradient [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "Recent studies have shown that the Langevin algorithm \u2013 in which an appropriately scaled isotropic Gaussian vector is added to a gradient descent update \u2013 globally optimizes f whenever the objective is dissipative, xi \u21b5kxk22 for \u21b5 > 0) with a Lipschitz gradient [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "The intuition behind the success of the Langevin algorithm is that the stochastic optimization method approximately tracks the continuous-time Langevin diffusion which admits the Gibbs measure \u2013 a distribution defined by p (x) / exp( f (x)) \u2013 as its invariant distribution",
        "> 0 is an inverse temperature parameter, and when is large, the Gibbs measure concentrates around its modes",
        "We prove an analogous global optimization property for the Euler discretization of any smooth and dissipative diffusion and show that different diffusions are suitable for solving different classes of convex and non-convex problems",
        "Diffusion\u2019s coefficients and Stein factors, i.e., bounds on the derivatives of the associated Poisson equation solution. For pseudo-Lipschitz f , we derive explicit first through fourth-order Stein factor bounds for every fast-coupling diffusion with smooth coefficients",
        "Since our bounds depend on Wasserstein coupling rates, we provide user-friendly, broadly applicable tools for computing these rates",
        "The resulting computable integration error bounds recover the known Markov chain Monte Carlo convergence rates of the Langevin algorithm in both convex and non-convex settings but apply more broadly. We introduce new explicit bounds on the expected suboptimality of sampling from a diffusion",
        "We demonstrate that improved optimization guarantees can be obtained by targeting distributions other than the standard Gibbs measure. We show that different diffusions are appropriate for different objectives f and detail concrete examples of global non-convex optimization enabled by our framework but not covered by the existing Langevin theory",
        "Our convergence analysis builds on the arguments of [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], and our Stein factor bounds rely on distant and uniform dissipativity conditions for L1-Wasserstein rate decay [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and the smoothing effect of the Markov semigroup [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "The integration error\u2014which captures both the short-term non-stationarity of the chain and the long-term bias due to discretization\u2014is the subject of Section 3; we develop explicit bounds using techniques that build upon [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "In Section 4, we extend the Gibbs measure Langevin diffusion bound of Raginsky et al [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], to more general invariant measures and associated diffusions and demonstrate the benefits of targeting non-Gibbs measures",
        "We develop our explicit bounds on integration error in three steps",
        "In Theorem 3.1, we bound integration error in terms of the polynomial growth and dissipativity of diffusion coefficients (Conditions 1 and 2) and Stein factors bounds on the derivatives of solutions to the diffusion\u2019s Poisson equation (Condition 3)",
        "We explore user-friendly conditions implying fast Wasserstein decay in Section 3.1 and detailed examples deploying these tools in Section 5",
        "We showed that the Euler discretization of any smooth and dissipative diffusion can be used for global non-convex optimization",
        "We established non-asymptotic bounds on global optimization error and integration error with convergence governed by Stein factors obtained from the solution of the Poisson equation",
        "We further provided explicit bounds on Stein factors for large classes of convex and non-convex objective functions, based on computable properties of the objective and the diffusion",
        "We designed suitable diffusions for optimizing non-convex functions not covered by the existing Langevin theory",
        "We demonstrated that targeting distributions other than the Gibbs measure can give rise to improved optimization guarantees"
    ],
    "summary": [
        "Consider the unconstrained and possibly non-convex optimization problem minimize f (x). x2Rd<br/><br/>Recent studies have shown that the Langevin algorithm \u2013 in which an appropriately scaled isotropic Gaussian vector is added to a gradient descent update \u2013 globally optimizes f whenever the objective is dissipative, xi \u21b5kxk22 for \u21b5 > 0) with a Lipschitz gradient [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>].",
        "1 \u270f2 bounds on the numerical integration error of discretized dissipative diffusions.",
        "For pseudo-Lipschitz f , we derive explicit first through fourth-order Stein factor bounds for every fast-coupling diffusion with smooth coefficients.",
        "Section 3 provides explicit bounds on integration error in terms of Stein factors and on Stein factors in terms of simple properties of f and the diffusion.",
        "Our convergence analysis builds on the arguments of [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], and our Stein factor bounds rely on distant and uniform dissipativity conditions for L1-Wasserstein rate decay [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] and the smoothing effect of the Markov semigroup [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "In Theorem 3.1, we bound integration error in terms of the polynomial growth and dissipativity of diffusion coefficients (Conditions 1 and 2) and Stein factors bounds on the derivatives of solutions to the diffusion\u2019s Poisson equation (Condition 3).",
        "We will derive explicit expressions for the Stein factors \u21e3i for a wide variety of diffusions and functions f , but first we will use the factors bound integration error of our discretized diffusion.",
        "As our second principal contribution, we derive explicit values for the Stein factors \u21e3i for any smooth and dissipative diffusion exhibiting fast L1-Wasserstein decay: Condition 4 (Wasserstein rate).",
        "Proved in Appendix E, provides a user-friendly set of sufficient conditions for verifying distant dissipativity and exponential Wasserstein decay in practice.",
        "We demonstrate that, for quadratic functions, the generalized Gibbs expected suboptimality bound (4.1) can be further refined to remove the log( /d)1/\u2713 dependence.",
        "We provide detailed examples of verifying that a given diffusion is appropriate for optimizing a given objective, using either uniform dissipativity (Prop.",
        "Kxk22), a simple non-convex objective which exhibits sublinear growth in kxk2 and does not satisfy dissippativity (Condition 2) when paired with the Gibbs measure Langevin diffusion (b = rf, = 2/ I).",
        "The reader can verify that all of these examples satisfy the remaining global optimization pre-conditions of Corollary 4.2 and Theorem 3.2.",
        "We showed that the Euler discretization of any smooth and dissipative diffusion can be used for global non-convex optimization.",
        "We established non-asymptotic bounds on global optimization error and integration error with convergence governed by Stein factors obtained from the solution of the Poisson equation.",
        "We further provided explicit bounds on Stein factors for large classes of convex and non-convex objective functions, based on computable properties of the objective and the diffusion.",
        "We demonstrated that targeting distributions other than the Gibbs measure can give rise to improved optimization guarantees"
    ],
    "headline": "We show that this property holds for any suitably smooth diffusion and that different diffusions are suitable for optimizing different classes of convex and non-convex functions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. L. Bartlett, M. I. Jordan, and J. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101:138\u2013156, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006"
        },
        {
            "id": "2",
            "entry": "[2] J.-M. Bismut. Large deviation and malliavin calculus. Progress in Mathematics, 45, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bismut%2C%20J.-M.%20Large%20deviation%20and%20malliavin%20calculus%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bismut%2C%20J.-M.%20Large%20deviation%20and%20malliavin%20calculus%201984"
        },
        {
            "id": "3",
            "entry": "[3] P. Cattiaux and A. Guillin. Semi log-concave Markov diffusions. In Seminaire de Probabilites XLVI, volume Springer, Cham, 2014. doi: 10.1007/978-3-319-11970-0 9.",
            "crossref": "https://dx.doi.org/10.1007/978-3-319-11970-0"
        },
        {
            "id": "4",
            "entry": "[4] S. Cerrai. Second order PDE\u2019s in finite and infinite dimension: a probabilistic approach, volume 1762. Springer Science & Business Media, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Cerrai%20Second%20order%20PDEs%20in%20finite%20and%20infinite%20dimension%20a%20probabilistic%20approach%20volume%201762%20Springer%20Science%20%20Business%20Media%202001"
        },
        {
            "id": "5",
            "entry": "[5] C. Chen, N. Ding, and L. Carin. On the convergence of stochastic gradient mcmc algorithms with high-order integrators. In Advances in Neural Information Processing Systems, pages 2278\u20132286, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20C.%20Ding%2C%20N.%20Carin%2C%20L.%20On%20the%20convergence%20of%20stochastic%20gradient%20mcmc%20algorithms%20with%20high-order%20integrators%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20C.%20Ding%2C%20N.%20Carin%2C%20L.%20On%20the%20convergence%20of%20stochastic%20gradient%20mcmc%20algorithms%20with%20high-order%20integrators%202015"
        },
        {
            "id": "6",
            "entry": "[6] X. Cheng, N. S. Chatterji, Y. Abbasi-Yadkori, P. L. Bartlett, and M. I. Jordan. Sharp convergence rates for langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01648"
        },
        {
            "id": "7",
            "entry": "[7] A. S. Dalalyan. Further and stronger analogy between sampling and optimization: Langevin monte carlo and gradient descent. arXiv preprint arXiv:1704.04752, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04752"
        },
        {
            "id": "8",
            "entry": "[8] A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):651\u2013676, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalalyan%2C%20A.S.%20Theoretical%20guarantees%20for%20approximate%20sampling%20from%20smooth%20and%20log-concave%20densities%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalalyan%2C%20A.S.%20Theoretical%20guarantees%20for%20approximate%20sampling%20from%20smooth%20and%20log-concave%20densities%202017"
        },
        {
            "id": "9",
            "entry": "[9] A. S. Dalalyan and A. Tsybakov. Sparse regression learning by aggregation and langevin monte-carlo. Journal of Computer and System Sciences, 78:1423\u20131443, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalalyan%2C%20A.S.%20Tsybakov%2C%20A.%20Sparse%20regression%20learning%20by%20aggregation%20and%20langevin%20monte-carlo%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalalyan%2C%20A.S.%20Tsybakov%2C%20A.%20Sparse%20regression%20learning%20by%20aggregation%20and%20langevin%20monte-carlo%202012"
        },
        {
            "id": "10",
            "entry": "[10] A. Durmus, E. Moulines, et al. Nonasymptotic convergence analysis for the unadjusted langevin algorithm. The Annals of Applied Probability, 27(3):1551\u20131587, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durmus%2C%20A.%20Moulines%2C%20E.%20Nonasymptotic%20convergence%20analysis%20for%20the%20unadjusted%20langevin%20algorithm%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durmus%2C%20A.%20Moulines%2C%20E.%20Nonasymptotic%20convergence%20analysis%20for%20the%20unadjusted%20langevin%20algorithm%202017"
        },
        {
            "id": "11",
            "entry": "[11] R. Dwivedi, Y. Chen, M. J. Wainwright, and B. Yu. Log-concave sampling: Metropolis-hastings algorithms are fast! arXiv preprint arXiv:1801.02309, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02309"
        },
        {
            "id": "12",
            "entry": "[12] A. Eberle. Reflection couplings and contraction rates for diffusions. Probability theory and related fields, 166(3-4):851\u2013886, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eberle%2C%20A.%20Reflection%20couplings%20and%20contraction%20rates%20for%20diffusions.%20Probability%20theory%20and%20related%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eberle%2C%20A.%20Reflection%20couplings%20and%20contraction%20rates%20for%20diffusions.%20Probability%20theory%20and%20related%202016"
        },
        {
            "id": "13",
            "entry": "[13] K. D. Elworthy and X.-M. Li. Formulae for the derivatives of heat semigroups. Journal of Functional Analysis, 125(1):252\u2013286, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elworthy%2C%20K.D.%20Li%2C%20X.-M.%20Formulae%20for%20the%20derivatives%20of%20heat%20semigroups%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elworthy%2C%20K.D.%20Li%2C%20X.-M.%20Formulae%20for%20the%20derivatives%20of%20heat%20semigroups%201994"
        },
        {
            "id": "14",
            "entry": "[14] S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in rd. SIAM Journal on Control and Optimization, 29(5):999\u20131018, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gelfand%2C%20S.B.%20Mitter%2C%20S.K.%20Recursive%20stochastic%20algorithms%20for%20global%20optimization%20in%20rd%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gelfand%2C%20S.B.%20Mitter%2C%20S.K.%20Recursive%20stochastic%20algorithms%20for%20global%20optimization%20in%20rd%201991"
        },
        {
            "id": "15",
            "entry": "[15] J. Gorham, A. B. Duncan, S. J. Vollmer, and L. Mackey. Measuring sample quality with diffusions. arXiv preprint arXiv:1611.06972, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06972"
        },
        {
            "id": "16",
            "entry": "[16] I. S. Gradshteyn and I. M. Ryzhik. Table of integrals, series, and products. Academic press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gradshteyn%2C%20I.S.%20Ryzhik%2C%20I.M.%20Table%20of%20integrals%2C%20series%2C%20and%20products%202014"
        },
        {
            "id": "17",
            "entry": "[17] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, ISBN: 0521540518, second edition, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartley%2C%20R.%20Zisserman%2C%20A.%20Multiple%20View%20Geometry%20in%20Computer%20Vision%202004"
        },
        {
            "id": "18",
            "entry": "[18] G. J. O. Jameson. Inequalities for gamma function ratios. The American Mathematical Monthly, 120(10): 936\u2013940, 2013. ISSN 00029890, 19300972. URL http://www.jstor.org/stable/10.4169/amer.math.monthly.120.10.936.",
            "url": "http://www.jstor.org/stable/10.4169/amer.math.monthly.120.10.936",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jameson%2C%20G.J.O.%20Inequalities%20for%20gamma%20function%20ratios%202013"
        },
        {
            "id": "19",
            "entry": "[19] R. Khasminskii. Stochastic stability of differential equations, volume 66. Springer Science & Business Media, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khasminskii%2C%20R.%20Stochastic%20stability%20of%20differential%20equations%2C%20volume%2066%202011"
        },
        {
            "id": "20",
            "entry": "[20] Y.-A. Ma, T. Chen, and E. Fox. A complete recipe for stochastic gradient mcmc. In Advances in Neural Information Processing Systems, pages 2917\u20132925, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Y.-A.%20Chen%2C%20T.%20Fox%2C%20E.%20A%20complete%20recipe%20for%20stochastic%20gradient%20mcmc%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Y.-A.%20Chen%2C%20T.%20Fox%2C%20E.%20A%20complete%20recipe%20for%20stochastic%20gradient%20mcmc%202015"
        },
        {
            "id": "21",
            "entry": "[21] A. Mathai and S. Provost. Quadratic forms in random variables: Theory and applications. 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mathai%2C%20A.%20Provost%2C%20S.%20Quadratic%20forms%20in%20random%20variables%3A%20Theory%20and%20applications%201992"
        },
        {
            "id": "22",
            "entry": "[22] J. C. Mattingly, A. M. Stuart, and D. J. Higham. Ergodicity for sdes and approximations: locally lipschitz vector fields and degenerate noise. Stochastic processes and their applications, 101(2):185\u2013232, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mattingly%2C%20J.C.%20Stuart%2C%20A.M.%20Higham%2C%20D.J.%20Ergodicity%20for%20sdes%20and%20approximations%3A%20locally%20lipschitz%20vector%20fields%20and%20degenerate%20noise.%20Stochastic%20processes%20and%20their%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mattingly%2C%20J.C.%20Stuart%2C%20A.M.%20Higham%2C%20D.J.%20Ergodicity%20for%20sdes%20and%20approximations%3A%20locally%20lipschitz%20vector%20fields%20and%20degenerate%20noise.%20Stochastic%20processes%20and%20their%202002"
        },
        {
            "id": "23",
            "entry": "[23] J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov. Convergence of numerical time-averaging and stationary measures via poisson equations. SIAM Journal on Numerical Analysis, 48(2):552\u2013577, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mattingly%2C%20J.C.%20Stuart%2C%20A.M.%20V%2C%20M.%20Tretyakov.%20Convergence%20of%20numerical%20time-averaging%20and%20stationary%20measures%20via%20poisson%20equations%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mattingly%2C%20J.C.%20Stuart%2C%20A.M.%20V%2C%20M.%20Tretyakov.%20Convergence%20of%20numerical%20time-averaging%20and%20stationary%20measures%20via%20poisson%20equations%202010"
        },
        {
            "id": "25",
            "entry": "[25] E. Pardoux and A. Veretennikov. On the Poisson equation and diffusion approximation. i. Ann. Probab., pages 1061\u20131085, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pardoux%2C%20E.%20Veretennikov%2C%20A.%20On%20the%20Poisson%20equation%20and%20diffusion%20approximation%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pardoux%2C%20E.%20Veretennikov%2C%20A.%20On%20the%20Poisson%20equation%20and%20diffusion%20approximation%202001"
        },
        {
            "id": "26",
            "entry": "[26] M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.03849"
        },
        {
            "id": "27",
            "entry": "[27] S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. Exploration of the (non-) asymptotic bias and variance of stochastic gradient langevin dynamics. Journal of Machine Learning Research 17, pages 1\u201348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vollmer%2C%20S.J.%20Zygalakis%2C%20K.C.%20Teh%2C%20Y.W.%20Exploration%20of%20the%20%28non-%29%20asymptotic%20bias%20and%20variance%20of%20stochastic%20gradient%20langevin%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vollmer%2C%20S.J.%20Zygalakis%2C%20K.C.%20Teh%2C%20Y.W.%20Exploration%20of%20the%20%28non-%29%20asymptotic%20bias%20and%20variance%20of%20stochastic%20gradient%20langevin%20dynamics%202016"
        },
        {
            "id": "28",
            "entry": "[28] F. Wang. Exponential Contraction in Wasserstein Distances for Diffusion Semigroups with Negative Curvature. arXiv:1608.04471, Mar. 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04471"
        },
        {
            "id": "29",
            "entry": "[29] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681\u2013688, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20learning%20via%20stochastic%20gradient%20langevin%20dynamics%202011"
        },
        {
            "id": "30",
            "entry": "[30] P. Xu, J. Chen, and Q. Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. arXiv preprint arXiv:1707.06618, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1707.06618"
        }
    ]
}
