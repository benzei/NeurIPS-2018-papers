{
    "filename": "7477-nonparametric-learning-from-bayesian-models-with-randomized-objective-functions.pdf",
    "metadata": {
        "title": "Nonparametric learning from Bayesian models with randomized objective functions",
        "author": "Simon Lyddon, Stephen Walker, Chris C. Holmes",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7477-nonparametric-learning-from-bayesian-models-with-randomized-objective-functions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic, particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small, and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests."
    },
    "keywords": [
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        },
        {
            "term": "bayesian updating",
            "url": "https://en.wikipedia.org/wiki/bayesian_updating"
        },
        {
            "term": "parametric model",
            "url": "https://en.wikipedia.org/wiki/parametric_model"
        },
        {
            "term": "Random forests",
            "url": "https://en.wikipedia.org/wiki/Random_forests"
        },
        {
            "term": "variational Bayes",
            "url": "https://en.wikipedia.org/wiki/variational_Bayes"
        }
    ],
    "highlights": [
        "Bayesian updating provides a principled and coherent approach to inference for probabilistic models [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], but is predicated on the model class being true",
        "One advantage of this approach is that a number of theoretical results hold, as for large enough n, under this adaptive scheme the parametric model is in effect \u2018switched off\u2019, and essentially the mixture of Dirichlet processes with c = 0 is used to generate posterior samples",
        "As detailed in Section 3 of the Supplementary Material, for small c we find that Bayesian RF and Random forests have similar performance, but as c increases, more weight is given to the externally-trained component and we find that Bayesian RF outperforms Random forests",
        "We have introduced a new approach for scalable Bayesian nonparametric learning, nonparametric learning, for parametric models that facilitates prior regularization via a baseline model, and corrects for model misspecification by incorporating an empirical component that has greater influence as the number of observations grows",
        "The NP posterior predictive mixes over the parametric model space as opposed to targeting F0 directly, though this may aid interpretation compared to fully nonparametric approaches"
    ],
    "key_statements": [
        "Bayesian updating provides a principled and coherent approach to inference for probabilistic models [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], but is predicated on the model class being true",
        "In the case of model misspecification, [S1], if the data contains no ties, this is the parametric Bayesian posterior under {f\u03b3(x), \u03c0(\u03b3)}, for which there is a large literature of computational methods available for sampling - see for example [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "One advantage of this approach is that a number of theoretical results hold, as for large enough n, under this adaptive scheme the parametric model is in effect \u2018switched off\u2019, and essentially the mixture of Dirichlet processes with c = 0 is used to generate posterior samples",
        "We demonstrate this in practice through a variational Bayes logistic regression model fit to the Statlog German Credit dataset, containing 1000 observations and 25 covariates, from the UCI ML repository [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], preprocessing via [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "In Fig. 2 we show that the NP update effectively corrects the variational Bayes approximation for small values of c",
        "As detailed in Section 3 of the Supplementary Material, for small c we find that Bayesian RF and Random forests have similar performance, but as c increases, more weight is given to the externally-trained component and we find that Bayesian RF outperforms Random forests",
        "We demonstrate direct inference for a functional of interest using the population median, under a misspecified univariate Gaussian model, as an example, where with parameter of interest is \u03b10 = arg min\u03b1 |\u03b1 \u2212 x|dF0(x), and an mixture of Dirichlet processes prior centered at a N (\u03b8, 1) with prior \u03c0(\u03b8) = N (0, 102) and data generated from a skew-normal distribution",
        "We have introduced a new approach for scalable Bayesian nonparametric learning, nonparametric learning, for parametric models that facilitates prior regularization via a baseline model, and corrects for model misspecification by incorporating an empirical component that has greater influence as the number of observations grows",
        "The NP posterior predictive mixes over the parametric model space as opposed to targeting F0 directly, though this may aid interpretation compared to fully nonparametric approaches",
        "We randomize an objective function according to a Bayesian nonparametric prior on the sampling distribution, leading to a quantification of subjective beliefs on the value that would be returned by the estimator under an infinite sample size"
    ],
    "summary": [
        "Bayesian updating provides a principled and coherent approach to inference for probabilistic models [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], but is predicated on the model class being true.",
        "We can see from the form of the conditional MDP (4) that the sampling distribution of the centering model, f\u03b8(x), regularizes the influence of the empirical data n i=1 \u03b4xi (\u00b7).",
        "The parametric mixing distribution \u03c0(\u03b8|x1:n) of the MDP posterior in (6) is constructed from the same centering model that defines the target parameter, \u03b80 = arg max\u03b8 log f\u03b8(x)dF0(x).",
        "In the case of model misspecification, [S1], if the data contains no ties, this is the parametric Bayesian posterior under {f\u03b3(x), \u03c0(\u03b3)}, for which there is a large literature of computational methods available for sampling - see for example [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>].",
        "One advantage of this approach is that a number of theoretical results hold, as for large enough n, under this adaptive scheme the parametric model is in effect \u2018switched off\u2019, and essentially the MDP with c = 0 is used to generate posterior samples.",
        "Uncertainty in the data-generating mechanism is quantified via a NP update that takes into account the model likelihood, prior, and concentration parameter c.",
        "We demonstrate direct inference for a functional of interest using the population median, under a misspecified univariate Gaussian model, as an example, where with parameter of interest is \u03b10 = arg min\u03b1 |\u03b1 \u2212 x|dF0(x), and an MDP prior centered at a N (\u03b8, 1) with prior \u03c0(\u03b8) = N (0, 102) and data generated from a skew-normal distribution.",
        "We use the posterior bootstrap to generate posterior samples that incorporate the prior model information with that from the data.",
        "We have introduced a new approach for scalable Bayesian nonparametric learning, NPL, for parametric models that facilitates prior regularization via a baseline model, and corrects for model misspecification by incorporating an empirical component that has greater influence as the number of observations grows.",
        "The Bayesian approach conditions on data and treats the unknown parameter of interest as if it was a random variable with some prior on a known model class.",
        "We randomize an objective function according to a Bayesian nonparametric prior on the sampling distribution, leading to a quantification of subjective beliefs on the value that would be returned by the estimator under an infinite sample size.",
        "The posterior bootstrap acts on an augmented dataset, comprised of data and posterior pseudo-samples, under which randomized maximum likelihood estimators provide a well-motivated quantification of uncertainty while assuming little about the data-generating mechanism."
    ],
    "headline": "We present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Hirotugu Akaike. Likelihood of a model and information criteria. Journal of Econometrics, 16(1):3\u201314, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akaike%2C%20Hirotugu%20Likelihood%20of%20a%20model%20and%20information%20criteria%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akaike%2C%20Hirotugu%20Likelihood%20of%20a%20model%20and%20information%20criteria%201981"
        },
        {
            "id": "2",
            "entry": "[2] Charles E. Antoniak. Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems. Annals of Statistics, 2(6):1152\u20131174, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antoniak%2C%20Charles%20E.%20Mixtures%20of%20Dirichlet%20Processes%20with%20Applications%20to%20Bayesian%20Nonparametric%20Problems%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antoniak%2C%20Charles%20E.%20Mixtures%20of%20Dirichlet%20Processes%20with%20Applications%20to%20Bayesian%20Nonparametric%20Problems%201974"
        },
        {
            "id": "3",
            "entry": "[3] Jos\u00e9 M. Bernardo and Adrian F. M. Smith. Bayesian Theory. Wiley, Chichester, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bernardo%2C%20Jos%C3%A9%20M.%20Smith%2C%20Adrian%20F.M.%20Bayesian%20Theory%202006"
        },
        {
            "id": "4",
            "entry": "[4] Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20Christopher%20M.%20Pattern%20Recognition%20and%20Machine%20Learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bishop%2C%20Christopher%20M.%20Pattern%20Recognition%20and%20Machine%20Learning%202006"
        },
        {
            "id": "5",
            "entry": "[5] P. G. Bissiri, C. C. Holmes, and S. G. Walker. A general framework for updating belief distributions. Journal of the Royal Statistical Society. Series B: Statistical Methodology, 78(5):1103\u20131130, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bissiri%2C%20P.G.%20Holmes%2C%20C.C.%20Walker%2C%20S.G.%20A%20general%20framework%20for%20updating%20belief%20distributions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bissiri%2C%20P.G.%20Holmes%2C%20C.C.%20Walker%2C%20S.G.%20A%20general%20framework%20for%20updating%20belief%20distributions%202016"
        },
        {
            "id": "6",
            "entry": "[6] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20Inference%3A%20A%20Review%20for%20Statisticians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Kucukelbir%2C%20Alp%20McAuliffe%2C%20Jon%20D.%20Variational%20Inference%3A%20A%20Review%20for%20Statisticians%202017"
        },
        {
            "id": "7",
            "entry": "[7] Leo Breiman. Random Forests. Machine Learning, pages 1\u201333, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leo%20Breiman%20Random%20Forests%20Machine%20Learning%20pages%20133%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leo%20Breiman%20Random%20Forests%20Machine%20Learning%20pages%20133%202001"
        },
        {
            "id": "8",
            "entry": "[8] K. P. Burnham and D. R. Anderson. Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. Springer New York, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burnham%2C%20K.P.%20Anderson%2C%20D.R.%20Model%20Selection%20and%20Multimodel%20Inference%3A%20A%20Practical%20Information-Theoretic%20Approach%202003"
        },
        {
            "id": "9",
            "entry": "[9] Gary Chamberlain and Guido W. Imbens. Nonparametric Applications of Bayesian Inference. Journal of Business & Economic Statistics, 21(1):12\u201318, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chamberlain%2C%20Gary%20W%2C%20Guido%20Imbens.%20Nonparametric%20Applications%20of%20Bayesian%20Inference%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chamberlain%2C%20Gary%20W%2C%20Guido%20Imbens.%20Nonparametric%20Applications%20of%20Bayesian%20Inference%202003"
        },
        {
            "id": "10",
            "entry": "[10] Dua Dheeru and Efi Karra Taniskidou. UCI Machine Learning Repository, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dua%20Dheeru%20and%20Efi%20Karra%20Taniskidou%20UCI%20Machine%20Learning%20Repository%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dua%20Dheeru%20and%20Efi%20Karra%20Taniskidou%20UCI%20Machine%20Learning%20Repository%202017"
        },
        {
            "id": "11",
            "entry": "[11] Manuel Fern\u00e1ndez-Delgado, Eva Cernadas, Sen\u00e9n Barro, and Dinani Amorim. Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? Journal of Machine Learning Research, 15:3133\u20133181, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fern%C3%A1ndez-Delgado%2C%20Manuel%20Cernadas%2C%20Eva%20Barro%2C%20Sen%C3%A9n%20Amorim%2C%20Dinani%20Do%20we%20Need%20Hundreds%20of%20Classifiers%20to%20Solve%20Real%20World%20Classification%20Problems%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fern%C3%A1ndez-Delgado%2C%20Manuel%20Cernadas%2C%20Eva%20Barro%2C%20Sen%C3%A9n%20Amorim%2C%20Dinani%20Do%20we%20Need%20Hundreds%20of%20Classifiers%20to%20Solve%20Real%20World%20Classification%20Problems%3F%202014"
        },
        {
            "id": "12",
            "entry": "[12] Tadayoshi Fushiki. Bootstrap prediction and Bayesian prediction under misspecified models. Bernoulli, 11(4):747\u2013758, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fushiki%2C%20Tadayoshi%20Bootstrap%20prediction%20and%20Bayesian%20prediction%20under%20misspecified%20models%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fushiki%2C%20Tadayoshi%20Bootstrap%20prediction%20and%20Bayesian%20prediction%20under%20misspecified%20models%202005"
        },
        {
            "id": "13",
            "entry": "[13] Tadayoshi Fushiki. Bayesian bootstrap prediction. Journal of Statistical Planning and Inference, 140(1):65\u201374, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fushiki%2C%20Tadayoshi%20Bayesian%20bootstrap%20prediction%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fushiki%2C%20Tadayoshi%20Bayesian%20bootstrap%20prediction%202010"
        },
        {
            "id": "14",
            "entry": "[14] Nils L. Hjort, Chris Holmes, Peter M\u00fcller, and Stephen G. Walker. Bayesian Nonparametrics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nils%20L.%20Hjort%2C%20Chris%20Holmes%2C%20Peter%20M%C3%BCller%20Walker%2C%20Stephen%20G.%20Bayesian%20Nonparametrics%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nils%20L.%20Hjort%2C%20Chris%20Holmes%2C%20Peter%20M%C3%BCller%20Walker%2C%20Stephen%20G.%20Bayesian%20Nonparametrics%202010"
        },
        {
            "id": "15",
            "entry": "[15] Peter J. Huber. The behavior of maximum likelihood estimates under nonstandard conditions. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics, pages 221\u2013233, Berkeley, Calif., 1967. University of California Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huber%2C%20Peter%20J.%20The%20behavior%20of%20maximum%20likelihood%20estimates%20under%20nonstandard%20conditions%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huber%2C%20Peter%20J.%20The%20behavior%20of%20maximum%20likelihood%20estimates%20under%20nonstandard%20conditions%201967"
        },
        {
            "id": "16",
            "entry": "[16] Tommi S. Jaakkola and Michael I. Jordan. A variational approach to Bayesian logistic regression models and their extensions. International Workshop on Artificial Intelligence and Statistics, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaakkola%2C%20Tommi%20S.%20Jordan%2C%20Michael%20I.%20A%20variational%20approach%20to%20Bayesian%20logistic%20regression%20models%20and%20their%20extensions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaakkola%2C%20Tommi%20S.%20Jordan%2C%20Michael%20I.%20A%20variational%20approach%20to%20Bayesian%20logistic%20regression%20models%20and%20their%20extensions%201997"
        },
        {
            "id": "17",
            "entry": "[17] Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic Variational Inference in Stan. In Advances in Neural Information Processing Systems 28, volume 1, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alp%20Kucukelbir%20Rajesh%20Ranganath%20Andrew%20Gelman%20and%20David%20M%20Blei%20Automatic%20Variational%20Inference%20in%20Stan%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2028%20volume%201%20pages%2019%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alp%20Kucukelbir%20Rajesh%20Ranganath%20Andrew%20Gelman%20and%20David%20M%20Blei%20Automatic%20Variational%20Inference%20in%20Stan%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2028%20volume%201%20pages%2019%202015"
        },
        {
            "id": "18",
            "entry": "[18] Albert Y. Lo. On a Class of Bayesian Nonparametric Estimates: I. Density Estimates. Annals of Statistics, 12(1):351\u2013357, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lo%2C%20Albert%20Y.%20On%20a%20Class%20of%20Bayesian%20Nonparametric%20Estimates%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lo%2C%20Albert%20Y.%20On%20a%20Class%20of%20Bayesian%20Nonparametric%20Estimates%201984"
        },
        {
            "id": "19",
            "entry": "[19] S. P. Lyddon, C. C. Holmes, and S. G. Walker. General Bayesian Updating and the LossLikelihood Bootstrap. arxiv preprint 1709.07616, pages 1\u201314, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1709.07616"
        },
        {
            "id": "20",
            "entry": "[20] Ulrich K. M\u00fcller. Risk of Bayesian Inference in Misspecified Models, and the Sandwich Covariance Matrix. Econometrica, 81(5):1805\u20131849, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%BCller%2C%20Ulrich%20K.%20Risk%20of%20Bayesian%20Inference%20in%20Misspecified%20Models%2C%20and%20the%20Sandwich%20Covariance%20Matrix%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%BCller%2C%20Ulrich%20K.%20Risk%20of%20Bayesian%20Inference%20in%20Misspecified%20Models%2C%20and%20the%20Sandwich%20Covariance%20Matrix%202013"
        },
        {
            "id": "21",
            "entry": "[21] Michael A. Newton and Adrian E. Raftery. Approximate Bayesian Inference with the Weighted Likelihood Bootstrap. Journal of the Royal Statistical Society Series B-Statistical Methodology, 56(1):3\u201348, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newton%2C%20Michael%20A.%20Raftery%2C%20Adrian%20E.%20Approximate%20Bayesian%20Inference%20with%20the%20Weighted%20Likelihood%20Bootstrap%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newton%2C%20Michael%20A.%20Raftery%2C%20Adrian%20E.%20Approximate%20Bayesian%20Inference%20with%20the%20Weighted%20Likelihood%20Bootstrap%201994"
        },
        {
            "id": "22",
            "entry": "[22] Nicholas G. Polson, James G. Scott, and Jesse Windle. Bayesian Inference for Logistic Models Using P\u00f3lya\u2013Gamma Latent Variables. Journal of the American Statistical Association, 108(504):1339\u20131349, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polson%2C%20Nicholas%20G.%20Scott%2C%20James%20G.%20Windle%2C%20Jesse%20Bayesian%20Inference%20for%20Logistic%20Models%20Using%20P%C3%B3lya%E2%80%93Gamma%20Latent%20Variables%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polson%2C%20Nicholas%20G.%20Scott%2C%20James%20G.%20Windle%2C%20Jesse%20Bayesian%20Inference%20for%20Logistic%20Models%20Using%20P%C3%B3lya%E2%80%93Gamma%20Latent%20Variables%202013"
        },
        {
            "id": "23",
            "entry": "[23] Christian P. Robert. The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation. Springer Texts in Statistics. Springer New York, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robert%2C%20Christian%20P.%20The%20Bayesian%20Choice%3A%20From%20Decision-Theoretic%20Foundations%20to%20Computational%20Implementation.%20Springer%20Texts%20in%20Statistics%202007"
        },
        {
            "id": "24",
            "entry": "[24] Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer Texts in Statistics. Springer New York, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robert%2C%20Christian%20P.%20Casella%2C%20George%20Monte%20Carlo%20Statistical%20Methods.%20Springer%20Texts%20in%20Statistics%202005"
        },
        {
            "id": "25",
            "entry": "[25] Jayaram Sethuraman. A constructive definition of Dirichlet Process Priors. Statistica Sinica, 4:639\u2013650, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sethuraman%2C%20Jayaram%20A%20constructive%20definition%20of%20Dirichlet%20Process%20Priors%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sethuraman%2C%20Jayaram%20A%20constructive%20definition%20of%20Dirichlet%20Process%20Priors%201994"
        },
        {
            "id": "26",
            "entry": "[26] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90(2):227\u2013244, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shimodaira%2C%20Hidetoshi%20Improving%20predictive%20inference%20under%20covariate%20shift%20by%20weighting%20the%20log-likelihood%20function%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shimodaira%2C%20Hidetoshi%20Improving%20predictive%20inference%20under%20covariate%20shift%20by%20weighting%20the%20log-likelihood%20function%202000"
        },
        {
            "id": "27",
            "entry": "[27] Stephen G. Walker. Bayesian inference with misspecified models. Journal of Statistical Planning and Inference, 143(10):1621\u20131633, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walker%2C%20Stephen%20G.%20Bayesian%20inference%20with%20misspecified%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walker%2C%20Stephen%20G.%20Bayesian%20inference%20with%20misspecified%20models%202013"
        }
    ]
}
