{
    "filename": "8034-statistical-optimality-of-stochastic-gradient-descent-on-hard-learning-problems-through-multiple-passes.pdf",
    "metadata": {
        "title": "Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes",
        "author": "Loucas Pillaud-Vivien, Alessandro Rudi, Francis Bach",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8034-statistical-optimality-of-stochastic-gradient-descent-on-hard-learning-problems-through-multiple-passes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider stochastic gradient descent (SGD) for least-squares regression with potentially several passes over the data. While several passes have been widely reported to perform practically better in terms of predictive performance on unseen data, the existing theoretical analysis of SGD suggests that a single pass is statistically optimal. While this is true for low-dimensional easy problems, we show that for hard problems, multiple passes lead to statistically optimal predictions while single pass does not; we also show that in these hard models, the optimal number of passes over the data increases with sample size. In order to define the notion of hardness and show that our predictive performances are optimal, we consider potentially infinite-dimensional models and notions typically associated to kernel methods, namely, the decay of eigenvalues of the covariance matrix of the features and the complexity of the optimal predictor as measured through the covariance matrix. We illustrate our results on synthetic experiments with non-linear kernel methods and on a classical benchmark with a linear model."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "Stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Stochastic gradient descent (SGD) and its multiple variants\u2014averaged [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], accelerated [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], variancereduced [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]\u2014are the workhorses of large-scale machine learning, because (a) these methods looks at only a few observations before updating the corresponding model, and (b) they are known in theory and in practice to generalize well to unseen data [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].<br/><br/>Beyond the choice of step-size, the number of passes to make on the data remains an important practical and theoretical issue",
        "Most of the theoretical work only apply to single pass algorithms, with some exceptions leading to analyses of multiple passes when the step-size is taken smaller than the best known setting [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "While the optimality of Stochastic gradient descent was known for the regime {2r\u21b5 + 1 > \u21b5 \\ 2r > \u03bc}, here we extend the optimality to the new regime \u21b5 > 2r\u21b5 + 1 > \u03bc\u21b5, covering essentially all the region 2r > \u03bc, as it is possible to observe in Figure 1, where for clarity we plotted the best possible value for \u03bc that is \u03bc = 1/\u21b5 [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "Is known to be a lower bound on the optimal minimax rate, but the best upper-bound so far is O(n 2r) and is achieved by empirical risk minimization [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] or stochastic gradient descent [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], and the optimal rate is not known",
        "), multiple passes lead to statistical optimality with a number of passes that somewhat surprisingly need to grow with sample size, with a convergence rate which is superior to previous analyses of stochastic gradient",
        "(g) to reduce the computational complexity of the algorithm, while retaining the statistical guarantees, we could combine multi-pass stochastic gradient descent, with approximation techniques like random features [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], extending the analysis of [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] to the more general setting considered in this paper"
    ],
    "key_statements": [
        "Stochastic gradient descent (SGD) and its multiple variants\u2014averaged [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], accelerated [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], variancereduced [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]\u2014are the workhorses of large-scale machine learning, because (a) these methods looks at only a few observations before updating the corresponding model, and (b) they are known in theory and in practice to generalize well to unseen data [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].<br/><br/>Beyond the choice of step-size, the number of passes to make on the data remains an important practical and theoretical issue",
        "Most of the theoretical work only apply to single pass algorithms, with some exceptions leading to analyses of multiple passes when the step-size is taken smaller than the best known setting [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "While the optimality of Stochastic gradient descent was known for the regime {2r\u21b5 + 1 > \u21b5 \\ 2r > \u03bc}, here we extend the optimality to the new regime \u21b5 > 2r\u21b5 + 1 > \u03bc\u21b5, covering essentially all the region 2r > \u03bc, as it is possible to observe in Figure 1, where for clarity we plotted the best possible value for \u03bc that is \u03bc = 1/\u21b5 [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "Is known to be a lower bound on the optimal minimax rate, but the best upper-bound so far is O(n 2r) and is achieved by empirical risk minimization [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] or stochastic gradient descent [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], and the optimal rate is not known",
        "), multiple passes lead to statistical optimality with a number of passes that somewhat surprisingly need to grow with sample size, with a convergence rate which is superior to previous analyses of stochastic gradient",
        "(g) to reduce the computational complexity of the algorithm, while retaining the statistical guarantees, we could combine multi-pass stochastic gradient descent, with approximation techniques like random features [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], extending the analysis of [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] to the more general setting considered in this paper"
    ],
    "summary": [
        "Stochastic gradient descent (SGD) and its multiple variants\u2014averaged [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], accelerated [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], variancereduced [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]\u2014are the workhorses of large-scale machine learning, because (a) these methods looks at only a few observations before updating the corresponding model, and (b) they are known in theory and in practice to generalize well to unseen data [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].<br/><br/>Beyond the choice of step-size, the number of passes to make on the data remains an important practical and theoretical issue.",
        "We consider the following algorithm, which is stochastic gradient descent with sampling with replacement with multiple passes over the data.",
        "If 2\u21b5r + 1 > \u21b5, that is, easy problems, it has been shown by [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] that a single pass with a smaller step-size than the one we propose here is optimal, and our result does not apply.",
        "Note that these rates are theoretically only upper bounds on the optimal number of passes over the data, and one should be cautious when drawing conclusions; our simulations on synthetic data, see Figure 2 in Section 5, confirm that our proposed scalings for the number of passes is observed in practice.",
        "That our bounds that depends on \u21b5, r and \u03bc are independent of the dimension, and intuitively, following [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], should apply immediately to infinite-dimensional spaces.",
        ", which are common assumptions in the context of kernel methods [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], essentially controlling in a more refined way the regularity of the whole space of functions associated to H, with respect to the L1-norm, compared to the too crude inequality kgkL1 = supx | h",
        "When r 2 (0, 1] is fixed, but there are no assumptions on \u21b5 or \u03bc, the optimal minimax rate of convergence is O(n 2r/(2r+1)), attained by regularized empirical risk minimization [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and other spectral filters on the empirical covariance operator [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>].",
        "Is known to be a lower bound on the optimal minimax rate, but the best upper-bound so far is O(n 2r) and is achieved by empirical risk minimization [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] or stochastic gradient descent [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], and the optimal rate is not known.",
        "This is attained by regularized empirical risk minimization when 2r > \u03bc [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and by SGD with multiple passes, and it is the optimal rate in this situation.",
        "In Figure 3, we performed linear least-squares regression on the MNIST dataset and plotted the optimal number of passes over the data that leads to the smallest error on the test set.",
        "), multiple passes lead to statistical optimality with a number of passes that somewhat surprisingly need to grow with sample size, with a convergence rate which is superior to previous analyses of stochastic gradient.",
        "(g) to reduce the computational complexity of the algorithm, while retaining the statistical guarantees, we could combine multi-pass stochastic gradient descent, with approximation techniques like random features [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], extending the analysis of [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] to the more general setting considered in this paper."
    ],
    "headline": "While this is true for low-dimensional easy problems, we show that for hard problems, multiple passes lead to statistically optimal predictions while single pass does not; we show that in these hard models, the optimal number of passes over the data increases with sample size",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20B.T.%20Juditsky%2C%20A.B.%20Acceleration%20of%20stochastic%20approximation%20by%20averaging%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20B.T.%20Juditsky%2C%20A.B.%20Acceleration%20of%20stochastic%20approximation%20by%20averaging%201992"
        },
        {
            "id": "2",
            "entry": "[2] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2):365\u2013397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20Guanghui%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20Guanghui%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012"
        },
        {
            "id": "3",
            "entry": "[3] Nicolas L. Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In Advances in Neural Information Processing Systems (NIPS), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "4",
            "entry": "[4] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "5",
            "entry": "[5] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "6",
            "entry": "[6] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Curtis%2C%20Frank%20E.%20Nocedal%2C%20Jorge%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Curtis%2C%20Frank%20E.%20Nocedal%2C%20Jorge%20Optimization%20methods%20for%20large-scale%20machine%20learning%202018"
        },
        {
            "id": "7",
            "entry": "[7] A. S. Nemirovski and D. B. Yudin. Problem complexity and method efficiency in optimization. John Wiley, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.S.%20Yudin%2C%20D.B.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983"
        },
        {
            "id": "8",
            "entry": "[8] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "9",
            "entry": "[9] Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. Journal of Machine Learning Research, 18(97):1\u201347, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Junhong%20Rosasco%2C%20Lorenzo%20Optimal%20rates%20for%20multi-pass%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Junhong%20Rosasco%2C%20Lorenzo%20Optimal%20rates%20for%20multi-pass%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "10",
            "entry": "[10] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algorithm. Technical Report 1702.07254, arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fischer%2C%20Simon%20Steinwart%2C%20Ingo%20Sobolev%20norm%20learning%20rates%20for%20regularized%20least-squares%20algorithm%202017"
        },
        {
            "id": "11",
            "entry": "[11] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007"
        },
        {
            "id": "12",
            "entry": "[12] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yao%2C%20Yuan%20Rosasco%2C%20Lorenzo%20Caponnetto%2C%20Andrea%20On%20early%20stopping%20in%20gradient%20descent%20learning%202007"
        },
        {
            "id": "13",
            "entry": "[13] Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes. The Annals of Statistics, 44(4):1363\u20131399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieuleveut%2C%20Aymeric%20Bach%2C%20Francis%20Nonparametric%20stochastic%20approximation%20with%20large%20step-sizes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieuleveut%2C%20Aymeric%20Bach%2C%20Francis%20Nonparametric%20stochastic%20approximation%20with%20large%20step-sizes%202016"
        },
        {
            "id": "14",
            "entry": "[14] Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral algorithms with least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018"
        },
        {
            "id": "15",
            "entry": "[15] L Lo Gerfo, L Rosasco, F Odone, E De Vito, and A Verri. Spectral algorithms for supervised learning. Neural Computation, 20(7):1873\u20131897, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerfo%2C%20L.Lo%20Rosasco%2C%20L.%20Odone%2C%20F.%20Vito%2C%20E.De%20Spectral%20algorithms%20for%20supervised%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gerfo%2C%20L.Lo%20Rosasco%2C%20L.%20Odone%2C%20F.%20Vito%2C%20E.De%20Spectral%20algorithms%20for%20supervised%20learning%202008"
        },
        {
            "id": "16",
            "entry": "[16] Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization. In Advances in Neural Information Processing Systems, pages 1630\u20131638, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosasco%2C%20Lorenzo%20Villa%2C%20Silvia%20Learning%20with%20incremental%20iterative%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rosasco%2C%20Lorenzo%20Villa%2C%20Silvia%20Learning%20with%20incremental%20iterative%20regularization%202015"
        },
        {
            "id": "17",
            "entry": "[17] Gilles Blanchard and Nicole Kr\u00e4mer. Convergence rates of kernel conjugate gradient for random design regression. Analysis and Applications, 14(06):763\u2013794, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blanchard%2C%20Gilles%20Kr%C3%A4mer%2C%20Nicole%20Convergence%20rates%20of%20kernel%20conjugate%20gradient%20for%20random%20design%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blanchard%2C%20Gilles%20Kr%C3%A4mer%2C%20Nicole%20Convergence%20rates%20of%20kernel%20conjugate%20gradient%20for%20random%20design%20regression%202016"
        },
        {
            "id": "18",
            "entry": "[18] Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), pages 773\u2013781, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Francis%20Bach%20and%20Eric%20Moulines.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20O%281/n%29%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Francis%20Bach%20and%20Eric%20Moulines.%20Non-strongly-convex%20smooth%20stochastic%20approximation%20with%20convergence%20rate%20O%281/n%29%202013"
        },
        {
            "id": "19",
            "entry": "[19] Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence rates for least-squares regression. Journal of Machine Learning Research, 18(1):3520\u2013 3570, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dieuleveut%2C%20Aymeric%20Flammarion%2C%20Nicolas%20Harder%2C%20Francis%20Bach%20better%2C%20faster%20stronger%20convergence%20rates%20for%20least-squares%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dieuleveut%2C%20Aymeric%20Flammarion%2C%20Nicolas%20Harder%2C%20Francis%20Bach%20better%2C%20faster%20stronger%20convergence%20rates%20for%20least-squares%20regression%202017"
        },
        {
            "id": "20",
            "entry": "[20] Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Foundations of Computational Mathematics, 8(5):561\u2013596, Oct 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Yiming%20Ying%20and%20Massimiliano%20Pontil.%20Online%20gradient%20descent%20learning%20algorithms%202008-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Yiming%20Ying%20and%20Massimiliano%20Pontil.%20Online%20gradient%20descent%20learning%20algorithms%202008-10"
        },
        {
            "id": "21",
            "entry": "[21] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pages 3215\u20133225, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "22",
            "entry": "[22] Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares regression. In Proc. COLT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression%202009"
        },
        {
            "id": "23",
            "entry": "[23] R. A. Adams. Sobolev spaces / Robert A. Adams. Academic Press New York, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adams%2C%20R.A.%20Sobolev%20spaces%20/%20Robert%20A.%20Adams%201975"
        },
        {
            "id": "24",
            "entry": "[24] G. Wahba. Spline Models for Observational Data. Society for Industrial and Applied Mathematics, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wahba%2C%20G.%20Spline%20Models%20for%20Observational%20Data.%20Society%20for%20Industrial%20and%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wahba%2C%20G.%20Spline%20Models%20for%20Observational%20Data.%20Society%20for%20Industrial%20and%201990"
        },
        {
            "id": "25",
            "entry": "[25] Holger Wendland. Scattered data approximation, volume 17. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wendland%2C%20Holger%20Scattered%20data%20approximation%2C%20volume%2017%202004"
        },
        {
            "id": "26",
            "entry": "[26] Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions. Journal of Machine Learning Research, 18(21):1\u201338, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20On%20the%20equivalence%20between%20kernel%20quadrature%20rules%20and%20random%20feature%20expansions%202017"
        },
        {
            "id": "27",
            "entry": "[27] Gilles Blanchard and Nicole M\u00fccke. Optimal rates for regularization of statistical inverse learning problems. Foundations of Computational Mathematics, pages 1\u201343, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blanchard%2C%20Gilles%20M%C3%BCcke%2C%20Nicole%20Optimal%20rates%20for%20regularization%20of%20statistical%20inverse%20learning%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blanchard%2C%20Gilles%20M%C3%BCcke%2C%20Nicole%20Optimal%20rates%20for%20regularization%20of%20statistical%20inverse%20learning%20problems%202017"
        },
        {
            "id": "28",
            "entry": "[28] Ohad Shamir. Without-replacement sampling for stochastic gradient methods. In Advances in Neural Information Processing Systems 29, pages 46\u201354, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Without-replacement%20sampling%20for%20stochastic%20gradient%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Without-replacement%20sampling%20for%20stochastic%20gradient%20methods%202016"
        },
        {
            "id": "29",
            "entry": "[29] Mert G\u00fcrb\u00fczbalaban, Asu Ozdaglar, and Pablo Parrilo. Why random reshuffling beats stochastic gradient descent. Technical Report 1510.08560, arXiv, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%C3%BCrb%C3%BCzbalaban%2C%20Mert%20Ozdaglar%2C%20Asu%20Parrilo%2C%20Pablo%20Why%20random%20reshuffling%20beats%20stochastic%20gradient%20descent%202015"
        },
        {
            "id": "30",
            "entry": "[30] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing error for stochastic gradient methods. In Proceedings of the 31st Conference On Learning Theory, volume 75, pages 250\u2013296, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pillaud-Vivien%2C%20Loucas%20Rudi%2C%20Alessandro%20Bach%2C%20Francis%20Exponential%20convergence%20of%20testing%20error%20for%20stochastic%20gradient%20methods%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pillaud-Vivien%2C%20Loucas%20Rudi%2C%20Alessandro%20Bach%2C%20Francis%20Exponential%20convergence%20of%20testing%20error%20for%20stochastic%20gradient%20methods%202018"
        },
        {
            "id": "31",
            "entry": "[31] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. Technical Report 1611.03530, arXiv, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202016"
        },
        {
            "id": "32",
            "entry": "[32] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A consistent regularization approach for structured prediction. In Advances in neural information processing systems, pages 4412\u2013 4420, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016"
        },
        {
            "id": "33",
            "entry": "[33] Anton Osokin, Francis Bach, and Simon Lacoste-Julien. On structured prediction theory with calibrated convex surrogate losses. In Advances in Neural Information Processing Systems, pages 302\u2013313, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017"
        },
        {
            "id": "34",
            "entry": "[34] Carlo Ciliberto, Francis Bach, and Alessandro Rudi. Localized structured prediction. arXiv preprint arXiv:1806.02402, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02402"
        },
        {
            "id": "35",
            "entry": "[35] Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, and Massimiliano Pontil. Consistent multitask learning with nonlinear output relations. In Advances in Neural Information Processing Systems, pages 1986\u20131996, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986"
        },
        {
            "id": "36",
            "entry": "[36] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177\u20131184, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "37",
            "entry": "[37] Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with sgd and random features. arXiv preprint arXiv:1807.06343, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06343"
        },
        {
            "id": "38",
            "entry": "[38] R. Aguech, E. Moulines, and P. Priouret. On a perturbation approach for the analysis of stochastic tracking algorithms. SIAM J. Control and Optimization, 39(3):872\u2013899, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aguech%2C%20R.%20Moulines%2C%20E.%20Priouret%2C%20P.%20On%20a%20perturbation%20approach%20for%20the%20analysis%20of%20stochastic%20tracking%20algorithms%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aguech%2C%20R.%20Moulines%2C%20E.%20Priouret%2C%20P.%20On%20a%20perturbation%20approach%20for%20the%20analysis%20of%20stochastic%20tracking%20algorithms%202000"
        },
        {
            "id": "39",
            "entry": "[39] Alessandro Rudi, Guillermo D Canas, and Lorenzo Rosasco. On the sample complexity of subspace learning. In Advances in Neural Information Processing Systems, pages 2067\u20132075, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Canas%2C%20Guillermo%20D.%20Rosasco%2C%20Lorenzo%20On%20the%20sample%20complexity%20of%20subspace%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Canas%2C%20Guillermo%20D.%20Rosasco%2C%20Lorenzo%20On%20the%20sample%20complexity%20of%20subspace%20learning%202013"
        },
        {
            "id": "40",
            "entry": "[40] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389\u2013434, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20User-friendly%20tail%20bounds%20for%20sums%20of%20random%20matrices.%20Foundations%20of%20computational%202012"
        },
        {
            "id": "41",
            "entry": "[41] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel method. In Advances in Neural Information Processing Systems, pages 3888\u20133898, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017"
        }
    ]
}
