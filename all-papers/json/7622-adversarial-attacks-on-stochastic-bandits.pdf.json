{
    "filename": "7622-adversarial-attacks-on-stochastic-bandits.pdf",
    "metadata": {
        "title": "Adversarial Attacks on Stochastic Bandits",
        "author": "Kwang-Sung Jun, Lihong Li, Yuzhe Ma, Jerry Zhu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7622-adversarial-attacks-on-stochastic-bandits.pdf"
        },
        "abstract": "We study adversarial attacks that manipulate the reward signals to control the actions chosen by a stochastic multi-armed bandit algorithm. We propose the first attack against two popular bandit algorithms: \u270f-greedy and UCB, without knowledge of the mean rewards. The attacker is able to spend only logarithmic effort, multiplied by a problem-specific parameter that becomes smaller as the bandit problem gets easier to attack. The result means the attacker can easily hijack the behavior of the bandit algorithm to promote or obstruct certain actions, say, a particular medical treatment. As bandits are seeing increasingly wide use in practice, our study exposes a significant security threat."
    },
    "keywords": [
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "medical treatment",
            "url": "https://en.wikipedia.org/wiki/medical_treatment"
        },
        {
            "term": "news article",
            "url": "https://en.wikipedia.org/wiki/news_article"
        },
        {
            "term": "bandit problem",
            "url": "https://en.wikipedia.org/wiki/bandit_problem"
        },
        {
            "term": "multi-armed bandits",
            "url": "https://en.wikipedia.org/wiki/multi-armed_bandits"
        }
    ],
    "highlights": [
        "Designing trustworthy machine learning systems requires understanding how they may be attacked",
        "Little is known on adversarial attacks against stochastic multi-armed bandits (MABs), a form of online learning with limited feedback",
        "This is potentially hazardous since stochastic multi-armed bandits are widely used in the industry to recommend news articles [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], display advertisements [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], improve search results [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], allocate medical treatment [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], and promote users\u2019 well-being [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], among many others",
        "As we show, an adversarial attacker can modify the reward signal to manipulate the multi-armed bandits for nefarious goals",
        "We presented a reward-manipulating attack on stochastic multi-armed bandits"
    ],
    "key_statements": [
        "Designing trustworthy machine learning systems requires understanding how they may be attacked",
        "Little is known on adversarial attacks against stochastic multi-armed bandits (MABs), a form of online learning with limited feedback",
        "This is potentially hazardous since stochastic multi-armed bandits are widely used in the industry to recommend news articles [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], display advertisements [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], improve search results [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], allocate medical treatment [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], and promote users\u2019 well-being [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], among many others",
        "As we show, an adversarial attacker can modify the reward signal to manipulate the multi-armed bandits for nefarious goals",
        "We presented a reward-manipulating attack on stochastic multi-armed bandits"
    ],
    "summary": [
        "Designing trustworthy machine learning systems requires understanding how they may be attacked.",
        "Alice can perform the following oracle attack: in any round where a non-target arm It 6= K",
        "This oracle attack transforms the original bandit problem into one where all non-target arms have expected reward less than \u03bcK .",
        "Alice\u2019s cumulative attack cost will be sublinear in time, because the total number of non-target arm pulls is sublinear in the transformed problem.",
        "We assume all arm rewards are 2-sub-Gaussian where 2 is known to both Alice and Bob. Let Ni(t) be the number of pulls of arm i up to round t.",
        "We say the attack is successful after T rounds if the number of target-arm pulls is NK (T ) minimizing the cumulative attack cost",
        "A successful attack means that Alice pulls the target arm T o(T ) times; the attack cost is necessarily linear in T , which is inefficient.",
        "Alice wants to make Bob always pull the target arm during exploitation rounds.",
        "With this \u21b5t, we claim that (i) Alice forces Bob to pull the target arm in all exploitation rounds as shown in Lemma 2, and the cumulative attack cost is logarithmic in t for standard \u270f-greedy learner exploration scheme \u270ft = O(1/t) as shown in Corollary 1.",
        "To prove Theorem 1, we first show that in (2) is a high-probability bound on the pre-attack empirical mean of all arms on all rounds.",
        "1/2 and under event E, attacks (4) force Bob to always pull the target arm K in exploitation rounds.",
        "We present the main theorem on Alice\u2019s cumulative attack cost against Bob who runs UCB.",
        ", Alice forces Bob rounds, using a cumulative attack cost at most",
        "At any round t 2K, the cumulative attack cost to any fixed arm i",
        "Bounds over all non-target arms ), and the cumulative attack cost ii.sSPpetTc=i1fi\u21b5catll=y, i<K t2\u2327i(T ) \u21b5t.",
        "Alice\u2019s attack dramatically forces Bob to pull the target arm.",
        "In 10000 rounds, Bob is forced to pull the target arm 9994 rounds with the attack, compared to only 6 rounds if Alice was not present.",
        "Related to MAB, there has been empirical evidence that suggests adversarial attacks can be quite effective, even in the more general multi-step reinforcement learning problems, as opposed to the bandit case considered in this paper.",
        "We presented a reward-manipulating attack on stochastic MABs. We analyzed the attack against \u270f-greedy and a generalization of the UCB algorithm, and proved that the attacker can force the algorithms to almost always pull a suboptimal target arm.",
        "Given the wide use of MABs in practice, this is a significant security threat"
    ],
    "headline": "We study adversarial attacks that manipulate the reward signals to control the actions chosen by a stochastic multi-armed bandit algorithm",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Abbasi-Yadkori, Yasin, P\u00e1l, D\u00e1vid, and Szepesv\u00e1ri, Csaba. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems (NIPS), pp. 2312\u2013 2320, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011"
        },
        {
            "id": "2",
            "entry": "[2] Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford, John, Li, Lihong, and Schapire, Robert E. Taming the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning (ICML), pp. 1638\u20131646, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Hsu%2C%20Daniel%20Kale%2C%20Satyen%20Langford%2C%20John%20Taming%20the%20monster%3A%20A%20fast%20and%20simple%20algorithm%20for%20contextual%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Hsu%2C%20Daniel%20Kale%2C%20Satyen%20Langford%2C%20John%20Taming%20the%20monster%3A%20A%20fast%20and%20simple%20algorithm%20for%20contextual%20bandits%202014"
        },
        {
            "id": "3",
            "entry": "[3] Agarwal, Alekh, Bird, Sarah, Cozowicz, Markus, Hoang, Luong, Langford, John, Lee, Stephen, Li, Jiaji, Melamed, Dan, Oshri, Gal, Ribas, Oswaldo, Sen, Siddhartha, and Slivkins, Alex. Making contextual decisions with low technical debt. CoRR abs/1606.03966, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03966"
        },
        {
            "id": "4",
            "entry": "[4] Agrawal, Shipra and Goyal, Navin. Analysis of Thompson Sampling for the Multi-armed Bandit Problem. In In Proceedings of the Conference on Learning Theory (COLT), volume 23, pp. 39.1\u201339.26, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Analysis%20of%20Thompson%20Sampling%20for%20the%20Multi-armed%20Bandit%20Problem%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Analysis%20of%20Thompson%20Sampling%20for%20the%20Multi-armed%20Bandit%20Problem%202012"
        },
        {
            "id": "5",
            "entry": "[5] Agrawal, Shipra and Goyal, Navin. Thompson Sampling for Contextual Bandits with Linear Payoffs. In Proceedings of the International Conference on Machine Learning (ICML), pp. 127\u2013135, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Thompson%20Sampling%20for%20Contextual%20Bandits%20with%20Linear%20Payoffs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Goyal%2C%20Navin%20Thompson%20Sampling%20for%20Contextual%20Bandits%20with%20Linear%20Payoffs%202013"
        },
        {
            "id": "6",
            "entry": "[6] Auer, Peter, Cesa-Bianchi, Nicol\u00f2, and Fischer, Paul. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2\u20133):235\u2013256, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "7",
            "entry": "[7] Auer, Peter, Cesa-Bianchi, Nicol\u00f2, Freund, Yoav, and Schapire, Robert E. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48\u201377, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20Peter%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "8",
            "entry": "[8] Bubeck, S\u00e9bastien and Cesa-Bianchi, Nicol\u00f2. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning, 5:1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Regret%20Analysis%20of%20Stochastic%20and%20Nonstochastic%20Multi-armed%20Bandit%20Problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Regret%20Analysis%20of%20Stochastic%20and%20Nonstochastic%20Multi-armed%20Bandit%20Problems%202012"
        },
        {
            "id": "9",
            "entry": "[9] Chapelle, Olivier, Manavoglu, Eren, and Rosales, Romer. Simple and scalable response prediction for display advertising. ACM Transactions on Intelligent Systems and Technology, 5 (4):61:1\u201361:34, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Manavoglu%2C%20Eren%20Rosales%2C%20Romer%20Simple%20and%20scalable%20response%20prediction%20for%20display%20advertising%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Manavoglu%2C%20Eren%20Rosales%2C%20Romer%20Simple%20and%20scalable%20response%20prediction%20for%20display%20advertising%202014"
        },
        {
            "id": "10",
            "entry": "[10] Dorigo, Marco and Colombetti, Luca Marco. Robot Shaping: An Experiment in Behavior Engineering. MIT Press, 1997. ISBN 0-262-04164-2.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dorigo%2C%20Marco%20Colombetti%2C%20Luca%20Marco%20Robot%20Shaping%3A%20An%20Experiment%20in%20Behavior%20Engineering%201997"
        },
        {
            "id": "11",
            "entry": "[11] Even-Dar, Eyal, Kakade, Sham M., and Mansour, Yishay. Online Markov decision processes. Mathematics of Operations Research, 34(3):726\u2013736, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20Eyal%20Kakade%2C%20Sham%20M.%20Mansour%2C%20Yishay%20Online%20Markov%20decision%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20Eyal%20Kakade%2C%20Sham%20M.%20Mansour%2C%20Yishay%20Online%20Markov%20decision%20processes%202009"
        },
        {
            "id": "12",
            "entry": "[12] Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "13",
            "entry": "[13] Greenewald, Kristjan, Tewari, Ambuj, Murphy, Susan A., and Klasnja, Predrag V. Action centered contextual bandits. In Advances in Neural Information Processing Systems 30 (NIPS), pp. 5979\u20135987, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greenewald%2C%20Kristjan%20Tewari%2C%20Ambuj%20Murphy%2C%20Susan%20A.%20Klasnja%2C%20Predrag%20V.%20Action%20centered%20contextual%20bandits%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Greenewald%2C%20Kristjan%20Tewari%2C%20Ambuj%20Murphy%2C%20Susan%20A.%20Klasnja%2C%20Predrag%20V.%20Action%20centered%20contextual%20bandits%202017"
        },
        {
            "id": "14",
            "entry": "[14] Huang, Sandy, Papernot, Nicolas, Goodfellow, Ian, Duan, Yan, and Abbeel, Pieter. Adversarial attacks on neural network policies, 2017. arXiv:1702.02284.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02284"
        },
        {
            "id": "15",
            "entry": "[15] Joseph, Anthony D., Nelson, Blaine, Rubinstein, Benjamin I. P., and Tygar, J.D. Adversarial Machine Learning. Cambridge University Press, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20Anthony%20D.%20Nelson%2C%20Blaine%20Rubinstein%2C%20Benjamin%20I.P.%20Tygar%2C%20J.D.%20Adversarial%20Machine%20Learning%202018"
        },
        {
            "id": "16",
            "entry": "[16] Kuleshov, Volodymyr and Precup, Doina. Algorithms for multi-armed bandit problems. CoRR abs/1402.6028, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.6028"
        },
        {
            "id": "17",
            "entry": "[17] Kveton, Branislav, Szepesv\u00e1ri, Csaba, Wen, Zheng, and Ashkan, Azin. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pp. 767\u2013776, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cascading%20bandits%3A%20Learning%20to%20rank%20in%20the%20cascade%20model%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cascading%20bandits%3A%20Learning%20to%20rank%20in%20the%20cascade%20model%202015"
        },
        {
            "id": "18",
            "entry": "[18] Li, Lihong, Chu, Wei, Langford, John, and Schapire, Robert E. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the Nineteenth International Conference on World Wide Web (WWW), pp. 661\u2013670, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Schapire%2C%20Robert%20E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Lihong%20Chu%2C%20Wei%20Langford%2C%20John%20Schapire%2C%20Robert%20E.%20A%20contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010"
        },
        {
            "id": "19",
            "entry": "[19] Lin, Yen-Chen, Hong, Zhang-Wei, Liao, Yuan-Hong, Shih, Meng-Li, Liu, Ming-Yu, and Sun, Min. Tactics of adversarial attack on deep reinforcement learning agents. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI), pp. 3756\u20133762, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Yen-Chen%20Hong%2C%20Zhang-Wei%20Liao%2C%20Yuan-Hong%20Shih%2C%20Meng-Li%20Tactics%20of%20adversarial%20attack%20on%20deep%20reinforcement%20learning%20agents%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Yen-Chen%20Hong%2C%20Zhang-Wei%20Liao%2C%20Yuan-Hong%20Shih%2C%20Meng-Li%20Tactics%20of%20adversarial%20attack%20on%20deep%20reinforcement%20learning%20agents%202017"
        },
        {
            "id": "20",
            "entry": "[20] Lykouris, Thodoris, Mirrokni, Vahab, and Paes Leme, Renato. Stochastic bandits robust to adversarial corruptions. In Proceedings of the Annual ACM SIGACT Symposium on Theory of Computing (STOC), pp. 114\u2013122, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lykouris%2C%20Thodoris%20Mirrokni%2C%20Vahab%20Leme%2C%20Paes%20Renato%20Stochastic%20bandits%20robust%20to%20adversarial%20corruptions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lykouris%2C%20Thodoris%20Mirrokni%2C%20Vahab%20Leme%2C%20Paes%20Renato%20Stochastic%20bandits%20robust%20to%20adversarial%20corruptions%202018"
        },
        {
            "id": "21",
            "entry": "[21] Ma, Yuzhe, Jun, Kwang-Sung, Li, Lihong, and Zhu, Xiaojin. Data poisoning attacks in contextual bandits. arXiv preprint arXiv:1808.05760, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05760"
        },
        {
            "id": "22",
            "entry": "[22] Ng, Andrew Y., Harada, Daishi, and Russell, Stuart J. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the 16th International Conference on Machine Learning (ICML), pp. 278\u2013287, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "23",
            "entry": "[23] Thompson, William R. On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples. Biometrika, 25(3/4):285, 1933. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20William%20R.%20On%20the%20Likelihood%20that%20One%20Unknown%20Probability%20Exceeds%20Another%20in%20View%20of%20the%20Evidence%20of%20Two%20Samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20William%20R.%20On%20the%20Likelihood%20that%20One%20Unknown%20Probability%20Exceeds%20Another%20in%20View%20of%20the%20Evidence%20of%20Two%20Samples%201933"
        }
    ]
}
