{
    "filename": "7705-cola-decentralized-linear-learning.pdf",
    "metadata": {
        "date": 2018,
        "title": "COLA: Decentralized Linear Learning",
        "author": "Lie He\u21e4 EPFL",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7705-cola-decentralized-linear-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run ondevice, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and allows for unreliable and heterogeneous participating devices."
    },
    "keywords": [
        {
            "term": "network topology",
            "url": "https://en.wikipedia.org/wiki/network_topology"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "COLA",
            "url": "https://en.wikipedia.org/wiki/COLA"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "coordinate ascent",
            "url": "https://en.wikipedia.org/wiki/coordinate_ascent"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "linear model",
            "url": "https://en.wikipedia.org/wiki/linear_model"
        }
    ],
    "highlights": [
        "With the immense growth of data, decentralized machine learning has become not only attractive but a necessity",
        "Many users have gotten accustomed to giving up control over their data in return for useful machine learning predictions, which benefits from joint training on the data of all users combined in a centralized fashion",
        "We illustrate the advantages of COLA in three respects: firstly we investigate the application in different network topologies and with varying subproblem quality \u21e5; secondly, we compare COLA with state-of-the-art decentralized baselines: 1 , DIGing [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], which generalizes the gradient-tracking technique of the EXTRA algorithm [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], and 2 , Decentralized ADMM, which extends the classical ADMM (Alternating Direction Method of Multipliers) method [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] to the decentralized setting [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]; we show that COLA works in the challenging unreliable network environment where each node has a certain chance to drop out of the network",
        "In this work we have studied training generalized linear models in the fully decentralized setting",
        "We proposed a communication-efficient decentralized framework, termed COLA, which is free of parameter tuning",
        "We demonstrated that COLA offers full adaptivity to heterogenous distributed systems on arbitrary network topologies, and is adaptive to changes in network size and data, and offers fault tolerance and elasticity"
    ],
    "key_statements": [
        "With the immense growth of data, decentralized machine learning has become not only attractive but a necessity",
        "Many users have gotten accustomed to giving up control over their data in return for useful machine learning predictions, which benefits from joint training on the data of all users combined in a centralized fashion",
        "We illustrate the advantages of COLA in three respects: firstly we investigate the application in different network topologies and with varying subproblem quality \u21e5; secondly, we compare COLA with state-of-the-art decentralized baselines: 1 , DIGing [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], which generalizes the gradient-tracking technique of the EXTRA algorithm [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], and 2 , Decentralized ADMM, which extends the classical ADMM (Alternating Direction Method of Multipliers) method [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] to the decentralized setting [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]; we show that COLA works in the challenging unreliable network environment where each node has a certain chance to drop out of the network",
        "Table 1 describes the datasets4 used in the experiments",
        "In this work we have studied training generalized linear models in the fully decentralized setting",
        "We proposed a communication-efficient decentralized framework, termed COLA, which is free of parameter tuning",
        "We demonstrated that COLA offers full adaptivity to heterogenous distributed systems on arbitrary network topologies, and is adaptive to changes in network size and data, and offers fault tolerance and elasticity"
    ],
    "summary": [
        "With the immense growth of data, decentralized machine learning has become not only attractive but a necessity.",
        "Our main contribution is to propose COLA, a new decentralized framework for training generalized linear models with convergence guarantees.",
        "We consider the task of joint training of a global machine learning model in a decentralized network of K nodes.",
        "To ensure convergence of decentralized algorithms, we impose the standard assumption of positive spectral gap of the network which includes all connected graphs, such as e.g. a ring or 2-D grid topology, see Appendix B for details.",
        "Our work provides a fully decentralized alternative algorithm for federated learning with generalized linear models.",
        "In COCOA [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] the subproblem is defined in terms of a global aggregated shared vector vc := Ax 2 Rd, which is not available in the decentralized setting.2 The aggregation parameter 2 [0, 1] does not need to be tuned; we use the default := 1 throughout the paper, see [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] for a discussion.",
        "Scalability and elasticity in terms of availability and computational capacity can be modelled by a node-specific local accuracy parameter \u21e5k in Assumption 1, as proposed by [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>].",
        "If we use the time-varying model in [21, Assumption 1], where an undirected graph is connected with B gossip steps, changing COLA to perform B communication steps and one computation step per round still guarantees convergence.",
        "We present a convergence analysis of the proposed decentralized algorithm COLA for both general convex and strongly convex objectives.",
        "The dual problem and duality gap of the decentralized objective (DA) are given in Lemma 2.",
        "We show that COLA enjoys a O(1/T ) sublinear rate of convergence for all network topologies with a positive spectral gap.",
        "We illustrate the advantages of COLA in three respects: firstly we investigate the application in different network topologies and with varying subproblem quality \u21e5; secondly, we compare COLA with state-of-the-art decentralized baselines: 1 , DIGing [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], which generalizes the gradient-tracking technique of the EXTRA algorithm [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], and 2 , Decentralized ADMM, which extends the classical ADMM (Alternating Direction Method of Multipliers) method [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] to the decentralized setting [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]; we show that COLA works in the challenging unreliable network environment where each node has a certain chance to drop out of the network.",
        "One can observe that for both generally and strongly convex objectives, COLA significantly outperforms DIGing and decentralized ADMM in terms of number of communication rounds and computation time."
    ],
    "headline": "We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pages 873\u2013881, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Duchi%2C%20John%20C.%20Distributed%20delayed%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Duchi%2C%20John%20C.%20Distributed%20delayed%20stochastic%20optimization%202011"
        },
        {
            "id": "2",
            "entry": "[2] Pascal Bianchi, Walid Hachem, and Franck Iutzeler. A coordinate descent primal-dual algorithm and application to distributed asynchronous optimization. IEEE Transactions on Automatic Control, 61(10):2947\u20132957, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bianchi%2C%20Pascal%20Hachem%2C%20Walid%20Iutzeler%2C%20Franck%20A%20coordinate%20descent%20primal-dual%20algorithm%20and%20application%20to%20distributed%20asynchronous%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bianchi%2C%20Pascal%20Hachem%2C%20Walid%20Iutzeler%2C%20Franck%20A%20coordinate%20descent%20primal-dual%20algorithm%20and%20application%20to%20distributed%20asynchronous%20optimization%202016"
        },
        {
            "id": "3",
            "entry": "[3] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in Machine learning, 3(1):1\u2013122, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Parikh%2C%20Neal%20Chu%2C%20Eric%20Peleato%2C%20Borja%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20Stephen%20Parikh%2C%20Neal%20Chu%2C%20Eric%20Peleato%2C%20Borja%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011"
        },
        {
            "id": "4",
            "entry": "[4] Volkan Cevher, Stephen Becker, and Mark Schmidt. Convex Optimization for Big Data: Scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Processing Magazine, 31(5):32\u201343, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cevher%2C%20Volkan%20Becker%2C%20Stephen%20Schmidt%2C%20Mark%20Convex%20Optimization%20for%20Big%20Data%3A%20Scalable%2C%20randomized%2C%20and%20parallel%20algorithms%20for%20big%20data%20analytics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cevher%2C%20Volkan%20Becker%2C%20Stephen%20Schmidt%2C%20Mark%20Convex%20Optimization%20for%20Big%20Data%3A%20Scalable%2C%20randomized%2C%20and%20parallel%20algorithms%20for%20big%20data%20analytics%202014"
        },
        {
            "id": "5",
            "entry": "[5] J C Duchi, A Agarwal, and M J Wainwright. Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling. IEEE Transactions on Automatic Control, 57(3):592\u2013606, March 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.C.%20Agarwal%2C%20A.%20Wainwright%2C%20M.J.%20Dual%20Averaging%20for%20Distributed%20Optimization%3A%20Convergence%20Analysis%20and%20Network%20Scaling%202012-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.C.%20Agarwal%2C%20A.%20Wainwright%2C%20M.J.%20Dual%20Averaging%20for%20Distributed%20Optimization%3A%20Convergence%20Analysis%20and%20Network%20Scaling%202012-03"
        },
        {
            "id": "6",
            "entry": "[6] Celestine Dunner, Simone Forte, Martin Takac, and Martin Jaggi. Primal-Dual Rates and Certificates. In ICML 2016 - Proceedings of the 33th International Conference on Machine Learning, pages 783\u2013792, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dunner%2C%20Celestine%20Forte%2C%20Simone%20Takac%2C%20Martin%20Jaggi%2C%20Martin%20Primal-Dual%20Rates%20and%20Certificates%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dunner%2C%20Celestine%20Forte%2C%20Simone%20Takac%2C%20Martin%20Jaggi%2C%20Martin%20Primal-Dual%20Rates%20and%20Certificates%202016"
        },
        {
            "id": "7",
            "entry": "[7] Celestine Dunner, Aurelien Lucchi, Matilde Gargiani, An Bian, Thomas Hofmann, and Martin Jaggi. A Distributed Second-Order Algorithm You Can Trust. In ICML 2018 - Proceedings of the 35th International Conference on Machine Learning, pages 1357\u20131365, July 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dunner%2C%20Celestine%20Lucchi%2C%20Aurelien%20Gargiani%2C%20Matilde%20Bian%2C%20An%20A%20Distributed%20Second-Order%20Algorithm%20You%20Can%20Trust%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dunner%2C%20Celestine%20Lucchi%2C%20Aurelien%20Gargiani%2C%20Matilde%20Bian%2C%20An%20A%20Distributed%20Second-Order%20Algorithm%20You%20Can%20Trust%202018-07"
        },
        {
            "id": "8",
            "entry": "[8] Matilde Gargiani. Hessian-CoCoA: a general parallel and distributed framework for non-strongly convex regularizers. Master\u2019s thesis, ETH Zurich, June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gargiani%2C%20Matilde%20Hessian-CoCoA%3A%20a%20general%20parallel%20and%20distributed%20framework%20for%20non-strongly%20convex%20regularizers%202017-06"
        },
        {
            "id": "9",
            "entry": "[9] W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. Biometrika, 57(1):97\u2013109, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastings%2C%20W.Keith%20Monte%20carlo%20sampling%20methods%20using%20markov%20chains%20and%20their%20applications%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastings%2C%20W.Keith%20Monte%20carlo%20sampling%20methods%20using%20markov%20chains%20and%20their%20applications%201970"
        },
        {
            "id": "10",
            "entry": "[10] Martin Jaggi, Virginia Smith, Martin Takac, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 3068\u20133076, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Takac%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Takac%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014"
        },
        {
            "id": "11",
            "entry": "[11] Dusan Jakovetic, Joao Xavier, and Jose MF Moura. Convergence rate analysis of distributed gradient methods for smooth optimization. In Telecommunications Forum (TELFOR), 2012 20th, pages 867\u2013870. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jakovetic%2C%20Dusan%20Xavier%2C%20Joao%20Moura%2C%20Jose%20M.F.%20Convergence%20rate%20analysis%20of%20distributed%20gradient%20methods%20for%20smooth%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jakovetic%2C%20Dusan%20Xavier%2C%20Joao%20Moura%2C%20Jose%20M.F.%20Convergence%20rate%20analysis%20of%20distributed%20gradient%20methods%20for%20smooth%20optimization%202012"
        },
        {
            "id": "12",
            "entry": "[12] Jakub Konecny, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.03575"
        },
        {
            "id": "13",
            "entry": "[13] Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05492"
        },
        {
            "id": "14",
            "entry": "[14] Ching-pei Lee and Kai-Wei Chang. Distributed block-diagonal approximation methods for regularized empirical risk minimization. arXiv preprint arXiv:1709.03043, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03043"
        },
        {
            "id": "15",
            "entry": "[15] Ching-pei Lee, Cong Han Lim, and Stephen J Wright. A distributed quasi-newton algorithm for empirical risk minimization with nonsmooth regularization. In ACM International Conference on Knowledge Discovery and Data Mining, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Ching-pei%20Lim%2C%20Cong%20Han%20Wright%2C%20Stephen%20J.%20A%20distributed%20quasi-newton%20algorithm%20for%20empirical%20risk%20minimization%20with%20nonsmooth%20regularization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Ching-pei%20Lim%2C%20Cong%20Han%20Wright%2C%20Stephen%20J.%20A%20distributed%20quasi-newton%20algorithm%20for%20empirical%20risk%20minimization%20with%20nonsmooth%20regularization%202018"
        },
        {
            "id": "16",
            "entry": "[16] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 5336\u20135346, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20Xiangru%20Zhang%2C%20Ce%20Zhang%2C%20Huan%20Hsieh%2C%20Cho-Jui%20Can%20decentralized%20algorithms%20outperform%20centralized%20algorithms%3F%20a%20case%20study%20for%20decentralized%20parallel%20stochastic%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20Xiangru%20Zhang%2C%20Ce%20Zhang%2C%20Huan%20Hsieh%2C%20Cho-Jui%20Can%20decentralized%20algorithms%20outperform%20centralized%20algorithms%3F%20a%20case%20study%20for%20decentralized%20parallel%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "17",
            "entry": "[17] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In ICML 2018 - Proceedings of the 35th International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20Xiangru%20Zhang%2C%20Wei%20Zhang%2C%20Ce%20Liu%2C%20Ji%20Asynchronous%20decentralized%20parallel%20stochastic%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20Xiangru%20Zhang%2C%20Wei%20Zhang%2C%20Ce%20Liu%2C%20Ji%20Asynchronous%20decentralized%20parallel%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "18",
            "entry": "[18] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I Jordan, Peter Richtarik, and Martin Takac. Adding vs. Averaging in Distributed Primal-Dual Optimization. In ICML 2015 - Proceedings of the 32th International Conference on Machine Learning, pages 1973\u20131982, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Chenxin%20Smith%2C%20Virginia%20Jaggi%2C%20Martin%20Jordan%2C%20Michael%20I.%20Adding%20vs.%20Averaging%20in%20Distributed%20Primal-Dual%20Optimization%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Chenxin%20Smith%2C%20Virginia%20Jaggi%2C%20Martin%20Jordan%2C%20Michael%20I.%20Adding%20vs.%20Averaging%20in%20Distributed%20Primal-Dual%20Optimization%201973"
        },
        {
            "id": "19",
            "entry": "[19] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273\u20131282, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202017"
        },
        {
            "id": "20",
            "entry": "[20] Aryan Mokhtari and Alejandro Ribeiro. DSA: Decentralized double stochastic averaging gradient algorithm. Journal of Machine Learning Research, 17(61):1\u201335, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mokhtari%2C%20Aryan%20Ribeiro%2C%20Alejandro%20DSA%3A%20Decentralized%20double%20stochastic%20averaging%20gradient%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mokhtari%2C%20Aryan%20Ribeiro%2C%20Alejandro%20DSA%3A%20Decentralized%20double%20stochastic%20averaging%20gradient%20algorithm%202016"
        },
        {
            "id": "21",
            "entry": "[21] Angelia Nedic, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM Journal on Optimization, 27(4):2597\u20132633, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20Angelia%20Olshevsky%2C%20Alex%20Shi%2C%20Wei%20Achieving%20geometric%20convergence%20for%20distributed%20optimization%20over%20time-varying%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20Angelia%20Olshevsky%2C%20Alex%20Shi%2C%20Wei%20Achieving%20geometric%20convergence%20for%20distributed%20optimization%20over%20time-varying%20graphs%202017"
        },
        {
            "id": "22",
            "entry": "[22] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48\u201361, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20Angelia%20Ozdaglar%2C%20Asuman%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20Angelia%20Ozdaglar%2C%20Asuman%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009"
        },
        {
            "id": "23",
            "entry": "[23] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS Workshop on Autodiff, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPS%20Workshop%20on%20Autodiff%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPS%20Workshop%20on%20Autodiff%202017"
        },
        {
            "id": "24",
            "entry": "[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011"
        },
        {
            "id": "25",
            "entry": "[25] Sashank J Reddi, Jakub Konecny, Peter Richtarik, Barnabas Poczos, and Alex Smola. Aide: Fast and communication efficient distributed optimization. arXiv preprint arXiv:1608.06879, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06879"
        },
        {
            "id": "26",
            "entry": "[26] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20Ralph%20Tyrell%20Convex%20analysis%202015"
        },
        {
            "id": "27",
            "entry": "[27] Kevin Scaman, Francis Bach, Sebastien Bubeck, Yin Tat Lee, and Laurent Massoulie. Optimal algorithms for non-smooth distributed optimization in networks. arXiv preprint arXiv:1806.00291, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00291"
        },
        {
            "id": "28",
            "entry": "[28] Kevin Scaman, Francis R. Bach, Sebastien Bubeck, Yin Tat Lee, and Laurent Massoulie. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 3027\u20133036, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scaman%2C%20Kevin%20Bach%2C%20Francis%20R.%20Bubeck%2C%20Sebastien%20Lee%2C%20Yin%20Tat%20Optimal%20algorithms%20for%20smooth%20and%20strongly%20convex%20distributed%20optimization%20in%20networks%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scaman%2C%20Kevin%20Bach%2C%20Francis%20R.%20Bubeck%2C%20Sebastien%20Lee%2C%20Yin%20Tat%20Optimal%20algorithms%20for%20smooth%20and%20strongly%20convex%20distributed%20optimization%20in%20networks%202017-08"
        },
        {
            "id": "29",
            "entry": "[29] Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944\u2013966, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Wei%20Ling%2C%20Qing%20Wu%2C%20Gang%20Yin%2C%20Wotao%20Extra%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Wei%20Ling%2C%20Qing%20Wu%2C%20Gang%20Yin%2C%20Wotao%20Extra%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015"
        },
        {
            "id": "30",
            "entry": "[30] Wei Shi, Qing Ling, Kun Yuan, Gang Wu, and Wotao Yin. On the Linear Convergence of the ADMM in Decentralized Consensus Optimization. IEEE Transactions on Signal Processing, 62(7):1750\u20131761, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Wei%20Ling%2C%20Qing%20Yuan%2C%20Kun%20Wu%2C%20Gang%20On%20the%20Linear%20Convergence%20of%20the%20ADMM%20in%20Decentralized%20Consensus%20Optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Wei%20Ling%2C%20Qing%20Yuan%2C%20Kun%20Wu%2C%20Gang%20On%20the%20Linear%20Convergence%20of%20the%20ADMM%20in%20Decentralized%20Consensus%20Optimization%202014"
        },
        {
            "id": "31",
            "entry": "[31] Benjamin Sirb and Xiaojing Ye. Decentralized consensus algorithm with delayed and stochastic gradients. SIAM Journal on Optimization, 28(2):1232\u20131254, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sirb%2C%20Benjamin%20Ye%2C%20Xiaojing%20Decentralized%20consensus%20algorithm%20with%20delayed%20and%20stochastic%20gradients%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sirb%2C%20Benjamin%20Ye%2C%20Xiaojing%20Decentralized%20consensus%20algorithm%20with%20delayed%20and%20stochastic%20gradients%202018"
        },
        {
            "id": "32",
            "entry": "[32] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated Multi-Task Learning. In NIPS 2017 - Advances in Neural Information Processing Systems 30, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Virginia%20Chiang%2C%20Chao-Kai%20Sanjabi%2C%20Maziar%20Talwalkar%2C%20Ameet%20Federated%20Multi-Task%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20Virginia%20Chiang%2C%20Chao-Kai%20Sanjabi%2C%20Maziar%20Talwalkar%2C%20Ameet%20Federated%20Multi-Task%20Learning%202017"
        },
        {
            "id": "33",
            "entry": "[33] Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I Jordan, and Martin Jaggi. CoCoA: A General Framework for Communication-Efficient Distributed Optimization. Journal of Machine Learning Research, 18(230):1\u201349, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Virginia%20Forte%2C%20Simone%20Ma%2C%20Chenxin%20Takac%2C%20Martin%20CoCoA%3A%20A%20General%20Framework%20for%20Communication-Efficient%20Distributed%20Optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20Virginia%20Forte%2C%20Simone%20Ma%2C%20Chenxin%20Takac%2C%20Martin%20CoCoA%3A%20A%20General%20Framework%20for%20Communication-Efficient%20Distributed%20Optimization%202018"
        },
        {
            "id": "34",
            "entry": "[34] Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. In NIPS 2018 - Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stich%2C%20Sebastian%20U.%20Cordonnier%2C%20Jean-Baptiste%20Jaggi%2C%20Martin%20Sparsified%20sgd%20with%20memory%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stich%2C%20Sebastian%20U.%20Cordonnier%2C%20Jean-Baptiste%20Jaggi%2C%20Martin%20Sparsified%20sgd%20with%20memory%202018"
        },
        {
            "id": "35",
            "entry": "[35] Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, and Ji Liu. Communication compression for decentralized training. In NIPS 2018 - Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Hanlin%20Gan%2C%20Shaoduo%20Zhang%2C%20Ce%20Zhang%2C%20Tong%20Communication%20compression%20for%20decentralized%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Hanlin%20Gan%2C%20Shaoduo%20Zhang%2C%20Ce%20Zhang%2C%20Tong%20Communication%20compression%20for%20decentralized%20training%202018"
        },
        {
            "id": "36",
            "entry": "[36] Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over decentralized data. arXiv preprint arXiv:1803.07068, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.07068"
        },
        {
            "id": "37",
            "entry": "[37] John N Tsitsiklis, Dimitri P Bertsekas, and Michael Athans. Distributed asynchronous deterministic and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control, 31(9):803\u2013812, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20John%20N.%20Bertsekas%2C%20Dimitri%20P.%20Athans%2C%20Michael%20Distributed%20asynchronous%20deterministic%20and%20stochastic%20gradient%20optimization%20algorithms%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20John%20N.%20Bertsekas%2C%20Dimitri%20P.%20Athans%2C%20Michael%20Distributed%20asynchronous%20deterministic%20and%20stochastic%20gradient%20optimization%20algorithms%201986"
        },
        {
            "id": "38",
            "entry": "[38] Jialei Wang, Weiran Wang, and Nathan Srebro. Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox. In ICML 2017 - Proceedings of the 34th International Conference on Machine Learning, pages 1882\u20131919, June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Srebro.%20Memory%20and%20Communication%20Efficient%20Distributed%20Stochastic%20Optimization%20with%20Minibatch%20Prox%202017-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Srebro.%20Memory%20and%20Communication%20Efficient%20Distributed%20Stochastic%20Optimization%20with%20Minibatch%20Prox%202017-06"
        },
        {
            "id": "39",
            "entry": "[39] Ermin Wei and Asuman Ozdaglar. On the O(1/k) Convergence of Asynchronous Distributed Alternating Direction Method of Multipliers. arXiv, July 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Ermin%20Ozdaglar%2C%20Asuman%20On%20the%20O%281/k%29%20Convergence%20of%20Asynchronous%20Distributed%20Alternating%20Direction%20Method%20of%20Multipliers%202013-07"
        },
        {
            "id": "40",
            "entry": "[40] Tianyu Wu, Kun Yuan, Qing Ling, Wotao Yin, and Ali H Sayed. Decentralized consensus optimization with asynchrony and delays. IEEE Transactions on Signal and Information Processing over Networks, 4(2):293\u2013307, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Tianyu%20Yuan%2C%20Kun%20Ling%2C%20Qing%20Yin%2C%20Wotao%20Decentralized%20consensus%20optimization%20with%20asynchrony%20and%20delays%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Tianyu%20Yuan%2C%20Kun%20Ling%2C%20Qing%20Yin%2C%20Wotao%20Decentralized%20consensus%20optimization%20with%20asynchrony%20and%20delays%202018"
        },
        {
            "id": "41",
            "entry": "[41] Tianbao Yang. Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent. In NIPS 2014 - Advances in Neural Information Processing Systems 27, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Tianbao%20Trading%20Computation%20for%20Communication%3A%20Distributed%20Stochastic%20Dual%20Coordinate%20Ascent%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Tianbao%20Trading%20Computation%20for%20Communication%3A%20Distributed%20Stochastic%20Dual%20Coordinate%20Ascent%202013"
        },
        {
            "id": "42",
            "entry": "[42] Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835\u20131854, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Kun%20Ling%2C%20Qing%20Yin%2C%20Wotao%20On%20the%20convergence%20of%20decentralized%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Kun%20Ling%2C%20Qing%20Yin%2C%20Wotao%20On%20the%20convergence%20of%20decentralized%20gradient%20descent%202016"
        },
        {
            "id": "43",
            "entry": "[43] Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with Elastic Averaging SGD. In NIPS 2015 - Advances in Neural Information Processing Systems 28, pages 685\u2013693, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Sixin%20Choromanska%2C%20Anna%20E.%20LeCun%2C%20Yann%20Deep%20learning%20with%20Elastic%20Averaging%20SGD%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Sixin%20Choromanska%2C%20Anna%20E.%20LeCun%2C%20Yann%20Deep%20learning%20with%20Elastic%20Averaging%20SGD%202015"
        },
        {
            "id": "44",
            "entry": "[44] Yuchen Zhang and Xiao Lin. Disco: Distributed optimization for self-concordant empirical loss. In International conference on machine learning, pages 362\u2013370, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20Disco%3A%20Distributed%20optimization%20for%20self-concordant%20empirical%20loss%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20Disco%3A%20Distributed%20optimization%20for%20self-concordant%20empirical%20loss%202015"
        },
        {
            "id": "45",
            "entry": "[45] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 2595\u20132603, 2010. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Weimer%2C%20Markus%20Li%2C%20Lihong%20Smola%2C%20Alex%20J.%20Parallelized%20stochastic%20gradient%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Weimer%2C%20Markus%20Li%2C%20Lihong%20Smola%2C%20Alex%20J.%20Parallelized%20stochastic%20gradient%20descent%202010"
        }
    ]
}
