{
    "filename": "7964-disconnected-manifold-learning-for-generative-adversarial-networks.pdf",
    "metadata": {
        "title": "Disconnected Manifold Learning for Generative Adversarial Networks",
        "author": "Mahyar Khayatkhoei, Maneesh K. Singh, Ahmed Elgammal",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7964-disconnected-manifold-learning-for-generative-adversarial-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode dropping and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues."
    },
    "keywords": [
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        }
    ],
    "highlights": [
        "Picture of a bird and picture of a cat for example, can we continuously transform the bird into the cat without ever generating a picture that is not neither bird nor cat? In other words, is there a continuous transformation between the two that never leaves the manifold of \"real looking\" images? It is often the case that real world data falls on a union of several disjoint manifolds and such a transformation does not exist, i.e. the real data distribution is supported on a disconnected manifold, and an effective generative model needs to be able to learn such manifolds.<br/><br/>Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], model the problem of finding the unknown distribution of real data as a two player game where one player, called the discriminator, tries to perfectly separate real data from the data generated by a second player, called the generator, while the second player tries to generate data that can perfectly fool the first player",
        "Picture of a bird and picture of a cat for example, can we continuously transform the bird into the cat without ever generating a picture that is not neither bird nor cat? In other words, is there a continuous transformation between the two that never leaves the manifold of \"real looking\" images? It is often the case that real world data falls on a union of several disjoint manifolds and such a transformation does not exist, i.e. the real data distribution is supported on a disconnected manifold, and an effective generative model needs to be able to learn such manifolds",
        "Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], model the problem of finding the unknown distribution of real data as a two player game where one player, called the discriminator, tries to perfectly separate real data from the data generated by a second player, called the generator, while the second player tries to generate data that can perfectly fool the first player",
        "Note that a single generator Generative Adversarial Networks model can not memorize the dataset because it can not learn a distribution supported on N disjoint components as discussed in Section 2",
        "While InfoGAN and DeLiGAN can generate disconnected manifolds, they both assume a fixed number of discreet components equal to the number of underlying classes and have no prior learning over these components, suffering from the issues discussed in Section 3.1",
        "In this work we showed why the single generator Generative Adversarial Networks can not correctly learn distributions supported on disconnected manifolds, what consequences this shortcoming has in practice, and how multigenerator Generative Adversarial Networks can effectively address these issues"
    ],
    "key_statements": [
        "Picture of a bird and picture of a cat for example, can we continuously transform the bird into the cat without ever generating a picture that is not neither bird nor cat? In other words, is there a continuous transformation between the two that never leaves the manifold of \"real looking\" images? It is often the case that real world data falls on a union of several disjoint manifolds and such a transformation does not exist, i.e. the real data distribution is supported on a disconnected manifold, and an effective generative model needs to be able to learn such manifolds.<br/><br/>Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], model the problem of finding the unknown distribution of real data as a two player game where one player, called the discriminator, tries to perfectly separate real data from the data generated by a second player, called the generator, while the second player tries to generate data that can perfectly fool the first player",
        "Picture of a bird and picture of a cat for example, can we continuously transform the bird into the cat without ever generating a picture that is not neither bird nor cat? In other words, is there a continuous transformation between the two that never leaves the manifold of \"real looking\" images? It is often the case that real world data falls on a union of several disjoint manifolds and such a transformation does not exist, i.e. the real data distribution is supported on a disconnected manifold, and an effective generative model needs to be able to learn such manifolds",
        "Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], model the problem of finding the unknown distribution of real data as a two player game where one player, called the discriminator, tries to perfectly separate real data from the data generated by a second player, called the generator, while the second player tries to generate data that can perfectly fool the first player",
        "Note that a single generator Generative Adversarial Networks model can not memorize the dataset because it can not learn a distribution supported on N disjoint components as discussed in Section 2",
        "While InfoGAN and DeLiGAN can generate disconnected manifolds, they both assume a fixed number of discreet components equal to the number of underlying classes and have no prior learning over these components, suffering from the issues discussed in Section 3.1",
        "In this work we showed why the single generator Generative Adversarial Networks can not correctly learn distributions supported on disconnected manifolds, what consequences this shortcoming has in practice, and how multigenerator Generative Adversarial Networks can effectively address these issues"
    ],
    "summary": [
        "Picture of a bird and picture of a cat for example, can we continuously transform the bird into the cat without ever generating a picture that is not neither bird nor cat? In other words, is there a continuous transformation between the two that never leaves the manifold of \"real looking\" images? It is often the case that real world data falls on a union of several disjoint manifolds and such a transformation does not exist, i.e. the real data distribution is supported on a disconnected manifold, and an effective generative model needs to be able to learn such manifolds.<br/><br/>Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], model the problem of finding the unknown distribution of real data as a two player game where one player, called the discriminator, tries to perfectly separate real data from the data generated by a second player, called the generator, while the second player tries to generate data that can perfectly fool the first player.",
        "We identify a shortcoming of GANs in modeling distributions with disconnected support, and investigate its consequences, namely mode dropping, worse sample quality, and worse local convergence (Section 2).",
        "We use the term mode dropping to refer to the situation where one or several submanifolds of real data are not completely covered by the support of the generated distribution.",
        "When the generator can only learn a distribution with globally connected support, it has to learn a cover of the real data submanifolds, in other words, the generator can not reduce the probability density of the off real-manifold space beyond a certain value.",
        "When the generator can not learn the correct support of the real data distribution, as is in our discussion, the resulting equilibrium may not be locally convergent.",
        "While most multi-generator models consider p(c) as a uniform distribution over generators, this naive choice of prior can cause certain difficulties in learning a disconnected support.",
        "We can take an EM approach to learning the prior: the expected value of q(c|x) over the real data distribution gives us an approximation of p(c) (E step), which we can use to train the DMGAN model (M step).",
        "Note that a single generator GAN model can not memorize the dataset because it can not learn a distribution supported on N disjoint components as discussed in Section 2.",
        "Note that if we were to assign one unique point of the Z space to each dataset sample, a neural network could learn to memorize the dataset by mapping each selected z \u2208 Z to its corresponding real sample, this is not how GANs are modeled.",
        "In case of distributions with disconnected support, these models do not provide much advantage over common GANs and suffer from the same issues we discussed in Section 2 due to having a single generator.",
        "MAD-GAN [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], uses k generators, together with a k + 1-class discriminator which is trained to correctly classify samples from each generator and true data, Model",
        "While InfoGAN and DeLiGAN can generate disconnected manifolds, they both assume a fixed number of discreet components equal to the number of underlying classes and have no prior learning over these components, suffering from the issues discussed in Section 3.1.",
        "In this work we showed why the single generator GANs can not correctly learn distributions supported on disconnected manifolds, what consequences this shortcoming has in practice, and how multigenerator GANs can effectively address these issues.",
        "Extending the prior learning to other methods, such as learning a prior over shape of Z space, and investigating the effects of adding diversity to discriminator as well as the generators, remain as exciting future paths for research"
    ],
    "headline": "We show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator Generative Adversarial Networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Martin Arjovsky and L\u00e9on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04862"
        },
        {
            "id": "2",
            "entry": "[2] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pages 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martin%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00573"
        },
        {
            "id": "4",
            "entry": "[4] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "5",
            "entry": "[5] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.02136"
        },
        {
            "id": "6",
            "entry": "[6] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "7",
            "entry": "[7] Jeff Donahue, Philipp Kr\u00e4henb\u00fchl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09782"
        },
        {
            "id": "8",
            "entry": "[8] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "9",
            "entry": "[9] Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri, Philip HS Torr, and Puneet K Dokania. Multi-agent diverse generative adversarial networks. arXiv preprint arXiv:1704.02906, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02906"
        },
        {
            "id": "10",
            "entry": "[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "11",
            "entry": "[11] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "12",
            "entry": "[12] Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and V Babu Radhakrishnan. Deligan: Generative adversarial networks for diverse and limited data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gurumurthy%2C%20Swaminathan%20Sarvadevabhatla%2C%20Ravi%20Kiran%20Radhakrishnan%2C%20V.Babu%20Deligan%3A%20Generative%20adversarial%20networks%20for%20diverse%20and%20limited%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gurumurthy%2C%20Swaminathan%20Sarvadevabhatla%2C%20Ravi%20Kiran%20Radhakrishnan%2C%20V.Babu%20Deligan%3A%20Generative%20adversarial%20networks%20for%20diverse%20and%20limited%20data%202017"
        },
        {
            "id": "13",
            "entry": "[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6629\u20136640, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "14",
            "entry": "[14] Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. MGAN: Training generative adversarial nets with multiple generators. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rkmu5b0a-.",
            "url": "https://openreview.net/forum?id=rkmu5b0a-",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoang%2C%20Quan%20Nguyen%2C%20Tu%20Dinh%20Le%2C%20Trung%20Phung%2C%20Dinh%20MGAN%3A%20Training%20generative%20adversarial%20nets%20with%20multiple%20generators%202018"
        },
        {
            "id": "15",
            "entry": "[15] John L Kelley. General topology. Courier Dover Publications, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelley%2C%20John%20L.%20General%20topology%202017"
        },
        {
            "id": "16",
            "entry": "[16] Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "17",
            "entry": "[17] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "18",
            "entry": "[18] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10337"
        },
        {
            "id": "19",
            "entry": "[19] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? arXiv preprint arXiv:1801.04406, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04406"
        },
        {
            "id": "20",
            "entry": "[20] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02163"
        },
        {
            "id": "21",
            "entry": "[21] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems, pages 5591\u20135600, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "22",
            "entry": "[22] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "23",
            "entry": "[23] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "24",
            "entry": "[24] Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing Systems, pages 3310\u20133320, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017"
        },
        {
            "id": "25",
            "entry": "[25] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tenenbaum%2C%20Joshua%20B.%20Silva%2C%20Vin%20De%20Langford%2C%20John%20C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tenenbaum%2C%20Joshua%20B.%20Silva%2C%20Vin%20De%20Langford%2C%20John%20C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000"
        },
        {
            "id": "26",
            "entry": "[26] Ming-Hsuan Yang. Extended isomap for pattern classification. In AAAI/IAAI, pages 224\u2013229, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Ming-Hsuan%20Extended%20isomap%20for%20pattern%20classification%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Ming-Hsuan%20Extended%20isomap%20for%20pattern%20classification%202002"
        },
        {
            "id": "27",
            "entry": "[27] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. ",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        }
    ]
}
