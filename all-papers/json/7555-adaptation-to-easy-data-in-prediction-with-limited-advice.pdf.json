{
    "filename": "7555-adaptation-to-easy-data-in-prediction-with-limited-advice.pdf",
    "metadata": {
        "title": "Adaptation to Easy Data in Prediction with Limited Advice",
        "author": "Tobias Thune, Yevgeny Seldin",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7555-adaptation-to-easy-data-in-prediction-with-limited-advice.pdf"
        },
        "abstract": "We derive an online learning algorithm with improved regret guarantees for \u201ceasy\u201d\nloss sequences. We consider two types of \u201ceasiness\u201d: (a) stochastic loss sequences and (b) adversarial loss sequences with small effective range of the losses. While a number of algorithms have been proposed for exploiting small effective range in the full information setting, <a class=\"ref-link\" id=\"cGerchinovitz_2016_a\" href=\"#rGerchinovitz_2016_a\">Gerchinovitz and Lattimore [2016</a>] have shown the impossibility of regret scaling with the effective range of the losses in the bandit setting. We show that just one additional observation per round is sufficient to circumvent the impossibility result. The proposed Second Order Difference"
    },
    "keywords": [
        {
            "term": "second order",
            "url": "https://en.wikipedia.org/wiki/second_order"
        },
        {
            "term": "adversarial bandit",
            "url": "https://en.wikipedia.org/wiki/adversarial_bandit"
        }
    ],
    "highlights": [
        "Online learning algorithms with both worst-case regret guarantees and refined guarantees for \u201ceasy\u201d<br/><br/>loss sequences have come into research focus in recent years",
        "We introduce the Second Order Difference Adjustments (SODA) algorithm, summarized in Algorithm 1",
        "First we provide an upper bound for the expected regret of Second Order Difference Adjustments against oblivious adversaries that produce loss sequences with effective loss range bounded by \u03b5",
        "We have presented the Second Order Difference Adjustments algorithm for prediction with limited advice with two observations per round",
        "We have shown that the algorithm adapts to two types of simplicity of loss sequences simultaneously: (a) it provides improved regret guarantees for adversarial sequences with bounded effective range of the losses and (b) for stochastic loss sequences",
        "Our result demonstrates that just one extra observation per round is sufficient to circumvent the impossibility result of <a class=\"ref-link\" id=\"cGerchinovitz_2016_a\" href=\"#rGerchinovitz_2016_a\">Gerchinovitz and Lattimore [2016</a>] and significantly relaxes the assumptions made by <a class=\"ref-link\" id=\"cCesa-Bianchi_2018_a\" href=\"#rCesa-Bianchi_2018_a\">Cesa-Bianchi and Shamir [2018</a>] to achieve the same goal"
    ],
    "key_statements": [
        "Online learning algorithms with both worst-case regret guarantees and refined guarantees for \u201ceasy\u201d<br/><br/>loss sequences have come into research focus in recent years",
        "We introduce the Second Order Difference Adjustments (SODA) algorithm, summarized in Algorithm 1",
        "We are ready to present the regret bounds for Second Order Difference Adjustments",
        "First we provide an upper bound for the expected regret of Second Order Difference Adjustments against oblivious adversaries that produce loss sequences with effective loss range bounded by \u03b5",
        "The expected regret of Second Order Difference Adjustments applied to stochastic loss sequences with gaps \u2206a satisfies",
        "We have presented the Second Order Difference Adjustments algorithm for prediction with limited advice with two observations per round",
        "We have shown that the algorithm adapts to two types of simplicity of loss sequences simultaneously: (a) it provides improved regret guarantees for adversarial sequences with bounded effective range of the losses and (b) for stochastic loss sequences",
        "Our result demonstrates that just one extra observation per round is sufficient to circumvent the impossibility result of <a class=\"ref-link\" id=\"cGerchinovitz_2016_a\" href=\"#rGerchinovitz_2016_a\">Gerchinovitz and Lattimore [2016</a>] and significantly relaxes the assumptions made by <a class=\"ref-link\" id=\"cCesa-Bianchi_2018_a\" href=\"#rCesa-Bianchi_2018_a\">Cesa-Bianchi and Shamir [2018</a>] to achieve the same goal"
    ],
    "summary": [
        "Online learning algorithms with both worst-case regret guarantees and refined guarantees for \u201ceasy\u201d<br/><br/>loss sequences have come into research focus in recent years.",
        "We propose an algorithm, which achieves improved regret guarantees both when the effective loss range is small and when the losses are stochastic.",
        "We note that in the adversarial setting the losses are considered deterministic and the second expectation can be omitted, whereas in the stochastic setting the definition coincides with the definition of pseudo-regret [<a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\"><a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\">Bubeck and Cesa-Bianchi, 2012</a></a>, <a class=\"ref-link\" id=\"cSeldin_2017_a\" href=\"#rSeldin_2017_a\">Seldin and Lugosi, 2017</a>].",
        "SODA samples the \u201csecondary\u201d action Bt uniformly from K \u2212 1 arms, all except At, and the (K \u2212 1) term above corresponds to importance weighting with respect to the sampling of Bt. The loss difference estimators scale with the effective range of the losses and they can be positive and negative.",
        "We start with regret upper and lower bounds in the adversarial regime and show that the algorithm simultaneously achieves improved regret guarantee in the stochastic regime.",
        "First we provide an upper bound for the expected regret of SODA against oblivious adversaries that produce loss sequences with effective loss range bounded by \u03b5.",
        "\u221a The theorem is proven by adaptation of the \u03a9( KT ) lower bound by Seldin et al [2014] for prediction with limited advice with unrestricted losses in [0, 1] and one extra observation.",
        "The expected regret of SODA applied to stochastic loss sequences with gaps \u2206a satisfies",
        "SODA achieves adversarial regret guarantee that scales with the effective loss range and almost matches the lower bound and simultaneously has improved regret guarantee in the stochastic regime.",
        "The expected cumulative loss difference estimators are equal to the negative expect regret against the corresponding arm a: T",
        "Similar to the analysis of the anytime version of EXP3 in <a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\">Bubeck and Cesa-Bianchi [2012</a>], which builds on Auer et al [2002b], we consider upper and lower bounds on the expectation of the incremental update.",
        "Proof of Theorem 1 We apply Lemma 1, which leads to the following inequality for any learning rate scheme \u03b7t for t = 1, 2, .",
        "Unlike in the anytime analysis of EXP3, where this term is negative [<a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\"><a class=\"ref-link\" id=\"cBubeck_2012_a\" href=\"#rBubeck_2012_a\">Bubeck and Cesa-Bianchi, 2012</a></a>], in our case it turns to be related to the second moment of the loss difference estimators.",
        "We have shown that the algorithm adapts to two types of simplicity of loss sequences simultaneously: (a) it provides improved regret guarantees for adversarial sequences with bounded effective range of the losses and (b) for stochastic loss sequences."
    ],
    "headline": "We show that just one additional observation per round is sufficient to circumvent the impossibility result",
    "reference_links": [
        {
            "id": "Alon_et+al_2017_a",
            "entry": "Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Journal on Computing, 46(6):1785\u20131826, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alon%2C%20Noga%20Cesa-Bianchi%2C%20Nicolo%20Gentile%2C%20Claudio%20Mannor%2C%20Shie%20Nonstochastic%20multi-armed%20bandits%20with%20graph-structured%20feedback%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alon%2C%20Noga%20Cesa-Bianchi%2C%20Nicolo%20Gentile%2C%20Claudio%20Mannor%2C%20Shie%20Nonstochastic%20multi-armed%20bandits%20with%20graph-structured%20feedback%202017"
        },
        {
            "id": "Peter_2016_a",
            "entry": "Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peter%20Auer%20and%20Chao-Kai%20Chiang.%20An%20algorithm%20with%20nearly%20optimal%20pseudo-regret%20for%20both%20stochastic%20and%20adversarial%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peter%20Auer%20and%20Chao-Kai%20Chiang.%20An%20algorithm%20with%20nearly%20optimal%20pseudo-regret%20for%20both%20stochastic%20and%20adversarial%20bandits%202016"
        },
        {
            "id": "Peter_0000_a",
            "entry": "Peter Auer, Nicol\u00f2 Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47, 2002a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peter%20Auer%2C%20Nicol%C3%B2%20Cesa-Bianchi%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peter%20Auer%2C%20Nicol%C3%B2%20Cesa-Bianchi%20Fischer%2C%20Paul%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem"
        },
        {
            "id": "Peter_0000_b",
            "entry": "Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal of Computing, 32(1), 2002b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peter%20Auer%2C%20Nicol%C3%B2%20Cesa-Bianchi%2C%20Yoav%20Freund%20Schapire%2C%20Robert%20E.%20The%20nonstochastic%20multiarmed%20bandit%20problem",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peter%20Auer%2C%20Nicol%C3%B2%20Cesa-Bianchi%2C%20Yoav%20Freund%20Schapire%2C%20Robert%20E.%20The%20nonstochastic%20multiarmed%20bandit%20problem"
        },
        {
            "id": "Bubeck_2012_a",
            "entry": "S\u00e9bastien Bubeck and Nicol\u00f2 Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. Foundations and Trends in Machine Learning, 5, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multiarmed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicol%C3%B2%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multiarmed%20bandit%20problems%202012"
        },
        {
            "id": "S\u00e9bastien_2012_a",
            "entry": "S\u00e9bastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial bandits. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20S%C3%A9bastien%20Bubeck%20and%20Aleksandrs%20Slivkins.%20The%20best%20of%20both%20worlds%3A%20stochastic%20and%20adversarial%20bandits%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20S%C3%A9bastien%20Bubeck%20and%20Aleksandrs%20Slivkins.%20The%20best%20of%20both%20worlds%3A%20stochastic%20and%20adversarial%20bandits%202012"
        },
        {
            "id": "Cesa-Bianchi_2006_a",
            "entry": "Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicol%C3%B2%20Prediction%2C%20G%C3%A1bor%20Lugosi%20Learning%2C%20and%20Games%202006"
        },
        {
            "id": "Cesa-Bianchi_2018_a",
            "entry": "Nicol\u00f2 Cesa-Bianchi and Ohad Shamir. Bandit regret scaling with the effective loss range. In Proceedings of the International Conference on Algorithmic Learning Theory (ALT), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicol%C3%B2%20Shamir%2C%20Ohad%20Bandit%20regret%20scaling%20with%20the%20effective%20loss%20range%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20Nicol%C3%B2%20Shamir%2C%20Ohad%20Bandit%20regret%20scaling%20with%20the%20effective%20loss%20range%202018"
        },
        {
            "id": "Cesa-Bianchi_et+al_2007_a",
            "entry": "Nicol\u00f2 Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicol%C3%B2%20Mansour%2C%20Yishay%20Stoltz%2C%20Gilles%20Improved%20second-order%20bounds%20for%20prediction%20with%20expert%20advice%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20Nicol%C3%B2%20Mansour%2C%20Yishay%20Stoltz%2C%20Gilles%20Improved%20second-order%20bounds%20for%20prediction%20with%20expert%20advice%202007"
        },
        {
            "id": "Gaillard_et+al_2014_a",
            "entry": "Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gaillard%2C%20Pierre%20Stoltz%2C%20Gilles%20van%20Erven%2C%20Tim%20A%20second-order%20bound%20with%20excess%20losses%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gaillard%2C%20Pierre%20Stoltz%2C%20Gilles%20van%20Erven%2C%20Tim%20A%20second-order%20bound%20with%20excess%20losses%202014"
        },
        {
            "id": "Gerchinovitz_2016_a",
            "entry": "S\u00e9bastien Gerchinovitz and Tor Lattimore. Refined lower bounds for adversarial bandits. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerchinovitz%2C%20S%C3%A9bastien%20Lattimore%2C%20Tor%20Refined%20lower%20bounds%20for%20adversarial%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gerchinovitz%2C%20S%C3%A9bastien%20Lattimore%2C%20Tor%20Refined%20lower%20bounds%20for%20adversarial%20bandits%202016"
        },
        {
            "id": "Kale_2014_a",
            "entry": "Satyen Kale. Multiarmed bandits with limited expert advice. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kale%2C%20Satyen%20Multiarmed%20bandits%20with%20limited%20expert%20advice%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kale%2C%20Satyen%20Multiarmed%20bandits%20with%20limited%20expert%20advice%202014"
        },
        {
            "id": "Koolen_2015_a",
            "entry": "Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combinatorial games. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koolen%2C%20Wouter%20M.%20van%20Erven%2C%20Tim%20Second-order%20quantile%20methods%20for%20experts%20and%20combinatorial%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koolen%2C%20Wouter%20M.%20van%20Erven%2C%20Tim%20Second-order%20quantile%20methods%20for%20experts%20and%20combinatorial%20games%202015"
        },
        {
            "id": "Lai_1985_a",
            "entry": "Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20Tze%20Leung%20Robbins%2C%20Herbert%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20Tze%20Leung%20Robbins%2C%20Herbert%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985"
        },
        {
            "id": "Littlestone_1994_a",
            "entry": "Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Information and Computation, 108, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littlestone%2C%20Nick%20Warmuth%2C%20Manfred%20K.%20The%20weighted%20majority%20algorithm%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littlestone%2C%20Nick%20Warmuth%2C%20Manfred%20K.%20The%20weighted%20majority%20algorithm%201994"
        },
        {
            "id": "Luo_2015_a",
            "entry": "Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adanormalhedge. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Haipeng%20Schapire%2C%20Robert%20E.%20Achieving%20all%20with%20no%20parameters%3A%20Adanormalhedge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Haipeng%20Schapire%2C%20Robert%20E.%20Achieving%20all%20with%20no%20parameters%3A%20Adanormalhedge%202015"
        },
        {
            "id": "Robbins_1952_a",
            "entry": "Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 1952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20Herbert%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952"
        },
        {
            "id": "Seldin_2017_a",
            "entry": "Yevgeny Seldin and G\u00e1bor Lugosi. An improved parametrization and analysis of the EXP3++ algorithm for stochastic and adversarial bandits. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seldin%2C%20Yevgeny%20Lugosi%2C%20G%C3%A1bor%20An%20improved%20parametrization%20and%20analysis%20of%20the%20EXP3%2B%2B%20algorithm%20for%20stochastic%20and%20adversarial%20bandits%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seldin%2C%20Yevgeny%20Lugosi%2C%20G%C3%A1bor%20An%20improved%20parametrization%20and%20analysis%20of%20the%20EXP3%2B%2B%20algorithm%20for%20stochastic%20and%20adversarial%20bandits%202017"
        },
        {
            "id": "Seldin_2014_a",
            "entry": "Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In Proceedings of the International Conference on Machine Learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seldin%2C%20Yevgeny%20Slivkins%2C%20Aleksandrs%20One%20practical%20algorithm%20for%20both%20stochastic%20and%20adversarial%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seldin%2C%20Yevgeny%20Slivkins%2C%20Aleksandrs%20One%20practical%20algorithm%20for%20both%20stochastic%20and%20adversarial%20bandits%202014"
        },
        {
            "id": "Seldin_et+al_2013_a",
            "entry": "Yevgeny Seldin, Koby Crammer, and Peter L. Bartlett. Open problem: Adversarial multiarmed bandits with limited advice. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seldin%2C%20Yevgeny%20Crammer%2C%20Koby%20Bartlett%2C%20Peter%20L.%20Open%20problem%3A%20Adversarial%20multiarmed%20bandits%20with%20limited%20advice%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seldin%2C%20Yevgeny%20Crammer%2C%20Koby%20Bartlett%2C%20Peter%20L.%20Open%20problem%3A%20Adversarial%20multiarmed%20bandits%20with%20limited%20advice%202013"
        },
        {
            "id": "Seldin_et+al_2014_b",
            "entry": "Yevgeny Seldin, Peter L. Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with limited advice and multiarmed bandits with paid observations. In Proceedings of the International Conference on Machine Learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seldin%2C%20Yevgeny%20Bartlett%2C%20Peter%20L.%20Crammer%2C%20Koby%20Abbasi-Yadkori%2C%20Yasin%20Prediction%20with%20limited%20advice%20and%20multiarmed%20bandits%20with%20paid%20observations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seldin%2C%20Yevgeny%20Bartlett%2C%20Peter%20L.%20Crammer%2C%20Koby%20Abbasi-Yadkori%2C%20Yasin%20Prediction%20with%20limited%20advice%20and%20multiarmed%20bandits%20with%20paid%20observations%202014"
        },
        {
            "id": "Vovk_1990_a",
            "entry": "Vladimir Vovk. Aggregating strategies. In Proceedings of the International Conference on Computational Learning Theory (COLT), 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vovk%2C%20Vladimir%20Aggregating%20strategies%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vovk%2C%20Vladimir%20Aggregating%20strategies%201990"
        },
        {
            "id": "Wei_2018_a",
            "entry": "Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Chen-Yu%20Luo%2C%20Haipeng%20More%20adaptive%20algorithms%20for%20adversarial%20bandits%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Chen-Yu%20Luo%2C%20Haipeng%20More%20adaptive%20algorithms%20for%20adversarial%20bandits%202018"
        },
        {
            "id": "Wintenberger_2017_a",
            "entry": "Olivier Wintenberger. Optimal learning with Bernstein online aggregation. Machine Learning, 106, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wintenberger%2C%20Olivier%20Optimal%20learning%20with%20Bernstein%20online%20aggregation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wintenberger%2C%20Olivier%20Optimal%20learning%20with%20Bernstein%20online%20aggregation%202017"
        },
        {
            "id": "Zimmert_2018_a",
            "entry": "Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits. Technical report, https://arxiv.org/abs/1807.07623, 2018.",
            "url": "https://arxiv.org/abs/1807.07623",
            "arxiv_url": "https://arxiv.org/pdf/1807.07623"
        }
    ]
}
