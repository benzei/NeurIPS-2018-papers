{
    "filename": "7454-collaborative-learning-for-deep-neural-networks.pdf",
    "metadata": {
        "title": "Collaborative Learning for Deep Neural Networks",
        "author": "Guocong Song, Wei Chai",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7454-collaborative-learning-for-deep-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation rescaling aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "graphics processing unit",
            "url": "https://en.wikipedia.org/wiki/graphics_processing_unit"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "computational complexity",
            "url": "https://en.wikipedia.org/wiki/computational_complexity"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "collaborative learning",
            "url": "https://en.wikipedia.org/wiki/collaborative_learning"
        }
    ],
    "highlights": [
        "When training deep neural networks, we must confront the challenges of general nonconvex optimization problems",
        "Local gradient descent methods that most deep learning systems rely on, such as variants of stochastic gradient descent (SGD), have no guarantee that the optimization algorithm will converge to a global minimum",
        "Experiments have been performed with several popular deep neural networks on different datasets to benchmark performance, and their results demonstrate that collaborative learning provides significant accuracy improvement for image classification problems in a generic way",
        "There are two major mechanisms collaborative learning benefits from: 1) The consensus of multiple views from different classifier heads on the same data provides supplementary information and regularization to each classifier.\n2) Besides computational complexity reduction benefited from intermediate-level representation sharing, backpropagation rescaling aggregates the gradient flows from all heads in a balanced way, which leads to additional performance enhancement",
        "We have proposed a framework of collaborative learning to train a deep neural network in a group of generated classifiers based on the target network",
        "By well aggregating the gradient flows from all heads, intermediate-level representation sharing with backpropagation rescaling not only lowers training computational cost, but facilitates supervision to the shared layers"
    ],
    "key_statements": [
        "When training deep neural networks, we must confront the challenges of general nonconvex optimization problems",
        "Local gradient descent methods that most deep learning systems rely on, such as variants of stochastic gradient descent (SGD), have no guarantee that the optimization algorithm will converge to a global minimum",
        "To keep the exact same computational complexity for inference, several training techniques have been developed by adding additional networks in the training graph to boost accuracy without affecting the inference graph, including auxiliary training [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], multi-task learning [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and knowledge distillation [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "Auxiliary training is introduced to improve the convergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "We propose a framework of collaborative learning that trains several classifier heads of the same network simultaneously on the same training data to cope with the above challenges",
        "Experiments have been performed with several popular deep neural networks on different datasets to benchmark performance, and their results demonstrate that collaborative learning provides significant accuracy improvement for image classification problems in a generic way",
        "There are two major mechanisms collaborative learning benefits from: 1) The consensus of multiple views from different classifier heads on the same data provides supplementary information and regularization to each classifier.\n2) Besides computational complexity reduction benefited from intermediate-level representation sharing, backpropagation rescaling aggregates the gradient flows from all heads in a balanced way, which leads to additional performance enhancement",
        "The major contributions are summarized as follows.\n1) Collaborative learning provides a new training framework that for any given model architecture, we can use the proposed collaborative training method to potentially improve accuracy, with no extra inference cost, with no need to design another model architecture, with minimal hyperparameter re-tuning.\n2) We introduce intermediate-level representation sharing into codistillation that not only enhances training time/memory efficiency but improves generalization error.\n3) Backpropagation rescaling we propose to avoid gradient explosion when the number of heads is big is proven able to improve accuracy when the number of heads is small.\n4) Collaborative learning is demonstrated to be robust to label noise.\n2 Related work",
        "The optimization here is mainly designed to take new concepts involved in collaborative learning into account, including a group of classifiers, and intermediate-level representation sharing",
        "Backpropagation rescaling is essential for intermediate-level representation sharing to have better performance by just reusing a training configuration well tuned in individual learning",
        "We evaluate how collaborative learning helps improve the performance of ResNet-50 network",
        "As mentioned in Section 3.1, collaborative learning brings some extra training cost since it generates more classifier heads in training, and intermediate-level representation sharing is designed for training speedup and memory consumption reduction",
        "We have proposed a framework of collaborative learning to train a deep neural network in a group of generated classifiers based on the target network",
        "By well aggregating the gradient flows from all heads, intermediate-level representation sharing with backpropagation rescaling not only lowers training computational cost, but facilitates supervision to the shared layers"
    ],
    "summary": [
        "When training deep neural networks, we must confront the challenges of general nonconvex optimization problems.",
        "We propose a framework of collaborative learning that trains several classifier heads of the same network simultaneously on the same training data to cope with the above challenges.",
        "Experiments have been performed with several popular deep neural networks on different datasets to benchmark performance, and their results demonstrate that collaborative learning provides significant accuracy improvement for image classification problems in a generic way.",
        "The framework of collaborative learning consists of three major parts: the generation of a population of classifier heads in the training graph, the formulation of the learning objective, and optimization",
        "The structure symmetry for all heads does not require additional different weights associated with loss functions to well balance injected backpropagation error flows, because an equal weight for each head\u2019s objective is optimal for training.",
        "The optimization here is mainly designed to take new concepts involved in collaborative learning into account, including a group of classifiers, and ILR sharing.",
        "Backpropagation rescaling is essential for ILR sharing to have better performance by just reusing a training configuration well tuned in individual learning.",
        "The performance of any model trained with collaborative learning is evaluated using the first classifier head without head selection.",
        "ILR sharing reduces not only GPU memory consumption and training time but the generalization error considerately.",
        "To train a ResNet-32, we use a simple ILR sharing topology with four heads, and the split point located after the first residual block.",
        "We aim at validating the noisy label resistance of collaborative learning on the CIFAR-10 dataset with ResNet-32.",
        "With ResNet-50 on ImageNet. As mentioned in Section 3.1, collaborative learning brings some extra training cost since it generates more classifier heads in training, and ILR sharing is designed for training speedup and memory consumption reduction.",
        "Compared to distillation1, collaborative learning can achieve a lower error rate with a much less training time in an end-to-end way.",
        "We have proposed a framework of collaborative learning to train a deep neural network in a group of generated classifiers based on the target network.",
        "By well aggregating the gradient flows from all heads, ILR sharing with backpropagation rescaling not only lowers training computational cost, but facilitates supervision to the shared layers.",
        "Empirical results have validated the advantages of simultaneous optimization and backpropagation rescaling in group learning.",
        "Collaborative learning provides a flexible and powerful end-to-end training approach for deep neural networks to achieve better performance.",
        "Other machine learning tasks, such as regression, may take advantage of collaborative learning as well"
    ],
    "headline": "We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man\u00e9, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi\u00e9gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Agarwal%2C%20A.%20Barham%2C%20P.%20Brevdo%2C%20E.%20TensorFlow%3A%20Large-scale%20machine%20learning%20on%20heterogeneous%20systems%202015"
        },
        {
            "id": "2",
            "entry": "[2] R. Anil, G. Pereyra, A. Passos, R. Ormandi, G. E. Dahl, and G. E. Hinton. Large scale distributed neural network training through online distillation. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anil%2C%20R.%20Pereyra%2C%20G.%20Passos%2C%20A.%20Ormandi%2C%20R.%20Large%20scale%20distributed%20neural%20network%20training%20through%20online%20distillation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anil%2C%20R.%20Pereyra%2C%20G.%20Passos%2C%20A.%20Ormandi%2C%20R.%20Large%20scale%20distributed%20neural%20network%20training%20through%20online%20distillation%202018"
        },
        {
            "id": "3",
            "entry": "[3] J. Baxter. Learning internal representations. In Proceedings of the Eighth Annual Conference on Computational Learning Theory, COLT \u201995, pages 311\u2013320. ACM, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Baxter%20Learning%20internal%20representations%20In%20Proceedings%20of%20the%20Eighth%20Annual%20Conference%20on%20Computational%20Learning%20Theory%20COLT%2095%20pages%20311320%20ACM%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Baxter%20Learning%20internal%20representations%20In%20Proceedings%20of%20the%20Eighth%20Annual%20Conference%20on%20Computational%20Learning%20Theory%20COLT%2095%20pages%20311320%20ACM%201995"
        },
        {
            "id": "4",
            "entry": "[4] R. Caruana. Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning (ICML), pages 41\u201348. Morgan Kaufmann, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20R.%20Multitask%20learning%3A%20A%20knowledge-based%20source%20of%20inductive%20bias%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20R.%20Multitask%20learning%3A%20A%20knowledge-based%20source%20of%20inductive%20bias%201993"
        },
        {
            "id": "5",
            "entry": "[5] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost. arXiv, abs/1604.06174, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.06174"
        },
        {
            "id": "6",
            "entry": "[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "7",
            "entry": "[7] T. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and A. Anandkumar. Born again neural networks. In International Conference on Machine Learning (ICML), July 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Furlanello%2C%20T.%20Lipton%2C%20Z.C.%20Tschannen%2C%20M.%20Itti%2C%20L.%20Born%20again%20neural%20networks%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Furlanello%2C%20T.%20Lipton%2C%20Z.C.%20Tschannen%2C%20M.%20Itti%2C%20L.%20Born%20again%20neural%20networks%202018-07"
        },
        {
            "id": "8",
            "entry": "[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "9",
            "entry": "[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016-06"
        },
        {
            "id": "10",
            "entry": "[10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.%20Vinyals%2C%20O.%20Dean%2C%20J.%20Distilling%20the%20knowledge%20in%20a%20neural%20network.%20In%20NIPS%20Deep%20Learning%20and%20Representation%20Learning%20Workshop%202015"
        },
        {
            "id": "11",
            "entry": "[11] G. Huang, Z. Liu, L. van der Maaten, and K. Weinberger. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20van%20der%20Maaten%2C%20L.%20Weinberger%2C%20K.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20van%20der%20Maaten%2C%20L.%20Weinberger%2C%20K.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "12",
            "entry": "[12] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and K. Murphy. Speed/accuracy trade-offs for modern convolutional object detectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20J.%20Rathod%2C%20V.%20Sun%2C%20C.%20Zhu%2C%20M.%20Speed/accuracy%20trade-offs%20for%20modern%20convolutional%20object%20detectors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20J.%20Rathod%2C%20V.%20Sun%2C%20C.%20Zhu%2C%20M.%20Speed/accuracy%20trade-offs%20for%20modern%20convolutional%20object%20detectors%202017"
        },
        {
            "id": "13",
            "entry": "[13] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "14",
            "entry": "[14] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laine%2C%20S.%20Aila%2C%20T.%20Temporal%20ensembling%20for%20semi-supervised%20learning%202017"
        },
        {
            "id": "15",
            "entry": "[15] L. Mescheder, S. Nowozin, and A. Geiger. The numerics of gans. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20numerics%20of%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mescheder%2C%20L.%20Nowozin%2C%20S.%20Geiger%2C%20A.%20The%20numerics%20of%20gans%202017"
        },
        {
            "id": "16",
            "entry": "[16] V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20V.%20Kolter%2C%20J.Z.%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20V.%20Kolter%2C%20J.Z.%20Gradient%20descent%20GAN%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "17",
            "entry": "[17] M. Rhu, N. Gimelshein, J. Clemons, A. Zulfiqar, and S. W. Keckler. vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design. In 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rhu%2C%20M.%20Gimelshein%2C%20N.%20Clemons%2C%20J.%20Zulfiqar%2C%20A.%20vDNN%3A%20Virtualized%20deep%20neural%20networks%20for%20scalable%2C%20memory-efficient%20neural%20network%20design%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rhu%2C%20M.%20Gimelshein%2C%20N.%20Clemons%2C%20J.%20Zulfiqar%2C%20A.%20vDNN%3A%20Virtualized%20deep%20neural%20networks%20for%20scalable%2C%20memory-efficient%20neural%20network%20design%202016"
        },
        {
            "id": "18",
            "entry": "[18] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romero%2C%20A.%20Ballas%2C%20N.%20Kahou%2C%20S.E.%20Chassang%2C%20A.%20Fitnets%3A%20Hints%20for%20thin%20deep%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romero%2C%20A.%20Ballas%2C%20N.%20Kahou%2C%20S.E.%20Chassang%2C%20A.%20Fitnets%3A%20Hints%20for%20thin%20deep%20nets%202015"
        },
        {
            "id": "19",
            "entry": "[19] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "20",
            "entry": "[20] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "21",
            "entry": "[21] Y. Yang and T. Hospedales. Deep multi-task representation learning: A tensor factorisation approach. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Y.%20Hospedales%2C%20T.%20Deep%20multi-task%20representation%20learning%3A%20A%20tensor%20factorisation%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Y.%20Hospedales%2C%20T.%20Deep%20multi-task%20representation%20learning%3A%20A%20tensor%20factorisation%20approach%202017"
        },
        {
            "id": "22",
            "entry": "[22] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Cisse%2C%20M.%20Dauphin%2C%20Y.N.%20D.%20Lopez-Paz.%20mixup%3A%20Beyond%20empirical%20risk%20minimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Cisse%2C%20M.%20Dauphin%2C%20Y.N.%20D.%20Lopez-Paz.%20mixup%3A%20Beyond%20empirical%20risk%20minimization%202018"
        },
        {
            "id": "23",
            "entry": "[23] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep mutual learning. arXiv, abs/1706.00384, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1706.00384"
        }
    ]
}
