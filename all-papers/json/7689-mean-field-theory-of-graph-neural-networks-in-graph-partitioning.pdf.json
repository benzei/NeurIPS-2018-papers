{
    "filename": "7689-mean-field-theory-of-graph-neural-networks-in-graph-partitioning.pdf",
    "metadata": {
        "title": "Mean-field theory of graph neural networks in graph partitioning",
        "author": "Tatsuro Kawamoto, Masashi Tsubaki, Tomoyuki Obuchi",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7689-mean-field-theory-of-graph-neural-networks-in-graph-partitioning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments."
    },
    "keywords": [
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        },
        {
            "term": "belief propagation",
            "url": "https://en.wikipedia.org/wiki/belief_propagation"
        },
        {
            "term": "mean field theory",
            "url": "https://en.wikipedia.org/wiki/mean_field_theory"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "stochastic block model",
            "url": "https://en.wikipedia.org/wiki/stochastic_block_model"
        },
        {
            "term": "graph partitioning",
            "url": "https://en.wikipedia.org/wiki/graph_partitioning"
        },
        {
            "term": "neural network model",
            "url": "https://en.wikipedia.org/wiki/neural_network_model"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "Deep neural networks have been subject to significant attention concerning many tasks in machine learning, and a plethora of models and algorithms have been proposed in recent years",
        "While excellent performances of graph neural network have been reported in the literature, many of these results rely on experimental studies, and seem to be based on the blind belief that the nonlinear nature of graph neural network leads to such strong performances",
        "We develop a mean-field theory of graph neural network, focusing on a problem of graph partitioning",
        "In a minimal graph neural network model, the adjacency matrix A is employed for the connections between intermediate layers",
        "The minimal graph neural network that we considered in this paper is not the state of the art for the inference of the symmetric stochastic block model"
    ],
    "key_statements": [
        "Deep neural networks have been subject to significant attention concerning many tasks in machine learning, and a plethora of models and algorithms have been proposed in recent years",
        "While excellent performances of graph neural network have been reported in the literature, many of these results rely on experimental studies, and seem to be based on the blind belief that the nonlinear nature of graph neural network leads to such strong performances",
        "We develop a mean-field theory of graph neural network, focusing on a problem of graph partitioning",
        "We evaluated the performance of a graph neural network trained by backpropagation",
        "In a minimal graph neural network model, the adjacency matrix A is employed for the connections between intermediate layers",
        "The minimal graph neural network that we considered in this paper is not the state of the art for the inference of the symmetric stochastic block model"
    ],
    "summary": [
        "Deep neural networks have been subject to significant attention concerning many tasks in machine learning, and a plethora of models and algorithms have been proposed in recent years.",
        "If the fine-tuning of the model parameters via learning is crucial, the result for the untrained GNN is again useful to observe the extent to which the performance is improved.",
        "Before moving on to the mean-field theory, we first clarify the algorithmic relationship between GNN and other methods of graph partitioning.",
        "To infer the group assignments, a D \u00d7 K matrix W out of the readout classifier is applied at the end of the last layer.",
        "In the case of unsupervised learning, graph instances of a statistical model can be employed as the training set.",
        "This is a random graph model with a planted group structure, and is commonly employed as the generative",
        "By employing a classifier such as the k-means method [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], W out is not required, and the inference of the SBM can be performed without any training procedure.",
        "The consistency between our mean-field theory and a specific implementation of an untrained GNN is examined.",
        "For the SBMs with various values for the average degree c and the strength of group structure \u03b5, the overlap, i.e., the fraction of vertices that coincide with their planted labels, is calculated.",
        "Note that because even a completely random clustering can correctly infer half of the labels on average, the minimum of the overlap is 0.5.5 As mentioned above, we adopt \u03c6 = tanh as the specific choice of activation function.",
        "We evaluate the performance of the untrained GNN in which the resulting state X is read out using the k-means classifier.",
        "The Laplacians and modularity matrix, for example, have the same detectability limit when the graph is regular or the average degree is sufficiently large.",
        "It is known that the detectability limit of the BP algorithm [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] coincides with this information-theoretic limit so long as the model parameters are correctly learned.",
        "The region of \u03b5 where the overlap suddenly deteriorates still coincides with our mean-field estimate for the untrained GNN.",
        "The untrained and trained GNNs exhibit a clear difference in overlap when XT is employed as the readout classifier.",
        "It should be noted that the untrained GNN where \u03c6(XT ) is adopted as the readout classifier exhibits a performance close to that of the trained GNN.",
        "Our mean-field theory and numerical experiment using the k-means readout classifier clarify that an untrained GNN with a simple architecture already performs well."
    ],
    "headline": "We develop a mean-field theory of graph neural network, focusing on a problem of graph partitioning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In International Conference on Knowledge Discovery and Data Mining, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grover%2C%20Aditya%20Leskovec%2C%20Jure%20node2vec%3A%20Scalable%20feature%20learning%20for%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grover%2C%20Aditya%20Leskovec%2C%20Jure%20node2vec%3A%20Scalable%20feature%20learning%20for%20networks%202016"
        },
        {
            "id": "2",
            "entry": "[2] David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duvenaud%2C%20David%20K.%20Maclaurin%2C%20Dougal%20Iparraguirre%2C%20Jorge%20Bombarell%2C%20Rafael%20Convolutional%20networks%20on%20graphs%20for%20learning%20molecular%20fingerprints%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duvenaud%2C%20David%20K.%20Maclaurin%2C%20Dougal%20Iparraguirre%2C%20Jorge%20Bombarell%2C%20Rafael%20Convolutional%20networks%20on%20graphs%20for%20learning%20molecular%20fingerprints%202015"
        },
        {
            "id": "3",
            "entry": "[3] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. Trans. Neur. Netw., 20(1):61\u201380, January 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009-01"
        },
        {
            "id": "4",
            "entry": "[4] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Williams.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Williams.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        },
        {
            "id": "5",
            "entry": "[5] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.02907"
        },
        {
            "id": "6",
            "entry": "[6] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pages 2014\u20132023, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niepert%2C%20Mathias%20Ahmed%2C%20Mohamed%20Kutzkov%2C%20Konstantin%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niepert%2C%20Mathias%20Ahmed%2C%20Mohamed%20Kutzkov%2C%20Konstantin%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014"
        },
        {
            "id": "7",
            "entry": "[7] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1024\u20131034. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20Will%20Ying%2C%20Zhitao%20Leskovec%2C%20Jure%20Inductive%20representation%20learning%20on%20large%20graphs%202017"
        },
        {
            "id": "8",
            "entry": "[8] Tao Lei, Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Deriving neural architectures from sequence and graph kernels. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2024\u20132033, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Tao%20Jin%2C%20Wengong%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Deriving%20neural%20architectures%20from%20sequence%20and%20graph%20kernels%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Tao%20Jin%2C%20Wengong%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Deriving%20neural%20architectures%20from%20sequence%20and%20graph%20kernels%202017-08"
        },
        {
            "id": "9",
            "entry": "[9] Ulrike Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395\u2013416, August 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luxburg%2C%20Ulrike%20A%20tutorial%20on%20spectral%20clustering%202007-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luxburg%2C%20Ulrike%20A%20tutorial%20on%20spectral%20clustering%202007-08"
        },
        {
            "id": "10",
            "entry": "[10] M. E. J. Newman. Finding community structure in networks using the eigenvectors of matrices. Phys. Rev. E, 74:036104, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newman%2C%20M.E.J.%20Finding%20community%20structure%20in%20networks%20using%20the%20eigenvectors%20of%20matrices%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newman%2C%20M.E.J.%20Finding%20community%20structure%20in%20networks%20using%20the%20eigenvectors%20of%20matrices%202006"
        },
        {
            "id": "11",
            "entry": "[11] Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional stochastic blockmodel. The Annals of Statistics, 39(4):1878\u20131915, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rohe%2C%20Karl%20Chatterjee%2C%20Sourav%20Yu%2C%20Bin%20Spectral%20clustering%20and%20the%20high-dimensional%20stochastic%20blockmodel%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rohe%2C%20Karl%20Chatterjee%2C%20Sourav%20Yu%2C%20Bin%20Spectral%20clustering%20and%20the%20high-dimensional%20stochastic%20blockmodel%202011"
        },
        {
            "id": "12",
            "entry": "[12] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gene%20H%20Golub%20and%20Charles%20F%20Van%20Loan%20Matrix%20computations%20volume%203%20JHU%20Press%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gene%20H%20Golub%20and%20Charles%20F%20Van%20Loan%20Matrix%20computations%20volume%203%20JHU%20Press%202012"
        },
        {
            "id": "13",
            "entry": "[13] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00e1. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Decelle%2C%20Aurelien%20Krzakala%2C%20Florent%20Moore%2C%20Cristopher%20Zdeborov%C3%A1%2C%20Lenka%20Asymptotic%20analysis%20of%20the%20stochastic%20block%20model%20for%20modular%20networks%20and%20its%20algorithmic%20applications%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Decelle%2C%20Aurelien%20Krzakala%2C%20Florent%20Moore%2C%20Cristopher%20Zdeborov%C3%A1%2C%20Lenka%20Asymptotic%20analysis%20of%20the%20stochastic%20block%20model%20for%20modular%20networks%20and%20its%20algorithmic%20applications%202011"
        },
        {
            "id": "14",
            "entry": "[14] Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborov\u00e1, and Pan Zhang. Spectral redemption in clustering sparse networks. Proc. Natl. Acad. Sci. U.S.A., 110(52):20935\u201340, December 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krzakala%2C%20Florent%20Moore%2C%20Cristopher%20Mossel%2C%20Elchanan%20Neeman%2C%20Joe%20Spectral%20redemption%20in%20clustering%20sparse%20networks%202013-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krzakala%2C%20Florent%20Moore%2C%20Cristopher%20Mossel%2C%20Elchanan%20Neeman%2C%20Joe%20Spectral%20redemption%20in%20clustering%20sparse%20networks%202013-12"
        },
        {
            "id": "15",
            "entry": "[15] Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. GraphRNN: A deep generative model for graphs. arXiv preprint arXiv:1802.08773, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08773"
        },
        {
            "id": "16",
            "entry": "[16] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01212"
        },
        {
            "id": "17",
            "entry": "[17] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. arXiv preprint arXiv:1703.06103, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.06103"
        },
        {
            "id": "18",
            "entry": "[18] Tiago P. Peixoto. Efficient monte carlo and greedy heuristic for the inference of stochastic block models. Phys. Rev. E, 89:012804, Jan 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peixoto%2C%20Tiago%20P.%20Efficient%20monte%20carlo%20and%20greedy%20heuristic%20for%20the%20inference%20of%20stochastic%20block%20models%202014-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peixoto%2C%20Tiago%20P.%20Efficient%20monte%20carlo%20and%20greedy%20heuristic%20for%20the%20inference%20of%20stochastic%20block%20models%202014-01"
        },
        {
            "id": "19",
            "entry": "[19] Tiago P Peixoto. Bayesian stochastic blockmodeling. arXiv preprint arXiv:1705.10225, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10225"
        },
        {
            "id": "20",
            "entry": "[20] M. E. J. Newman and Gesine Reinert. Estimating the number of communities in a network. Phys. Rev. Lett., 117:078301, Aug 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newman%2C%20M.E.J.%20Reinert%2C%20Gesine%20Estimating%20the%20number%20of%20communities%20in%20a%20network%202016-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newman%2C%20M.E.J.%20Reinert%2C%20Gesine%20Estimating%20the%20number%20of%20communities%20in%20a%20network%202016-08"
        },
        {
            "id": "21",
            "entry": "[21] Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition model. Probability Theory and Related Fields, 162(3):431\u2013461, Aug 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mossel%2C%20Elchanan%20Neeman%2C%20Joe%20Sly%2C%20Allan%20Reconstruction%20and%20estimation%20in%20the%20planted%20partition%20model%202015-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mossel%2C%20Elchanan%20Neeman%2C%20Joe%20Sly%2C%20Allan%20Reconstruction%20and%20estimation%20in%20the%20planted%20partition%20model%202015-08"
        },
        {
            "id": "22",
            "entry": "[22] Laurent Massouli\u00e9. Community detection thresholds and the weak ramanujan property. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 694\u2013703, New York, 2014. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Massouli%C3%A9%2C%20Laurent%20Community%20detection%20thresholds%20and%20the%20weak%20ramanujan%20property%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Massouli%C3%A9%2C%20Laurent%20Community%20detection%20thresholds%20and%20the%20weak%20ramanujan%20property%202014"
        },
        {
            "id": "23",
            "entry": "[23] Cristopher Moore. The computer science and physics of community detection: landscapes, phase transitions, and hardness. arXiv preprint arXiv:1702.00467, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.00467"
        },
        {
            "id": "24",
            "entry": "[24] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3360\u20133368. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016"
        },
        {
            "id": "25",
            "entry": "[25] Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01232"
        },
        {
            "id": "26",
            "entry": "[26] H. Sompolinsky and Annette Zippelius. Relaxational dynamics of the edwards-anderson model and the mean-field theory of spin-glasses. Phys. Rev. B, 25:6860\u20136875, Jun 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sompolinsky%2C%20H.%20Zippelius%2C%20Annette%20Relaxational%20dynamics%20of%20the%20edwards-anderson%20model%20and%20the%20mean-field%20theory%20of%20spin-glasses%201982-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sompolinsky%2C%20H.%20Zippelius%2C%20Annette%20Relaxational%20dynamics%20of%20the%20edwards-anderson%20model%20and%20the%20mean-field%20theory%20of%20spin-glasses%201982-06"
        },
        {
            "id": "27",
            "entry": "[27] A. Crisanti and H. Sompolinsky. Dynamics of spin systems with randomly asymmetric bonds: Ising spins and glauber dynamics. Phys. Rev. A, 37:4865\u20134874, Jun 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crisanti%2C%20A.%20Sompolinsky%2C%20H.%20Dynamics%20of%20spin%20systems%20with%20randomly%20asymmetric%20bonds%3A%20Ising%20spins%20and%20glauber%20dynamics%201988-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crisanti%2C%20A.%20Sompolinsky%2C%20H.%20Dynamics%20of%20spin%20systems%20with%20randomly%20asymmetric%20bonds%3A%20Ising%20spins%20and%20glauber%20dynamics%201988-06"
        },
        {
            "id": "28",
            "entry": "[28] A Crisanti, HJ Sommers, and H Sompolinsky. Chaos in neural networks: chaotic solutions. preprint, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crisanti%2C%20A.%20Sommers%2C%20H.J.%20Sompolinsky%2C%20H.%20Chaos%20in%20neural%20networks%3A%20chaotic%20solutions%201990"
        },
        {
            "id": "29",
            "entry": "[29] Manfred Opper, Burak \u00c7akmak, and Ole Winther. A theory of solving tap equations for ising models with general invariant random matrices. Journal of Physics A: Mathematical and Theoretical, 49(11):114002, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opper%2C%20Manfred%20%C3%87akmak%2C%20Burak%20Winther%2C%20Ole%20A%20theory%20of%20solving%20tap%20equations%20for%20ising%20models%20with%20general%20invariant%20random%20matrices%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Opper%2C%20Manfred%20%C3%87akmak%2C%20Burak%20Winther%2C%20Ole%20A%20theory%20of%20solving%20tap%20equations%20for%20ising%20models%20with%20general%20invariant%20random%20matrices%202016"
        },
        {
            "id": "30",
            "entry": "[30] David Arthur and Sergei Vassilvitskii. K-means++: The advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201907, pages 1027\u20131035, Philadelphia, PA, USA, 2007. Society for Industrial and Applied Mathematics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arthur%2C%20David%20Vassilvitskii%2C%20Sergei%20K-means%2B%2B%3A%20The%20advantages%20of%20careful%20seeding%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arthur%2C%20David%20Vassilvitskii%2C%20Sergei%20K-means%2B%2B%3A%20The%20advantages%20of%20careful%20seeding%202007"
        },
        {
            "id": "31",
            "entry": "[31] Krzysztof Nowicki and Tom A. B Snijders. Estimation and prediction for stochastic blockstructures. Journal of the American Statistical Association, 96(455):1077\u20131087, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowicki%2C%20Krzysztof%20Snijders%2C%20Tom%20A.B.%20Estimation%20and%20prediction%20for%20stochastic%20blockstructures%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowicki%2C%20Krzysztof%20Snijders%2C%20Tom%20A.B.%20Estimation%20and%20prediction%20for%20stochastic%20blockstructures%202001"
        },
        {
            "id": "32",
            "entry": "[32] Joan Bruna and Xiang Li. Community detection with graph neural networks. arXiv preprint arXiv:1705.08415, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08415"
        },
        {
            "id": "33",
            "entry": "[33] Tatsuro Kawamoto and Yoshiyuki Kabashima. Limitations in the spectral method for graph partitioning: Detectability threshold and localization of eigenvectors. Phys. Rev. E, 91:062803, Jun 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawamoto%2C%20Tatsuro%20Kabashima%2C%20Yoshiyuki%20Limitations%20in%20the%20spectral%20method%20for%20graph%20partitioning%3A%20Detectability%20threshold%20and%20localization%20of%20eigenvectors%202015-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawamoto%2C%20Tatsuro%20Kabashima%2C%20Yoshiyuki%20Limitations%20in%20the%20spectral%20method%20for%20graph%20partitioning%3A%20Detectability%20threshold%20and%20localization%20of%20eigenvectors%202015-06"
        },
        {
            "id": "34",
            "entry": "[34] T. Kawamoto and Y. Kabashima. Detectability of the spectral method for sparse graph partitioning. Eur. Phys. Lett., 112(4):40007, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawamoto%2C%20T.%20Kabashima%2C%20Y.%20Detectability%20of%20the%20spectral%20method%20for%20sparse%20graph%20partitioning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawamoto%2C%20T.%20Kabashima%2C%20Y.%20Detectability%20of%20the%20spectral%20method%20for%20sparse%20graph%20partitioning%202015"
        },
        {
            "id": "35",
            "entry": "[35] Tatsuro Kawamoto. Algorithmic detectability threshold of the stochastic block model. Phys. Rev. E, 97:032301, Mar 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawamoto%2C%20Tatsuro%20Algorithmic%20detectability%20threshold%20of%20the%20stochastic%20block%20model%202018-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawamoto%2C%20Tatsuro%20Algorithmic%20detectability%20threshold%20of%20the%20stochastic%20block%20model%202018-03"
        },
        {
            "id": "36",
            "entry": "[36] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tokui%2C%20Seiya%20Oono%2C%20Kenta%20Hido%2C%20Shohei%20Clayton%2C%20Justin%20Chainer%3A%20a%20next-generation%20open%20source%20framework%20for%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tokui%2C%20Seiya%20Oono%2C%20Kenta%20Hido%2C%20Shohei%20Clayton%2C%20Justin%20Chainer%3A%20a%20next-generation%20open%20source%20framework%20for%20deep%20learning%202015"
        },
        {
            "id": "37",
            "entry": "[37] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "38",
            "entry": "[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "39",
            "entry": "[39] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "40",
            "entry": "[40] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6203"
        },
        {
            "id": "41",
            "entry": "[41] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3844\u20133852. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defferrard%2C%20Micha%C3%ABl%20Bresson%2C%20Xavier%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defferrard%2C%20Micha%C3%ABl%20Bresson%2C%20Xavier%20Vandergheynst%2C%20Pierre%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016"
        },
        {
            "id": "42",
            "entry": "[42] Liang Yang, Xiaochun Cao, Dongxiao He, Chuan Wang, Xiao Wang, and Weixiong Zhang. Modularity based community detection with deep learning. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI\u201916, pages 2252\u20132258. AAAI Press, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Liang%20Cao%2C%20Xiaochun%20Dongxiao%20He%2C%20Chuan%20Wang%2C%20Xiao%20Wang%2C%20and%20Weixiong%20Zhang.%20Modularity%20based%20community%20detection%20with%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Liang%20Cao%2C%20Xiaochun%20Dongxiao%20He%2C%20Chuan%20Wang%2C%20Xiao%20Wang%2C%20and%20Weixiong%20Zhang.%20Modularity%20based%20community%20detection%20with%20deep%20learning%202016"
        }
    ]
}
