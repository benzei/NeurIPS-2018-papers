{
    "filename": "7742-efficient-neural-network-robustness-certification-with-general-activation-functions.pdf",
    "metadata": {
        "title": "Efficient Neural Network Robustness Certification with General Activation Functions",
        "author": "Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7742-efficient-neural-network-robustness-certification-with-general-activation-functions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a nontrivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan."
    },
    "keywords": [
        {
            "term": "activation function",
            "url": "https://en.wikipedia.org/wiki/activation_function"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "data point",
            "url": "https://en.wikipedia.org/wiki/data_point"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "mixed integer linear programming",
            "url": "https://en.wikipedia.org/wiki/mixed_integer_linear_programming"
        },
        {
            "term": "CROWN",
            "url": "https://en.wikipedia.org/wiki/Crown"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        }
    ],
    "highlights": [
        "For Rectified Linear Unit networks, finding the minimum adversarial distortion for a given input data point x0 can be cast as a mixed integer linear programming (MILP) problem [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We propose a generic analysis framework CROWN for certifying neural networks using linear or quadratic upper and lower bounds for general activation functions that are not necessarily piece-wise linear",
        "Figure 2 shows the certified lower bound for 2 and \u221e distortions found by different algorithms on small networks, where Reluplex is feasible and we can observe the gap between different certified lower bounds and the true minimum adversarial distortion",
        "Improvements of CROWN-Ada over Fast-Lin are more significant in larger neural networks, as we show below",
        "Table 7 compares the certified lower bound computed by CROWNgeneral for four activation functions and different p norm on large networks",
        "We have presented a general framework CROWN to efficiently compute a certified lower bound of minimum distortion in neural networks for any given data point x0"
    ],
    "key_statements": [
        "For Rectified Linear Unit networks, finding the minimum adversarial distortion for a given input data point x0 can be cast as a mixed integer linear programming (MILP) problem [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We propose a generic analysis framework CROWN for certifying neural networks using linear or quadratic upper and lower bounds for general activation functions that are not necessarily piece-wise linear",
        "In Section 3.2, we demonstrate how to provide robustness certification for four widely-used activation functions (ReLU, tanh, sigmoid and arctan) using CROWN",
        "We evaluate CROWN and other baselines on multi-layer perceptron (MLP) models trained on MNIST and CIFAR-10 datasets",
        "Figure 2 shows the certified lower bound for 2 and \u221e distortions found by different algorithms on small networks, where Reluplex is feasible and we can observe the gap between different certified lower bounds and the true minimum adversarial distortion",
        "Improvements of CROWN-Ada over Fast-Lin are more significant in larger neural networks, as we show below",
        "Table 7 compares the certified lower bound computed by CROWNgeneral for four activation functions and different p norm on large networks",
        "We have presented a general framework CROWN to efficiently compute a certified lower bound of minimum distortion in neural networks for any given data point x0"
    ],
    "summary": [
        "For ReLU networks, finding the minimum adversarial distortion for a given input data point x0 can be cast as a mixed integer linear programming (MILP) problem [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>].",
        "[<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] use the idea of a convex outer adversarial polytope in ReLU networks to compute a certified lower bound by relaxing the MILP certification problem to linear programing (LP).",
        "[<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] exploit the ReLU property to bound the activation function and provide efficient algorithms (Fast-Lin and Fast-Lip) for computing a certified lower bound, achieving state-of-the-art performance.",
        "We present a general framework CROWN for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point x0 with general activation functions in larger NNs. We first provide principles in Section 3.1 to derive output bounds of NNs when the inputs are perturbed within an p ball and each neuron has different linear approximation bounds on its activation function.",
        "We note there are multiple ways of segmenting the activation functions and defining the partitioned sets, u rather than their signs), and we can incorporate this into our framework to provide the corresponding explicit output bounds for the new partition sets.",
        "Following the procedure of unwrapping the activation functions at the layer m \u2212 1, we show in Appendix D that the output upper bound and lower bound with quadratic approximations are: fjU (x) = \u03a6m\u22122(x) QU(m\u22121)\u03a6m\u22122(x) + 2p(Um\u22121)\u03a6m\u22122(x) + sU(m\u22121), (3)",
        "By properly choosing the quadratic bounds, we can make the problem maxx\u2208Bp(x0, ) fjU (x) into a convex Quadratic Programming problem; for example, we can let \u03b7U(m,i\u22121) = 0 for all Wj(,mi ) > 0 and let \u03b7L(m,i\u22121) > 0 to make D(Um\u22121) have only negative and zero diagonals for the maximization problem \u2013 this is equivalent to applying a linear upper bound and a quadratic lower bound to bound the activation function.",
        "Table 7 compares the certified lower bound computed by CROWNgeneral for four activation functions and different p norm on large networks.",
        "CROWN-general is able to certify non-trivial lower bounds for all four activation functions efficiently.",
        "Comparing to CROWN-Ada on ReLU networks, certifying general activations that are not piece-wise linear only incurs about 20% additional computational overhead.",
        "We have presented a general framework CROWN to efficiently compute a certified lower bound of minimum distortion in neural networks for any given data point x0.",
        "Experiments show that (1) CROWN outperforms state-of-the-art baselines on ReLU networks and (2) CROWN can efficiently certify non-trivial lower bounds for large networks with over 10K neurons and with different activation functions."
    ],
    "headline": "In this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Fawzi, S.-M. Moosavi-Dezfooli, and P. Frossard, \u201cThe robustness of deep networks: A geometrical perspective,\u201d IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 50\u201362, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fawzi%2C%20A.%20Moosavi-Dezfooli%2C%20S.-M.%20Frossard%2C%20P.%20The%20robustness%20of%20deep%20networks%3A%20A%20geometrical%20perspective%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fawzi%2C%20A.%20Moosavi-Dezfooli%2C%20S.-M.%20Frossard%2C%20P.%20The%20robustness%20of%20deep%20networks%3A%20A%20geometrical%20perspective%2C%202017"
        },
        {
            "id": "2",
            "entry": "[2] B. Biggio and F. Roli, \u201cWild patterns: Ten years after the rise of adversarial machine learning,\u201d arXiv preprint arXiv:1712.03141, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03141"
        },
        {
            "id": "3",
            "entry": "[3] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "4",
            "entry": "[4] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20harnessing%20adversarial%20examples%2C%202015"
        },
        {
            "id": "5",
            "entry": "[5] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, \u201cDeepfool: a simple and accurate method to fool deep neural networks,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2574\u20132582.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.-M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20S.-M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%2C%202016"
        },
        {
            "id": "6",
            "entry": "[6] N. Carlini and D. Wagner, \u201cTowards evaluating the robustness of neural networks,\u201d in IEEE Symposium on Security and Privacy (SP), 2017, pp. 39\u201357.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%2C%202017"
        },
        {
            "id": "7",
            "entry": "[7] H. Chen, H. Zhang, P.-Y. Chen, J. Yi, and C.-J. Hsieh, \u201cShow-and-fool: Crafting adversarial examples for neural image captioning,\u201d arXiv preprint arXiv:1712.02051, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02051"
        },
        {
            "id": "8",
            "entry": "[8] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, \u201cPractical black-box attacks against machine learning,\u201d in ACM Asia Conference on Computer and Communications Security, 2017, pp. 506\u2013519.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Goodfellow%2C%20I.%20Jha%2C%20S.%20Practical%20black-box%20attacks%20against%20machine%20learning%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Goodfellow%2C%20I.%20Jha%2C%20S.%20Practical%20black-box%20attacks%20against%20machine%20learning%2C%202017"
        },
        {
            "id": "9",
            "entry": "[9] Y. Liu, X. Chen, C. Liu, and D. Song, \u201cDelving into transferable adversarial examples and black-box attacks,\u201d ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Y.%20Chen%2C%20X.%20Liu%2C%20C.%20Song%2C%20D.%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Y.%20Chen%2C%20X.%20Liu%2C%20C.%20Song%2C%20D.%20Delving%20into%20transferable%20adversarial%20examples%20and%20black-box%20attacks%2C%202017"
        },
        {
            "id": "10",
            "entry": "[10] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, \u201cZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models,\u201d in ACM Workshop on Artificial Intelligence and Security, 2017, pp. 15\u201326.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20P.-Y.%20Zhang%2C%20H.%20Sharma%2C%20Y.%20Yi%2C%20J.%20ZOO%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20P.-Y.%20Zhang%2C%20H.%20Sharma%2C%20Y.%20Yi%2C%20J.%20ZOO%3A%20Zeroth%20order%20optimization%20based%20black-box%20attacks%20to%20deep%20neural%20networks%20without%20training%20substitute%20models%2C%202017"
        },
        {
            "id": "11",
            "entry": "[11] W. Brendel, J. Rauber, and M. Bethge, \u201cDecision-based adversarial attacks: Reliable attacks against black-box machine learning models,\u201d ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brendel%2C%20W.%20Rauber%2C%20J.%20Bethge%2C%20M.%20Decision-based%20adversarial%20attacks%3A%20Reliable%20attacks%20against%20black-box%20machine%20learning%20models%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brendel%2C%20W.%20Rauber%2C%20J.%20Bethge%2C%20M.%20Decision-based%20adversarial%20attacks%3A%20Reliable%20attacks%20against%20black-box%20machine%20learning%20models%2C%202018"
        },
        {
            "id": "12",
            "entry": "[12] A. Kurakin, I. Goodfellow, and S. Bengio, \u201cAdversarial examples in the physical world,\u201d arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "13",
            "entry": "[13] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song, \u201cRobust physical-world attacks on machine learning models,\u201d arXiv preprint arXiv:1707.08945, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08945"
        },
        {
            "id": "14",
            "entry": "[14] A. Athalye and I. Sutskever, \u201cSynthesizing robust adversarial examples,\u201d arXiv preprint arXiv:1707.07397, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "15",
            "entry": "[15] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, \u201cReluplex: An efficient smt solver for verifying deep neural networks,\u201d in International Conference on Computer Aided Verification. Springer, 2017, pp. 97\u2013117.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katz%2C%20G.%20Barrett%2C%20C.%20Dill%2C%20D.L.%20Julian%2C%20K.%20Reluplex%3A%20An%20efficient%20smt%20solver%20for%20verifying%20deep%20neural%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katz%2C%20G.%20Barrett%2C%20C.%20Dill%2C%20D.L.%20Julian%2C%20K.%20Reluplex%3A%20An%20efficient%20smt%20solver%20for%20verifying%20deep%20neural%20networks%2C%202017"
        },
        {
            "id": "16",
            "entry": "[16] A. Sinha, H. Namkoong, and J. Duchi, \u201cCertifiable distributional robustness with principled adversarial training,\u201d ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20A.%20Namkoong%2C%20H.%20Duchi%2C%20J.%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20A.%20Namkoong%2C%20H.%20Duchi%2C%20J.%20Certifiable%20distributional%20robustness%20with%20principled%20adversarial%20training%2C%202018"
        },
        {
            "id": "17",
            "entry": "[17] J. Peck, J. Roels, B. Goossens, and Y. Saeys, \u201cLower bounds on the robustness to adversarial perturbations,\u201d in NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peck%2C%20J.%20Roels%2C%20J.%20Goossens%2C%20B.%20Saeys%2C%20Y.%20%E2%80%9CLower%20bounds%20on%20the%20robustness%20to%20adversarial%20perturbations%2C%E2%80%9D%20in%20NIPS%202017"
        },
        {
            "id": "18",
            "entry": "[18] J. Z. Kolter and E. Wong, \u201cProvable defenses against adversarial examples via the convex outer adversarial polytope,\u201d ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolter%2C%20J.Z.%20Wong%2C%20E.%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolter%2C%20J.Z.%20Wong%2C%20E.%20Provable%20defenses%20against%20adversarial%20examples%20via%20the%20convex%20outer%20adversarial%20polytope%2C%202018"
        },
        {
            "id": "19",
            "entry": "[19] A. Raghunathan, J. Steinhardt, and P. Liang, \u201cCertified defenses against adversarial examples,\u201d ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghunathan%2C%20A.%20Steinhardt%2C%20J.%20Liang%2C%20P.%20Certified%20defenses%20against%20adversarial%20examples%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghunathan%2C%20A.%20Steinhardt%2C%20J.%20Liang%2C%20P.%20Certified%20defenses%20against%20adversarial%20examples%2C%202018"
        },
        {
            "id": "20",
            "entry": "[20] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel, \u201cTowards fast computation of certified robustness for relu networks,\u201d ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20T.-W.%20Zhang%2C%20H.%20Chen%2C%20H.%20Song%2C%20Z.%20Towards%20fast%20computation%20of%20certified%20robustness%20for%20relu%20networks%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20T.-W.%20Zhang%2C%20H.%20Chen%2C%20H.%20Song%2C%20Z.%20Towards%20fast%20computation%20of%20certified%20robustness%20for%20relu%20networks%2C%202018"
        },
        {
            "id": "21",
            "entry": "[21] A. Lomuscio and L. Maganti, \u201cAn approach to reachability analysis for feed-forward relu neural networks,\u201d arXiv preprint arXiv:1706.07351, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.07351"
        },
        {
            "id": "22",
            "entry": "[22] C.-H. Cheng, G. N\u00fchrenberg, and H. Ruess, \u201cMaximum resilience of artificial neural networks,\u201d in International Symposium on Automated Technology for Verification and Analysis. Springer, 2017, pp. 251\u2013268.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20C.-H.%20N%C3%BChrenberg%2C%20G.%20Ruess%2C%20H.%20%E2%80%9CMaximum%20resilience%20of%20artificial%20neural%20networks%2C%E2%80%9D%20in%20International%20Symposium%20on%20Automated%20Technology%20for%20Verification%20and%20Analysis%202017"
        },
        {
            "id": "23",
            "entry": "[23] M. Fischetti and J. Jo, \u201cDeep neural networks as 0-1 mixed integer linear programs: A feasibility study,\u201d arXiv preprint arXiv:1712.06174, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06174"
        },
        {
            "id": "24",
            "entry": "[24] N. Carlini, G. Katz, C. Barrett, and D. L. Dill, \u201cProvably minimally-distorted adversarial examples,\u201d arXiv preprint arXiv:1709.10207, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10207"
        },
        {
            "id": "25",
            "entry": "[25] R. Ehlers, \u201cFormal verification of piece-wise linear feed-forward neural networks,\u201d in International Symposium on Automated Technology for Verification and Analysis. Springer, 2017, pp. 269\u2013286.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ehlers%2C%20R.%20%E2%80%9CFormal%20verification%20of%20piece-wise%20linear%20feed-forward%20neural%20networks%2C%E2%80%9D%20in%20International%20Symposium%20on%20Automated%20Technology%20for%20Verification%20and%20Analysis%202017"
        },
        {
            "id": "26",
            "entry": "[26] M. Hein and M. Andriushchenko, \u201cFormal guarantees on the robustness of a classifier against adversarial manipulation,\u201d in NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20M.%20Andriushchenko%2C%20M.%20%E2%80%9CFormal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%2C%E2%80%9D%20in%20NIPS%202017"
        },
        {
            "id": "27",
            "entry": "[27] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh, and L. Daniel, \u201cEvaluating the robustness of neural networks: An extreme value theory approach,\u201d ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20T.-W.%20Zhang%2C%20H.%20Chen%2C%20P.-Y.%20Yi%2C%20J.%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%20extreme%20value%20theory%20approach%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20T.-W.%20Zhang%2C%20H.%20Chen%2C%20P.-Y.%20Yi%2C%20J.%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%20extreme%20value%20theory%20approach%2C%202018"
        },
        {
            "id": "28",
            "entry": "[28] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, \u201cAi2: Safety and robustness certification of neural networks with abstract interpretation,\u201d in IEEE Symposium on Security and Privacy (SP), vol. 00, 2018, pp. 948\u2013963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehr%2C%20T.%20Mirman%2C%20M.%20Drachsler-Cohen%2C%20D.%20Tsankov%2C%20P.%20Ai2%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehr%2C%20T.%20Mirman%2C%20M.%20Drachsler-Cohen%2C%20D.%20Tsankov%2C%20P.%20Ai2%3A%20Safety%20and%20robustness%20certification%20of%20neural%20networks%20with%20abstract%20interpretation%2C%202018"
        },
        {
            "id": "29",
            "entry": "[29] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, \u201cA dual approach to scalable verification of deep networks,\u201d UAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dvijotham%2C%20K.%20Stanforth%2C%20R.%20Gowal%2C%20S.%20Mann%2C%20T.%20A%20dual%20approach%20to%20scalable%20verification%20of%20deep%20networks%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dvijotham%2C%20K.%20Stanforth%2C%20R.%20Gowal%2C%20S.%20Mann%2C%20T.%20A%20dual%20approach%20to%20scalable%20verification%20of%20deep%20networks%2C%202018"
        },
        {
            "id": "30",
            "entry": "[30] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20deep%20learning%20models%20resistant%20to%20adversarial%20attacks%2C%202018"
        },
        {
            "id": "31",
            "entry": "[31] X. Cao and N. Z. Gong, \u201cMitigating evasion attacks to deep neural networks via region-based classification,\u201d in ACM Annual Computer Security Applications Conference, 2017, pp. 278\u2013287.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20X.%20Gong%2C%20N.Z.%20Mitigating%20evasion%20attacks%20to%20deep%20neural%20networks%20via%20region-based%20classification%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20X.%20Gong%2C%20N.Z.%20Mitigating%20evasion%20attacks%20to%20deep%20neural%20networks%20via%20region-based%20classification%2C%202017"
        },
        {
            "id": "32",
            "entry": "[32] P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh, \u201cEAD: elastic-net attacks to deep neural networks via adversarial examples,\u201d AAAI, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20P.-Y.%20Sharma%2C%20Y.%20Zhang%2C%20H.%20Yi%2C%20J.%20EAD%3A%20elastic-net%20attacks%20to%20deep%20neural%20networks%20via%20adversarial%20examples%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20P.-Y.%20Sharma%2C%20Y.%20Zhang%2C%20H.%20Yi%2C%20J.%20EAD%3A%20elastic-net%20attacks%20to%20deep%20neural%20networks%20via%20adversarial%20examples%2C%202018"
        }
    ]
}
