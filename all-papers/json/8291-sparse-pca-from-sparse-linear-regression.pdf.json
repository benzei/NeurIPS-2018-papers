{
    "filename": "8291-sparse-pca-from-sparse-linear-regression.pdf",
    "metadata": {
        "title": "Sparse PCA from Sparse Linear Regression",
        "author": "Guy Bresler, Sung Min Park, Madalina Persu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8291-sparse-pca-from-sparse-linear-regression.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but an explicit connection between the two had not been made. We show how to efficiently transform a black-box solver for SLR into an algorithm for SPCA: assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the SPCA algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomialtime algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. We provide experimental results on simulated data comparing our proposed framework to other algorithms for SPCA."
    },
    "keywords": [
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        },
        {
            "term": "Principal component analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_component_analysis"
        },
        {
            "term": "linear model",
            "url": "https://en.wikipedia.org/wiki/linear_model"
        }
    ],
    "highlights": [
        "We give a simple, general, and efficient procedure for transforming a black-box solver for sparse linear regression to an algorithm for sparse principal component analysis",
        "We study sparse principal component analysis under the Gaussian spiked covariance model introduced by [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]: we observe n samples of a random variable X distributed according to a Gaussian distribution N (0, Id + \u2713uu>), where ||u||2 = 1 with at most k nonzero entries,2 Id is the d \u21e5 d identity matrix, and \u2713 is the signal-to-noise parameter",
        "We observe a response vector y 2 Rn and a design matrix X 2 Rn\u21e5d that are linked by the linear model y = X \u21e4 + w, where w 2 Rn is some form of observation noise, typically with i.i.d",
        "We focus on the restricted eigenvalue condition, which roughly stated makes the prediction loss strongly convex near the optimum: Definition 2.1 (Restricted eigenvalue [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>])",
        "Regression based approaches To the best of our knowledge, our work is the first to give a general framework for sparse principal component analysis that uses Sparse Linear Regression in a black-box fashion. [<a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>] uses specific algorithms for Sparse Linear Regression such as Lasso as a subroutine, but they use a heuristic alternating minimization procedure to solve a non-convex problem, and lack any theoretical guarantees. [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] applies a regression based approach to a restricted class of graphical models",
        "The work in this paper remains limited to the Gaussian setting and to the single spiked covariance model"
    ],
    "key_statements": [
        "We give a simple, general, and efficient procedure for transforming a black-box solver for sparse linear regression to an algorithm for sparse principal component analysis",
        "We study sparse principal component analysis under the Gaussian spiked covariance model introduced by [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]: we observe n samples of a random variable X distributed according to a Gaussian distribution N (0, Id + \u2713uu>), where ||u||2 = 1 with at most k nonzero entries,2 Id is the d \u21e5 d identity matrix, and \u2713 is the signal-to-noise parameter",
        "We demonstrate that using popular existing Sparse Linear Regression algorithms as our black-box results in reasonable performance",
        "Our experimental results indicate that with an appropriate choice of black-box, our Q algorithm outperforms covariance thresholding 5Solving sparse principal component analysis based on the correlation matrix was suggested in a few earlier works [<a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]",
        "We observe a response vector y 2 Rn and a design matrix X 2 Rn\u21e5d that are linked by the linear model y = X \u21e4 + w, where w 2 Rn is some form of observation noise, typically with i.i.d",
        "We focus on the restricted eigenvalue condition, which roughly stated makes the prediction loss strongly convex near the optimum: Definition 2.1 (Restricted eigenvalue [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>])",
        "We obviously do not have access to \u21e4, so we must use the estimate b = Sparse Linear Regression(y, X, k) (y, X are as defined in Section 3.1) which we get from our black-box. This substitution does not affect much of the discriminative power of Qi as long as the Sparse Linear Regression black-box satisfies prediction error guarantees stated in Condition 2.2",
        "Regression based approaches To the best of our knowledge, our work is the first to give a general framework for sparse principal component analysis that uses Sparse Linear Regression in a black-box fashion. [<a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>] uses specific algorithms for Sparse Linear Regression such as Lasso as a subroutine, but they use a heuristic alternating minimization procedure to solve a non-convex problem, and lack any theoretical guarantees. [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] applies a regression based approach to a restricted class of graphical models",
        "Their iterative algorithm depends crucially on a good initialization done by a diagonal thresholding-like pre-processing step, which fails under rescaling of the data.13",
        "Several directions are open for future work",
        "The work in this paper remains limited to the Gaussian setting and to the single spiked covariance model"
    ],
    "summary": [
        "We give a simple, general, and efficient procedure for transforming a black-box solver for sparse linear regression to an algorithm for SPCA.",
        "Our experimental results indicate that with an appropriate choice of black-box, our Q algorithm outperforms covariance thresholding 5Solving SPCA based on the correlation matrix was suggested in a few earlier works [<a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>].",
        "This substitution does not affect much of the discriminative power of Qi as long as the SLR black-box satisfies prediction error guarantees stated in Condition 2.2.",
        "We give algorithms for hypothesis testing and for support recovery, based on the Q statistic: 6By the theory of linear minimum mean-square-error (LMMSE) confirms that this choice of \u21e4 minimizes the error 2.",
        "RE for sample design matrix Because population covariance \u2303 = E[XX>] has minimum eigenvalue 1, with high probability the sample design matrix X has constant restricted eigenvalue value given enough samples, i.e. n is large enough, and the prediction error guarantee of Condition 2.2 will be good enough for our analysis.",
        "Hypothesis testing We generate data under two different distributions: for the spiked covariance model, we generate a spike u by sampling a uniformly random direction from the k-dimensional unit sphere, and embedding the vector at a random subset of k coordinates among d coordinates; for the null, we draw from standard isotropic Gaussian.",
        "Regression based approaches To the best of our knowledge, our work is the first to give a general framework for SPCA that uses SLR in a black-box fashion.",
        "Their algorithm requires extraneous conditions on the data.[<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] uses a reduction to linear regression for their problem of sparse subspace estimation.",
        "Their iterative algorithm depends crucially on a good initialization done by a diagonal thresholding-like pre-processing step, which fails under rescaling of the data.13 their framework uses regression for the specific case of orthogonal design, whereas our design matrix can be more general as long as it satisfies a condition similar to the restricted eigenvalue condition.",
        "Provable guarantees for the EM algorithm and variational methods are lacking in general, and it is not immediately obvious what signal threshold their algorithm achieves for the single spike covariance model.",
        "We gave a black-box reduction for reducing instances of the SPCA problem under the spiked covariance model to instances of SLR.",
        "Given oracle access to SLR black-box meeting a certain natural condition, the reduction is shown to efficiently solve hypothesis testing and support recovery.",
        "There is certainly room for improvement by tuning the choice of the SLR black-box to make the algorithm more efficient for use in practice"
    ],
    "headline": "We show how to efficiently transform a black-box solver for Sparse Linear Regression into an algorithm for sparse principal component analysis: assuming the Sparse Linear Regression solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the sparse principal component analysis algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomialtime algorithms",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Arash A Amini and Martin J Wainwright. High-dimensional analysis of semidefinite relaxations for sparse principal components. In Information Theory, 2008. ISIT 2008. IEEE International Symposium on, pages 2454\u20132458. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amini%2C%20Arash%20A.%20Wainwright%2C%20Martin%20J.%20High-dimensional%20analysis%20of%20semidefinite%20relaxations%20for%20sparse%20principal%20components%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amini%2C%20Arash%20A.%20Wainwright%2C%20Martin%20J.%20High-dimensional%20analysis%20of%20semidefinite%20relaxations%20for%20sparse%20principal%20components%202008"
        },
        {
            "id": "2",
            "entry": "[2] Afonso S Bandeira, Edgar Dobriban, Dustin G Mixon, and William F Sawin. Certifying the restricted isometry property is hard. IEEE transactions on information theory, 59(6):3448\u20133450, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bandeira%2C%20Afonso%20S.%20Dobriban%2C%20Edgar%20Mixon%2C%20Dustin%20G.%20Sawin%2C%20William%20F.%20Certifying%20the%20restricted%20isometry%20property%20is%20hard%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bandeira%2C%20Afonso%20S.%20Dobriban%2C%20Edgar%20Mixon%2C%20Dustin%20G.%20Sawin%2C%20William%20F.%20Certifying%20the%20restricted%20isometry%20property%20is%20hard%202013"
        },
        {
            "id": "3",
            "entry": "[3] Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In Conference on Learning Theory, pages 1046\u20131066, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berthet%2C%20Quentin%20Rigollet%2C%20Philippe%20Complexity%20theoretic%20lower%20bounds%20for%20sparse%20principal%20component%20detection%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berthet%2C%20Quentin%20Rigollet%2C%20Philippe%20Complexity%20theoretic%20lower%20bounds%20for%20sparse%20principal%20component%20detection%202013"
        },
        {
            "id": "4",
            "entry": "[4] Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high dimension. The Annals of Statistics, 41(4):1780\u20131815, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berthet%2C%20Quentin%20Rigollet%2C%20Philippe%20Optimal%20detection%20of%20sparse%20principal%20components%20in%20high%20dimension%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Berthet%2C%20Quentin%20Rigollet%2C%20Philippe%20Optimal%20detection%20of%20sparse%20principal%20components%20in%20high%20dimension%202013"
        },
        {
            "id": "5",
            "entry": "[5] Peter J Bickel, Ya\u2019acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, pages 1705\u20131732, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bickel%2C%20Peter%20J.%20Ritov%2C%20Ya%E2%80%99acov%20Tsybakov%2C%20Alexandre%20B.%20Simultaneous%20analysis%20of%20lasso%20and%20dantzig%20selector%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bickel%2C%20Peter%20J.%20Ritov%2C%20Ya%E2%80%99acov%20Tsybakov%2C%20Alexandre%20B.%20Simultaneous%20analysis%20of%20lasso%20and%20dantzig%20selector%202009"
        },
        {
            "id": "6",
            "entry": "[6] Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3):265\u2013274, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blumensath%2C%20Thomas%20Davies%2C%20Mike%20E.%20Iterative%20hard%20thresholding%20for%20compressed%20sensing%202009"
        },
        {
            "id": "7",
            "entry": "[7] Florentina Bunea, Alexandre B Tsybakov, and Marten H Wegkamp. Aggregation for gaussian regression. The Annals of Statistics, 35(4):1674\u20131697, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bunea%2C%20Florentina%20Tsybakov%2C%20Alexandre%20B.%20Wegkamp%2C%20Marten%20H.%20Aggregation%20for%20gaussian%20regression%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bunea%2C%20Florentina%20Tsybakov%2C%20Alexandre%20B.%20Wegkamp%2C%20Marten%20H.%20Aggregation%20for%20gaussian%20regression%202007"
        },
        {
            "id": "8",
            "entry": "[8] Florentina Bunea, Alexandre B Tsybakov, and Marten H Wegkamp. Sparse density estimation with1 penalties. In Learning theory, pages 530\u2013543.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bunea%2C%20Florentina%20Tsybakov%2C%20Alexandre%20B.%20Wegkamp%2C%20Marten%20H.%20Sparse%20density%20estimation%20with1%20penalties",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bunea%2C%20Florentina%20Tsybakov%2C%20Alexandre%20B.%20Wegkamp%2C%20Marten%20H.%20Sparse%20density%20estimation%20with1%20penalties"
        },
        {
            "id": "9",
            "entry": "[9] T Tony Cai, Zongming Ma, and Yihong Wu. Sparse pca: Optimal rates and adaptive estimation. The Annals of Statistics, 41(6):3074\u20133110, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20T.Tony%20Ma%2C%20Zongming%20Wu%2C%20Yihong%20Sparse%20pca%3A%20Optimal%20rates%20and%20adaptive%20estimation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20T.Tony%20Ma%2C%20Zongming%20Wu%2C%20Yihong%20Sparse%20pca%3A%20Optimal%20rates%20and%20adaptive%20estimation%202013"
        },
        {
            "id": "10",
            "entry": "[10] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, pages 2313\u20132351, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20Tao%2C%20Terence%20The%20dantzig%20selector%3A%20statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20Tao%2C%20Terence%20The%20dantzig%20selector%3A%20statistical%20estimation%20when%20p%20is%20much%20larger%20than%20n%202007"
        },
        {
            "id": "11",
            "entry": "[11] Emmanuel J Candes and Terence Tao. Decoding by linear programming. Information Theory, IEEE Transactions on, 51(12):4203\u20134215, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Decoding%20by%20linear%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Decoding%20by%20linear%20programming%202005"
        },
        {
            "id": "12",
            "entry": "[12] Dong Dai, Philippe Rigollet, Lucy Xia, and Tong Zhang. Aggregation of affine estimators. Electronic Journal of Statistics, 8(1):302\u2013327, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Dong%20Rigollet%2C%20Philippe%20Xia%2C%20Lucy%20Zhang%2C%20Tong%20Aggregation%20of%20affine%20estimators%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Dong%20Rigollet%2C%20Philippe%20Xia%2C%20Lucy%20Zhang%2C%20Tong%20Aggregation%20of%20affine%20estimators%202014"
        },
        {
            "id": "13",
            "entry": "[13] Alexandre d\u2019Aspremont, Laurent El Ghaoui, Michael I Jordan, and Gert RG Lanckriet. A direct formulation for sparse pca using semidefinite programming. SIAM review, 49(3):434\u2013448, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d%E2%80%99Aspremont%2C%20Alexandre%20Ghaoui%2C%20Laurent%20El%20Jordan%2C%20Michael%20I.%20and%20Gert%20RG%20Lanckriet.%20A%20direct%20formulation%20for%20sparse%20pca%20using%20semidefinite%20programming%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=d%E2%80%99Aspremont%2C%20Alexandre%20Ghaoui%2C%20Laurent%20El%20Jordan%2C%20Michael%20I.%20and%20Gert%20RG%20Lanckriet.%20A%20direct%20formulation%20for%20sparse%20pca%20using%20semidefinite%20programming%202007"
        },
        {
            "id": "14",
            "entry": "[14] Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. In Advances in Neural Information Processing Systems, pages 334\u2013342, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Sparse%20pca%20via%20covariance%20thresholding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deshpande%2C%20Yash%20Montanari%2C%20Andrea%20Sparse%20pca%20via%20covariance%20thresholding%202014"
        },
        {
            "id": "15",
            "entry": "[15] Thong T Do, Lu Gan, Nam Nguyen, and Trac D Tran. Sparsity adaptive matching pursuit algorithm for practical compressed sensing. In Signals, Systems and Computers, 2008 42nd Asilomar Conference on, pages 581\u2013587. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Do%2C%20Thong%20T.%20Gan%2C%20Lu%20Nguyen%2C%20Nam%20Tran%2C%20Trac%20D.%20Sparsity%20adaptive%20matching%20pursuit%20algorithm%20for%20practical%20compressed%20sensing%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Do%2C%20Thong%20T.%20Gan%2C%20Lu%20Nguyen%2C%20Nam%20Tran%2C%20Trac%20D.%20Sparsity%20adaptive%20matching%20pursuit%20algorithm%20for%20practical%20compressed%20sensing%202008"
        },
        {
            "id": "16",
            "entry": "[16] Alexandre d\u2019Aspremont, Francis Bach, and Laurent El Ghaoui. Approximation bounds for sparse principal component analysis. Mathematical Programming, 148(1-2):89\u2013110, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d%E2%80%99Aspremont%2C%20Alexandre%20Bach%2C%20Francis%20Ghaoui%2C%20Laurent%20El%20Approximation%20bounds%20for%20sparse%20principal%20component%20analysis%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=d%E2%80%99Aspremont%2C%20Alexandre%20Bach%2C%20Francis%20Ghaoui%2C%20Laurent%20El%20Approximation%20bounds%20for%20sparse%20principal%20component%20analysis%202014"
        },
        {
            "id": "17",
            "entry": "[17] Alyson K Fletcher, Sundeep Rangan, and Vivek K Goyal. Necessary and sufficient conditions for sparsity pattern recovery. IEEE Transactions on Information Theory, 55(12):5758\u20135772, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fletcher%2C%20Alyson%20K.%20Rangan%2C%20Sundeep%20Goyal%2C%20Vivek%20K.%20Necessary%20and%20sufficient%20conditions%20for%20sparsity%20pattern%20recovery%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fletcher%2C%20Alyson%20K.%20Rangan%2C%20Sundeep%20Goyal%2C%20Vivek%20K.%20Necessary%20and%20sufficient%20conditions%20for%20sparsity%20pattern%20recovery%202009"
        },
        {
            "id": "18",
            "entry": "[18] Milana Gataric, Tengyao Wang, and Richard J Samworth. Sparse principal component analysis via random projections. arXiv preprint arXiv:1712.05630, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05630"
        },
        {
            "id": "19",
            "entry": "[19] Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis. Annals of statistics, pages 295\u2013327, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnstone%2C%20Iain%20M.%20On%20the%20distribution%20of%20the%20largest%20eigenvalue%20in%20principal%20components%20analysis%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnstone%2C%20Iain%20M.%20On%20the%20distribution%20of%20the%20largest%20eigenvalue%20in%20principal%20components%20analysis%202001"
        },
        {
            "id": "20",
            "entry": "[20] Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnstone%2C%20Iain%20M.%20Lu%2C%20Arthur%20Yu%20On%20consistency%20and%20sparsity%20for%20principal%20components%20analysis%20in%20high%20dimensions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnstone%2C%20Iain%20M.%20Lu%2C%20Arthur%20Yu%20On%20consistency%20and%20sparsity%20for%20principal%20components%20analysis%20in%20high%20dimensions%202009"
        },
        {
            "id": "21",
            "entry": "[21] Ian T Jolliffe, Nickolay T Trendafilov, and Mudassir Uddin. A modified principal component technique based on the lasso. Journal of computational and Graphical Statistics, 12(3):531\u2013547, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jolliffe%2C%20Ian%20T.%20Trendafilov%2C%20Nickolay%20T.%20Uddin%2C%20Mudassir%20A%20modified%20principal%20component%20technique%20based%20on%20the%20lasso%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jolliffe%2C%20Ian%20T.%20Trendafilov%2C%20Nickolay%20T.%20Uddin%2C%20Mudassir%20A%20modified%20principal%20component%20technique%20based%20on%20the%20lasso%202003"
        },
        {
            "id": "22",
            "entry": "[22] Michel Journ\u00e9e, Yurii Nesterov, Peter Richt\u00e1rik, and Rodolphe Sepulchre. Generalized power method for sparse principal component analysis. Journal of Machine Learning Research, 11(Feb):517\u2013553, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Journ%C3%A9e%2C%20Michel%20Nesterov%2C%20Yurii%20Richt%C3%A1rik%2C%20Peter%20Sepulchre%2C%20Rodolphe%20Generalized%20power%20method%20for%20sparse%20principal%20component%20analysis%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Journ%C3%A9e%2C%20Michel%20Nesterov%2C%20Yurii%20Richt%C3%A1rik%2C%20Peter%20Sepulchre%2C%20Rodolphe%20Generalized%20power%20method%20for%20sparse%20principal%20component%20analysis%202010"
        },
        {
            "id": "23",
            "entry": "[23] Rajiv Khanna, Joydeep Ghosh, Russell A Poldrack, and Oluwasanmi Koyejo. Sparse submodular probabilistic pca. In AISTATS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khanna%2C%20Rajiv%20Ghosh%2C%20Joydeep%20Poldrack%2C%20Russell%20A.%20Koyejo%2C%20Oluwasanmi%20Sparse%20submodular%20probabilistic%20pca%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khanna%2C%20Rajiv%20Ghosh%2C%20Joydeep%20Poldrack%2C%20Russell%20A.%20Koyejo%2C%20Oluwasanmi%20Sparse%20submodular%20probabilistic%20pca%202015"
        },
        {
            "id": "24",
            "entry": "[24] Oluwasanmi O Koyejo, Rajiv Khanna, Joydeep Ghosh, and Russell Poldrack. On prior distributions and approximate inference for structured variables. In Advances in Neural Information Processing Systems, pages 676\u2013684, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koyejo%2C%20Oluwasanmi%20O.%20Khanna%2C%20Rajiv%20Ghosh%2C%20Joydeep%20Poldrack%2C%20Russell%20On%20prior%20distributions%20and%20approximate%20inference%20for%20structured%20variables%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koyejo%2C%20Oluwasanmi%20O.%20Khanna%2C%20Rajiv%20Ghosh%2C%20Joydeep%20Poldrack%2C%20Russell%20On%20prior%20distributions%20and%20approximate%20inference%20for%20structured%20variables%202014"
        },
        {
            "id": "25",
            "entry": "[25] Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations really solve sparse pca. Technical report, Technical report, Weizmann Institute of Science, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krauthgamer%2C%20Robert%20Nadler%2C%20Boaz%20Vilenchik%2C%20Dan%20Do%20semidefinite%20relaxations%20really%20solve%20sparse%20pca%202013"
        },
        {
            "id": "26",
            "entry": "[26] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302\u20131338, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Beatrice%20Massart%2C%20Pascal%20Adaptive%20estimation%20of%20a%20quadratic%20functional%20by%20model%20selection%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Beatrice%20Massart%2C%20Pascal%20Adaptive%20estimation%20of%20a%20quadratic%20functional%20by%20model%20selection%202000"
        },
        {
            "id": "27",
            "entry": "[27] Zongming Ma et al. Sparse principal component analysis and iterative thresholding. The Annals of Statistics, 41(2):772\u2013801, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Zongming%20Sparse%20principal%20component%20analysis%20and%20iterative%20thresholding%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Zongming%20Sparse%20principal%20component%20analysis%20and%20iterative%20thresholding%202013"
        },
        {
            "id": "28",
            "entry": "[28] St\u00e9phane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. Signal Processing, IEEE Transactions on, 41(12):3397\u20133415, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mallat%2C%20St%C3%A9phane%20G.%20Zhang%2C%20Zhifeng%20Matching%20pursuits%20with%20time-frequency%20dictionaries%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mallat%2C%20St%C3%A9phane%20G.%20Zhang%2C%20Zhifeng%20Matching%20pursuits%20with%20time-frequency%20dictionaries%201993"
        },
        {
            "id": "29",
            "entry": "[29] Nicolai Meinshausen and Peter B\u00fchlmann. High-dimensional graphs and variable selection with the lasso. The annals of statistics, pages 1436\u20131462, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meinshausen%2C%20Nicolai%20B%C3%BChlmann%2C%20Peter%20High-dimensional%20graphs%20and%20variable%20selection%20with%20the%20lasso%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meinshausen%2C%20Nicolai%20B%C3%BChlmann%2C%20Peter%20High-dimensional%20graphs%20and%20variable%20selection%20with%20the%20lasso%202006"
        },
        {
            "id": "30",
            "entry": "[30] Deanna Needell and Joel A Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26(3):301\u2013321, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Deanna%20Needell%20and%20Joel%20A%20Tropp.%20Cosamp%3A%20Iterative%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20samples%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Deanna%20Needell%20and%20Joel%20A%20Tropp.%20Cosamp%3A%20Iterative%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20samples%202009"
        },
        {
            "id": "31",
            "entry": "[31] Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework for highdimensional analysis of m-estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, pages 1348\u20131356, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20Sahand%20Yu%2C%20Bin%20Wainwright%2C%20Martin%20J.%20Ravikumar%2C%20Pradeep%20K.%20A%20unified%20framework%20for%20highdimensional%20analysis%20of%20m-estimators%20with%20decomposable%20regularizers%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20Sahand%20Yu%2C%20Bin%20Wainwright%2C%20Martin%20J.%20Ravikumar%2C%20Pradeep%20K.%20A%20unified%20framework%20for%20highdimensional%20analysis%20of%20m-estimators%20with%20decomposable%20regularizers%202009"
        },
        {
            "id": "32",
            "entry": "[32] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. The Journal of Machine Learning Research, 11:2241\u20132259, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010"
        },
        {
            "id": "33",
            "entry": "[33] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high-dimensional linear regression overq-balls. Information Theory, IEEE Transactions on, 57(10):6976\u20136994, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Minimax%20rates%20of%20estimation%20for%20high-dimensional%20linear%20regression%20overq-balls%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Minimax%20rates%20of%20estimation%20for%20high-dimensional%20linear%20regression%20overq-balls%202011"
        },
        {
            "id": "34",
            "entry": "[34] Philippe Rigollet and Alexandre Tsybakov. Exponential screening and optimal rates of sparse estimation. The Annals of Statistics, 39(2):731\u2013771, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20Philippe%20Tsybakov%2C%20Alexandre%20Exponential%20screening%20and%20optimal%20rates%20of%20sparse%20estimation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rigollet%2C%20Philippe%20Tsybakov%2C%20Alexandre%20Exponential%20screening%20and%20optimal%20rates%20of%20sparse%20estimation%202011"
        },
        {
            "id": "35",
            "entry": "[35] Mark Rudelson and Shuheng Zhou. Reconstruction from anisotropic random measurements. In Conference on Learning Theory, pages 10\u20131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudelson%2C%20Mark%20Zhou%2C%20Shuheng%20Reconstruction%20from%20anisotropic%20random%20measurements%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudelson%2C%20Mark%20Zhou%2C%20Shuheng%20Reconstruction%20from%20anisotropic%20random%20measurements%202012"
        },
        {
            "id": "36",
            "entry": "[36] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "37",
            "entry": "[37] Sara van de Geer. The deterministic lasso. Seminar f\u00fcr Statistik, Eidgen\u00f6ssische Technische Hochschule (ETH) Z\u00fcrich, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20de%20Geer%2C%20Sara%20The%20deterministic%20lasso%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20de%20Geer%2C%20Sara%20The%20deterministic%20lasso%202007"
        },
        {
            "id": "38",
            "entry": "[38] Sara A Van De Geer, Peter B\u00fchlmann, et al. On the conditions used to prove oracle results for the lasso. Electronic Journal of Statistics, 3:1360\u20131392, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sara%20A%20Van%20De%20Geer%2C%20Peter%20B%C3%BChlmann%20On%20the%20conditions%20used%20to%20prove%20oracle%20results%20for%20the%20lasso%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sara%20A%20Van%20De%20Geer%2C%20Peter%20B%C3%BChlmann%20On%20the%20conditions%20used%20to%20prove%20oracle%20results%20for%20the%20lasso%202009"
        },
        {
            "id": "39",
            "entry": "[39] Vincent Q Vu, Juhee Cho, Jing Lei, and Karl Rohe. Fantope projection and selection: A near-optimal convex relaxation of sparse pca. In Advances in Neural Information Processing Systems, pages 2670\u20132678, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vu%2C%20Vincent%20Q.%20Cho%2C%20Juhee%20Lei%2C%20Jing%20Rohe%2C%20Karl%20Fantope%20projection%20and%20selection%3A%20A%20near-optimal%20convex%20relaxation%20of%20sparse%20pca%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vu%2C%20Vincent%20Q.%20Cho%2C%20Juhee%20Lei%2C%20Jing%20Rohe%2C%20Karl%20Fantope%20projection%20and%20selection%3A%20A%20near-optimal%20convex%20relaxation%20of%20sparse%20pca%202013"
        },
        {
            "id": "40",
            "entry": "[40] Martin Wainwright. Information-theoretic bounds on sparsity recovery in the high-dimensional and noisy setting. In Information Theory, 2007. ISIT 2007. IEEE International Symposium on, pages 961\u2013965. IEEE, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20Information-theoretic%20bounds%20on%20sparsity%20recovery%20in%20the%20high-dimensional%20and%20noisy%20setting%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20Information-theoretic%20bounds%20on%20sparsity%20recovery%20in%20the%20high-dimensional%20and%20noisy%20setting%202007"
        },
        {
            "id": "41",
            "entry": "[41] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (lasso). Information Theory, IEEE Transactions on, 55(5):2183\u20132202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using-constrained%20quadratic%20programming%20%28lasso%29%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20J.%20Sharp%20thresholds%20for%20high-dimensional%20and%20noisy%20sparsity%20recovery%20using-constrained%20quadratic%20programming%20%28lasso%29%202009"
        },
        {
            "id": "42",
            "entry": "[42] Tengyao Wang, Quentin Berthet, Richard J Samworth, et al. Statistical and computational trade-offs in estimation of sparse principal components. The Annals of Statistics, 44(5):1896\u20131930, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Tengyao%20Berthet%2C%20Quentin%20Samworth%2C%20Richard%20J.%20Statistical%20and%20computational%20trade-offs%20in%20estimation%20of%20sparse%20principal%20components%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Tengyao%20Berthet%2C%20Quentin%20Samworth%2C%20Richard%20J.%20Statistical%20and%20computational%20trade-offs%20in%20estimation%20of%20sparse%20principal%20components%202016"
        },
        {
            "id": "43",
            "entry": "[43] Xiao-Tong Yuan and Tong Zhang. Truncated power method for sparse eigenvalue problems. The Journal of Machine Learning Research, 14(1):899\u2013925, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Xiao-Tong%20Zhang%2C%20Tong%20Truncated%20power%20method%20for%20sparse%20eigenvalue%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Xiao-Tong%20Zhang%2C%20Tong%20Truncated%20power%20method%20for%20sparse%20eigenvalue%20problems%202013"
        },
        {
            "id": "44",
            "entry": "[44] Tong Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In Advances in Neural Information Processing Systems, pages 1921\u20131928, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Adaptive%20forward-backward%20greedy%20algorithm%20for%20sparse%20learning%20with%20linear%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Adaptive%20forward-backward%20greedy%20algorithm%20for%20sparse%20learning%20with%20linear%20models%202009"
        },
        {
            "id": "45",
            "entry": "[45] Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Lower bounds on the performance of polynomial-time algorithms for sparse linear regression. arXiv preprint arXiv:1402.1918, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.1918"
        },
        {
            "id": "46",
            "entry": "[46] Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Optimal prediction for sparse linear models? lower bounds for coordinate-separable m-estimators. arXiv preprint arXiv:1503.03188, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.03188"
        },
        {
            "id": "47",
            "entry": "[47] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265\u2013286, 2006. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Sparse%20principal%20component%20analysis%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Sparse%20principal%20component%20analysis%202006"
        }
    ]
}
