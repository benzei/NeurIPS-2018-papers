{
    "filename": "7819-learning-to-multitask.pdf",
    "metadata": {
        "title": "Learning to Multitask",
        "author": "Yu Zhang, Ying Wei, Qiang Yang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7819-learning-to-multitask.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Multitask learning has shown promising performance in many applications and many multitask models have been proposed. In order to identify an effective multitask model for a given multitask problem, we propose a learning framework called Learning to MultiTask (L2MT). To achieve the goal, L2MT exploits historical multitask experience which is organized as a training set consisting of several tuples, each of which contains a multitask problem with multiple tasks, a multitask model, and the relative test error. Based on such training set, L2MT first uses a proposed layerwise graph neural network to learn task embeddings for all the tasks in a multitask problem and then learns an estimation function to estimate the relative test error based on task embeddings and the representation of the multitask model based on a unified formulation. Given a new multitask problem, the estimation function is used to identify a suitable multitask model. Experiments on benchmark datasets show the effectiveness of the proposed L2MT framework."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "positive semidefinite",
            "url": "https://en.wikipedia.org/wiki/positive_semidefinite"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "Multitask learning [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] aims to leverage useful information contained in multiple tasks to help improve the generalization performance of those tasks",
        "For a new multitask problem, we can obtain the task embedding matrix via the Layerwise Graph Neural Network learned on the training set and in order to achieve a low relative test error, we minimize the estimation function to learn the task covariance matrix as well as the corresponding multitask model",
        "We propose L2MT to identify a good multitask model for a multitask problem based on historical multitask problems",
        "We propose an end-to-end procedure, which employs the Layerwise Graph Neural Network to learn task embedding matrices for multitask problems and uses the estimation function to approximate the relative test error",
        "Given a new multitask problem, minimizing the estimation function leads to the identification of the task covariance matrix",
        "We will extend the proposed L2MT method to learn good feature covariances for multitask problems based on this formulation"
    ],
    "key_statements": [
        "Multitask learning [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] aims to leverage useful information contained in multiple tasks to help improve the generalization performance of those tasks",
        "Based on this training set, we propose an end-to-end approach to learn the mapping from both the multitask problem and the multitask model to the relative test error, where we need to determine the representations of the multitask problem and the multitask model",
        "For a new multitask problem, we can obtain the task embedding matrix via the Layerwise Graph Neural Network learned on the training set and in order to achieve a low relative test error, we minimize the estimation function to learn the task covariance matrix as well as the corresponding multitask model",
        "We propose a method to represent the task embedding based on neural networks with powerful capacities",
        "Based on the graph representation, we propose the Layerwise Graph Neural Network to obtain the task embedding",
        "Based on each aforementioned dataset , we construct the training set for L2MT in the following two steps: 1) We first construct a multitask problem where each task is a binary classification task",
        "For a multitask problem with m tasks, we just randomly sample m pairs of classes along with their data where each task is to distinguish between each pair of classes.\n2) We sample q multi-task problems to constitute the training set for L2MT",
        "All the relative test errors of single-task learner are Figure 2: Results of different models on four datasets when varyequal to 1, and the performance ing the size of training data",
        "According to Figure 2, we can see that some multitask models perform worse than single-task learner with relative test errors larger than 1, which can be explained by the mismatch between data and model assumptions imposed on the task covariance",
        "By learning the task covariance directly from data without explicit assumptions, the proposed L2MT performs better than all the baseline methods under different settings, which demonstrates the effectiveness of L2MT",
        "After fine-tuning, the average test errors of L2MT are reduced by about 5% compared to L2MT without fine-tuning, which demonstrates the effectiveness of L2MT on not only improving the performance of multitask problems but learning good features",
        "We propose L2MT to identify a good multitask model for a multitask problem based on historical multitask problems",
        "We propose an end-to-end procedure, which employs the Layerwise Graph Neural Network to learn task embedding matrices for multitask problems and uses the estimation function to approximate the relative test error",
        "Given a new multitask problem, minimizing the estimation function leads to the identification of the task covariance matrix",
        "We will extend the proposed L2MT method to learn good feature covariances for multitask problems based on this formulation"
    ],
    "summary": [
        "Multitask learning [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] aims to leverage useful information contained in multiple tasks to help improve the generalization performance of those tasks.",
        "For a new multitask problem, we can obtain the task embedding matrix via the LGNN learned on the training set and in order to achieve a low relative test error, we minimize the estimation function to learn the task covariance matrix as well as the corresponding multitask model.",
        "In order to learn the estimation function in the training process, the first thing we need to do is to determine the representation of multitask problems {Si}.",
        "We aim to learn an estimation function mapping from both the task embedding matrix and the task covariance matrix to the relative test error, i.e., f (Ei, \u03a9i) \u2248 \u03c5 for i = 1, .",
        "The training process of L2MT induces a new learning problem where each multitask problem is used to predict the relative test error and the task embedding matrix contains meta features to describe the multitask problem.",
        "Based on each aforementioned dataset , we construct the training set for L2MT in the following two steps: 1) We first construct a multitask problem where each task is a binary classification task.",
        "All the relative test errors of STL are Figure 2: Results of different models on four datasets when varyequal to 1, and the performance ing the size of training data.",
        "According to Figure 2, we can see that some multitask models perform worse than STL with relative test errors larger than 1, which can be explained by the mismatch between data and model assumptions imposed on the task covariance.",
        "By learning the task covariance directly from data without explicit assumptions, the proposed L2MT performs better than all the baseline methods under different settings, which demonstrates the effectiveness of L2MT.",
        "To assess the quality of the learned task covariance matrices by different models, we conduct a case study by constructing a multitask problem consisting of three tasks from the Caltech256 dataset.",
        "We propose an end-to-end procedure, which employs the LGNN to learn task embedding matrices for multitask problems and uses the estimation function to approximate the relative test error.",
        "Given a new multitask problem, minimizing the estimation function leads to the identification of the task covariance matrix.",
        "We will extend the proposed L2MT method to learn good feature covariances for multitask problems based on this formulation.",
        "The proposed L2MT method can be extended to meta learning where LGNN can be used to learn hidden representations for datasets"
    ],
    "headline": "In order to identify an effective multitask model for a given multitask problem, we propose a learning framework called Learning to MultiTask ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817\u20131853, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ando%2C%20R.K.%20Zhang%2C%20T.%20A%20framework%20for%20learning%20predictive%20structures%20from%20multiple%20tasks%20and%20unlabeled%20data%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ando%2C%20R.K.%20Zhang%2C%20T.%20A%20framework%20for%20learning%20predictive%20structures%20from%20multiple%20tasks%20and%20unlabeled%20data%202005"
        },
        {
            "id": "2",
            "entry": "[2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems 19, pages 41\u201348, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argyriou%2C%20A.%20Evgeniou%2C%20T.%20Pontil%2C%20M.%20Multi-task%20feature%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argyriou%2C%20A.%20Evgeniou%2C%20T.%20Pontil%2C%20M.%20Multi-task%20feature%20learning%202006"
        },
        {
            "id": "3",
            "entry": "[3] A. Argyriou, C. A. Micchelli, M. Pontil, and Y. Ying. A spectral regularization framework for multi-task structure learning. In Advances in Neural Information Processing Systems 20, pages 25\u201332, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argyriou%2C%20A.%20Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Ying%2C%20Y.%20A%20spectral%20regularization%20framework%20for%20multi-task%20structure%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argyriou%2C%20A.%20Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Ying%2C%20Y.%20A%20spectral%20regularization%20framework%20for%20multi-task%20structure%20learning%202007"
        },
        {
            "id": "4",
            "entry": "[4] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems 29, pages 1993\u20132001, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Atwood%2C%20J.%20Towsley%2C%20D.%20Diffusion-convolutional%20neural%20networks%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Atwood%2C%20J.%20Towsley%2C%20D.%20Diffusion-convolutional%20neural%20networks%201993"
        },
        {
            "id": "5",
            "entry": "[5] O. Banerjee, L. E. Ghaoui, A. d\u2019Aspremont, and G. Natsoulis. Convex optimization techniques for fitting sparse Gaussian graphical models. In Proceedings of the Twenty-Third International Conference on Machine Learning, pages 89\u201396, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20O.%20Ghaoui%2C%20L.E.%20d%E2%80%99Aspremont%2C%20A.%20Natsoulis%2C%20G.%20Convex%20optimization%20techniques%20for%20fitting%20sparse%20Gaussian%20graphical%20models%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20O.%20Ghaoui%2C%20L.E.%20d%E2%80%99Aspremont%2C%20A.%20Natsoulis%2C%20G.%20Convex%20optimization%20techniques%20for%20fitting%20sparse%20Gaussian%20graphical%20models%202006"
        },
        {
            "id": "6",
            "entry": "[6] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463\u2013482, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20Gaussian%20complexities%3A%20Risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "7",
            "entry": "[7] J. Baxter. A model of inductive bias learning. Journal of Artifical Intelligence Research, 12:149\u2013198, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baxter%2C%20J.%20A%20model%20of%20inductive%20bias%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baxter%2C%20J.%20A%20model%20of%20inductive%20bias%20learning%202000"
        },
        {
            "id": "8",
            "entry": "[8] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. CoRR, abs/1312.6203, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6203"
        },
        {
            "id": "9",
            "entry": "[9] R. Caruana. Multitask learning. Machine Learning, 28(1):41\u201375, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20R.%20Multitask%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20R.%20Multitask%20learning%201997"
        },
        {
            "id": "10",
            "entry": "[10] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse and low-rank patterns from multiple tasks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1179\u20131188, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20J.%20Liu%2C%20J.%20Ye%2C%20J.%20Learning%20incoherent%20sparse%20and%20low-rank%20patterns%20from%20multiple%20tasks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20J.%20Liu%2C%20J.%20Ye%2C%20J.%20Learning%20incoherent%20sparse%20and%20low-rank%20patterns%20from%20multiple%20tasks%202010"
        },
        {
            "id": "11",
            "entry": "[11] J. Chen, L. Tang, J. Liu, and J. Ye. A convex formulation for learning shared structures from multiple tasks. In Proceedings of the 26th International Conference on Machine Learning, pages 137\u2013144, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20J.%20Tang%2C%20L.%20Liu%2C%20J.%20Ye%2C%20J.%20A%20convex%20formulation%20for%20learning%20shared%20structures%20from%20multiple%20tasks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20J.%20Tang%2C%20L.%20Liu%2C%20J.%20Ye%2C%20J.%20A%20convex%20formulation%20for%20learning%20shared%20structures%20from%20multiple%20tasks%202009"
        },
        {
            "id": "12",
            "entry": "[12] Z. Chen and B. Liu. Lifelong Machine Learning. Morgan & Claypool, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Z.%20Liu%2C%20B.%20Lifelong%20Machine%20Learning%202016"
        },
        {
            "id": "13",
            "entry": "[13] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6:615\u2013637, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evgeniou%2C%20T.%20Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Learning%20multiple%20tasks%20with%20kernel%20methods%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evgeniou%2C%20T.%20Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Learning%20multiple%20tasks%20with%20kernel%20methods%202005"
        },
        {
            "id": "14",
            "entry": "[14] T. Evgeniou and M. Pontil. Regularized multi-task learning. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 109\u2013117, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evgeniou%2C%20T.%20Pontil%2C%20M.%20Regularized%20multi-task%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evgeniou%2C%20T.%20Pontil%2C%20M.%20Regularized%20multi-task%20learning%202004"
        },
        {
            "id": "15",
            "entry": "[15] L. Han and Y. Zhang. Learning multi-level task groups in multi-task learning. In Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20L.%20Zhang%2C%20Y.%20Learning%20multi-level%20task%20groups%20in%20multi-task%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20L.%20Zhang%2C%20Y.%20Learning%20multi-level%20task%20groups%20in%20multi-task%20learning%202015"
        },
        {
            "id": "16",
            "entry": "[16] L. Han and Y. Zhang. Learning tree structure in multi-task learning. In Proceedings of the 21st ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20L.%20Zhang%2C%20Y.%20Learning%20tree%20structure%20in%20multi-task%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20L.%20Zhang%2C%20Y.%20Learning%20tree%20structure%20in%20multi-task%20learning%202015"
        },
        {
            "id": "17",
            "entry": "[17] L. Han and Y. Zhang. Multi-stage multi-task learning with reduced rank. In Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20L.%20Zhang%2C%20Y.%20Multi-stage%20multi-task%20learning%20with%20reduced%20rank%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20L.%20Zhang%2C%20Y.%20Multi-stage%20multi-task%20learning%20with%20reduced%20rank%202016"
        },
        {
            "id": "18",
            "entry": "[18] L. Jacob, F. Bach, and J.-P. Vert. Clustered multi-task learning: A convex formulation. In Advances in Neural Information Processing Systems 21, pages 745\u2013752, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%2C%20L.%20Bach%2C%20F.%20Vert%2C%20J.-P.%20Clustered%20multi-task%20learning%3A%20A%20convex%20formulation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%2C%20L.%20Bach%2C%20F.%20Vert%2C%20J.-P.%20Clustered%20multi-task%20learning%3A%20A%20convex%20formulation%202008"
        },
        {
            "id": "19",
            "entry": "[19] A. Jalali, P. D. Ravikumar, S. Sanghavi, and C. Ruan. A dirty model for multi-task learning. In Advances in Neural Information Processing Systems 23, pages 964\u2013972, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jalali%2C%20A.%20Ravikumar%2C%20P.D.%20Sanghavi%2C%20S.%20Ruan%2C%20C.%20A%20dirty%20model%20for%20multi-task%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jalali%2C%20A.%20Ravikumar%2C%20P.D.%20Sanghavi%2C%20S.%20Ruan%2C%20C.%20A%20dirty%20model%20for%20multi-task%20learning%202010"
        },
        {
            "id": "20",
            "entry": "[20] A. Kumar and H. Daum\u00e9 III. Learning task grouping and overlap in multi-task learning. In Proceedings of the 29 th International Conference on Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20A.%20Daum%C3%A9%2C%20III%2C%20H.%20Learning%20task%20grouping%20and%20overlap%20in%20multi-task%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20A.%20Daum%C3%A9%2C%20III%2C%20H.%20Learning%20task%20grouping%20and%20overlap%20in%20multi-task%20learning%202012"
        },
        {
            "id": "21",
            "entry": "[21] G. Lee, E. Yang, and S. J. Hwang. Asymmetric multi-task learning based on task relatedness and loss. In Proceedings of the 33rd International Conference on Machine Learning, pages 230\u2013238, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20G.%20Yang%2C%20E.%20Hwang%2C%20S.J.%20Asymmetric%20multi-task%20learning%20based%20on%20task%20relatedness%20and%20loss%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20G.%20Yang%2C%20E.%20Hwang%2C%20S.J.%20Asymmetric%20multi-task%20learning%20based%20on%20task%20relatedness%20and%20loss%202016"
        },
        {
            "id": "22",
            "entry": "[22] A. Maurer. A chain rule for the expected suprema of Gaussian processes. In Proceedings of the 25th International Conference on Algorithmic Learning Theory, pages 245\u2013259, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20A%20chain%20rule%20for%20the%20expected%20suprema%20of%20Gaussian%20processes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20A.%20A%20chain%20rule%20for%20the%20expected%20suprema%20of%20Gaussian%20processes%202014"
        },
        {
            "id": "23",
            "entry": "[23] A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning. Journal of Machine Learning Research, 17:1\u201332, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20A.%20Pontil%2C%20M.%20Romera-Paredes%2C%20B.%20The%20benefit%20of%20multitask%20representation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20A.%20Pontil%2C%20M.%20Romera-Paredes%2C%20B.%20The%20benefit%20of%20multitask%20representation%20learning%202016"
        },
        {
            "id": "24",
            "entry": "[24] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099\u20131125, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Learning%20the%20kernel%20function%20via%20regularization%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Micchelli%2C%20C.A.%20Pontil%2C%20M.%20Learning%20the%20kernel%20function%20via%20regularization%202005"
        },
        {
            "id": "25",
            "entry": "[25] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch networks for multi-task learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3994\u20134003, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20I.%20Shrivastava%2C%20A.%20Gupta%2C%20A.%20Hebert%2C%20M.%20Cross-stitch%20networks%20for%20multi-task%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20I.%20Shrivastava%2C%20A.%20Gupta%2C%20A.%20Hebert%2C%20M.%20Cross-stitch%20networks%20for%20multi-task%20learning%202016"
        },
        {
            "id": "26",
            "entry": "[26] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In Proceedings of the 33nd International Conference on Machine Learning, pages 2014\u20132023, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niepert%2C%20M.%20Ahmed%2C%20M.%20Kutzkov%2C%20K.%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niepert%2C%20M.%20Ahmed%2C%20M.%20Kutzkov%2C%20K.%20Learning%20convolutional%20neural%20networks%20for%20graphs%202014"
        },
        {
            "id": "27",
            "entry": "[27] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20S.J.%20Yang%2C%20Q.%20A%20survey%20on%20transfer%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20S.J.%20Yang%2C%20Q.%20A%20survey%20on%20transfer%20learning%202010"
        },
        {
            "id": "28",
            "entry": "[28] T. K. Pong, P. Tseng, S. Ji, and J. Ye. Trace norm regularization: Reformulations, algorithms, and multi-task learning. SIAM Journal on Optimization, 20(6):3465\u20133489, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pong%2C%20T.K.%20Tseng%2C%20P.%20Ji%2C%20S.%20Ye%2C%20J.%20Trace%20norm%20regularization%3A%20Reformulations%2C%20algorithms%2C%20and%20multi-task%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pong%2C%20T.K.%20Tseng%2C%20P.%20Ji%2C%20S.%20Ye%2C%20J.%20Trace%20norm%20regularization%3A%20Reformulations%2C%20algorithms%2C%20and%20multi-task%20learning%202010"
        },
        {
            "id": "29",
            "entry": "[29] P. Rai, A. Kumar, and H. Daume. Simultaneously leveraging output and task structures for multiple-output regression. In Advances in Neural Information Processing Systems 25, pages 3185\u20133193, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rai%2C%20P.%20Kumar%2C%20A.%20H.%20Daume.%20Simultaneously%20leveraging%20output%20and%20task%20structures%20for%20multiple-output%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rai%2C%20P.%20Kumar%2C%20A.%20H.%20Daume.%20Simultaneously%20leveraging%20output%20and%20task%20structures%20for%20multiple-output%20regression%202012"
        },
        {
            "id": "30",
            "entry": "[30] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20F.%20Gori%2C%20M.%20Tsoi%2C%20A.C.%20Hagenbuchner%2C%20M.%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20F.%20Gori%2C%20M.%20Tsoi%2C%20A.C.%20Hagenbuchner%2C%20M.%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "31",
            "entry": "[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "32",
            "entry": "[32] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. CoRR, abs/1505.00387, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1505.00387"
        },
        {
            "id": "33",
            "entry": "[33] Y. Wei, Y. Zhang, J. Huang, and Q. Yang. Transfer learning via learning to transfer. In Proceedings of the 35th International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Y.%20Zhang%2C%20Y.%20Huang%2C%20J.%20Yang%2C%20Q.%20Transfer%20learning%20via%20learning%20to%20transfer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Y.%20Zhang%2C%20Y.%20Huang%2C%20J.%20Yang%2C%20Q.%20Transfer%20learning%20via%20learning%20to%20transfer%202018"
        },
        {
            "id": "34",
            "entry": "[34] S. Yang, L. Yuan, Y.-C. Lai, X. Shen, P. Wonka, and J. Ye. Feature grouping and selection over an undirected graph. In Proceedings of ACM SIGKDD Conference on Kownledge Discovery and Data Mining, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20S.%20Yuan%2C%20L.%20Lai%2C%20Y.-C.%20Shen%2C%20X.%20Feature%20grouping%20and%20selection%20over%20an%20undirected%20graph%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20S.%20Yuan%2C%20L.%20Lai%2C%20Y.-C.%20Shen%2C%20X.%20Feature%20grouping%20and%20selection%20over%20an%20undirected%20graph%202012"
        },
        {
            "id": "35",
            "entry": "[35] Y. Zhang. Heterogeneous-neighborhood-based multi-task local learning algorithms. In Advances in Neural Information Processing Systems 26, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Heterogeneous-neighborhood-based%20multi-task%20local%20learning%20algorithms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Heterogeneous-neighborhood-based%20multi-task%20local%20learning%20algorithms%202013"
        },
        {
            "id": "36",
            "entry": "[36] Y. Zhang and J. G. Schneider. Learning multiple tasks with a sparse matrix-normal penalty. In Advances in Neural Information Processing Systems 23, pages 2550\u20132558, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Schneider%2C%20J.G.%20Learning%20multiple%20tasks%20with%20a%20sparse%20matrix-normal%20penalty%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Schneider%2C%20J.G.%20Learning%20multiple%20tasks%20with%20a%20sparse%20matrix-normal%20penalty%202010"
        },
        {
            "id": "37",
            "entry": "[37] Y. Zhang and Q. Yang. Learning sparse task relations in multi-task learning. In Proceedings of the 31th AAAI Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Yang%2C%20Q.%20Learning%20sparse%20task%20relations%20in%20multi-task%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Yang%2C%20Q.%20Learning%20sparse%20task%20relations%20in%20multi-task%20learning%202017"
        },
        {
            "id": "38",
            "entry": "[38] Y. Zhang and Q. Yang. A survey on multi-task learning. arXiv preprint, arXiv:1707.08114v2, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08114v2"
        },
        {
            "id": "39",
            "entry": "[39] Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, pages 733\u2013742, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20A%20convex%20formulation%20for%20learning%20task%20relationships%20in%20multi-task%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20A%20convex%20formulation%20for%20learning%20task%20relationships%20in%20multi-task%20learning%202010"
        },
        {
            "id": "40",
            "entry": "[40] Y. Zhang and D.-Y. Yeung. Multi-task learning using generalized t process. In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, pages 964\u2013971, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20Multi-task%20learning%20using%20generalized%20t%20process%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20Multi-task%20learning%20using%20generalized%20t%20process%202010"
        },
        {
            "id": "41",
            "entry": "[41] Y. Zhang and D.-Y. Yeung. Learning high-order task relationships in multi-task learning. In Proceedings of the 23rd International Joint Conference on Artificial Intelligence, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20Learning%20high-order%20task%20relationships%20in%20multi-task%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20Learning%20high-order%20task%20relationships%20in%20multi-task%20learning%202013"
        },
        {
            "id": "42",
            "entry": "[42] Y. Zhang and D.-Y. Yeung. A regularization approach to learning task relationships in multitask learning. ACM Transactions on Knowledge Discovery from Data, 8(3):article 12, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20A%20regularization%20approach%20to%20learning%20task%20relationships%20in%20multitask%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20A%20regularization%20approach%20to%20learning%20task%20relationships%20in%20multitask%20learning"
        },
        {
            "id": "43",
            "entry": "[43] Y. Zhang, D.-Y. Yeung, and Q. Xu. Probabilistic multi-task feature selection. In Advances in Neural Information Processing Systems 23, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20Xu%2C%20Q.%20Probabilistic%20multi-task%20feature%20selection%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Yeung%2C%20D.-Y.%20Xu%2C%20Q.%20Probabilistic%20multi-task%20feature%20selection%202010"
        },
        {
            "id": "44",
            "entry": "[44] A. Zweig and D. Weinshall. Hierarchical regularization cascade for joint learning. In Proceedings of the 30th International Conference on Machine Learning, pages 37\u201345, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zweig%2C%20A.%20Weinshall%2C%20D.%20Hierarchical%20regularization%20cascade%20for%20joint%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zweig%2C%20A.%20Weinshall%2C%20D.%20Hierarchical%20regularization%20cascade%20for%20joint%20learning%202013"
        }
    ]
}
