{
    "filename": "7890-proximal-scope-for-distributed-sparse-learning.pdf",
    "metadata": {
        "title": "Proximal SCOPE for Distributed Sparse Learning",
        "author": "Shenyi Zhao, Gong-Duo Zhang, Ming-Wei Li, Wu-Jun Li",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7890-proximal-scope-for-distributed-sparse-learning.pdf"
        },
        "abstract": "Distributed sparse learning with a cluster of multiple machines has attracted much attention in machine learning, especially for large-scale applications with high-dimensional data. One popular way to implement sparse learning is to use L1 regularization. In this paper, we propose a novel method, called proximal SCOPE (pSCOPE), for distributed sparse learning with L1 regularization. pSCOPE is based on a cooperative autonomous local learning (CALL) framework. In the CALL framework of pSCOPE, we find that the data partition affects the convergence of the learning procedure, and subsequently we define a metric to measure the goodness of a data partition. Based on the defined metric, we theoretically prove that pSCOPE is convergent with a linear convergence rate if the data partition is good enough. We also prove that better data partition implies faster convergence rate. Furthermore, pSCOPE is also communication efficient. Experimental results on real data sets show that pSCOPE can outperform other state-of-the-art distributed methods for sparse learning."
    },
    "keywords": [
        {
            "term": "SCOPE",
            "url": "https://en.wikipedia.org/wiki/SCOPE"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "logistic regression",
            "url": "https://en.wikipedia.org/wiki/logistic_regression"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Researchers have recently proposed several distributed proximal stochastic methods for sparse learning [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]1",
        "We propose a novel method, called proximal scalable composite optimization for learning, for distributed sparse learning with L1 regularization. proximal scalable composite optimization for learning is a proximal generalization of the scalable composite optimization for learning (SCOPE) [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]",
        "Experimental results on real data sets show that proximal scalable composite optimization for learning can outperform other state-of-theart distributed methods for sparse learning.\n1In this paper, we mainly focus on distributed sparse learning with L1 regularization",
        "We propose a metric to measure the goodness of a data partition, based on which the convergence of proximal scalable composite optimization for learning can be theoretically proved",
        "We propose a novel method, called proximal scalable composite optimization for learning, for distributed sparse learning",
        "We theoretically analyze how the data partition affects the convergence of proximal scalable composite optimization for learning. proximal scalable composite optimization for learning is both communication and computation efficient"
    ],
    "key_statements": [
        "Researchers have recently proposed several distributed proximal stochastic methods for sparse learning [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]1",
        "We propose a novel method, called proximal scalable composite optimization for learning, for distributed sparse learning with L1 regularization. proximal scalable composite optimization for learning is a proximal generalization of the scalable composite optimization for learning (SCOPE) [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>]",
        "Experimental results on real data sets show that proximal scalable composite optimization for learning can outperform other state-of-theart distributed methods for sparse learning.\n1In this paper, we mainly focus on distributed sparse learning with L1 regularization",
        "PSCOPE is mainly motivated by sparse learning with L1 regularization, the algorithm and theory of proximal scalable composite optimization for learning can be used for smooth regularization like L2 regularization",
        "When the data partition is good enough, proximal scalable composite optimization for learning can avoid the extra term c in the update rule of scalable composite optimization for learning, which is necessary for convergence guarantee of scalable composite optimization for learning.\n4 Effect of Data Partition",
        "We propose a metric to measure the goodness of a data partition, based on which the convergence of proximal scalable composite optimization for learning can be theoretically proved",
        "AsyProx-stochastic variance reduced gradient is slow on the two large datasets avazu and kdd2012, and we only present the results of it on the datasets cov and rcv1",
        "We evaluate the speedup of proximal scalable composite optimization for learning on the four datasets for logistic regression",
        "We evaluate proximal scalable composite optimization for learning under different data partitions",
        "We propose a novel method, called proximal scalable composite optimization for learning, for distributed sparse learning",
        "We theoretically analyze how the data partition affects the convergence of proximal scalable composite optimization for learning. proximal scalable composite optimization for learning is both communication and computation efficient"
    ],
    "summary": [
        "Researchers have recently proposed several distributed proximal stochastic methods for sparse learning [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]1.",
        "We propose a novel method, called proximal SCOPE, for distributed sparse learning with L1 regularization.",
        "PSCOPE is based on a cooperative autonomous local learning (CALL) framework.",
        "In pSCOPE, a recovery strategy is proposed to reduce the cost of proximal mapping when handling high dimensional sparse data.",
        "Experimental results on real data sets show that pSCOPE can outperform other state-of-theart distributed methods for sparse learning.",
        "Update its local parameter uk,m by a proximal mapping with learning rate \u03b7: uk,m+1 = proxR,\u03b7.",
        "From Algorithm 1, we can find that pSCOPE is based on a cooperative autonomous local learning (CALL) framework.",
        "When the data partition is good enough, pSCOPE can avoid the extra term c in the update rule of SCOPE, which is necessary for convergence guarantee of SCOPE.",
        "We propose a metric to measure the goodness of a data partition, based on which the convergence of pSCOPE can be theoretically proved.",
        "E[vk,m|uk,m] = \u2207Fk + Gk. According to the theory in [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], in the inner iterations of pSCOPE, each worker tries to optimize the local objective function Pk(w; wt) using proximal",
        "In Lasso regression, it is easy to get that the corresponding local-global gap l\u03c0(a) is convex according to Lemma 1 and the fact that Gk(a) is an affine function in this case.",
        "Lemma 2 implies that as long as the size of training data is large enough, \u03b3(\u03c0; ) will be small and \u03c0 will be a good partition.",
        "We can find that pSCOPE has a linear convergence rate if the partition is ( , \u03be)-good, which implies pSCOPE is computation efficient and we need T = O) outer iterations to get a -optimal solution.",
        "In the above theorems and corollaries, we only assume that the local loss function Fk(\u00b7) is strongly convex.",
        "For the cases with high dimensional sparse data, we propose recovery strategy to reduce the cost of proximal mapping so that it can accelerate the training procedure.",
        "DBCD and PROXCOCOA+ adopt a coordinate distributed strategy to partition the data.",
        "We propose a novel method, called pSCOPE, for distributed sparse learning.",
        "Experiments on real data show that pSCOPE can outperform other state-of-the-art methods to achieve the best performance"
    ],
    "headline": "We propose a novel method, called proximal scalable composite optimization for learning , for distributed sparse learning with L1 regularization. proximal scalable composite optimization for learning is based on a cooperative autonomous local learning  framework",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Necdet S. Aybat, Zi Wang, and Garud Iyengar. An asynchronous distributed proximal gradient method for composite convex optimization. In Proceedings of the 32nd International Conference on Machine Learning, pages 2454\u20132462, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aybat%2C%20Necdet%20S.%20Wang%2C%20Zi%20Iyengar%2C%20Garud%20An%20asynchronous%20distributed%20proximal%20gradient%20method%20for%20composite%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aybat%2C%20Necdet%20S.%20Wang%2C%20Zi%20Iyengar%2C%20Garud%20An%20asynchronous%20distributed%20proximal%20gradient%20method%20for%20composite%20convex%20optimization%202015"
        },
        {
            "id": "2",
            "entry": "[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "3",
            "entry": "[3] Joseph K. Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin. Parallel coordinate descent for l1regularized loss minimization. In Proceedings of the 28th International Conference on Machine Learning, pages 321\u2013328, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bradley%2C%20Joseph%20K.%20Kyrola%2C%20Aapo%20Bickson%2C%20Danny%20Guestrin%2C%20Carlos%20Parallel%20coordinate%20descent%20for%20l1regularized%20loss%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bradley%2C%20Joseph%20K.%20Kyrola%2C%20Aapo%20Bickson%2C%20Danny%20Guestrin%2C%20Carlos%20Parallel%20coordinate%20descent%20for%20l1regularized%20loss%20minimization%202011"
        },
        {
            "id": "4",
            "entry": "[4] Richard H. Byrd, S. L. Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008\u20131031, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20Richard%20H.%20Hansen%2C%20S.L.%20Nocedal%2C%20Jorge%20Singer%2C%20Yoram%20A%20stochastic%20quasi-newton%20method%20for%20large-scale%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20Richard%20H.%20Hansen%2C%20S.L.%20Nocedal%2C%20Jorge%20Singer%2C%20Yoram%20A%20stochastic%20quasi-newton%20method%20for%20large-scale%20optimization%202016"
        },
        {
            "id": "5",
            "entry": "[5] Soham De and Tom Goldstein. Efficient distributed SGD with variance reduction. In Proceedings of the 16th IEEE International Conference on Data Mining, pages 111\u2013120, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=De%2C%20Soham%20Goldstein%2C%20Tom%20Efficient%20distributed%20SGD%20with%20variance%20reduction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=De%2C%20Soham%20Goldstein%2C%20Tom%20Efficient%20distributed%20SGD%20with%20variance%20reduction%202016"
        },
        {
            "id": "6",
            "entry": "[6] John C. Duchi and Yoram Singer. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899\u20132934, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Singer%2C%20Yoram%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Singer%2C%20Yoram%20Efficient%20online%20and%20batch%20learning%20using%20forward%20backward%20splitting%202009"
        },
        {
            "id": "7",
            "entry": "[7] Olivier Fercoq and Peter Richt\u00e1rik. Optimization in high dimensions via accelerated, parallel, and proximal coordinate descent. SIAM Review, 58(4):739\u2013771, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fercoq%2C%20Olivier%20Richt%C3%A1rik%2C%20Peter%20Optimization%20in%20high%20dimensions%20via%20accelerated%2C%20parallel%2C%20and%20proximal%20coordinate%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fercoq%2C%20Olivier%20Richt%C3%A1rik%2C%20Peter%20Optimization%20in%20high%20dimensions%20via%20accelerated%2C%20parallel%2C%20and%20proximal%20coordinate%20descent%202016"
        },
        {
            "id": "8",
            "entry": "[8] Pinghua Gong and Jieping Ye. A modified orthant-wise limited memory quasi-newton method with convergence analysis. In Proceedings of the 32nd International Conference on Machine Learning, pages 276\u2013284, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pinghua%20Gong%20and%20Jieping%20Ye.%20A%20modified%20orthant-wise%20limited%20memory%20quasi-newton%20method%20with%20convergence%20analysis%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pinghua%20Gong%20and%20Jieping%20Ye.%20A%20modified%20orthant-wise%20limited%20memory%20quasi-newton%20method%20with%20convergence%20analysis%202015"
        },
        {
            "id": "9",
            "entry": "[9] Zhouyuan Huo, Bin Gu, and Heng Huang. Decoupled asynchronous proximal stochastic gradient descent with variance reduction. CoRR, abs/1609.06804, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.06804"
        },
        {
            "id": "10",
            "entry": "[10] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "11",
            "entry": "[11] John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated gradient. In Advances in Neural Information Processing Systems, pages 905\u2013912, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Li%2C%20Lihong%20Zhang%2C%20Tong%20Sparse%20online%20learning%20via%20truncated%20gradient%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Li%2C%20Lihong%20Zhang%2C%20Tong%20Sparse%20online%20learning%20via%20truncated%20gradient%202008"
        },
        {
            "id": "12",
            "entry": "[12] R\u00e9mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. ASAGA: asynchronous parallel SAGA. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pages 46\u201354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leblond%2C%20R%C3%A9mi%20Pedregosa%2C%20Fabian%20Lacoste-Julien%2C%20Simon%20ASAGA%3A%20asynchronous%20parallel%20SAGA%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leblond%2C%20R%C3%A9mi%20Pedregosa%2C%20Fabian%20Lacoste-Julien%2C%20Simon%20ASAGA%3A%20asynchronous%20parallel%20SAGA%202017"
        },
        {
            "id": "13",
            "entry": "[13] Jason D. Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang. Distributed stochastic variance reduced gradient methods by sampling extra data with replacement. Journal of Machine Learning Research, 18:122:1\u2013122:43, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Lin%2C%20Qihang%20Ma%2C%20Tengyu%20Yang%2C%20Tianbao%20Distributed%20stochastic%20variance%20reduced%20gradient%20methods%20by%20sampling%20extra%20data%20with%20replacement%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Lin%2C%20Qihang%20Ma%2C%20Tengyu%20Yang%2C%20Tianbao%20Distributed%20stochastic%20variance%20reduced%20gradient%20methods%20by%20sampling%20extra%20data%20with%20replacement%202017"
        },
        {
            "id": "14",
            "entry": "[14] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation, pages 583\u2013598, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "15",
            "entry": "[15] Yitan Li, Linli Xu, Xiaowei Zhong, and Qing Ling. Make workers work harder: decoupled asynchronous proximal stochastic gradient descent. CoRR, abs/1605.06619, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.06619"
        },
        {
            "id": "16",
            "entry": "[16] Dhruv Mahajan, S. Sathiya Keerthi, and S. Sundararajan. A distributed block coordinate descent method for training l1 regularized linear classifiers. Journal of Machine Learning Research, 18:91:1\u201391:35, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhruv%20Mahajan%2C%20S.Sathiya%20Keerthi%20Sundararajan%2C%20S.%20A%20distributed%20block%20coordinate%20descent%20method%20for%20training%20l1%20regularized%20linear%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhruv%20Mahajan%2C%20S.Sathiya%20Keerthi%20Sundararajan%2C%20S.%20A%20distributed%20block%20coordinate%20descent%20method%20for%20training%20l1%20regularized%20linear%20classifiers%202017"
        },
        {
            "id": "17",
            "entry": "[17] Qi Meng, Wei Chen, Jingcheng Yu, Taifeng Wang, Zhiming Ma, and Tie-Yan Liu. Asynchronous stochastic proximal optimization algorithms with variance reduction. In Proceedings of the 31th AAAI Conference on Artificial Intelligence, pages 2329\u20132335, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20Qi%20Chen%2C%20Wei%20Yu%2C%20Jingcheng%20Taifeng%20Wang%2C%20Zhiming%20Ma%2C%20and%20Tie-Yan%20Liu.%20Asynchronous%20stochastic%20proximal%20optimization%20algorithms%20with%20variance%20reduction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20Qi%20Chen%2C%20Wei%20Yu%2C%20Jingcheng%20Taifeng%20Wang%2C%20Zhiming%20Ma%2C%20and%20Tie-Yan%20Liu.%20Asynchronous%20stochastic%20proximal%20optimization%20algorithms%20with%20variance%20reduction%202017"
        },
        {
            "id": "18",
            "entry": "[18] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%2C%20Guanghui%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20Arkadi%20Juditsky%2C%20Anatoli%20Lan%2C%20Guanghui%20Shapiro%2C%20Alexander%20Robust%20stochastic%20approximation%20approach%20to%20stochastic%20programming%202009"
        },
        {
            "id": "19",
            "entry": "[19] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander J. Smola. On variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in Neural Information Processing Systems, pages 2647\u20132655, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20On%20variance%20reduction%20in%20stochastic%20gradient%20descent%20and%20its%20asynchronous%20variants%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Hefny%2C%20Ahmed%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20On%20variance%20reduction%20in%20stochastic%20gradient%20descent%20and%20its%20asynchronous%20variants%202015"
        },
        {
            "id": "20",
            "entry": "[20] Peter Richt\u00e1rik and Martin Tak\u00e1c. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1-2):433\u2013484, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20Parallel%20coordinate%20descent%20methods%20for%20big%20data%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richt%C3%A1rik%2C%20Peter%20Tak%C3%A1c%2C%20Martin%20Parallel%20coordinate%20descent%20methods%20for%20big%20data%20optimization%202016"
        },
        {
            "id": "21",
            "entry": "[21] Chad Scherrer, Mahantesh Halappanavar, Ambuj Tewari, and David Haglin. Scaling up coordinate descent algorithms for large l1 regularization problems. In Proceedings of the 29th International Conference on Machine Learning, pages 1407\u20131414, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scherrer%2C%20Chad%20Halappanavar%2C%20Mahantesh%20Tewari%2C%20Ambuj%20Haglin%2C%20David%20Scaling%20up%20coordinate%20descent%20algorithms%20for%20large%20l1%20regularization%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scherrer%2C%20Chad%20Halappanavar%2C%20Mahantesh%20Tewari%2C%20Ambuj%20Haglin%2C%20David%20Scaling%20up%20coordinate%20descent%20algorithms%20for%20large%20l1%20regularization%20problems%202012"
        },
        {
            "id": "22",
            "entry": "[22] Mark W. Schmidt, Nicolas Le Roux, and Francis R. Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83\u2013112, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20Mark%20W.%20Roux%2C%20Nicolas%20Le%20Bach%2C%20Francis%20R.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20Mark%20W.%20Roux%2C%20Nicolas%20Le%20Bach%2C%20Francis%20R.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017"
        },
        {
            "id": "23",
            "entry": "[23] Shai Shalev-Shwartz. SDCA without duality, regularization, and individual convexity. In Proceedings of the 33nd International Conference on Machine Learning, pages 747\u2013754, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20SDCA%20without%20duality%2C%20regularization%2C%20and%20individual%20convexity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20SDCA%20without%20duality%2C%20regularization%2C%20and%20individual%20convexity%202016"
        },
        {
            "id": "24",
            "entry": "[24] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. Journal of Machine Learning Research, 14(1):567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%202013"
        },
        {
            "id": "25",
            "entry": "[25] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In Proceedings of the 31th International Conference on Machine Learning, pages 64\u201372, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014"
        },
        {
            "id": "26",
            "entry": "[26] Ziqiang Shi and Rujie Liu. Large scale optimization with proximal stochastic newton-type gradient descent. In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, pages 691\u2013704, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Ziqiang%20Liu%2C%20Rujie%20Large%20scale%20optimization%20with%20proximal%20stochastic%20newton-type%20gradient%20descent%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Ziqiang%20Liu%2C%20Rujie%20Large%20scale%20optimization%20with%20proximal%20stochastic%20newton-type%20gradient%20descent%202015"
        },
        {
            "id": "27",
            "entry": "[27] Virginia Smith, Simone Forte, Michael I. Jordan, and Martin Jaggi. L1-regularized distributed optimization: A communication-efficient primal-dual framework. CoRR, abs/1512.04011, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.04011"
        },
        {
            "id": "28",
            "entry": "[28] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267\u2013288, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201994"
        },
        {
            "id": "29",
            "entry": "[29] Paul Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109(3):475\u2013494, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tseng%2C%20Paul%20Convergence%20of%20a%20block%20coordinate%20descent%20method%20for%20nondifferentiable%20minimization%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tseng%2C%20Paul%20Convergence%20of%20a%20block%20coordinate%20descent%20method%20for%20nondifferentiable%20minimization%202001"
        },
        {
            "id": "30",
            "entry": "[30] Jialei Wang, Mladen Kolar, Nathan Srebro, and Tong Zhang. Efficient distributed learning with sparsity. In Proceedings of the 34th International Conference on Machine Learning, pages 3636\u20133645, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jialei%20Kolar%2C%20Mladen%20Srebro%2C%20Nathan%20Zhang%2C%20Tong%20Efficient%20distributed%20learning%20with%20sparsity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jialei%20Kolar%2C%20Mladen%20Srebro%2C%20Nathan%20Zhang%2C%20Tong%20Efficient%20distributed%20learning%20with%20sparsity%202017"
        },
        {
            "id": "31",
            "entry": "[31] Tong T. Wu and Kenneth Lange. Coordinate descent algorithms for lasso penalized regression. The Annals of Applied Statistics, 2(1):224\u2013244, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Tong%20T.%20Lange%2C%20Kenneth%20Coordinate%20descent%20algorithms%20for%20lasso%20penalized%20regression%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Tong%20T.%20Lange%2C%20Kenneth%20Coordinate%20descent%20algorithms%20for%20lasso%20penalized%20regression%202008"
        },
        {
            "id": "32",
            "entry": "[32] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Lin%20Zhang%2C%20Tong%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014"
        },
        {
            "id": "33",
            "entry": "[33] Eric P. Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu. Petuum: A new platform for distributed machine learning on big data. In Proceedings of the 21th International Conference on Knowledge Discovery and Data Mining, pages 1335\u20131344, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xing%2C%20Eric%20P.%20Ho%2C%20Qirong%20Dai%2C%20Wei%20Kim%2C%20Jin%20Kyu%20Petuum%3A%20A%20new%20platform%20for%20distributed%20machine%20learning%20on%20big%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xing%2C%20Eric%20P.%20Ho%2C%20Qirong%20Dai%2C%20Wei%20Kim%2C%20Jin%20Kyu%20Petuum%3A%20A%20new%20platform%20for%20distributed%20machine%20learning%20on%20big%20data%202015"
        },
        {
            "id": "34",
            "entry": "[34] Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, and Wu-Jun Li. SCOPE: scalable composite optimization for learning on Spark. In Proceedings of the 31th AAAI Conference on Artificial Intelligence, pages 2928\u20132934, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Shen-Yi%20Xiang%2C%20Ru%20Shi%2C%20Ying-Hao%20Gao%2C%20Peng%20SCOPE%3A%20scalable%20composite%20optimization%20for%20learning%20on%20Spark%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Shen-Yi%20Xiang%2C%20Ru%20Shi%2C%20Ying-Hao%20Gao%2C%20Peng%20SCOPE%3A%20scalable%20composite%20optimization%20for%20learning%20on%20Spark%202017"
        },
        {
            "id": "35",
            "entry": "[35] Shen-Yi Zhao, Gong-Duo Zhang, Ming-Wei Li, and Wu-Jun Li. Proximal SCOPE for distributed sparse learning: Better data partition implies faster convergence rate. CoRR, abs/1803.05621, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05621"
        },
        {
            "id": "36",
            "entry": "[36] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67:301\u2013320, 2005. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20Hui%20Hastie%2C%20Trevor%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005"
        }
    ]
}
