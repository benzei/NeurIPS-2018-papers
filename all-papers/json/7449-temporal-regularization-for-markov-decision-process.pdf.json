{
    "filename": "7449-temporal-regularization-for-markov-decision-process.pdf",
    "metadata": {
        "title": "Temporal Regularization for Markov Decision Process",
        "author": "Pierre Thodoroff, Audrey Durand, Joelle Pineau, Doina Precup",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7449-temporal-regularization-for-markov-decision-process.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization. Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories. This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games."
    },
    "keywords": [
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "decision process",
            "url": "https://en.wikipedia.org/wiki/decision_process"
        },
        {
            "term": "function approximation",
            "url": "https://en.wikipedia.org/wiki/function_approximation"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "markov chain",
            "url": "https://en.wikipedia.org/wiki/markov_chain"
        }
    ],
    "highlights": [
        "There has been much progress in Reinforcement Learning (RL) techniques, with some impressive success with games [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], and several interesting applications on the horizon [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "Reinforcement Learning methods are too often hampered by high variance, whether due to randomness in data collection, effects of initial conditions, complexity of learner function class, hyper-parameter configuration, or sparsity of the reward signal [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We begin by introducing discrete Markov chains concepts that will be used to study the properties of temporally regularized Markov Decision Process",
        "A problem that arises when treating a partially observable Markov Decision Process (POMDP) as a fully observable is that it may no longer be possible to assume that the value function is smooth over the state space [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "Conclusion: This paper tackles the problem of regularization in Reinforcement Learning from a new angle, that is from a temporal perspective",
        "In contrast with typical spatial regularization, where one assumes that rewards are close for nearby states in the state space, temporal regularization rather assumes that rewards are close for states visited closely in time"
    ],
    "key_statements": [
        "There has been much progress in Reinforcement Learning (RL) techniques, with some impressive success with games [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], and several interesting applications on the horizon [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "Reinforcement Learning methods are too often hampered by high variance, whether due to randomness in data collection, effects of initial conditions, complexity of learner function class, hyper-parameter configuration, or sparsity of the reward signal [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We begin by introducing discrete Markov chains concepts that will be used to study the properties of temporally regularized Markov Decision Process",
        "We show how this concept can be extended to exploit information from the entire trajectory by casting temporal regularization as a time series prediction problem",
        "This means that temporal regularization only reweighs the reward on each state based on the Markov reversal, while preserving the average reward",
        "Figure 5 shows the proximity of the estimated state value to the optimal value with and without temporal regularization",
        "Previous experiments (Sec. 5.5), temporal regularization being independent of spatial representation makes it more robust to mis-specification of the state features, which is a challenge in some of these games",
        "Noisy states: Is is often assumed that the full state can be determined, while in practice, the Markov property rarely holds. This is the case, for example, when taking the four last frames to represent the state in Atari games [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "A problem that arises when treating a partially observable Markov Decision Process (POMDP) as a fully observable is that it may no longer be possible to assume that the value function is smooth over the state space [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "The observed features may be similar for two states that are intrinsically different, leading to highly different values for states that are nearby in the state space",
        "Previous experiments on noisy state representation (Sec. 5.5) and on the Atari games (Sec. 5.6) show that temporal regularization provides robustness to those cases. This makes it an appealing technique in real-world environments, where it is harder to provide the agent with the full state",
        "Conclusion: This paper tackles the problem of regularization in Reinforcement Learning from a new angle, that is from a temporal perspective",
        "In contrast with typical spatial regularization, where one assumes that rewards are close for nearby states in the state space, temporal regularization rather assumes that rewards are close for states visited closely in time"
    ],
    "summary": [
        "There has been much progress in Reinforcement Learning (RL) techniques, with some impressive success with games [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], and several interesting applications on the horizon [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>].",
        "We begin by introducing discrete Markov chains concepts that will be used to study the properties of temporally regularized MDPs. A discrete-time Markov chain [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] is defined by a discrete set of states S and a transition function P : S \u00d7 S \u2192 [0, 1] which can be written in matrix form as Pij = P(i|j).",
        "In the policy evaluation setting, the bias between the original value function v\u03c0 and the regularized one v\u03b2\u03c0 can be characterized as a function of the difference between P and its Markov reversal P , weighted by \u03b2 and the reward distribution.",
        "Discounted average reward case: The temporally regularized MDP has the same discounted average reward as the original one as it is possible to define the discounted average reward [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>] as a function of the stationary distribution \u03c0, the reward vector and \u03b3 .",
        "This means that temporal regularization only reweighs the reward on each state based on the Markov reversal, while preserving the average reward.",
        "This first experiment showcases that the underlying Markov chain of a MDP can have a smaller mixing time when temporally regularized.",
        "We are interested in the error when estimating the optimal state value of S1, v\u2217(S1), with and without temporal regularization, denoted v\u03b2\u03c0(S1), v\u03c0(S1), respectively.",
        "We observe that temporal regularization reduces the variance and helps the learning process by making the value function easier to learn.",
        "We illustrate with a simple experiment how temporal regularization allows the information to spread faster among the different states of the MDP.",
        "Figure 5 shows the proximity of the estimated state value to the optimal value with and without temporal regularization.",
        "This suggest that temporal regularization could help agents explore faster by using their prior from the previous visited state for learning the corresponding optimal value faster.",
        "This experiment highlights a case where temporal regularization is effective even in the absence of smoothness in the state space.",
        "Previous experiments (Sec. 5.5), temporal regularization being independent of spatial representation makes it more robust to mis-specification of the state features, which is a challenge in some of these games.",
        "Previous experiments on noisy state representation (Sec. 5.5) and on the Atari games (Sec. 5.6) show that temporal regularization provides robustness to those cases.",
        "The robustness of the proposed approach to noisy state representations and its interesting properties should motivate further work to explore novel ways of exploiting temporal information."
    ],
    "headline": "This paper explores a class of methods for temporal regularization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] L. Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995, pages 30\u201337.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baird%2C%20L.%20Residual%20algorithms%3A%20Reinforcement%20learning%20with%20function%20approximation%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baird%2C%20L.%20Residual%20algorithms%3A%20Reinforcement%20learning%20with%20function%20approximation%201995"
        },
        {
            "id": "2",
            "entry": "[2] P. L. Bartlett and A. Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 35\u201342. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Tewari%2C%20A.%20Regal%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20mdps%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Tewari%2C%20A.%20Regal%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20mdps%202009"
        },
        {
            "id": "3",
            "entry": "[3] J. Baxter and P. L. Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15:319\u2013350, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baxter%2C%20J.%20Bartlett%2C%20P.L.%20Infinite-horizon%20policy-gradient%20estimation%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baxter%2C%20J.%20Bartlett%2C%20P.L.%20Infinite-horizon%20policy-gradient%20estimation%202001"
        },
        {
            "id": "4",
            "entry": "[4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "5",
            "entry": "[5] G. Box, G. M. Jenkins, and G. C. Reinsel. Time Series Analysis: Forecasting and Control (3rd ed.). Prentice-Hall, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Box%2C%20G.%20Jenkins%2C%20G.M.%20Reinsel%2C%20G.C.%20Time%20Series%20Analysis%3A%20Forecasting%20and%20Control%201994"
        },
        {
            "id": "6",
            "entry": "[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20optimization%202004"
        },
        {
            "id": "7",
            "entry": "[7] K.-M. Chung, H. Lam, Z. Liu, and M. Mitzenmacher. Chernoff-hoeffding bounds for markov chains: Generalized and simplified. arXiv preprint arXiv:1201.0559, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1201.0559"
        },
        {
            "id": "8",
            "entry": "[8] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "9",
            "entry": "[9] B. Dhingra, L. Li, X. Li, J. Gao, Y.-N. Chen, F. Ahmed, and L. Deng. Towards end-to-end reinforcement learning of dialogue agents for information access. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 484\u2013495, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhingra%2C%20B.%20Li%2C%20L.%20Li%2C%20X.%20Gao%2C%20J.%20Towards%20end-to-end%20reinforcement%20learning%20of%20dialogue%20agents%20for%20information%20access%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhingra%2C%20B.%20Li%2C%20L.%20Li%2C%20X.%20Gao%2C%20J.%20Towards%20end-to-end%20reinforcement%20learning%20of%20dialogue%20agents%20for%20information%20access%202017"
        },
        {
            "id": "10",
            "entry": "[10] A.-m. Farahmand. Regularization in reinforcement learning. PhD thesis, University of Alberta, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20A.-m%20Regularization%20in%20reinforcement%20learning%202011"
        },
        {
            "id": "11",
            "entry": "[11] A.-m. Farahmand, M. Ghavamzadeh, C. Szepesv\u00e1ri, and S. Mannor. Regularized fitted qiteration for planning in continuous-space markovian decision problems. In American Control Conference, pages 725\u2013730. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farahmand%2C%20A.-m%20Ghavamzadeh%2C%20M.%20Szepesv%C3%A1ri%2C%20C.%20Mannor%2C%20S.%20Regularized%20fitted%20qiteration%20for%20planning%20in%20continuous-space%20markovian%20decision%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farahmand%2C%20A.-m%20Ghavamzadeh%2C%20M.%20Szepesv%C3%A1ri%2C%20C.%20Mannor%2C%20S.%20Regularized%20fitted%20qiteration%20for%20planning%20in%20continuous-space%20markovian%20decision%20problems%202009"
        },
        {
            "id": "12",
            "entry": "[12] E. S. Gardner. Exponential smoothing: The state of the art\u2014part ii. International journal of forecasting, 22(4):637\u2013666, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gardner%2C%20E.S.%20Exponential%20smoothing%3A%20The%20state%20of%20the%20art%E2%80%94part%20ii%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gardner%2C%20E.S.%20Exponential%20smoothing%3A%20The%20state%20of%20the%20art%E2%80%94part%20ii%202006"
        },
        {
            "id": "13",
            "entry": "[13] E. S. Gardner Jr. Exponential smoothing: The state of the art. Journal of forecasting, 4(1):1\u201328, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gardner%2C%20Jr%2C%20E.S.%20Exponential%20smoothing%3A%20The%20state%20of%20the%20art%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gardner%2C%20Jr%2C%20E.S.%20Exponential%20smoothing%3A%20The%20state%20of%20the%20art%201985"
        },
        {
            "id": "14",
            "entry": "[14] C. Harrigan. Deep reinforcement learning with regularized convolutional neural fitted q iteration. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harrigan%2C%20C.%20Deep%20reinforcement%20learning%20with%20regularized%20convolutional%20neural%20fitted%20q%20iteration%202016"
        },
        {
            "id": "15",
            "entry": "[15] P. Henderson, R. Islam, and P. J. P. D. M. D. Bachman, P. Deep reinforcement learning that matters. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henderson%2C%20P.%20Islam%2C%20R.%20Bachman%2C%20P.J.P.D.M.D.%20P.%20Deep%20reinforcement%20learning%20that%20matters%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henderson%2C%20P.%20Islam%2C%20R.%20Bachman%2C%20P.J.P.D.M.D.%20P.%20Deep%20reinforcement%20learning%20that%20matters%202018"
        },
        {
            "id": "16",
            "entry": "[16] J. G. Kemeny and J. L. Snell. Finite markov chains, undergraduate texts in mathematics. 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kemeny%2C%20J.G.%20Snell%2C%20J.L.%20Finite%20markov%20chains%2C%20undergraduate%20texts%20in%20mathematics%201976"
        },
        {
            "id": "17",
            "entry": "[17] K. Koedinger, E. Brunskill, R. Baker, and E. McLaughlin. New potentials for data-driven intelligent tutoring system development and optimization. AAAI magazine, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koedinger%2C%20K.%20Brunskill%2C%20E.%20Baker%2C%20R.%20McLaughlin%2C%20E.%20New%20potentials%20for%20data-driven%20intelligent%20tutoring%20system%20development%20and%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koedinger%2C%20K.%20Brunskill%2C%20E.%20Baker%2C%20R.%20McLaughlin%2C%20E.%20New%20potentials%20for%20data-driven%20intelligent%20tutoring%20system%20development%20and%20optimization%202018"
        },
        {
            "id": "18",
            "entry": "[18] V. S. Laroche. In reinforcement learning, all objective functions are not equal. ICLR Workshop, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laroche%2C%20V.S.%20In%20reinforcement%20learning%2C%20all%20objective%20functions%20are%20not%20equal%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laroche%2C%20V.S.%20In%20reinforcement%20learning%2C%20all%20objective%20functions%20are%20not%20equal%202018"
        },
        {
            "id": "19",
            "entry": "[19] D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levin%2C%20D.A.%20Peres%2C%20Y.%20Markov%20chains%20and%20mixing%20times%2C%20volume%20107%202008"
        },
        {
            "id": "20",
            "entry": "[20] L. Li. A worst-case comparison between temporal difference and residual gradient with linear function approximation. In Proceedings of the 25th international conference on machine learning, pages 560\u2013567. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20A%20worst-case%20comparison%20between%20temporal%20difference%20and%20residual%20gradient%20with%20linear%20function%20approximation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20A%20worst-case%20comparison%20between%20temporal%20difference%20and%20residual%20gradient%20with%20linear%20function%20approximation%202008"
        },
        {
            "id": "21",
            "entry": "[21] B. Liu, S. Mahadevan, and J. Liu. Regularized off-policy td-learning. In Advances in Neural Information Processing Systems, pages 836\u2013844, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20B.%20Mahadevan%2C%20S.%20Liu%2C%20J.%20Regularized%20off-policy%20td-learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20B.%20Mahadevan%2C%20S.%20Liu%2C%20J.%20Regularized%20off-policy%20td-learning%202012"
        },
        {
            "id": "22",
            "entry": "[22] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "23",
            "entry": "[23] G. Neu, A. Jonsson, and V. G\u00f3mez. A unified view of entropy-regularized markov decision processes. arXiv preprint arXiv:1705.07798, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07798"
        },
        {
            "id": "24",
            "entry": "[24] J. Pazis and R. Parr. Non-parametric approximate linear programming for mdps. In AAAI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pazis%2C%20J.%20Parr%2C%20R.%20Non-parametric%20approximate%20linear%20programming%20for%20mdps%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pazis%2C%20J.%20Parr%2C%20R.%20Non-parametric%20approximate%20linear%20programming%20for%20mdps%202011"
        },
        {
            "id": "25",
            "entry": "[25] M. Petrik, G. Taylor, R. Parr, and S. Zilberstein. Feature selection using regularization in approximate linear programs for markov decision processes. arXiv preprint arXiv:1005.1860, 2010.",
            "arxiv_url": "https://arxiv.org/pdf/1005.1860"
        },
        {
            "id": "26",
            "entry": "[26] N. Prasad, L. Cheng, C. Chivers, M. Draugelis, and B. Engelhardt. A reinforcement learning approach to weaning of mechanical ventilation in intensive care units. In UAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prasad%2C%20N.%20Cheng%2C%20L.%20Chivers%2C%20C.%20Draugelis%2C%20M.%20A%20reinforcement%20learning%20approach%20to%20weaning%20of%20mechanical%20ventilation%20in%20intensive%20care%20units%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prasad%2C%20N.%20Cheng%2C%20L.%20Chivers%2C%20C.%20Draugelis%2C%20M.%20A%20reinforcement%20learning%20approach%20to%20weaning%20of%20mechanical%20ventilation%20in%20intensive%20care%20units%202017"
        },
        {
            "id": "27",
            "entry": "[27] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20M.L.%20Markov%20decision%20processes%3A%20discrete%20stochastic%20dynamic%20programming%201994"
        },
        {
            "id": "28",
            "entry": "[28] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "29",
            "entry": "[29] S. Shortreed, E. Laber, D. Lizotte, S. Stroup, J. Pineau, and S. Murphy. Informing sequential clinical decision-making through reinforcement learning: an empirical study. Machine Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shortreed%2C%20S.%20Laber%2C%20E.%20Lizotte%2C%20D.%20Stroup%2C%20S.%20Informing%20sequential%20clinical%20decision-making%20through%20reinforcement%20learning%3A%20an%20empirical%20study%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shortreed%2C%20S.%20Laber%2C%20E.%20Lizotte%2C%20D.%20Stroup%2C%20S.%20Informing%20sequential%20clinical%20decision-making%20through%20reinforcement%20learning%3A%20an%20empirical%20study%202011"
        },
        {
            "id": "30",
            "entry": "[30] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "31",
            "entry": "[31] S. P. Singh, T. Jaakkola, and M. I. Jordan. Learning without state-estimation in partially observable markovian decision processes. In Machine Learning Proceedings 1994, pages 284\u2013292.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20S.P.%20Jaakkola%2C%20T.%20Jordan%2C%20M.I.%20Learning%20without%20state-estimation%20in%20partially%20observable%20markovian%20decision%20processes%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20S.P.%20Jaakkola%2C%20T.%20Jordan%2C%20M.I.%20Learning%20without%20state-estimation%20in%20partially%20observable%20markovian%20decision%20processes%201994"
        },
        {
            "id": "32",
            "entry": "[32] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press Cambridge, 1st edition, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "33",
            "entry": "[33] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press Cambridge, (in progress) 2nd edition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%202017"
        },
        {
            "id": "34",
            "entry": "[34] R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, C. Szepesv\u00e1ri, and E. Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 993\u20131000. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Maei%2C%20H.R.%20Precup%2C%20D.%20Bhatnagar%2C%20S.%20Fast%20gradient-descent%20methods%20for%20temporal-difference%20learning%20with%20linear%20function%20approximation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20R.S.%20Maei%2C%20H.R.%20Precup%2C%20D.%20Bhatnagar%2C%20S.%20Fast%20gradient-descent%20methods%20for%20temporal-difference%20learning%20with%20linear%20function%20approximation%202009"
        },
        {
            "id": "35",
            "entry": "[35] J. N. Tsitsiklis and B. Van Roy. On average versus discounted reward temporal-difference learning. Machine Learning, 49(2-3):179\u2013191, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20J.N.%20Roy%2C%20B.Van%20On%20average%20versus%20discounted%20reward%20temporal-difference%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20J.N.%20Roy%2C%20B.Van%20On%20average%20versus%20discounted%20reward%20temporal-difference%20learning%202002"
        },
        {
            "id": "36",
            "entry": "[36] Z. Xu, J. Modayil, H. P. van Hasselt, A. Barreto, D. Silver, and T. Schaul. Natural value approximators: Learning when to trust past estimates. In Advances in Neural Information Processing Systems, pages 2117\u20132125, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Z.%20Modayil%2C%20J.%20van%20Hasselt%2C%20H.P.%20Barreto%2C%20A.%20Natural%20value%20approximators%3A%20Learning%20when%20to%20trust%20past%20estimates%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Z.%20Modayil%2C%20J.%20van%20Hasselt%2C%20H.P.%20Barreto%2C%20A.%20Natural%20value%20approximators%3A%20Learning%20when%20to%20trust%20past%20estimates%202017"
        }
    ]
}
