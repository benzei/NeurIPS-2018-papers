{
    "filename": "7836-memory-replay-gans-learning-to-generate-new-categories-without-forgetting.pdf",
    "metadata": {
        "title": "Memory Replay GANs: Learning to Generate New Categories without Forgetting",
        "author": "Chenshen Wu, Luis Herranz, Xialei Liu, yaxing wang, Joost van de Weijer, Bogdan Raducanu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7836-memory-replay-gans-learning-to-generate-new-categories-without-forgetting.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories. 1"
    },
    "keywords": [
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "semantic property",
            "url": "https://en.wikipedia.org/wiki/semantic_property"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] are a popular framework for image generation due to their capability to learn a mapping between a low-dimensional latent space and a complex distribution of interest, such as natural images",
        "We implemented two additional methods based on related works: the adaptation of elastic weight consolidation to conditional Generative adversarial networks proposed by [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], and the deep generative replay (DGR) module of [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], implemented as an unconditional Generative adversarial networks followed by a classifier to predict the label",
        "elastic weight consolidation performs worse than deep generative replay since it does not leverage replays, it significantly mitigates the phenomenon of catastrophic forgetting by increasing the accuracy from 19.87 to 70.62 on MNIST, and from 19.35 to 39.84 on SVHN compared to sequential fine tuning in the case of 5 tasks",
        "We have studied the problem of sequential learning in the context of image generation with Generative adversarial networks, where the main challenge is to effectively address catastrophic forgetting",
        "Our results show their effectiveness in retaining the ability to generate competitive images of previous tasks even after learning several new ones",
        "We showed that image generation provides an interesting way to visualize the interference between tasks and potential forgetting by directly observing generated images"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] are a popular framework for image generation due to their capability to learn a mapping between a low-dimensional latent space and a complex distribution of interest, such as natural images",
        "While previous works study forgetting in discriminative tasks, in this paper we focus on forgetting in generative models (GANs in particular) through the problem of generating images when categories are presented sequentially as disjoint tasks",
        "LDGAN + \u03bbCLSLCDLS. This method could be related to the deep generative replay in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], where the authors use an unconditional Generative adversarial networks and the category is predicted with a classifier",
        "We evaluated the two variants of the proposed memory replay Generative adversarial networks: joint training with replay (MeRGAN-JTR) and replay alignment (MeRGAN-RA)",
        "As upper and lower bounds we evaluated joint training (JT) with all data and sequential fine tuning (SFT)",
        "We implemented two additional methods based on related works: the adaptation of elastic weight consolidation to conditional Generative adversarial networks proposed by [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], and the deep generative replay (DGR) module of [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], implemented as an unconditional Generative adversarial networks followed by a classifier to predict the label",
        "elastic weight consolidation performs worse than deep generative replay since it does not leverage replays, it significantly mitigates the phenomenon of catastrophic forgetting by increasing the accuracy from 19.87 to 70.62 on MNIST, and from 19.35 to 39.84 on SVHN compared to sequential fine tuning in the case of 5 tasks",
        "We have studied the problem of sequential learning in the context of image generation with Generative adversarial networks, where the main challenge is to effectively address catastrophic forgetting",
        "Our results show their effectiveness in retaining the ability to generate competitive images of previous tasks even after learning several new ones",
        "We showed that image generation provides an interesting way to visualize the interference between tasks and potential forgetting by directly observing generated images"
    ],
    "summary": [
        "Generative adversarial networks (GANs) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] are a popular framework for image generation due to their capability to learn a mapping between a low-dimensional latent space and a complex distribution of interest, such as natural images.",
        ", M ), each of them corresponding to learning to generate images from a new training set St. For simplicity, we restrict each St to contain only images from a particular category c, i.e. t = c.",
        "When the network learns to adjust its parameters to generate images of the new domain via gradient descent, that very drifting away from the original solution for the previous task will cause catastrophic forgetting [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "The elastic weight consolidation (EWC) regularization [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] has been adapted to prevent forgetting in GANs [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] and included as an augmented objective when training the generator as min \u03b8tG",
        "This method could be related to the deep generative replay in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], where the authors use an unconditional GAN and the category is predicted with a classifier.",
        "In contrast to the previous method, in this case the discriminator is only trained with images of the current task, and there is no classification task.",
        "Spatial alignment Image generation is a one-to-many task with many latent factors of variability that can result in completely different images yet sharing the same input category.",
        "We implemented two additional methods based on related works: the adaptation of EWC to conditional GANs proposed by [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], and the deep generative replay (DGR) module of [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], implemented as an unconditional GAN followed by a classifier to predict the label.",
        "Figure 3 compares the images generated by the different methods after sequentially training the ten tasks.",
        "EWC performs worse than DGR since it does not leverage replays, it significantly mitigates the phenomenon of catastrophic forgetting by increasing the accuracy from 19.87 to 70.62 on MNIST, and from 19.35 to 39.84 on SVHN compared to SFT in the case of 5 tasks.",
        "The architectures are based on [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] with 18-layer ResNet generator and discriminator, and for every batch of training data for the new category we generate a batch of replayed images per category.",
        "Each block column corresponds to a different method, and inside, each row shows images generated for a particular condition and each column corresponds to images generated after learning a particular task, using the same latent vector.",
        "We have studied the problem of sequential learning in the context of image generation with GANs, where the main challenge is to effectively address catastrophic forgetting.",
        "Our results show their effectiveness in retaining the ability to generate competitive images of previous tasks even after learning several new ones.",
        "We showed that image generation provides an interesting way to visualize the interference between tasks and potential forgetting by directly observing generated images"
    ],
    "headline": "We investigate generative adversarial networks  in the task of learning new categories in a sequential fashion",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "2",
            "entry": "[2] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "3",
            "entry": "[3] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20Vincent%20Shlens%2C%20Jonathon%20Kudlur%2C%20Manjunath%20A%20learned%20representation%20for%20artistic%20style%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20Vincent%20Shlens%2C%20Jonathon%20Kudlur%2C%20Manjunath%20A%20learned%20representation%20for%20artistic%20style%202017"
        },
        {
            "id": "4",
            "entry": "[4] James Kirkpatrick et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America, 14(13):3521\u20133526, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20James%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20James%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Steffen Gais et al. Sleep transforms the cerebral trace of declarative memories. Proceedings of the National Academy of Sciences, 104(47):18778\u201318783, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gais%2C%20Steffen%20Sleep%20transforms%20the%20cerebral%20trace%20of%20declarative%20memories%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gais%2C%20Steffen%20Sleep%20transforms%20the%20cerebral%20trace%20of%20declarative%20memories%202007"
        },
        {
            "id": "6",
            "entry": "[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "7",
            "entry": "[7] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "8",
            "entry": "[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20nash%20equilibrium%202017"
        },
        {
            "id": "9",
            "entry": "[9] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20growing%20of%20gans%20for%20improved%20quality%2C%20stability%2C%20and%20variation%202018"
        },
        {
            "id": "10",
            "entry": "[10] Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kemker%2C%20Ronald%20Kanan%2C%20Christopher%20Fearnet%3A%20Brain-inspired%20model%20for%20incremental%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kemker%2C%20Ronald%20Kanan%2C%20Christopher%20Fearnet%3A%20Brain-inspired%20model%20for%20incremental%20learning%202018"
        },
        {
            "id": "11",
            "entry": "[11] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "12",
            "entry": "[12] Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "13",
            "entry": "[13] Zhizhong Li and Derek Hoiem. Learning without forgetting. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20without%20forgetting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Zhizhong%20Hoiem%2C%20Derek%20Learning%20without%20forgetting%202016"
        },
        {
            "id": "14",
            "entry": "[14] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov. Rotate your networks: Better weight consolidation and less catastrophic forgetting. In ICPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Xialei%20Masana%2C%20Marc%20Herranz%2C%20Luis%20de%20Weijer%2C%20Joost%20Van%20Rotate%20your%20networks%3A%20Better%20weight%20consolidation%20and%20less%20catastrophic%20forgetting%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Xialei%20Masana%2C%20Marc%20Herranz%2C%20Luis%20de%20Weijer%2C%20Joost%20Van%20Rotate%20your%20networks%3A%20Better%20weight%20consolidation%20and%20less%20catastrophic%20forgetting%202018"
        },
        {
            "id": "15",
            "entry": "[15] David Lopez-Paz et al. Gradient episodic memory for continual learning. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20David%20Gradient%20episodic%20memory%20for%20continual%20learning%202017"
        },
        {
            "id": "16",
            "entry": "[16] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "17",
            "entry": "[17] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. The psychology of learning and motivatio, 24:109\u2013165, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCloskey%2C%20Michael%20Cohen%2C%20Neal%20J.%20Catastrophic%20interference%20in%20connectionist%20networks%3A%20The%20sequential%20learning%20problem.%20The%20psychology%20of%20learning%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCloskey%2C%20Michael%20Cohen%2C%20Neal%20J.%20Catastrophic%20interference%20in%20connectionist%20networks%3A%20The%20sequential%20learning%20problem.%20The%20psychology%20of%20learning%201989"
        },
        {
            "id": "18",
            "entry": "[18] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "19",
            "entry": "[19] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "20",
            "entry": "[20] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Olah%2C%20Christopher%20Shlens%2C%20Jonathon%20Conditional%20image%20synthesis%20with%20auxiliary%20classifier%20gans%202016"
        },
        {
            "id": "21",
            "entry": "[21] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "22",
            "entry": "[22] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Cristoph H. Lampert. icarl: Incremental classifier and representation learning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20H%2C%20Cristoph%20Lampert.%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20Sperl%2C%20Georg%20H%2C%20Cristoph%20Lampert.%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017"
        },
        {
            "id": "23",
            "entry": "[23] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123\u2013 146, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robins%2C%20Anthony%20Catastrophic%20forgetting%2C%20rehearsal%20and%20pseudorehearsal%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robins%2C%20Anthony%20Catastrophic%20forgetting%2C%20rehearsal%20and%20pseudorehearsal%201995"
        },
        {
            "id": "24",
            "entry": "[24] Ari Seff, Alex Beatson, Daniel Suo, and Han Liu. Continual learning in generative adversarial nets. arXiv prepprint arXiv:1705.08395v1, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08395v1"
        },
        {
            "id": "25",
            "entry": "[25] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shin%2C%20Hanul%20Lee%2C%20Jung%20Kwon%20Kim%2C%20Jaehong%20Kim%2C%20Jiwon%20Continual%20learning%20with%20deep%20generative%20replay%202017"
        },
        {
            "id": "26",
            "entry": "[26] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "27",
            "entry": "[27] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring GANs: generating images from limited data. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Yaxing%20Wu%2C%20Chenshen%20Herranz%2C%20Luis%20van%20de%20Weijer%2C%20Joost%20Transferring%20GANs%3A%20generating%20images%20from%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Yaxing%20Wu%2C%20Chenshen%20Herranz%2C%20Luis%20van%20de%20Weijer%2C%20Joost%20Transferring%20GANs%3A%20generating%20images%20from%202018"
        },
        {
            "id": "28",
            "entry": "[28] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. ",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        }
    ]
}
