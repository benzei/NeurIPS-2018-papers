{
    "filename": "8110-glomo-unsupervised-learning-of-transferable-relational-graphs.pdf",
    "metadata": {
        "title": "GLoMo: Unsupervised Learning of Transferable Relational Graphs",
        "author": "Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Ruslan R. Salakhutdinov, Yann LeCun",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8110-glomo-unsupervised-learning-of-transferable-relational-graphs.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels."
    },
    "keywords": [
        {
            "term": "word embedding",
            "url": "https://en.wikipedia.org/wiki/word_embedding"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "sentiment analysis",
            "url": "https://en.wikipedia.org/wiki/sentiment_analysis"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Recent advances in deep learning have largely relied on building blocks such as convolutional networks (CNNs) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and recurrent networks (RNNs) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] augmented with attention mechanisms [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "convolutional networks and recurrent networks largely rely on high expressiveness to model complex structural phenomena, compensating the fact that they do not explicitly leverage structural, graphical representations",
        "This paradigm has led to a standardized norm in transfer learning and pretraining\u2014fitting an expressive function on a large dataset with or without supervision, and applying the function to downstream task data for feature extraction",
        "We demonstrate that the learned graphs are generic enough to work with various sets of features on which the graphs have not been trained, including GloVe embeddings [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], ELMo embeddings [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], and taskspecific recurrent networks states",
        "We present a novel transfer learning scheme based on latent relational graph learning, which is orthogonal to but can be combined with the traditional feature transfer learning framework"
    ],
    "key_statements": [
        "Recent advances in deep learning have largely relied on building blocks such as convolutional networks (CNNs) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and recurrent networks (RNNs) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] augmented with attention mechanisms [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "convolutional networks and recurrent networks largely rely on high expressiveness to model complex structural phenomena, compensating the fact that they do not explicitly leverage structural, graphical representations",
        "This paradigm has led to a standardized norm in transfer learning and pretraining\u2014fitting an expressive function on a large dataset with or without supervision, and applying the function to downstream task data for feature extraction",
        "We demonstrate that the learned graphs are generic enough to work with various sets of features on which the graphs have not been trained, including GloVe embeddings [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], ELMo embeddings [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], and taskspecific recurrent networks states",
        "To demonstrate the generality of our framework, we further show improved results on image classification by applying GLoMo to model the relational dependencies between the pixels.\n2 Unsupervised Relational Graph Learning",
        "Sentiment Analysis We use the movie review dataset collected in [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], with 25,000 training and 25,000 testing samples crawled from IMDB",
        "The learned graphs are generic enough to work with various sets of features, including GloVe embeddings, ELMo embeddings, and recurrent networks output.\n3.3",
        "In this paper we explore the use of unsupervised learning objectives for discovering such structures",
        "We present a novel transfer learning scheme based on latent relational graph learning, which is orthogonal to but can be combined with the traditional feature transfer learning framework"
    ],
    "summary": [
        "Recent advances in deep learning have largely relied on building blocks such as convolutional networks (CNNs) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and recurrent networks (RNNs) [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] augmented with attention mechanisms [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "The task of latent relational graph learning is to learn an affinity matrix where the weights capture the dependencies between any pair of input units.",
        "We demonstrate that the learned graphs are generic enough to work with various sets of features on which the graphs have not been trained, including GloVe embeddings [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], ELMo embeddings [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], and taskspecific RNN states.",
        "The feature predictor network f takes the graphs G and the original input x to perform a predictive task.",
        "Given an input x from a downstream task, we use the graph predictor g to extract graphs G = g(x ).",
        "This setup of latent graph transfer is general and easy to be plugged in, as the graphs can be applied to any layer in the network architecture, with either learned or pretrained features H, at variable length.",
        "We experimented with applying the transferred graphs to various sets of features, including GloVe embeddings, ELMo embeddings, and the first RNN layer\u2019s output.",
        "We focus on competitive baselines with general architectures that the SOTA models are based on to test the graph transfer performance and exclude independent influence factors.",
        "The learned graphs are generic enough to work with various sets of features, including GloVe embeddings, ELMo embeddings, and RNN output.",
        "We experimented with coupling the two networks, removing the ReLU activations, using only a single layer of graphs, using a sentence-level Skip-thought objective [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and reducing the context length to one [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>].",
        "Drawing from natural language graph predictor g(\u00b7) leads the unsupervised training phase in vision domain to a PixelCNN-like setup [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], but with a sequence prediction window",
        "In the image domain it is standard practice to use features learned in a supervised manner on the ImageNet [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] dataset for other downstream prediction tasks [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>].",
        "Our work is related to these methods, but our goal is to learn a universal structure using an unsupervised objective and transfer it for use with various supervised tasks.",
        "Through a variety of experiments in language and vision, this framework is demonstrated to be capable of improving performance and learning generic graphs applicable to various types of features.",
        "We hope to extend the framework to more diverse setups such as knowledge based inference, video modeling, and hierarchical reinforcement learning where rich graph-like structures abound."
    ],
    "headline": "We show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained , or embedding-free units such as image pixels",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "2",
            "entry": "[2] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02167"
        },
        {
            "id": "3",
            "entry": "[3] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.01261"
        },
        {
            "id": "4",
            "entry": "[4] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.05326"
        },
        {
            "id": "5",
            "entry": "[5] Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1657\u20131668, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Qian%20Zhu%2C%20Xiaodan%20Ling%2C%20Zhen-Hua%20Wei%2C%20Si%20Enhanced%20lstm%20for%20natural%20language%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Qian%20Zhu%2C%20Xiaodan%20Ling%2C%20Zhen-Hua%20Wei%2C%20Si%20Enhanced%20lstm%20for%20natural%20language%20inference%202017"
        },
        {
            "id": "6",
            "entry": "[6] Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Unsupervised learning of task-specific tree structures with tree-lstms. AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20Jihun%20Yoo%2C%20Kang%20Min%20Lee%2C%20Sang-goo%20Unsupervised%20learning%20of%20task-specific%20tree%20structures%20with%20tree-lstms%202018"
        },
        {
            "id": "7",
            "entry": "[7] Noam Chomsky. Aspects of the Theory of Syntax, volume 11. MIT press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chomsky%2C%20Noam%20Aspects%20of%20the%20Theory%20of%20Syntax%2C%20volume%2011%202014"
        },
        {
            "id": "8",
            "entry": "[8] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.3555"
        },
        {
            "id": "9",
            "entry": "[9] Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. arXiv preprint arXiv:1710.10723, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10723"
        },
        {
            "id": "10",
            "entry": "[10] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. Proceedings of the 2017 conference on empirical methods in natural language processing (EMNLP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Schwenk%2C%20Holger%20Barrault%2C%20Loic%20Supervised%20learning%20of%20universal%20sentence%20representations%20from%20natural%20language%20inference%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conneau%2C%20Alexis%20Kiela%2C%20Douwe%20Schwenk%2C%20Holger%20Barrault%2C%20Loic%20Supervised%20learning%20of%20universal%20sentence%20representations%20from%20natural%20language%20inference%20data%202017"
        },
        {
            "id": "11",
            "entry": "[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "12",
            "entry": "[12] Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for reasoning over multiple mentions using coreference. NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhingra%2C%20Bhuwan%20Jin%2C%20Qiao%20Yang%2C%20Zhilin%20Cohen%2C%20William%20W.%20Neural%20models%20for%20reasoning%20over%20multiple%20mentions%20using%20coreference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhingra%2C%20Bhuwan%20Jin%2C%20Qiao%20Yang%2C%20Zhilin%20Cohen%2C%20William%20W.%20Neural%20models%20for%20reasoning%20over%20multiple%20mentions%20using%20coreference%202018"
        },
        {
            "id": "13",
            "entry": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "14",
            "entry": "[14] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "15",
            "entry": "[15] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01507"
        },
        {
            "id": "16",
            "entry": "[16] Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, and Noah A Smith. Dynamic entity representations in neural language models. arXiv preprint arXiv:1708.00781, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.00781"
        },
        {
            "id": "17",
            "entry": "[17] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04687"
        },
        {
            "id": "18",
            "entry": "[18] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015"
        },
        {
            "id": "19",
            "entry": "[19] Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Convolutional%20networks%20for%20images%2C%20speech%2C%20and%20time%20series%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Convolutional%20networks%20for%20images%2C%20speech%2C%20and%20time%20series%201995"
        },
        {
            "id": "20",
            "entry": "[20] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.10198"
        },
        {
            "id": "21",
            "entry": "[21] Xiaodong Liu, Kevin Duh, and Jianfeng Gao. Stochastic answer networks for natural language inference. arXiv preprint arXiv:1804.07888, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07888"
        },
        {
            "id": "22",
            "entry": "[22] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maas%2C%20Andrew%20L.%20Daly%2C%20Raymond%20E.%20Pham%2C%20Peter%20T.%20Huang%2C%20Dan%20Learning%20word%20vectors%20for%20sentiment%20analysis%202011-06"
        },
        {
            "id": "23",
            "entry": "[23] Jean Maillard, Stephen Clark, and Dani Yogatama. Jointly learning sentence embeddings and syntax with unsupervised tree-lstms. arXiv preprint arXiv:1705.09189, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09189"
        },
        {
            "id": "24",
            "entry": "[24] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "25",
            "entry": "[25] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semisupervised text classification. arXiv preprint arXiv:1605.07725, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07725"
        },
        {
            "id": "26",
            "entry": "[26] Renato Negrinho and Geoff Gordon. Deeparchitect: Automatically designing and training deep architectures. arXiv preprint arXiv:1704.08792, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08792"
        },
        {
            "id": "27",
            "entry": "[27] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "28",
            "entry": "[28] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, \u0141ukasz Kaiser, Noam Shazeer, and Alexander Ku. Image transformer. arXiv preprint arXiv:1802.05751, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05751"
        },
        {
            "id": "29",
            "entry": "[29] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "30",
            "entry": "[30] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05365"
        },
        {
            "id": "31",
            "entry": "[31] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.05250"
        },
        {
            "id": "32",
            "entry": "[32] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512\u2013519. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Razavian%2C%20Ali%20Sharif%20Azizpour%2C%20Hossein%20Sullivan%2C%20Josephine%20Carlsson%2C%20Stefan%20Cnn%20features%20off-the-shelf%3A%20an%20astounding%20baseline%20for%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Razavian%2C%20Ali%20Sharif%20Azizpour%2C%20Hossein%20Sullivan%2C%20Josephine%20Carlsson%2C%20Stefan%20Cnn%20features%20off-the-shelf%3A%20an%20astounding%20baseline%20for%20recognition%202014"
        },
        {
            "id": "33",
            "entry": "[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015"
        },
        {
            "id": "34",
            "entry": "[34] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.05517"
        },
        {
            "id": "35",
            "entry": "[35] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20Franco%20Gori%2C%20Marco%20Tsoi%2C%20Ah%20Chung%20Hagenbuchner%2C%20Markus%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "36",
            "entry": "[36] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. Disan: Directional self-attention network for rnn/cnn-free language understanding. arXiv preprint arXiv:1709.04696, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04696"
        },
        {
            "id": "37",
            "entry": "[37] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "38",
            "entry": "[38] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 129\u2013136, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrew%20Y%20Ng.%20Parsing%20natural%20scenes%20and%20natural%20language%20with%20recursive%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrew%20Y%20Ng.%20Parsing%20natural%20scenes%20and%20natural%20language%20with%20recursive%20neural%20networks%202011"
        },
        {
            "id": "39",
            "entry": "[39] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Chuang%2C%20Jason%20Andrew%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013"
        },
        {
            "id": "40",
            "entry": "[40] Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. Linguistically-informed self-attention for semantic role labeling. arXiv preprint arXiv:1804.08199, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08199"
        },
        {
            "id": "41",
            "entry": "[41] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.00075"
        },
        {
            "id": "42",
            "entry": "[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "43",
            "entry": "[43] Patrick Verga, Emma Strubell, Ofer Shai, and Andrew McCallum. Attending to all mention pairs for full abstract biological relation extraction. arXiv preprint arXiv:1710.08312, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.08312"
        },
        {
            "id": "44",
            "entry": "[44] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. arXiv preprint arXiv:1711.07971, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07971"
        },
        {
            "id": "45",
            "entry": "[45] Adina Williams, Andrew Drozdov, and Samuel R Bowman. Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association for Computational Linguistics, 6:253\u2013267, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Adina%20Drozdov%2C%20Andrew%20Bowman%2C%20Samuel%20R.%20Do%20latent%20tree%20learning%20models%20identify%20meaningful%20structure%20in%20sentences%3F%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Adina%20Drozdov%2C%20Andrew%20Bowman%2C%20Samuel%20R.%20Do%20latent%20tree%20learning%20models%20identify%20meaningful%20structure%20in%20sentences%3F%202018"
        },
        {
            "id": "46",
            "entry": "[46] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05426"
        },
        {
            "id": "47",
            "entry": "[47] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: a high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.03953"
        },
        {
            "id": "48",
            "entry": "[48] Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to compose words into sentences with reinforcement learning. ICLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yogatama%2C%20Dani%20Blunsom%2C%20Phil%20Dyer%2C%20Chris%20Grefenstette%2C%20Edward%20Learning%20to%20compose%20words%20into%20sentences%20with%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yogatama%2C%20Dani%20Blunsom%2C%20Phil%20Dyer%2C%20Chris%20Grefenstette%2C%20Edward%20Learning%20to%20compose%20words%20into%20sentences%20with%20reinforcement%20learning%202016"
        },
        {
            "id": "49",
            "entry": "[49] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.09541"
        },
        {
            "id": "50",
            "entry": "[50] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "51",
            "entry": "[51] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        }
    ]
}
