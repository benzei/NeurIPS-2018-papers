{
    "filename": "8217-a-spectral-view-of-adversarially-robust-features.pdf",
    "metadata": {
        "title": "A Spectral View of Adversarially Robust Features",
        "author": "Shivam Garg, Vatsal Sharan, Brian Zhang, Gregory Valiant",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8217-a-spectral-view-of-adversarially-robust-features.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Given the apparent difficulty of learning models that are robust to adversarial perturbations, we propose tackling the simpler problem of developing adversarially robust features. Specifically, given a dataset and metric of interest, the goal is to return a function (or multiple functions) that 1) is robust to adversarial perturbations, and 2) has significant variation across the datapoints. We establish strong connections between adversarially robust features and a natural spectral property of the geometry of the dataset and metric of interest. This connection can be leveraged to provide both robust features, and a lower bound on the robustness of any function that has significant variance across the dataset. Finally, we provide empirical evidence that the adversarially robust features given by this spectral approach can be fruitfully leveraged to learn a robust (and accurate) model."
    },
    "keywords": [
        {
            "term": "spectral graph theory",
            "url": "https://en.wikipedia.org/wiki/spectral_graph_theory"
        },
        {
            "term": "physical world",
            "url": "https://en.wikipedia.org/wiki/physical_world"
        }
    ],
    "highlights": [
        "The lower bound suggests a fundamental connection between the spectral properties of the graph obtained from the dataset, and the inherent extent to which the data supports adversarial robustness",
        "Considering this simpler question of understanding robust features might yield important insights into the geometry of datasets, and the specific metrics under which the robustness is being considered For example, by providing a lower bound on the robustness of any function, we trivially obtain a lower bound on the robustness of any classification model.\n1.1",
        "We show a synthetic setting in which some of the existing models such as neural networks, and nearest-neighbor classifier are known to be vulnerable to adversarial perturbations, while our approach provably works well",
        "In Section 3, we show a lower bound which, in certain parameter regimes, implies that if our spectral features are not robust, no robust features exist",
        "We considered the task of learning adversarially robust features as a simplification of the more common goal of learning adversarially robust classifiers",
        "We showed that this task has a natural connection to spectral graph theory, and that spectral properties of a graph associated to the underlying data have implications for the robustness of any feature learned on the data"
    ],
    "key_statements": [
        "The lower bound suggests a fundamental connection between the spectral properties of the graph obtained from the dataset, and the inherent extent to which the data supports adversarial robustness",
        "Considering this simpler question of understanding robust features might yield important insights into the geometry of datasets, and the specific metrics under which the robustness is being considered For example, by providing a lower bound on the robustness of any function, we trivially obtain a lower bound on the robustness of any classification model.\n1.1",
        "We show a synthetic setting in which some of the existing models such as neural networks, and nearest-neighbor classifier are known to be vulnerable to adversarial perturbations, while our approach provably works well",
        "In Section 3, we show a lower bound which, in certain parameter regimes, implies that if our spectral features are not robust, no robust features exist",
        "We construct a robust feature FX using the second eigenvector of the Laplacian of a graph corresponding to X, defined in terms of the metric in question",
        "We argue that our method, yields a near-perfect classifier\u2014one that makes almost no errors on natural or adversarial examples\u2014even when trained on a modest amount of data",
        "We considered the task of learning adversarially robust features as a simplification of the more common goal of learning adversarially robust classifiers",
        "We showed that this task has a natural connection to spectral graph theory, and that spectral properties of a graph associated to the underlying data have implications for the robustness of any feature learned on the data"
    ],
    "summary": [
        "The lower bound suggests a fundamental connection between the spectral properties of the graph obtained from the dataset, and the inherent extent to which the data supports adversarial robustness.",
        "We construct a robust feature FX using the second eigenvector of the Laplacian of a graph corresponding to X, defined in terms of the metric in question.",
        "Theorem 1 essentially guarantees that the features, as defined above, are robust up to sign-flip, as long as the eigengap between the second and third eigenvalues is large, and the second eigenvalue does not change significantly if we slightly perturb the distance threshold used to determine whether an edge exists in the graph in question.",
        "This bound is very similar to bound obtained in Theorem 1, and says that the function f is robust, as long as the eigengap between the second and third eigenvalues is sufficiently large for G(X) and G\u2212(x), and the second eigenvalue does not change significantly if we slightly perturb the distance threshold used to determine whether an edge exists in the graph in question.",
        "If we choose distance threshold 2 + 2\u03b5 in our spectral approach, the resulting graph will, with high probability, contain a single large connected component of size N for the inner sphere, and N isolated points on the outer sphere that are connected to nothing.",
        "Pick initial distance threshold T = 2+2\u03b5 in the 2 norm, and use the first N nontrivial eigenvectors as proposed in Section 2.1 to construct a k-dimensional feature map f : Rd \u2192 Rk. with probability at least 1 \u2212 N 2e\u2212\u03a9(d) over the random choice of training set, f maps the entire inner sphere to the same point, and the entire outer sphere to some other point, except for a \u03b3-fraction of both spheres, where \u03b3 = N e\u2212\u03a9(d).",
        "Nearest-neighbors will fail at classifying the outer sphere, and <a class=\"ref-link\" id=\"cGilmer_et+al_2018_a\" href=\"#rGilmer_et+al_2018_a\">Gilmer et al [2018</a>] demonstrate in practice that training adversarially robust models on the concentric spheres dataset using standard neural network architectures is extremely difficult when the dimension d grows large.",
        "Spectral features obtained using scaled Laplacian, and linear classifier: We use the 2 norm as a distance metric, and distance threshold T = 9 to construct a graph on all 11,000 data points.",
        "Spectral features obtained using scaled Laplacian with weighted edges, and linear classifier: This is similar to the previous model, with the only difference being the way in which the graph is constructed.",
        "We believe that exploring this simpler task of learning robust features, and further developing the connections to spectral graph theory, are promising steps towards the end goal of building robust machine learning models"
    ],
    "headline": "Given the apparent difficulty of learning models that are robust to adversarial perturbations, we propose tackling the simpler problem of developing adversarially robust features",
    "reference_links": [
        {
            "id": "Athalye_2017_a",
            "entry": "Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "Athalye_et+al_2018_a",
            "entry": "Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00420"
        },
        {
            "id": "Brown_et+al_2017_a",
            "entry": "TB Brown, D Man\u00e9, A Roy, M Abadi, and J Gilmer. Adversarial patch. arxiv e-prints (dec. 2017). arXiv preprint cs.CV/1712.09665, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20T.B.%20Man%C3%A9%2C%20D.%20Roy%2C%20A.%20Abadi%2C%20M.%20Adversarial%20patch.%20arxiv%20e-prints%202017-12"
        },
        {
            "id": "Bubeck_2018_a",
            "entry": "S\u00e9bastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints. arXiv preprint arXiv:1805.10204, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.10204"
        },
        {
            "id": "Chung_1997_a",
            "entry": "Fan RK Chung. Spectral graph theory. Number 92. American Mathematical Soc., 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Fan%20R.K.%20Spectral%20graph%20theory%201997"
        },
        {
            "id": "Evtimov_et+al_2017_a",
            "entry": "Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08945"
        },
        {
            "id": "Fawzi_et+al_2018_a",
            "entry": "Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv preprint arXiv:1802.08686, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08686"
        },
        {
            "id": "Gilmer_et+al_2018_a",
            "entry": "Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.02774"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Hein_2017_a",
            "entry": "Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems, pages 2263\u20132273, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20Matthias%20Andriushchenko%2C%20Maksym%20Formal%20guarantees%20on%20the%20robustness%20of%20a%20classifier%20against%20adversarial%20manipulation%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Kolter_2017_a",
            "entry": "J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "Kurakin_et+al_2016_a",
            "entry": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "Madry_et+al_2017_a",
            "entry": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "Papernot_et+al_2016_a",
            "entry": "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07277"
        },
        {
            "id": "Peck_et+al_2017_a",
            "entry": "Jonathan Peck, Joris Roels, Bart Goossens, and Yvan Saeys. Lower bounds on the robustness to adversarial perturbations. In Advances in Neural Information Processing Systems, pages 804\u2013813, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peck%2C%20Jonathan%20Roels%2C%20Joris%20Goossens%2C%20Bart%20Saeys%2C%20Yvan%20Lower%20bounds%20on%20the%20robustness%20to%20adversarial%20perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peck%2C%20Jonathan%20Roels%2C%20Joris%20Goossens%2C%20Bart%20Saeys%2C%20Yvan%20Lower%20bounds%20on%20the%20robustness%20to%20adversarial%20perturbations%202017"
        },
        {
            "id": "Raghunathan_et+al_2018_a",
            "entry": "Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09344"
        },
        {
            "id": "Schmidt_et+al_2018_a",
            "entry": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.11285"
        },
        {
            "id": "Sinha_et+al_2017_a",
            "entry": "Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10571"
        },
        {
            "id": "Spielman_2007_a",
            "entry": "Daniel A Spielman. Spectral graph theory and its applications. In Foundations of Computer Science, 2007. FOCS\u201907. 48th Annual IEEE Symposium on, pages 29\u201338. IEEE, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spielman%2C%20Daniel%20A.%20Spectral%20graph%20theory%20and%20its%20applications.%20In%20Foundations%20of%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spielman%2C%20Daniel%20A.%20Spectral%20graph%20theory%20and%20its%20applications.%20In%20Foundations%20of%202007"
        },
        {
            "id": "Szegedy_et+al_2013_a",
            "entry": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        }
    ]
}
