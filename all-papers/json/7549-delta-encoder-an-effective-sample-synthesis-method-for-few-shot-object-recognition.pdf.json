{
    "filename": "7549-delta-encoder-an-effective-sample-synthesis-method-for-few-shot-object-recognition.pdf",
    "metadata": {
        "title": "Delta-encoder: an effective sample synthesis method for few-shot object recognition",
        "author": "Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, Alex Bronstein",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7549-delta-encoder-an-effective-sample-synthesis-method-for-few-shot-object-recognition.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning to classify new categories based on just one or a few examples is a long-standing challenge in modern computer vision. In this work, we propose a simple yet effective method for few-shot (and one-shot) object recognition. Our approach is based on a modified auto-encoder, denoted \u2206-encoder, that learns to synthesize new samples for an unseen category just by seeing few examples from it. The synthesized samples are then used to train a classifier. The proposed approach learns to both extract transferable intra-class deformations, or \"deltas\", between same-class pairs of training examples, and to apply those deltas to the few provided examples of a novel class (unseen during training) in order to efficiently synthesize samples from that new class. The proposed method improves the state-of-the-art of one-shot object-recognition and performs comparably in the few-shot case."
    },
    "keywords": [
        {
            "term": "auto encoder",
            "url": "https://en.wikipedia.org/wiki/auto_encoder"
        },
        {
            "term": "object recognition",
            "url": "https://en.wikipedia.org/wiki/object_recognition"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "object detection",
            "url": "https://en.wikipedia.org/wiki/object_detection"
        }
    ],
    "highlights": [
        "Following the great success of deep learning, the field of visual classification has made a significant leap forward, reaching \u2013 and in some cases, surpassing \u2013 human levels performance [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>]",
        "We have evaluated the few-shot classification performance of the proposed method on multiple datasets, which are the benchmarks of choice for the majority of few-shot learning literature, namely: miniImageNet, CIFAR-100, Caltech-256, Caltech-UCSD Birds 200, APY, Scene UNderstanding and Animals with Attributes 2",
        "We followed the standard splits used for few-shot learning for the first four datasets; for the other datasets that are not commonly used for few-shot, we used the split suggested in [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] for zero-shot learning",
        "We proposed a novel auto-encoder like architecture, the \u2206-encoder. This model learns to generate novel samples from a distribution of a class unseen during training using as little as one example from that class",
        "New candidate examples for labeling can be selected by first generating new samples using the \u2206-encoder, and picking the data points that are farthest from the generated samples"
    ],
    "key_statements": [
        "Following the great success of deep learning, the field of visual classification has made a significant leap forward, reaching \u2013 and in some cases, surpassing \u2013 human levels performance [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>]",
        "We have evaluated the few-shot classification performance of the proposed method on multiple datasets, which are the benchmarks of choice for the majority of few-shot learning literature, namely: miniImageNet, CIFAR-100, Caltech-256, Caltech-UCSD Birds 200, APY, Scene UNderstanding and Animals with Attributes 2",
        "We followed the standard splits used for few-shot learning for the first four datasets; for the other datasets that are not commonly used for few-shot, we used the split suggested in [<a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>] for zero-shot learning",
        "We proposed a novel auto-encoder like architecture, the \u2206-encoder. This model learns to generate novel samples from a distribution of a class unseen during training using as little as one example from that class",
        "New candidate examples for labeling can be selected by first generating new samples using the \u2206-encoder, and picking the data points that are farthest from the generated samples"
    ],
    "summary": [
        "Following the great success of deep learning, the field of visual classification has made a significant leap forward, reaching \u2013 and in some cases, surpassing \u2013 human levels performance [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>].",
        "The proposed approach learns to extract and later to sample transferable non-linear deformations between pairs of examples of seen classes.",
        "It learns to apply those deltas to the few provided examples of novel categories, unseen during training, in order to efficiently synthesize new samples from these categories.",
        "Generative and augmentation-based few-shot approaches: In this line of methods, either generative models are trained to synthesize new data based on few examples, or additional examples are obtained by some other form of transfer learning from external data.",
        "We propose a method for few-shot classification by learning to synthesize samples of novel categories when only a single or a few real examples are available.",
        "Dubbed as the \u2206-encoder, learns to sample from the category distribution, while being seeded by only one or few examples from that distribution.",
        "In each of the one-shot experiments, for a novel unseen class U we are provided with an example Y u, from which we synthesize a set of samples for the class U using our trained generator model: {D(Zi, Y u)}.",
        "In order to evaluate performance on the episode, we use our trained network to synthesize a total of 1024 samples per category based on those k examples.",
        "This is followed by training a simple linear N -class classifier over those 1024 \u00b7 N samples, and the calculation of the few-shot classification accuracy on a set of M real samples from the tested N categories.",
        "We remark in the table whenever a method uses some form of additional external data, be it training on an external large-scale dataset, using word embedding applied to the category name, or using human-annotated class attributes.",
        "As a first step towards one-shot learning, we have tested the same architecture but using another sample from the same class instead of the attributes vector (Figure 3b).",
        "That means we sample linear shifts from same-class pairs in the training set and use them to augment the single example of a new class Y u that we have.",
        "We have observed a significant few-shot performance boost when using the proposed method for sample synthesis when compared to the baseline of using just the few provided examples.",
        "This model learns to generate novel samples from a distribution of a class unseen during training using as little as one example from that class."
    ],
    "headline": "We propose a simple yet effective method for few-shot  object recognition",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Antoniou, A. Storkey, and H. Edwards. Data Augmentation Generative Adversarial Networks. arXiv:1711.04340, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04340"
        },
        {
            "id": "2",
            "entry": "[2] J. L. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Predicting deep zero-shot convolutional neural networks using textual descriptions. Proceedings of the IEEE International Conference on Computer Vision, 2015 Inter:4247\u20134255, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20J.L.%20Swersky%2C%20K.%20Fidler%2C%20S.%20Salakhutdinov%2C%20R.%20Predicting%20deep%20zero-shot%20convolutional%20neural%20networks%20using%20textual%20descriptions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20J.L.%20Swersky%2C%20K.%20Fidler%2C%20S.%20Salakhutdinov%2C%20R.%20Predicting%20deep%20zero-shot%20convolutional%20neural%20networks%20using%20textual%20descriptions%202015"
        },
        {
            "id": "3",
            "entry": "[3] M. Bucher, S. Herbin, and F. Jurie. Generating Visual Representations for Zero-Shot Classification. arXiv:1708.06975, 8 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.06975"
        },
        {
            "id": "4",
            "entry": "[4] Z. Chen, Y. Fu, Y. Zhang, Y.-G. Jiang, X. Xue, and L. Sigal. Semantic Feature Augmentation in Few-shot Learning. arXiv:1804.05298v2, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05298v2"
        },
        {
            "id": "5",
            "entry": "[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li. ImageNet: A large-scale hierarchical image database. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "6",
            "entry": "[6] X. Dong, L. Zheng, F. Ma, Y. Yang, and D. Meng. Few-shot Object Detection. Arxiv:1706.08249, pages 1\u201311, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08249"
        },
        {
            "id": "7",
            "entry": "[7] A. Dosovitskiy, J. T. Springenberg, M. Tatarchenko, and T. Brox. Learning to Generate Chairs, Tables and Cars with Convolutional Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):692\u2013705, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dosovitskiy%2C%20A.%20Springenberg%2C%20J.T.%20Tatarchenko%2C%20M.%20Brox%2C%20T.%20Learning%20to%20Generate%20Chairs%2C%20Tables%20and%20Cars%20with%20Convolutional%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dosovitskiy%2C%20A.%20Springenberg%2C%20J.T.%20Tatarchenko%2C%20M.%20Brox%2C%20T.%20Learning%20to%20Generate%20Chairs%2C%20Tables%20and%20Cars%20with%20Convolutional%20Networks%202017"
        },
        {
            "id": "8",
            "entry": "[8] I. Durugkar, I. Gemp, and S. Mahadevan. Generative Multi-Adversarial Networks. International Conference on Learning Representations (ICLR), pages 1\u201314, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durugkar%2C%20I.%20Gemp%2C%20I.%20Mahadevan%2C%20S.%20Generative%20Multi-Adversarial%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durugkar%2C%20I.%20Gemp%2C%20I.%20Mahadevan%2C%20S.%20Generative%20Multi-Adversarial%20Networks%202017"
        },
        {
            "id": "9",
            "entry": "[9] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing Objects by their Attributes. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1778\u20131785, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farhadi%2C%20A.%20Endres%2C%20I.%20Hoiem%2C%20D.%20Forsyth%2C%20D.%20Describing%20Objects%20by%20their%20Attributes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Farhadi%2C%20A.%20Endres%2C%20I.%20Hoiem%2C%20D.%20Forsyth%2C%20D.%20Describing%20Objects%20by%20their%20Attributes%202009"
        },
        {
            "id": "10",
            "entry": "[10] C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv:1703.03400, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03400"
        },
        {
            "id": "11",
            "entry": "[11] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong. Transductive Multi-View Zero-Shot Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(11):2332\u20132345, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Y.%20Hospedales%2C%20T.M.%20Xiang%2C%20T.%20Gong%2C%20S.%20Transductive%20Multi-View%20Zero-Shot%20Learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Y.%20Hospedales%2C%20T.M.%20Xiang%2C%20T.%20Gong%2C%20S.%20Transductive%20Multi-View%20Zero-Shot%20Learning%202015"
        },
        {
            "id": "12",
            "entry": "[12] Y. Fu and L. Sigal. Semi-supervised Vocabulary-informed Learning. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5337\u20135346, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Y.%20Sigal%2C%20L.%20Semi-supervised%20Vocabulary-informed%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Y.%20Sigal%2C%20L.%20Semi-supervised%20Vocabulary-informed%20Learning%202016"
        },
        {
            "id": "13",
            "entry": "[13] V. Garcia and J. Bruna. Few-Shot Learning with Graph Neural Networks. arXiv:1711.04043, pages 1\u201313, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04043"
        },
        {
            "id": "14",
            "entry": "[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Nets. Advances in Neural Information Processing Systems 27, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20Adversarial%20Nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20Adversarial%20Nets%202014"
        },
        {
            "id": "15",
            "entry": "[15] G. Griffin, a. Holub, and P. Perona. Caltech-256 object category dataset. Caltech mimeo, 11(1):20, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffin%2C%20G.%20a.%20Holub%20Perona%2C%20P.%20Caltech-256%20object%20category%20dataset%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffin%2C%20G.%20a.%20Holub%20Perona%2C%20P.%20Caltech-256%20object%20category%20dataset%202007"
        },
        {
            "id": "16",
            "entry": "[16] K. Guu, T. B. Hashimoto, Y. Oren, and P. Liang. Generating Sentences by Editing Prototypes. Arxiv:1709.08878, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.08878"
        },
        {
            "id": "17",
            "entry": "[17] B. Hariharan and R. Girshick. Low-shot Visual Recognition by Shrinking and Hallucinating Features. IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hariharan%2C%20B.%20Girshick%2C%20R.%20Low-shot%20Visual%20Recognition%20by%20Shrinking%20and%20Hallucinating%20Features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hariharan%2C%20B.%20Girshick%2C%20R.%20Low-shot%20Visual%20Recognition%20by%20Shrinking%20and%20Hallucinating%20Features%202017"
        },
        {
            "id": "18",
            "entry": "[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "19",
            "entry": "[19] N. Hilliard, L. Phillips, S. Howland, A. Yankov, C. D. Corley, and N. O. Hodas. Few-Shot Learning with Metric-Agnostic Conditional Embeddings. 2 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hilliard%2C%20N.%20Phillips%2C%20L.%20Howland%2C%20S.%20Yankov%2C%20A.%20Few-Shot%20Learning%20with%20Metric-Agnostic%20Conditional%20Embeddings%202018"
        },
        {
            "id": "20",
            "entry": "[20] G. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Weinberger. Densely Connected Convolutional Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261\u20132269, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20v.%20d.%20Maaten%2C%20L.%20Weinberger%2C%20K.Q.%20Densely%20Connected%20Convolutional%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20v.%20d.%20Maaten%2C%20L.%20Weinberger%2C%20K.Q.%20Densely%20Connected%20Convolutional%20Networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] E. S. Jun-Yan Zhu, Philipp Krahenbuhl and A. Efros. Generative Visual Manipulation on the Natural Image Manifold. European Conference on Computer Vision (ECCV)., pages 597\u2013613, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20E.S.Jun-Yan%20Krahenbuhl%2C%20Philipp%20Efros%2C%20A.%20Generative%20Visual%20Manipulation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20E.S.Jun-Yan%20Krahenbuhl%2C%20Philipp%20Efros%2C%20A.%20Generative%20Visual%20Manipulation%202016"
        },
        {
            "id": "22",
            "entry": "[22] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical report. Science Department, University of Toronto, Tech., pages 1\u201360, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images%202009"
        },
        {
            "id": "23",
            "entry": "[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems, pages 1\u20139, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%202012"
        },
        {
            "id": "24",
            "entry": "[24] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lake%2C%20B.M.%20Salakhutdinov%2C%20R.%20Tenenbaum%2C%20J.B.%20Human-level%20concept%20learning%20through%20probabilistic%20program%20induction%202015"
        },
        {
            "id": "25",
            "entry": "[25] Z. Li and D. Hoiem. Learning without Forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u201313, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Z.%20Hoiem%2C%20D.%20Learning%20without%20Forgetting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Z.%20Hoiem%2C%20D.%20Learning%20without%20Forgetting%202016"
        },
        {
            "id": "26",
            "entry": "[26] Z. Li, F. Zhou, F. Chen, and H. Li. Meta-SGD: Learning to Learn Quickly for Few-Shot Learning. arXiv:1707.09835, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09835"
        },
        {
            "id": "27",
            "entry": "[27] J. J. Lim, R. Salakhutdinov, and A. Torralba. Transfer Learning by Borrowing Examples for Multiclass Object Detection. Advances in Neural Information Processing Systems 26 (NIPS), pages 1\u20139, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lim%2C%20J.J.%20Salakhutdinov%2C%20R.%20Torralba%2C%20A.%20Transfer%20Learning%20by%20Borrowing%20Examples%20for%20Multiclass%20Object%20Detection%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lim%2C%20J.J.%20Salakhutdinov%2C%20R.%20Torralba%2C%20A.%20Transfer%20Learning%20by%20Borrowing%20Examples%20for%20Multiclass%20Object%20Detection%202012"
        },
        {
            "id": "28",
            "entry": "[28] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r. Focal Loss for Dense Object Detection. arXiv:1708.02002, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02002"
        },
        {
            "id": "29",
            "entry": "[29] X. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, and S. P. Smolley. Least Squares Generative Adversarial Networks. IEEE International Conference on Computer Vision (ICCV), pages 1\u201316, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.Y.K.%20Least%20Squares%20Generative%20Adversarial%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.Y.K.%20Least%20Squares%20Generative%20Adversarial%20Networks%202016"
        },
        {
            "id": "30",
            "entry": "[30] A. Mehrotra and A. Dukkipati. Generative Adversarial Residual Pairwise Networks for One Shot Learning. arXiv:1703.08033, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.08033"
        },
        {
            "id": "31",
            "entry": "[31] T. Munkhdalai and H. Yu. Meta Networks. arXiv:1703.00837, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00837"
        },
        {
            "id": "32",
            "entry": "[32] D. Park and D. Ramanan. Articulated pose estimation with tiny synthetic videos. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015-Octob:58\u201366, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Park%2C%20D.%20Ramanan%2C%20D.%20Articulated%20pose%20estimation%20with%20tiny%20synthetic%20videos%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Park%2C%20D.%20Ramanan%2C%20D.%20Articulated%20pose%20estimation%20with%20tiny%20synthetic%20videos%202015"
        },
        {
            "id": "33",
            "entry": "[33] A. Radford, L. Metz, and S. Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv:1511.06434, pages 1\u201316, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "34",
            "entry": "[34] S. Ravi and H. Larochelle. Optimization As a Model for Few-Shot Learning. International Conference on Learning Representations (ICLR), pages 1\u201311, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20As%20a%20Model%20for%20Few-Shot%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20S.%20Larochelle%2C%20H.%20Optimization%20As%20a%20Model%20for%20Few-Shot%20Learning%202017"
        },
        {
            "id": "35",
            "entry": "[35] S. Reed, Y. Chen, T. Paine, A. van den Oord, S. M. A. Eslami, D. Rezende, O. Vinyals, and N. de Freitas. Few-shot autoregressive density estimation: towards learning to learn distributions. arXiv:1710.10304, (2016):1\u201311, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10304"
        },
        {
            "id": "36",
            "entry": "[36] O. Rippel, M. Paluri, P. Dollar, and L. Bourdev. Metric Learning with Adaptive Density Discrimination. arXiv:1511.05939, (Dml):1\u201315, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05939"
        },
        {
            "id": "37",
            "entry": "[37] F. Schroff, D. Kalenichenko, and J. Philbin. FaceNet: A unified embedding for face recognition and clustering. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 815\u2013823, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schroff%2C%20F.%20Kalenichenko%2C%20D.%20Philbin%2C%20J.%20FaceNet%3A%20A%20unified%20embedding%20for%20face%20recognition%20and%20clustering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schroff%2C%20F.%20Kalenichenko%2C%20D.%20Philbin%2C%20J.%20FaceNet%3A%20A%20unified%20embedding%20for%20face%20recognition%20and%20clustering%202015"
        },
        {
            "id": "38",
            "entry": "[38] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR arXiv:1409.1556, abs/1409.1:1\u201314, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "39",
            "entry": "[39] J. Snell, K. Swersky, and R. S. Zemel. Prototypical Networks for Few-shot Learning. Advances In Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snell%2C%20J.%20Swersky%2C%20K.%20Zemel%2C%20R.S.%20Prototypical%20Networks%20for%20Few-shot%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snell%2C%20J.%20Swersky%2C%20K.%20Zemel%2C%20R.S.%20Prototypical%20Networks%20for%20Few-shot%20Learning%202017"
        },
        {
            "id": "40",
            "entry": "[40] H. Su, C. R. Qi, Y. Li, and L. J. Guibas. Render for CNN Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views.pdf. IEEE International Conference on Computer Vision (ICCV), pages 2686\u20132694, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20H.%20Qi%2C%20C.R.%20Li%2C%20Y.%20Guibas%2C%20L.J.%20Render%20for%20CNN%20Viewpoint%20Estimation%20in%20Images%20Using%20CNNs%20Trained%20with%20Rendered%203D%20Model%20Views.pdf%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Su%2C%20H.%20Qi%2C%20C.R.%20Li%2C%20Y.%20Guibas%2C%20L.J.%20Render%20for%20CNN%20Viewpoint%20Estimation%20in%20Images%20Using%20CNNs%20Trained%20with%20Rendered%203D%20Model%20Views.pdf%202015"
        },
        {
            "id": "41",
            "entry": "[41] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, and T. M. Hospedales. Learning to Compare: Relation Network for Few-Shot Learning. arXiv:1711.06025, 11 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06025"
        },
        {
            "id": "42",
            "entry": "[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), volume 07-12-June, pages 1\u20139. IEEE, 6 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Liu%2C%20W.%20Jia%2C%20Y.%20Sermanet%2C%20P.%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "43",
            "entry": "[43] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching Networks for One Shot Learning. Advances In Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Blundell%2C%20C.%20Lillicrap%2C%20T.%20Kavukcuoglu%2C%20K.%20Matching%20Networks%20for%20One%20Shot%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Blundell%2C%20C.%20Lillicrap%2C%20T.%20Kavukcuoglu%2C%20K.%20Matching%20Networks%20for%20One%20Shot%20Learning%202016"
        },
        {
            "id": "44",
            "entry": "[44] Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariharan. Low-Shot Learning from Imaginary Data. arXiv:1801.05401, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05401"
        },
        {
            "id": "45",
            "entry": "[45] Y.-X. Wang and M. Hebert. Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs. Advances In Neural Information Processing Systems (NIPS), (Nips):244\u2013252, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.-X.%20Hebert%2C%20M.%20Learning%20from%20Small%20Sample%20Sets%20by%20Combining%20Unsupervised%20Meta-Training%20with%20CNNs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.-X.%20Hebert%2C%20M.%20Learning%20from%20Small%20Sample%20Sets%20by%20Combining%20Unsupervised%20Meta-Training%20with%20CNNs%202016"
        },
        {
            "id": "46",
            "entry": "[46] Y.-X. Wang and M. Hebert. Learning to Learn: Model Regression Networks for Easy Small Sample Learning. European Conference on Computer Vision (ECCV), pages 616\u2013634, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.-X.%20Hebert%2C%20M.%20Learning%20to%20Learn%3A%20Model%20Regression%20Networks%20for%20Easy%20Small%20Sample%20Learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.-X.%20Hebert%2C%20M.%20Learning%20to%20Learn%3A%20Model%20Regression%20Networks%20for%20Easy%20Small%20Sample%20Learning%202016"
        },
        {
            "id": "47",
            "entry": "[47] K. Q. Weinberger and L. K. Saul. Distance Metric Learning for Large Margin Nearest Neighbor Classification. The Journal of Machine Learning Research, 10:207\u2013244, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20K.Q.%20Saul%2C%20L.K.%20Distance%20Metric%20Learning%20for%20Large%20Margin%20Nearest%20Neighbor%20Classification%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20K.Q.%20Saul%2C%20L.K.%20Distance%20Metric%20Learning%20for%20Large%20Margin%20Nearest%20Neighbor%20Classification%202009"
        },
        {
            "id": "48",
            "entry": "[48] P. Welinder, S. Branson, T. Mita, and C. Wah. Caltech-UCSD birds 200. pages 1\u201315, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welinder%2C%20P.%20Branson%2C%20S.%20Mita%2C%20T.%20Wah%2C%20C.%20Caltech-UCSD%20birds%202010"
        },
        {
            "id": "49",
            "entry": "[49] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly. arXiv:1707.00600, pages 1\u201314, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00600"
        },
        {
            "id": "50",
            "entry": "[50] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3485\u20133492. IEEE, 6 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20J.%20Hays%2C%20J.%20Ehinger%2C%20K.A.%20Oliva%2C%20A.%20SUN%20database%3A%20Large-scale%20scene%20recognition%20from%20abbey%20to%20zoo%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20J.%20Hays%2C%20J.%20Ehinger%2C%20K.A.%20Oliva%2C%20A.%20SUN%20database%3A%20Large-scale%20scene%20recognition%20from%20abbey%20to%20zoo%202010"
        },
        {
            "id": "51",
            "entry": "[51] A. Yu and K. Grauman. Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images. Proceedings of the IEEE International Conference on Computer Vision, 2017-Octob:5571\u20135580, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20A.%20Grauman%2C%20K.%20Semantic%20Jitter%3A%20Dense%20Supervision%20for%20Visual%20Comparisons%20via%20Synthetic%20Images%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20A.%20Grauman%2C%20K.%20Semantic%20Jitter%3A%20Dense%20Supervision%20for%20Visual%20Comparisons%20via%20Synthetic%20Images%202017"
        },
        {
            "id": "52",
            "entry": "[52] F. Zhou, B. Wu, and Z. Li. Deep Meta-Learning: Learning to Learn in the Concept Space. arXiv:1802.03596, 2 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03596"
        },
        {
            "id": "53",
            "entry": "[53] J. Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. Proceedings of the IEEE International Conference on Computer Vision, 2017-Octob:2242\u20132251, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20Image-to-Image%20Translation%20Using%20Cycle-Consistent%20Adversarial%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20Image-to-Image%20Translation%20Using%20Cycle-Consistent%20Adversarial%20Networks%202017"
        },
        {
            "id": "54",
            "entry": "[54] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning Transferable Architectures for Scalable Image Recognition. arXiv:1707.07012, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1707.07012"
        }
    ]
}
