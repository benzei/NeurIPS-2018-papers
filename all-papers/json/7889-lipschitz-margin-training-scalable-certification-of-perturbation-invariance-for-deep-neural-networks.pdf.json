{
    "filename": "7889-lipschitz-margin-training-scalable-certification-of-perturbation-invariance-for-deep-neural-networks.pdf",
    "metadata": {
        "title": "Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks",
        "author": "Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7889-lipschitz-margin-training-scalable-certification-of-perturbation-invariance-for-deep-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "High sensitivity of neural networks against malicious perturbations on inputs causes security concerns. To take a steady step towards robust classifiers, we aim to create neural network models provably defended from perturbations. Prior certification work requires strong assumptions on network structures and massive computational costs, and thus the range of their applications was limited. From the relationship between the Lipschitz constants and prediction margins, we present a computationally efficient calculation technique to lower-bound the size of adversarial perturbations that can deceive networks, and that is widely applicable to various complicated networks. Moreover, we propose an efficient training procedure that robustifies networks and significantly improves the provably guarded areas around data points. In experimental evaluations, our method showed its ability to provide a non-trivial guarantee and enhance robustness for even large networks."
    },
    "keywords": [
        {
            "term": "object recognition",
            "url": "https://en.wikipedia.org/wiki/object_recognition"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        }
    ],
    "highlights": [
        "Deep neural networks are highly vulnerable against intentionally created small perturbations on inputs [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], called adversarial perturbations, which cause serious security concerns in applications such as self-driving cars",
        "One approach to defend from adversarial perturbations is to mask gradients",
        "Carlini and Wagner [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] showed that we can create adversarial perturbations that deceive networks trained with defensive distillation",
        "We describe the relationships between the Lipschitz constants and some functionals which frequently appears in deep neural networks: composition, addition, and concatenation",
        "Robustness against attack: We evaluated the robustness of trained networks against adversarial perturbations created by the current attacks"
    ],
    "key_statements": [
        "Deep neural networks are highly vulnerable against intentionally created small perturbations on inputs [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], called adversarial perturbations, which cause serious security concerns in applications such as self-driving cars",
        "One approach to defend from adversarial perturbations is to mask gradients",
        "Carlini and Wagner [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] showed that we can create adversarial perturbations that deceive networks trained with defensive distillation",
        "Recent work has repeatedly succeeded to create adversarial perturbations for networks protected with heuristics in the literature [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "Athalye et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] reported that many ICLR 2018 defense papers did not adequately protect networks soon after the announcement of their acceptance. This indicates that even protected networks can be unexpectedly vulnerable, which is a crucial problem for this specific line of research because the primary concern of these studies is security threats",
        "This work tackled this problem, and we provide a widely applicable, yet, highly scalable method that ensures large guarded areas for a wide range of network structures",
        "We show that we can overcome such looseness with our improved and unified bounds and a developed training procedure",
        "Proposition 1 sees network F as a function with a multidimensional output. This connects the Lipschitz constant of a network, which has been discussed in Szegedy et al [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and Cisse et al [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], with the absence of adversarial perturbations",
        "To ensure non-trivial guarded areas, we propose a training procedure that enlarges the provably guarded area",
        "We experimentally evaluate its generalization to test data in Sec. 6",
        "We show in Sec. 5 that its computational cost is almost the same as increasing the batch size by one",
        "While prior work required separate analysis for slightly different components [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], we provide a more unified analysis",
        "We describe the relationships between the Lipschitz constants and some functionals which frequently appears in deep neural networks: composition, addition, and concatenation",
        "By using the following theorem, we proposed a more unified algorithm than Yoshida and Miyato [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]",
        "Lists of the Lipschitz constant of pooling layers and activation functions are summarized in Appendix D of the supplementary material.\n5.3",
        "We empirically found that calculating the Lipschitz constants using the same bound as inference time effectively regularizes the Lipschitz constant",
        "We evaluated the robustness of trained networks against current attacks and confirmed that Lipschitz-margin training robustifies networks (Secs. 6.1 and 6.2)",
        "Improvement in each component: We evaluated the difference of bounds in convolutional layers in networks trained using a usual training procedure and Lipschitz-margin training",
        "Analysis of tightness: Let L be an upper-bound of the Lipschitz constant calculated by our method",
        "We used lower bounds of the local and global Lipschitz constant calculated by the maximum size of gradients found",
        "We evaluated our method with a larger and more complex network to confirm its broad applicability and scalability",
        "Size of provably guarded area: For a model trained with Lipschitz-margin training, we could ensure larger guarded areas than 0.029 for more than half of test data",
        "Robustness against attack: We evaluated the robustness of trained networks against adversarial perturbations created by the current attacks",
        "We successfully provided non-trivial certification for small to large networks with negligible computational costs"
    ],
    "summary": [
        "Deep neural networks are highly vulnerable against intentionally created small perturbations on inputs [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], called adversarial perturbations, which cause serious security concerns in applications such as self-driving cars.",
        "Our goal is to ensure the lower bounds on the size of adversarial perturbations that can deceive networks for each input.",
        "We explain how to calculate the guarded area using the Lipschitz constant.",
        "This connects the Lipschitz constant of a network, which has been discussed in Szegedy et al [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and Cisse et al [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], with the absence of adversarial perturbations.",
        "Calculations of the Lipschitz constant, which is not straightforward in large and complex networks, will be explained in Sec. 5.",
        "We first describe a method to calculate upper bounds of the Lipschitz constant.",
        "The Lipschitz constant of output for each functional is bounded as follows: composition f \u25e6 g : L1 \u00b7 L2, addition f + g : L1 + L2, concatenation (f, g) : L12 + L22.",
        "Careful counting of n leads to improved bounds on the relationship between the Lipschitz constant of a convolutional layer and the spectral norm of its reshaped kernel from the previous result [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "Lists of the Lipschitz constant of pooling layers and activation functions are summarized in Appendix D of the supplementary material.",
        "3. Our calculation technique of the guarded area and LMT are available for modern large and complex networks (Sec. 6.2).",
        "For calculating the Lipschitz constant and guarded area, we used Prop.",
        "Improvement in each component: We evaluated the difference of bounds in convolutional layers in networks trained using a usual training procedure and LMT.",
        "This results in significant differences in upper-bounds of the Lipschitz constants of the whole networks.",
        "Analysis of tightness: Let L be an upper-bound of the Lipschitz constant calculated by our method.",
        "We used lower bounds of the local and global Lipschitz constant calculated by the maximum size of gradients found.",
        "This shows that the trained network became smooth and Lipschitz constant based certifications became significantly tighter when we use LMT.",
        "Size of provably guarded area: For a model trained with LMT, we could ensure larger guarded areas than 0.029 for more than half of test data.",
        "This corresponds to that changing 400 elements of input by \u00b11 in usual image scales (0\u2013255) cannot cause error over 50% for the trained network.",
        "2. We introduced general and fast calculation algorithm for the upper bound of operator norms and its differentiable approximation.",
        "3. We proposed a training algorithm which effectively constrains networks to be smooth, and achieves better certification and robustness against attacks.",
        "We proposed a training algorithm which effectively constrains networks to be smooth, and achieves better certification and robustness against attacks"
    ],
    "headline": "From the relationship between the Lipschitz constants and prediction margins, we present a computationally efficient calculation technique to lower-bound the size of adversarial perturbations that can deceive networks, and that is widely applicable to various complicated networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] N. Akhtar and A. Mian. Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey. CoRR, abs/1801.00553, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00553"
        },
        {
            "id": "2",
            "entry": "[2] A. Athalye, N. Carlini, and D. Wagner. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. In Proceedings of the 35th International Conference on Machine Learning, pages 274\u2013283, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Athalye%2C%20A.%20Carlini%2C%20N.%20Wagner%2C%20D.%20Obfuscated%20Gradients%20Give%20a%20False%20Sense%20of%20Security%3A%20Circumventing%20Defenses%20to%20Adversarial%20Examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Athalye%2C%20A.%20Carlini%2C%20N.%20Wagner%2C%20D.%20Obfuscated%20Gradients%20Give%20a%20False%20Sense%20of%20Security%3A%20Circumventing%20Defenses%20to%20Adversarial%20Examples%202018"
        },
        {
            "id": "3",
            "entry": "[3] P. L. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized Margin Bounds for Neural Networks. In Advances in Neural Information Processing Systems 30, pages 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.%20Spectrally-normalized%20Margin%20Bounds%20for%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.%20Spectrally-normalized%20Margin%20Bounds%20for%20Neural%20Networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. V. Nori, and A. Criminisi. Measuring Neural Net Robustness with Constraints. In Advances in Neural Information Processing Systems 28, pages 2613\u20132621, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bastani%2C%20O.%20Ioannou%2C%20Y.%20Lampropoulos%2C%20L.%20Vytiniotis%2C%20D.%20Measuring%20Neural%20Net%20Robustness%20with%20Constraints%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bastani%2C%20O.%20Ioannou%2C%20Y.%20Lampropoulos%2C%20L.%20Vytiniotis%2C%20D.%20Measuring%20Neural%20Net%20Robustness%20with%20Constraints%202015"
        },
        {
            "id": "5",
            "entry": "[5] N. Carlini and D. Wagner. Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 3\u201314, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Adversarial%20Examples%20Are%20Not%20Easily%20Detected%3A%20Bypassing%20Ten%20Detection%20Methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Adversarial%20Examples%20Are%20Not%20Easily%20Detected%3A%20Bypassing%20Ten%20Detection%20Methods%202017"
        },
        {
            "id": "6",
            "entry": "[6] N. Carlini and D. A. Wagner. Towards Evaluating the Robustness of Neural Networks. In Proceedings of the 2017 IEEE Symposium on Security and Privacy, pages 39\u201357. IEEE Computer Society, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.A.%20Towards%20Evaluating%20the%20Robustness%20of%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.A.%20Towards%20Evaluating%20the%20Robustness%20of%20Neural%20Networks%202017"
        },
        {
            "id": "7",
            "entry": "[7] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval Networks: Improving Robustness to Adversarial Examples. In Proceedings of the 34th International Conference on Machine Learning, pages 854\u2013863, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20M.%20Bojanowski%2C%20P.%20Grave%2C%20E.%20Dauphin%2C%20Y.%20Parseval%20Networks%3A%20Improving%20Robustness%20to%20Adversarial%20Examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20M.%20Bojanowski%2C%20P.%20Grave%2C%20E.%20Dauphin%2C%20Y.%20Parseval%20Networks%3A%20Improving%20Robustness%20to%20Adversarial%20Examples%202017"
        },
        {
            "id": "8",
            "entry": "[8] J. Friedman. Error bounds on the power method for determining the largest eigenvalue of a symmetric, positive definite matrix. Linear Algebra and its Applications, 280(2):199\u2013216, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20J.%20Error%20bounds%20on%20the%20power%20method%20for%20determining%20the%20largest%20eigenvalue%20of%20a%20symmetric%2C%20positive%20definite%20matrix%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20J.%20Error%20bounds%20on%20the%20power%20method%20for%20determining%20the%20largest%20eigenvalue%20of%20a%20symmetric%2C%20positive%20definite%20matrix%201998"
        },
        {
            "id": "9",
            "entry": "[9] I. J. Goodfellow. Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size. CoRR, abs/1804.07870, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07870"
        },
        {
            "id": "10",
            "entry": "[10] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20Harnessing%20Adversarial%20Examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Shlens%2C%20J.%20Szegedy%2C%20C.%20Explaining%20and%20Harnessing%20Adversarial%20Examples%202015"
        },
        {
            "id": "11",
            "entry": "[11] C. Guo, M. Rana, M. Cisse, and L. v. d. Maaten. Countering Adversarial Images using Input Transformations. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20C.%20Rana%2C%20M.%20Cisse%2C%20M.%20v.%20d.%20Maaten%2C%20L.%20Countering%20Adversarial%20Images%20using%20Input%20Transformations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20C.%20Rana%2C%20M.%20Cisse%2C%20M.%20v.%20d.%20Maaten%2C%20L.%20Countering%20Adversarial%20Images%20using%20Input%20Transformations%202018"
        },
        {
            "id": "12",
            "entry": "[12] M. Hein and M. Andriushchenko. Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation. In Advances in Neural Information Processing Systems 30, pages 2263\u20132273, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hein%2C%20M.%20Andriushchenko%2C%20M.%20Formal%20Guarantees%20on%20the%20Robustness%20of%20a%20Classifier%20against%20Adversarial%20Manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hein%2C%20M.%20Andriushchenko%2C%20M.%20Formal%20Guarantees%20on%20the%20Robustness%20of%20a%20Classifier%20against%20Adversarial%20Manipulation%202017"
        },
        {
            "id": "13",
            "entry": "[13] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Towards Proving the Adversarial Robustness of Deep Neural Networks. In Proceedings of the First Workshop on Formal Verification of Autonomous Vehicles, pages 19\u201326, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katz%2C%20G.%20Barrett%2C%20C.%20Dill%2C%20D.L.%20Julian%2C%20K.%20Towards%20Proving%20the%20Adversarial%20Robustness%20of%20Deep%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katz%2C%20G.%20Barrett%2C%20C.%20Dill%2C%20D.L.%20Julian%2C%20K.%20Towards%20Proving%20the%20Adversarial%20Robustness%20of%20Deep%20Neural%20Networks%202017"
        },
        {
            "id": "14",
            "entry": "[14] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. In Computer Aided Verification Part I, pages 97\u2013117, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katz%2C%20G.%20Barrett%2C%20C.W.%20Dill%2C%20D.L.%20Julian%2C%20K.%20Reluplex%3A%20An%20Efficient%20SMT%20Solver%20for%20Verifying%20Deep%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katz%2C%20G.%20Barrett%2C%20C.W.%20Dill%2C%20D.L.%20Julian%2C%20K.%20Reluplex%3A%20An%20Efficient%20SMT%20Solver%20for%20Verifying%20Deep%20Neural%20Networks%202017"
        },
        {
            "id": "15",
            "entry": "[15] J. Z. Kolter and E. Wong. Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope. In Proceedings of the 35th International Conference on Machine Learning, pages 5286\u20135295, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolter%2C%20J.Z.%20Wong%2C%20E.%20Provable%20Defenses%20against%20Adversarial%20Examples%20via%20the%20Convex%20Outer%20Adversarial%20Polytope%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolter%2C%20J.Z.%20Wong%2C%20E.%20Provable%20Defenses%20against%20Adversarial%20Examples%20via%20the%20Convex%20Outer%20Adversarial%20Polytope%202018"
        },
        {
            "id": "16",
            "entry": "[16] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial Machine Learning at Scale. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20A.%20Goodfellow%2C%20I.J.%20Bengio%2C%20S.%20Adversarial%20Machine%20Learning%20at%20Scale%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurakin%2C%20A.%20Goodfellow%2C%20I.J.%20Bengio%2C%20S.%20Adversarial%20Machine%20Learning%20at%20Scale%202017"
        },
        {
            "id": "17",
            "entry": "[17] J. Langford and J. Shawe-Taylor. PAC-Bayes & Margins. In Advances in Neural Information Processing Systems 15, pages 439\u2013446, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20Shawe-Taylor%2C%20J.%20PAC-Bayes%20%26%20Margins%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20J.%20Shawe-Taylor%2C%20J.%20PAC-Bayes%20%26%20Margins%202002"
        },
        {
            "id": "18",
            "entry": "[18] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20Deep%20Learning%20Models%20Resistant%20to%20Adversarial%20Attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20Deep%20Learning%20Models%20Resistant%20to%20Adversarial%20Attacks%202018"
        },
        {
            "id": "19",
            "entry": "[19] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574\u20132582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20DeepFool%3A%20A%20Simple%20and%20Accurate%20Method%20to%20Fool%20Deep%20Neural%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20S.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20DeepFool%3A%20A%20Simple%20and%20Accurate%20Method%20to%20Fool%20Deep%20Neural%20Networks%202016"
        },
        {
            "id": "20",
            "entry": "[20] V. Nair and G. E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20Linear%20Units%20Improve%20Restricted%20Boltzmann%20Machines%202010-06-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20V.%20Hinton%2C%20G.E.%20Rectified%20Linear%20Units%20Improve%20Restricted%20Boltzmann%20Machines%202010-06-21"
        },
        {
            "id": "21",
            "entry": "[21] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading Digits in Natural Images with Unsupervised Feature Learning. Neural Information Processing Systems Workshop, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Y.%20Wang%2C%20T.%20Coates%2C%20A.%20Bissacco%2C%20A.%20Reading%20Digits%20in%20Natural%20Images%20with%20Unsupervised%20Feature%20Learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Y.%20Wang%2C%20T.%20Coates%2C%20A.%20Bissacco%2C%20A.%20Reading%20Digits%20in%20Natural%20Images%20with%20Unsupervised%20Feature%20Learning%202011"
        },
        {
            "id": "22",
            "entry": "[22] B. Neyshabur, S. Bhojanapalli, and N. Srebro. A PAC-Bayesian Approach to SpectrallyNormalized Margin Bounds for Neural Networks. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20Srebro%2C%20N.%20A%20PAC-Bayesian%20Approach%20to%20SpectrallyNormalized%20Margin%20Bounds%20for%20Neural%20Networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20Srebro%2C%20N.%20A%20PAC-Bayesian%20Approach%20to%20SpectrallyNormalized%20Margin%20Bounds%20for%20Neural%20Networks%202018"
        },
        {
            "id": "23",
            "entry": "[23] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks. In Proceedings of the IEEE Symposium on Security and Privacy, pages 582\u2013597, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.D.%20Wu%2C%20X.%20Jha%2C%20S.%20Distillation%20as%20a%20Defense%20to%20Adversarial%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.D.%20Wu%2C%20X.%20Jha%2C%20S.%20Distillation%20as%20a%20Defense%20to%20Adversarial%202016"
        },
        {
            "id": "24",
            "entry": "[24] J. Peck, J. Roels, B. Goossens, and Y. Saeys. Lower Bounds on the Robustness to Adversarial Perturbations. In Advances in Neural Information Processing Systems 30, pages 804\u2013813, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peck%2C%20J.%20Roels%2C%20J.%20Goossens%2C%20B.%20Saeys%2C%20Y.%20Lower%20Bounds%20on%20the%20Robustness%20to%20Adversarial%20Perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peck%2C%20J.%20Roels%2C%20J.%20Goossens%2C%20B.%20Saeys%2C%20Y.%20Lower%20Bounds%20on%20the%20Robustness%20to%20Adversarial%20Perturbations%202017"
        },
        {
            "id": "25",
            "entry": "[25] A. Raghunathan, J. Steinhardt, and P. Liang. Certified Defenses against Adversarial Examples . International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghunathan%2C%20A.%20Steinhardt%2C%20J.%20Liang%2C%20P.%20Certified%20Defenses%20against%20Adversarial%20Examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghunathan%2C%20A.%20Steinhardt%2C%20J.%20Liang%2C%20P.%20Certified%20Defenses%20against%20Adversarial%20Examples%202018"
        },
        {
            "id": "26",
            "entry": "[26] W. Ruan, X. Huang, and M. Kwiatkowska. Reachability Analysis of Deep Neural Networks with Provable Guarantees. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, pages 2651\u20132659, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruan%2C%20W.%20Huang%2C%20X.%20Kwiatkowska%2C%20M.%20Reachability%20Analysis%20of%20Deep%20Neural%20Networks%20with%20Provable%20Guarantees%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruan%2C%20W.%20Huang%2C%20X.%20Kwiatkowska%2C%20M.%20Reachability%20Analysis%20of%20Deep%20Neural%20Networks%20with%20Provable%20Guarantees%202018"
        },
        {
            "id": "27",
            "entry": "[27] A. Sinha, H. Namkoong, and J. Duchi. Certifiable Distributional Robustness with Principled Adversarial Training. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinha%2C%20A.%20Namkoong%2C%20H.%20Duchi%2C%20J.%20Certifiable%20Distributional%20Robustness%20with%20Principled%20Adversarial%20Training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinha%2C%20A.%20Namkoong%2C%20H.%20Duchi%2C%20J.%20Certifiable%20Distributional%20Robustness%20with%20Principled%20Adversarial%20Training%202018"
        },
        {
            "id": "28",
            "entry": "[28] J. Su, D. V. Vargas, and S. Kouichi. One pixel attack for fooling deep neural networks. CoRR, abs/1710.08864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.08864"
        },
        {
            "id": "29",
            "entry": "[29] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing Properties of Neural Networks. International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20Szegedy%20W%20Zaremba%20I%20Sutskever%20J%20Bruna%20D%20Erhan%20I%20J%20Goodfellow%20and%20R%20Fergus%20Intriguing%20Properties%20of%20Neural%20Networks%20International%20Conference%20on%20Learning%20Representations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20Szegedy%20W%20Zaremba%20I%20Sutskever%20J%20Bruna%20D%20Erhan%20I%20J%20Goodfellow%20and%20R%20Fergus%20Intriguing%20Properties%20of%20Neural%20Networks%20International%20Conference%20on%20Learning%20Representations%202014"
        },
        {
            "id": "30",
            "entry": "[30] F. Tram\u00e8r, A. Kurakin, N. Papernot, D. Boneh, and P. D. McDaniel. Ensemble Adversarial Training: Attacks and Defenses. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tram%C3%A8r%2C%20F.%20Kurakin%2C%20A.%20Papernot%2C%20N.%20Boneh%2C%20D.%20Ensemble%20Adversarial%20Training%3A%20Attacks%20and%20Defenses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tram%C3%A8r%2C%20F.%20Kurakin%2C%20A.%20Papernot%2C%20N.%20Boneh%2C%20D.%20Ensemble%20Adversarial%20Training%3A%20Attacks%20and%20Defenses%202018"
        },
        {
            "id": "31",
            "entry": "[31] T. Weng, H. Zhang, P. Chen, J. Yi, D. Su, Y. Gao, C. Hsieh, and L. Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20T.%20Zhang%2C%20H.%20Chen%2C%20P.%20Yi%2C%20J.%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20T.%20Zhang%2C%20H.%20Chen%2C%20P.%20Yi%2C%20J.%20Evaluating%20the%20robustness%20of%20neural%20networks%3A%20An%202018"
        },
        {
            "id": "32",
            "entry": "[32] W. Xu, D. Evans, and Y. Qi. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. Network and Distributed Systems Security Symposium, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20W.%20Evans%2C%20D.%20Qi%2C%20Y.%20Feature%20Squeezing%3A%20Detecting%20Adversarial%20Examples%20in%20Deep%20Neural%20Networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20W.%20Evans%2C%20D.%20Qi%2C%20Y.%20Feature%20Squeezing%3A%20Detecting%20Adversarial%20Examples%20in%20Deep%20Neural%20Networks%202018"
        },
        {
            "id": "33",
            "entry": "[33] Y. Yoshida and T. Miyato. Spectral Norm Regularization for Improving the Generalizability of Deep Learning. CoRR, abs/1705.10941, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10941"
        },
        {
            "id": "34",
            "entry": "[34] S. Zagoruyko and N. Komodakis. Wide Residual Networks. In Proceedings of the British Machine Vision Conference, pages 87.1\u201387.12, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20S.%20Wide%2C%20N.Komodakis%20Residual%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20S.%20Wide%2C%20N.Komodakis%20Residual%20Networks%202016"
        }
    ]
}
