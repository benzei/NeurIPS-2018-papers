{
    "filename": "7827-differential-properties-of-sinkhorn-approximation-for-learning-with-wasserstein-distance.pdf",
    "metadata": {
        "title": "Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance",
        "author": "Giulia Luise, Alessandro Rudi, Massimiliano Pontil, Carlo Ciliberto",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7827-differential-properties-of-sinkhorn-approximation-for-learning-with-wasserstein-distance.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Applications of optimal transport have recently gained remarkable attention as a result of the computational advantages of entropic regularization. However, in most situations the Sinkhorn approximation to the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn approximation, proving that it enjoys the same smoothness of its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula is used to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis."
    },
    "keywords": [
        {
            "term": "wasserstein distance",
            "url": "https://en.wikipedia.org/wiki/wasserstein_distance"
        },
        {
            "term": "structured prediction",
            "url": "https://en.wikipedia.org/wiki/structured_prediction"
        },
        {
            "term": "optimal transport",
            "url": "https://en.wikipedia.org/wiki/optimal_transport"
        },
        {
            "term": "dictionary learning",
            "url": "https://en.wikipedia.org/wiki/dictionary_learning"
        },
        {
            "term": "probability measure",
            "url": "https://en.wikipedia.org/wiki/probability_measure"
        }
    ],
    "highlights": [
        "Optimal Transport and Wasserstein Distance<br/><br/>Optimal transport theory investigates how to compare probability measures over a domain X",
        "While approaches based on automatic differentiation have been successfully recently adopted [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], in this work we are interested in investigating the analytic properties of the gradient of the sharp Sinkhorn, for which we provide an explicit algorithm in the following.\n4 Differential Properties of Sinkhorn Approximations",
        "In this paper we investigated the differential properties of Sinkhorn approximations",
        "We proved the high order smoothness of the two functions and derived as a by-product of the proof an explicit algorithm to efficiently compute the gradient of the sharp Sinkhorn",
        "The characterization of smoothness proved to be a key tool to study the statistical properties of the Sinkhorn approximation as loss function",
        "In particular we considered a structured prediction estimator for which we proved universal consistency and excess risk bounds"
    ],
    "key_statements": [
        "Optimal Transport and Wasserstein Distance<br/><br/>Optimal transport theory investigates how to compare probability measures over a domain X",
        "Despite the comparable differential properties, sharp and regularized Sinkhorn approximations show a rather different behaviour when adopted in optimization problems such as the computation of barycenters [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "In contrast to the Wasserstein distance, the regularized Sinkhorn is differentiable ( smooth, as we show in this work in Thm. 2) and particularly appealing for practical applications where the goal is to solve a minimization over probability spaces",
        "While approaches based on automatic differentiation have been successfully recently adopted [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], in this work we are interested in investigating the analytic properties of the gradient of the sharp Sinkhorn, for which we provide an explicit algorithm in the following.\n4 Differential Properties of Sinkhorn Approximations",
        "From [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], which adopted an empirical risk minimization approach, we address the problem in Eq (16) from a structured prediction perspective [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] following a recent trend of works addressing the problem within the setting of statistical learning theory [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]",
        "We evaluated the Sinkhorn approximations in an image reconstruction problem similar to the one considered in [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>] for structured prediction",
        "In this paper we investigated the differential properties of Sinkhorn approximations",
        "We proved the high order smoothness of the two functions and derived as a by-product of the proof an explicit algorithm to efficiently compute the gradient of the sharp Sinkhorn",
        "The characterization of smoothness proved to be a key tool to study the statistical properties of the Sinkhorn approximation as loss function",
        "In particular we considered a structured prediction estimator for which we proved universal consistency and excess risk bounds"
    ],
    "summary": [
        "Optimal Transport and Wasserstein Distance<br/><br/>Optimal transport theory investigates how to compare probability measures over a domain X.",
        "While as a direct consequence of [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] it can be shown that the original Sinkhorn approach provides a sharp approximation to the Wasserstein distance [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], the same is not guaranteed for its regularized version.",
        "We take this as a motivation to study the differential properties of the sharp Sinkhorn with the goal of deriving a strategy to address optimization and learning problems over probability distributions.",
        "Despite the comparable differential properties, sharp and regularized Sinkhorn approximations show a rather different behaviour when adopted in optimization problems such as the computation of barycenters [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>].",
        "As a by-product of the proof of the smoothness, we obtain an explicit formula to efficiently compute the gradient of the sharp Sinkhorn approximation, which proves to be viable alternative to automatic differentiation [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>].",
        "The computational benefit provided by the regularized Sinkhorn is paid in terms of the approximation with respect to the Wasserstein distance.",
        "This intuition is further supported by the following discussion where we compare the behaviour of the two approximations on the problem of finding an optimal transport barycenter of probability distributions.",
        "IFniinmdiiznigngthteheWwasesigerhstteedinavbearraygceendtiesrtainscceombeptwuteaetinonaalllldyisvterirbyuetixopnesnisnivteheansdetthDe, typical approach is to approximate it with the barycenter \u03bc\u03bb\u2217 , obtained by substituting the Wasserstein distance W with the regularized Sinkhorn S\u03bb in the the objective functional of Eq (9).",
        "NWdihniglebwaericdeenfeter ra\u03bcth\u2217\u03bboorofutghhe empirical comparison of the two barycenters to Sec. 6, here we consider a simple scenario in which the sharp Sinkhorn can be proved to be a significantly better approximation of the Wasserstein distance.",
        "While approaches based on automatic differentiation have been successfully recently adopted [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], in this work we are interested in investigating the analytic properties of the gradient of the sharp Sinkhorn, for which we provide an explicit algorithm in the following.",
        "As pointed out in [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], the gradient of the regularized Sinkhorn approximation can be obtained directly from the dual solution as \u2207aS\u03bb(a, b) = \u03b1\u2217(a, b), for any a \u2208 Rn and b \u2208 Rm. This characterization is possible because of well-known properties of primal and dual optimization problems [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "Tab. 1 reports the absolute improvement of the barycenter of the sharp Sinkhorn with respect to the one obtained with the regularized Sinkhorn, averaged over 10 independent dataset generation for each support size k.",
        "We proved the high order smoothness of the two functions and derived as a by-product of the proof an explicit algorithm to efficiently compute the gradient of the sharp Sinkhorn.",
        "Future work will focus on further applications and a more extensive comparison with the existing literature"
    ],
    "headline": "In this work we characterize the differential properties of the original Sinkhorn approximation, proving that it enjoys the same smoothness of its regularized version and we explicitly provide an efficient algorithm to compute its gradient",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2292\u20132300. Curran Associates, Inc., 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20Marco%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20Marco%20Sinkhorn%20distances%3A%20Lightspeed%20computation%20of%20optimal%20transport%202013"
        },
        {
            "id": "2",
            "entry": "[2] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport. Technical report, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peyr%C3%A9%2C%20Gabriel%20Cuturi%2C%20Marco%20Computational%20optimal%20transport%202017"
        },
        {
            "id": "3",
            "entry": "[3] Stephan Dempe. Foundations of bilevel programming. Springer Science & Business Media, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempe%2C%20Stephan%20Foundations%20of%20bilevel%20programming%202002"
        },
        {
            "id": "4",
            "entry": "[4] Aude Genevay, Gabriel Peyr\u00e9, and Marco Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics, pages 1608\u20131617, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Genevay%2C%20Aude%20Peyr%C3%A9%2C%20Gabriel%20Cuturi%2C%20Marco%20Learning%20generative%20models%20with%20sinkhorn%20divergences%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Genevay%2C%20Aude%20Peyr%C3%A9%2C%20Gabriel%20Cuturi%2C%20Marco%20Learning%20generative%20models%20with%20sinkhorn%20divergences%202018"
        },
        {
            "id": "5",
            "entry": "[5] Nicolas Courty, R\u00e9mi Flamary, and Devis Tuia. Domain adaptation with regularized optimal transport. In ECML/PKDD 2014, LNCS, pages 1\u201316, Nancy, France, September 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Courty%2C%20Nicolas%20Flamary%2C%20R%C3%A9mi%20Tuia%2C%20Devis%20Domain%20adaptation%20with%20regularized%20optimal%20transport%202014-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Courty%2C%20Nicolas%20Flamary%2C%20R%C3%A9mi%20Tuia%2C%20Devis%20Domain%20adaptation%20with%20regularized%20optimal%20transport%202014-09"
        },
        {
            "id": "6",
            "entry": "[6] Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio. Learning with a wasserstein loss. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2053\u20132061, Cambridge, MA, USA, 2015. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frogner%2C%20Charlie%20Zhang%2C%20Chiyuan%20Mobahi%2C%20Hossein%20Araya-Polo%2C%20Mauricio%20Learning%20with%20a%20wasserstein%20loss%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frogner%2C%20Charlie%20Zhang%2C%20Chiyuan%20Mobahi%2C%20Hossein%20Araya-Polo%2C%20Mauricio%20Learning%20with%20a%20wasserstein%20loss%202015"
        },
        {
            "id": "7",
            "entry": "[7] Antoine Rolet, Marco Cuturi, and Gabriel Peyr\u00e9. Fast dictionary learning with a smoothed wasserstein loss. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 630\u2013638, Cadiz, Spain, 09\u201311 May 2016. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rolet%2C%20Antoine%20Cuturi%2C%20Marco%20Peyr%C3%A9%2C%20Gabriel%20Fast%20dictionary%20learning%20with%20a%20smoothed%20wasserstein%20loss%202016-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rolet%2C%20Antoine%20Cuturi%2C%20Marco%20Peyr%C3%A9%2C%20Gabriel%20Fast%20dictionary%20learning%20with%20a%20smoothed%20wasserstein%20loss%202016-05"
        },
        {
            "id": "8",
            "entry": "[8] Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 685\u2013693, Bejing, China, 22\u201324 Jun 2014. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20Marco%20Doucet%2C%20Arnaud%20Fast%20computation%20of%20wasserstein%20barycenters%202014-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20Marco%20Doucet%2C%20Arnaud%20Fast%20computation%20of%20wasserstein%20barycenters%202014-06"
        },
        {
            "id": "9",
            "entry": "[9] Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr\u00e9. Iterative bregman projections for regularized transportation problems. SIAM J. Scientific Computing, 37(2), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benamou%2C%20Jean-David%20Carlier%2C%20Guillaume%20Cuturi%2C%20Marco%20Nenna%2C%20Luca%20Iterative%20bregman%20projections%20for%20regularized%20transportation%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benamou%2C%20Jean-David%20Carlier%2C%20Guillaume%20Cuturi%2C%20Marco%20Nenna%2C%20Luca%20Iterative%20bregman%20projections%20for%20regularized%20transportation%20problems%202015"
        },
        {
            "id": "10",
            "entry": "[10] Nicolas Bonneel, Gabriel Peyr\u00e9, and Marco Cuturi. Wasserstein barycentric coordinates: histogram regression using optimal transport. ACM Trans. Graph., 35(4):71\u20131, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonneel%2C%20Nicolas%20Peyr%C3%A9%2C%20Gabriel%20Cuturi%2C%20Marco%20Wasserstein%20barycentric%20coordinates%3A%20histogram%20regression%20using%20optimal%20transport%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bonneel%2C%20Nicolas%20Peyr%C3%A9%2C%20Gabriel%20Cuturi%2C%20Marco%20Wasserstein%20barycentric%20coordinates%3A%20histogram%20regression%20using%20optimal%20transport%202016"
        },
        {
            "id": "11",
            "entry": "[11] Morgan A Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Ngole, David Coeurjolly, Marco Cuturi, Gabriel Peyr\u00e9, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning. SIAM Journal on Imaging Sciences, 11(1):643\u2013678, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmitz%2C%20Morgan%20A.%20Heitz%2C%20Matthieu%20Bonneel%2C%20Nicolas%20Ngole%2C%20Fred%20Wasserstein%20dictionary%20learning%3A%20Optimal%20transport-based%20unsupervised%20nonlinear%20dictionary%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmitz%2C%20Morgan%20A.%20Heitz%2C%20Matthieu%20Bonneel%2C%20Nicolas%20Ngole%2C%20Fred%20Wasserstein%20dictionary%20learning%3A%20Optimal%20transport-based%20unsupervised%20nonlinear%20dictionary%20learning%202018"
        },
        {
            "id": "12",
            "entry": "[12] R\u00e9mi Flamary, Marco Cuturi, Nicolas Courty, and Alain Rakotomamonjy. Wasserstein discriminant analysis. Machine Learning, May 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flamary%2C%20R%C3%A9mi%20Cuturi%2C%20Marco%20Courty%2C%20Nicolas%20Rakotomamonjy%2C%20Alain%20Wasserstein%20discriminant%20analysis%202018-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flamary%2C%20R%C3%A9mi%20Cuturi%2C%20Marco%20Courty%2C%20Nicolas%20Rakotomamonjy%2C%20Alain%20Wasserstein%20discriminant%20analysis%202018-05"
        },
        {
            "id": "13",
            "entry": "[13] R. Cominetti and J. San Mart\u00edn. Asymptotic analysis of the exponential penalty trajectory in linear programming. Mathematical Programming, 67(1):169\u2013187, Oct 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cominetti%2C%20R.%20Mart%C3%ADn%2C%20J.San%20Asymptotic%20analysis%20of%20the%20exponential%20penalty%20trajectory%20in%20linear%20programming%201994-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cominetti%2C%20R.%20Mart%C3%ADn%2C%20J.San%20Asymptotic%20analysis%20of%20the%20exponential%20penalty%20trajectory%20in%20linear%20programming%201994-10"
        },
        {
            "id": "14",
            "entry": "[14] J. Ye, P. Wu, J. Z. Wang, and J. Li. Fast discrete distribution clustering using wasserstein barycenter with sparse support. IEEE Transactions on Signal Processing, 65(9):2317\u20132332, May 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20J.%20Wu%2C%20P.%20Wang%2C%20J.Z.%20Li%2C%20J.%20Fast%20discrete%20distribution%20clustering%20using%20wasserstein%20barycenter%20with%20sparse%20support%202017-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20J.%20Wu%2C%20P.%20Wang%2C%20J.Z.%20Li%2C%20J.%20Fast%20discrete%20distribution%20clustering%20using%20wasserstein%20barycenter%20with%20sparse%20support%202017-05"
        },
        {
            "id": "15",
            "entry": "[15] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A consistent regularization approach for structured prediction. In Advances in Neural Information Processing Systems, pages 4412\u20134420. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016"
        },
        {
            "id": "16",
            "entry": "[16] C. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C.%20Optimal%20Transport%3A%20Old%20and%20New.%20Grundlehren%20der%20mathematischen%20Wissenschaften%202008"
        },
        {
            "id": "17",
            "entry": "[17] D. Bertsimas and J. Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20D.%20Tsitsiklis%2C%20J.%20Introduction%20to%20Linear%20Optimization%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsimas%2C%20D.%20Tsitsiklis%2C%20J.%20Introduction%20to%20Linear%20Optimization%201997"
        },
        {
            "id": "18",
            "entry": "[18] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific J. Math., 21(2):343\u2013348, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sinkhorn%2C%20Richard%20Knopp%2C%20Paul%20Concerning%20nonnegative%20matrices%20and%20doubly%20stochastic%20matrices%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sinkhorn%2C%20Richard%20Knopp%2C%20Paul%20Concerning%20nonnegative%20matrices%20and%20doubly%20stochastic%20matrices%201967"
        },
        {
            "id": "19",
            "entry": "[19] Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algorithms for optimal transport via sinkhorn iteration. In NIPS, pages 1961\u20131971, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altschuler%2C%20Jason%20Weed%2C%20Jonathan%20Rigollet%2C%20Philippe%20Near-linear%20time%20approximation%20algorithms%20for%20optimal%20transport%20via%20sinkhorn%20iteration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altschuler%2C%20Jason%20Weed%2C%20Jonathan%20Rigollet%2C%20Philippe%20Near-linear%20time%20approximation%20algorithms%20for%20optimal%20transport%20via%20sinkhorn%20iteration%202017"
        },
        {
            "id": "20",
            "entry": "[20] Marco Cuturi and Gabriel Peyr\u00e9. A smoothed dual approach for variational wasserstein problems. SIAM J. Imaging Sciences, 9(1):320\u2013343, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuturi%2C%20Marco%20Peyr%C3%A9%2C%20Gabriel%20A%20smoothed%20dual%20approach%20for%20variational%20wasserstein%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuturi%2C%20Marco%20Peyr%C3%A9%2C%20Gabriel%20A%20smoothed%20dual%20approach%20for%20variational%20wasserstein%20problems%202016"
        },
        {
            "id": "21",
            "entry": "[21] C.H. Edwards. Advanced Calculus of Several Variables. Dover Books on Mathematics. Dover Publications, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edwards%2C%20C.H.%20Advanced%20Calculus%20of%20Several%20Variables.%20Dover%20Books%20on%20Mathematics%202012"
        },
        {
            "id": "22",
            "entry": "[22] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8), 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Gradient-based%20optimization%20of%20hyperparameters%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Gradient-based%20optimization%20of%20hyperparameters%202000"
        },
        {
            "id": "23",
            "entry": "[23] Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. Choosing multiple parameters for support vector machines. Machine learning, 46(1-3):131\u2013159, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Vapnik%2C%20Vladimir%20Bousquet%2C%20Olivier%20Mukherjee%2C%20Sayan%20Choosing%20multiple%20parameters%20for%20support%20vector%20machines%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Vapnik%2C%20Vladimir%20Bousquet%2C%20Olivier%20Mukherjee%2C%20Sayan%20Choosing%20multiple%20parameters%20for%20support%20vector%20machines%202002"
        },
        {
            "id": "24",
            "entry": "[24] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. arXiv preprint arXiv:1602.02355, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02355"
        },
        {
            "id": "25",
            "entry": "[25] GH Bakir, T Hofmann, B Sch\u00f6lkopf, AJ Smola, B Taskar, and SVN Vishwanathan. Predicting structured data. neural information processing, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bakir%2C%20G.H.%20Hofmann%2C%20T.%20Sch%C3%B6lkopf%2C%20B.%20Smola%2C%20A.J.%20Predicting%20structured%20data.%20neural%20information%20processing%202007"
        },
        {
            "id": "26",
            "entry": "[26] Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, and Massimiliano Pontil. Consistent multitask learning with nonlinear output relations. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%202017"
        },
        {
            "id": "27",
            "entry": "[27] Anton Osokin, Francis Bach, and Simon Lacoste-Julien. On structured prediction theory with calibrated convex surrogate losses. In Advances in Neural Information Processing Systems, pages 302\u2013313, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20Anton%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20On%20structured%20prediction%20theory%20with%20calibrated%20convex%20surrogate%20losses%202017"
        },
        {
            "id": "28",
            "entry": "[28] Anna Korba, Alexandre Garcia, and Florence d\u2019Alch\u00e9 Buc. A structured prediction approach for label ranking. arXiv preprint arXiv:1807.02374, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.02374"
        },
        {
            "id": "29",
            "entry": "[29] Alessandro Rudi, Carlo Ciliberto, Gian Maria Marconi, and Lorenzo Rosasco. Manifold structured prediction. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Ciliberto%2C%20Carlo%20Marconi%2C%20Gian%20Maria%20Rosasco%2C%20Lorenzo%20Manifold%20structured%20prediction%202018"
        },
        {
            "id": "30",
            "entry": "[30] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337\u2013404, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950"
        },
        {
            "id": "31",
            "entry": "[31] Alex J Smola and Bernhard Sch\u00f6lkopf. Sparse greedy matrix approximation for machine learning. 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20Alex%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20Sparse%20greedy%20matrix%20approximation%20for%20machine%20learning%202000"
        },
        {
            "id": "32",
            "entry": "[32] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177\u20131184, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "33",
            "entry": "[33] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pages 3215\u20133225, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "34",
            "entry": "[34] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel method. In Advances in Neural Information Processing Systems, pages 3888\u20133898, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017"
        },
        {
            "id": "35",
            "entry": "[35] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20vector%20machines%202008"
        },
        {
            "id": "36",
            "entry": "[36] J. Feydy, T. S\u00e9journ\u00e9, F.-X. Vialard, S.-i. Amari, A. Trouv\u00e9, and G. Peyr\u00e9. Interpolating between Optimal Transport and MMD using Sinkhorn Divergences. ArXiv e-prints, October 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feydy%2C%20J.%20S%C3%A9journ%C3%A9%2C%20T.%20Vialard%2C%20F.-X.%20Amari%2C%20S.-i%20Interpolating%20between%20Optimal%20Transport%20and%20MMD%20using%20Sinkhorn%20Divergences.%20ArXiv%20e-prints%202018-10"
        },
        {
            "id": "37",
            "entry": "[37] J. Weston, O. Chapelle, A. Elisseeff, B. Sch\u00f6lkopf, and V. Vapnik. Kernel dependency estimation. In Advances in Neural Information Processing Systems 15, pages 873\u2013880, Cambridge, MA, USA, October 2003. Max-Planck-Gesellschaft, MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weston%2C%20J.%20Chapelle%2C%20O.%20Elisseeff%2C%20A.%20Sch%C3%B6lkopf%2C%20B.%20Kernel%20dependency%20estimation%202003-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weston%2C%20J.%20Chapelle%2C%20O.%20Elisseeff%2C%20A.%20Sch%C3%B6lkopf%2C%20B.%20Kernel%20dependency%20estimation%202003-10"
        },
        {
            "id": "38",
            "entry": "[38] Inc Google. QuickDraw Dataset. https://github.com/googlecreativelab/quickdraw-dataset.",
            "url": "https://github.com/googlecreativelab/quickdraw-dataset"
        },
        {
            "id": "39",
            "entry": "[39] T. Kollo and D. von Rosen. Advanced Multivariate Statistics with Matrices. Mathematics and Its Applications. Springer Netherlands, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kollo%2C%20T.%20von%20Rosen%2C%20D.%20Advanced%20Multivariate%20Statistics%20with%20Matrices.%20Mathematics%20and%20Its%20Applications%202006"
        },
        {
            "id": "40",
            "entry": "[40] Miroslav Fiedler. Bounds for eigenvalues of doubly stochastic matrices. Linear Algebra and its Applications, 5(3):299 \u2013 310, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fiedler%2C%20Miroslav%20Bounds%20for%20eigenvalues%20of%20doubly%20stochastic%20matrices%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fiedler%2C%20Miroslav%20Bounds%20for%20eigenvalues%20of%20doubly%20stochastic%20matrices%201972"
        },
        {
            "id": "41",
            "entry": "[41] H. Brezis. Functional Analysis, Sobolev Spaces and Partial Differential Equations. Universitext. Springer New York, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brezis%2C%20H.%20Functional%20Analysis%2C%20Sobolev%20Spaces%20and%20Partial%20Differential%20Equations%202010"
        },
        {
            "id": "42",
            "entry": "[42] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer Science & Business Media, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Berlinet%2C%20Alain%20Thomas-Agnan%2C%20Christine%20Reproducing%20kernel%20Hilbert%20spaces%20in%20probability%20and%20statistics%202011"
        },
        {
            "id": "43",
            "entry": "[43] V. Moretti. Spectral Theory and Quantum Mechanics: With an Introduction to the Algebraic Formulation. UNITEXT. Springer Milan, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moretti%2C%20V.%20Spectral%20Theory%20and%20Quantum%20Mechanics%3A%20With%20an%20Introduction%20to%20the%20Algebraic%20Formulation%202013"
        },
        {
            "id": "44",
            "entry": "[44] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007"
        },
        {
            "id": "45",
            "entry": "[45] Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral algorithms with least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018"
        }
    ]
}
