{
    "filename": "8176-theoretical-guarantees-for-em-under-misspecified-gaussian-mixture-models.pdf",
    "metadata": {
        "title": "Theoretical guarantees for EM under misspecified Gaussian mixture models",
        "author": "Raaz Dwivedi, nh\u1eadt H\u1ed3, Koulik Khamaru, Martin J. Wainwright, Michael I. Jordan",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8176-theoretical-guarantees-for-em-under-misspecified-gaussian-mixture-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Recent years have witnessed substantial progress in understanding the behavior of EM for mixture models that are correctly specified. Given that model misspecification is common in practice, it is important to understand EM in this more general setting. We provide non-asymptotic guarantees for population and sample-based EM for parameter estimation under a few specific univariate settings of misspecified Gaussian mixture models. Due to misspecification, the EM iterates no longer converge to the true model and instead converge to the projection of the true model over the set of models being searched over. We provide two classes of theoretical guarantees: first, we characterize the bias introduced due to the misspecification; and second, we prove that population EM converges at a geometric rate to the model projection under a suitable initialization condition. This geometripc convergence rate for population EM imply a statistical complexity of order 1/ n when running EM with n samples. We validate our theoretical findings in different cases via several numerical examples."
    },
    "keywords": [
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "expectation maximization",
            "url": "https://en.wikipedia.org/wiki/expectation_maximization"
        },
        {
            "term": "maximum likelihood estimate",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimate"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "em algorithm",
            "url": "https://en.wikipedia.org/wiki/em_algorithm"
        },
        {
            "term": "gaussian mixture model",
            "url": "https://en.wikipedia.org/wiki/gaussian_mixture_model"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        }
    ],
    "highlights": [
        "Mixture models play a central role in statistical applications, where they are used to capture heterogeneity of data arising from several underlying subpopulations",
        "We provide a number of theoretical guarantees for the expectation maximization algorithm applied to two classes of misspecified mixture models",
        "We provide sufficient conditions for the bias to be small enough, and prove that under such cases the population expectation maximization updates have a geometric rate of convergence to the KL projection of the true model onto the fitted model class",
        "Corollary 2 consists of two key terms: the first term is the linear rate of convergence from the population expectation maximization operator in Theorem 2 while the second term characterizes the radius of convergence in terms of sample complexity, which is of",
        "While this paper focused on estimating the location parameter, it is natural to investigate the behavior of expectation maximization algorithm, when we fit a model with scale as well",
        "Besides deriving sharper results for the settings considered in this paper, developing a framework to analyze the behavior of expectation maximization for nonGaussian and more general mixture models is an appealing venue for future work"
    ],
    "key_statements": [
        "Mixture models play a central role in statistical applications, where they are used to capture heterogeneity of data arising from several underlying subpopulations",
        "In the specific setting of Gaussian mixtures, population expectation maximization was shown to have a range of behavior from super-linear convergence to slow convergence like a first-order method depending on the overlap between the mixtures [10, 20]",
        "We provide a number of theoretical guarantees for the expectation maximization algorithm applied to two classes of misspecified mixture models",
        "We provide sufficient conditions for the bias to be small enough, and prove that under such cases the population expectation maximization updates have a geometric rate of convergence to the KL projection of the true model onto the fitted model class",
        "In Section 2, we introduce the problem set-up and provide the background information on the expectation maximization algorithm",
        "Corollary 2 consists of two key terms: the first term is the linear rate of convergence from the population expectation maximization operator in Theorem 2 while the second term characterizes the radius of convergence in terms of sample complexity, which is of",
        "While this paper focused on estimating the location parameter, it is natural to investigate the behavior of expectation maximization algorithm, when we fit a model with scale as well",
        "Besides deriving sharper results for the settings considered in this paper, developing a framework to analyze the behavior of expectation maximization for nonGaussian and more general mixture models is an appealing venue for future work"
    ],
    "summary": [
        "Mixture models play a central role in statistical applications, where they are used to capture heterogeneity of data arising from several underlying subpopulations.",
        "For well-specified setting which includes two components location-Gaussian mixtures as an example, they provided sufficient conditions under which well-initialized EM algorithm converges to a close",
        "We provide sufficient conditions for the bias to be small enough, and prove that under such cases the population EM updates have a geometric rate of convergence to the KL projection of the true model onto the fitted model class.",
        "In Section 3, we present our results for the first framework and provide expressions for the bias and rate of convergence of EM for different 3 component mixture of Gaussians.",
        "We study the convergence of the EM algorithm in the setting of under-fitted mixtures, where the number of components in the true model is larger than that in the fitted model.",
        "We consider the case, where the true model has distribution P\u21e4 is a mixture of three components location-Gaussian mixtures given by: P\u21e4",
        "Corollary 2 consists of two key terms: the first term is the linear rate of convergence from the population EM operator in Theorem 2 while the second term characterizes the radius of convergence in terms of sample complexity, which is of",
        "Under the setting of two components location-Gaussian mixtures [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] and fitted model [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], the population EM operator \u2713 !7 M (\u2713) satisfies",
        "We use EM to fit 2-component Gaussian mixtures for the three misspecified settings considered above.",
        "Case 1 refers to the true model [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] from Section 3.1, namely a three component Gaussian mixture where two of the components very close to each other and the quantity \u21e2 2 (0, 1) denotes the extent of weak separation.",
        "Case 2 refers to the true model [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] from Section 3.2, namely, a three components Gaussian mixture where one of the components has very small weight at origin and the quantity !",
        "We established several new convergence results for population and sample-based EM updates under few specific settings of misspecified mixture models.",
        "While this paper focused on estimating the location parameter, it is natural to investigate the behavior of EM algorithm, when we fit a model with scale as well.",
        "Besides deriving sharper results for the settings considered in this paper, developing a framework to analyze the behavior of EM for nonGaussian and more general mixture models is an appealing venue for future work."
    ],
    "headline": "We provide non-asymptotic guarantees for population and sample-based expectation maximization for parameter estimation under a few specific univariate settings of misspecified Gaussian mixture models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the em algorithm: From population to sample-based analysis. Annals of Statistics, 45:77\u2013120, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balakrishnan%2C%20S.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20Statistical%20guarantees%20for%20the%20em%20algorithm%3A%20From%20population%20to%20sample-based%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balakrishnan%2C%20S.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20Statistical%20guarantees%20for%20the%20em%20algorithm%3A%20From%20population%20to%20sample-based%20analysis%202017"
        },
        {
            "id": "2",
            "entry": "[2] Y. Baraud, C. Giraud, and S. Huet. Gaussian model selection with an unknown variance. The Annals of Statistics, 37(2):630\u2013672, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baraud%2C%20Y.%20Giraud%2C%20C.%20Huet%2C%20S.%20Gaussian%20model%20selection%20with%20an%20unknown%20variance%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baraud%2C%20Y.%20Giraud%2C%20C.%20Huet%2C%20S.%20Gaussian%20model%20selection%20with%20an%20unknown%20variance%202009"
        },
        {
            "id": "3",
            "entry": "[3] T. T. Cai, J. Ma, and L. Zhang. CHIME: Clustering of high-dimensional Gaussian mixtures with EM algorithm and its optimality. Annals of Statistics, To Appear.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20T.T.%20Ma%2C%20J.%20Zhang%2C%20L.%20CHIME%3A%20Clustering%20of%20high-dimensional%20Gaussian%20mixtures%20with%20EM%20algorithm%20and%20its%20optimality",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20T.T.%20Ma%2C%20J.%20Zhang%2C%20L.%20CHIME%3A%20Clustering%20of%20high-dimensional%20Gaussian%20mixtures%20with%20EM%20algorithm%20and%20its%20optimality"
        },
        {
            "id": "4",
            "entry": "[4] C. Daskalakis, C. Tzamos, and M. Zampetakis. Ten steps of EM suffice for mixtures of two Gaussians. In Proceedings of the 2017 Conference on Learning Theory, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20C.%20Tzamos%2C%20C.%20Zampetakis%2C%20M.%20Ten%20steps%20of%20EM%20suffice%20for%20mixtures%20of%20two%20Gaussians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20C.%20Tzamos%2C%20C.%20Zampetakis%2C%20M.%20Ten%20steps%20of%20EM%20suffice%20for%20mixtures%20of%20two%20Gaussians%202017"
        },
        {
            "id": "5",
            "entry": "[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 39:1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977"
        },
        {
            "id": "6",
            "entry": "[6] B. Hao, W. Sun, Y. Liu, and G. Cheng. Simultaneous clustering and estimation of heterogeneous graphical models. Journal of Machine Learning Research, To Appear.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hao%2C%20B.%20Sun%2C%20W.%20Liu%2C%20Y.%20Cheng%2C%20G.%20Simultaneous%20clustering%20and%20estimation%20of%20heterogeneous%20graphical%20models",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hao%2C%20B.%20Sun%2C%20W.%20Liu%2C%20Y.%20Cheng%2C%20G.%20Simultaneous%20clustering%20and%20estimation%20of%20heterogeneous%20graphical%20models"
        },
        {
            "id": "7",
            "entry": "[7] P. Heinrich and J. Kahn. Strong identifiability and optimal minimax rates for finite mixture estimation. Annals of Statistics, 46:2844\u20132870, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heinrich%2C%20P.%20Kahn%2C%20J.%20Strong%20identifiability%20and%20optimal%20minimax%20rates%20for%20finite%20mixture%20estimation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heinrich%2C%20P.%20Kahn%2C%20J.%20Strong%20identifiability%20and%20optimal%20minimax%20rates%20for%20finite%20mixture%20estimation%202018"
        },
        {
            "id": "8",
            "entry": "[8] C. Jin, Y. Zhang, S. Balakrishnan, M. J. Wainwright, and M. I. Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences. In Advances in Neural Information Processing Systems 29, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20C.%20Zhang%2C%20Y.%20Balakrishnan%2C%20S.%20Wainwright%2C%20M.J.%20Local%20maxima%20in%20the%20likelihood%20of%20gaussian%20mixture%20models%3A%20Structural%20results%20and%20algorithmic%20consequences%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20C.%20Zhang%2C%20Y.%20Balakrishnan%2C%20S.%20Wainwright%2C%20M.J.%20Local%20maxima%20in%20the%20likelihood%20of%20gaussian%20mixture%20models%3A%20Structural%20results%20and%20algorithmic%20consequences%202016"
        },
        {
            "id": "9",
            "entry": "[9] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer-Verlag, New York, NY, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20M.%20Talagrand%2C%20M.%20Probability%20in%20Banach%20Spaces%3A%20Isoperimetry%20and%20Processes%201991"
        },
        {
            "id": "10",
            "entry": "[10] J. Ma, L. Xu, and M. I. Jordan. Asymptotic convergence rate of the EM algorithm for Gaussian mixtures. Neural Computation, 12:2881\u20132907, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20J.%20Xu%2C%20L.%20Jordan%2C%20M.I.%20Asymptotic%20convergence%20rate%20of%20the%20EM%20algorithm%20for%20Gaussian%20mixtures%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20J.%20Xu%2C%20L.%20Jordan%2C%20M.I.%20Asymptotic%20convergence%20rate%20of%20the%20EM%20algorithm%20for%20Gaussian%20mixtures%202000"
        },
        {
            "id": "11",
            "entry": "[11] P. Massart. Concentration inequalities and model selection. Springer, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Massart%2C%20P.%20Concentration%20inequalities%20and%20model%20selection%202007"
        },
        {
            "id": "12",
            "entry": "[12] X. Nguyen. Convergence of latent mixing measures in finite and infinite mixture models. Annals of Statistics, 4(1):370\u2013400, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20X.%20Convergence%20of%20latent%20mixing%20measures%20in%20finite%20and%20infinite%20mixture%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20X.%20Convergence%20of%20latent%20mixing%20measures%20in%20finite%20and%20infinite%20mixture%20models%202013"
        },
        {
            "id": "13",
            "entry": "[13] H. Teicher. Identifiability of finite mixtures. Ann. Math. Statist., 32:1265\u20131269, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teicher%2C%20H.%20Identifiability%20of%20finite%20mixtures%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teicher%2C%20H.%20Identifiability%20of%20finite%20mixtures%201963"
        },
        {
            "id": "14",
            "entry": "[14] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer-Verlag, New York, NY, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Vaart%2C%20A.W.%20Wellner%2C%20J.A.%20Weak%20Convergence%20and%20Empirical%20Processes%3A%20With%20Applications%20to%20Statistics%202000"
        },
        {
            "id": "16",
            "entry": "[16] C. Villani. Optimal transport: Old and New. Springer, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C.%20Optimal%20transport%3A%20Old%20and%20New%202008"
        },
        {
            "id": "17",
            "entry": "[17] Z. Wang, Q. Gu, Y. Ning, and H. Liu. High-dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality. In Advances in Neural Information Processing Systems 28, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Gu%2C%20Q.%20Ning%2C%20Y.%20Liu%2C%20H.%20High-dimensional%20expectation-maximization%20algorithm%3A%20Statistical%20optimization%20and%20asymptotic%20normality%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Gu%2C%20Q.%20Ning%2C%20Y.%20Liu%2C%20H.%20High-dimensional%20expectation-maximization%20algorithm%3A%20Statistical%20optimization%20and%20asymptotic%20normality%202015"
        },
        {
            "id": "18",
            "entry": "[18] C. F. J. Wu. On the convergence properties of the EM algorithm. Annals of Statistics, 11:95\u2013 103, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20C.F.J.%20On%20the%20convergence%20properties%20of%20the%20EM%20algorithm%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20C.F.J.%20On%20the%20convergence%20properties%20of%20the%20EM%20algorithm%201983"
        },
        {
            "id": "19",
            "entry": "[19] J. Xu, D. Hsu, and A. Maleki. Global analysis of expectation maximization for mixtures of two Gaussians. In Advances in Neural Information Processing Systems 29, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20J.%20Hsu%2C%20D.%20Maleki%2C%20A.%20Global%20analysis%20of%20expectation%20maximization%20for%20mixtures%20of%20two%20Gaussians%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20J.%20Hsu%2C%20D.%20Maleki%2C%20A.%20Global%20analysis%20of%20expectation%20maximization%20for%20mixtures%20of%20two%20Gaussians%202016"
        },
        {
            "id": "20",
            "entry": "[20] L. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures. Neural Computation, 8:129\u2013151, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20L.%20Jordan%2C%20M.I.%20On%20convergence%20properties%20of%20the%20EM%20algorithm%20for%20Gaussian%20mixtures%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20L.%20Jordan%2C%20M.I.%20On%20convergence%20properties%20of%20the%20EM%20algorithm%20for%20Gaussian%20mixtures%201996"
        },
        {
            "id": "21",
            "entry": "[21] B. Yan, M. Yin, and P. Sarkar. Convergence of gradient EM on multi-component mixture of Gaussians. In Advances in Neural Information Processing Systems 30, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20B.%20Yin%2C%20M.%20Sarkar%2C%20P.%20Convergence%20of%20gradient%20EM%20on%20multi-component%20mixture%20of%20Gaussians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20B.%20Yin%2C%20M.%20Sarkar%2C%20P.%20Convergence%20of%20gradient%20EM%20on%20multi-component%20mixture%20of%20Gaussians%202017"
        },
        {
            "id": "22",
            "entry": "[22] X. Yi and C. Caramanis. Regularized em algorithms: A unified framework and statistical guarantees. In Advances in Neural Information Processing Systems 28, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20X.%20Caramanis%2C%20C.%20Regularized%20em%20algorithms%3A%20A%20unified%20framework%20and%20statistical%20guarantees%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yi%2C%20X.%20Caramanis%2C%20C.%20Regularized%20em%20algorithms%3A%20A%20unified%20framework%20and%20statistical%20guarantees%202015"
        }
    ]
}
