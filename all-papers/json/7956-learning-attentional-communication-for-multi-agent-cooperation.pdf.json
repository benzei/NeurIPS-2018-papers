{
    "filename": "7956-learning-attentional-communication-for-multi-agent-cooperation.pdf",
    "metadata": {
        "title": "Learning Attentional Communication for Multi-Agent Cooperation",
        "author": "Jiechuan Jiang, Zongqing Lu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7956-learning-attentional-communication-for-multi-agent-cooperation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely helps, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in a variety of cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "ATOC",
            "url": "https://en.wikipedia.org/wiki/ATOC"
        }
    ],
    "highlights": [
        "Unlike CommNet and BiCNet that perform arithmetic mean and weighted mean of internal states, respectively, our LSTM unit selectively outputs important information for cooperative decision making, which makes it possible for agents to learn coordinated strategies in dynamic communication environments",
        "We propose an attentional communication model, called ATOC, to enable agents to learn effective and efficient communication under partially observable distributed environment for large-scale multi-agent reinforcement learning",
        "Since all agents share the parameters of the policy network, Q-network, attention unit, and communication channel, ATOC is suitable for large-scale multi-agent environments",
        "Combining reinforcement learning with a class of deep neural networks, Deep Q-Networks [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] has performed at a level that is comparable to a professional game player",
        "Agents that share a policy network may be homogeneous in terms of strategy, e.g., Deep Deterministic Policy Gradient agents are all aggressive to seize the landmarks while CommNet and BiCNet agents are all conservative",
        "We have proposed an attentional communication model in large-scale multi-agent environments, where agents learn an attention unit that dynamically determines whether communication is needed for cooperation and a bi-directional LSTM unit as a communication channel to interpret encoded information from other agents"
    ],
    "key_statements": [
        "Unlike CommNet and BiCNet that perform arithmetic mean and weighted mean of internal states, respectively, our LSTM unit selectively outputs important information for cooperative decision making, which makes it possible for agents to learn coordinated strategies in dynamic communication environments",
        "We propose an attentional communication model, called ATOC, to enable agents to learn effective and efficient communication under partially observable distributed environment for large-scale multi-agent reinforcement learning",
        "Since all agents share the parameters of the policy network, Q-network, attention unit, and communication channel, ATOC is suitable for large-scale multi-agent environments",
        "We empirically show the success of ATOC in three scenarios, which correspond to the cooperation of agents for local reward, a shared global reward, and reward in competition, respectively",
        "Combining reinforcement learning with a class of deep neural networks, Deep Q-Networks [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] has performed at a level that is comparable to a professional game player",
        "Consider a game with N agents, and the critic, actor, communication channel, and attention unit of ATOC is parameterized by \u2713Q,",
        "We evaluate ATOC and the baselines by running 30 test games and measure average mean reward, number of collisions, and percentage of occupied landmarks",
        "Agents that share a policy network may be homogeneous in terms of strategy, e.g., Deep Deterministic Policy Gradient agents are all aggressive to seize the landmarks while CommNet and BiCNet agents are all conservative",
        "The second bar cluster shows the scores of the games where ATOC predators are against Deep Deterministic Policy Gradient, CommNet, and BiCNet preys",
        "The third bar cluster shows the games where Deep Deterministic Policy Gradient, CommNet, and BiCNet predators are against ATOC preys",
        "We argue that ATOC leads to better cooperation than the baselines even in competitive environments and the learned policy of ATOC predators and preys can generalize to the opponents with different policies",
        "We have proposed an attentional communication model in large-scale multi-agent environments, where agents learn an attention unit that dynamically determines whether communication is needed for cooperation and a bi-directional LSTM unit as a communication channel to interpret encoded information from other agents"
    ],
    "summary": [
        "Unlike CommNet and BiCNet that perform arithmetic mean and weighted mean of internal states, respectively, our LSTM unit selectively outputs important information for cooperative decision making, which makes it possible for agents to learn coordinated strategies in dynamic communication environments.",
        "Since all agents share the parameters of the policy network, Q-network, attention unit, and communication channel, ATOC is suitable for large-scale multi-agent environments.",
        "By sharing encoding of local observation and action intention within a dynamically formed group, individual agents could build up relatively more global perception of the environment, infer the intent of other agents, and cooperate on decision making.",
        "Our attention unit never senses the environment in full, but only uses encoding of observable field and action intention of an agent and decides whether communication is helpful in terms of cooperation.",
        "Unlike existing work on learning communication in MARL, e.g., CommNet and BiCNet, where all agents communicate with each other all the time, our attention unit enables dynamic communication among agents only when necessary.",
        "Consider a game with N agents, and the critic, actor, communication channel, and attention unit of ATOC is parameterized by \u2713Q,",
        "Unlike CommNet and BiCNet, ATOC exploits the attention unit to dynamically perform communication, and most information is from nearby agents and helpful for decision making.",
        "Agents that share a policy network may be homogeneous in terms of strategy, e.g., DDPG agents are all aggressive to seize the landmarks while CommNet and BiCNet agents are all conservative.",
        "In ATOC, cooperative policy gradients can backpropagate to update individual policy networks, which enables agents to infer the actions of other agents without communication and behaves cooperatively.",
        "The second bar cluster shows the scores of the games where ATOC predators are against DDPG, CommNet, and BiCNet preys.",
        "The third bar cluster shows the games where DDPG, CommNet, and BiCNet predators are against ATOC preys.",
        "We argue that ATOC leads to better cooperation than the baselines even in competitive environments and the learned policy of ATOC predators and preys can generalize to the opponents with different policies.",
        "We have proposed an attentional communication model in large-scale multi-agent environments, where agents learn an attention unit that dynamically determines whether communication is needed for cooperation and a bi-directional LSTM unit as a communication channel to interpret encoded information from other agents.",
        "ATOC outperforms existing methods in a variety of cooperative multi-agent environments"
    ],
    "headline": "In this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Yongcan Cao, Wenwu Yu, Wei Ren, and Guanrong Chen. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1):427\u2013438, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20Yongcan%20Yu%2C%20Wenwu%20Ren%2C%20Wei%20Chen%2C%20Guanrong%20An%20overview%20of%20recent%20progress%20in%20the%20study%20of%20distributed%20multi-agent%20coordination%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20Yongcan%20Yu%2C%20Wenwu%20Ren%2C%20Wei%20Chen%2C%20Guanrong%20An%20overview%20of%20recent%20progress%20in%20the%20study%20of%20distributed%20multi-agent%20coordination%202013"
        },
        {
            "id": "2",
            "entry": "[2] Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. Deep communicating agents for abstractive summarization. In NAACL, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Celikyilmaz%2C%20Asli%20Bosselut%2C%20Antoine%20He%2C%20Xiaodong%20Choi%2C%20Yejin%20Deep%20communicating%20agents%20for%20abstractive%20summarization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Celikyilmaz%2C%20Asli%20Bosselut%2C%20Antoine%20He%2C%20Xiaodong%20Choi%2C%20Yejin%20Deep%20communicating%20agents%20for%20abstractive%20summarization%202018"
        },
        {
            "id": "3",
            "entry": "[3] Dorothy Cheney and Robert Seyfarth. Constraints and preadaptations in the earliest stages of language evolution. Linguistic Review, 22(2-4):135\u2013159, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheney%2C%20Dorothy%20Seyfarth%2C%20Robert%20Constraints%20and%20preadaptations%20in%20the%20earliest%20stages%20of%20language%20evolution%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheney%2C%20Dorothy%20Seyfarth%2C%20Robert%20Constraints%20and%20preadaptations%20in%20the%20earliest%20stages%20of%20language%20evolution%202005"
        },
        {
            "id": "4",
            "entry": "[4] Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20Assael%2C%20Ioannis%20Alexandros%20de%20Freitas%2C%20Nando%20Whiteson%2C%20Shimon%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20Assael%2C%20Ioannis%20Alexandros%20de%20Freitas%2C%20Nando%20Whiteson%2C%20Shimon%20Learning%20to%20communicate%20with%20deep%20multi-agent%20reinforcement%20learning%202016"
        },
        {
            "id": "5",
            "entry": "[5] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence (AAAI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202018"
        },
        {
            "id": "6",
            "entry": "[6] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In IEEE International Conference on Robotics and Automation (ICRA), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Shixiang%20Holly%2C%20Ethan%20Lillicrap%2C%20Timothy%20Levine%2C%20Sergey%20Deep%20reinforcement%20learning%20for%20robotic%20manipulation%20with%20asynchronous%20off-policy%20updates%202017"
        },
        {
            "id": "7",
            "entry": "[7] Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: learning to communicate with sequences of symbols. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Havrylov%2C%20Serhii%20Titov%2C%20Ivan%20Emergence%20of%20language%20with%20multi-agent%20games%3A%20learning%20to%20communicate%20with%20sequences%20of%20symbols%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Havrylov%2C%20Serhii%20Titov%2C%20Ivan%20Emergence%20of%20language%20with%20multi-agent%20games%3A%20learning%20to%20communicate%20with%20sequences%20of%20symbols%202017"
        },
        {
            "id": "8",
            "entry": "[8] Xiangyu Kong, Bo Xin, Fangchen Liu, and Yizhou Wang. Revisiting the master-slave architecture in multi-agent deep reinforcement learning. arXiv preprint arXiv:1712.07305, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07305"
        },
        {
            "id": "9",
            "entry": "[9] Guillaume Lample and Devendra Singh Chaplot. Playing fps games with deep reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20Guillaume%20Chaplot%2C%20Devendra%20Singh%20Playing%20fps%20games%20with%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lample%2C%20Guillaume%20Chaplot%2C%20Devendra%20Singh%20Playing%20fps%20games%20with%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "10",
            "entry": "[10] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Julien Perolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20Marc%20Zambaldi%2C%20Vinicius%20Gruslys%2C%20Audrunas%20Lazaridou%2C%20Angeliki%20and%20Thore%20Graepel.%20A%20unified%20game-theoretic%20approach%20to%20multiagent%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lanctot%2C%20Marc%20Zambaldi%2C%20Vinicius%20Gruslys%2C%20Audrunas%20Lazaridou%2C%20Angeliki%20and%20Thore%20Graepel.%20A%20unified%20game-theoretic%20approach%20to%20multiagent%20reinforcement%20learning%202017"
        },
        {
            "id": "11",
            "entry": "[11] Hoang M Le, Yisong Yue, Peter Carr, and Patrick Lucey. Coordinated multi-agent imitation learning. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Hoang%20M.%20Yue%2C%20Yisong%20Carr%2C%20Peter%20Lucey%2C%20Patrick%20Coordinated%20multi-agent%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Hoang%20M.%20Yue%2C%20Yisong%20Carr%2C%20Peter%20Lucey%2C%20Patrick%20Coordinated%20multi-agent%20imitation%20learning%202017"
        },
        {
            "id": "12",
            "entry": "[12] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "13",
            "entry": "[13] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20Continuous%20control%20with%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "14",
            "entry": "[14] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lowe%2C%20Ryan%20Wu%2C%20Yi%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Multi-agent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017"
        },
        {
            "id": "15",
            "entry": "[15] La\u00ebtitia Matignon, Laurent Jeanpierre, Abdel-Illah Mouaddib, et al. Coordinated multi-robot exploration under communication constraints using decentralized markov decision processes. In AAAI Conference on Artificial Intelligence (AAAI), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matignon%2C%20La%C3%ABtitia%20Jeanpierre%2C%20Laurent%20Mouaddib%2C%20Abdel-Illah%20Coordinated%20multi-robot%20exploration%20under%20communication%20constraints%20using%20decentralized%20markov%20decision%20processes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matignon%2C%20La%C3%ABtitia%20Jeanpierre%2C%20Laurent%20Mouaddib%2C%20Abdel-Illah%20Coordinated%20multi-robot%20exploration%20under%20communication%20constraints%20using%20decentralized%20markov%20decision%20processes%202012"
        },
        {
            "id": "16",
            "entry": "[16] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual attention. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Kavukcuoglu%2C%20Koray%20Recurrent%20models%20of%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Heess%2C%20Nicolas%20Graves%2C%20Alex%20Kavukcuoglu%2C%20Koray%20Recurrent%20models%20of%20visual%20attention%202014"
        },
        {
            "id": "17",
            "entry": "[17] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "18",
            "entry": "[18] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. In AAAI Conference on Artificial Intelligence (AAAI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mordatch%2C%20Igor%20Abbeel%2C%20Pieter%20Emergence%20of%20grounded%20compositional%20language%20in%20multi-agent%20populations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mordatch%2C%20Igor%20Abbeel%2C%20Pieter%20Emergence%20of%20grounded%20compositional%20language%20in%20multi-agent%20populations%202018"
        },
        {
            "id": "19",
            "entry": "[19] Peng Peng, Quan Yuan, Ying Wen, Yaodong Yang, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10069"
        },
        {
            "id": "20",
            "entry": "[20] Manisa Pipattanasomporn, Hassan Feroze, and Saifur Rahman. Multi-agent systems in a distributed smart grid: Design and implementation. In IEEE/PES Power Systems Conference and Exposition, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pipattanasomporn%2C%20Manisa%20Feroze%2C%20Hassan%20Rahman%2C%20Saifur%20Multi-agent%20systems%20in%20a%20distributed%20smart%20grid%3A%20Design%20and%20implementation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pipattanasomporn%2C%20Manisa%20Feroze%2C%20Hassan%20Rahman%2C%20Saifur%20Multi-agent%20systems%20in%20a%20distributed%20smart%20grid%3A%20Design%20and%20implementation%202009"
        },
        {
            "id": "21",
            "entry": "[21] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning (ICML), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014"
        },
        {
            "id": "22",
            "entry": "[22] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "23",
            "entry": "[23] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Szlam%2C%20Arthur%20Fergus%2C%20Rob%20Learning%20multiagent%20communication%20with%20backpropagation%202016"
        },
        {
            "id": "24",
            "entry": "[24] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Yaodong%20Luo%2C%20Rui%20Li%2C%20Minne%20Zhou%2C%20Ming%20Mean%20field%20multi-agent%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Yaodong%20Luo%2C%20Rui%20Li%2C%20Minne%20Zhou%2C%20Ming%20Mean%20field%20multi-agent%20reinforcement%20learning%202018"
        }
    ]
}
