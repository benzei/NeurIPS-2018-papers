{
    "filename": "8002-cooperative-learning-of-audio-and-video-models-from-self-supervised-synchronization.pdf",
    "metadata": {
        "title": "Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization",
        "author": "Bruno Korbar, Du Tran, Lorenzo Torresani",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8002-cooperative-learning-of-audio-and-video-models-from-self-supervised-synchronization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9% in action recognition accuracy on UCF101 and a boost of +17.7% on HMDB51."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "action recognition",
            "url": "https://en.wikipedia.org/wiki/action_recognition"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "audio visual",
            "url": "https://en.wikipedia.org/wiki/audio_visual"
        },
        {
            "term": "ECCV",
            "url": "https://en.wikipedia.org/wiki/ECCV"
        }
    ],
    "highlights": [
        "Image recognition has undergone dramatic progress since the breakthrough of AlexNet [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and the widespread availability of progressively large datasets such as Imagenet [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "On downstream tasks, we found audio-visual temporal synchronization fine-tuning using the cross-entropy loss to yield no further improvement after the contrastive loss optimization.\n2.3",
        "We found that when hard negatives are introduced from the beginning \u2014 either fully or as a proportion \u2014 the objective is very difficult to optimize and test results on the audio-visual temporal synchronization task are poor",
        "Inclusion of hard negatives during training allows us to maintain high performance even when hard negatives are included in the testing set (70% on Kinetics), whereas we found that the performance of L3-Net drops drastically when hard-negatives are included in the test set (57% for L3-Net vs 70% for our audio-visual temporal synchronization)",
        "We report action recognition results using the I3D-RGB [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] network trained in several ways: learned from scratch, pretrained using our self-supervised audio-visual temporal synchronization, or pretrained with category labels",
        "In this work we have shown that the self-supervised mechanism of audio-visual temporal synchronization (AVTS) can be used to learn general and effective models for both the audio and the vision domain"
    ],
    "key_statements": [
        "Image recognition has undergone dramatic progress since the breakthrough of AlexNet [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and the widespread availability of progressively large datasets such as Imagenet [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "In our experiments we study several such applications, including pretraining for action recognition in video, feature extraction for audio classification, as well as multisensory video categorization",
        "We show that our visual subnet provides a very effective initialization to improve the performance of action recognition networks on medium-size video classification datasets, such as HMDB51 [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] and UCF101 [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We found both these approaches to perform on the audio-visual temporal synchronization task, with a slight edge in favor of the fine-tuning solution",
        "On downstream tasks, we found audio-visual temporal synchronization fine-tuning using the cross-entropy loss to yield no further improvement after the contrastive loss optimization.\n2.3",
        "We found that including either hard or super-hard negatives as additional training examples was detrimental when the negative examples in the test set consisted of only \u201ceasy\u201d negatives",
        "We found that when hard negatives are introduced from the beginning \u2014 either fully or as a proportion \u2014 the objective is very difficult to optimize and test results on the audio-visual temporal synchronization task are poor",
        "Even more remarkable are the performance improvements enabled by curriculum feature learning on the downstream tasks of audio classification and action recognition. 2.5",
        "Inclusion of hard negatives during training allows us to maintain high performance even when hard negatives are included in the testing set (70% on Kinetics), whereas we found that the performance of L3-Net drops drastically when hard-negatives are included in the test set (57% for L3-Net vs 70% for our audio-visual temporal synchronization)",
        "We report action recognition results using the I3D-RGB [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] network trained in several ways: learned from scratch, pretrained using our self-supervised audio-visual temporal synchronization, or pretrained with category labels",
        "We evaluate the audio features learned by our audio-visual temporal synchronization procedure by minimization of the contrastive loss",
        "We can observe that audio features learned on audio-visual temporal synchronization generalize extremely well on both of these two sound classification tasks, yielding performance superior or close to the state-of-the-art",
        "We evaluate our approach on the task of multi-modal action recognition, i.e., using both the audio and the visual stream in the video to predict actions",
        "In this work we have shown that the self-supervised mechanism of audio-visual temporal synchronization (AVTS) can be used to learn general and effective models for both the audio and the vision domain",
        "While in this work we trained audio-visual temporal synchronization on established, labeled video dataset in order to have a direct comparison with fully-supervised pretraining methods, our approach is self-supervised and does not require any manual labeling"
    ],
    "summary": [
        "Image recognition has undergone dramatic progress since the breakthrough of AlexNet [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and the widespread availability of progressively large datasets such as Imagenet [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>].",
        "We show that our visual subnet provides a very effective initialization to improve the performance of action recognition networks on medium-size video classification datasets, such as HMDB51 [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] and UCF101 [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "The purpose of hard negatives is to train the network to recognize temporal synchronization as opposed to mere semantic correspondence between the audio and the visual input.",
        "Even more remarkable are the performance improvements enabled by curriculum feature learning on the downstream tasks of audio classification and action recognition.",
        "After AVTS training with contrastive loss on Kinetics, we fine-tune our video subnetwork on two medium-size action recognition benchmarks: UCF101 [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] and HMDB51 [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>].",
        "We report action recognition results using the I3D-RGB [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] network trained in several ways: learned from scratch, pretrained using our self-supervised AVTS, or pretrained with category labels.",
        "We can observe that audio features learned on AVTS generalize extremely well on both of these two sound classification tasks, yielding performance superior or close to the state-of-the-art.",
        "As for AVTS classification, we concatenate the audio and the visual features obtained from our two subnetworks and add two fully connected layers, and fine-tune the entire system for action recognition with cross entropy loss as the training objective.",
        "We broaden the scope of the study to encompass arbitrary human activities and experimentally evaluate the task as a self-supervised mechanism for general audio-visual feature learning.",
        "The work of Owens and Efros [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] is similar in spirit to our own but we present stronger experimental results on comparable benchmarks (e.g., UCF101) and our technical approach differs substantially in the use of hard-negatives, curriculum learning and the choice of contrastive loss as learning objective.",
        "In this work we have shown that the self-supervised mechanism of audio-visual temporal synchronization (AVTS) can be used to learn general and effective models for both the audio and the vision domain.",
        "While in this work we trained AVTS on established, labeled video dataset in order to have a direct comparison with fully-supervised pretraining methods, our approach is self-supervised and does not require any manual labeling.",
        "We believe that this may yield further improvements in the generality and effectiveness of our models for downstream tasks in the audio and video domain and it may help bridge the remaining gap with respect to fully-supervised pretraining that rely on costly manual annotations.",
        "We have shown that curriculum learning, i.e., introducing harder negatives in a second stage of training, significantly improves the quality of the features on all end tasks"
    ],
    "headline": "We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "2",
            "entry": "[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "3",
            "entry": "[3] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 91\u201399, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20B.%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20B.%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015-12-07"
        },
        {
            "id": "4",
            "entry": "[4] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 2980\u20132988, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaiming%20He%20Georgia%20Gkioxari%20Piotr%20Doll%C3%A1r%20and%20Ross%20B%20Girshick%20Mask%20RCNN%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%202017%20Venice%20Italy%20October%202229%202017%20pages%2029802988%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaiming%20He%20Georgia%20Gkioxari%20Piotr%20Doll%C3%A1r%20and%20Ross%20B%20Girshick%20Mask%20RCNN%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%202017%20Venice%20Italy%20October%202229%202017%20pages%2029802988%202017"
        },
        {
            "id": "5",
            "entry": "[5] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20Zhe%20Simon%2C%20Tomas%20Wei%2C%20Shih-En%20Sheikh%2C%20Yaser%20Realtime%20multi-person%202d%20pose%20estimation%20using%20part%20affinity%20fields%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20Zhe%20Simon%2C%20Tomas%20Wei%2C%20Shih-En%20Sheikh%2C%20Yaser%20Realtime%20multi-person%202d%20pose%20estimation%20using%20part%20affinity%20fields%202017"
        },
        {
            "id": "6",
            "entry": "[6] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 4724\u20134732, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Shih-En%20Ramakrishna%2C%20Varun%20Kanade%2C%20Takeo%20Sheikh%2C%20Yaser%20Convolutional%20pose%20machines%202016-06-27",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Shih-En%20Ramakrishna%2C%20Varun%20Kanade%2C%20Takeo%20Sheikh%2C%20Yaser%20Convolutional%20pose%20machines%202016-06-27"
        },
        {
            "id": "7",
            "entry": "[7] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3431\u20133440, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015-06-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Jonathan%20Shelhamer%2C%20Evan%20Darrell%2C%20Trevor%20Fully%20convolutional%20networks%20for%20semantic%20segmentation%202015-06-07"
        },
        {
            "id": "8",
            "entry": "[8] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. CoRR, abs/1511.07122, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07122"
        },
        {
            "id": "9",
            "entry": "[9] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Fei-Fei Li. Large-scale video classification with convolutional neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 1725\u20131732, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014-06-23",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karpathy%2C%20Andrej%20Toderici%2C%20George%20Shetty%2C%20Sanketh%20Leung%2C%20Thomas%20Large-scale%20video%20classification%20with%20convolutional%20neural%20networks%202014-06-23"
        },
        {
            "id": "10",
            "entry": "[10] Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 4489\u20134497, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20D.%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran%2C%20Du%20Bourdev%2C%20Lubomir%20D.%20Fergus%2C%20Rob%20Torresani%2C%20Lorenzo%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks%202015-12-07"
        },
        {
            "id": "11",
            "entry": "[11] Heng Wang and Cordelia Schmid. Action recognition with improved trajectories. In IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 3551\u20133558, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Heng%20Schmid%2C%20Cordelia%20Action%20recognition%20with%20improved%20trajectories%202013-12-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Heng%20Schmid%2C%20Cordelia%20Action%20recognition%20with%20improved%20trajectories%202013-12-01"
        },
        {
            "id": "12",
            "entry": "[12] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. CoRR, abs/1705.06950, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06950"
        },
        {
            "id": "13",
            "entry": "[13] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Caroline Pantofaru, David A. Ross, George Toderici, Yeqing Li, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik. AVA: A video dataset of spatio-temporally localized atomic visual actions. CoRR, abs/1705.08421, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08421"
        },
        {
            "id": "14",
            "entry": "[14] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 961\u2013970, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heilbron%2C%20Fabian%20Caba%20Escorcia%2C%20Victor%20Ghanem%2C%20Bernard%20Niebles%2C%20Juan%20Carlos%20Activitynet%3A%20A%20large-scale%20video%20benchmark%20for%20human%20activity%20understanding%202015-06-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heilbron%2C%20Fabian%20Caba%20Escorcia%2C%20Victor%20Ghanem%2C%20Bernard%20Niebles%2C%20Juan%20Carlos%20Activitynet%3A%20A%20large-scale%20video%20benchmark%20for%20human%20activity%20understanding%202015-06-07"
        },
        {
            "id": "15",
            "entry": "[15] Hang Zhao, Zhicheng Yan, Heng Wang, Lorenzo Torresani, and Antonio Torralba. SLAC: A sparsely labeled dataset for action classification and localization. CoRR, abs/1712.09374, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09374"
        },
        {
            "id": "16",
            "entry": "[16] Gunnar A. Sigurdsson, G\u00fcl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I, pages 510\u2013526, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunnar%20A%20Sigurdsson%20G%C3%BCl%20Varol%20Xiaolong%20Wang%20Ali%20Farhadi%20Ivan%20Laptev%20and%20Abhinav%20Gupta%20Hollywood%20in%20homes%20Crowdsourcing%20data%20collection%20for%20activity%20understanding%20In%20Computer%20Vision%20%20ECCV%202016%20%2014th%20European%20Conference%20Amsterdam%20The%20Netherlands%20October%201114%202016%20Proceedings%20Part%20I%20pages%20510526%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunnar%20A%20Sigurdsson%20G%C3%BCl%20Varol%20Xiaolong%20Wang%20Ali%20Farhadi%20Ivan%20Laptev%20and%20Abhinav%20Gupta%20Hollywood%20in%20homes%20Crowdsourcing%20data%20collection%20for%20activity%20understanding%20In%20Computer%20Vision%20%20ECCV%202016%20%2014th%20European%20Conference%20Amsterdam%20The%20Netherlands%20October%201114%202016%20Proceedings%20Part%20I%20pages%20510526%202016"
        },
        {
            "id": "17",
            "entry": "[17] Christoph Feichtenhofer, Axel Pinz, and Richard Wildes. Spatiotemporal residual networks for video action recognition. In Advances in neural information processing systems, pages 3468\u20133476, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feichtenhofer%2C%20Christoph%20Pinz%2C%20Axel%20Wildes%2C%20Richard%20Spatiotemporal%20residual%20networks%20for%20video%20action%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feichtenhofer%2C%20Christoph%20Pinz%2C%20Axel%20Wildes%2C%20Richard%20Spatiotemporal%20residual%20networks%20for%20video%20action%20recognition%202016"
        },
        {
            "id": "18",
            "entry": "[18] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 4724\u20134733, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20Jo%C3%A3o%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20A%20new%20model%20and%20the%20kinetics%20dataset%202017-07-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20Jo%C3%A3o%20Zisserman%2C%20Andrew%20Quo%20vadis%2C%20action%20recognition%3F%20A%20new%20model%20and%20the%20kinetics%20dataset%202017-07-21"
        },
        {
            "id": "19",
            "entry": "[19] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran%2C%20Du%20Wang%2C%20Heng%20Torresani%2C%20Lorenzo%20Ray%2C%20Jamie%20A%20closer%20look%20at%20spatiotemporal%20convolutions%20for%20action%20recognition%202017"
        },
        {
            "id": "20",
            "entry": "[20] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled video. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 892\u2013900, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aytar%2C%20Yusuf%20Vondrick%2C%20Carl%20Torralba%2C%20Antonio%20Soundnet%3A%20Learning%20sound%20representations%20from%20unlabeled%20video%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aytar%2C%20Yusuf%20Vondrick%2C%20Carl%20Torralba%2C%20Antonio%20Soundnet%3A%20Learning%20sound%20representations%20from%20unlabeled%20video%202016-12-05"
        },
        {
            "id": "21",
            "entry": "[21] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 609\u2013617, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Relja%20Arandjelovic%20and%20Andrew%20Zisserman%20Look%20listen%20and%20learn%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%202017%20Venice%20Italy%20October%202229%202017%20pages%20609617%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Relja%20Arandjelovic%20and%20Andrew%20Zisserman%20Look%20listen%20and%20learn%20In%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%202017%20Venice%20Italy%20October%202229%202017%20pages%20609617%202017"
        },
        {
            "id": "22",
            "entry": "[22] Relja Arandjelovic and Andrew Zisserman. Objects that sound. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part I, pages 451\u2013466, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Relja%20Arandjelovic%20and%20Andrew%20Zisserman%20Objects%20that%20sound%20In%20Computer%20Vision%20%20ECCV%202018%20%2015th%20European%20Conference%20Munich%20Germany%20September%20814%202018%20Proceedings%20Part%20I%20pages%20451466%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Relja%20Arandjelovic%20and%20Andrew%20Zisserman%20Objects%20that%20sound%20In%20Computer%20Vision%20%20ECCV%202018%20%2015th%20European%20Conference%20Munich%20Germany%20September%20814%202018%20Proceedings%20Part%20I%20pages%20451466%202018"
        },
        {
            "id": "23",
            "entry": "[23] Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 41\u201348, New York, NY, USA, 2009. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "24",
            "entry": "[24] Hilde Kuehne, Hueihan Jhuang, Rainer Stiefelhagen, and Thomas Serre. Hmdb51: A large video database for human motion recognition. In High Performance Computing in Science and Engineering \u201812, pages 571\u2013582.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuehne%2C%20Hilde%20Jhuang%2C%20Hueihan%20Stiefelhagen%2C%20Rainer%20Serre%2C%20Thomas%20Hmdb51%3A%20A%20large%20video%20database%20for%20human%20motion%20recognition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuehne%2C%20Hilde%20Jhuang%2C%20Hueihan%20Stiefelhagen%2C%20Rainer%20Serre%2C%20Thomas%20Hmdb51%3A%20A%20large%20video%20database%20for%20human%20motion%20recognition"
        },
        {
            "id": "25",
            "entry": "[25] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.0402"
        },
        {
            "id": "26",
            "entry": "[26] Joon Son Chung and Andrew Zisserman. Out of time: Automated lip sync in the wild. In Computer Vision - ACCV 2016 Workshops - ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II, pages 251\u2013263, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joon%20Son%20Chung%20and%20Andrew%20Zisserman%20Out%20of%20time%20Automated%20lip%20sync%20in%20the%20wild%20In%20Computer%20Vision%20%20ACCV%202016%20Workshops%20%20ACCV%202016%20International%20Workshops%20Taipei%20Taiwan%20November%202024%202016%20Revised%20Selected%20Papers%20Part%20II%20pages%20251263%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joon%20Son%20Chung%20and%20Andrew%20Zisserman%20Out%20of%20time%20Automated%20lip%20sync%20in%20the%20wild%20In%20Computer%20Vision%20%20ACCV%202016%20Workshops%20%20ACCV%202016%20International%20Workshops%20Taipei%20Taiwan%20November%202024%202016%20Revised%20Selected%20Papers%20Part%20II%20pages%20251263%202016"
        },
        {
            "id": "27",
            "entry": "[27] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop, volume 2, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koch%2C%20Gregory%20Zemel%2C%20Richard%20Salakhutdinov%2C%20Ruslan%20Siamese%20neural%20networks%20for%20one-shot%20image%20recognition%202015"
        },
        {
            "id": "28",
            "entry": "[28] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In Proc. IEEE ICASSP 2017, New Orleans, LA, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gemmeke%2C%20Jort%20F.%20Ellis%2C%20Daniel%20P.W.%20Freedman%2C%20Dylan%20Jansen%2C%20Aren%20Audio%20set%3A%20An%20ontology%20and%20human-labeled%20dataset%20for%20audio%20events%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gemmeke%2C%20Jort%20F.%20Ellis%2C%20Daniel%20P.W.%20Freedman%2C%20Dylan%20Jansen%2C%20Aren%20Audio%20set%3A%20An%20ontology%20and%20human-labeled%20dataset%20for%20audio%20events%202017"
        },
        {
            "id": "29",
            "entry": "[29] Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, pages 1015\u20131018. ACM Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Piczak%2C%20Karol%20J.%20ESC%3A%20Dataset%20for%20Environmental%20Sound%20Classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Piczak%2C%20Karol%20J.%20ESC%3A%20Dataset%20for%20Environmental%20Sound%20Classification%202015"
        },
        {
            "id": "30",
            "entry": "[30] Dan Stowell, Dimitrios Giannoulis, Emmanouil Benetos, Mathieu Lagrange, and Mark D Plumbley. Detection and classification of acoustic scenes and events. IEEE Transactions on Multimedia, 17(10):1733\u20131746, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stowell%2C%20Dan%20Giannoulis%2C%20Dimitrios%20Benetos%2C%20Emmanouil%20Lagrange%2C%20Mathieu%20Detection%20and%20classification%20of%20acoustic%20scenes%20and%20events%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stowell%2C%20Dan%20Giannoulis%2C%20Dimitrios%20Benetos%2C%20Emmanouil%20Lagrange%2C%20Mathieu%20Detection%20and%20classification%20of%20acoustic%20scenes%20and%20events%202015"
        },
        {
            "id": "31",
            "entry": "[31] Hardik B Sailor, Dharmesh M Agrawal, and Hemant A Patil. Unsupervised filterbank learning using convolutional restricted boltzmann machine for environmental sound classification. Proc. Interspeech 2017, pages 3107\u20133111, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sailor%2C%20Hardik%20B.%20Agrawal%2C%20Dharmesh%20M.%20and%20Hemant%20A%20Patil.%20Unsupervised%20filterbank%20learning%20using%20convolutional%20restricted%20boltzmann%20machine%20for%20environmental%20sound%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sailor%2C%20Hardik%20B.%20Agrawal%2C%20Dharmesh%20M.%20and%20Hemant%20A%20Patil.%20Unsupervised%20filterbank%20learning%20using%20convolutional%20restricted%20boltzmann%20machine%20for%20environmental%20sound%20classification%202017"
        },
        {
            "id": "32",
            "entry": "[32] Andrew Owens and Alexei A. Efros. Audio-visual scene analysis with self-supervised multisensory features. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VI, pages 639\u2013658, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrew%20Owens%20and%20Alexei%20A%20Efros%20Audiovisual%20scene%20analysis%20with%20selfsupervised%20multisensory%20features%20In%20Computer%20Vision%20%20ECCV%202018%20%2015th%20European%20Conference%20Munich%20Germany%20September%20814%202018%20Proceedings%20Part%20VI%20pages%20639658%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrew%20Owens%20and%20Alexei%20A%20Efros%20Audiovisual%20scene%20analysis%20with%20selfsupervised%20multisensory%20features%20In%20Computer%20Vision%20%20ECCV%202018%20%2015th%20European%20Conference%20Munich%20Germany%20September%20814%202018%20Proceedings%20Part%20VI%20pages%20639658%202018"
        },
        {
            "id": "33",
            "entry": "[33] Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527\u20131554, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee%20Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee%20Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006"
        },
        {
            "id": "34",
            "entry": "[34] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pages 153\u2013160, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202006-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202006-12"
        },
        {
            "id": "35",
            "entry": "[35] Marc\u2019Aurelio Ranzato, Fu Jie Huang, Y-Lan Boureau, and Yann LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007), 18-23 June 2007, Minneapolis, Minnesota, USA, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20Marc%E2%80%99Aurelio%20Fu%20Jie%20Huang%2C%20Y.-Lan%20Boureau%20LeCun%2C%20Yann%20Unsupervised%20learning%20of%20invariant%20feature%20hierarchies%20with%20applications%20to%20object%20recognition%202007-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20Marc%E2%80%99Aurelio%20Fu%20Jie%20Huang%2C%20Y.-Lan%20Boureau%20LeCun%2C%20Yann%20Unsupervised%20learning%20of%20invariant%20feature%20hierarchies%20with%20applications%20to%20object%20recognition%202007-06"
        },
        {
            "id": "36",
            "entry": "[36] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efficient sparse coding algorithms. In Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pages 801\u2013808, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Honglak%20Battle%2C%20Alexis%20Raina%2C%20Rajat%20Ng%2C%20Andrew%20Y.%20Efficient%20sparse%20coding%20algorithms%202006-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Honglak%20Battle%2C%20Alexis%20Raina%2C%20Rajat%20Ng%2C%20Andrew%20Y.%20Efficient%20sparse%20coding%20algorithms%202006-12"
        },
        {
            "id": "37",
            "entry": "[37] Quoc Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, and Andrew Ng. Building high-level features using large scale unsupervised learning. In ICML, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202011"
        },
        {
            "id": "38",
            "entry": "[38] Quoc V. Le, Will Y. Zou, Serena Y. Yeung, and Andrew Y. Ng. Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In The 24th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011, Colorado Springs, CO, USA, 20-25 June 2011, pages 3361\u20133368, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20V.%20Zou%2C%20Will%20Y.%20Yeung%2C%20Serena%20Y.%20Ng%2C%20Andrew%20Y.%20Learning%20hierarchical%20invariant%20spatio-temporal%20features%20for%20action%20recognition%20with%20independent%20subspace%20analysis%202011-06-20",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Quoc%20V.%20Zou%2C%20Will%20Y.%20Yeung%2C%20Serena%20Y.%20Ng%2C%20Andrew%20Y.%20Learning%20hierarchical%20invariant%20spatio-temporal%20features%20for%20action%20recognition%20with%20independent%20subspace%20analysis%202011-06-20"
        },
        {
            "id": "39",
            "entry": "[39] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Xiaolong%20Gupta%2C%20Abhinav%20Unsupervised%20learning%20of%20visual%20representations%20using%20videos%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Xiaolong%20Gupta%2C%20Abhinav%20Unsupervised%20learning%20of%20visual%20representations%20using%20videos%202015"
        },
        {
            "id": "40",
            "entry": "[40] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20Carl%20Gupta%2C%20Abhinav%20Efros%2C%20Alexei%20A.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20Carl%20Gupta%2C%20Abhinav%20Efros%2C%20Alexei%20A.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015"
        },
        {
            "id": "41",
            "entry": "[41] Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video representation learning with odd-one-out networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5729\u20135738, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fernando%2C%20Basura%20Bilen%2C%20Hakan%20Gavves%2C%20Efstratios%20Gould%2C%20Stephen%20Self-supervised%20video%20representation%20learning%20with%20odd-one-out%20networks%202017-07-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fernando%2C%20Basura%20Bilen%2C%20Hakan%20Gavves%2C%20Efstratios%20Gould%2C%20Stephen%20Self-supervised%20video%20representation%20learning%20with%20odd-one-out%20networks%202017-07-21"
        },
        {
            "id": "42",
            "entry": "[42] Ishan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuffle and learn: Unsupervised learning using temporal order verification. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision \u2013 ECCV 2016, pages 527\u2013544, Cham, 2016. Springer International Publishing.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ishan%20Misra%2C%20C.Lawrence%20Zitnick%20Hebert%2C%20Martial%20Shuffle%20and%20learn%3A%20Unsupervised%20learning%20using%20temporal%20order%20verification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ishan%20Misra%2C%20C.Lawrence%20Zitnick%20Hebert%2C%20Martial%20Shuffle%20and%20learn%3A%20Unsupervised%20learning%20using%20temporal%20order%20verification%202016"
        },
        {
            "id": "43",
            "entry": "[43] Saurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross modal distillation for supervision transfer. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2827\u20132836, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Saurabh%20Hoffman%2C%20Judy%20Malik%2C%20Jitendra%20Cross%20modal%20distillation%20for%20supervision%20transfer%202016-06-27",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Saurabh%20Hoffman%2C%20Judy%20Malik%2C%20Jitendra%20Cross%20modal%20distillation%20for%20supervision%20transfer%202016-06-27"
        },
        {
            "id": "44",
            "entry": "[44] Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman, and Antonio Torralba. Ambient sound provides supervision for visual learning. In Computer Vision - ECCV 2016 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I, pages 801\u2013816, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Owens%2C%20Andrew%20Wu%2C%20Jiajun%20McDermott%2C%20Josh%20H.%20Freeman%2C%20William%20T.%20Ambient%20sound%20provides%20supervision%20for%20visual%20learning%202016-10-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Owens%2C%20Andrew%20Wu%2C%20Jiajun%20McDermott%2C%20Josh%20H.%20Freeman%2C%20William%20T.%20Ambient%20sound%20provides%20supervision%20for%20visual%20learning%202016-10-11"
        },
        {
            "id": "45",
            "entry": "[45] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh H. McDermott, and Antonio Torralba. The sound of pixels. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part I, pages 587\u2013604, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hang%20Zhao%20Chuang%20Gan%20Andrew%20Rouditchenko%20Carl%20Vondrick%20Josh%20H%20McDermott%20and%20Antonio%20Torralba%20The%20sound%20of%20pixels%20In%20Computer%20Vision%20%20ECCV%202018%20%2015th%20European%20Conference%20Munich%20Germany%20September%20814%202018%20Proceedings%20Part%20I%20pages%20587604%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hang%20Zhao%20Chuang%20Gan%20Andrew%20Rouditchenko%20Carl%20Vondrick%20Josh%20H%20McDermott%20and%20Antonio%20Torralba%20The%20sound%20of%20pixels%20In%20Computer%20Vision%20%20ECCV%202018%20%2015th%20European%20Conference%20Munich%20Germany%20September%20814%202018%20Proceedings%20Part%20I%20pages%20587604%202018"
        }
    ]
}
