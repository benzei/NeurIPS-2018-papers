{
    "filename": "7614-multi-layered-gradient-boosting-decision-trees.pdf",
    "metadata": {
        "title": "Multi-Layered Gradient Boosting Decision Trees",
        "author": "Ji Feng, Yang Yu, Zhi-Hua Zhou",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7614-multi-layered-gradient-boosting-decision-trees.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Multi-layered distributed representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are still the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical distributed representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive backpropagation nor differentiability. Experiments confirmed the effectiveness of the model in terms of performance and representation learning ability."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "GBDT",
            "url": "https://en.wikipedia.org/wiki/Gbdt"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "decision tree",
            "url": "https://en.wikipedia.org/wiki/decision_tree"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        }
    ],
    "highlights": [
        "The development of deep neural networks has achieved remarkable advancement in the field of machine learning during the past decade",
        "We propose the first multi-layered structure using gradient boosting decision trees as building blocks per layer with an explicit emphasis on its representation learning ability and the training procedure can be jointly optimized via a variant of target propagation",
        "Multi-layered gradient boosting decision trees forest achieved the highest accuracy compared to DNN approaches trained by either back-prop or target-prop, given the same model structure",
        "We present a novel multi-layered gradient boosting decision trees forest with explicit representation learning ability that can be jointly trained with a variant of target propagation",
        "Due to the excellent performance of tree ensembles, this approach is of great potentials in many application areas where neural networks are not the best fit"
    ],
    "key_statements": [
        "The development of deep neural networks has achieved remarkable advancement in the field of machine learning during the past decade",
        "It is still not clear how to construct a multi-layered model by forest that explicitly examine its representation learning ability. Such explorations for representation learning should be made since many previous researches have suggested that, a multi-layered distributed representations [Hinton et al, 1986] may be the key reason for the success of deep neural networks [<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013a</a>]",
        "We propose the first multi-layered structure using gradient boosting decision trees as building blocks per layer with an explicit emphasis on its representation learning ability and the training procedure can be jointly optimized via a variant of target propagation",
        "Multi-layered gradient boosting decision trees forest achieved the highest accuracy compared to DNN approaches trained by either back-prop or target-prop, given the same model structure",
        "We present a novel multi-layered gradient boosting decision trees forest with explicit representation learning ability that can be jointly trained with a variant of target propagation",
        "Due to the excellent performance of tree ensembles, this approach is of great potentials in many application areas where neural networks are not the best fit"
    ],
    "summary": [
        "The development of deep neural networks has achieved remarkable advancement in the field of machine learning during the past decade.",
        "Such explorations for representation learning should be made since many previous researches have suggested that, a multi-layered distributed representations [Hinton et al, 1986] may be the key reason for the success of deep neural networks [<a class=\"ref-link\" id=\"cBengio_et+al_2013_a\" href=\"#rBengio_et+al_2013_a\">Bengio et al, 2013a</a>].",
        "We propose the first multi-layered structure using gradient boosting decision trees as building blocks per layer with an explicit emphasis on its representation learning ability and the training procedure can be jointly optimized via a variant of target propagation.",
        "For a multi-layered deep model with differentiable components, back-propagation is still the dominant way for training.",
        "The learning task is to learn the mappings Fi : Rdi\u22121 \u2192 Rdi for each layer i > 0, such that the final output oM minimize the empirical loss L on training set.",
        "Once the training is done, the output for the intermediate layers can be regarded as the new representation learned by the model.",
        "For tree-structured model described here, it is not a trivial task to draw a random tree structure from the distribution of all the possible tree configurations, instead of initializing the tree structure at random, we produce some Gaussian noise to be the output of intermediate layers and train some very tiny trees to obtain Fi0, where index 0 denote the tree structures obtained in this initialization stage.",
        "A similar procedure such as target propagation [<a class=\"ref-link\" id=\"cBengio_2014_a\" href=\"#rBengio_2014_a\">Bengio, 2014</a>] has been proposed to use the inter-layer feedback mappings to train a neural network.",
        "The experiments for this section is mainly designed to empirically examine if it is feasible to jointly train the multi-layered structure proposed by this work.",
        "Multi-layered GBDT forest achieved the highest accuracy compared to DNN approaches trained by either back-prop or target-prop, given the same model structure.",
        "N N T argetP rop converges not as good as N N BackP rop as expected (a consistent result with <a class=\"ref-link\" id=\"cLee_et+al_2015_a\" href=\"#rLee_et+al_2015_a\">Lee et al [2015</a>]), whereas the same structure using GBDT layers can achieve a lower training loss without over-fitting.",
        "We present a novel multi-layered GBDT forest with explicit representation learning ability that can be jointly trained with a variant of target propagation.",
        "It is interesting to integrating several mGBDT layers as feature extractor into the deep forest structure to make the system not only capable of learning representations but can automatically determine its model complexity."
    ],
    "headline": "We propose the multi-layered gradient boosting decision trees forest , with an explicit emphasis on exploring the ability to learn hierarchical distributed representations by stacking several layers of regression gradient boosting decision trees as its building block",
    "reference_links": [
        {
            "id": "Bengio_et+al_2013_a",
            "entry": "Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Courville%2C%20A.%20Vincent%2C%20P.%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Courville%2C%20A.%20Vincent%2C%20P.%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "Bengio_et+al_2013_b",
            "entry": "Y. Bengio, G. Mesnil, Y. Dauphin, and S. Rifai. Better mixing via deep representations. In ICML, pages 552\u2013560, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Mesnil%2C%20G.%20Dauphin%2C%20Y.%20Rifai%2C%20S.%20Better%20mixing%20via%20deep%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Mesnil%2C%20G.%20Dauphin%2C%20Y.%20Rifai%2C%20S.%20Better%20mixing%20via%20deep%20representations%202013"
        },
        {
            "id": "Bengio_et+al_2013_c",
            "entry": "Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized denoising auto-encoders as generative models. In NIPS, pages 899\u2013907, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Yao%2C%20L.%20Alain%2C%20G.%20Vincent%2C%20P.%20Generalized%20denoising%20auto-encoders%20as%20generative%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Yao%2C%20L.%20Alain%2C%20G.%20Vincent%2C%20P.%20Generalized%20denoising%20auto-encoders%20as%20generative%20models%202013"
        },
        {
            "id": "Bengio_2014_a",
            "entry": "Y. Bengio. How auto-encoders could provide credit assignment in deep networks via target propagation. arXiv:1407.7906, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.7906"
        },
        {
            "id": "Breiman_1996_a",
            "entry": "L. Breiman. Bagging predictors. Machine Learning, 24(2):123\u2013140, 1996. L. Breiman. Random forests. Machine Learning, 45(1):5\u201332, 2001. T.-Q. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In KDD, pages 785\u2013794, 2016. T.-Q. Chen and T. He. Higgs boson discovery with boosted trees. In NIPS Workshop, pages 69\u201380, 2015. F. H. Clarke. On the inverse function theorem. Pacific Journal of Mathematics, 64(1):97\u2013102, 1976. J. Feng and Z.-H. Zhou. Autoencoder by forest. In AAAI, 2018. Y. Freund and R. E. Schapire. A short introduction to boosting. Journal of Japanese Society for Artificial",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20L.%20Bagging%20predictors%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20L.%20Bagging%20predictors%201996"
        },
        {
            "id": "Intelligence_1999_a",
            "entry": "Intelligence, 14(5):771\u2013780, 1999. J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189\u2013",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Intelligence%20Greedy%20function%20approximation%3A%20A%20gradient%20boosting%20machine%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Intelligence%20Greedy%20function%20approximation%3A%20A%20gradient%20boosting%20machine%201999"
        },
        {
            "id": "Frosst_2017_a",
            "entry": "1232, 2000. N. Frosst and G. E. Hinton. Distilling a neural network into a soft decision tree. arXiv:1711.09784, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09784"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge, MA, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Bengio%2C%20Y.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "He_et+al_2014_a",
            "entry": "X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Qui\u00f1onero. Practical lessons from predicting clicks on ads at facebook. In ADKDD, pages 5:1\u20135:9, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20X.%20Pan%2C%20J.%20Jin%2C%20O.%20Xu%2C%20T.%20Practical%20lessons%20from%20predicting%20clicks%20on%20ads%20at%20facebook%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20X.%20Pan%2C%20J.%20Jin%2C%20O.%20Xu%2C%20T.%20Practical%20lessons%20from%20predicting%20clicks%20on%20ads%20at%20facebook%202014"
        },
        {
            "id": "Processing:_1986_a",
            "entry": "Processing: Explorations in the Microstructure of Cognition, Vol. 1, pages 77\u2013109. 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Processing%20Explorations%20in%20the%20Microstructure%20of%20Cognition%20Vol%201%20pages%2077109%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Processing%20Explorations%20in%20the%20Microstructure%20of%20Cognition%20Vol%201%20pages%2077109%201986"
        },
        {
            "id": "D_2014_a",
            "entry": "arXiv:1702.02284, 2017. D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1702.02284"
        },
        {
            "id": "Korlakai_0000_a",
            "entry": "1467\u20131475, 2015. V. R. Korlakai and G. B. Ran. DART: dropouts meet multiple additive regression trees. In AISTATS, pages",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Korlakai%2C%20V.R.%20Ran%2C%20G.B.%20DART%3A%20dropouts%20meet%20multiple%20additive%20regression%20trees",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Korlakai%2C%20V.R.%20Ran%2C%20G.B.%20DART%3A%20dropouts%20meet%20multiple%20additive%20regression%20trees"
        },
        {
            "id": "Lee_et+al_2015_a",
            "entry": "489\u2013497, 2015. D.-H. Lee, S. Zhang, A. Fischer, and Y. Bengio. Difference target propagation. In ECML PKDD, pages 498\u2013515, 2015. M. Lichman. UCI machine learning repository, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20D.-H.%20Zhang%2C%20S.%20Fischer%2C%20A.%20Bengio%2C%20Y.%20Difference%20target%20propagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20D.-H.%20Zhang%2C%20S.%20Fischer%2C%20A.%20Bengio%2C%20Y.%20Difference%20target%20propagation%202015"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "T. P. Lillicrap, D. Cownden, D. B. Tweed, and C. J. Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature communications, 7:13276, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20T.P.%20Cownden%2C%20D.%20Tweed%2C%20D.B.%20Akerman%2C%20C.J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20T.P.%20Cownden%2C%20D.%20Tweed%2C%20D.B.%20Akerman%2C%20C.J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016"
        },
        {
            "id": "Nguyen_et+al_2015_a",
            "entry": "A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. pages 427\u2013436, 2015. A. N\u00f8kland. Direct feedback alignment provides learning in deep neural networks. In NIPS, pages 1037\u20131045, 2016. M. Rory and F. Eibe. Accelerating the xgboost algorithm using GPU computing. PeerJ Computer Science, 3:127, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20A.%20Yosinski%2C%20J.%20Clune%2C%20J.%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20A.%20Yosinski%2C%20J.%20Clune%2C%20J.%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015"
        },
        {
            "id": "Rumelhart_et+al_1986_a",
            "entry": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        },
        {
            "id": "Si_et+al_2015_a",
            "entry": "S. Si, H. Zhang, S. S. Keerthi, D. Mahajan, I. S. Dhillon, and C.-J. Hsieh. Gradient boosted decision trees for high dimensional sparse output. In ICML, pages 3182\u20133190, 2017. N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. arXiv:1503.02406, 2015. L.J.P van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. Journal of Machine",
            "arxiv_url": "https://arxiv.org/pdf/1503.02406"
        },
        {
            "id": "Werbos_2008_a",
            "entry": "Learning Research, 9:2579\u20132605, 2008. P. Werbos. Beyond regression: New tools for prediction and analysis in the behavioral sciences. PhD thesis, Harvard University, 1974. Z.-H. Zhou and J. Feng. Deep forest: Towards an alternative to deep neural networks. In IJCAI, pages 3553\u20133559, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20P.%20Beyond%20regression%3A%20New%20tools%20for%20prediction%20and%20analysis%20in%20the%20behavioral%20sciences%202008"
        },
        {
            "id": "Zhou_2018_a",
            "entry": "Z.-H. Zhou and J. Feng. Deep forest. National Science Review, 2018. Z.-H. Zhou. Ensemble Methods: Foundations and Algorithms. CRC, Boca Raton, FL, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Z.-H.%20forest%2C%20J.Feng%20Deep%20Z.-H.%20Zhou.%20Ensemble%20Methods%3A%20Foundations%20and%20Algorithms%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Z.-H.%20forest%2C%20J.Feng%20Deep%20Z.-H.%20Zhou.%20Ensemble%20Methods%3A%20Foundations%20and%20Algorithms%202018"
        }
    ]
}
