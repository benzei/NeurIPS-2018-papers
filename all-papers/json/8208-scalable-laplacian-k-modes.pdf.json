{
    "filename": "8208-scalable-laplacian-k-modes.pdf",
    "metadata": {
        "title": "Scalable Laplacian K-modes",
        "author": "Imtiaz Ziko, Eric Granger, Ismail Ben Ayed",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8208-scalable-laplacian-k-modes.pdf"
        },
        "abstract": "We advocate Laplacian K-modes for joint clustering and density mode finding, and propose a concave-convex relaxation of the problem, which yields a parallel algorithm that scales up to large datasets and high dimensions. We optimize a tight bound (auxiliary function) of our relaxation, which, at each iteration, amounts to computing an independent update for each cluster-assignment variable, with guaranteed convergence. Therefore, our bound optimizer can be trivially distributed for large-scale data sets. Furthermore, we show that the density modes can be obtained as byproducts of the assignment variables via simple maximum-value operations whose additional computational cost is linear in the number of data points. Our formulation does not need storing a full affinity matrix and computing its eigenvalue decomposition, neither does it perform expensive projection steps and Lagrangian-dual inner iterates for the simplex constraints of each point. Furthermore, unlike mean-shift, our density-mode estimation does not require innerloop gradient-ascent iterates. It has a complexity independent of feature-space dimension, yields modes that are valid data points in the input set and is applicable to discrete domains as well as arbitrary kernels. We report comprehensive experiments over various data sets, which show that our algorithm yields very competitive performances in term of optimization quality (i.e., the value of the discrete-variable objective at convergence) and clustering accuracy."
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "data set",
            "url": "https://en.wikipedia.org/wiki/data_set"
        },
        {
            "term": "mean shift",
            "url": "https://en.wikipedia.org/wiki/mean_shift"
        },
        {
            "term": "conditional random fields",
            "url": "https://en.wikipedia.org/wiki/conditional_random_fields"
        },
        {
            "term": "feature space",
            "url": "https://en.wikipedia.org/wiki/feature_space"
        },
        {
            "term": "expectation maximization",
            "url": "https://en.wikipedia.org/wiki/expectation_maximization"
        }
    ],
    "highlights": [
        "We advocate Laplacian K-modes for joint clustering and density mode finding, and propose a concave-convex relaxation of the problem, which yields a parallel algorithm that scales up to large data sets and high dimensions",
        "We report comprehensive experiments over various data sets, which show that our algorithm yields very competitive performances in term of optimization quality4 and clustering accuracy, while being scalable to large-scale and high-dimensional problems.\n2 Concave-convex relaxation",
        "We report comprehensive evaluations of the proposed algorithm9 as well as comparisons to the following related baseline methods: Laplacian K-modes (LK) [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], K-means, NCUT [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], K-modes [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], Kernel K-means (KK-means) [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and Spectralnet [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "We presented Scalable Laplacian K-modes (SLK), a method for joint clustering and density mode estimation, which scales up to high-dimensional and large-scale problems",
        "We showed that the density modes can be estimated directly from the assignment variables using simple maximum-value operations, with an additional computational cost that is linear in the number of data points",
        "We showed competitive performances of the proposed solution in term of optimization quality and accuracy"
    ],
    "key_statements": [
        "We advocate Laplacian K-modes for joint clustering and density mode finding, and propose a concave-convex relaxation of the problem, which yields a parallel algorithm that scales up to large data sets and high dimensions",
        "One way to tackle the problem is to alternate optimization over assignment variables and updates of the modes, with the latter performed as inner-loop mean-shift iterates, as in [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We report comprehensive experiments over various data sets, which show that our algorithm yields very competitive performances in term of optimization quality4 and clustering accuracy, while being scalable to large-scale and high-dimensional problems.\n2 Concave-convex relaxation",
        "We show that the density modes can be obtained as byproducts of the z-updates via simple maximum-value operations whose additional computational cost is linear in N",
        "We report comprehensive evaluations of the proposed algorithm9 as well as comparisons to the following related baseline methods: Laplacian K-modes (LK) [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], K-means, NCUT [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], K-modes [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], Kernel K-means (KK-means) [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] and Spectralnet [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "We presented Scalable Laplacian K-modes (SLK), a method for joint clustering and density mode estimation, which scales up to high-dimensional and large-scale problems",
        "We showed that the density modes can be estimated directly from the assignment variables using simple maximum-value operations, with an additional computational cost that is linear in the number of data points",
        "We showed competitive performances of the proposed solution in term of optimization quality and accuracy"
    ],
    "summary": [
        "We advocate Laplacian K-modes for joint clustering and density mode finding, and propose a concave-convex relaxation of the problem, which yields a parallel algorithm that scales up to large data sets and high dimensions.",
        "One way to tackle the problem is to alternate optimization over assignment variables and updates of the modes, with the latter performed as inner-loop mean-shift iterates, as in [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "This yields a scalable algorithm for large N , which computes independent updates for assignment variables zp, while guaranteeing convergence to a minimum of the relaxation.",
        "We report comprehensive experiments over various data sets, which show that our algorithm yields very competitive performances in term of optimization quality4 and clustering accuracy, while being scalable to large-scale and high-dimensional problems.",
        "As we will see later, concavity yields a scalable algorithm for large N , which computes independent updates for assignment variables zp.",
        "The first term we introduced in (2) is a convex negative-entropy barrier function, which completely avoids expensive projection steps and Lagrangian-dual inner iterations for the simplex constraints of each point.",
        "As we will see in our experiments, our bound optimizer yields consistently lower values of function E at convergence than the proximal algorithm in [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], while being highly scalable to large-scale and high-dimensional problems.",
        "To assess the optimization quality of our optimizer, we computed the values of discrete-variable objective E in model (1) at convergence for our concave-convex relaxation (SLK-MS) as well as for the convex relaxation in [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] (LK).",
        "We presented Scalable Laplacian K-modes (SLK), a method for joint clustering and density mode estimation, which scales up to high-dimensional and large-scale problems.",
        "Our solver results in independent updates for cluster-assignment variables, with guaranteed convergence, thereby enabling distributed implementations for large-scale data sets.",
        "We showed that the density modes can be estimated directly from the assignment variables using simple maximum-value operations, with an additional computational cost that is linear in the number of data points.",
        "Unlike the convex relaxation in [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], it does not require expensive projection steps and Lagrangian-dual inner iterates for the simplex constraints of each point.",
        "It has a complexity independent of feature-space dimension, yields modes that are valid data points in the input set and is applicable to discrete domains as well as arbitrary kernels.",
        "It will be interesting to investigate joint feature learning and SLK clustering"
    ],
    "headline": "We show that the density modes can be obtained as byproducts of the assignment variables via simple maximum-value operations whose additional computational cost is linear in the number of data points",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arthur%2C%20D.%20Vassilvitskii%2C%20S.%20k-means%2B%2B%3A%20The%20advantages%20of%20careful%20seeding%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arthur%2C%20D.%20Vassilvitskii%2C%20S.%20k-means%2B%2B%3A%20The%20advantages%20of%20careful%20seeding%202007"
        },
        {
            "id": "2",
            "entry": "[2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399\u20132434, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20M.%20Niyogi%2C%20P.%20Sindhwani%2C%20V.%20Manifold%20regularization%3A%20A%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20M.%20Niyogi%2C%20P.%20Sindhwani%2C%20V.%20Manifold%20regularization%3A%20A%20geometric%20framework%20for%20learning%20from%20labeled%20and%20unlabeled%20examples%202006"
        },
        {
            "id": "3",
            "entry": "[3] Y. Bengio, J.-f. Paiement, P. Vincent, O. Delalleau, N. L. Roux, and M. Ouimet. Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering. In Neural Information Processing Systems (NIPS), pages 177\u2013184, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Paiement%2C%20J.-f%20Vincent%2C%20P.%20Delalleau%2C%20O.%20Out-of-sample%20extensions%20for%20lle%2C%20isomap%2C%20mds%2C%20eigenmaps%2C%20and%20spectral%20clustering%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Paiement%2C%20J.-f%20Vincent%2C%20P.%20Delalleau%2C%20O.%20Out-of-sample%20extensions%20for%20lle%2C%20isomap%2C%20mds%2C%20eigenmaps%2C%20and%20spectral%20clustering%202004"
        },
        {
            "id": "4",
            "entry": "[4] M. \u00c1. Carreira-Perpi\u00f1\u00e1n. Gaussian mean-shift is an em algorithm. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(5):767\u2013776, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira-Perpi%C3%B1%C3%A1n%2C%20M.%C3%81.%20Gaussian%20mean-shift%20is%20an%20em%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira-Perpi%C3%B1%C3%A1n%2C%20M.%C3%81.%20Gaussian%20mean-shift%20is%20an%20em%20algorithm%202007"
        },
        {
            "id": "5",
            "entry": "[5] M. \u00c1. Carreira-Perpi\u00f1\u00e1n. A review of mean-shift algorithms for clustering. arXiv preprint arXiv:1503.00687, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.00687"
        },
        {
            "id": "6",
            "entry": "[6] M. \u00c1. Carreira-Perpi\u00f1\u00e1n and W. Wang. The k-modes algorithm for clustering. arXiv preprint arXiv:1304.6478, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1304.6478"
        },
        {
            "id": "7",
            "entry": "[7] C. Chen, H. Liu, D. Metaxas, and T. Zhao. Mode estimation for high dimensional discrete tree graphical models. In Neural Information Processing Systems (NIPS), pages 1323\u20131331, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20C.%20Liu%2C%20H.%20Metaxas%2C%20D.%20Zhao%2C%20T.%20Mode%20estimation%20for%20high%20dimensional%20discrete%20tree%20graphical%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20C.%20Liu%2C%20H.%20Metaxas%2C%20D.%20Zhao%2C%20T.%20Mode%20estimation%20for%20high%20dimensional%20discrete%20tree%20graphical%20models%202014"
        },
        {
            "id": "8",
            "entry": "[8] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603\u2013619, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Comaniciu%2C%20D.%20Meer%2C%20P.%20Mean%20shift%3A%20A%20robust%20approach%20toward%20feature%20space%20analysis%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Comaniciu%2C%20D.%20Meer%2C%20P.%20Mean%20shift%3A%20A%20robust%20approach%20toward%20feature%20space%20analysis%202002"
        },
        {
            "id": "9",
            "entry": "[9] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: spectral clustering and normalized cuts. In International Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 551\u2013556, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dhillon%2C%20I.S.%20Guan%2C%20Y.%20Kulis%2C%20B.%20Kernel%20k-means%3A%20spectral%20clustering%20and%20normalized%20cuts%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dhillon%2C%20I.S.%20Guan%2C%20Y.%20Kulis%2C%20B.%20Kernel%20k-means%3A%20spectral%20clustering%20and%20normalized%20cuts%202004"
        },
        {
            "id": "10",
            "entry": "[10] K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang. Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization. In International Conference on Computer Vision (ICCV), pages 5747\u20135756, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dizaji%2C%20K.Ghasedi%20Herandi%2C%20A.%20Deng%2C%20C.%20Cai%2C%20W.%20Deep%20clustering%20via%20joint%20convolutional%20autoencoder%20embedding%20and%20relative%20entropy%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dizaji%2C%20K.Ghasedi%20Herandi%2C%20A.%20Deng%2C%20C.%20Cai%2C%20W.%20Deep%20clustering%20via%20joint%20convolutional%20autoencoder%20embedding%20and%20relative%20entropy%20minimization%202017"
        },
        {
            "id": "11",
            "entry": "[11] Y. Gong, M. Pawlowski, F. Yang, L. Brandy, L. Bourdev, and R. Fergus. Web scale photo hash clustering on a single machine. In Computer Vision and Pattern Recognition (CVPR), pages 19\u201327, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20Y.%20Pawlowski%2C%20M.%20Yang%2C%20F.%20Brandy%2C%20L.%20Web%20scale%20photo%20hash%20clustering%20on%20a%20single%20machine%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20Y.%20Pawlowski%2C%20M.%20Yang%2C%20F.%20Brandy%2C%20L.%20Web%20scale%20photo%20hash%20clustering%20on%20a%20single%20machine%202015"
        },
        {
            "id": "12",
            "entry": "[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Neural Information Processing Systems (NIPS), pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20Neural%20Information%20Processing%20Systems%20NIPS%20pages%2026722680%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20Neural%20Information%20Processing%20Systems%20NIPS%20pages%2026722680%202014"
        },
        {
            "id": "13",
            "entry": "[13] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: An unsupervised and generative approach to clustering. In International Joint Conference on Artificial Intelligence (IJCAI), pages 1965\u20131972, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Z.%20Zheng%2C%20Y.%20Tan%2C%20H.%20Tang%2C%20B.%20Variational%20deep%20embedding%3A%20An%20unsupervised%20and%20generative%20approach%20to%20clustering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Z.%20Zheng%2C%20Y.%20Tan%2C%20H.%20Tang%2C%20B.%20Variational%20deep%20embedding%3A%20An%20unsupervised%20and%20generative%20approach%20to%20clustering%202017"
        },
        {
            "id": "14",
            "entry": "[14] P. Kr\u00e4henb\u00fchl and V. Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In Neural Information Processing Systems (NIPS), pages 109\u2013117, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kr%C3%A4henb%C3%BChl%2C%20P.%20Koltun%2C%20V.%20Efficient%20inference%20in%20fully%20connected%20crfs%20with%20gaussian%20edge%20potentials%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kr%C3%A4henb%C3%BChl%2C%20P.%20Koltun%2C%20V.%20Efficient%20inference%20in%20fully%20connected%20crfs%20with%20gaussian%20edge%20potentials%202011"
        },
        {
            "id": "15",
            "entry": "[15] P. Kr\u00e4henb\u00fchl and V. Koltun. Parameter learning and convergent inference for dense random fields. In International Conference on Machine Learning (ICML), pages 513\u2013521, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kr%C3%A4henb%C3%BChl%2C%20P.%20Koltun%2C%20V.%20Parameter%20learning%20and%20convergent%20inference%20for%20dense%20random%20fields%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kr%C3%A4henb%C3%BChl%2C%20P.%20Koltun%2C%20V.%20Parameter%20learning%20and%20convergent%20inference%20for%20dense%20random%20fields%202013"
        },
        {
            "id": "16",
            "entry": "[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Neural Information Processing Systems (NIPS), pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "17",
            "entry": "[17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "18",
            "entry": "[18] J. Li, S. Ray, and B. G. Lindsay. A nonparametric statistical approach to clustering via mode identification. Journal of Machine Learning Research, 8:1687\u20131723, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Ray%2C%20S.%20Lindsay%2C%20B.G.%20A%20nonparametric%20statistical%20approach%20to%20clustering%20via%20mode%20identification%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Ray%2C%20S.%20Lindsay%2C%20B.G.%20A%20nonparametric%20statistical%20approach%20to%20clustering%20via%20mode%20identification%202007"
        },
        {
            "id": "19",
            "entry": "[19] M. Muja and D. G. Lowe. Scalable nearest neighbor algorithms for high dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(11):2227\u20132240, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muja%2C%20M.%20Lowe%2C%20D.G.%20Scalable%20nearest%20neighbor%20algorithms%20for%20high%20dimensional%20data%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muja%2C%20M.%20Lowe%2C%20D.G.%20Scalable%20nearest%20neighbor%20algorithms%20for%20high%20dimensional%20data%202014"
        },
        {
            "id": "20",
            "entry": "[20] J. Munkres. Algorithms for the assignment and transportation problems. Journal of the society for industrial and applied mathematics, 5(1):32\u201338, 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munkres%2C%20J.%20Algorithms%20for%20the%20assignment%20and%20transportation%20problems%201957",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munkres%2C%20J.%20Algorithms%20for%20the%20assignment%20and%20transportation%20problems%201957"
        },
        {
            "id": "21",
            "entry": "[21] M. Narasimhan and J. Bilmes. A submodular-supermodular procedure with applications to discriminative structure learning. In Conference on Uncertainty in Artificial Intelligence (UAI), pages 404\u2013412, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Narasimhan%2C%20M.%20Bilmes%2C%20J.%20A%20submodular-supermodular%20procedure%20with%20applications%20to%20discriminative%20structure%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Narasimhan%2C%20M.%20Bilmes%2C%20J.%20A%20submodular-supermodular%20procedure%20with%20applications%20to%20discriminative%20structure%20learning%202005"
        },
        {
            "id": "22",
            "entry": "[22] J. Newling and F. Fleuret. Nested mini-batch k-means. In Neural Information Processing Systems (NIPS), pages 1352\u20131360, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newling%2C%20J.%20Fleuret%2C%20F.%20Nested%20mini-batch%20k-means%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newling%2C%20J.%20Fleuret%2C%20F.%20Nested%20mini-batch%20k-means%202016"
        },
        {
            "id": "23",
            "entry": "[23] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3):145\u2013175, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oliva%2C%20A.%20Torralba%2C%20A.%20Modeling%20the%20shape%20of%20the%20scene%3A%20A%20holistic%20representation%20of%20the%20spatial%20envelope%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oliva%2C%20A.%20Torralba%2C%20A.%20Modeling%20the%20shape%20of%20the%20scene%3A%20A%20holistic%20representation%20of%20the%20spatial%20envelope%202001"
        },
        {
            "id": "24",
            "entry": "[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011"
        },
        {
            "id": "25",
            "entry": "[25] M. B. Salah, I. B. Ayed, J. Yuan, and H. Zhang. Convex-relaxed kernel mapping for image segmentation. IEEE Transactions on Image Processing, 23(3):1143\u20131153, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salah%2C%20M.B.%20Ayed%2C%20I.B.%20Yuan%2C%20J.%20Zhang%2C%20H.%20Convex-relaxed%20kernel%20mapping%20for%20image%20segmentation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salah%2C%20M.B.%20Ayed%2C%20I.B.%20Yuan%2C%20J.%20Zhang%2C%20H.%20Convex-relaxed%20kernel%20mapping%20for%20image%20segmentation%202014"
        },
        {
            "id": "26",
            "entry": "[26] U. Shaham, K. Stanton, H. Li, R. Basri, B. Nadler, and Y. Kluger. Spectralnet: Spectral clustering using deep neural networks. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shaham%2C%20U.%20Stanton%2C%20K.%20Li%2C%20H.%20Basri%2C%20R.%20Spectralnet%3A%20Spectral%20clustering%20using%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shaham%2C%20U.%20Stanton%2C%20K.%20Li%2C%20H.%20Basri%2C%20R.%20Spectralnet%3A%20Spectral%20clustering%20using%20deep%20neural%20networks%202018"
        },
        {
            "id": "27",
            "entry": "[27] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888\u2013905, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20J.%20Malik%2C%20J.%20Normalized%20cuts%20and%20image%20segmentation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20J.%20Malik%2C%20J.%20Normalized%20cuts%20and%20image%20segmentation%202000"
        },
        {
            "id": "28",
            "entry": "[28] A. Strehl and J. Ghosh. Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research, 3(12):583\u2013617, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20A.%20Ghosh%2C%20J.%20Cluster%20ensembles%E2%80%94a%20knowledge%20reuse%20framework%20for%20combining%20multiple%20partitions%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20A.%20Ghosh%2C%20J.%20Cluster%20ensembles%E2%80%94a%20knowledge%20reuse%20framework%20for%20combining%20multiple%20partitions%202002"
        },
        {
            "id": "29",
            "entry": "[29] M. Tang, D. Marin, I. B. Ayed, and Y. Boykov. Kernel cuts: Kernel and spectral clustering meet regularization. International Journal of Computer Vision, In press:1\u201335, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20M.%20Marin%2C%20D.%20Ayed%2C%20I.B.%20Boykov%2C%20Y.%20Kernel%20cuts%3A%20Kernel%20and%20spectral%20clustering%20meet%20regularization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20M.%20Marin%2C%20D.%20Ayed%2C%20I.B.%20Boykov%2C%20Y.%20Kernel%20cuts%3A%20Kernel%20and%20spectral%20clustering%20meet%20regularization%202018"
        },
        {
            "id": "30",
            "entry": "[30] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In AAAI Conference on Artificial Intelligence, pages 1293\u20131299, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tian%2C%20F.%20Gao%2C%20B.%20Cui%2C%20Q.%20Chen%2C%20E.%20Learning%20deep%20representations%20for%20graph%20clustering%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tian%2C%20F.%20Gao%2C%20B.%20Cui%2C%20Q.%20Chen%2C%20E.%20Learning%20deep%20representations%20for%20graph%20clustering%202014"
        },
        {
            "id": "31",
            "entry": "[31] M. Vladymyrov and M. Carreira-Perpi\u00f1\u00e1n. The variational nystrom method for large-scale spectral problems. In International Conference on Machine Learning (ICML), pages 211\u2013220, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vladymyrov%2C%20M.%20Carreira-Perpi%C3%B1%C3%A1n%2C%20M.%20The%20variational%20nystrom%20method%20for%20large-scale%20spectral%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vladymyrov%2C%20M.%20Carreira-Perpi%C3%B1%C3%A1n%2C%20M.%20The%20variational%20nystrom%20method%20for%20large-scale%20spectral%20problems%202016"
        },
        {
            "id": "32",
            "entry": "[32] U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395\u2013416, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luxburg%2C%20U.Von%20A%20tutorial%20on%20spectral%20clustering%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luxburg%2C%20U.Von%20A%20tutorial%20on%20spectral%20clustering%202007"
        },
        {
            "id": "33",
            "entry": "[33] W. Wang and M. A. Carreira-Perpin\u00e1n. The laplacian k-modes algorithm for clustering. arXiv preprint arXiv:1406.3895, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.3895"
        },
        {
            "id": "34",
            "entry": "[34] L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained videos with matched background similarity. In Computer Vision and Pattern Recognition (CVPR), pages 529\u2013534, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolf%2C%20L.%20Hassner%2C%20T.%20Maoz%2C%20I.%20Face%20recognition%20in%20unconstrained%20videos%20with%20matched%20background%20similarity%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolf%2C%20L.%20Hassner%2C%20T.%20Maoz%2C%20I.%20Face%20recognition%20in%20unconstrained%20videos%20with%20matched%20background%20similarity%202011"
        },
        {
            "id": "35",
            "entry": "[35] J. Yuan, K. Yin, Y. Bai, X. Feng, and X. Tai. Bregman-proximal augmented lagrangian approach to multiphase image segmentation. In Scale Space and Variational Methods in Computer Vision (SSVM), pages 524\u2013534, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20J.%20Yin%2C%20K.%20Bai%2C%20Y.%20Feng%2C%20X.%20Bregman-proximal%20augmented%20lagrangian%20approach%20to%20multiphase%20image%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20J.%20Yin%2C%20K.%20Bai%2C%20Y.%20Feng%2C%20X.%20Bregman-proximal%20augmented%20lagrangian%20approach%20to%20multiphase%20image%20segmentation%202017"
        },
        {
            "id": "36",
            "entry": "[36] A. L. Yuille and A. Rangarajan. The concave-convex procedure (CCCP). In Neural Information Processing Systems (NIPS), pages 1033\u20131040, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuille%2C%20A.L.%20Rangarajan%2C%20A.%20The%20concave-convex%20procedure%20%28CCCP%29%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuille%2C%20A.L.%20Rangarajan%2C%20A.%20The%20concave-convex%20procedure%20%28CCCP%29%202001"
        },
        {
            "id": "37",
            "entry": "[37] Z. Zhang, J. T. Kwok, and D.-Y. Yeung. Surrogate maximization/minimization algorithms and extensions. Machine Learning, 69:1\u201333, 2007. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Z.%20Kwok%2C%20J.T.%20Yeung%2C%20D.-Y.%20Surrogate%20maximization/minimization%20algorithms%20and%20extensions%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Z.%20Kwok%2C%20J.T.%20Yeung%2C%20D.-Y.%20Surrogate%20maximization/minimization%20algorithms%20and%20extensions%202007"
        }
    ]
}
