{
    "filename": "8073-variational-inverse-control-with-events-a-general-framework-for-data-driven-reward-definition.pdf",
    "metadata": {
        "title": "Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition",
        "author": "Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, Sergey Levine",
        "date": 2016,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8073-variational-inverse-control-with-events-a-general-framework-for-data-driven-reward-definition.pdf"
        },
        "abstract": "The design of a reward function often poses a major practical challenge to realworld applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent\u2019s goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on highdimensional observations like images where rewards are hard or even impossible to specify."
    },
    "keywords": [
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "robotic control",
            "url": "https://en.wikipedia.org/wiki/robotic_control"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision processes",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_processes"
        }
    ],
    "highlights": [
        "Reinforcement learning (RL) has shown remarkable promise in recent years, with results on a range of complex tasks such as robotic control (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>) and playing video games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) from raw sensory input",
        "As explained in Section 5, our work generalizes inverse reinforcement learning to cases where we only provide examples of a desired outcome or goal, which is significantly easier to provide in practice since we do not need to know how to achieve the goal",
        "We described how the connection between control and inference can be extended to derive a reinforcement learning framework that dispenses with the conventional notion of rewards, and replaces them with events",
        "Recasting reinforcement learning into the event-based framework allows us to express various goals as different inference queries in the corresponding graphical model",
        "The case where we learn event probabilities corresponds to a generalization of inverse reinforcement learning where rather than assuming access to expert demonstrations, we assume access to states and actions where an event occurs",
        "inverse reinforcement learning corresponds to the case where we assume the event happens at every timestep, and we extend this notion to alternate graphical model queries where events may happen at a single timestep"
    ],
    "key_statements": [
        "Reinforcement learning (RL) has shown remarkable promise in recent years, with results on a range of complex tasks such as robotic control (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>) and playing video games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) from raw sensory input",
        "Reinforcement learning algorithms solve these problems by learning a policy that maximizes a reward function that is considered as part of the problem formulation",
        "As explained in Section 5, our work generalizes inverse reinforcement learning to cases where we only provide examples of a desired outcome or goal, which is significantly easier to provide in practice since we do not need to know how to achieve the goal",
        "We presented a control framework that operates on events rather than reward functions, and discussed how the user can choose from among a variety of inference queries to obtain a desired outcome",
        "(2) Does our event learning framework (VICE) outperform simple alternatives, such as offline classifier training, when learning event probabilities from data? We study this question in settings where it is difficult to manually specify a reward function, such as when the agent receives raw image observations",
        "We described how the connection between control and inference can be extended to derive a reinforcement learning framework that dispenses with the conventional notion of rewards, and replaces them with events",
        "Recasting reinforcement learning into the event-based framework allows us to express various goals as different inference queries in the corresponding graphical model",
        "The case where we learn event probabilities corresponds to a generalization of inverse reinforcement learning where rather than assuming access to expert demonstrations, we assume access to states and actions where an event occurs",
        "inverse reinforcement learning corresponds to the case where we assume the event happens at every timestep, and we extend this notion to alternate graphical model queries where events may happen at a single timestep"
    ],
    "summary": [
        "Reinforcement learning (RL) has shown remarkable promise in recent years, with results on a range of complex tasks such as robotic control (<a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\"><a class=\"ref-link\" id=\"cLevine_et+al_2016_a\" href=\"#rLevine_et+al_2016_a\">Levine et al, 2016</a></a>) and playing video games (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) from raw sensory input.",
        "Our framework provides a more principled approach, where classifier training corresponds to learning probabilistic graphical model parameters, and policy optimization corresponds to inferring the optimal actions.",
        "Our experiments demonstrate how different queries conv layers fc layers can result in different behaviors which align with Figure 2: Our framework learns event probabilities the corresponding intentions.",
        "Our method substantially outperforms baselines such as sparse reward RL, indicating that our framework provides an automated shaping effect when learning events, making it feasible to solve otherwise hard tasks.",
        "The strength of the events framework for task specification lies in both its intuitive interpretation and flexibility: though we can obtain similar behavior in standard reinforcement learning, it may require considerable reward tuning and changes to the overall problem statement, including the dynamics.",
        "This query is related to first-exit RL problems, where an agent receives a reward of 1 when a specified goal is reached and is immediately moved to an absorbing state but it does not require the event to be observed, which makes it applicable to a variety of real-world situations that have uncertainty over the goal.",
        "We presented a control framework that operates on events rather than reward functions, and discussed how the user can choose from among a variety of inference queries to obtain a desired outcome.",
        "We compare our event probability learning framework, which we call variational inverse control with events (VICE), against an offline classifier training baseline.",
        "In the Maze and Ant tasks, the agent seeks to sparse rewards, \u201cobserves\u201d the event, providing reach a pre-specified goal position.",
        "Iterations negative examples from the current policy into the learning process, and appropriately models the event probabilities together with the dynamical properties of the task, analogously to IRL.",
        "This indicates that the event probability distribution learned by our method has a reward-shaping effect, which greatly simplifies the policy search process.",
        "We described how the connection between control and inference can be extended to derive a reinforcement learning framework that dispenses with the conventional notion of rewards, and replaces them with events.",
        "Recasting reinforcement learning into the event-based framework allows us to express various goals as different inference queries in the corresponding graphical model.",
        "The case where we learn event probabilities corresponds to a generalization of IRL where rather than assuming access to expert demonstrations, we assume access to states and actions where an event occurs.",
        "IRL corresponds to the case where we assume the event happens at every timestep, and we extend this notion to alternate graphical model queries where events may happen at a single timestep"
    ],
    "headline": "We propose variational inverse control with events , which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available",
    "reference_links": [
        {
            "id": "Amodei_2016_a",
            "entry": "Amodei, Dario, Olah, Chris, Steinhardt, Jacob, Christiano, Paul, Schulman, John, and Man\u00e9, Dan. Concrete problems in AI safety. ArXiv Preprint, abs/1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "Argall_et+al_2009_a",
            "entry": "Argall, Brenna D., Chernova, Sonia, Veloso, Manuela, and Browning, Brett. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20Browning%2C%20Brett%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argall%2C%20Brenna%20D.%20Chernova%2C%20Sonia%20Veloso%2C%20Manuela%20Browning%2C%20Brett%20A%20survey%20of%20robot%20learning%20from%20demonstration%202009"
        },
        {
            "id": "Finn_et+al_2016_a",
            "entry": "Finn, C., Tan, X., Duan, Y., Darrell, T., Levine, S., and Abbeel, P. Deep spatial autoencoders for visuomotor learning. In ICRA, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Tan%2C%20X.%20Duan%2C%20Y.%20Darrell%2C%20T.%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Tan%2C%20X.%20Duan%2C%20Y.%20Darrell%2C%20T.%20Deep%20spatial%20autoencoders%20for%20visuomotor%20learning%202016"
        },
        {
            "id": "Finn_et+al_2016_b",
            "entry": "Finn, Chelsea, Christiano, Paul, Abbeel, Pieter, and Levine, Sergey. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. abs/1611.03852, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Christiano%2C%20Paul%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20A%20connection%20between%20generative%20adversarial%20networks%2C%20inverse%20reinforcement%20learning%2C%20and%20energy-based%20models%202016"
        },
        {
            "id": "Fu_et+al_2018_a",
            "entry": "Fu, Justin, Luo, Katie, and Levine, Sergey. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20Justin%20Luo%2C%20Katie%20Levine%2C%20Sergey%20Learning%20robust%20rewards%20with%20adversarial%20inverse%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20Justin%20Luo%2C%20Katie%20Levine%2C%20Sergey%20Learning%20robust%20rewards%20with%20adversarial%20inverse%20reinforcement%20learning%202018"
        },
        {
            "id": "Haarnoja_et+al_2017_a",
            "entry": "Haarnoja, Tuomas, Tang, Haoran, Abbeel, Pieter, and Levine, Sergey. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20Tuomas%20Tang%2C%20Haoran%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Reinforcement%20learning%20with%20deep%20energy-based%20policies%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20Tuomas%20Tang%2C%20Haoran%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Reinforcement%20learning%20with%20deep%20energy-based%20policies%202017"
        },
        {
            "id": "Hadfield-Menell_et+al_2017_a",
            "entry": "Hadfield-Menell, Dylan, Milli, Smitha, Abbeel, Pieter, Russell, Stuart J., and Dragan, Anca D. Inverse reward design. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HadfieldMenell%20Dylan%20Milli%20Smitha%20Abbeel%20Pieter%20Russell%20Stuart%20J%20and%20Dragan%20Anca%20D%20Inverse%20reward%20design%20In%20NIPS%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=HadfieldMenell%20Dylan%20Milli%20Smitha%20Abbeel%20Pieter%20Russell%20Stuart%20J%20and%20Dragan%20Anca%20D%20Inverse%20reward%20design%20In%20NIPS%202017"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Ho, Jonathan and Ermon, Stefano. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Kalman_1960_a",
            "entry": "Kalman, Rudolf. A new approach to linear filtering and prediction problems. 82:35\u201345, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalman%2C%20Rudolf%20A%20new%20approach%20to%20linear%20filtering%20and%20prediction%20problems%201960"
        },
        {
            "id": "Kappen_et+al_2009_a",
            "entry": "Kappen, Hilbert J., Gomez, Vicenc, and Opper, Manfred. Optimal control as a graphical model inference problem. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kappen%2C%20Hilbert%20J.%20Gomez%2C%20Vicenc%20Opper%2C%20Manfred%20Optimal%20control%20as%20a%20graphical%20model%20inference%20problem%202009"
        },
        {
            "id": "Levine_et+al_2016_a",
            "entry": "Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep visuomotor policies. Journal of Machine Learning (JMLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, feb 2015. ISSN 0028-0836.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015-02"
        },
        {
            "id": "Nachum_et+al_2017_a",
            "entry": "Nachum, Ofir, Norouzi, Mohammad, Xu, Kelvin, and Schuurmans, Dale. Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nachum%2C%20Ofir%20Norouzi%2C%20Mohammad%20Xu%2C%20Kelvin%20Schuurmans%2C%20Dale%20Bridging%20the%20gap%20between%20value%20and%20policy%20based%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nachum%2C%20Ofir%20Norouzi%2C%20Mohammad%20Xu%2C%20Kelvin%20Schuurmans%2C%20Dale%20Bridging%20the%20gap%20between%20value%20and%20policy%20based%20reinforcement%20learning%202017"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Ng, Andrew and Russell, Stuart. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning (ICML), 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "O_et+al_2016_a",
            "entry": "O\u2019Donoghue, Brendan, Munos, Remi, Kavukcuoglu, Koray, and Mnih, Volodymyr. Combining policy gradient and q-learning. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Donoghue%2C%20Brendan%20Munos%2C%20Remi%20Kavukcuoglu%2C%20Koray%20Mnih%2C%20Volodymyr%20Combining%20policy%20gradient%20and%20q-learning%202016"
        },
        {
            "id": "Peng_et+al_2017_a",
            "entry": "Peng, Xue Bin, Andrychowicz, Marcin, Zaremba, Wojciech, and Abbeel, Pieter. Sim-to-real transfer of robotic control with dynamics randomization. CoRR, abs/1710.06537, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06537"
        },
        {
            "id": "Pinto_2016_a",
            "entry": "Pinto, Lerrel and Gupta, Abhinav. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In IEEE International Conference on Robotics and Automation (ICRA), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Supersizing%20self-supervision%3A%20Learning%20to%20grasp%20from%2050k%20tries%20and%20700%20robot%20hours%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Supersizing%20self-supervision%3A%20Learning%20to%20grasp%20from%2050k%20tries%20and%20700%20robot%20hours%202016"
        },
        {
            "id": "Rawlik_et+al_2012_a",
            "entry": "Rawlik, Konrad, Toussaint, Marc, and Vijayakumar, Sethu. On stochastic optimal control and reinforcement learning by approximate inference. In Robotics: Science and Systems (RSS), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rawlik%2C%20Konrad%20Toussaint%2C%20Marc%20Vijayakumar%2C%20Sethu%20On%20stochastic%20optimal%20control%20and%20reinforcement%20learning%20by%20approximate%20inference%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rawlik%2C%20Konrad%20Toussaint%2C%20Marc%20Vijayakumar%2C%20Sethu%20On%20stochastic%20optimal%20control%20and%20reinforcement%20learning%20by%20approximate%20inference%202012"
        },
        {
            "id": "Russell_2003_a",
            "entry": "Russell, Stuart J. and Norvig, Peter. Artificial Intelligence: A Modern Approach. Pearson Education, 2 edition, 2003. ISBN 0137903952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russell%2C%20Stuart%20J.%20Norvig%2C%20Peter%20Artificial%20Intelligence%3A%20A%20Modern%20Approach.%20Pearson%20Education%202003"
        },
        {
            "id": "Rusu_et+al_2017_a",
            "entry": "Rusu, Andrei A., Vecerik, Matej, Roth\u00f6rl, Thomas, Heess, Nicolas, Pascanu, Razvan, and Hadsell, Raia. Sim-to-real robot learning from pixels with progressive nets. In Conference on Robot Learning (CoRL), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rusu%2C%20Andrei%20A.%20Vecerik%2C%20Matej%20Roth%C3%B6rl%2C%20Thomas%20Heess%2C%20Nicolas%20Sim-to-real%20robot%20learning%20from%20pixels%20with%20progressive%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rusu%2C%20Andrei%20A.%20Vecerik%2C%20Matej%20Roth%C3%B6rl%2C%20Thomas%20Heess%2C%20Nicolas%20Sim-to-real%20robot%20learning%20from%20pixels%20with%20progressive%20nets%202017"
        },
        {
            "id": "Schulman_et+al_2015_a",
            "entry": "Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael I., and Abbeel, Pieter. Trust Region Policy Optimization. In International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Moritz%2C%20Philipp%20Jordan%2C%20Michael%20I.%20Trust%20Region%20Policy%20Optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Moritz%2C%20Philipp%20Jordan%2C%20Michael%20I.%20Trust%20Region%20Policy%20Optimization%202015"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "Schulman, John, Chen, Xi, and Abbeel, Pieter. Equivalence between policy gradients and soft q-learning. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Chen%2C%20Xi%20Abbeel%2C%20Pieter%20Equivalence%20between%20policy%20gradients%20and%20soft%20q-learning%202017"
        },
        {
            "id": "Singh_et+al_2010_a",
            "entry": "Singh, S., Lewis, R., and Barto, A. Where do rewards come from? In Proceedings of the International Symposium on AI Inspired Biology - A Symposium at the AISB 2010 Convention, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20S.%20Lewis%2C%20R.%20Barto%2C%20A.%20Where%20do%20rewards%20come%20from%3F%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20S.%20Lewis%2C%20R.%20Barto%2C%20A.%20Where%20do%20rewards%20come%20from%3F%202010"
        },
        {
            "id": "Sorg_et+al_2010_a",
            "entry": "Sorg, Jonathan, Singh, Satinder P., and Lewis, Richard L. Reward design via online gradient ascent. In NIPS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sorg%2C%20Jonathan%20Singh%2C%20Satinder%20P.%20Lewis%2C%20Richard%20L.%20Reward%20design%20via%20online%20gradient%20ascent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sorg%2C%20Jonathan%20Singh%2C%20Satinder%20P.%20Lewis%2C%20Richard%20L.%20Reward%20design%20via%20online%20gradient%20ascent%202010"
        },
        {
            "id": "Todorov_2007_a",
            "entry": "Todorov, Emo. Linearly-solvable markov decision problems. In Advances in Neural Information Processing Systems (NIPS), 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emo%20Linearly-solvable%20markov%20decision%20problems%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emo%20Linearly-solvable%20markov%20decision%20problems%202007"
        },
        {
            "id": "Todorov_2008_a",
            "entry": "Todorov, Emo. General duality between optimal control and estimation. In IEEE Conference on Decision and Control (CDC), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emo%20General%20duality%20between%20optimal%20control%20and%20estimation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emo%20General%20duality%20between%20optimal%20control%20and%20estimation%202008"
        },
        {
            "id": "Toussaint_2009_a",
            "entry": "Toussaint, Marc. Robot trajectory optimization using approximate inference. In International Conference on Machine Learning (ICML), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Toussaint%2C%20Marc%20Robot%20trajectory%20optimization%20using%20approximate%20inference%202009"
        },
        {
            "id": "Tung_et+al_2018_a",
            "entry": "Tung, Hsiao-Yu Fish, Harley, Adam W., Huang, Liang-Kang, and Fragkiadaki, Katerina. Reward learning from narrated demonstrations. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tung%2C%20Hsiao-Yu%20Fish%20Harley%2C%20Adam%20W.%20Huang%2C%20Liang-Kang%20Fragkiadaki%2C%20Katerina%20Reward%20learning%20from%20narrated%20demonstrations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tung%2C%20Hsiao-Yu%20Fish%20Harley%2C%20Adam%20W.%20Huang%2C%20Liang-Kang%20Fragkiadaki%2C%20Katerina%20Reward%20learning%20from%20narrated%20demonstrations%202018"
        },
        {
            "id": "Ziebart_2010_a",
            "entry": "Ziebart, Brian. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD thesis, Carnegie Mellon University, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20Modeling%20purposeful%20adaptive%20behavior%20with%20the%20principle%20of%20maximum%20causal%20entropy%202010"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "Ziebart, Brian, Maas, Andrew, Bagnell, Andrew, and Dey, Anind. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20Maas%2C%20Andrew%20Bagnell%2C%20Andrew%20Dey%2C%20Anind%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20Maas%2C%20Andrew%20Bagnell%2C%20Andrew%20Dey%2C%20Anind%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        }
    ]
}
