{
    "filename": "7786-the-spectrum-of-the-fisher-information-matrix-of-a-single-hidden-layer-neural-network.pdf",
    "metadata": {
        "title": "The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network",
        "author": "Jeffrey Pennington, Pratik Worah",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7786-the-spectrum-of-the-fisher-information-matrix-of-a-single-hidden-layer-neural-network.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods."
    },
    "keywords": [
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "drug discovery",
            "url": "https://en.wikipedia.org/wiki/drug_discovery"
        },
        {
            "term": "fisher information matrix",
            "url": "https://en.wikipedia.org/wiki/fisher_information_matrix"
        },
        {
            "term": "random matrix",
            "url": "https://en.wikipedia.org/wiki/random_matrix"
        },
        {
            "term": "second order",
            "url": "https://en.wikipedia.org/wiki/second_order"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        }
    ],
    "highlights": [
        "The success of deep learning has spread from classical problems in image recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], audio synthesis [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], translation [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and speech recognition [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] to more diverse applications in unexpected areas such as protein structure prediction [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], quantum chemistry [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] and drug discovery [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "Our explicit results indicate that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks may have numerous small eigenvalues they are generically non-degenerate",
        "We showed that by tuning the nonlinearity it is possible to adjust the spectrum in such a way that the efficiency of first-order optimization methods can be improved",
        "We demonstrated how to extend the techniques developed in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] for studying random matrices with nonlinear dependencies to the block-structured curvature matrices that are relevant for optimization in deep learning",
        "The techniques presented here pave the way for future work studying deep learning via random matrix theory"
    ],
    "key_statements": [
        "The success of deep learning has spread from classical problems in image recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], audio synthesis [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], translation [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and speech recognition [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] to more diverse applications in unexpected areas such as protein structure prediction [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], quantum chemistry [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] and drug discovery [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "A central difficulty in analyzing deep learning systems stems from the complexity of neural network loss surfaces, which are highly non-convex functions, often of millions or even billions [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] of parameters",
        "It is not obvious why first-order methods are often successful in deep learning since it is known that first-order methods perform poorly in the presence of pathological curvature",
        "Among the variety of objects that may be used to quantify the geometry of the loss surface, two matrices have elevated importance: the Hessian matrix and the Fisher information matrix",
        "Our explicit results indicate that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks may have numerous small eigenvalues they are generically non-degenerate",
        "We showed that by tuning the nonlinearity it is possible to adjust the spectrum in such a way that the efficiency of first-order optimization methods can be improved",
        "We demonstrated how to extend the techniques developed in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] for studying random matrices with nonlinear dependencies to the block-structured curvature matrices that are relevant for optimization in deep learning",
        "The techniques presented here pave the way for future work studying deep learning via random matrix theory"
    ],
    "summary": [
        "The success of deep learning has spread from classical problems in image recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], audio synthesis [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], translation [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], and speech recognition [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] to more diverse applications in unexpected areas such as protein structure prediction [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], quantum chemistry [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] and drug discovery [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].",
        "The result of our analysis is an explicit characterization of the spectrum of the Fisher information matrix of a single-hidden-layer neural network with squared loss, random Gaussian weights and random Gaussian input data in the limit of large width.",
        "Consider a single-hidden-layer neural network with weight matrices W (1), W (2) \u2208 Rn\u00d7n and pointwise activation function f : R \u2192 R.",
        "For models with squared loss, it is known that the Gauss-Newton matrix is equal to the Fisher information matrix of the model distribution with respect to its parameters [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "The distribution of eigenvalues or spectrum of curvature matrices like H(0) plays an important role in optimization, as it characterizes the local geometry of the loss surface and the efficiency of first-order optimization methods.",
        "We will use the moment method in order to compute the limiting spectral density of the Fisher.",
        "2\u03c0 the Stieltjes transform of the limiting spectral density of H(0) is given by the following theorem.",
        "The Stieltjes transform of the spectral density of the Fisher information matrix of a single-hidden-layer neural network with squared loss, activation function f , weight matrices",
        "Owing to eqn (6), the branch points and poles of G(z) encode information about the delta function peaks, spectral edges, and discontinuities in the derivative of \u03c1(\u03bb).",
        "The Supplementary Material shows that the equality \u03b7 = \u03b6 only holds for linear networks, which implies that the minimum eigenvalue is nonzero for every nonlinear activation function.",
        "Note that we have a linear activation removed function the freedom to rescale fopt by a constant by maximizes the ratio, implying that nonlinearity invariably improves conditioning, at least by this measure.",
        "Reduction \u2206L The curves are highly correlated, suggesting the possibility of improved first-order optimization performance by tuning the spectrum of the Fisher through the choice of activation function.",
        "We computed the spectrum of the Fisher information matrix of a single-hidden-layer neural network with squared loss and Gaussian weights and Gaussian data in the limit of large network width.",
        "We showed that by tuning the nonlinearity it is possible to adjust the spectrum in such a way that the efficiency of first-order optimization methods can be improved.",
        "We demonstrated how to extend the techniques developed in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] for studying random matrices with nonlinear dependencies to the block-structured curvature matrices that are relevant for optimization in deep learning.",
        "The techniques presented here pave the way for future work studying deep learning via random matrix theory"
    ],
    "headline": "We extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "2",
            "entry": "[2] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "3",
            "entry": "[3] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.08144"
        },
        {
            "id": "4",
            "entry": "[4] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012"
        },
        {
            "id": "5",
            "entry": "[5] Garrett Goh, Nathan Hodas, and Abhinav Vishnu. Deep Learning for Computational Chemistry. arXiv preprint arXiv:1701.04503, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.04503"
        },
        {
            "id": "6",
            "entry": "[6] Han Altae-Tran, Bharath Ramsundar, Aneesh S. Pappu, and Vijay Pande. Low Data Drug Discovery with One-Shot Learning. American Chemical Society Central Science, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altae-Tran%2C%20Han%20Ramsundar%2C%20Bharath%20Pappu%2C%20Aneesh%20S.%20Pande%2C%20Vijay%20Low%20Data%20Drug%20Discovery%20with%20One-Shot%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altae-Tran%2C%20Han%20Ramsundar%2C%20Bharath%20Pappu%2C%20Aneesh%20S.%20Pande%2C%20Vijay%20Low%20Data%20Drug%20Discovery%20with%20One-Shot%20Learning%202017"
        },
        {
            "id": "7",
            "entry": "[7] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural language models using sparsely gated mixtures of experts. ICLR, 2017. URL http://arxiv.org/abs/1701.06538.",
            "url": "http://arxiv.org/abs/1701.06538",
            "arxiv_url": "https://arxiv.org/pdf/1701.06538"
        },
        {
            "id": "8",
            "entry": "[8] Shankar Krishnan, Ying Xiao, and Rif A. Saurous. Neumann optimizer: A practical optimization algorithm for deep neural networks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnan%2C%20Shankar%20Xiao%2C%20Ying%20Saurous%2C%20Rif%20A.%20Neumann%20optimizer%3A%20A%20practical%20optimization%20algorithm%20for%20deep%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishnan%2C%20Shankar%20Xiao%2C%20Ying%20Saurous%2C%20Rif%20A.%20Neumann%20optimizer%3A%20A%20practical%20optimization%20algorithm%20for%20deep%20neural%20networks%202018"
        },
        {
            "id": "9",
            "entry": "[9] Roger B. Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 573\u2013582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grosse%2C%20Roger%20B.%20Martens%2C%20James%20A%20kronecker-factored%20approximate%20fisher%20matrix%20for%20convolution%20layers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grosse%2C%20Roger%20B.%20Martens%2C%20James%20A%20kronecker-factored%20approximate%20fisher%20matrix%20for%20convolution%20layers%202016"
        },
        {
            "id": "10",
            "entry": "[10] John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20Subgradient%20Methods%20for%20Online%20Learning%20and%20Stochastic%20Optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20Subgradient%20Methods%20for%20Online%20Learning%20and%20Stochastic%20Optimization%202011"
        },
        {
            "id": "11",
            "entry": "[11] Diedrik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arxiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "12",
            "entry": "[12] S.I.Amari. Natural gradient works efficiently in learning. Neural Computation, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "13",
            "entry": "[13] Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Advances in Neural Information Processing Systems, pages 2634\u20132643, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Worah%2C%20Pratik%20Nonlinear%20random%20matrix%20theory%20for%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Worah%2C%20Pratik%20Nonlinear%20random%20matrix%20theory%20for%20deep%20learning%202017"
        },
        {
            "id": "14",
            "entry": "[14] Tom Heskes. On \u201cnatural\u201d learning and pruning in multilayered perceptrons. Neural Computation, 12(4):881\u2013901, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heskes%2C%20Tom%20On%20%E2%80%9Cnatural%E2%80%9D%20learning%20and%20pruning%20in%20multilayered%20perceptrons%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heskes%2C%20Tom%20On%20%E2%80%9Cnatural%E2%80%9D%20learning%20and%20pruning%20in%20multilayered%20perceptrons%202000"
        },
        {
            "id": "15",
            "entry": "[15] BC Carlson. A table of elliptic integrals of the third kind. Mathematics of computation, 51 (183):267\u2013280, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlson%2C%20B.C.%20A%20table%20of%20elliptic%20integrals%20of%20the%20third%20kind.%20Mathematics%20of%20computation%2C%2051%201988"
        },
        {
            "id": "16",
            "entry": "[16] Mariano Giaquinta and Stefan Hilderbrandt. Calculus of Variations 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giaquinta%2C%20Mariano%20Hilderbrandt%2C%20Stefan%20Calculus%20of%20Variations%201"
        },
        {
            "id": "17",
            "entry": "[17] Richard Stanley. Polygon Dissections and Standard Young Tableaux. Journal of Combinatorial Theory, Series A, 1996. 10 ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stanley%2C%20Richard%20Polygon%20Dissections%20and%20Standard%20Young%20Tableaux%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stanley%2C%20Richard%20Polygon%20Dissections%20and%20Standard%20Young%20Tableaux%201996"
        }
    ]
}
