{
    "filename": "7505-local-differential-privacy-for-evolving-data.pdf",
    "metadata": {
        "title": "Local Differential Privacy for Evolving Data",
        "author": "Matthew Joseph, Aaron Roth, Jonathan Ullman, Bo Waggoner",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7505-local-differential-privacy-for-evolving-data.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the \u201clocal model\u201d of differential privacy that these systems use. In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation."
    },
    "keywords": [
        {
            "term": "differential privacy",
            "url": "https://en.wikipedia.org/wiki/differential_privacy"
        }
    ],
    "highlights": [
        "After over a decade of research, differential privacy [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] is moving from theory to practice, with notable deployments by Google [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], Apple [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Microsoft [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and the U.S Census Bureau [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "The bulk of the differential privacy literature has focused on the central model, in which user data is collected by a trusted aggregator who performs and publishes the results of a differentially private computation [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "Google, Apple, and Microsoft have instead chosen to operate in the local model [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], where users individually randomize their data on their own devices and send it to a potentially untrusted aggregator for analysis [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "In this paper we present a technique that makes it possible to repeatedly recompute a statistic with error that decays with the number of times the statistic changes significantly, rather than the number of times we recompute the current value of the statistic, all while satisfying local differential privacy",
        "Our techniques are rather general, we first focus our attention on the problem of privately estimating the average of bits, with one bit held by each user. This simple problem is widely applicable because most algorithms in the local model have the following structure: on each individual\u2019s device, data records are translated into a short bit vector using sketching or hashing techniques",
        "We use the methods developed above to obtain similar guarantees for a common problem in local differential privacy known as heavy hitters"
    ],
    "key_statements": [
        "After over a decade of research, differential privacy [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] is moving from theory to practice, with notable deployments by Google [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], Apple [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Microsoft [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and the U.S Census Bureau [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "The bulk of the differential privacy literature has focused on the central model, in which user data is collected by a trusted aggregator who performs and publishes the results of a differentially private computation [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "Google, Apple, and Microsoft have instead chosen to operate in the local model [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], where users individually randomize their data on their own devices and send it to a potentially untrusted aggregator for analysis [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "In this paper we present a technique that makes it possible to repeatedly recompute a statistic with error that decays with the number of times the statistic changes significantly, rather than the number of times we recompute the current value of the statistic, all while satisfying local differential privacy",
        "Our techniques are rather general, we first focus our attention on the problem of privately estimating the average of bits, with one bit held by each user. This simple problem is widely applicable because most algorithms in the local model have the following structure: on each individual\u2019s device, data records are translated into a short bit vector using sketching or hashing techniques",
        "We use the methods developed above to obtain similar guarantees for a common problem in local differential privacy known as heavy hitters",
        "We show how our techniques combine with an approach of Bassily and Smith [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] to obtain the first guarantees for heavy hitters on evolving data"
    ],
    "summary": [
        "After over a decade of research, differential privacy [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] is moving from theory to practice, with notable deployments by Google [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], Apple [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Microsoft [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and the U.S Census Bureau [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "Our main result is an algorithm that takes data generated according to our model, guarantees a fixed level of local privacy \u03b5 that grows with the number of distributional changes rather than the number of epochs, and guarantees that the estimates released at the end of each epoch are accurate up to error that scales sublinearly in 1 and only polylogarithmically with the total number of epochs T .",
        "There is an \u03b5-differentially private local protocol that achieves the following guarantee: with probability at least 1\u2212\u03b4, while the total number of elapsed epochs t where some subgroup distribution has changed is fewer than \u03b5 \u22c5 min",
        "There is an \u03b5-differentially private local protocol that achieves the following guarantee: with probability at least 1 \u2212 \u03b4, while the total number of elapsed epochs t where some subgroup distribution has changed is fewer than \u03b5 \u22c5 min oracles ft such that for all v \u2208 [d]",
        "We can update this global estimate pt using randomized response: each user submits some differentially private estimate of the mean of their data, and the center aggregates these responses to obtain pt.",
        "Users will examine their data and privately publish a vote for whether they believe the global estimate needs to be updated.",
        "The problem with this protocol is that small changes in the underlying mean pt may cause some users to vote 1 and others to vote 0, and this might continue for an arbitrarily long time without inducing a global update.",
        "Our accuracy theorem needs the following assumption on L, the size of the smallest subgroup, to guarantee that a global update occurs whenever any subgroup has all of its member users vote \u201cyes\".",
        "This implies that if every user in some subgroup computes a local estimate pit such that pit \u2212 pfi (t) exceeds the highest threshold, every user sends a \u201cyes\" vote and a global update occurs, bringing with it the global accuracy guarantee proven above.",
        "We use the methods developed above to obtain similar guarantees for a common problem in local differential privacy known as heavy hitters.",
        "Our privacy guarantee is almost immediate: since HEAVYTHRESH shares its voting protocols with THRESH, the only additional analysis needed is for the estimation randomizer R."
    ],
    "headline": "We introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] John M. Abowd. The challenge of scientific reproducibility and privacy protection for statistical agencies. Census Scientific Advisory Committee, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abowd%2C%20John%20M.%20The%20challenge%20of%20scientific%20reproducibility%20and%20privacy%20protection%20for%20statistical%20agencies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abowd%2C%20John%20M.%20The%20challenge%20of%20scientific%20reproducibility%20and%20privacy%20protection%20for%20statistical%20agencies%202016"
        },
        {
            "id": "2",
            "entry": "[2] Differential Privacy Team Apple. Learning with privacy at scale. Technical report, Apple, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Differential%20Privacy%20Team%20Apple.%20Learning%20with%20privacy%20at%20scale%202017"
        },
        {
            "id": "3",
            "entry": "[3] Raef Bassily and Adam Smith. Local, private, efficient protocols for succinct histograms. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 127\u2013135. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bassily%2C%20Raef%20Local%2C%20Adam%20Smith%20private%20efficient%20protocols%20for%20succinct%20histograms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bassily%2C%20Raef%20Local%2C%20Adam%20Smith%20private%20efficient%20protocols%20for%20succinct%20histograms%202015"
        },
        {
            "id": "4",
            "entry": "[4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Differentially private empirical risk minimization: Efficient algorithms and tight error bounds. arXiv preprint arXiv:1405.7085, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1405.7085"
        },
        {
            "id": "5",
            "entry": "[5] Raef Bassily, Uri Stemmer, and Abhradeep Guha Thakurta. Practical locally private heavy hitters. In Advances in Neural Information Processing Systems, pages 2285\u20132293, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bassily%2C%20Raef%20Stemmer%2C%20Uri%20and%20Abhradeep%20Guha%20Thakurta.%20Practical%20locally%20private%20heavy%20hitters%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bassily%2C%20Raef%20Stemmer%2C%20Uri%20and%20Abhradeep%20Guha%20Thakurta.%20Practical%20locally%20private%20heavy%20hitters%202017"
        },
        {
            "id": "6",
            "entry": "[6] Andrea Bittau, \u00dalfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David Lie, Mitch Rudominer, Usharsee Kode, Julien Tinnes, and Bernhard Seefeld. Prochlo: Strong privacy for analytics in the crowd. arXiv preprint arXiv:1710.00901, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.00901"
        },
        {
            "id": "7",
            "entry": "[7] Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to noninteractive database privacy. Journal of the ACM (JACM), 60(2):12, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Ligett%2C%20Katrina%20Roth%2C%20Aaron%20A%20learning%20theory%20approach%20to%20noninteractive%20database%20privacy%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Ligett%2C%20Katrina%20Roth%2C%20Aaron%20A%20learning%20theory%20approach%20to%20noninteractive%20database%20privacy%202013"
        },
        {
            "id": "8",
            "entry": "[8] Mark Bun, Jelani Nelson, and Uri Stemmer. Heavy hitters and the structure of local privacy. arXiv preprint arXiv:1711.04740, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04740"
        },
        {
            "id": "9",
            "entry": "[9] Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(Mar):1069\u20131109, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20Kamalika%20Monteleoni%2C%20Claire%20Sarwate%2C%20Anand%20D.%20Differentially%20private%20empirical%20risk%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20Kamalika%20Monteleoni%2C%20Claire%20Sarwate%2C%20Anand%20D.%20Differentially%20private%20empirical%20risk%20minimization%202011"
        },
        {
            "id": "10",
            "entry": "[10] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In Proceedings of Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ding%2C%20Bolin%20Kulkarni%2C%20Janardhan%20Yekhanin%2C%20Sergey%20Collecting%20telemetry%20data%20privately%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ding%2C%20Bolin%20Kulkarni%2C%20Janardhan%20Yekhanin%2C%20Sergey%20Collecting%20telemetry%20data%20privately%202017"
        },
        {
            "id": "11",
            "entry": "[11] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%202014"
        },
        {
            "id": "12",
            "entry": "[12] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265\u2013284.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20McSherry%2C%20Frank%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20Calibrating%20noise%20to%20sensitivity%20in%20private%20data%20analysis",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20McSherry%2C%20Frank%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20Calibrating%20noise%20to%20sensitivity%20in%20private%20data%20analysis"
        },
        {
            "id": "13",
            "entry": "[13] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381\u2013390. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Naor%2C%20Moni%20Reingold%2C%20Omer%20Rothblum%2C%20Guy%20N.%20On%20the%20complexity%20of%20differentially%20private%20data%20release%3A%20efficient%20algorithms%20and%20hardness%20results%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Naor%2C%20Moni%20Reingold%2C%20Omer%20Rothblum%2C%20Guy%20N.%20On%20the%20complexity%20of%20differentially%20private%20data%20release%3A%20efficient%20algorithms%20and%20hardness%20results%202009"
        },
        {
            "id": "14",
            "entry": "[14] Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N Rothblum. Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 715\u2013724. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Naor%2C%20Moni%20Pitassi%2C%20Toniann%20Rothblum%2C%20Guy%20N.%20Differential%20privacy%20under%20continual%20observation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Naor%2C%20Moni%20Pitassi%2C%20Toniann%20Rothblum%2C%20Guy%20N.%20Differential%20privacy%20under%20continual%20observation%202010"
        },
        {
            "id": "15",
            "entry": "[15] \u00dalfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC conference on computer and communications security, pages 1054\u20131067. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erlingsson%2C%20%C3%9Alfar%20Pihur%2C%20Vasyl%20Korolova%2C%20Aleksandra%20Rappor%3A%20Randomized%20aggregatable%20privacy-preserving%20ordinal%20response%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erlingsson%2C%20%C3%9Alfar%20Pihur%2C%20Vasyl%20Korolova%2C%20Aleksandra%20Rappor%3A%20Randomized%20aggregatable%20privacy-preserving%20ordinal%20response%202014"
        },
        {
            "id": "16",
            "entry": "[16] Moritz Hardt and Guy N Rothblum. A multiplicative weights mechanism for privacy-preserving data analysis. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 61\u201370. IEEE, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Rothblum%2C%20Guy%20N.%20A%20multiplicative%20weights%20mechanism%20for%20privacy-preserving%20data%20analysis%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Rothblum%2C%20Guy%20N.%20A%20multiplicative%20weights%20mechanism%20for%20privacy-preserving%20data%20analysis%202010"
        },
        {
            "id": "17",
            "entry": "[17] Justin Hsu, Sanjeev Khanna, and Aaron Roth. Distributed private heavy hitters. In International Colloquium on Automata, Languages, and Programming, pages 461\u2013472.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Justin%20Khanna%2C%20Sanjeev%20Roth%2C%20Aaron%20Distributed%20private%20heavy%20hitters",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20Justin%20Khanna%2C%20Sanjeev%20Roth%2C%20Aaron%20Distributed%20private%20heavy%20hitters"
        },
        {
            "id": "18",
            "entry": "[18] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? In Proceedings of the 54th Annual Symposium on Foundations of Computer Science, pages 531\u2013540, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kasiviswanathan%2C%20Shiva%20Prasad%20Lee%2C%20Homin%20K.%20Nissim%2C%20Kobbi%20Raskhodnikova%2C%20Sofya%20What%20can%20we%20learn%20privately%3F%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kasiviswanathan%2C%20Shiva%20Prasad%20Lee%2C%20Homin%20K.%20Nissim%2C%20Kobbi%20Raskhodnikova%2C%20Sofya%20What%20can%20we%20learn%20privately%3F%202008"
        },
        {
            "id": "19",
            "entry": "[19] Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, and Steven Z Wu. Accuracy first: Selecting a differential privacy level for accuracy constrained erm. In Advances in Neural Information Processing Systems, pages 2563\u20132573, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ligett%2C%20Katrina%20Neel%2C%20Seth%20Roth%2C%20Aaron%20Waggoner%2C%20Bo%20Accuracy%20first%3A%20Selecting%20a%20differential%20privacy%20level%20for%20accuracy%20constrained%20erm%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ligett%2C%20Katrina%20Neel%2C%20Seth%20Roth%2C%20Aaron%20Waggoner%2C%20Bo%20Accuracy%20first%3A%20Selecting%20a%20differential%20privacy%20level%20for%20accuracy%20constrained%20erm%202017"
        },
        {
            "id": "20",
            "entry": "[20] Nina Mishra and Mark Sandler. Privacy via pseudorandom sketches. In Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 143\u2013152. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20Nina%20Sandler%2C%20Mark%20Privacy%20via%20pseudorandom%20sketches%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20Nina%20Sandler%2C%20Mark%20Privacy%20via%20pseudorandom%20sketches%202006"
        },
        {
            "id": "21",
            "entry": "[21] Ryan M Rogers, Aaron Roth, Jonathan Ullman, and Salil Vadhan. Privacy odometers and filters: Pay-as-you-go composition. In Advances in Neural Information Processing Systems, pages 1921\u20131929, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rogers%2C%20Ryan%20M.%20Roth%2C%20Aaron%20Ullman%2C%20Jonathan%20Vadhan%2C%20Salil%20Privacy%20odometers%20and%20filters%3A%20Pay-as-you-go%20composition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rogers%2C%20Ryan%20M.%20Roth%2C%20Aaron%20Ullman%2C%20Jonathan%20Vadhan%2C%20Salil%20Privacy%20odometers%20and%20filters%3A%20Pay-as-you-go%20composition%202016"
        },
        {
            "id": "22",
            "entry": "[22] Aaron Roth and Tim Roughgarden. Interactive privacy via the median mechanism. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 765\u2013774. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roth%2C%20Aaron%20Roughgarden%2C%20Tim%20Interactive%20privacy%20via%20the%20median%20mechanism%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roth%2C%20Aaron%20Roughgarden%2C%20Tim%20Interactive%20privacy%20via%20the%20median%20mechanism%202010"
        },
        {
            "id": "23",
            "entry": "[23] Jun Tang, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, and Xiaofeng Wang. Privacy loss in apple\u2019s implementation of differential privacy on macos 10.12. arXiv preprint arXiv:1709.02753, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.02753"
        },
        {
            "id": "24",
            "entry": "[24] Jonathan Ullman. Tight lower bounds for locally differentially private selection. Manuscript, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullman%2C%20Jonathan%20Tight%20lower%20bounds%20for%20locally%20differentially%20private%20selection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullman%2C%20Jonathan%20Tight%20lower%20bounds%20for%20locally%20differentially%20private%20selection%202018"
        }
    ]
}
