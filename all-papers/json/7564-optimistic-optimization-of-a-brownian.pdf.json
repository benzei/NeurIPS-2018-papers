{
    "filename": "7564-optimistic-optimization-of-a-brownian.pdf",
    "metadata": {
        "title": "Optimistic optimization of a Brownian",
        "author": "Jean-Bastien Grill, Michal Valko, Remi Munos",
        "date": 1996,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7564-optimistic-optimization-of-a-brownian.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We address the problem of optimizing a Brownian motion. We consider a (random) realization W of a Brownian motion with input space in [0, 1]. Given W , our goal is to return an \u03b5-approximation of its maximum using the smallest possible number of function evaluations, the sample complexity of the algorithm. We provide an algorithm with sample complexity of order log2(1/\u03b5). This improves over previous results of <a class=\"ref-link\" id=\"cAl-Mharmah_1996_a\" href=\"#rAl-Mharmah_1996_a\">Al-Mharmah and Calvin (1996</a>) and Calvin et al. (2017) which provided only polynomial rates. Our algorithm is adaptive\u2014each query depends on previous values\u2014and is an instance of the optimism-in-the-face-of-uncertainty principle."
    },
    "keywords": [
        {
            "term": "probably approximately correct",
            "url": "https://en.wikipedia.org/wiki/probably_approximately_correct"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "brownian motion",
            "url": "https://en.wikipedia.org/wiki/brownian_motion"
        }
    ],
    "highlights": [
        "Motivation There are two types of situations where this problem is useful",
        "The problem considered in the present paper can be seen as a way to approximately perform this step in a computationally efficient way when this Gaussian process is a Brownian motion",
        "Our contribution We introduce the algorithm OOB = optimistic optimization of the Brownian motion",
        "We show that for any \u03b5, there exists some particular metric \u03b5 such that the Brownian motion W is \u03b5-Lipschitz with probability 1 \u2212 \u03b5, and there exists a constant C(\u03b5) = O) such that (d = 0, C(\u03b5)) characterizes the Brownian motion",
        "The final sample complexity bound is essentially constituted by one O) term coming from the standard DOO error for deterministic function optimization and another O) term because we need to adapt our pseudo-distance to \u03b5 such that the Brownian is -Lipschitz with probability 1 \u2212 \u03b5",
        "We presented OOB, an algorithm inspired by DOO (<a class=\"ref-link\" id=\"cMunos_2011_a\" href=\"#rMunos_2011_a\">Munos, 2011</a>) that efficiently optimizes a Brownian motion"
    ],
    "key_statements": [
        "Motivation There are two types of situations where this problem is useful",
        "The problem considered in the present paper can be seen as a way to approximately perform this step in a computationally efficient way when this Gaussian process is a Brownian motion",
        "Our contribution We introduce the algorithm OOB = optimistic optimization of the Brownian motion",
        "We show that for any \u03b5, there exists some particular metric \u03b5 such that the Brownian motion W is \u03b5-Lipschitz with probability 1 \u2212 \u03b5, and there exists a constant C(\u03b5) = O) such that (d = 0, C(\u03b5)) characterizes the Brownian motion",
        "We prove a property specific to W by bounding the number of near-optimal points of the Brownian motion in expectation",
        "The final sample complexity bound is essentially constituted by one O) term coming from the standard DOO error for deterministic function optimization and another O) term because we need to adapt our pseudo-distance to \u03b5 such that the Brownian is -Lipschitz with probability 1 \u2212 \u03b5",
        "We presented OOB, an algorithm inspired by DOO (<a class=\"ref-link\" id=\"cMunos_2011_a\" href=\"#rMunos_2011_a\">Munos, 2011</a>) that efficiently optimizes a Brownian motion",
        "We proved that the sample complexity of OOB is of order O log2(1/\u03b5) which improves over the previously best known bound (<a class=\"ref-link\" id=\"cCalvin_et+al_2017_a\" href=\"#rCalvin_et+al_2017_a\">Calvin et al, 2017</a>)",
        "As we are not aware of a lower bound for the Brownian motion optimization, we do not know whether O log2(1/\u03b5) is the minimax rate of the sample complexity or if there exists an algorithm that can do even better",
        "To bound the expected number of near-optimal nodes, we use the result of <a class=\"ref-link\" id=\"cDenisov_1984_a\" href=\"#rDenisov_1984_a\">Denisov (1984</a>) which is based on 2 components: 1 A Brownian motion can be rewritten as an affine transformation of a Brownian motion conditioned to be positive, translated by an time at which the Brownian motion attains its maximum.\n2 A Brownian motion conditioned to be positive is a Brownian meander"
    ],
    "summary": [
        "Motivation There are two types of situations where this problem is useful.",
        "The number of function evaluations required by the algorithm to achieve this \u03b5-approximation of the maximum defines the sample-complexity.",
        "This begets a simple algorithm that requires an expected number of queries of the order of log2(1/\u03b5) to return an \u03b5-approximation of the maximum, with probability at least 1 \u2212 \u03b5 w.r.t. the random sample of the Brownian motion.",
        "We claim that the expectation of the number of samples that OOB needs to optimize this realization with precision \u03b5 is O log2(1/\u03b5) .",
        "We combine the definition of C with the termination condition of OOB to get that under event C, the best point M\u03b5 so far, is close to the maximum M of the Brownian up to \u03b5.",
        "A necessary condition for an interval [a, b] to be split is that max(W (a), W (b)) \u2265 M \u2212 \u03b7 which means that either a or b or both are \u03b7-near-optimal which is the key point of Lemma 2.",
        "Lemma 2 explicitly links the near-optimality from Definition 2 with the number of samples N\u03b5 performed by OOB before it terminates.",
        "We define a high-probability event C under which the number of samples is bounded by the number of near-optimal points Nh for all h \u2264 hmax.",
        "= 2 Nh. We prove a property specific to W by bounding the number of near-optimal points of the Brownian motion in expectation.",
        "The final sample complexity bound is essentially constituted by one O) term coming from the standard DOO error for deterministic function optimization and another O) term because we need to adapt our pseudo-distance to \u03b5 such that the Brownian is -Lipschitz with probability 1 \u2212 \u03b5.",
        "As we are not aware of a lower bound for the Brownian motion optimization, we do not know whether O log2(1/\u03b5) is the minimax rate of the sample complexity or if there exists an algorithm that can do even better.",
        "That Lemma 3 bounds the number of near-optimal nodes of a Brownian motion in expectation.",
        "To bound the expected number of near-optimal nodes, we use the result of <a class=\"ref-link\" id=\"cDenisov_1984_a\" href=\"#rDenisov_1984_a\">Denisov (1984</a>) which is based on 2 components: 1 A Brownian motion can be rewritten as an affine transformation of a Brownian motion conditioned to be positive, translated by an time at which the Brownian motion attains its maximum."
    ],
    "headline": "We address the problem of optimizing a Brownian motion",
    "reference_links": [
        {
            "id": "Al-Mharmah_1996_a",
            "entry": "Hisham Al-Mharmah and James M. Calvin. Optimal random non-adaptive algorithm for global optimization of Brownian motion. Journal of Global Optimization, 8(1):81\u201390, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Al-Mharmah%2C%20Hisham%20Calvin%2C%20James%20M.%20Optimal%20random%20non-adaptive%20algorithm%20for%20global%20optimization%20of%20Brownian%20motion%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Mharmah%2C%20Hisham%20Calvin%2C%20James%20M.%20Optimal%20random%20non-adaptive%20algorithm%20for%20global%20optimization%20of%20Brownian%20motion%201996"
        },
        {
            "id": "Basu_2018_a",
            "entry": "Kinjal Basu and Souvik Ghosh. Analysis of Thompson sampling for Gaussian process optimization in the bandit setting. arXiv preprint arXiv:1705.06808, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1705.06808"
        },
        {
            "id": "Calvin_et+al_2017_a",
            "entry": "James M. Calvin, Mario Hefter, and Andre Herzwurm. Adaptive approximation of the minimum of Brownian motion. Journal of Complexity, 39(C):17\u201337, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calvin%2C%20James%20M.%20Hefter%2C%20Mario%20Herzwurm%2C%20Andre%20Adaptive%20approximation%20of%20the%20minimum%20of%20Brownian%20motion%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calvin%2C%20James%20M.%20Hefter%2C%20Mario%20Herzwurm%2C%20Andre%20Adaptive%20approximation%20of%20the%20minimum%20of%20Brownian%20motion%202017"
        },
        {
            "id": "Chapelle_2011_a",
            "entry": "Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Neural Information Processing Systems. 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Li%2C%20Lihong%20An%20empirical%20evaluation%20of%20Thompson%20sampling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Li%2C%20Lihong%20An%20empirical%20evaluation%20of%20Thompson%20sampling%202011"
        },
        {
            "id": "Denisov_1984_a",
            "entry": "I. V. Denisov. A random walk and a Wiener process near a maximum. Theory of Probability & Its Applications, 28(4):821\u2013824, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denisov%2C%20I.V.%20A%20random%20walk%20and%20a%20Wiener%20process%20near%20a%20maximum%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denisov%2C%20I.V.%20A%20random%20walk%20and%20a%20Wiener%20process%20near%20a%20maximum%201984"
        },
        {
            "id": "Durrett_et+al_1977_a",
            "entry": "Richard T. Durrett, Donald L. Iglehart, and Douglas R. Miller. Weak convergence to Brownian meander and Brownian excursion. The Annals of Probability, 5(1):117\u2013129, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Durrett%2C%20Richard%20T.%20Iglehart%2C%20Donald%20L.%20Miller%2C%20Douglas%20R.%20Weak%20convergence%20to%20Brownian%20meander%20and%20Brownian%20excursion%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Durrett%2C%20Richard%20T.%20Iglehart%2C%20Donald%20L.%20Miller%2C%20Douglas%20R.%20Weak%20convergence%20to%20Brownian%20meander%20and%20Brownian%20excursion%201977"
        },
        {
            "id": "Hefter_2017_a",
            "entry": "Mario Hefter and Andre Herzwurm. Optimal strong approximation of the one-dimensional squared Bessel process. Communications in Mathematical Sciences, 15(8):2121\u20132141, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hefter%2C%20Mario%20Herzwurm%2C%20Andre%20Optimal%20strong%20approximation%20of%20the%20one-dimensional%20squared%20Bessel%20process%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hefter%2C%20Mario%20Herzwurm%2C%20Andre%20Optimal%20strong%20approximation%20of%20the%20one-dimensional%20squared%20Bessel%20process%202017"
        },
        {
            "id": "Munos_2011_a",
            "entry": "Remi Munos. Optimistic optimization of deterministic functions without the knowledge of its smoothness. In Neural Information Processing Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20Remi%20Optimistic%20optimization%20of%20deterministic%20functions%20without%20the%20knowledge%20of%20its%20smoothness%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20Remi%20Optimistic%20optimization%20of%20deterministic%20functions%20without%20the%20knowledge%20of%20its%20smoothness%202011"
        },
        {
            "id": "Russo_et+al_2018_a",
            "entry": "Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. Foundations and Trends in Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20Kazerouni%2C%20Abbas%20Osband%2C%20Ian%20A%20tutorial%20on%20Thompson%20sampling%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20Kazerouni%2C%20Abbas%20Osband%2C%20Ian%20A%20tutorial%20on%20Thompson%20sampling%202018"
        },
        {
            "id": "Thompson_1933_a",
            "entry": "William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25:285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        }
    ]
}
