{
    "filename": "7644-learning-sparse-neural-networks-via-sensitivity-driven-regularization.pdf",
    "metadata": {
        "title": "Learning sparse neural networks via sensitivity-driven regularization",
        "author": "Enzo Tartaglione, Skjalg Leps\u00f8y, Attilio Fiandrotti, Gianluca Francini",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7644-learning-sparse-neural-networks-via-sensitivity-driven-regularization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The ever-increasing number of parameters in deep neural networks poses challenges for memory-limited applications. Regularize-and-prune methods aim at meeting these challenges by sparsifying the network weights. In this context we quantify the output sensitivity to the parameters (i.e. their relevance to the network output) and introduce a regularization term that gradually lowers the absolute value of parameters with low sensitivity. Thus, a very large fraction of the parameters approach zero and are eventually set to zero by simple thresholding. Our method surpasses most of the recent techniques both in terms of sparsity and error rates. In some cases, the method reaches twice the sparsity obtained by other techniques at equal error rates."
    },
    "keywords": [
        {
            "term": "error rate",
            "url": "https://en.wikipedia.org/wiki/error_rate"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        }
    ],
    "highlights": [
        "Deep neural networks achieve state-of-the-art performance in a number of tasks by means of complex architectures",
        "The complexity of a neural network can be reduced by promoting sparse interconnection structures",
        "We derive a perparameter general formulation of a regularization term based on sensitivity, having as particular case ReLU-activated neural networks",
        "We investigate how our sensitivity-based regularization term affects the network generalization ability, which is the ultimate goal of regularization",
        "In this work we have proposed a sensitivity-based regularize-and-prune method for the supervised learning of sparse network topologies",
        "We have introduced a regularization term that selectively drives towards zero parameters that are less sensitive, i.e. have little importance on the network output, and can be pruned without affecting the network performance"
    ],
    "key_statements": [
        "Deep neural networks achieve state-of-the-art performance in a number of tasks by means of complex architectures",
        "The complexity of a neural network can be reduced by promoting sparse interconnection structures",
        "We derive a perparameter general formulation of a regularization term based on sensitivity, having as particular case ReLU-activated neural networks",
        "We investigate how our sensitivity-based regularization term affects the network generalization ability, which is the ultimate goal of regularization",
        "As we focus on the effects of the regularization term, no thresholding or pruning is applied and we consider the unspecific sensitivity formulation in (17)",
        "In this work we have proposed a sensitivity-based regularize-and-prune method for the supervised learning of sparse network topologies",
        "We have introduced a regularization term that selectively drives towards zero parameters that are less sensitive, i.e. have little importance on the network output, and can be pruned without affecting the network performance"
    ],
    "summary": [
        "Deep neural networks achieve state-of-the-art performance in a number of tasks by means of complex architectures.",
        "Letting \u03b8 denote the network parameters and \u03bb the regularization factor, the problem minimize L(\u03b8) with respect to \u03b8",
        "The original contribution of this work is a regularization and pruning method that takes advantage of output sensitivity to each parameter.",
        "In Sec. 4 we experiment with our proposed training scheme over different network architectures.",
        "Zhu et al [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], have shown that a large sparse architecture improves the network generalization ability in a number of different scenarios.",
        "A direct strategy to introduce sparsity in neural networks is l0 regularization, which entails solving a highly complex optimization problem (e.g., Louizos et al [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]).",
        "We propose to selectively prune each network parameter using the knowledge of sensitivity.",
        "We derive a perparameter general formulation of a regularization term based on sensitivity, having as particular case ReLU-activated neural networks.",
        "We experiment training a number of well-known neural network architectures and over a number of different image datasets.",
        "For each trained network we measure the sparsity with layer granularity and the corresponding memory footprint assuming single precision float representation of each parameter.",
        "We investigate how our sensitivity-based regularization term affects the network generalization ability, which is the ultimate goal of regularization.",
        "As we focus on the effects of the regularization term, no thresholding or pruning is applied and we consider the unspecific sensitivity formulation in (17).",
        "VGG-16 is a 13 convolutional, 3 fully connected layers deep network having more than 100M parameters while ImageNet consists of 224x224 24-bit colour images of 1000 different types of objects.",
        "As previous experiment revealed our method enables improved sparsification for comparable error, here we train the network up to the point where the Top-1 error is minimized.",
        "In this work we have proposed a sensitivity-based regularize-and-prune method for the supervised learning of sparse network topologies.",
        "We have introduced a regularization term that selectively drives towards zero parameters that are less sensitive, i.e. have little importance on the network output, and can be pruned without affecting the network performance.",
        "Our proposed method enables more effective sparsification than other regularization-based methods for both the specific and the unspecific formulation of the sensitivity in fully-connected architectures.",
        "Future work involves an investigation into the observed improvement of generalization, a study of the trade-offs between specific and unspecific sensitivity, and the extension of the sensitivity term to the case of shared parameters."
    ],
    "headline": "In Sec. 2 we review the relevant literature concerning sparse neural architectures",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Karen Simonyan and Andrew Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "2",
            "entry": "[2] Jie Hu, Li Shen, and Gang Sun, \u201cSqueeze-and-excitation networks,\u201d in Conference on Computer Vision and Pattern Recognition, CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Jie%20Shen%2C%20Li%20Sun%2C%20Gang%20Squeeze-and-excitation%20networks%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Jie%20Shen%2C%20Li%20Sun%2C%20Gang%20Squeeze-and-excitation%20networks%2C%202018"
        },
        {
            "id": "3",
            "entry": "[3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio, \u201cDeep sparse rectifier neural networks,\u201d in Proceedings of the 14th International Conference on Artiicial Intelligence and Statistics (AISTATS), 2011, pp. 315\u2013323.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Deep%20sparse%20rectifier%20neural%20networks%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bordes%2C%20Antoine%20Bengio%2C%20Yoshua%20Deep%20sparse%20rectifier%20neural%20networks%2C%202011"
        },
        {
            "id": "4",
            "entry": "[4] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz, \u201cSGD learns overparameterized networks that provably generalize on linearly separable data,\u201d arXiv preprint arXiv:1710.10174, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10174"
        },
        {
            "id": "5",
            "entry": "[5] Hrushikesh N Mhaskar and Tomaso Poggio, \u201cDeep vs. shallow networks: An approximation theory perspective,\u201d Analysis and Applications, vol. 14, no. 06, pp. 829\u2013848, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mhaskar%2C%20Hrushikesh%20N.%20Poggio%2C%20Tomaso%20Deep%20vs.%20shallow%20networks%3A%20An%20approximation%20theory%20perspective%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mhaskar%2C%20Hrushikesh%20N.%20Poggio%2C%20Tomaso%20Deep%20vs.%20shallow%20networks%3A%20An%20approximation%20theory%20perspective%2C%202016"
        },
        {
            "id": "6",
            "entry": "[6] Charles W. Groetsch, Inverse Problems in the Mathematical Sciences, Vieweg, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Charles%20W%20Groetsch%20Inverse%20Problems%20in%20the%20Mathematical%20Sciences%20Vieweg%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Charles%20W%20Groetsch%20Inverse%20Problems%20in%20the%20Mathematical%20Sciences%20Vieweg%201993"
        },
        {
            "id": "7",
            "entry": "[7] Sayan Mukherjee, Partha Niyogic, Tomaso Poggio, and Ryan Rifkin, \u201cLearning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization,\u201d Advances in Computational Mathematics, vol. 25, pp. 161\u2013193, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mukherjee%2C%20Sayan%20Niyogic%2C%20Partha%20Poggio%2C%20Tomaso%20Rifkin%2C%20Ryan%20Learning%20theory%3A%20stability%20is%20sufficient%20for%20generalization%20and%20necessary%20and%20sufficient%20for%20consistency%20of%20empirical%20risk%20minimization%2C%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mukherjee%2C%20Sayan%20Niyogic%2C%20Partha%20Poggio%2C%20Tomaso%20Rifkin%2C%20Ryan%20Learning%20theory%3A%20stability%20is%20sufficient%20for%20generalization%20and%20necessary%20and%20sufficient%20for%20consistency%20of%20empirical%20risk%20minimization%2C%202006"
        },
        {
            "id": "8",
            "entry": "[8] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li, \u201cLearning structured sparsity in deep neural networks,\u201d in Advances in Neural Information Processing Systems, 2016, pp. 2074\u20132082.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Wu%2C%20Chunpeng%20Wang%2C%20Yandan%20Chen%2C%20Yiran%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%2C%202016"
        },
        {
            "id": "9",
            "entry": "[9] Song Han, Jeff Pool, John Tran, and William Dally, \u201cLearning both weights and connections for efficient neural network,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 1135\u20131143.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Song%20Pool%2C%20Jeff%20Tran%2C%20John%20Dally%2C%20William%20Learning%20both%20weights%20and%20connections%20for%20efficient%20neural%20network%2C%202015"
        },
        {
            "id": "10",
            "entry": "[10] Karen Ullrich, Edward Meeds, and Max Welling, \u201cSoft weight-sharing for neural network compression,\u201d arXiv preprint arXiv:1702.04008, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04008"
        },
        {
            "id": "11",
            "entry": "[11] Michael Zhu and Suyog Gupta, \u201cTo prune, or not to prune: exploring the efficacy of pruning for model compression,\u201d arXiv preprint arXiv:1710.01878, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.01878"
        },
        {
            "id": "12",
            "entry": "[12] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky, \u201cSparse convolutional neural networks,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 806\u2013814.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Baoyuan%20Wang%2C%20Min%20Foroosh%2C%20Hassan%20Tappen%2C%20Marshall%20Sparse%20convolutional%20neural%20networks%2C%202015"
        },
        {
            "id": "13",
            "entry": "[13] Christos Louizos, Max Welling, and Diederik P Kingma, \u201cLearning sparse neural networks through l_0 regularization,\u201d arXiv preprint arXiv:1712.01312, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01312"
        },
        {
            "id": "14",
            "entry": "[14] Y-lan Boureau, Yann L Cun, et al., \u201cSparse feature learning for deep belief networks,\u201d in Advances in neural information processing systems, 2008, pp. 1185\u20131192.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boureau%2C%20Y.-lan%20Cun%2C%20Yann%20L.%20%E2%80%9CSparse%20feature%20learning%20for%20deep%20belief%20networks%2C%E2%80%9D%20in%20Advances%20in%20neural%20information%20processing%20systems%202008"
        },
        {
            "id": "15",
            "entry": "[15] Eugenio Culurciello, Ralph Etienne-Cummings, and Kwabena A Boahen, \u201cA biomorphic digital image sensor,\u201d IEEE Journal of Solid-State Circuits, vol. 38, no. 2, pp. 281\u2013294, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Culurciello%2C%20Eugenio%20Etienne-Cummings%2C%20Ralph%20and%20Kwabena%20A%20Boahen%2C%20%E2%80%9CA%20biomorphic%20digital%20image%20sensor%2C%E2%80%9D%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Culurciello%2C%20Eugenio%20Etienne-Cummings%2C%20Ralph%20and%20Kwabena%20A%20Boahen%2C%20%E2%80%9CA%20biomorphic%20digital%20image%20sensor%2C%E2%80%9D%202003"
        },
        {
            "id": "16",
            "entry": "[16] Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov, \u201cVariational dropout sparsifies deep neural networks,\u201d arXiv preprint arXiv:1701.05369, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.05369"
        },
        {
            "id": "17",
            "entry": "[17] Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz\u00e1r, \u201cFaster gaze prediction with dense networks and fisher pruning,\u201d arXiv preprint arXiv:1801.05787, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05787"
        },
        {
            "id": "18",
            "entry": "[18] Song Han, Huizi Mao, and William J Dally, \u201cDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding,\u201d arXiv preprint arXiv:1510.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "19",
            "entry": "[19] Andries P Engelbrecht and Ian Cloete, \u201cA sensitivity analysis algorithm for pruning feedforward neural networks,\u201d in Neural Networks, 1996., IEEE International Conference on. IEEE, 1996, vol. 2, pp. 1274\u20131278.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engelbrecht%2C%20Andries%20P.%20Cloete%2C%20Ian%20A%20sensitivity%20analysis%20algorithm%20for%20pruning%20feedforward%20neural%20networks%2C%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engelbrecht%2C%20Andries%20P.%20Cloete%2C%20Ian%20A%20sensitivity%20analysis%20algorithm%20for%20pruning%20feedforward%20neural%20networks%2C%201996"
        },
        {
            "id": "20",
            "entry": "[20] Iveta Mr\u00e1zov\u00e1 and Zuzana Reitermanov\u00e1, \u201cA new sensitivity-based pruning technique for feed-forward neural networks that improves generalization,\u201d in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 2143\u20132150.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mr%C3%A1zov%C3%A1%2C%20Iveta%20Reitermanov%C3%A1%2C%20Zuzana%20A%20new%20sensitivity-based%20pruning%20technique%20for%20feed-forward%20neural%20networks%20that%20improves%20generalization%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mr%C3%A1zov%C3%A1%2C%20Iveta%20Reitermanov%C3%A1%2C%20Zuzana%20A%20new%20sensitivity-based%20pruning%20technique%20for%20feed-forward%20neural%20networks%20that%20improves%20generalization%2C%202011"
        },
        {
            "id": "21",
            "entry": "[21] Iveta Mrazova and Marek Kukacka, \u201cCan deep neural networks discover meaningful pattern features?,\u201d Procedia Computer Science, vol. 12, pp. 194\u2013199, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mrazova%2C%20Iveta%20Kukacka%2C%20Marek%20Can%20deep%20neural%20networks%20discover%20meaningful%20pattern%20features%3F%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mrazova%2C%20Iveta%20Kukacka%2C%20Marek%20Can%20deep%20neural%20networks%20discover%20meaningful%20pattern%20features%3F%2C%202012"
        },
        {
            "id": "22",
            "entry": "[22] Deniz Yuret, \u201cKnet: beginning deep learning with 100 lines of julia,\u201d in Machine Learning Systems Workshop at NIPS 2016, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuret%2C%20Deniz%20Knet%3A%20beginning%20deep%20learning%20with%20100%20lines%20of%20julia%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuret%2C%20Deniz%20Knet%3A%20beginning%20deep%20learning%20with%20100%20lines%20of%20julia%2C%202016"
        },
        {
            "id": "23",
            "entry": "[23] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278 \u2013 2324, Nov. 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%2C%201998-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%2C%201998-11"
        },
        {
            "id": "24",
            "entry": "[24] Yiwen Guo, Anbang Yao, and Yurong Chen, \u201cDynamic network surgery for efficient dnns,\u201d in Advances In Neural Information Processing Systems, 2016, pp. 1379\u20131387.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Yiwen%20Yao%2C%20Anbang%20Chen%2C%20Yurong%20Dynamic%20network%20surgery%20for%20efficient%20dnns%2C%202016"
        },
        {
            "id": "25",
            "entry": "[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei, \u201cImagenet large scale visual recognition challenge,\u201d International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, Dec. 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%2C%202015-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20Imagenet%20large%20scale%20visual%20recognition%20challenge%2C%202015-12"
        }
    ]
}
