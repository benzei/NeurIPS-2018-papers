{
    "filename": "8123-robot-learning-in-homes-improving-generalization-and-reducing-dataset-bias.pdf",
    "metadata": {
        "title": "Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias",
        "author": "Abhinav Gupta, Adithyavairavan Murali, Dhiraj Prakashchand Gandhi, Lerrel Pinto",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8123-robot-learning-in-homes-improving-generalization-and-reducing-dataset-bias.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Data-driven approaches to solving robotic tasks have gained a lot of traction in recent years. However, most existing policies are trained on large-scale datasets collected in curated lab settings. If we aim to deploy these models in unstructured visual environments like people\u2019s homes, they will be unable to cope with the mismatch in data distribution. In such light, we present the first systematic effort in collecting a large dataset for robotic grasping in homes. First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3K USD. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28K grasps collected in several houses under an array of different environmental conditions. We evaluate our models by physically executing grasps on a collection of novel objects in multiple unseen homes. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise. We hope this effort inspires the robotics community to look outside the lab and embrace learning based approaches to handle inaccurate cheap robots."
    },
    "keywords": [
        {
            "term": "mobile manipulator",
            "url": "https://en.wikipedia.org/wiki/mobile_manipulator"
        },
        {
            "term": "Expectation Maximization",
            "url": "https://en.wikipedia.org/wiki/Expectation_Maximization"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "degrees of freedom",
            "url": "https://en.wikipedia.org/wiki/degrees_of_freedom"
        },
        {
            "term": "low cost",
            "url": "https://en.wikipedia.org/wiki/low_cost"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        }
    ],
    "highlights": [
        "Powered by the availability of cheaper robots, robust simulators and greater processing speeds, the last decade has witnessed the rise of data-driven approaches in robotics",
        "We demonstrate that collecting data in diverse households is crucial for our learned models to generalize to unseen home environments",
        "We first assemble a mobile manipulator which costs under 3K USD and collect a dataset of about 28K grasps in six homes under varying environmental conditions",
        "Collecting data with cheap inaccurate robots introduces the challenge of noisy labels and we present an architectural framework which factors out the noise in the data",
        "We demonstrate that it is crucial to train models with data collected in households if the goal is to eventually test them in homes",
        "The model trained with our home dataset showed a 43.7% improvement over a model trained with data collected in the lab"
    ],
    "key_statements": [
        "Powered by the availability of cheaper robots, robust simulators and greater processing speeds, the last decade has witnessed the rise of data-driven approaches in robotics",
        "Towards this goal: (a) we assemble a robot which costs less than 3K USD; (b) we use this robot to collect data inside 6 different homes for training and 3 homes for testing; (c) we present an approach that models and factors the noise in labeled data; (d) we demonstrate how data collected from these diverse home environment leads to superior performance and requires little-to-no domain adaptation",
        "We demonstrate that collecting data in diverse households is crucial for our learned models to generalize to unseen home environments",
        "We show that modelling the error of low cost robots in our Robust-Grasp architecture significantly improves grasping performance",
        "The motion planning pipeline is carefully designed since our under-constrained robot only has 5 degrees of freedom",
        "Real Low Cost Arm (Real-Low Cost Arm): We evaluated the physical grasping performance of our learned models on the low cost arm in this setting",
        "We used two datasets for the baseline: grasp data collected by [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] (Lab-Baxter) as well as data collected with our low cost arms in a single environment (Lab-Low Cost Arm)",
        "Our Robust-Grasp model trained on Home-Low Cost Arm achieves 77.5% grasping accuracy",
        "This is a significant improvement over the 56.25% grasping accuracy of the Patch-Grasp baseline trained on the same dataset",
        "We first assemble a mobile manipulator which costs under 3K USD and collect a dataset of about 28K grasps in six homes under varying environmental conditions",
        "Collecting data with cheap inaccurate robots introduces the challenge of noisy labels and we present an architectural framework which factors out the noise in the data",
        "We demonstrate that it is crucial to train models with data collected in households if the goal is to eventually test them in homes",
        "The model trained with our home dataset showed a 43.7% improvement over a model trained with data collected in the lab",
        "We demonstrate that our model improves grasp performance in curated environments like the lab"
    ],
    "summary": [
        "Powered by the availability of cheaper robots, robust simulators and greater processing speeds, the last decade has witnessed the rise of data-driven approaches in robotics.",
        "We present our method for learning a robotic grasping model given low-cost data.",
        "Learnt Parameters which infers grasp angles based on the image patch of the object b) the Noise Modelling Network",
        "We only train GPN using the noisy patch which allows it to learn a good initialization for grasp prediction and in turn provide better gradients to NMN.",
        "We demonstrate that collecting data in diverse households is crucial for our learned models to generalize to unseen home environments.",
        "The goal of this experiment is to show that training models with data collected in homes improves task performance in curated environments like the lab.",
        "Since we want to evaluate the performance of both the home robot dataset (Home-LCA) and the Robust-Grasp architecture, we used baselines for both the data and model.",
        "We used two datasets for the baseline: grasp data collected by [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] (Lab-Baxter) as well as data collected with our low cost arms in a single environment (Lab-LCA).",
        "As shown in Table 1, models trained on only lab data overfit to their respective environments and do not generalize to the more challenging Home-LCA environment, corresponding to a lower binary classification accuracy score.",
        "To illustrate the importance of collecting a large Home-LCA dataset, we compare to a common domain adaptation baseline: fine-tuning the model learned on Lab-LCA with 5K home grasps (\u2018Fine-tuned\u2019 in Table 1).",
        "Since it is difficult to collect noise-free data on our home robots, we use Lab-Baxter for benchmarking.",
        "The improvements of the Robust-Grasp model is demonstrated with the binary classification metric in Table 1, where it outperforms the Patch-Grasp by about 4% on the Lab-Baxter and Home-LCA datasets.",
        "We show how low-cost data in a variety of homes can be used to train grasping models.",
        "We present the first effort in collecting large scale robot data inside diverse environments like people\u2019s homes.",
        "Collecting data with cheap inaccurate robots introduces the challenge of noisy labels and we present an architectural framework which factors out the noise in the data.",
        "We physically tested them by grasping a set of 20 novel objects in lab and in three unseen home environments from Airbnb.",
        "Our model was able to successfully disentangle the structured noise in the data and improved performance by about 10%"
    ],
    "headline": "We present the first systematic effort in collecting a large dataset for robotic grasping in homes",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. 2017. URL https://arxiv.org/abs/1703.06907.",
            "url": "https://arxiv.org/abs/1703.06907",
            "arxiv_url": "https://arxiv.org/pdf/1703.06907"
        },
        {
            "id": "2",
            "entry": "[2] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. arXiv preprint arXiv:1710.06537, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06537"
        },
        {
            "id": "3",
            "entry": "[3] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. Robotics Science and Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinto%2C%20Lerrel%20Andrychowicz%2C%20Marcin%20Welinder%2C%20Peter%20Zaremba%2C%20Wojciech%20Asymmetric%20actor%20critic%20for%20image-based%20robot%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinto%2C%20Lerrel%20Andrychowicz%2C%20Marcin%20Welinder%2C%20Peter%20Zaremba%2C%20Wojciech%20Asymmetric%20actor%20critic%20for%20image-based%20robot%20learning%202018"
        },
        {
            "id": "4",
            "entry": "[4] Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. CoRR, abs/1509.06825, 2015. URL http://arxiv.org/abs/1509.06825.",
            "url": "http://arxiv.org/abs/1509.06825",
            "arxiv_url": "https://arxiv.org/pdf/1509.06825"
        },
        {
            "id": "5",
            "entry": "[5] Adithyavairavan Murali, Lerrel Pinto, Dhiraj Gandhi, and Abhinav Gupta. CASSL: Curriculum accelerated self-supervised learning. International Conference on Robotics and Automation, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murali%2C%20Adithyavairavan%20Pinto%2C%20Lerrel%20Gandhi%2C%20Dhiraj%20Gupta%2C%20Abhinav%20CASSL%3A%20Curriculum%20accelerated%20self-supervised%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murali%2C%20Adithyavairavan%20Pinto%2C%20Lerrel%20Gandhi%2C%20Dhiraj%20Gupta%2C%20Abhinav%20CASSL%3A%20Curriculum%20accelerated%20self-supervised%20learning%202018"
        },
        {
            "id": "6",
            "entry": "[6] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20Sergey%20Finn%2C%20Chelsea%20Darrell%2C%20Trevor%20Abbeel%2C%20Pieter%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "7",
            "entry": "[7] Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. CoRR, abs/1603.02199, 2016. URL http://arxiv.org/abs/1603.02199.",
            "url": "http://arxiv.org/abs/1603.02199",
            "arxiv_url": "https://arxiv.org/pdf/1603.02199"
        },
        {
            "id": "8",
            "entry": "[8] Pulkit Agarwal, Ashwin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. 2016. URL http://arxiv.org/abs/1606.07419.",
            "url": "http://arxiv.org/abs/1606.07419",
            "arxiv_url": "https://arxiv.org/pdf/1606.07419"
        },
        {
            "id": "9",
            "entry": "[9] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In Advances in neural information processing systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Goodfellow%2C%20Ian%20Levine%2C%20Sergey%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Goodfellow%2C%20Ian%20Levine%2C%20Sergey%20Unsupervised%20learning%20for%20physical%20interaction%20through%20video%20prediction%202016"
        },
        {
            "id": "10",
            "entry": "[10] Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Combining self-supervised learning and imitation for vision-based rope manipulation. International Conference on Robotics and Automation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Ashvin%20Chen%2C%20Dian%20Agrawal%2C%20Pulkit%20Isola%2C%20Phillip%20Combining%20self-supervised%20learning%20and%20imitation%20for%20vision-based%20rope%20manipulation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Ashvin%20Chen%2C%20Dian%20Agrawal%2C%20Pulkit%20Isola%2C%20Phillip%20Combining%20self-supervised%20learning%20and%20imitation%20for%20vision-based%20rope%20manipulation%202017"
        },
        {
            "id": "11",
            "entry": "[11] Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Gupta. Multiple interactions made easy (mime): Large scale demonstrations data for imitation. arXiv preprint arXiv:1810.07121, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.07121"
        },
        {
            "id": "12",
            "entry": "[12] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Chen%20Shrivastava%2C%20Abhinav%20Singh%2C%20Saurabh%20Gupta%2C%20Abhinav%20Revisiting%20unreasonable%20effectiveness%20of%20data%20in%20deep%20learning%20era%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Chen%20Shrivastava%2C%20Abhinav%20Singh%2C%20Saurabh%20Gupta%2C%20Abhinav%20Revisiting%20unreasonable%20effectiveness%20of%20data%20in%20deep%20learning%20era%202017"
        },
        {
            "id": "13",
            "entry": "[13] George Dahl, Dong Yu, Li Deng, and Alex Acero. Context dependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dahl%2C%20George%20Yu%2C%20Dong%20Deng%2C%20Li%20Acero%2C%20Alex%20Context%20dependent%20pre-trained%20deep%20neural%20networks%20for%20large%20vocabulary%20speech%20recognition%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dahl%2C%20George%20Yu%2C%20Dong%20Deng%2C%20Li%20Acero%2C%20Alex%20Context%20dependent%20pre-trained%20deep%20neural%20networks%20for%20large%20vocabulary%20speech%20recognition%202010"
        },
        {
            "id": "14",
            "entry": "[14] Dobot Magician. https://www.dobot.cc/dobot-magician/specification.html.",
            "url": "https://www.dobot.cc/dobot-magician/specification.html"
        },
        {
            "id": "15",
            "entry": "[15] Kobuki Base. http://kobuki.yujinrobot.com/about2/.",
            "url": "http://kobuki.yujinrobot.com/about2/"
        },
        {
            "id": "16",
            "entry": "[16] Makerblock Gripper. http://store.makeblock.com/robot-gripper.",
            "url": "http://store.makeblock.com/robot-gripper"
        },
        {
            "id": "17",
            "entry": "[17] Leonid Keselman, John Iselin Woodfill, Anders Grunnet-Jepsen, and Achintya Bhowmik. Intel realsense stereoscopic depth cameras. arXiv preprint arXiv:1705.05548, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.05548"
        },
        {
            "id": "18",
            "entry": "[18] On Board Computer. https://www.acer.com/ac/en/US/content/model/NX.GTSAA.005.",
            "url": "https://www.acer.com/ac/en/US/content/model/NX.GTSAA.005"
        },
        {
            "id": "19",
            "entry": "[19] Ishan Misra, Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. Seeing through the human reporting bias:visual classifiers from noisy human-centric labels. CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Ishan%20Zitnick%2C%20Lawrence%20Mitchell%2C%20Margaret%20Girshick%2C%20Ross%20Seeing%20through%20the%20human%20reporting%20bias%3Avisual%20classifiers%20from%20noisy%20human-centric%20labels%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Ishan%20Zitnick%2C%20Lawrence%20Mitchell%2C%20Margaret%20Girshick%2C%20Ross%20Seeing%20through%20the%20human%20reporting%20bias%3Avisual%20classifiers%20from%20noisy%20human-centric%20labels%202016"
        },
        {
            "id": "20",
            "entry": "[20] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977"
        },
        {
            "id": "21",
            "entry": "[21] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "22",
            "entry": "[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "23",
            "entry": "[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "24",
            "entry": "[24] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08242"
        },
        {
            "id": "25",
            "entry": "[25] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. RSS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahler%2C%20Jeffrey%20Liang%2C%20Jacky%20Niyaz%2C%20Sherdil%20Laskey%2C%20Michael%20Dex-net%202.0%3A%20Deep%20learning%20to%20plan%20robust%20grasps%20with%20synthetic%20point%20clouds%20and%20analytic%20grasp%20metrics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahler%2C%20Jeffrey%20Liang%2C%20Jacky%20Niyaz%2C%20Sherdil%20Laskey%2C%20Michael%20Dex-net%202.0%3A%20Deep%20learning%20to%20plan%20robust%20grasps%20with%20synthetic%20point%20clouds%20and%20analytic%20grasp%20metrics%202017"
        },
        {
            "id": "26",
            "entry": "[26] Ian Lenz, Honglak Lee, and Ashutosh Saxena. Deep learning for detecting robotic grasps. IJRR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lenz%2C%20Ian%20Lee%2C%20Honglak%20Saxena%2C%20Ashutosh%20Deep%20learning%20for%20detecting%20robotic%20grasps%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lenz%2C%20Ian%20Lee%2C%20Honglak%20Saxena%2C%20Ashutosh%20Deep%20learning%20for%20detecting%20robotic%20grasps%202015"
        },
        {
            "id": "27",
            "entry": "[27] Lerrel Pinto and Abhinav Gupta. Learning to push by grasping: Using multiple tasks for effective learning. International Conference on Robotics and Automation, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Learning%20to%20push%20by%20grasping%3A%20Using%20multiple%20tasks%20for%20effective%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pinto%2C%20Lerrel%20Gupta%2C%20Abhinav%20Learning%20to%20push%20by%20grasping%3A%20Using%20multiple%20tasks%20for%20effective%20learning%202017"
        },
        {
            "id": "28",
            "entry": "[28] Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, and Sergey Levine. Collective robot reinforcement learning with distributed asynchronous guided policy search. IROS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yahya%2C%20Ali%20Li%2C%20Adrian%20Kalakrishnan%2C%20Mrinal%20Chebotar%2C%20Yevgen%20Collective%20robot%20reinforcement%20learning%20with%20distributed%20asynchronous%20guided%20policy%20search%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yahya%2C%20Ali%20Li%2C%20Adrian%20Kalakrishnan%2C%20Mrinal%20Chebotar%2C%20Yevgen%20Collective%20robot%20reinforcement%20learning%20with%20distributed%20asynchronous%20guided%20policy%20search%202017"
        },
        {
            "id": "29",
            "entry": "[29] Zackory Erickson, Sonia Chernova, and Charles C Kemp. Semi-supervised haptic material recognition for robots using generative adversarial networks. Conference on Robot Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erickson%2C%20Zackory%20Chernova%2C%20Sonia%20Kemp%2C%20Charles%20C.%20Semi-supervised%20haptic%20material%20recognition%20for%20robots%20using%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erickson%2C%20Zackory%20Chernova%2C%20Sonia%20Kemp%2C%20Charles%20C.%20Semi-supervised%20haptic%20material%20recognition%20for%20robots%20using%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "30",
            "entry": "[30] Adithyavairavan Murali, Yin Li, Dhiraj Gandhi, and Abhinav Gupta. Learning to grasp without seeing. International Symposium on Experimental Robotics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murali%2C%20Adithyavairavan%20Li%2C%20Yin%20Gandhi%2C%20Dhiraj%20Gupta%2C%20Abhinav%20Learning%20to%20grasp%20without%20seeing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murali%2C%20Adithyavairavan%20Li%2C%20Yin%20Gandhi%2C%20Dhiraj%20Gupta%2C%20Abhinav%20Learning%20to%20grasp%20without%20seeing%202018"
        },
        {
            "id": "31",
            "entry": "[31] Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wenzhen Yuan, Justin Lin, Edward Adelson, and Sergey Levine. The feeling of success: Does touch sensing help predict grasp outcomes? Conference on Robot Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calandra%2C%20Roberto%20Owens%2C%20Andrew%20Upadhyaya%2C%20Manu%20Yuan%2C%20Wenzhen%20The%20feeling%20of%20success%3A%20Does%20touch%20sensing%20help%20predict%20grasp%20outcomes%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calandra%2C%20Roberto%20Owens%2C%20Andrew%20Upadhyaya%2C%20Manu%20Yuan%2C%20Wenzhen%20The%20feeling%20of%20success%3A%20Does%20touch%20sensing%20help%20predict%20grasp%20outcomes%3F%202017"
        },
        {
            "id": "32",
            "entry": "[32] Manuela Veloso, Joydeep Biswas, Brian Coltin, Stephanie Rosenthal, Tom Kollar, Cetin Mericli, Mehdi Samadi, Susana Brandao, and Rodrigo Ventura. Cobots: Collaborative robots servicing multi-floor buildings. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5446\u20135447. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veloso%2C%20Manuela%20Biswas%2C%20Joydeep%20Coltin%2C%20Brian%20Rosenthal%2C%20Stephanie%20Cobots%3A%20Collaborative%20robots%20servicing%20multi-floor%20buildings%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veloso%2C%20Manuela%20Biswas%2C%20Joydeep%20Coltin%2C%20Brian%20Rosenthal%2C%20Stephanie%20Cobots%3A%20Collaborative%20robots%20servicing%20multi-floor%20buildings%202012"
        },
        {
            "id": "33",
            "entry": "[33] Nick Hawes, Christopher Burbridge, Ferdian Jovan, Lars Kunze, Bruno Lacerda, Lenka Mudrova, Jay Young, Jeremy Wyatt, Denise Hebesberger, Tobias Kortner, et al. The strands project: Long-term autonomy in everyday environments. IEEE Robotics & Automation Magazine, 24 (3):146\u2013156, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hawes%2C%20Nick%20Burbridge%2C%20Christopher%20Jovan%2C%20Ferdian%20Kunze%2C%20Lars%20The%20strands%20project%3A%20Long-term%20autonomy%20in%20everyday%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hawes%2C%20Nick%20Burbridge%2C%20Christopher%20Jovan%2C%20Ferdian%20Kunze%2C%20Lars%20The%20strands%20project%3A%20Long-term%20autonomy%20in%20everyday%20environments%202017"
        },
        {
            "id": "34",
            "entry": "[34] Antonio Bicchi and Vijay Kumar. Robotic grasping and contact: a review. In International Conference on Robotics and Automation, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bicchi%2C%20Antonio%20Kumar%2C%20Vijay%20Robotic%20grasping%20and%20contact%3A%20a%20review%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bicchi%2C%20Antonio%20Kumar%2C%20Vijay%20Robotic%20grasping%20and%20contact%3A%20a%20review%202000"
        },
        {
            "id": "35",
            "entry": "[35] Jeannette Bohg, Antonio Morales, Tamim Asfour, and Danica Kragic. Data-driven grasp synthesis\u2014a survey. IEEE Transactions on Robotics, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bohg%2C%20Jeannette%20Morales%2C%20Antonio%20Asfour%2C%20Tamim%20Kragic%2C%20Danica%20Data-driven%20grasp%20synthesis%E2%80%94a%20survey%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bohg%2C%20Jeannette%20Morales%2C%20Antonio%20Asfour%2C%20Tamim%20Kragic%2C%20Danica%20Data-driven%20grasp%20synthesis%E2%80%94a%20survey%202014"
        },
        {
            "id": "36",
            "entry": "[36] Van-Duc Nguyen. Constructing force-closure grasps. The International Journal of Robotics Research, 7(3):3\u201316, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Van-Duc%20Constructing%20force-closure%20grasps%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Van-Duc%20Constructing%20force-closure%20grasps%201988"
        },
        {
            "id": "37",
            "entry": "[37] Jeffrey Mahler, Florian T Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kr\u00f6ger, James Kuffner, and Ken Goldberg. Dex-net 1.0: A cloudbased network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 1957\u20131964. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahler%2C%20Jeffrey%20Pokorny%2C%20Florian%20T.%20Hou%2C%20Brian%20Roderick%2C%20Melrose%20Dex-net%201.0%3A%20A%20cloudbased%20network%20of%203d%20objects%20for%20robust%20grasp%20planning%20using%20a%20multi-armed%20bandit%20model%20with%20correlated%20rewards%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahler%2C%20Jeffrey%20Pokorny%2C%20Florian%20T.%20Hou%2C%20Brian%20Roderick%2C%20Melrose%20Dex-net%201.0%3A%20A%20cloudbased%20network%20of%203d%20objects%20for%20robust%20grasp%20planning%20using%20a%20multi-armed%20bandit%20model%20with%20correlated%20rewards%202016"
        },
        {
            "id": "38",
            "entry": "[38] Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 4243\u20134250. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousmalis%2C%20Konstantinos%20Irpan%2C%20Alex%20Wohlhart%2C%20Paul%20Bai%2C%20Yunfei%20Using%20simulation%20and%20domain%20adaptation%20to%20improve%20efficiency%20of%20deep%20robotic%20grasping%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousmalis%2C%20Konstantinos%20Irpan%2C%20Alex%20Wohlhart%2C%20Paul%20Bai%2C%20Yunfei%20Using%20simulation%20and%20domain%20adaptation%20to%20improve%20efficiency%20of%20deep%20robotic%20grasping%202018"
        },
        {
            "id": "39",
            "entry": "[39] Kuan Fang, Yunfei Bai, Stefan Hinterstoisser, Silvio Savarese, and Mrinal Kalakrishnan. Multitask domain adaptation for deep learning of instance grasping from simulation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 3516\u20133523. IEEE, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fang%2C%20Kuan%20Bai%2C%20Yunfei%20Hinterstoisser%2C%20Stefan%20Savarese%2C%20Silvio%20Multitask%20domain%20adaptation%20for%20deep%20learning%20of%20instance%20grasping%20from%20simulation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fang%2C%20Kuan%20Bai%2C%20Yunfei%20Hinterstoisser%2C%20Stefan%20Savarese%2C%20Silvio%20Multitask%20domain%20adaptation%20for%20deep%20learning%20of%20instance%20grasping%20from%20simulation%202018"
        },
        {
            "id": "40",
            "entry": "[40] Marc Peter Deisenroth, Carl Edward Rasmussen, and Dieter Fox. Learning to control a low-cost manipulator using data-efficient reinforcement learning. RSS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20Marc%20Peter%20Rasmussen%2C%20Carl%20Edward%20Fox%2C%20Dieter%20Learning%20to%20control%20a%20low-cost%20manipulator%20using%20data-efficient%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20Marc%20Peter%20Rasmussen%2C%20Carl%20Edward%20Fox%2C%20Dieter%20Learning%20to%20control%20a%20low-cost%20manipulator%20using%20data-efficient%20reinforcement%20learning%202011"
        },
        {
            "id": "41",
            "entry": "[41] David F Nettleton, Albert Orriols-Puig, and Albert Fornells. A study of the effect of different types of noise on the precision of supervised learning techniques. Artificial intelligence review, 33(4):275\u2013306, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nettleton%2C%20David%20F.%20Orriols-Puig%2C%20Albert%20Fornells%2C%20Albert%20A%20study%20of%20the%20effect%20of%20different%20types%20of%20noise%20on%20the%20precision%20of%20supervised%20learning%20techniques%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nettleton%2C%20David%20F.%20Orriols-Puig%2C%20Albert%20Fornells%2C%20Albert%20A%20study%20of%20the%20effect%20of%20different%20types%20of%20noise%20on%20the%20precision%20of%20supervised%20learning%20techniques%202010"
        },
        {
            "id": "42",
            "entry": "[42] Beno\u00eet Fr\u00e9nay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE transactions on neural networks and learning systems, 25(5):845\u2013869, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fr%C3%A9nay%2C%20Beno%C3%AEt%20Verleysen%2C%20Michel%20Classification%20in%20the%20presence%20of%20label%20noise%3A%20a%20survey%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fr%C3%A9nay%2C%20Beno%C3%AEt%20Verleysen%2C%20Michel%20Classification%20in%20the%20presence%20of%20label%20noise%3A%20a%20survey%202014"
        },
        {
            "id": "43",
            "entry": "[43] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2691\u20132699, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Tong%20Xia%2C%20Tian%20Yang%2C%20Yi%20Huang%2C%20Chang%20Learning%20from%20massive%20noisy%20labeled%20data%20for%20image%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20Tong%20Xia%2C%20Tian%20Yang%2C%20Yi%20Huang%2C%20Chang%20Learning%20from%20massive%20noisy%20labeled%20data%20for%20image%20classification%202015"
        }
    ]
}
