{
    "filename": "8227-learning-to-share-and-hide-intentions-using-information-regularization.pdf",
    "metadata": {
        "title": "Learning to Share and Hide Intentions using Information Regularization",
        "author": "Daniel Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, David J. Schwab",
        "date": 2005,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8227-learning-to-share-and-hide-intentions-using-information-regularization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning to cooperate with friends and compete with foes is a key component of multi-agent reinforcement learning. Typically to do so, one requires access to either a model of or interaction with the other agent(s). Here we show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction. Our approach is to encourage an agent to reveal or hide their intentions using an information-theoretic regularizer. We consider both the mutual information between goal and action given state, as well as the mutual information between goal and state. We show how to stochastically optimize these regularizers in a way that is easy to integrate with policy gradient reinforcement learning. Finally, we demonstrate that cooperative (competitive) policies learned with our approach lead to more (less) reward for a second agent in two simple asymmetric information games."
    },
    "keywords": [
        {
            "term": "inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/inverse_reinforcement_learning"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        }
    ],
    "highlights": [
        "In order to effectively interact with others, an intelligent agent must understand the intentions of others",
        "We discuss regularization via optimizing the mutual information between goal and action, Iaction \u2261 I(A; G | S), where G is the goal for the episode, A is the chosen action, and S is the state of the agent",
        "We show that training Alice with information regularization effectively encourages both goal signaling and hiding, depending on the sign of the coefficient \u03b2",
        "We demonstrate that our approach allows agents to learn effective cooperative and competitive strategies in asymmetric information games without an explicit model or interaction with the other agent(s)",
        "Such an approach could be particularly useful in settings where interactive training with other agents could be dangerous or costly, such as the training of expensive robots or the deployment of financial trading strategies",
        "For optimizing Istate with continuous states, one can no longer count states exactly, so these counts could be replaced with, for example, a pseudo-count based on an approximate density model. [<a class=\"ref-link\" id=\"cBellemare_et+al_2016_a\" href=\"#rBellemare_et+al_2016_a\">Bellemare et al, 2016</a>, <a class=\"ref-link\" id=\"cOstrovski_et+al_2017_a\" href=\"#rOstrovski_et+al_2017_a\">Ostrovski et al, 2017</a>] Of course, for both types of information regularization, continuous states or actions necessitate using function approximation for the policy representation"
    ],
    "key_statements": [
        "In order to effectively interact with others, an intelligent agent must understand the intentions of others",
        "We discuss regularization via optimizing the mutual information between goal and action, Iaction \u2261 I(A; G | S), where G is the goal for the episode, A is the chosen action, and S is the state of the agent",
        "We show that training Alice with information regularization effectively encourages both goal signaling and hiding, depending on the sign of the coefficient \u03b2",
        "Alice\u2019s policy is a random mixture of the competitive and cooperative strategies, the details of which are determined by initialization and the randomness of training trajectories",
        "We demonstrate that our approach allows agents to learn effective cooperative and competitive strategies in asymmetric information games without an explicit model or interaction with the other agent(s)",
        "Such an approach could be particularly useful in settings where interactive training with other agents could be dangerous or costly, such as the training of expensive robots or the deployment of financial trading strategies",
        "For optimizing Istate with continuous states, one can no longer count states exactly, so these counts could be replaced with, for example, a pseudo-count based on an approximate density model. [<a class=\"ref-link\" id=\"cBellemare_et+al_2016_a\" href=\"#rBellemare_et+al_2016_a\">Bellemare et al, 2016</a>, <a class=\"ref-link\" id=\"cOstrovski_et+al_2017_a\" href=\"#rOstrovski_et+al_2017_a\">Ostrovski et al, 2017</a>] Of course, for both types of information regularization, continuous states or actions necessitate using function approximation for the policy representation"
    ],
    "summary": [
        "In order to effectively interact with others, an intelligent agent must understand the intentions of others.",
        "Using state information instead only assumes that Bob can observe Alice\u2019s states, it does so at the cost of requiring Alice to count goal-dependent state frequencies under the current policy.",
        "We discuss regularization via optimizing the mutual information between goal and action, Iaction \u2261 I(A; G | S), where G is the goal for the episode, A is the chosen action, and S is the state of the agent.",
        "The second term in equation 6 encourages the agent to alter the policy to share or hide information in the present state.",
        "The first term, on the other hand, encourages modifications which lead the agent to states in the future which result in reward and the sharing or hiding of information.",
        "We show that training Alice with information regularization effectively encourages both goal signaling and hiding, depending on the sign of the coefficient \u03b2.",
        "In the cooperative case with action information regularization, Alice wants to maximize KL[\u03c0g | \u03c00] and she wants her goal-dependent policies to differ as much as possible.",
        "In the competitive case with action information regularization, Alice instead wants to minimize KL[\u03c0g | \u03c00] and she wants her goal-dependent policies to match as much as possible.",
        "Alice\u2019s policy is a random mixture of the competitive and cooperative strategies, the details of which are determined by initialization and the randomness of training trajectories.",
        "To demonstrate that Alice\u2019s goal revealing and hiding behaviors are useful for cooperation and competition, respectively, we trained a second agent, Bob, who does not have access to the goal and instead must infer it from observing Alice.",
        "Details are available in the supplemental information, but in brief, Bob processes Alice\u2019s state-action trajectories with an RNN to form a belief about the goal, which feeds into his policy, all of which is trained end-to-end via REINFORCE.",
        "Our approach successfully encourages Alice us to forgo rewards during solo training in order to later compete more effectively in an interactive setting.",
        "We demonstrate that our approach allows agents to learn effective cooperative and competitive strategies in asymmetric information games without an explicit model or interaction with the other agent(s).",
        "[<a class=\"ref-link\" id=\"cBellemare_et+al_2016_a\" href=\"#rBellemare_et+al_2016_a\">Bellemare et al, 2016</a>, <a class=\"ref-link\" id=\"cOstrovski_et+al_2017_a\" href=\"#rOstrovski_et+al_2017_a\">Ostrovski et al, 2017</a>] Of course, for both types of information regularization, continuous states or actions necessitate using function approximation for the policy representation."
    ],
    "headline": "We show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI\u201916, pages 265\u2013283, Berkeley, CA, USA, 2016. USENIX Association. ISBN 978-1-931971-33-1. URL http://dl.acm.org/citation.cfm?id=3026877.3026899.",
            "url": "http://dl.acm.org/citation.cfm?id=3026877.3026899",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "Baker_et+al_2009_a",
            "entry": "Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. Action understanding as inverse planning. Cognition, 113(3):329\u2013349, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Chris%20L.%20Saxe%2C%20Rebecca%20Tenenbaum%2C%20Joshua%20B.%20Action%20understanding%20as%20inverse%20planning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Chris%20L.%20Saxe%2C%20Rebecca%20Tenenbaum%2C%20Joshua%20B.%20Action%20understanding%20as%20inverse%20planning%202009"
        },
        {
            "id": "Bellemare_et+al_2016_a",
            "entry": "M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying Count-Based Exploration and Intrinsic Motivation. ArXiv e-prints, June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Srinivasan%2C%20S.%20Ostrovski%2C%20G.%20Schaul%2C%20T.%20Unifying%20Count-Based%20Exploration%20and%20Intrinsic%20Motivation.%20ArXiv%20e-prints%202016-06"
        },
        {
            "id": "Cho_et+al_2014_a",
            "entry": "K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. ArXiv e-prints, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20K.%20van%20Merrienboer%2C%20B.%20Gulcehre%2C%20C.%20Bahdanau%2C%20D.%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.%20ArXiv%20e-prints%202014"
        },
        {
            "id": "Dragan_et+al_2013_a",
            "entry": "Anca D. Dragan, Kenton C.T. Lee, and Siddhartha S. Srinivasa. Legibility and predictability of robot motion. International Conference on Human-Robot Interaction (HRI), pages 301\u2013308, 2013. URL http://dl.acm.org/citation.cfm?id=2447556.2447672.",
            "url": "http://dl.acm.org/citation.cfm?id=2447556.2447672",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dragan%2C%20Anca%20D.%20Lee%2C%20Kenton%20C.T.%20Srinivasa%2C%20Siddhartha%20S.%20Legibility%20and%20predictability%20of%20robot%20motion%202013"
        },
        {
            "id": "Eysenbach_et+al_2018_a",
            "entry": "B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is All You Need: Learning Skills without a Reward Function. ArXiv e-prints, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eysenbach%2C%20B.%20Gupta%2C%20A.%20Ibarz%2C%20J.%20Levine%2C%20S.%20Diversity%20is%20All%20You%20Need%3A%20Learning%20Skills%20without%20a%20Reward%20Function.%20ArXiv%20e-prints%202018-02"
        },
        {
            "id": "Hadfield-Menell_et+al_2016_a",
            "entry": "Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. In Advances in neural information processing systems, pages 3909\u20133917, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadfield-Menell%2C%20Dylan%20Russell%2C%20Stuart%20J.%20Abbeel%2C%20Pieter%20Dragan%2C%20Anca%20Cooperative%20inverse%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadfield-Menell%2C%20Dylan%20Russell%2C%20Stuart%20J.%20Abbeel%2C%20Pieter%20Dragan%2C%20Anca%20Cooperative%20inverse%20reinforcement%20learning%202016"
        },
        {
            "id": "Ho_et+al_2016_a",
            "entry": "Mark K Ho, Michael Littman, James MacGlashan, Fiery Cushman, and Joseph L Austerweil. Showing versus doing: Teaching by demonstration. In Advances In Neural Information Processing Systems, pages 3027\u20133035, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Mark%20K.%20Littman%2C%20Michael%20MacGlashan%2C%20James%20Cushman%2C%20Fiery%20Showing%20versus%20doing%3A%20Teaching%20by%20demonstration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Mark%20K.%20Littman%2C%20Michael%20MacGlashan%2C%20James%20Cushman%2C%20Fiery%20Showing%20versus%20doing%3A%20Teaching%20by%20demonstration%202016"
        },
        {
            "id": "Jaderberg_et+al_2016_a",
            "entry": "M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary Tasks, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Mnih%2C%20V.%20Czarnecki%2C%20W.M.%20Schaul%2C%20T.%20Reinforcement%20Learning%20with%20Unsupervised%20Auxiliary%20Tasks%202016"
        },
        {
            "id": "Kleiman-Weiner_et+al_2016_a",
            "entry": "Max Kleiman-Weiner, Mark K Ho, Joseph L Austerweil, Michael L Littman, and Joshua B Tenenbaum. Coordinate to cooperate or compete: abstract goals and joint intentions in social interaction. In Proceedings of the 38th Annual Conference of the Cognitive Science Society, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kleiman-Weiner%2C%20Max%20Ho%2C%20Mark%20K.%20Austerweil%2C%20Joseph%20L.%20Littman%2C%20Michael%20L.%20Coordinate%20to%20cooperate%20or%20compete%3A%20abstract%20goals%20and%20joint%20intentions%20in%20social%20interaction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kleiman-Weiner%2C%20Max%20Ho%2C%20Mark%20K.%20Austerweil%2C%20Joseph%20L.%20Littman%2C%20Michael%20L.%20Coordinate%20to%20cooperate%20or%20compete%3A%20abstract%20goals%20and%20joint%20intentions%20in%20social%20interaction%202016"
        },
        {
            "id": "Leibo_et+al_2017_a",
            "entry": "Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent reinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pages 464\u2013473. International Foundation for Autonomous Agents and Multiagent Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leibo%2C%20Joel%20Z.%20Zambaldi%2C%20Vinicius%20Lanctot%2C%20Marc%20Marecki%2C%20Janusz%20Multi-agent%20reinforcement%20learning%20in%20sequential%20social%20dilemmas%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leibo%2C%20Joel%20Z.%20Zambaldi%2C%20Vinicius%20Lanctot%2C%20Marc%20Marecki%2C%20Janusz%20Multi-agent%20reinforcement%20learning%20in%20sequential%20social%20dilemmas%202017"
        },
        {
            "id": "Littman_1994_a",
            "entry": "Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings 1994, pages 157\u2013163.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994"
        },
        {
            "id": "Littman_2001_a",
            "entry": "Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pages 322\u2013328, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Friend-or-foe%20q-learning%20in%20general-sum%20games%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Friend-or-foe%20q-learning%20in%20general-sum%20games%202001"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "V. Mnih, A. Puigdom\u00e8nech Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. ArXiv e-prints, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Badia%2C%20A.Puigdom%C3%A8nech%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning.%20ArXiv%20e-prints%202016"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, pages 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20J.%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Ostrovski_et+al_2017_a",
            "entry": "G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos. Count-Based Exploration with Neural Density Models. ArXiv e-prints, March 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ostrovski%2C%20G.%20Bellemare%2C%20M.G.%20van%20den%20Oord%2C%20A.%20Munos%2C%20R.%20Count-Based%20Exploration%20with%20Neural%20Density%20Models%202017-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ostrovski%2C%20G.%20Bellemare%2C%20M.G.%20van%20den%20Oord%2C%20A.%20Munos%2C%20R.%20Count-Based%20Exploration%20with%20Neural%20Density%20Models%202017-03"
        },
        {
            "id": "Rabinowitz_et+al_2018_a",
            "entry": "N. C. Rabinowitz, F. Perbet, H. F. Song, C. Zhang, S. M. A. Eslami, and M. Botvinick. Machine Theory of Mind. ArXiv e-prints, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rabinowitz%2C%20N.C.%20Perbet%2C%20F.%20Song%2C%20H.F.%20Zhang%2C%20C.%20Machine%20Theory%20of%20Mind.%20ArXiv%20e-prints%202018-02"
        },
        {
            "id": "Shafto_et+al_2014_a",
            "entry": "Patrick Shafto, Noah D Goodman, and Thomas L Griffiths. A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Cognitive psychology, 71:55\u201389, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shafto%2C%20Patrick%20Goodman%2C%20Noah%20D.%20Griffiths%2C%20Thomas%20L.%20A%20rational%20account%20of%20pedagogical%20reasoning%3A%20Teaching%20by%2C%20and%20learning%20from%2C%20examples%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shafto%2C%20Patrick%20Goodman%2C%20Noah%20D.%20Griffiths%2C%20Thomas%20L.%20A%20rational%20account%20of%20pedagogical%20reasoning%3A%20Teaching%20by%2C%20and%20learning%20from%2C%20examples%202014"
        },
        {
            "id": "Sutton_et+al_1999_a",
            "entry": "Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS\u201999, pages 1057\u20131063, Cambridge, MA, USA, 1999. MIT Press. URL http://dl.acm.org/citation.cfm?id=3009657.3009806.",
            "url": "http://dl.acm.org/citation.cfm?id=3009657.3009806",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20Singh%2C%20Satinder%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%201999"
        },
        {
            "id": "Tomasello_et+al_2005_a",
            "entry": "Michael Tomasello, Malinda Carpenter, Josep Call, Tanya Behne, and Henrike Moll. Understanding and sharing intentions: The origins of cultural cognition. Behavioral and brain sciences, 28(05): 675\u2013691, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomasello%2C%20Michael%20Carpenter%2C%20Malinda%20Call%2C%20Josep%20Behne%2C%20Tanya%20Understanding%20and%20sharing%20intentions%3A%20The%20origins%20of%20cultural%20cognition%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomasello%2C%20Michael%20Carpenter%2C%20Malinda%20Call%2C%20Josep%20Behne%2C%20Tanya%20Understanding%20and%20sharing%20intentions%3A%20The%20origins%20of%20cultural%20cognition%202005"
        },
        {
            "id": "Ullman_et+al_2009_a",
            "entry": "Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, and Joshua B Tenenbaum. Help or hinder: Bayesian models of social goal inference. In Advances in neural information processing systems, pages 1874\u20131882, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ullman%2C%20Tomer%20Baker%2C%20Chris%20Macindoe%2C%20Owen%20Evans%2C%20Owain%20Help%20or%20hinder%3A%20Bayesian%20models%20of%20social%20goal%20inference%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ullman%2C%20Tomer%20Baker%2C%20Chris%20Macindoe%2C%20Owen%20Evans%2C%20Owain%20Help%20or%20hinder%3A%20Bayesian%20models%20of%20social%20goal%20inference%202009"
        },
        {
            "id": "Teh_et+al_2017_a",
            "entry": "Y. Whye Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pascanu. Distral: Robust Multitask Reinforcement Learning. ArXiv e-prints, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Y.Whye%20Bapst%2C%20V.%20Czarnecki%2C%20W.M.%20Quan%2C%20J.%20Distral%3A%20Robust%20Multitask%20Reinforcement%20Learning.%20ArXiv%20e-prints%202017"
        }
    ]
}
