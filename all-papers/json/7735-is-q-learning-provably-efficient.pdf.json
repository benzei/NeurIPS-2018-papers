{
    "filename": "7735-is-q-learning-provably-efficient.pdf",
    "metadata": {
        "title": "Is Q-Learning Provably Efficient?",
        "author": "Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael I. Jordan",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7735-is-q-learning-provably-efficient.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]. The theoretical question of \u201cwhether model-free algorithms can be made sample efficient\u201d is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions."
    },
    "keywords": [
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "q learning",
            "url": "https://en.wikipedia.org/wiki/q_learning"
        },
        {
            "term": "value function",
            "url": "https://en.wikipedia.org/wiki/value_function"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "highlights": [
        "Reinforcement Learning (RL) is a control-theoretic problem in which an agent tries to maximize its cumulative rewards via interacting with an unknown environment through time [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "Oin(\u221aanHe3pSisAoTdi)c, Markov Decision Process where setting, Q-learning with upper-confidence bound exploration S and A are the numbers of states and actions, H is the number of steps per episode, and T is the total number of steps",
        "Model-free approaches dispense with the model and directly update the value function\u2014the expected reward starting from each state, or the policy\u2014the mapping from states to their subsequent actions",
        "To the best of our knowledge, the only existing theoretical result on model-free Reinforcement Learning that applies to the episodic setting is for delayed Q-learning; this algorithm is quite sample-inefficient compared to model-based approaches [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We show that Q-learning, when equipped with a upper-confidence bound exploration policy values and assign exploration bonuses, achieves tthoattalinrecgorreptoOrat(e\u221as Hes3tiSmAaTte)s",
        "We present a theorem that establishes an information-theoretic lower bound for episodic Markov Decision Process"
    ],
    "key_statements": [
        "Reinforcement Learning (RL) is a control-theoretic problem in which an agent tries to maximize its cumulative rewards via interacting with an unknown environment through time [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "Oin(\u221aanHe3pSisAoTdi)c, Markov Decision Process where setting, Q-learning with upper-confidence bound exploration S and A are the numbers of states and actions, H is the number of steps per episode, and T is the total number of steps",
        "Model-free approaches dispense with the model and directly update the value function\u2014the expected reward starting from each state, or the policy\u2014the mapping from states to their subsequent actions",
        "To the best of our knowledge, the only existing theoretical result on model-free Reinforcement Learning that applies to the episodic setting is for delayed Q-learning; this algorithm is quite sample-inefficient compared to model-based approaches [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We show that Q-learning, when equipped with a upper-confidence bound exploration policy values and assign exploration bonuses, achieves tthoattalinrecgorreptoOrat(e\u221as Hes3tiSmAaTte)s",
        "We use Vh\u03c0 : S \u2192 R to denote the value function at step h under policy \u03c0, so that Vh\u03c0(x) gives the expected sum of remaining rewards received under policy \u03c0, starting from xh = x, until the end of the episode",
        "We define Q\u03c0h : S \u00d7 A \u2192 R to denote Q-value function at step h so that Qh\u03c0(x, a) gives the expected sum of remaining rewards received under policy \u03c0, starting from xh = x, ah = a, till the end of the episode",
        "We present a theorem that establishes an information-theoretic lower bound for episodic Markov Decision Process",
        "The key observation is that, in the worst case the value function is at most H for any state-action pair, if we sum up the \u201ctotal variance of the value function\u201d for an entire episode, we obtain a factor of only O(H2) as opposed to the naive O(H3) bound (see",
        "To demonstrate the sharpness of our results, we note an information-theoretic lower bound for the episodic Markov Decision Process setting studied in this paper: Theorem 4",
        "The episodic Markov Decision Process with H steps per epsiode can be viewed as a contextual bandit of H \u201clayers.\u201d The key challenge here is to control the way error and confidence propagate through different \u201clayers\u201d in an online fashion, where our specific choice of exploration bonus and learning rate make the regret as sharp as possible"
    ],
    "summary": [
        "Reinforcement Learning (RL) is a control-theoretic problem in which an agent tries to maximize its cumulative rewards via interacting with an unknown environment through time [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "Oin(\u221aanHe3pSisAoTdi)c, MDP where setting, Q-learning with UCB exploration S and A are the numbers of states and actions, H is the number of steps per episode, and T is the total number of steps.",
        "In the model-based setting, a recent line of research has imported ideas from the bandit literature\u2014including the use of upper confidence bounds (UCB) and improved design of exploration bonuses\u2014and has obtained asymptotically optimal sample efficiency [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "To the best of our knowledge, the only existing theoretical result on model-free RL that applies to the episodic setting is for delayed Q-learning; this algorithm is quite sample-inefficient compared to model-based approaches [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "In episodic setting of this paper, a model-free algorithm has space complexity o(S2AH).",
        "We note that despite the sharp regret guarantees, most of the results in this line of research crucially rely on estimating and storing the entire transition matrix and suffer from unfavorable time and space complexities compared to model-free algorithms.",
        "In the model-free setting, Strehl et al [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] introduced delayed Q-learning, where, to find an \u03b5optimal policy, the Q value for each state-action pair is updated only once every m = O(1/\u03b52)",
        "We define Q\u03c0h : S \u00d7 A \u2192 R to denote Q-value function at step h so that Qh\u03c0(x, a) gives the expected sum of remaining rewards received under policy \u03c0, starting from xh = x, ah = a, till the end of the episode.",
        "Compared to the previous model-based results, Theorem 2 shows that the regret of this version of Q-learning is as good as the best model-based one in terms of the dependency on the number of states S, actions A and the total number of steps T .",
        "The key observation is that, in the worst case the value function is at most H for any state-action pair, if we sum up the \u201ctotal variance of the value function\u201d for an entire episode, we obtain a factor of only O(H2) as opposed to the naive O(H3) bound.",
        "To demonstrate the sharpness of our results, we note an information-theoretic lower bound for the episodic MDP setting studied in this paper: Theorem 4.",
        "The episodic MDP with H steps per epsiode can be viewed as a contextual bandit of H \u201clayers.\u201d The key challenge here is to control the way error and confidence propagate through different \u201clayers\u201d in an online fashion, where our specific choice of exploration bonus and learning rate make the regret as sharp as possible."
    ],
    "headline": "Achieves regret",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In Conference on Neural Information Processing Systems, pages 1184\u20131194. Curran Associates Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Optimistic%20posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Optimistic%20posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017"
        },
        {
            "id": "2",
            "entry": "[2] Mohammad Azar, Remi Munos, Mohammad Ghavamzadeh, and Hilbert J Kappen. Speedy Qlearning. In Conference on Neural Information Processing Systems, pages 2411\u20132419. Curran Associates Inc., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohammad%20Azar%20Remi%20Munos%20Mohammad%20Ghavamzadeh%20and%20Hilbert%20J%20Kappen%20Speedy%20Qlearning%20In%20Conference%20on%20Neural%20Information%20Processing%20Systems%20pages%2024112419%20Curran%20Associates%20Inc%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohammad%20Azar%20Remi%20Munos%20Mohammad%20Ghavamzadeh%20and%20Hilbert%20J%20Kappen%20Speedy%20Qlearning%20In%20Conference%20on%20Neural%20Information%20Processing%20Systems%20pages%2024112419%20Curran%20Associates%20Inc%202011"
        },
        {
            "id": "3",
            "entry": "[3] Mohammad Azar, Remi Munos, and Hilbert J. Kappen. On the sample complexity of reinforcement learning with a generative model. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20Mohammad%20Munos%2C%20Remi%20Kappen%2C%20Hilbert%20J.%20On%20the%20sample%20complexity%20of%20reinforcement%20learning%20with%20a%20generative%20model%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azar%2C%20Mohammad%20Munos%2C%20Remi%20Kappen%2C%20Hilbert%20J.%20On%20the%20sample%20complexity%20of%20reinforcement%20learning%20with%20a%20generative%20model%202012"
        },
        {
            "id": "4",
            "entry": "[4] Mohammad Azar, Remi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91(3):325\u2013 349, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20Mohammad%20Munos%2C%20Remi%20Kappen%2C%20Hilbert%20J.%20Minimax%20PAC%20bounds%20on%20the%20sample%20complexity%20of%20reinforcement%20learning%20with%20a%20generative%20model%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azar%2C%20Mohammad%20Munos%2C%20Remi%20Kappen%2C%20Hilbert%20J.%20Minimax%20PAC%20bounds%20on%20the%20sample%20complexity%20of%20reinforcement%20learning%20with%20a%20generative%20model%202013"
        },
        {
            "id": "5",
            "entry": "[5] Mohammad Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 263\u2013272, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azar%2C%20Mohammad%20Osband%2C%20Ian%20Munos%2C%20Remi%20Minimax%20regret%20bounds%20for%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azar%2C%20Mohammad%20Osband%2C%20Ian%20Munos%2C%20Remi%20Minimax%20regret%20bounds%20for%20reinforcement%20learning%202017"
        },
        {
            "id": "6",
            "entry": "[6] Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20Sebastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20Sebastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "7",
            "entry": "[7] Marc Deisenroth and Carl E Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML), pages 465\u2013472, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20Marc%20Rasmussen%2C%20Carl%20E.%20PILCO%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20Marc%20Rasmussen%2C%20Carl%20E.%20PILCO%3A%20A%20model-based%20and%20data-efficient%20approach%20to%20policy%20search%202011"
        },
        {
            "id": "8",
            "entry": "[8] Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning. Journal of Machine Learning Research, 5(Dec):1\u201325, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20Eyal%20Mansour%2C%20Yishay%20Learning%20rates%20for%20Q-learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20Eyal%20Mansour%2C%20Yishay%20Learning%20rates%20for%20Q-learning%202003"
        },
        {
            "id": "9",
            "entry": "[9] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for linearized control problems. arXiv preprint arXiv:1801.05039, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05039"
        },
        {
            "id": "10",
            "entry": "[10] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11:1563\u20131600, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010"
        },
        {
            "id": "11",
            "entry": "[11] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. arXiv preprint arXiv:1610.09512, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09512"
        },
        {
            "id": "12",
            "entry": "[12] Sham Kakade, Mengdi Wang, and Lin F Yang. Variance reduction methods for sublinear reinforcement learning. ArXiv e-prints, abs/1802.09184, April 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09184"
        },
        {
            "id": "13",
            "entry": "[13] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209\u2013232, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kearns%2C%20Michael%20Singh%2C%20Satinder%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kearns%2C%20Michael%20Singh%2C%20Satinder%20Near-optimal%20reinforcement%20learning%20in%20polynomial%20time%202002"
        },
        {
            "id": "14",
            "entry": "[14] Sven Koenig and Reid G Simmons. Complexity analysis of real-time reinforcement learning. In AAAI Conference on Artificial Intelligence, pages 99\u2013105, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koenig%2C%20Sven%20Simmons%2C%20Reid%20G.%20Complexity%20analysis%20of%20real-time%20reinforcement%20learning%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koenig%2C%20Sven%20Simmons%2C%20Reid%20G.%20Complexity%20analysis%20of%20real-time%20reinforcement%20learning%201993"
        },
        {
            "id": "15",
            "entry": "[15] Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In International Conference on Algorithmic Learning Theory, pages 320\u2013334, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lattimore%2C%20Tor%20Hutter%2C%20Marcus%20PAC%20bounds%20for%20discounted%20MDPs%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lattimore%2C%20Tor%20Hutter%2C%20Marcus%20PAC%20bounds%20for%20discounted%20MDPs%202012"
        },
        {
            "id": "16",
            "entry": "[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "17",
            "entry": "[17] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (ICML), pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "18",
            "entry": "[18] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. arXiv preprint arXiv:1708.02596, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02596"
        },
        {
            "id": "19",
            "entry": "[19] Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. ArXiv e-prints, abs/1608.02732, April 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.02732"
        },
        {
            "id": "20",
            "entry": "[20] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. arXiv preprint arXiv:1402.0635, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0635"
        },
        {
            "id": "21",
            "entry": "[21] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-free deep RL for model-based control. arXiv preprint arXiv:1802.09081, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09081"
        },
        {
            "id": "22",
            "entry": "[22] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning (ICML), pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "23",
            "entry": "[23] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster algorithms for solving Markov decision processes. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 770\u2013787. SIAM, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sidford%2C%20Aaron%20Wang%2C%20Mengdi%20Wu%2C%20Xian%20Ye%2C%20Yinyu%20Variance%20reduced%20value%20iteration%20and%20faster%20algorithms%20for%20solving%20Markov%20decision%20processes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sidford%2C%20Aaron%20Wang%2C%20Mengdi%20Wu%2C%20Xian%20Ye%2C%20Yinyu%20Variance%20reduced%20value%20iteration%20and%20faster%20algorithms%20for%20solving%20Markov%20decision%20processes%202018"
        },
        {
            "id": "24",
            "entry": "[24] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529 (7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%207587",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%207587"
        },
        {
            "id": "25",
            "entry": "[25] Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. PAC model-free reinforcement learning. In Proceedings of the 23rd International Conference on Machine Learning, pages 881\u2013888. ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Wiewiora%2C%20Eric%20Langford%2C%20John%20PAC%20model-free%20reinforcement%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Wiewiora%2C%20Eric%20Langford%2C%20John%20PAC%20model-free%20reinforcement%20learning%202006"
        },
        {
            "id": "26",
            "entry": "[26] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20Learning%3A%20An%20Introduction%201998"
        },
        {
            "id": "27",
            "entry": "[27] Christopher Watkins. Learning from delayed rewards. PhD thesis, King\u2019s College, Cambridge, 1989. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watkins%2C%20Christopher%20Learning%20from%20delayed%20rewards%201989"
        }
    ]
}
